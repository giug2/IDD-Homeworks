<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.00054] Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data</title><meta property="og:description" content="Pre-training models on Imagenet or other massive datasets of real images has led to major advances in computer vision, albeit accompanied with shortcomings related to curation cost, privacy, usage rights, and ethical iâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.00054">

<!--Generated on Sat Mar  2 02:37:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Samarth Mishra<sup id="id12.12.id1" class="ltx_sup"><span id="id12.12.id1.1" class="ltx_text ltx_font_italic">â€ 1</span></sup> â€ƒRameswar Panda<sup id="id13.13.id2" class="ltx_sup"><span id="id13.13.id2.1" class="ltx_text ltx_font_italic">2</span></sup> â€ƒCheng Perng Phoo<sup id="id14.14.id3" class="ltx_sup"><span id="id14.14.id3.1" class="ltx_text ltx_font_italic">â€ 3</span></sup> â€ƒChun-Fu (Richard) Chen<sup id="id15.15.id4" class="ltx_sup"><span id="id15.15.id4.1" class="ltx_text ltx_font_italic">âˆ—2</span></sup> 
<br class="ltx_break">â€ƒLeonid Karlinsky<sup id="id16.16.id5" class="ltx_sup">2</sup> â€ƒKate Saenko<sup id="id17.17.id6" class="ltx_sup"><span id="id17.17.id6.1" class="ltx_text ltx_font_italic">1,2</span></sup> â€ƒâ€ƒVenkatesh Saligrama<sup id="id18.18.id7" class="ltx_sup">1</sup> â€ƒâ€ƒRogerio S. Feris<sup id="id19.19.id8" class="ltx_sup"><span id="id19.19.id8.1" class="ltx_text ltx_font_italic">2</span></sup> 
<br class="ltx_break"><sup id="id20.20.id9" class="ltx_sup">1</sup>Boston University â€ƒ<sup id="id21.21.id10" class="ltx_sup">2</sup>MIT-IBM Watson AI Lab â€ƒ<sup id="id22.22.id11" class="ltx_sup">3</sup>Cornell University
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id23.id1" class="ltx_p">Pre-training models on Imagenet or other massive datasets of real images has led to major advances in computer vision, albeit accompanied with shortcomings related to curation cost, privacy, usage rights, and ethical issues. In this paper, for the first time, we study the transferability of pre-trained models based on synthetic data generated by graphics simulators to downstream tasks from very different domains.
In using such synthetic data for pre-training, we find that downstream performance on different tasks are favored by different configurations of simulation parameters (<em id="id23.id1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id23.id1.2" class="ltx_text"></span> lighting, object pose, backgrounds, etc.), and that there is no one-size-fits-all solution. It is thus better to tailor synthetic pre-training data to a specific downstream task, for best performance. We introduce Task2Sim, a unified model mapping downstream task representations to optimal simulation parameters to generate synthetic pre-training data for them. Task2Sim learns this mapping by training to find the set of best parameters on a set of â€œseenâ€ tasks. Once trained, it can then be used to predict best simulation parameters for novel â€œunseenâ€ tasks in one shot, without requiring additional training. Given a budget in number of images per class, our extensive experiments with 20 diverse downstream tasks show Task2Simâ€™s task-adaptive pre-training data results in significantly better downstream performance than non-adaptively choosing simulation parameters on both seen and unseen tasks. It is even competitive with pre-training on real images from Imagenet.</p>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><sup id="footnote1.1" class="ltx_sup">â€ </sup>Work done as interns at MIT-IBM Watson AI Lab.</span></span></span><span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><sup id="footnote1a.1" class="ltx_sup">âˆ—</sup>Now affiliated with JPMorgan Chase, FLARE. Work done when Chun-Fu was at MIT-IBM Watson AI Lab.</span></span></span><span id="footnote1b" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup>Project page : <a target="_blank" href="https://samarth4149.github.io/projects/task2sim.html" title="" class="ltx_ref ltx_href">https://samarth4149.github.io/projects/task2sim.html</a></span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2112.00054/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="332" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">We explore how synthetic data can be effectively used for training models that can transfer to a wide range of downstream tasks from various domains. Is a universal pre-trained model for all downstream tasks, the best approach?</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Using large-scale labeled (like ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>) or weakly-labeled (like JFT-300MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Instagram-3.5BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>) datasets collected from the web has been the go-to approach for pre-training classifiers for <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">downstream</em> tasks with a relative scarcity of labeled data.
Prior works have demonstrated that as we move to bigger datasets for pre-training, downstream accuracy improves on averageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
However, large-scale real image datasets bear the additional cost of curating labels, in addition to other concerns like privacy or copyright. Furthermore, large datasets like JFT-300M and Instagram-3.5B are not publicly available posing a bottleneck in reproducibility and fair comparison of algorithms.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Synthetic images generated via graphics engines provide an alternative quelling a substantial portion of these concerns. With 3D models and scenes, potentially infinite images can be generated by varying various scene or image-capture parameters. Although synthetic data has been used for transfer learning in various specialized tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, there has not been prior research dedicated to its transferability to a range of different recognition tasks from different domains (see <a href="#S1.F1" title="In 1 Introduction â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>). In conducting this first of its kind (to the best of our knowledge) study, we first ask the question : in synthetic pretraining for different downstream classification tasks, does a one-size-fits-all solution (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, a universal pre-trained model for all tasks) work well?</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With graphics engines, we can control various <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">simulation</em> parameters (lighting, pose, materials, etc.). So, in an experiment, we introduced more variations successively from different parameters into a pretraining dataset of 100k synthetic images from 237 different classes (as many categories as are available in Three-D-World <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>). We pre-trained a ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> on these, and evaluated this backbone with linear probing on different downstream tasks. The results are in Table <a href="#S1.T1" title="Table 1 â€£ 1 Introduction â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We see that some parameters like random object materials result in improved performance for some downstream tasks like SVHN and DTD, while hurting performance for other tasks like EuroSAT and Sketch. In general different pre-training data properties seem to favor different downstream tasks.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.6pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.7pt,7.2pt) scale(0.9,0.9) ;">
<table id="S1.T1.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.2.1.1.1" class="ltx_tr">
<th id="S1.T1.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" rowspan="2">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span>
<span id="S1.T1.2.1.1.1.1.1" class="ltx_text"><span id="S1.T1.2.1.1.1.1.1.1" class="ltx_text"></span> <span id="S1.T1.2.1.1.1.1.1.2" class="ltx_text">
<span id="S1.T1.2.1.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.2.1.1.1.1.1.2.1.1" class="ltx_tr">
<span id="S1.T1.2.1.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.2.1.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Pretraining Data</span></span></span>
<span id="S1.T1.2.1.1.1.1.1.2.1.2" class="ltx_tr">
<span id="S1.T1.2.1.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.2.1.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Variations</span></span></span>
</span></span> <span id="S1.T1.2.1.1.1.1.1.3" class="ltx_text"></span></span>
</th>
<td id="S1.T1.2.1.1.1.2" class="ltx_td ltx_align_center" colspan="4"><span id="S1.T1.2.1.1.1.2.1" class="ltx_text ltx_font_bold">Downstream Accuracy</span></td>
</tr>
<tr id="S1.T1.2.1.2.2" class="ltx_tr">
<td id="S1.T1.2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.2.1.1" class="ltx_text ltx_font_bold">EuroSAT</span></td>
<td id="S1.T1.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.2.2.1" class="ltx_text ltx_font_bold">SVHN</span></td>
<td id="S1.T1.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.2.3.1" class="ltx_text ltx_font_bold">Sketch</span></td>
<td id="S1.T1.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.2.4.1" class="ltx_text ltx_font_bold">DTD</span></td>
</tr>
<tr id="S1.T1.2.1.3.3" class="ltx_tr">
<td id="S1.T1.2.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Pose</td>
<td id="S1.T1.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">87.01</td>
<td id="S1.T1.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">28.49</td>
<td id="S1.T1.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">37.89</td>
<td id="S1.T1.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">37.39</td>
</tr>
<tr id="S1.T1.2.1.4.4" class="ltx_tr">
<td id="S1.T1.2.1.4.4.1" class="ltx_td ltx_align_left">+Lighting</td>
<td id="S1.T1.2.1.4.4.2" class="ltx_td ltx_align_center">88.57</td>
<td id="S1.T1.2.1.4.4.3" class="ltx_td ltx_align_center">32.36</td>
<td id="S1.T1.2.1.4.4.4" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.4.4.1" class="ltx_text ltx_font_bold">38.81</span></td>
<td id="S1.T1.2.1.4.4.5" class="ltx_td ltx_align_center">40.32</td>
</tr>
<tr id="S1.T1.2.1.5.5" class="ltx_tr">
<td id="S1.T1.2.1.5.5.1" class="ltx_td ltx_align_left">+Blur</td>
<td id="S1.T1.2.1.5.5.2" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.5.2.1" class="ltx_text ltx_font_bold">90.20</span></td>
<td id="S1.T1.2.1.5.5.3" class="ltx_td ltx_align_center">35.58</td>
<td id="S1.T1.2.1.5.5.4" class="ltx_td ltx_align_center">35.53</td>
<td id="S1.T1.2.1.5.5.5" class="ltx_td ltx_align_center">37.66</td>
</tr>
<tr id="S1.T1.2.1.6.6" class="ltx_tr">
<td id="S1.T1.2.1.6.6.1" class="ltx_td ltx_align_left">+Materials</td>
<td id="S1.T1.2.1.6.6.2" class="ltx_td ltx_align_center">84.54</td>
<td id="S1.T1.2.1.6.6.3" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.6.6.3.1" class="ltx_text ltx_font_bold">44.84</span></td>
<td id="S1.T1.2.1.6.6.4" class="ltx_td ltx_align_center">30.81</td>
<td id="S1.T1.2.1.6.6.5" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.6.6.5.1" class="ltx_text ltx_font_bold">38.51</span></td>
</tr>
<tr id="S1.T1.2.1.7.7" class="ltx_tr">
<td id="S1.T1.2.1.7.7.1" class="ltx_td ltx_align_left">+Background</td>
<td id="S1.T1.2.1.7.7.2" class="ltx_td ltx_align_center">80.44</td>
<td id="S1.T1.2.1.7.7.3" class="ltx_td ltx_align_center">29.93</td>
<td id="S1.T1.2.1.7.7.4" class="ltx_td ltx_align_center">14.60</td>
<td id="S1.T1.2.1.7.7.5" class="ltx_td ltx_align_center">32.39</td>
</tr>
<tr id="S1.T1.2.1.8.8" class="ltx_tr">
<td id="S1.T1.2.1.8.8.1" class="ltx_td ltx_align_left"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span></td>
<td id="S1.T1.2.1.8.8.2" class="ltx_td"></td>
<td id="S1.T1.2.1.8.8.3" class="ltx_td"></td>
<td id="S1.T1.2.1.8.8.4" class="ltx_td"></td>
<td id="S1.T1.2.1.8.8.5" class="ltx_td"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.4.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.5.2" class="ltx_text" style="font-size:90%;">Downstream task accuracies using linear probing with a Resnet-50 backbone pretrained on synthetic datasets with different varying parameters (successively added). We see different simulation parameters have different effects on downstream tasks.</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To maximize the benefit of pre-training, different optimal simulation parameters can be found for each specific downstream task. Because of the combinatorially large set of different simulation parameter configurations, a brute force search is out of the question. However, this might still suggest that some, presumably expensive, learning process is needed for each downstream task for an optimal synthetic image set for pre-training. We show this is not the case.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We introduce Task2Sim, a unified model that maps a downstream task representation to optimal simulation parameters for pre-training data generation to maximize downstream accuracy. Using vector representations for a set of downstream tasks (in the form of Task2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>), we train Task2Sim to find and thus learn a mapping to optimal parameters for each task from the set. Once trained on this set of â€œseenâ€ tasks, Task2Sim can also use Task2Vec representations of novel â€œunseenâ€ tasks to predict simulation parameters that would be best for their pre-training datasets. This efficient one-shot prediction for novel tasks is of significant practical value, if developed as an end-user application that can automatically generate and provide pre-training data, given some downstream examples.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our extensive experiments using <math id="S1.p6.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S1.p6.1.m1.1a"><mn id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><cn type="integer" id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">20</annotation></semantics></math> downstream classification datasets show that on seen tasks, given a number of images per category, Task2Simâ€™s output parameters generate pre-training datasets that are much better for downstream performance than approaches like domain randomizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> that are not task-adaptive. Moreover, we show Task2Sim also generalizes well to unseen tasks, maintaining an edge over non-adaptive approaches while being competitive with Imagenet pre-training.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In summary,
(i) We address a novel, and very practical, problemâ€”how to optimally leverage synthetic data to task-adaptively pre-train deep learning models for transfer to diverse downstream tasks.
To the best of our knowledge, this is the first time such a problem is being addressed in transfer learning research.
(ii) We propose Task2Sim, a unified parametric model that learns to map Task2Vec representations of downstream tasks to simulation parameters for optimal pre-training.
(iii) Task2Sim can generalize to novel â€œunseenâ€ tasks, not encountered during training, a feature of significant practical value as an application.
(iv) We provide a thorough analysis of the behavior of downstream accuracy with different sizes of pre-training data (in number of classes, object-meshes or simply images) and with different downstream evaluation methods.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Training with Synthetic Data.</span> Methods that learn from synthetic data have been extensively studied since the early days of computer visionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. In recent years, many approaches that rely on synthetic data representations have been proposed for image classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, semantic segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, action recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, visual reasoningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and embodied perceptionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. While most of these rely on some graphics engines to generate synthetic images mimicking real ones, it has been observed that images seemingly consistent of noise can still be useful for representation learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Unlike previous work, we focus on a different problem: how to build task-adaptive pre-trained models from synthetic data that can transfer to a wide range of downstream datasets from various domains.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2112.00054/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of our proposed approach<span id="S2.F2.4.2.1" class="ltx_text ltx_font_medium">. Given a batch of tasks represented by Task2Vec representations, our approach (Task2Sim) aims to map these representations to optimal simulation parameters for generating a dataset of synthetic images. The downstream classifierâ€™s accuracy for the set of tasks is then used as a reward to update Task2Simâ€™s parameters. Once trained, Task2Sim can be used not only for â€œseenâ€ tasks but also can be used in one-shot to generate simulation parameters for novel â€œunseenâ€ tasks.</span></span></figcaption>
</figure>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Synthetic to Real Transfer.</span> The majority of methods proposed to bridge the <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">reality gap</span> (between simulation and real data) are based on domain adaptationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. These include reconstruction-based techniques, using encoder-decoder models or GANs to improve the realism of synthetic dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, discrepancy-based methods, designed to align features between the two domainsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, and adversarial approaches, which rely on a domain discriminator to encourage domain-independent feature learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. Contrasting from these techniques, our work aims at building pre-trained models from synthetic data and does not assume the same label set for source and target domains.
The most prevalent approach in a setting similar to ours, is domain randomizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which learns pre-trained models from datasets generated by randomly varying simulator parameters. In contrast, Task2Sim <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">learns</span> simulator parameters to generate synthetic datasets that maximize transfer learning performance.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Optimization of Simulator Parameters.</span> Recently, a few approaches have been proposed to learn synthetic data generation by optimizing simulator parametersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. SPIRALÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, AVOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and Attr. Desc.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> minimize the distance between distributions of simulated data and real data. Learning to SimulateÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> optimizes simulator parameters using policy gradients that maximize validation accuracy for a specific task, while Auto-SimÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> speeds up the search process using a differentiable approximation of the objective.
Meta-SimÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> learns to modify attributes obtained from probabilistic scene grammars for data generation. These methods are specifically tailored to applications in autonomous driving, whereas our goal is to transfer synthetic data representations to a wide range of downstream tasks. Notably, our proposed approach is significantly different from previous methods, as it maps task representations to simulation parameters through a unified parametric model, enabling one-shot synthetic data generation, even for unseen tasks, without requiring expensive training.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Conditional Computation.</span> Albeit not apparent, our method is also related to dynamic neural network models that adaptively change computation depending on the inputÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. These methods have been effectively used to skip computation in deep neural networks conditioned on the inputÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, perform adaptive fine-tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and dynamically allocate computation across frames for efficient video analysisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. In particular, AdashareÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> learns different computational pathways for each task within a single multi-task network model, with the goal of improving efficiency and minimizing negative interference in multi-task learning. Analogously, our approach learns different <span id="S2.p4.1.2" class="ltx_text ltx_font_italic">data simulation pathways</span> (by adaptively deciding which rendering parameters to use) for each task, using a single parametric model, with the goal of generating task-specific pre-training data.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal is to create a unified model that maps task representations (e.g., obtained using task2vecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>) to simulation parameters, which are in turn used to render synthetic pre-training datasets for not only tasks that are seen during training, but also novel tasks.
This is a challenging problem, as the number of possible simulation parameter configurations is combinatorially large, making a brute-force approach infeasible when the number of parameters grows.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><a href="#S2.F2" title="In 2 Related Work â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of our approach. During training, a batch of â€œseenâ€ tasks is provided as input. Their task2vec vector representations are fed as input to Task2Sim, which is a parametric model (shared across all tasks) mapping these downstream task2vecs to simulation parameters, such as lighting direction, amount of blur, background variability, etc. These parameters are then used by a data generator (in our implementation, built using the Three-D-World platformÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) to generate a dataset of synthetic images. A classifier model then gets pre-trained on these synthetic images, and the backbone is subsequently used for evaluation on specific downstream task. The classifierâ€™s accuracy on this task is used as a reward to update Task2Simâ€™s parameters.
Once trained, Task2Sim can also be used to efficiently predict simulation parameters in <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">one-shot</span> for â€œunseenâ€ tasks that it has not encountered during training.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Task2Sim Model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.14" class="ltx_p">Let us denote Task2Simâ€™s parameters with <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\theta</annotation></semantics></math>. Given the task2vec representation of a downstream task <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\boldsymbol{x}\in\mathcal{X}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">ğ’™</mi><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">ğ’³</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><in id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></in><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ’™</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">ğ’³</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\boldsymbol{x}\in\mathcal{X}</annotation></semantics></math> as input, Task2Sim outputs simulation parameters <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="a\in\Omega" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">a</mi><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">âˆˆ</mo><mi mathvariant="normal" id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">Î©</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><in id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></in><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">Î©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">a\in\Omega</annotation></semantics></math>. The model consists of <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">M</annotation></semantics></math> output heads, one for each simulation parameter. In the following discussion, just as in our experiments, each simulation parameter is discretized to a few levels to limit the space of possible outputs. Each head outputs a categorical distribution <math id="S3.SS2.p1.5.m5.2" class="ltx_Math" alttext="\pi_{i}(\boldsymbol{x},\theta)\in\Delta^{k_{i}}" display="inline"><semantics id="S3.SS2.p1.5.m5.2a"><mrow id="S3.SS2.p1.5.m5.2.3" xref="S3.SS2.p1.5.m5.2.3.cmml"><mrow id="S3.SS2.p1.5.m5.2.3.2" xref="S3.SS2.p1.5.m5.2.3.2.cmml"><msub id="S3.SS2.p1.5.m5.2.3.2.2" xref="S3.SS2.p1.5.m5.2.3.2.2.cmml"><mi id="S3.SS2.p1.5.m5.2.3.2.2.2" xref="S3.SS2.p1.5.m5.2.3.2.2.2.cmml">Ï€</mi><mi id="S3.SS2.p1.5.m5.2.3.2.2.3" xref="S3.SS2.p1.5.m5.2.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.2.3.2.1" xref="S3.SS2.p1.5.m5.2.3.2.1.cmml">â€‹</mo><mrow id="S3.SS2.p1.5.m5.2.3.2.3.2" xref="S3.SS2.p1.5.m5.2.3.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m5.2.3.2.3.2.1" xref="S3.SS2.p1.5.m5.2.3.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">ğ’™</mi><mo id="S3.SS2.p1.5.m5.2.3.2.3.2.2" xref="S3.SS2.p1.5.m5.2.3.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.5.m5.2.2" xref="S3.SS2.p1.5.m5.2.2.cmml">Î¸</mi><mo stretchy="false" id="S3.SS2.p1.5.m5.2.3.2.3.2.3" xref="S3.SS2.p1.5.m5.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.5.m5.2.3.1" xref="S3.SS2.p1.5.m5.2.3.1.cmml">âˆˆ</mo><msup id="S3.SS2.p1.5.m5.2.3.3" xref="S3.SS2.p1.5.m5.2.3.3.cmml"><mi mathvariant="normal" id="S3.SS2.p1.5.m5.2.3.3.2" xref="S3.SS2.p1.5.m5.2.3.3.2.cmml">Î”</mi><msub id="S3.SS2.p1.5.m5.2.3.3.3" xref="S3.SS2.p1.5.m5.2.3.3.3.cmml"><mi id="S3.SS2.p1.5.m5.2.3.3.3.2" xref="S3.SS2.p1.5.m5.2.3.3.3.2.cmml">k</mi><mi id="S3.SS2.p1.5.m5.2.3.3.3.3" xref="S3.SS2.p1.5.m5.2.3.3.3.3.cmml">i</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.2b"><apply id="S3.SS2.p1.5.m5.2.3.cmml" xref="S3.SS2.p1.5.m5.2.3"><in id="S3.SS2.p1.5.m5.2.3.1.cmml" xref="S3.SS2.p1.5.m5.2.3.1"></in><apply id="S3.SS2.p1.5.m5.2.3.2.cmml" xref="S3.SS2.p1.5.m5.2.3.2"><times id="S3.SS2.p1.5.m5.2.3.2.1.cmml" xref="S3.SS2.p1.5.m5.2.3.2.1"></times><apply id="S3.SS2.p1.5.m5.2.3.2.2.cmml" xref="S3.SS2.p1.5.m5.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.2.3.2.2.1.cmml" xref="S3.SS2.p1.5.m5.2.3.2.2">subscript</csymbol><ci id="S3.SS2.p1.5.m5.2.3.2.2.2.cmml" xref="S3.SS2.p1.5.m5.2.3.2.2.2">ğœ‹</ci><ci id="S3.SS2.p1.5.m5.2.3.2.2.3.cmml" xref="S3.SS2.p1.5.m5.2.3.2.2.3">ğ‘–</ci></apply><interval closure="open" id="S3.SS2.p1.5.m5.2.3.2.3.1.cmml" xref="S3.SS2.p1.5.m5.2.3.2.3.2"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">ğ’™</ci><ci id="S3.SS2.p1.5.m5.2.2.cmml" xref="S3.SS2.p1.5.m5.2.2">ğœƒ</ci></interval></apply><apply id="S3.SS2.p1.5.m5.2.3.3.cmml" xref="S3.SS2.p1.5.m5.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.2.3.3.1.cmml" xref="S3.SS2.p1.5.m5.2.3.3">superscript</csymbol><ci id="S3.SS2.p1.5.m5.2.3.3.2.cmml" xref="S3.SS2.p1.5.m5.2.3.3.2">Î”</ci><apply id="S3.SS2.p1.5.m5.2.3.3.3.cmml" xref="S3.SS2.p1.5.m5.2.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.2.3.3.3.1.cmml" xref="S3.SS2.p1.5.m5.2.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.5.m5.2.3.3.3.2.cmml" xref="S3.SS2.p1.5.m5.2.3.3.3.2">ğ‘˜</ci><ci id="S3.SS2.p1.5.m5.2.3.3.3.3.cmml" xref="S3.SS2.p1.5.m5.2.3.3.3.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.2c">\pi_{i}(\boldsymbol{x},\theta)\in\Delta^{k_{i}}</annotation></semantics></math>, where <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">k</mi><mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">ğ‘˜</ci><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">k_{i}</annotation></semantics></math> is the number of discrete values for parameter <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="i\in[M]" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mrow id="S3.SS2.p1.7.m7.1.2" xref="S3.SS2.p1.7.m7.1.2.cmml"><mi id="S3.SS2.p1.7.m7.1.2.2" xref="S3.SS2.p1.7.m7.1.2.2.cmml">i</mi><mo id="S3.SS2.p1.7.m7.1.2.1" xref="S3.SS2.p1.7.m7.1.2.1.cmml">âˆˆ</mo><mrow id="S3.SS2.p1.7.m7.1.2.3.2" xref="S3.SS2.p1.7.m7.1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.7.m7.1.2.3.2.1" xref="S3.SS2.p1.7.m7.1.2.3.1.1.cmml">[</mo><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">M</mi><mo stretchy="false" id="S3.SS2.p1.7.m7.1.2.3.2.2" xref="S3.SS2.p1.7.m7.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.2.cmml" xref="S3.SS2.p1.7.m7.1.2"><in id="S3.SS2.p1.7.m7.1.2.1.cmml" xref="S3.SS2.p1.7.m7.1.2.1"></in><ci id="S3.SS2.p1.7.m7.1.2.2.cmml" xref="S3.SS2.p1.7.m7.1.2.2">ğ‘–</ci><apply id="S3.SS2.p1.7.m7.1.2.3.1.cmml" xref="S3.SS2.p1.7.m7.1.2.3.2"><csymbol cd="latexml" id="S3.SS2.p1.7.m7.1.2.3.1.1.cmml" xref="S3.SS2.p1.7.m7.1.2.3.2.1">delimited-[]</csymbol><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">ğ‘€</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">i\in[M]</annotation></semantics></math>, and <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="\Delta^{k_{i}}" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><msup id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">Î”</mi><msub id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml"><mi id="S3.SS2.p1.8.m8.1.1.3.2" xref="S3.SS2.p1.8.m8.1.1.3.2.cmml">k</mi><mi id="S3.SS2.p1.8.m8.1.1.3.3" xref="S3.SS2.p1.8.m8.1.1.3.3.cmml">i</mi></msub></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">superscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">Î”</ci><apply id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.3.1.cmml" xref="S3.SS2.p1.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.3.2.cmml" xref="S3.SS2.p1.8.m8.1.1.3.2">ğ‘˜</ci><ci id="S3.SS2.p1.8.m8.1.1.3.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">\Delta^{k_{i}}</annotation></semantics></math>, a standard <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><msub id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml"><mi id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">k</mi><mi id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2">ğ‘˜</ci><ci id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">k_{i}</annotation></semantics></math>-simplex. The set of argmax outputs <math id="S3.SS2.p1.10.m10.8" class="ltx_Math" alttext="\nu(\boldsymbol{x},\theta)=\{\nu_{i}|\nu_{i}=\operatorname*{arg\,max}_{j\in[k_{i}]}\pi_{i,j}~{}\forall i\in[M]\}" display="inline"><semantics id="S3.SS2.p1.10.m10.8a"><mrow id="S3.SS2.p1.10.m10.8.8" xref="S3.SS2.p1.10.m10.8.8.cmml"><mrow id="S3.SS2.p1.10.m10.8.8.4" xref="S3.SS2.p1.10.m10.8.8.4.cmml"><mi id="S3.SS2.p1.10.m10.8.8.4.2" xref="S3.SS2.p1.10.m10.8.8.4.2.cmml">Î½</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.10.m10.8.8.4.1" xref="S3.SS2.p1.10.m10.8.8.4.1.cmml">â€‹</mo><mrow id="S3.SS2.p1.10.m10.8.8.4.3.2" xref="S3.SS2.p1.10.m10.8.8.4.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.10.m10.8.8.4.3.2.1" xref="S3.SS2.p1.10.m10.8.8.4.3.1.cmml">(</mo><mi id="S3.SS2.p1.10.m10.4.4" xref="S3.SS2.p1.10.m10.4.4.cmml">ğ’™</mi><mo id="S3.SS2.p1.10.m10.8.8.4.3.2.2" xref="S3.SS2.p1.10.m10.8.8.4.3.1.cmml">,</mo><mi id="S3.SS2.p1.10.m10.5.5" xref="S3.SS2.p1.10.m10.5.5.cmml">Î¸</mi><mo stretchy="false" id="S3.SS2.p1.10.m10.8.8.4.3.2.3" xref="S3.SS2.p1.10.m10.8.8.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.10.m10.8.8.3" xref="S3.SS2.p1.10.m10.8.8.3.cmml">=</mo><mrow id="S3.SS2.p1.10.m10.8.8.2.2" xref="S3.SS2.p1.10.m10.8.8.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.10.m10.8.8.2.2.3" xref="S3.SS2.p1.10.m10.8.8.2.3.1.cmml">{</mo><msub id="S3.SS2.p1.10.m10.7.7.1.1.1" xref="S3.SS2.p1.10.m10.7.7.1.1.1.cmml"><mi id="S3.SS2.p1.10.m10.7.7.1.1.1.2" xref="S3.SS2.p1.10.m10.7.7.1.1.1.2.cmml">Î½</mi><mi id="S3.SS2.p1.10.m10.7.7.1.1.1.3" xref="S3.SS2.p1.10.m10.7.7.1.1.1.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.10.m10.8.8.2.2.4" xref="S3.SS2.p1.10.m10.8.8.2.3.1.cmml">|</mo><mrow id="S3.SS2.p1.10.m10.8.8.2.2.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.cmml"><msub id="S3.SS2.p1.10.m10.8.8.2.2.2.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.2.cmml"><mi id="S3.SS2.p1.10.m10.8.8.2.2.2.2.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.2.2.cmml">Î½</mi><mi id="S3.SS2.p1.10.m10.8.8.2.2.2.2.3" xref="S3.SS2.p1.10.m10.8.8.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p1.10.m10.8.8.2.2.2.3" xref="S3.SS2.p1.10.m10.8.8.2.2.2.3.cmml">=</mo><mrow id="S3.SS2.p1.10.m10.8.8.2.2.2.4" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.cmml"><mrow id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.cmml"><msub id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.cmml"><mrow id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.cmml"><mi id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.1" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.1.cmml">â€‹</mo><mi id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.3" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.3.cmml">max</mi></mrow><mrow id="S3.SS2.p1.10.m10.1.1.1" xref="S3.SS2.p1.10.m10.1.1.1.cmml"><mi id="S3.SS2.p1.10.m10.1.1.1.3" xref="S3.SS2.p1.10.m10.1.1.1.3.cmml">j</mi><mo id="S3.SS2.p1.10.m10.1.1.1.2" xref="S3.SS2.p1.10.m10.1.1.1.2.cmml">âˆˆ</mo><mrow id="S3.SS2.p1.10.m10.1.1.1.1.1" xref="S3.SS2.p1.10.m10.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.10.m10.1.1.1.1.1.2" xref="S3.SS2.p1.10.m10.1.1.1.1.2.1.cmml">[</mo><msub id="S3.SS2.p1.10.m10.1.1.1.1.1.1" xref="S3.SS2.p1.10.m10.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.10.m10.1.1.1.1.1.1.2" xref="S3.SS2.p1.10.m10.1.1.1.1.1.1.2.cmml">k</mi><mi id="S3.SS2.p1.10.m10.1.1.1.1.1.1.3" xref="S3.SS2.p1.10.m10.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p1.10.m10.1.1.1.1.1.3" xref="S3.SS2.p1.10.m10.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></msub><mo id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2a" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.cmml">â¡</mo><msub id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2.cmml"><mi id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2.2.cmml">Ï€</mi><mrow id="S3.SS2.p1.10.m10.3.3.2.4" xref="S3.SS2.p1.10.m10.3.3.2.3.cmml"><mi id="S3.SS2.p1.10.m10.2.2.1.1" xref="S3.SS2.p1.10.m10.2.2.1.1.cmml">i</mi><mo id="S3.SS2.p1.10.m10.3.3.2.4.1" xref="S3.SS2.p1.10.m10.3.3.2.3.cmml">,</mo><mi id="S3.SS2.p1.10.m10.3.3.2.2" xref="S3.SS2.p1.10.m10.3.3.2.2.cmml">j</mi></mrow></msub></mrow><mo lspace="0.167em" rspace="0em" id="S3.SS2.p1.10.m10.8.8.2.2.2.4.1" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.1.cmml">â€‹</mo><mrow id="S3.SS2.p1.10.m10.8.8.2.2.2.4.3" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.cmml"><mo rspace="0.167em" id="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.1" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.1.cmml">âˆ€</mo><mi id="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.2.cmml">i</mi></mrow></mrow><mo id="S3.SS2.p1.10.m10.8.8.2.2.2.5" xref="S3.SS2.p1.10.m10.8.8.2.2.2.5.cmml">âˆˆ</mo><mrow id="S3.SS2.p1.10.m10.8.8.2.2.2.6.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.6.1.cmml"><mo stretchy="false" id="S3.SS2.p1.10.m10.8.8.2.2.2.6.2.1" xref="S3.SS2.p1.10.m10.8.8.2.2.2.6.1.1.cmml">[</mo><mi id="S3.SS2.p1.10.m10.6.6" xref="S3.SS2.p1.10.m10.6.6.cmml">M</mi><mo stretchy="false" id="S3.SS2.p1.10.m10.8.8.2.2.2.6.2.2" xref="S3.SS2.p1.10.m10.8.8.2.2.2.6.1.1.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.p1.10.m10.8.8.2.2.5" xref="S3.SS2.p1.10.m10.8.8.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.8b"><apply id="S3.SS2.p1.10.m10.8.8.cmml" xref="S3.SS2.p1.10.m10.8.8"><eq id="S3.SS2.p1.10.m10.8.8.3.cmml" xref="S3.SS2.p1.10.m10.8.8.3"></eq><apply id="S3.SS2.p1.10.m10.8.8.4.cmml" xref="S3.SS2.p1.10.m10.8.8.4"><times id="S3.SS2.p1.10.m10.8.8.4.1.cmml" xref="S3.SS2.p1.10.m10.8.8.4.1"></times><ci id="S3.SS2.p1.10.m10.8.8.4.2.cmml" xref="S3.SS2.p1.10.m10.8.8.4.2">ğœˆ</ci><interval closure="open" id="S3.SS2.p1.10.m10.8.8.4.3.1.cmml" xref="S3.SS2.p1.10.m10.8.8.4.3.2"><ci id="S3.SS2.p1.10.m10.4.4.cmml" xref="S3.SS2.p1.10.m10.4.4">ğ’™</ci><ci id="S3.SS2.p1.10.m10.5.5.cmml" xref="S3.SS2.p1.10.m10.5.5">ğœƒ</ci></interval></apply><apply id="S3.SS2.p1.10.m10.8.8.2.3.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2"><csymbol cd="latexml" id="S3.SS2.p1.10.m10.8.8.2.3.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.3">conditional-set</csymbol><apply id="S3.SS2.p1.10.m10.7.7.1.1.1.cmml" xref="S3.SS2.p1.10.m10.7.7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.7.7.1.1.1.1.cmml" xref="S3.SS2.p1.10.m10.7.7.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.10.m10.7.7.1.1.1.2.cmml" xref="S3.SS2.p1.10.m10.7.7.1.1.1.2">ğœˆ</ci><ci id="S3.SS2.p1.10.m10.7.7.1.1.1.3.cmml" xref="S3.SS2.p1.10.m10.7.7.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2"><and id="S3.SS2.p1.10.m10.8.8.2.2.2a.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2"></and><apply id="S3.SS2.p1.10.m10.8.8.2.2.2b.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2"><eq id="S3.SS2.p1.10.m10.8.8.2.2.2.3.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.3"></eq><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.8.8.2.2.2.2.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.10.m10.8.8.2.2.2.2.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.2.2">ğœˆ</ci><ci id="S3.SS2.p1.10.m10.8.8.2.2.2.2.3.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.4.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4"><times id="S3.SS2.p1.10.m10.8.8.2.2.2.4.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.1"></times><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2"><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1">subscript</csymbol><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2"><times id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.1"></times><ci id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.2">arg</ci><ci id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.3.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.1.2.3">max</ci></apply><apply id="S3.SS2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1.1"><in id="S3.SS2.p1.10.m10.1.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.1.2"></in><ci id="S3.SS2.p1.10.m10.1.1.1.3.cmml" xref="S3.SS2.p1.10.m10.1.1.1.3">ğ‘—</ci><apply id="S3.SS2.p1.10.m10.1.1.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.10.m10.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.10.m10.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.p1.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.10.m10.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.1.1.1.1.2">ğ‘˜</ci><ci id="S3.SS2.p1.10.m10.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.10.m10.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2">subscript</csymbol><ci id="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.2.2.2">ğœ‹</ci><list id="S3.SS2.p1.10.m10.3.3.2.3.cmml" xref="S3.SS2.p1.10.m10.3.3.2.4"><ci id="S3.SS2.p1.10.m10.2.2.1.1.cmml" xref="S3.SS2.p1.10.m10.2.2.1.1">ğ‘–</ci><ci id="S3.SS2.p1.10.m10.3.3.2.2.cmml" xref="S3.SS2.p1.10.m10.3.3.2.2">ğ‘—</ci></list></apply></apply><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.3"><csymbol cd="latexml" id="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.1">for-all</csymbol><ci id="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.2.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.4.3.2">ğ‘–</ci></apply></apply></apply><apply id="S3.SS2.p1.10.m10.8.8.2.2.2c.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2"><in id="S3.SS2.p1.10.m10.8.8.2.2.2.5.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.5"></in><share href="#S3.SS2.p1.10.m10.8.8.2.2.2.4.cmml" id="S3.SS2.p1.10.m10.8.8.2.2.2d.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2"></share><apply id="S3.SS2.p1.10.m10.8.8.2.2.2.6.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.6.2"><csymbol cd="latexml" id="S3.SS2.p1.10.m10.8.8.2.2.2.6.1.1.cmml" xref="S3.SS2.p1.10.m10.8.8.2.2.2.6.2.1">delimited-[]</csymbol><ci id="S3.SS2.p1.10.m10.6.6.cmml" xref="S3.SS2.p1.10.m10.6.6">ğ‘€</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.8c">\nu(\boldsymbol{x},\theta)=\{\nu_{i}|\nu_{i}=\operatorname*{arg\,max}_{j\in[k_{i}]}\pi_{i,j}~{}\forall i\in[M]\}</annotation></semantics></math> is the set of simulation parameter values used for synthetic data generation. Subsequently, we drop annotating the dependence of <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><mi id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">Ï€</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><ci id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">ğœ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">\pi</annotation></semantics></math> and <math id="S3.SS2.p1.12.m12.1" class="ltx_Math" alttext="\nu" display="inline"><semantics id="S3.SS2.p1.12.m12.1a"><mi id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml">Î½</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><ci id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">ğœˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">\nu</annotation></semantics></math> on <math id="S3.SS2.p1.13.m13.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p1.13.m13.1a"><mi id="S3.SS2.p1.13.m13.1.1" xref="S3.SS2.p1.13.m13.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m13.1b"><ci id="S3.SS2.p1.13.m13.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m13.1c">\theta</annotation></semantics></math> and <math id="S3.SS2.p1.14.m14.1" class="ltx_Math" alttext="\boldsymbol{x}" display="inline"><semantics id="S3.SS2.p1.14.m14.1a"><mi id="S3.SS2.p1.14.m14.1.1" xref="S3.SS2.p1.14.m14.1.1.cmml">ğ’™</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m14.1b"><ci id="S3.SS2.p1.14.m14.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1">ğ’™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m14.1c">\boldsymbol{x}</annotation></semantics></math> when clear.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Task2Sim Training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Since Task2Sim aims to maximize downstream accuracy after pre-training, we use this accuracy as the reward in our training optimization<span id="footnote1c" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that our rewards depend only on the task2vec input and the output action and do not involve any states, and thus our problem can be considered similar to a stateless-RL or contextual bandits problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</span></span></span>.
Note that this downstream accuracy is a non-differentiable function of the output simulation parameters (assuming any simulation engine can be used as a black box) and hence direct gradient-based optimization cannot be used to train Task2Sim. Instead, we use REINFORCEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, to approximate gradients of downstream task performance with respect to model parameters <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\theta</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.7" class="ltx_p">Task2Simâ€™s outputs represent a distribution over â€œactionsâ€ corresponding to different values of the set of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">M</annotation></semantics></math> simulation parameters. <math id="S3.SS3.p2.2.m2.3" class="ltx_Math" alttext="P(a)=\prod_{i\in[M]}\pi_{i}(a_{i})" display="inline"><semantics id="S3.SS3.p2.2.m2.3a"><mrow id="S3.SS3.p2.2.m2.3.3" xref="S3.SS3.p2.2.m2.3.3.cmml"><mrow id="S3.SS3.p2.2.m2.3.3.3" xref="S3.SS3.p2.2.m2.3.3.3.cmml"><mi id="S3.SS3.p2.2.m2.3.3.3.2" xref="S3.SS3.p2.2.m2.3.3.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.3.3.3.1" xref="S3.SS3.p2.2.m2.3.3.3.1.cmml">â€‹</mo><mrow id="S3.SS3.p2.2.m2.3.3.3.3.2" xref="S3.SS3.p2.2.m2.3.3.3.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.3.3.3.3.2.1" xref="S3.SS3.p2.2.m2.3.3.3.cmml">(</mo><mi id="S3.SS3.p2.2.m2.2.2" xref="S3.SS3.p2.2.m2.2.2.cmml">a</mi><mo stretchy="false" id="S3.SS3.p2.2.m2.3.3.3.3.2.2" xref="S3.SS3.p2.2.m2.3.3.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.SS3.p2.2.m2.3.3.2" xref="S3.SS3.p2.2.m2.3.3.2.cmml">=</mo><mrow id="S3.SS3.p2.2.m2.3.3.1" xref="S3.SS3.p2.2.m2.3.3.1.cmml"><msub id="S3.SS3.p2.2.m2.3.3.1.2" xref="S3.SS3.p2.2.m2.3.3.1.2.cmml"><mo id="S3.SS3.p2.2.m2.3.3.1.2.2" xref="S3.SS3.p2.2.m2.3.3.1.2.2.cmml">âˆ</mo><mrow id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.1.3" xref="S3.SS3.p2.2.m2.1.1.1.3.cmml">i</mi><mo id="S3.SS3.p2.2.m2.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.2.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.2.m2.1.1.1.4.2" xref="S3.SS3.p2.2.m2.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.1.1.1.4.2.1" xref="S3.SS3.p2.2.m2.1.1.1.4.1.1.cmml">[</mo><mi id="S3.SS3.p2.2.m2.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml">M</mi><mo stretchy="false" id="S3.SS3.p2.2.m2.1.1.1.4.2.2" xref="S3.SS3.p2.2.m2.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></msub><mrow id="S3.SS3.p2.2.m2.3.3.1.1" xref="S3.SS3.p2.2.m2.3.3.1.1.cmml"><msub id="S3.SS3.p2.2.m2.3.3.1.1.3" xref="S3.SS3.p2.2.m2.3.3.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.3.3.1.1.3.2" xref="S3.SS3.p2.2.m2.3.3.1.1.3.2.cmml">Ï€</mi><mi id="S3.SS3.p2.2.m2.3.3.1.1.3.3" xref="S3.SS3.p2.2.m2.3.3.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.3.3.1.1.2" xref="S3.SS3.p2.2.m2.3.3.1.1.2.cmml">â€‹</mo><mrow id="S3.SS3.p2.2.m2.3.3.1.1.1.1" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.3.3.1.1.1.1.2" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p2.2.m2.3.3.1.1.1.1.1" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.2" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.3" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.2.m2.3.3.1.1.1.1.3" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.3b"><apply id="S3.SS3.p2.2.m2.3.3.cmml" xref="S3.SS3.p2.2.m2.3.3"><eq id="S3.SS3.p2.2.m2.3.3.2.cmml" xref="S3.SS3.p2.2.m2.3.3.2"></eq><apply id="S3.SS3.p2.2.m2.3.3.3.cmml" xref="S3.SS3.p2.2.m2.3.3.3"><times id="S3.SS3.p2.2.m2.3.3.3.1.cmml" xref="S3.SS3.p2.2.m2.3.3.3.1"></times><ci id="S3.SS3.p2.2.m2.3.3.3.2.cmml" xref="S3.SS3.p2.2.m2.3.3.3.2">ğ‘ƒ</ci><ci id="S3.SS3.p2.2.m2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2">ğ‘</ci></apply><apply id="S3.SS3.p2.2.m2.3.3.1.cmml" xref="S3.SS3.p2.2.m2.3.3.1"><apply id="S3.SS3.p2.2.m2.3.3.1.2.cmml" xref="S3.SS3.p2.2.m2.3.3.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.3.3.1.2.1.cmml" xref="S3.SS3.p2.2.m2.3.3.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.SS3.p2.2.m2.3.3.1.2.2.cmml" xref="S3.SS3.p2.2.m2.3.3.1.2.2">product</csymbol><apply id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"><in id="S3.SS3.p2.2.m2.1.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.2"></in><ci id="S3.SS3.p2.2.m2.1.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.1.3">ğ‘–</ci><apply id="S3.SS3.p2.2.m2.1.1.1.4.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.4.2"><csymbol cd="latexml" id="S3.SS3.p2.2.m2.1.1.1.4.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S3.SS3.p2.2.m2.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1">ğ‘€</ci></apply></apply></apply><apply id="S3.SS3.p2.2.m2.3.3.1.1.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1"><times id="S3.SS3.p2.2.m2.3.3.1.1.2.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.2"></times><apply id="S3.SS3.p2.2.m2.3.3.1.1.3.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.3.3.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.3">subscript</csymbol><ci id="S3.SS3.p2.2.m2.3.3.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.3.2">ğœ‹</ci><ci id="S3.SS3.p2.2.m2.3.3.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.3.3">ğ‘–</ci></apply><apply id="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.2">ğ‘</ci><ci id="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.2.m2.3.3.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.3c">P(a)=\prod_{i\in[M]}\pi_{i}(a_{i})</annotation></semantics></math> is the probability of picking action <math id="S3.SS3.p2.3.m3.2" class="ltx_Math" alttext="a=[a_{i}]_{i\in[M]}" display="inline"><semantics id="S3.SS3.p2.3.m3.2a"><mrow id="S3.SS3.p2.3.m3.2.2" xref="S3.SS3.p2.3.m3.2.2.cmml"><mi id="S3.SS3.p2.3.m3.2.2.3" xref="S3.SS3.p2.3.m3.2.2.3.cmml">a</mi><mo id="S3.SS3.p2.3.m3.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.cmml">=</mo><msub id="S3.SS3.p2.3.m3.2.2.1" xref="S3.SS3.p2.3.m3.2.2.1.cmml"><mrow id="S3.SS3.p2.3.m3.2.2.1.1.1" xref="S3.SS3.p2.3.m3.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p2.3.m3.2.2.1.1.1.2" xref="S3.SS3.p2.3.m3.2.2.1.1.2.1.cmml">[</mo><msub id="S3.SS3.p2.3.m3.2.2.1.1.1.1" xref="S3.SS3.p2.3.m3.2.2.1.1.1.1.cmml"><mi id="S3.SS3.p2.3.m3.2.2.1.1.1.1.2" xref="S3.SS3.p2.3.m3.2.2.1.1.1.1.2.cmml">a</mi><mi id="S3.SS3.p2.3.m3.2.2.1.1.1.1.3" xref="S3.SS3.p2.3.m3.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.3.m3.2.2.1.1.1.3" xref="S3.SS3.p2.3.m3.2.2.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.1.3" xref="S3.SS3.p2.3.m3.1.1.1.3.cmml">i</mi><mo id="S3.SS3.p2.3.m3.1.1.1.2" xref="S3.SS3.p2.3.m3.1.1.1.2.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.3.m3.1.1.1.4.2" xref="S3.SS3.p2.3.m3.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.SS3.p2.3.m3.1.1.1.4.2.1" xref="S3.SS3.p2.3.m3.1.1.1.4.1.1.cmml">[</mo><mi id="S3.SS3.p2.3.m3.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.cmml">M</mi><mo stretchy="false" id="S3.SS3.p2.3.m3.1.1.1.4.2.2" xref="S3.SS3.p2.3.m3.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.2b"><apply id="S3.SS3.p2.3.m3.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2"><eq id="S3.SS3.p2.3.m3.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2"></eq><ci id="S3.SS3.p2.3.m3.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.3">ğ‘</ci><apply id="S3.SS3.p2.3.m3.2.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.2.1.2.cmml" xref="S3.SS3.p2.3.m3.2.2.1">subscript</csymbol><apply id="S3.SS3.p2.3.m3.2.2.1.1.2.cmml" xref="S3.SS3.p2.3.m3.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.3.m3.2.2.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS3.p2.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.2.2.1.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.2.2.1.1.1.1.2">ğ‘</ci><ci id="S3.SS3.p2.3.m3.2.2.1.1.1.1.3.cmml" xref="S3.SS3.p2.3.m3.2.2.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"><in id="S3.SS3.p2.3.m3.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.2"></in><ci id="S3.SS3.p2.3.m3.1.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.3">ğ‘–</ci><apply id="S3.SS3.p2.3.m3.1.1.1.4.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.4.2"><csymbol cd="latexml" id="S3.SS3.p2.3.m3.1.1.1.4.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S3.SS3.p2.3.m3.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1">ğ‘€</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.2c">a=[a_{i}]_{i\in[M]}</annotation></semantics></math>, under policy <math id="S3.SS3.p2.4.m4.2" class="ltx_Math" alttext="\pi=[\pi_{i}]_{i\in[M]}" display="inline"><semantics id="S3.SS3.p2.4.m4.2a"><mrow id="S3.SS3.p2.4.m4.2.2" xref="S3.SS3.p2.4.m4.2.2.cmml"><mi id="S3.SS3.p2.4.m4.2.2.3" xref="S3.SS3.p2.4.m4.2.2.3.cmml">Ï€</mi><mo id="S3.SS3.p2.4.m4.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.cmml">=</mo><msub id="S3.SS3.p2.4.m4.2.2.1" xref="S3.SS3.p2.4.m4.2.2.1.cmml"><mrow id="S3.SS3.p2.4.m4.2.2.1.1.1" xref="S3.SS3.p2.4.m4.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p2.4.m4.2.2.1.1.1.2" xref="S3.SS3.p2.4.m4.2.2.1.1.2.1.cmml">[</mo><msub id="S3.SS3.p2.4.m4.2.2.1.1.1.1" xref="S3.SS3.p2.4.m4.2.2.1.1.1.1.cmml"><mi id="S3.SS3.p2.4.m4.2.2.1.1.1.1.2" xref="S3.SS3.p2.4.m4.2.2.1.1.1.1.2.cmml">Ï€</mi><mi id="S3.SS3.p2.4.m4.2.2.1.1.1.1.3" xref="S3.SS3.p2.4.m4.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.4.m4.2.2.1.1.1.3" xref="S3.SS3.p2.4.m4.2.2.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.3.cmml">i</mi><mo id="S3.SS3.p2.4.m4.1.1.1.2" xref="S3.SS3.p2.4.m4.1.1.1.2.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.4.m4.1.1.1.4.2" xref="S3.SS3.p2.4.m4.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.SS3.p2.4.m4.1.1.1.4.2.1" xref="S3.SS3.p2.4.m4.1.1.1.4.1.1.cmml">[</mo><mi id="S3.SS3.p2.4.m4.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.cmml">M</mi><mo stretchy="false" id="S3.SS3.p2.4.m4.1.1.1.4.2.2" xref="S3.SS3.p2.4.m4.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.2b"><apply id="S3.SS3.p2.4.m4.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2"><eq id="S3.SS3.p2.4.m4.2.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2.2"></eq><ci id="S3.SS3.p2.4.m4.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.3">ğœ‹</ci><apply id="S3.SS3.p2.4.m4.2.2.1.cmml" xref="S3.SS3.p2.4.m4.2.2.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.2.2.1.2.cmml" xref="S3.SS3.p2.4.m4.2.2.1">subscript</csymbol><apply id="S3.SS3.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.4.m4.2.2.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS3.p2.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.2.2.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1.1.1.2">ğœ‹</ci><ci id="S3.SS3.p2.4.m4.2.2.1.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1"><in id="S3.SS3.p2.4.m4.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.2"></in><ci id="S3.SS3.p2.4.m4.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.3">ğ‘–</ci><apply id="S3.SS3.p2.4.m4.1.1.1.4.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.4.2"><csymbol cd="latexml" id="S3.SS3.p2.4.m4.1.1.1.4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S3.SS3.p2.4.m4.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1">ğ‘€</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.2c">\pi=[\pi_{i}]_{i\in[M]}</annotation></semantics></math>. Remember that the output <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">Ï€</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">ğœ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\pi</annotation></semantics></math> is a function of the parameters <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mi id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><ci id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\theta</annotation></semantics></math> and the task representation <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="\boldsymbol{x}" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mi id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">ğ’™</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><ci id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">ğ’™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">\boldsymbol{x}</annotation></semantics></math>. To train the model, we maximize the expected reward under its policy, defined as</p>
<table id="A8.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.5" class="ltx_Math" alttext="\displaystyle R=\operatorname*{\mathbb{E}}_{a\in\Omega}[R(a)]=\sum_{a\in\Omega}P(a)R(a)" display="inline"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mi id="S3.E1.m1.5.5.4" xref="S3.E1.m1.5.5.4.cmml">R</mi><mo rspace="0.1389em" id="S3.E1.m1.5.5.5" xref="S3.E1.m1.5.5.5.cmml">=</mo><mrow id="S3.E1.m1.5.5.2.2" xref="S3.E1.m1.5.5.2.3.cmml"><munder id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><mo lspace="0.1389em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">ğ”¼</mo><mrow id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.3.cmml"><mi id="S3.E1.m1.4.4.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.3.2.cmml">a</mi><mo id="S3.E1.m1.4.4.1.1.1.3.1" xref="S3.E1.m1.4.4.1.1.1.3.1.cmml">âˆˆ</mo><mi mathvariant="normal" id="S3.E1.m1.4.4.1.1.1.3.3" xref="S3.E1.m1.4.4.1.1.1.3.3.cmml">Î©</mi></mrow></munder><mrow id="S3.E1.m1.5.5.2.2.2" xref="S3.E1.m1.5.5.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.2" xref="S3.E1.m1.5.5.2.3.cmml">[</mo><mrow id="S3.E1.m1.5.5.2.2.2.1" xref="S3.E1.m1.5.5.2.2.2.1.cmml"><mi id="S3.E1.m1.5.5.2.2.2.1.2" xref="S3.E1.m1.5.5.2.2.2.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.1.1" xref="S3.E1.m1.5.5.2.2.2.1.1.cmml">â€‹</mo><mrow id="S3.E1.m1.5.5.2.2.2.1.3.2" xref="S3.E1.m1.5.5.2.2.2.1.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.1.3.2.1" xref="S3.E1.m1.5.5.2.2.2.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">a</mi><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.1.3.2.2" xref="S3.E1.m1.5.5.2.2.2.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.3" xref="S3.E1.m1.5.5.2.3.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.5.5.6" xref="S3.E1.m1.5.5.6.cmml">=</mo><mrow id="S3.E1.m1.5.5.7" xref="S3.E1.m1.5.5.7.cmml"><mstyle displaystyle="true" id="S3.E1.m1.5.5.7.1" xref="S3.E1.m1.5.5.7.1.cmml"><munder id="S3.E1.m1.5.5.7.1a" xref="S3.E1.m1.5.5.7.1.cmml"><mo movablelimits="false" id="S3.E1.m1.5.5.7.1.2" xref="S3.E1.m1.5.5.7.1.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.5.5.7.1.3" xref="S3.E1.m1.5.5.7.1.3.cmml"><mi id="S3.E1.m1.5.5.7.1.3.2" xref="S3.E1.m1.5.5.7.1.3.2.cmml">a</mi><mo id="S3.E1.m1.5.5.7.1.3.1" xref="S3.E1.m1.5.5.7.1.3.1.cmml">âˆˆ</mo><mi mathvariant="normal" id="S3.E1.m1.5.5.7.1.3.3" xref="S3.E1.m1.5.5.7.1.3.3.cmml">Î©</mi></mrow></munder></mstyle><mrow id="S3.E1.m1.5.5.7.2" xref="S3.E1.m1.5.5.7.2.cmml"><mi id="S3.E1.m1.5.5.7.2.2" xref="S3.E1.m1.5.5.7.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.7.2.1" xref="S3.E1.m1.5.5.7.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.5.5.7.2.3.2" xref="S3.E1.m1.5.5.7.2.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.7.2.3.2.1" xref="S3.E1.m1.5.5.7.2.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S3.E1.m1.5.5.7.2.3.2.2" xref="S3.E1.m1.5.5.7.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.7.2.1a" xref="S3.E1.m1.5.5.7.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.5.5.7.2.4" xref="S3.E1.m1.5.5.7.2.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.7.2.1b" xref="S3.E1.m1.5.5.7.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.5.5.7.2.5.2" xref="S3.E1.m1.5.5.7.2.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.7.2.5.2.1" xref="S3.E1.m1.5.5.7.2.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">a</mi><mo stretchy="false" id="S3.E1.m1.5.5.7.2.5.2.2" xref="S3.E1.m1.5.5.7.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><and id="S3.E1.m1.5.5a.cmml" xref="S3.E1.m1.5.5"></and><apply id="S3.E1.m1.5.5b.cmml" xref="S3.E1.m1.5.5"><eq id="S3.E1.m1.5.5.5.cmml" xref="S3.E1.m1.5.5.5"></eq><ci id="S3.E1.m1.5.5.4.cmml" xref="S3.E1.m1.5.5.4">ğ‘…</ci><apply id="S3.E1.m1.5.5.2.3.cmml" xref="S3.E1.m1.5.5.2.2"><apply id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.2">ğ”¼</ci><apply id="S3.E1.m1.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3"><in id="S3.E1.m1.4.4.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1"></in><ci id="S3.E1.m1.4.4.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2">ğ‘</ci><ci id="S3.E1.m1.4.4.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3">Î©</ci></apply></apply><apply id="S3.E1.m1.5.5.2.2.2.1.cmml" xref="S3.E1.m1.5.5.2.2.2.1"><times id="S3.E1.m1.5.5.2.2.2.1.1.cmml" xref="S3.E1.m1.5.5.2.2.2.1.1"></times><ci id="S3.E1.m1.5.5.2.2.2.1.2.cmml" xref="S3.E1.m1.5.5.2.2.2.1.2">ğ‘…</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ‘</ci></apply></apply></apply><apply id="S3.E1.m1.5.5c.cmml" xref="S3.E1.m1.5.5"><eq id="S3.E1.m1.5.5.6.cmml" xref="S3.E1.m1.5.5.6"></eq><share href="#S3.E1.m1.5.5.2.cmml" id="S3.E1.m1.5.5d.cmml" xref="S3.E1.m1.5.5"></share><apply id="S3.E1.m1.5.5.7.cmml" xref="S3.E1.m1.5.5.7"><apply id="S3.E1.m1.5.5.7.1.cmml" xref="S3.E1.m1.5.5.7.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.7.1.1.cmml" xref="S3.E1.m1.5.5.7.1">subscript</csymbol><sum id="S3.E1.m1.5.5.7.1.2.cmml" xref="S3.E1.m1.5.5.7.1.2"></sum><apply id="S3.E1.m1.5.5.7.1.3.cmml" xref="S3.E1.m1.5.5.7.1.3"><in id="S3.E1.m1.5.5.7.1.3.1.cmml" xref="S3.E1.m1.5.5.7.1.3.1"></in><ci id="S3.E1.m1.5.5.7.1.3.2.cmml" xref="S3.E1.m1.5.5.7.1.3.2">ğ‘</ci><ci id="S3.E1.m1.5.5.7.1.3.3.cmml" xref="S3.E1.m1.5.5.7.1.3.3">Î©</ci></apply></apply><apply id="S3.E1.m1.5.5.7.2.cmml" xref="S3.E1.m1.5.5.7.2"><times id="S3.E1.m1.5.5.7.2.1.cmml" xref="S3.E1.m1.5.5.7.2.1"></times><ci id="S3.E1.m1.5.5.7.2.2.cmml" xref="S3.E1.m1.5.5.7.2.2">ğ‘ƒ</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğ‘</ci><ci id="S3.E1.m1.5.5.7.2.4.cmml" xref="S3.E1.m1.5.5.7.2.4">ğ‘…</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\displaystyle R=\operatorname*{\mathbb{E}}_{a\in\Omega}[R(a)]=\sum_{a\in\Omega}P(a)R(a)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.12" class="ltx_p">where <math id="S3.SS3.p2.8.m1.1" class="ltx_Math" alttext="\Omega" display="inline"><semantics id="S3.SS3.p2.8.m1.1a"><mi mathvariant="normal" id="S3.SS3.p2.8.m1.1.1" xref="S3.SS3.p2.8.m1.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m1.1b"><ci id="S3.SS3.p2.8.m1.1.1.cmml" xref="S3.SS3.p2.8.m1.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m1.1c">\Omega</annotation></semantics></math> is the space of all outputs <math id="S3.SS3.p2.9.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS3.p2.9.m2.1a"><mi id="S3.SS3.p2.9.m2.1.1" xref="S3.SS3.p2.9.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m2.1b"><ci id="S3.SS3.p2.9.m2.1.1.cmml" xref="S3.SS3.p2.9.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m2.1c">a</annotation></semantics></math> and <math id="S3.SS3.p2.10.m3.1" class="ltx_Math" alttext="R(a)" display="inline"><semantics id="S3.SS3.p2.10.m3.1a"><mrow id="S3.SS3.p2.10.m3.1.2" xref="S3.SS3.p2.10.m3.1.2.cmml"><mi id="S3.SS3.p2.10.m3.1.2.2" xref="S3.SS3.p2.10.m3.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.10.m3.1.2.1" xref="S3.SS3.p2.10.m3.1.2.1.cmml">â€‹</mo><mrow id="S3.SS3.p2.10.m3.1.2.3.2" xref="S3.SS3.p2.10.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.p2.10.m3.1.2.3.2.1" xref="S3.SS3.p2.10.m3.1.2.cmml">(</mo><mi id="S3.SS3.p2.10.m3.1.1" xref="S3.SS3.p2.10.m3.1.1.cmml">a</mi><mo stretchy="false" id="S3.SS3.p2.10.m3.1.2.3.2.2" xref="S3.SS3.p2.10.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m3.1b"><apply id="S3.SS3.p2.10.m3.1.2.cmml" xref="S3.SS3.p2.10.m3.1.2"><times id="S3.SS3.p2.10.m3.1.2.1.cmml" xref="S3.SS3.p2.10.m3.1.2.1"></times><ci id="S3.SS3.p2.10.m3.1.2.2.cmml" xref="S3.SS3.p2.10.m3.1.2.2">ğ‘…</ci><ci id="S3.SS3.p2.10.m3.1.1.cmml" xref="S3.SS3.p2.10.m3.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m3.1c">R(a)</annotation></semantics></math> is the reward when parameter values corresponding to action <math id="S3.SS3.p2.11.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS3.p2.11.m4.1a"><mi id="S3.SS3.p2.11.m4.1.1" xref="S3.SS3.p2.11.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m4.1b"><ci id="S3.SS3.p2.11.m4.1.1.cmml" xref="S3.SS3.p2.11.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m4.1c">a</annotation></semantics></math> are chosen. Since reward is the downstream accuracy, <math id="S3.SS3.p2.12.m5.3" class="ltx_Math" alttext="R(a)\in[0,100]" display="inline"><semantics id="S3.SS3.p2.12.m5.3a"><mrow id="S3.SS3.p2.12.m5.3.4" xref="S3.SS3.p2.12.m5.3.4.cmml"><mrow id="S3.SS3.p2.12.m5.3.4.2" xref="S3.SS3.p2.12.m5.3.4.2.cmml"><mi id="S3.SS3.p2.12.m5.3.4.2.2" xref="S3.SS3.p2.12.m5.3.4.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.12.m5.3.4.2.1" xref="S3.SS3.p2.12.m5.3.4.2.1.cmml">â€‹</mo><mrow id="S3.SS3.p2.12.m5.3.4.2.3.2" xref="S3.SS3.p2.12.m5.3.4.2.cmml"><mo stretchy="false" id="S3.SS3.p2.12.m5.3.4.2.3.2.1" xref="S3.SS3.p2.12.m5.3.4.2.cmml">(</mo><mi id="S3.SS3.p2.12.m5.1.1" xref="S3.SS3.p2.12.m5.1.1.cmml">a</mi><mo stretchy="false" id="S3.SS3.p2.12.m5.3.4.2.3.2.2" xref="S3.SS3.p2.12.m5.3.4.2.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p2.12.m5.3.4.1" xref="S3.SS3.p2.12.m5.3.4.1.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.12.m5.3.4.3.2" xref="S3.SS3.p2.12.m5.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.12.m5.3.4.3.2.1" xref="S3.SS3.p2.12.m5.3.4.3.1.cmml">[</mo><mn id="S3.SS3.p2.12.m5.2.2" xref="S3.SS3.p2.12.m5.2.2.cmml">0</mn><mo id="S3.SS3.p2.12.m5.3.4.3.2.2" xref="S3.SS3.p2.12.m5.3.4.3.1.cmml">,</mo><mn id="S3.SS3.p2.12.m5.3.3" xref="S3.SS3.p2.12.m5.3.3.cmml">100</mn><mo stretchy="false" id="S3.SS3.p2.12.m5.3.4.3.2.3" xref="S3.SS3.p2.12.m5.3.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.12.m5.3b"><apply id="S3.SS3.p2.12.m5.3.4.cmml" xref="S3.SS3.p2.12.m5.3.4"><in id="S3.SS3.p2.12.m5.3.4.1.cmml" xref="S3.SS3.p2.12.m5.3.4.1"></in><apply id="S3.SS3.p2.12.m5.3.4.2.cmml" xref="S3.SS3.p2.12.m5.3.4.2"><times id="S3.SS3.p2.12.m5.3.4.2.1.cmml" xref="S3.SS3.p2.12.m5.3.4.2.1"></times><ci id="S3.SS3.p2.12.m5.3.4.2.2.cmml" xref="S3.SS3.p2.12.m5.3.4.2.2">ğ‘…</ci><ci id="S3.SS3.p2.12.m5.1.1.cmml" xref="S3.SS3.p2.12.m5.1.1">ğ‘</ci></apply><interval closure="closed" id="S3.SS3.p2.12.m5.3.4.3.1.cmml" xref="S3.SS3.p2.12.m5.3.4.3.2"><cn type="integer" id="S3.SS3.p2.12.m5.2.2.cmml" xref="S3.SS3.p2.12.m5.2.2">0</cn><cn type="integer" id="S3.SS3.p2.12.m5.3.3.cmml" xref="S3.SS3.p2.12.m5.3.3">100</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.12.m5.3c">R(a)\in[0,100]</annotation></semantics></math>.
Using the REINFORCE rule, we have</p>
<table id="A8.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle\nabla_{\theta}R" display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">âˆ‡</mo><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">Î¸</mi></msub><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2">âˆ‡</ci><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">ğœƒ</ci></apply><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\nabla_{\theta}R</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.4" class="ltx_Math" alttext="\displaystyle=\operatorname*{\mathbb{E}}_{a\in\Omega}\left[(\nabla_{\theta}\log P(a))R(a)\right]" display="inline"><semantics id="S3.E2.m2.4a"><mrow id="S3.E2.m2.4.4" xref="S3.E2.m2.4.4.cmml"><mi id="S3.E2.m2.4.4.4" xref="S3.E2.m2.4.4.4.cmml"></mi><mo rspace="0.1389em" id="S3.E2.m2.4.4.3" xref="S3.E2.m2.4.4.3.cmml">=</mo><mrow id="S3.E2.m2.4.4.2.2" xref="S3.E2.m2.4.4.2.3.cmml"><munder id="S3.E2.m2.3.3.1.1.1" xref="S3.E2.m2.3.3.1.1.1.cmml"><mo lspace="0.1389em" rspace="0em" id="S3.E2.m2.3.3.1.1.1.2" xref="S3.E2.m2.3.3.1.1.1.2.cmml">ğ”¼</mo><mrow id="S3.E2.m2.3.3.1.1.1.3" xref="S3.E2.m2.3.3.1.1.1.3.cmml"><mi id="S3.E2.m2.3.3.1.1.1.3.2" xref="S3.E2.m2.3.3.1.1.1.3.2.cmml">a</mi><mo id="S3.E2.m2.3.3.1.1.1.3.1" xref="S3.E2.m2.3.3.1.1.1.3.1.cmml">âˆˆ</mo><mi mathvariant="normal" id="S3.E2.m2.3.3.1.1.1.3.3" xref="S3.E2.m2.3.3.1.1.1.3.3.cmml">Î©</mi></mrow></munder><mrow id="S3.E2.m2.4.4.2.2.2" xref="S3.E2.m2.4.4.2.3.cmml"><mo id="S3.E2.m2.4.4.2.2.2.2" xref="S3.E2.m2.4.4.2.3.cmml">[</mo><mrow id="S3.E2.m2.4.4.2.2.2.1" xref="S3.E2.m2.4.4.2.2.2.1.cmml"><mrow id="S3.E2.m2.4.4.2.2.2.1.1.1" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m2.4.4.2.2.2.1.1.1.2" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m2.4.4.2.2.2.1.1.1.1" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml"><mrow id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.cmml"><mrow id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.cmml"><msub id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.cmml"><mo rspace="0.167em" id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.2" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.2.cmml">âˆ‡</mo><mi id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.3" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.3.cmml">Î¸</mi></msub><mi id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.2" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.2.cmml">log</mi></mrow><mo lspace="0.167em" id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2a" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.cmml">â¡</mo><mi id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.2" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.2.2.2.1.1.1.1.1" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E2.m2.4.4.2.2.2.1.1.1.1.3.2" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m2.4.4.2.2.2.1.1.1.1.3.2.1" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml">(</mo><mi id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">a</mi><mo stretchy="false" id="S3.E2.m2.4.4.2.2.2.1.1.1.1.3.2.2" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m2.4.4.2.2.2.1.1.1.3" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.2.2.2.1.2" xref="S3.E2.m2.4.4.2.2.2.1.2.cmml">â€‹</mo><mi id="S3.E2.m2.4.4.2.2.2.1.3" xref="S3.E2.m2.4.4.2.2.2.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.2.2.2.1.2a" xref="S3.E2.m2.4.4.2.2.2.1.2.cmml">â€‹</mo><mrow id="S3.E2.m2.4.4.2.2.2.1.4.2" xref="S3.E2.m2.4.4.2.2.2.1.cmml"><mo stretchy="false" id="S3.E2.m2.4.4.2.2.2.1.4.2.1" xref="S3.E2.m2.4.4.2.2.2.1.cmml">(</mo><mi id="S3.E2.m2.2.2" xref="S3.E2.m2.2.2.cmml">a</mi><mo stretchy="false" id="S3.E2.m2.4.4.2.2.2.1.4.2.2" xref="S3.E2.m2.4.4.2.2.2.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m2.4.4.2.2.2.3" xref="S3.E2.m2.4.4.2.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.4b"><apply id="S3.E2.m2.4.4.cmml" xref="S3.E2.m2.4.4"><eq id="S3.E2.m2.4.4.3.cmml" xref="S3.E2.m2.4.4.3"></eq><csymbol cd="latexml" id="S3.E2.m2.4.4.4.cmml" xref="S3.E2.m2.4.4.4">absent</csymbol><apply id="S3.E2.m2.4.4.2.3.cmml" xref="S3.E2.m2.4.4.2.2"><apply id="S3.E2.m2.3.3.1.1.1.cmml" xref="S3.E2.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.3.3.1.1.1.1.cmml" xref="S3.E2.m2.3.3.1.1.1">subscript</csymbol><ci id="S3.E2.m2.3.3.1.1.1.2.cmml" xref="S3.E2.m2.3.3.1.1.1.2">ğ”¼</ci><apply id="S3.E2.m2.3.3.1.1.1.3.cmml" xref="S3.E2.m2.3.3.1.1.1.3"><in id="S3.E2.m2.3.3.1.1.1.3.1.cmml" xref="S3.E2.m2.3.3.1.1.1.3.1"></in><ci id="S3.E2.m2.3.3.1.1.1.3.2.cmml" xref="S3.E2.m2.3.3.1.1.1.3.2">ğ‘</ci><ci id="S3.E2.m2.3.3.1.1.1.3.3.cmml" xref="S3.E2.m2.3.3.1.1.1.3.3">Î©</ci></apply></apply><apply id="S3.E2.m2.4.4.2.2.2.1.cmml" xref="S3.E2.m2.4.4.2.2.2.1"><times id="S3.E2.m2.4.4.2.2.2.1.2.cmml" xref="S3.E2.m2.4.4.2.2.2.1.2"></times><apply id="S3.E2.m2.4.4.2.2.2.1.1.1.1.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1"><times id="S3.E2.m2.4.4.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.1"></times><apply id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2"><apply id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1"><apply id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.1.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.2.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.2">âˆ‡</ci><ci id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.3.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.1.3">ğœƒ</ci></apply><log id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.2.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.1.2"></log></apply><ci id="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E2.m2.4.4.2.2.2.1.1.1.1.2.2">ğ‘ƒ</ci></apply><ci id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1">ğ‘</ci></apply><ci id="S3.E2.m2.4.4.2.2.2.1.3.cmml" xref="S3.E2.m2.4.4.2.2.2.1.3">ğ‘…</ci><ci id="S3.E2.m2.2.2.cmml" xref="S3.E2.m2.2.2">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.4c">\displaystyle=\operatorname*{\mathbb{E}}_{a\in\Omega}\left[(\nabla_{\theta}\log P(a))R(a)\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\displaystyle=\operatorname*{\mathbb{E}}_{a\in\Omega}\left[\left(\sum_{i\in[M]}\nabla_{\theta}\log\pi_{i}(a_{i})\right)R(a)\right]" display="inline"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mi id="S3.E3.m1.4.4.4" xref="S3.E3.m1.4.4.4.cmml"></mi><mo rspace="0.1389em" id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml">=</mo><mrow id="S3.E3.m1.4.4.2.2" xref="S3.E3.m1.4.4.2.3.cmml"><munder id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mo lspace="0.1389em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.cmml">ğ”¼</mo><mrow id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.3.2.cmml">a</mi><mo id="S3.E3.m1.3.3.1.1.1.3.1" xref="S3.E3.m1.3.3.1.1.1.3.1.cmml">âˆˆ</mo><mi mathvariant="normal" id="S3.E3.m1.3.3.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.3.3.cmml">Î©</mi></mrow></munder><mrow id="S3.E3.m1.4.4.2.2.2" xref="S3.E3.m1.4.4.2.3.cmml"><mo id="S3.E3.m1.4.4.2.2.2.2" xref="S3.E3.m1.4.4.2.3.cmml">[</mo><mrow id="S3.E3.m1.4.4.2.2.2.1" xref="S3.E3.m1.4.4.2.2.2.1.cmml"><mrow id="S3.E3.m1.4.4.2.2.2.1.1.1" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.cmml"><mo id="S3.E3.m1.4.4.2.2.2.1.1.1.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.4.4.2.2.2.1.1.1.1" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.cmml"><munder id="S3.E3.m1.4.4.2.2.2.1.1.1.1.2a" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">i</mi><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">âˆˆ</mo><mrow id="S3.E3.m1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.4.2.1" xref="S3.E3.m1.1.1.1.4.1.1.cmml">[</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">M</mi><mo stretchy="false" id="S3.E3.m1.1.1.1.4.2.2" xref="S3.E3.m1.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></munder></mstyle><mrow id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.cmml"><mrow id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.cmml"><msub id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.cmml"><mo lspace="0.167em" rspace="0.167em" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.2.cmml">âˆ‡</mo><mi id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.3" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.3.cmml">Î¸</mi></msub><mi id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.2.cmml">log</mi></mrow><mo lspace="0.167em" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3a" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.cmml">â¡</mo><msub id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.2.cmml">Ï€</mi><mi id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.3" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.4.4.2.2.2.1.1.1.3" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.2.1.2" xref="S3.E3.m1.4.4.2.2.2.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.4.4.2.2.2.1.3" xref="S3.E3.m1.4.4.2.2.2.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.2.1.2a" xref="S3.E3.m1.4.4.2.2.2.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.4.4.2.2.2.1.4.2" xref="S3.E3.m1.4.4.2.2.2.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.1.4.2.1" xref="S3.E3.m1.4.4.2.2.2.1.cmml">(</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">a</mi><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.1.4.2.2" xref="S3.E3.m1.4.4.2.2.2.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.2.2.2.3" xref="S3.E3.m1.4.4.2.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><eq id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"></eq><csymbol cd="latexml" id="S3.E3.m1.4.4.4.cmml" xref="S3.E3.m1.4.4.4">absent</csymbol><apply id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.2"><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2">ğ”¼</ci><apply id="S3.E3.m1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.3"><in id="S3.E3.m1.3.3.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.3.1"></in><ci id="S3.E3.m1.3.3.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.3.2">ğ‘</ci><ci id="S3.E3.m1.3.3.1.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.3.3">Î©</ci></apply></apply><apply id="S3.E3.m1.4.4.2.2.2.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1"><times id="S3.E3.m1.4.4.2.2.2.1.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.2"></times><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1"><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.2.2"></sum><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><in id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></in><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">ğ‘–</ci><apply id="S3.E3.m1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.4.2"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.4.1.1.cmml" xref="S3.E3.m1.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ğ‘€</ci></apply></apply></apply><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1"><times id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.2"></times><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3"><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1"><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.2">âˆ‡</ci><ci id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.3.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.1.3">ğœƒ</ci></apply><log id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.1.2"></log></apply><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.2">ğœ‹</ci><ci id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.3.2.3">ğ‘–</ci></apply></apply><apply id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.2.2.2.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply><ci id="S3.E3.m1.4.4.2.2.2.1.3.cmml" xref="S3.E3.m1.4.4.2.2.2.1.3">ğ‘…</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\displaystyle=\operatorname*{\mathbb{E}}_{a\in\Omega}\left[\left(\sum_{i\in[M]}\nabla_{\theta}\log\pi_{i}(a_{i})\right)R(a)\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.14" class="ltx_p">where the 2nd step comes from linearity of the derivative. In practice, we use a point estimate of the above expectation at a sample <math id="S3.SS3.p2.13.m1.1" class="ltx_Math" alttext="a\sim(\pi+\epsilon)" display="inline"><semantics id="S3.SS3.p2.13.m1.1a"><mrow id="S3.SS3.p2.13.m1.1.1" xref="S3.SS3.p2.13.m1.1.1.cmml"><mi id="S3.SS3.p2.13.m1.1.1.3" xref="S3.SS3.p2.13.m1.1.1.3.cmml">a</mi><mo id="S3.SS3.p2.13.m1.1.1.2" xref="S3.SS3.p2.13.m1.1.1.2.cmml">âˆ¼</mo><mrow id="S3.SS3.p2.13.m1.1.1.1.1" xref="S3.SS3.p2.13.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p2.13.m1.1.1.1.1.2" xref="S3.SS3.p2.13.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p2.13.m1.1.1.1.1.1" xref="S3.SS3.p2.13.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.13.m1.1.1.1.1.1.2" xref="S3.SS3.p2.13.m1.1.1.1.1.1.2.cmml">Ï€</mi><mo id="S3.SS3.p2.13.m1.1.1.1.1.1.1" xref="S3.SS3.p2.13.m1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.SS3.p2.13.m1.1.1.1.1.1.3" xref="S3.SS3.p2.13.m1.1.1.1.1.1.3.cmml">Ïµ</mi></mrow><mo stretchy="false" id="S3.SS3.p2.13.m1.1.1.1.1.3" xref="S3.SS3.p2.13.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.13.m1.1b"><apply id="S3.SS3.p2.13.m1.1.1.cmml" xref="S3.SS3.p2.13.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.13.m1.1.1.2.cmml" xref="S3.SS3.p2.13.m1.1.1.2">similar-to</csymbol><ci id="S3.SS3.p2.13.m1.1.1.3.cmml" xref="S3.SS3.p2.13.m1.1.1.3">ğ‘</ci><apply id="S3.SS3.p2.13.m1.1.1.1.1.1.cmml" xref="S3.SS3.p2.13.m1.1.1.1.1"><plus id="S3.SS3.p2.13.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.13.m1.1.1.1.1.1.1"></plus><ci id="S3.SS3.p2.13.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.13.m1.1.1.1.1.1.2">ğœ‹</ci><ci id="S3.SS3.p2.13.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.13.m1.1.1.1.1.1.3">italic-Ïµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.13.m1.1c">a\sim(\pi+\epsilon)</annotation></semantics></math> (<math id="S3.SS3.p2.14.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.p2.14.m2.1a"><mi id="S3.SS3.p2.14.m2.1.1" xref="S3.SS3.p2.14.m2.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.14.m2.1b"><ci id="S3.SS3.p2.14.m2.1.1.cmml" xref="S3.SS3.p2.14.m2.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.14.m2.1c">\epsilon</annotation></semantics></math> being some exploration noise added to the Task2Sim output distribution) with a self-critical baseline following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>:</p>
<table id="A8.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.5" class="ltx_Math" alttext="\displaystyle\nabla_{\theta}R\approx\left(\sum_{i\in[M]}\nabla_{\theta}\log\pi_{i}(a_{i})\right)\left(R(a)-R(\nu)\right)" display="inline"><semantics id="S3.E4.m1.5a"><mrow id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml"><mrow id="S3.E4.m1.5.5.4" xref="S3.E4.m1.5.5.4.cmml"><msub id="S3.E4.m1.5.5.4.1" xref="S3.E4.m1.5.5.4.1.cmml"><mo id="S3.E4.m1.5.5.4.1.2" xref="S3.E4.m1.5.5.4.1.2.cmml">âˆ‡</mo><mi id="S3.E4.m1.5.5.4.1.3" xref="S3.E4.m1.5.5.4.1.3.cmml">Î¸</mi></msub><mi id="S3.E4.m1.5.5.4.2" xref="S3.E4.m1.5.5.4.2.cmml">R</mi></mrow><mo id="S3.E4.m1.5.5.3" xref="S3.E4.m1.5.5.3.cmml">â‰ˆ</mo><mrow id="S3.E4.m1.5.5.2" xref="S3.E4.m1.5.5.2.cmml"><mrow id="S3.E4.m1.4.4.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.cmml"><mo id="S3.E4.m1.4.4.1.1.1.2" xref="S3.E4.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.4.4.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E4.m1.4.4.1.1.1.1.2" xref="S3.E4.m1.4.4.1.1.1.1.2.cmml"><munder id="S3.E4.m1.4.4.1.1.1.1.2a" xref="S3.E4.m1.4.4.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E4.m1.4.4.1.1.1.1.2.2" xref="S3.E4.m1.4.4.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">i</mi><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">âˆˆ</mo><mrow id="S3.E4.m1.1.1.1.4.2" xref="S3.E4.m1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.4.2.1" xref="S3.E4.m1.1.1.1.4.1.1.cmml">[</mo><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">M</mi><mo stretchy="false" id="S3.E4.m1.1.1.1.4.2.2" xref="S3.E4.m1.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></munder></mstyle><mrow id="S3.E4.m1.4.4.1.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.4.4.1.1.1.1.1.3" xref="S3.E4.m1.4.4.1.1.1.1.1.3.cmml"><mrow id="S3.E4.m1.4.4.1.1.1.1.1.3.1" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.cmml"><msub id="S3.E4.m1.4.4.1.1.1.1.1.3.1.1" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.cmml"><mo lspace="0.167em" rspace="0.167em" id="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.2" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.2.cmml">âˆ‡</mo><mi id="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.3" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.3.cmml">Î¸</mi></msub><mi id="S3.E4.m1.4.4.1.1.1.1.1.3.1.2" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.2.cmml">log</mi></mrow><mo lspace="0.167em" id="S3.E4.m1.4.4.1.1.1.1.1.3a" xref="S3.E4.m1.4.4.1.1.1.1.1.3.cmml">â¡</mo><msub id="S3.E4.m1.4.4.1.1.1.1.1.3.2" xref="S3.E4.m1.4.4.1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m1.4.4.1.1.1.1.1.3.2.2" xref="S3.E4.m1.4.4.1.1.1.1.1.3.2.2.cmml">Ï€</mi><mi id="S3.E4.m1.4.4.1.1.1.1.1.3.2.3" xref="S3.E4.m1.4.4.1.1.1.1.1.3.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.1.1.1.1.1.2" xref="S3.E4.m1.4.4.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.4.4.1.1.1.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E4.m1.4.4.1.1.1.1.1.1.1.3" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.4.4.1.1.1.3" xref="S3.E4.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.2.3" xref="S3.E4.m1.5.5.2.3.cmml">â€‹</mo><mrow id="S3.E4.m1.5.5.2.2.1" xref="S3.E4.m1.5.5.2.2.1.1.cmml"><mo id="S3.E4.m1.5.5.2.2.1.2" xref="S3.E4.m1.5.5.2.2.1.1.cmml">(</mo><mrow id="S3.E4.m1.5.5.2.2.1.1" xref="S3.E4.m1.5.5.2.2.1.1.cmml"><mrow id="S3.E4.m1.5.5.2.2.1.1.2" xref="S3.E4.m1.5.5.2.2.1.1.2.cmml"><mi id="S3.E4.m1.5.5.2.2.1.1.2.2" xref="S3.E4.m1.5.5.2.2.1.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.2.2.1.1.2.1" xref="S3.E4.m1.5.5.2.2.1.1.2.1.cmml">â€‹</mo><mrow id="S3.E4.m1.5.5.2.2.1.1.2.3.2" xref="S3.E4.m1.5.5.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.2.2.1.1.2.3.2.1" xref="S3.E4.m1.5.5.2.2.1.1.2.cmml">(</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">a</mi><mo stretchy="false" id="S3.E4.m1.5.5.2.2.1.1.2.3.2.2" xref="S3.E4.m1.5.5.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.5.5.2.2.1.1.1" xref="S3.E4.m1.5.5.2.2.1.1.1.cmml">âˆ’</mo><mrow id="S3.E4.m1.5.5.2.2.1.1.3" xref="S3.E4.m1.5.5.2.2.1.1.3.cmml"><mi id="S3.E4.m1.5.5.2.2.1.1.3.2" xref="S3.E4.m1.5.5.2.2.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.2.2.1.1.3.1" xref="S3.E4.m1.5.5.2.2.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E4.m1.5.5.2.2.1.1.3.3.2" xref="S3.E4.m1.5.5.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.2.2.1.1.3.3.2.1" xref="S3.E4.m1.5.5.2.2.1.1.3.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">Î½</mi><mo stretchy="false" id="S3.E4.m1.5.5.2.2.1.1.3.3.2.2" xref="S3.E4.m1.5.5.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.5.5.2.2.1.3" xref="S3.E4.m1.5.5.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.5b"><apply id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5"><approx id="S3.E4.m1.5.5.3.cmml" xref="S3.E4.m1.5.5.3"></approx><apply id="S3.E4.m1.5.5.4.cmml" xref="S3.E4.m1.5.5.4"><apply id="S3.E4.m1.5.5.4.1.cmml" xref="S3.E4.m1.5.5.4.1"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.4.1.1.cmml" xref="S3.E4.m1.5.5.4.1">subscript</csymbol><ci id="S3.E4.m1.5.5.4.1.2.cmml" xref="S3.E4.m1.5.5.4.1.2">âˆ‡</ci><ci id="S3.E4.m1.5.5.4.1.3.cmml" xref="S3.E4.m1.5.5.4.1.3">ğœƒ</ci></apply><ci id="S3.E4.m1.5.5.4.2.cmml" xref="S3.E4.m1.5.5.4.2">ğ‘…</ci></apply><apply id="S3.E4.m1.5.5.2.cmml" xref="S3.E4.m1.5.5.2"><times id="S3.E4.m1.5.5.2.3.cmml" xref="S3.E4.m1.5.5.2.3"></times><apply id="S3.E4.m1.4.4.1.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1"><apply id="S3.E4.m1.4.4.1.1.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.2">subscript</csymbol><sum id="S3.E4.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.2.2"></sum><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><in id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></in><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">ğ‘–</ci><apply id="S3.E4.m1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.4.2"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.4.1.1.cmml" xref="S3.E4.m1.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">ğ‘€</ci></apply></apply></apply><apply id="S3.E4.m1.4.4.1.1.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1"><times id="S3.E4.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.2"></times><apply id="S3.E4.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3"><apply id="S3.E4.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1"><apply id="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.1">subscript</csymbol><ci id="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.2">âˆ‡</ci><ci id="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.3.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.1.3">ğœƒ</ci></apply><log id="S3.E4.m1.4.4.1.1.1.1.1.3.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.1.2"></log></apply><apply id="S3.E4.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.4.4.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.2.2">ğœ‹</ci><ci id="S3.E4.m1.4.4.1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.3.2.3">ğ‘–</ci></apply></apply><apply id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.4.4.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply><apply id="S3.E4.m1.5.5.2.2.1.1.cmml" xref="S3.E4.m1.5.5.2.2.1"><minus id="S3.E4.m1.5.5.2.2.1.1.1.cmml" xref="S3.E4.m1.5.5.2.2.1.1.1"></minus><apply id="S3.E4.m1.5.5.2.2.1.1.2.cmml" xref="S3.E4.m1.5.5.2.2.1.1.2"><times id="S3.E4.m1.5.5.2.2.1.1.2.1.cmml" xref="S3.E4.m1.5.5.2.2.1.1.2.1"></times><ci id="S3.E4.m1.5.5.2.2.1.1.2.2.cmml" xref="S3.E4.m1.5.5.2.2.1.1.2.2">ğ‘…</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">ğ‘</ci></apply><apply id="S3.E4.m1.5.5.2.2.1.1.3.cmml" xref="S3.E4.m1.5.5.2.2.1.1.3"><times id="S3.E4.m1.5.5.2.2.1.1.3.1.cmml" xref="S3.E4.m1.5.5.2.2.1.1.3.1"></times><ci id="S3.E4.m1.5.5.2.2.1.1.3.2.cmml" xref="S3.E4.m1.5.5.2.2.1.1.3.2">ğ‘…</ci><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">ğœˆ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.5c">\displaystyle\nabla_{\theta}R\approx\left(\sum_{i\in[M]}\nabla_{\theta}\log\pi_{i}(a_{i})\right)\left(R(a)-R(\nu)\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.15" class="ltx_p">where, as a reminder <math id="S3.SS3.p2.15.m1.1" class="ltx_Math" alttext="\nu" display="inline"><semantics id="S3.SS3.p2.15.m1.1a"><mi id="S3.SS3.p2.15.m1.1.1" xref="S3.SS3.p2.15.m1.1.1.cmml">Î½</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.15.m1.1b"><ci id="S3.SS3.p2.15.m1.1.1.cmml" xref="S3.SS3.p2.15.m1.1.1">ğœˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.15.m1.1c">\nu</annotation></semantics></math> is the set of the distribution argmax parameter values from the Task2Sim model heads.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">A pseudo-code of our approach is shown in <a href="#algorithm1" title="In 3.3 Task2Sim Training â€£ 3 Proposed Approach â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Algorithm</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>. Specifically, we update the model parameters <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\theta</annotation></semantics></math> using minibatches of tasks sampled from a set of â€œseenâ€ tasks. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, we also employ self-imitation learning biased towards actions found to have better rewards. This is done by keeping track of the best action encountered in the learning process and using it for additional updates to the model, besides the ones in <a href="#algorithm1" title="In 3.3 Task2Sim Training â€£ 3 Proposed Approach â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Algorithm</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> of <a href="#algorithm1" title="In 3.3 Task2Sim Training â€£ 3 Proposed Approach â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Algorithm</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.
Furthermore, we use the test accuracy of a 5-nearest neighbors classifier operating on features generated by the pretrained backbone as a proxy for downstream task performance since it is computationally much faster than other common evaluation criteria used in transfer learning, e.g., linear probing or full-network finetuning. Our experiments demonstrate that this proxy evaluation measure indeed correlates with, and thus, helps in final downstream performance with linear probing or full-network finetuning.</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.24" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.2.2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span><span id="algorithm1.2.2.1" class="ltx_text ltx_font_bold">Input:</span> Set of <math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="algorithm1.1.1.m1.1a"><mi id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><ci id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">N</annotation></semantics></math> â€œseenâ€ downstream tasks represented by task2vecs <math id="algorithm1.2.2.m2.3" class="ltx_Math" alttext="\mathcal{T}=\{\boldsymbol{x}_{i}|i\in[N]\}" display="inline"><semantics id="algorithm1.2.2.m2.3a"><mrow id="algorithm1.2.2.m2.3.3" xref="algorithm1.2.2.m2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.2.2.m2.3.3.4" xref="algorithm1.2.2.m2.3.3.4.cmml">ğ’¯</mi><mo id="algorithm1.2.2.m2.3.3.3" xref="algorithm1.2.2.m2.3.3.3.cmml">=</mo><mrow id="algorithm1.2.2.m2.3.3.2.2" xref="algorithm1.2.2.m2.3.3.2.3.cmml"><mo stretchy="false" id="algorithm1.2.2.m2.3.3.2.2.3" xref="algorithm1.2.2.m2.3.3.2.3.1.cmml">{</mo><msub id="algorithm1.2.2.m2.2.2.1.1.1" xref="algorithm1.2.2.m2.2.2.1.1.1.cmml"><mi id="algorithm1.2.2.m2.2.2.1.1.1.2" xref="algorithm1.2.2.m2.2.2.1.1.1.2.cmml">ğ’™</mi><mi id="algorithm1.2.2.m2.2.2.1.1.1.3" xref="algorithm1.2.2.m2.2.2.1.1.1.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m2.3.3.2.2.4" xref="algorithm1.2.2.m2.3.3.2.3.1.cmml">|</mo><mrow id="algorithm1.2.2.m2.3.3.2.2.2" xref="algorithm1.2.2.m2.3.3.2.2.2.cmml"><mi id="algorithm1.2.2.m2.3.3.2.2.2.2" xref="algorithm1.2.2.m2.3.3.2.2.2.2.cmml">i</mi><mo id="algorithm1.2.2.m2.3.3.2.2.2.1" xref="algorithm1.2.2.m2.3.3.2.2.2.1.cmml">âˆˆ</mo><mrow id="algorithm1.2.2.m2.3.3.2.2.2.3.2" xref="algorithm1.2.2.m2.3.3.2.2.2.3.1.cmml"><mo stretchy="false" id="algorithm1.2.2.m2.3.3.2.2.2.3.2.1" xref="algorithm1.2.2.m2.3.3.2.2.2.3.1.1.cmml">[</mo><mi id="algorithm1.2.2.m2.1.1" xref="algorithm1.2.2.m2.1.1.cmml">N</mi><mo stretchy="false" id="algorithm1.2.2.m2.3.3.2.2.2.3.2.2" xref="algorithm1.2.2.m2.3.3.2.2.2.3.1.1.cmml">]</mo></mrow></mrow><mo stretchy="false" id="algorithm1.2.2.m2.3.3.2.2.5" xref="algorithm1.2.2.m2.3.3.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m2.3b"><apply id="algorithm1.2.2.m2.3.3.cmml" xref="algorithm1.2.2.m2.3.3"><eq id="algorithm1.2.2.m2.3.3.3.cmml" xref="algorithm1.2.2.m2.3.3.3"></eq><ci id="algorithm1.2.2.m2.3.3.4.cmml" xref="algorithm1.2.2.m2.3.3.4">ğ’¯</ci><apply id="algorithm1.2.2.m2.3.3.2.3.cmml" xref="algorithm1.2.2.m2.3.3.2.2"><csymbol cd="latexml" id="algorithm1.2.2.m2.3.3.2.3.1.cmml" xref="algorithm1.2.2.m2.3.3.2.2.3">conditional-set</csymbol><apply id="algorithm1.2.2.m2.2.2.1.1.1.cmml" xref="algorithm1.2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="algorithm1.2.2.m2.2.2.1.1.1.1.cmml" xref="algorithm1.2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="algorithm1.2.2.m2.2.2.1.1.1.2.cmml" xref="algorithm1.2.2.m2.2.2.1.1.1.2">ğ’™</ci><ci id="algorithm1.2.2.m2.2.2.1.1.1.3.cmml" xref="algorithm1.2.2.m2.2.2.1.1.1.3">ğ‘–</ci></apply><apply id="algorithm1.2.2.m2.3.3.2.2.2.cmml" xref="algorithm1.2.2.m2.3.3.2.2.2"><in id="algorithm1.2.2.m2.3.3.2.2.2.1.cmml" xref="algorithm1.2.2.m2.3.3.2.2.2.1"></in><ci id="algorithm1.2.2.m2.3.3.2.2.2.2.cmml" xref="algorithm1.2.2.m2.3.3.2.2.2.2">ğ‘–</ci><apply id="algorithm1.2.2.m2.3.3.2.2.2.3.1.cmml" xref="algorithm1.2.2.m2.3.3.2.2.2.3.2"><csymbol cd="latexml" id="algorithm1.2.2.m2.3.3.2.2.2.3.1.1.cmml" xref="algorithm1.2.2.m2.3.3.2.2.2.3.2.1">delimited-[]</csymbol><ci id="algorithm1.2.2.m2.1.1.cmml" xref="algorithm1.2.2.m2.1.1">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m2.3c">\mathcal{T}=\{\boldsymbol{x}_{i}|i\in[N]\}</annotation></semantics></math>. 
</div>
<div id="algorithm1.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>
Given initial Task2Sim parameters <math id="algorithm1.3.3.m1.1" class="ltx_Math" alttext="\theta_{0}" display="inline"><semantics id="algorithm1.3.3.m1.1a"><msub id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml"><mi id="algorithm1.3.3.m1.1.1.2" xref="algorithm1.3.3.m1.1.1.2.cmml">Î¸</mi><mn id="algorithm1.3.3.m1.1.1.3" xref="algorithm1.3.3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><apply id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.3.3.m1.1.1.1.cmml" xref="algorithm1.3.3.m1.1.1">subscript</csymbol><ci id="algorithm1.3.3.m1.1.1.2.cmml" xref="algorithm1.3.3.m1.1.1.2">ğœƒ</ci><cn type="integer" id="algorithm1.3.3.m1.1.1.3.cmml" xref="algorithm1.3.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">\theta_{0}</annotation></semantics></math> and initial noise level <math id="algorithm1.4.4.m2.1" class="ltx_Math" alttext="\epsilon_{0}" display="inline"><semantics id="algorithm1.4.4.m2.1a"><msub id="algorithm1.4.4.m2.1.1" xref="algorithm1.4.4.m2.1.1.cmml"><mi id="algorithm1.4.4.m2.1.1.2" xref="algorithm1.4.4.m2.1.1.2.cmml">Ïµ</mi><mn id="algorithm1.4.4.m2.1.1.3" xref="algorithm1.4.4.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m2.1b"><apply id="algorithm1.4.4.m2.1.1.cmml" xref="algorithm1.4.4.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.4.4.m2.1.1.1.cmml" xref="algorithm1.4.4.m2.1.1">subscript</csymbol><ci id="algorithm1.4.4.m2.1.1.2.cmml" xref="algorithm1.4.4.m2.1.1.2">italic-Ïµ</ci><cn type="integer" id="algorithm1.4.4.m2.1.1.3.cmml" xref="algorithm1.4.4.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m2.1c">\epsilon_{0}</annotation></semantics></math>
</div>
<div id="algorithm1.5.5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>
Initialize <math id="algorithm1.5.5.m1.2" class="ltx_Math" alttext="a_{max}^{(i)}|i\in[N]" display="inline"><semantics id="algorithm1.5.5.m1.2a"><mrow id="algorithm1.5.5.m1.2.3" xref="algorithm1.5.5.m1.2.3.cmml"><mrow id="algorithm1.5.5.m1.2.3.2" xref="algorithm1.5.5.m1.2.3.2.cmml"><msubsup id="algorithm1.5.5.m1.2.3.2.2" xref="algorithm1.5.5.m1.2.3.2.2.cmml"><mi id="algorithm1.5.5.m1.2.3.2.2.2.2" xref="algorithm1.5.5.m1.2.3.2.2.2.2.cmml">a</mi><mrow id="algorithm1.5.5.m1.2.3.2.2.2.3" xref="algorithm1.5.5.m1.2.3.2.2.2.3.cmml"><mi id="algorithm1.5.5.m1.2.3.2.2.2.3.2" xref="algorithm1.5.5.m1.2.3.2.2.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.2.3.2.2.2.3.1" xref="algorithm1.5.5.m1.2.3.2.2.2.3.1.cmml">â€‹</mo><mi id="algorithm1.5.5.m1.2.3.2.2.2.3.3" xref="algorithm1.5.5.m1.2.3.2.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.2.3.2.2.2.3.1a" xref="algorithm1.5.5.m1.2.3.2.2.2.3.1.cmml">â€‹</mo><mi id="algorithm1.5.5.m1.2.3.2.2.2.3.4" xref="algorithm1.5.5.m1.2.3.2.2.2.3.4.cmml">x</mi></mrow><mrow id="algorithm1.5.5.m1.1.1.1.3" xref="algorithm1.5.5.m1.2.3.2.2.cmml"><mo stretchy="false" id="algorithm1.5.5.m1.1.1.1.3.1" xref="algorithm1.5.5.m1.2.3.2.2.cmml">(</mo><mi id="algorithm1.5.5.m1.1.1.1.1" xref="algorithm1.5.5.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.5.5.m1.1.1.1.3.2" xref="algorithm1.5.5.m1.2.3.2.2.cmml">)</mo></mrow></msubsup><mo fence="false" id="algorithm1.5.5.m1.2.3.2.1" xref="algorithm1.5.5.m1.2.3.2.1.cmml">|</mo><mi id="algorithm1.5.5.m1.2.3.2.3" xref="algorithm1.5.5.m1.2.3.2.3.cmml">i</mi></mrow><mo id="algorithm1.5.5.m1.2.3.1" xref="algorithm1.5.5.m1.2.3.1.cmml">âˆˆ</mo><mrow id="algorithm1.5.5.m1.2.3.3.2" xref="algorithm1.5.5.m1.2.3.3.1.cmml"><mo stretchy="false" id="algorithm1.5.5.m1.2.3.3.2.1" xref="algorithm1.5.5.m1.2.3.3.1.1.cmml">[</mo><mi id="algorithm1.5.5.m1.2.2" xref="algorithm1.5.5.m1.2.2.cmml">N</mi><mo stretchy="false" id="algorithm1.5.5.m1.2.3.3.2.2" xref="algorithm1.5.5.m1.2.3.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.2b"><apply id="algorithm1.5.5.m1.2.3.cmml" xref="algorithm1.5.5.m1.2.3"><in id="algorithm1.5.5.m1.2.3.1.cmml" xref="algorithm1.5.5.m1.2.3.1"></in><apply id="algorithm1.5.5.m1.2.3.2.cmml" xref="algorithm1.5.5.m1.2.3.2"><csymbol cd="latexml" id="algorithm1.5.5.m1.2.3.2.1.cmml" xref="algorithm1.5.5.m1.2.3.2.1">conditional</csymbol><apply id="algorithm1.5.5.m1.2.3.2.2.cmml" xref="algorithm1.5.5.m1.2.3.2.2"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.2.3.2.2.1.cmml" xref="algorithm1.5.5.m1.2.3.2.2">superscript</csymbol><apply id="algorithm1.5.5.m1.2.3.2.2.2.cmml" xref="algorithm1.5.5.m1.2.3.2.2"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.2.3.2.2.2.1.cmml" xref="algorithm1.5.5.m1.2.3.2.2">subscript</csymbol><ci id="algorithm1.5.5.m1.2.3.2.2.2.2.cmml" xref="algorithm1.5.5.m1.2.3.2.2.2.2">ğ‘</ci><apply id="algorithm1.5.5.m1.2.3.2.2.2.3.cmml" xref="algorithm1.5.5.m1.2.3.2.2.2.3"><times id="algorithm1.5.5.m1.2.3.2.2.2.3.1.cmml" xref="algorithm1.5.5.m1.2.3.2.2.2.3.1"></times><ci id="algorithm1.5.5.m1.2.3.2.2.2.3.2.cmml" xref="algorithm1.5.5.m1.2.3.2.2.2.3.2">ğ‘š</ci><ci id="algorithm1.5.5.m1.2.3.2.2.2.3.3.cmml" xref="algorithm1.5.5.m1.2.3.2.2.2.3.3">ğ‘</ci><ci id="algorithm1.5.5.m1.2.3.2.2.2.3.4.cmml" xref="algorithm1.5.5.m1.2.3.2.2.2.3.4">ğ‘¥</ci></apply></apply><ci id="algorithm1.5.5.m1.1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1.1.1">ğ‘–</ci></apply><ci id="algorithm1.5.5.m1.2.3.2.3.cmml" xref="algorithm1.5.5.m1.2.3.2.3">ğ‘–</ci></apply><apply id="algorithm1.5.5.m1.2.3.3.1.cmml" xref="algorithm1.5.5.m1.2.3.3.2"><csymbol cd="latexml" id="algorithm1.5.5.m1.2.3.3.1.1.cmml" xref="algorithm1.5.5.m1.2.3.3.2.1">delimited-[]</csymbol><ci id="algorithm1.5.5.m1.2.2.cmml" xref="algorithm1.5.5.m1.2.2">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.2c">a_{max}^{(i)}|i\in[N]</annotation></semantics></math> the maximum reward action for each seen task 
</div>
<div id="algorithm1.6.6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>
<span id="algorithm1.6.6.2" class="ltx_text ltx_font_bold">for</span>Â <em id="algorithm1.6.6.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.6.6.1.m1.1" class="ltx_Math" alttext="t\in[T]" display="inline"><semantics id="algorithm1.6.6.1.m1.1a"><mrow id="algorithm1.6.6.1.m1.1.2" xref="algorithm1.6.6.1.m1.1.2.cmml"><mi id="algorithm1.6.6.1.m1.1.2.2" xref="algorithm1.6.6.1.m1.1.2.2.cmml">t</mi><mo id="algorithm1.6.6.1.m1.1.2.1" xref="algorithm1.6.6.1.m1.1.2.1.cmml">âˆˆ</mo><mrow id="algorithm1.6.6.1.m1.1.2.3.2" xref="algorithm1.6.6.1.m1.1.2.3.1.cmml"><mo stretchy="false" id="algorithm1.6.6.1.m1.1.2.3.2.1" xref="algorithm1.6.6.1.m1.1.2.3.1.1.cmml">[</mo><mi id="algorithm1.6.6.1.m1.1.1" xref="algorithm1.6.6.1.m1.1.1.cmml">T</mi><mo stretchy="false" id="algorithm1.6.6.1.m1.1.2.3.2.2" xref="algorithm1.6.6.1.m1.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.1.m1.1b"><apply id="algorithm1.6.6.1.m1.1.2.cmml" xref="algorithm1.6.6.1.m1.1.2"><in id="algorithm1.6.6.1.m1.1.2.1.cmml" xref="algorithm1.6.6.1.m1.1.2.1"></in><ci id="algorithm1.6.6.1.m1.1.2.2.cmml" xref="algorithm1.6.6.1.m1.1.2.2">ğ‘¡</ci><apply id="algorithm1.6.6.1.m1.1.2.3.1.cmml" xref="algorithm1.6.6.1.m1.1.2.3.2"><csymbol cd="latexml" id="algorithm1.6.6.1.m1.1.2.3.1.1.cmml" xref="algorithm1.6.6.1.m1.1.2.3.2.1">delimited-[]</csymbol><ci id="algorithm1.6.6.1.m1.1.1.cmml" xref="algorithm1.6.6.1.m1.1.1">ğ‘‡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.1.m1.1c">t\in[T]</annotation></semantics></math></em>Â <span id="algorithm1.6.6.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.7.7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Set noise level <math id="algorithm1.7.7.m1.1" class="ltx_Math" alttext="\epsilon=\frac{\epsilon_{0}}{t}" display="inline"><semantics id="algorithm1.7.7.m1.1a"><mrow id="algorithm1.7.7.m1.1.1" xref="algorithm1.7.7.m1.1.1.cmml"><mi id="algorithm1.7.7.m1.1.1.2" xref="algorithm1.7.7.m1.1.1.2.cmml">Ïµ</mi><mo id="algorithm1.7.7.m1.1.1.1" xref="algorithm1.7.7.m1.1.1.1.cmml">=</mo><mfrac id="algorithm1.7.7.m1.1.1.3" xref="algorithm1.7.7.m1.1.1.3.cmml"><msub id="algorithm1.7.7.m1.1.1.3.2" xref="algorithm1.7.7.m1.1.1.3.2.cmml"><mi id="algorithm1.7.7.m1.1.1.3.2.2" xref="algorithm1.7.7.m1.1.1.3.2.2.cmml">Ïµ</mi><mn id="algorithm1.7.7.m1.1.1.3.2.3" xref="algorithm1.7.7.m1.1.1.3.2.3.cmml">0</mn></msub><mi id="algorithm1.7.7.m1.1.1.3.3" xref="algorithm1.7.7.m1.1.1.3.3.cmml">t</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m1.1b"><apply id="algorithm1.7.7.m1.1.1.cmml" xref="algorithm1.7.7.m1.1.1"><eq id="algorithm1.7.7.m1.1.1.1.cmml" xref="algorithm1.7.7.m1.1.1.1"></eq><ci id="algorithm1.7.7.m1.1.1.2.cmml" xref="algorithm1.7.7.m1.1.1.2">italic-Ïµ</ci><apply id="algorithm1.7.7.m1.1.1.3.cmml" xref="algorithm1.7.7.m1.1.1.3"><divide id="algorithm1.7.7.m1.1.1.3.1.cmml" xref="algorithm1.7.7.m1.1.1.3"></divide><apply id="algorithm1.7.7.m1.1.1.3.2.cmml" xref="algorithm1.7.7.m1.1.1.3.2"><csymbol cd="ambiguous" id="algorithm1.7.7.m1.1.1.3.2.1.cmml" xref="algorithm1.7.7.m1.1.1.3.2">subscript</csymbol><ci id="algorithm1.7.7.m1.1.1.3.2.2.cmml" xref="algorithm1.7.7.m1.1.1.3.2.2">italic-Ïµ</ci><cn type="integer" id="algorithm1.7.7.m1.1.1.3.2.3.cmml" xref="algorithm1.7.7.m1.1.1.3.2.3">0</cn></apply><ci id="algorithm1.7.7.m1.1.1.3.3.cmml" xref="algorithm1.7.7.m1.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m1.1c">\epsilon=\frac{\epsilon_{0}}{t}</annotation></semantics></math> 
</div>
<div id="algorithm1.10.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Sample minibatch <math id="algorithm1.8.8.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="algorithm1.8.8.m1.1a"><mi id="algorithm1.8.8.m1.1.1" xref="algorithm1.8.8.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.m1.1b"><ci id="algorithm1.8.8.m1.1.1.cmml" xref="algorithm1.8.8.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.m1.1c">\tau</annotation></semantics></math> of size <math id="algorithm1.9.9.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="algorithm1.9.9.m2.1a"><mi id="algorithm1.9.9.m2.1.1" xref="algorithm1.9.9.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m2.1b"><ci id="algorithm1.9.9.m2.1.1.cmml" xref="algorithm1.9.9.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m2.1c">n</annotation></semantics></math> from <math id="algorithm1.10.10.m3.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="algorithm1.10.10.m3.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.10.10.m3.1.1" xref="algorithm1.10.10.m3.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.m3.1b"><ci id="algorithm1.10.10.m3.1.1.cmml" xref="algorithm1.10.10.m3.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.m3.1c">\mathcal{T}</annotation></semantics></math> 
</div>
<div id="algorithm1.11.11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Get Task2Sim output distributions <math id="algorithm1.11.11.m1.2" class="ltx_Math" alttext="\pi^{(i)}|i\in[n]" display="inline"><semantics id="algorithm1.11.11.m1.2a"><mrow id="algorithm1.11.11.m1.2.3" xref="algorithm1.11.11.m1.2.3.cmml"><mrow id="algorithm1.11.11.m1.2.3.2" xref="algorithm1.11.11.m1.2.3.2.cmml"><msup id="algorithm1.11.11.m1.2.3.2.2" xref="algorithm1.11.11.m1.2.3.2.2.cmml"><mi id="algorithm1.11.11.m1.2.3.2.2.2" xref="algorithm1.11.11.m1.2.3.2.2.2.cmml">Ï€</mi><mrow id="algorithm1.11.11.m1.1.1.1.3" xref="algorithm1.11.11.m1.2.3.2.2.cmml"><mo stretchy="false" id="algorithm1.11.11.m1.1.1.1.3.1" xref="algorithm1.11.11.m1.2.3.2.2.cmml">(</mo><mi id="algorithm1.11.11.m1.1.1.1.1" xref="algorithm1.11.11.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.11.11.m1.1.1.1.3.2" xref="algorithm1.11.11.m1.2.3.2.2.cmml">)</mo></mrow></msup><mo fence="false" id="algorithm1.11.11.m1.2.3.2.1" xref="algorithm1.11.11.m1.2.3.2.1.cmml">|</mo><mi id="algorithm1.11.11.m1.2.3.2.3" xref="algorithm1.11.11.m1.2.3.2.3.cmml">i</mi></mrow><mo id="algorithm1.11.11.m1.2.3.1" xref="algorithm1.11.11.m1.2.3.1.cmml">âˆˆ</mo><mrow id="algorithm1.11.11.m1.2.3.3.2" xref="algorithm1.11.11.m1.2.3.3.1.cmml"><mo stretchy="false" id="algorithm1.11.11.m1.2.3.3.2.1" xref="algorithm1.11.11.m1.2.3.3.1.1.cmml">[</mo><mi id="algorithm1.11.11.m1.2.2" xref="algorithm1.11.11.m1.2.2.cmml">n</mi><mo stretchy="false" id="algorithm1.11.11.m1.2.3.3.2.2" xref="algorithm1.11.11.m1.2.3.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.m1.2b"><apply id="algorithm1.11.11.m1.2.3.cmml" xref="algorithm1.11.11.m1.2.3"><in id="algorithm1.11.11.m1.2.3.1.cmml" xref="algorithm1.11.11.m1.2.3.1"></in><apply id="algorithm1.11.11.m1.2.3.2.cmml" xref="algorithm1.11.11.m1.2.3.2"><csymbol cd="latexml" id="algorithm1.11.11.m1.2.3.2.1.cmml" xref="algorithm1.11.11.m1.2.3.2.1">conditional</csymbol><apply id="algorithm1.11.11.m1.2.3.2.2.cmml" xref="algorithm1.11.11.m1.2.3.2.2"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.2.3.2.2.1.cmml" xref="algorithm1.11.11.m1.2.3.2.2">superscript</csymbol><ci id="algorithm1.11.11.m1.2.3.2.2.2.cmml" xref="algorithm1.11.11.m1.2.3.2.2.2">ğœ‹</ci><ci id="algorithm1.11.11.m1.1.1.1.1.cmml" xref="algorithm1.11.11.m1.1.1.1.1">ğ‘–</ci></apply><ci id="algorithm1.11.11.m1.2.3.2.3.cmml" xref="algorithm1.11.11.m1.2.3.2.3">ğ‘–</ci></apply><apply id="algorithm1.11.11.m1.2.3.3.1.cmml" xref="algorithm1.11.11.m1.2.3.3.2"><csymbol cd="latexml" id="algorithm1.11.11.m1.2.3.3.1.1.cmml" xref="algorithm1.11.11.m1.2.3.3.2.1">delimited-[]</csymbol><ci id="algorithm1.11.11.m1.2.2.cmml" xref="algorithm1.11.11.m1.2.2">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.m1.2c">\pi^{(i)}|i\in[n]</annotation></semantics></math> 
</div>
<div id="algorithm1.12.12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Sample outputs <math id="algorithm1.12.12.m1.2" class="ltx_Math" alttext="a^{(i)}\sim\pi^{(i)}+\epsilon" display="inline"><semantics id="algorithm1.12.12.m1.2a"><mrow id="algorithm1.12.12.m1.2.3" xref="algorithm1.12.12.m1.2.3.cmml"><msup id="algorithm1.12.12.m1.2.3.2" xref="algorithm1.12.12.m1.2.3.2.cmml"><mi id="algorithm1.12.12.m1.2.3.2.2" xref="algorithm1.12.12.m1.2.3.2.2.cmml">a</mi><mrow id="algorithm1.12.12.m1.1.1.1.3" xref="algorithm1.12.12.m1.2.3.2.cmml"><mo stretchy="false" id="algorithm1.12.12.m1.1.1.1.3.1" xref="algorithm1.12.12.m1.2.3.2.cmml">(</mo><mi id="algorithm1.12.12.m1.1.1.1.1" xref="algorithm1.12.12.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.12.12.m1.1.1.1.3.2" xref="algorithm1.12.12.m1.2.3.2.cmml">)</mo></mrow></msup><mo id="algorithm1.12.12.m1.2.3.1" xref="algorithm1.12.12.m1.2.3.1.cmml">âˆ¼</mo><mrow id="algorithm1.12.12.m1.2.3.3" xref="algorithm1.12.12.m1.2.3.3.cmml"><msup id="algorithm1.12.12.m1.2.3.3.2" xref="algorithm1.12.12.m1.2.3.3.2.cmml"><mi id="algorithm1.12.12.m1.2.3.3.2.2" xref="algorithm1.12.12.m1.2.3.3.2.2.cmml">Ï€</mi><mrow id="algorithm1.12.12.m1.2.2.1.3" xref="algorithm1.12.12.m1.2.3.3.2.cmml"><mo stretchy="false" id="algorithm1.12.12.m1.2.2.1.3.1" xref="algorithm1.12.12.m1.2.3.3.2.cmml">(</mo><mi id="algorithm1.12.12.m1.2.2.1.1" xref="algorithm1.12.12.m1.2.2.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.12.12.m1.2.2.1.3.2" xref="algorithm1.12.12.m1.2.3.3.2.cmml">)</mo></mrow></msup><mo id="algorithm1.12.12.m1.2.3.3.1" xref="algorithm1.12.12.m1.2.3.3.1.cmml">+</mo><mi id="algorithm1.12.12.m1.2.3.3.3" xref="algorithm1.12.12.m1.2.3.3.3.cmml">Ïµ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.12.12.m1.2b"><apply id="algorithm1.12.12.m1.2.3.cmml" xref="algorithm1.12.12.m1.2.3"><csymbol cd="latexml" id="algorithm1.12.12.m1.2.3.1.cmml" xref="algorithm1.12.12.m1.2.3.1">similar-to</csymbol><apply id="algorithm1.12.12.m1.2.3.2.cmml" xref="algorithm1.12.12.m1.2.3.2"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.2.3.2.1.cmml" xref="algorithm1.12.12.m1.2.3.2">superscript</csymbol><ci id="algorithm1.12.12.m1.2.3.2.2.cmml" xref="algorithm1.12.12.m1.2.3.2.2">ğ‘</ci><ci id="algorithm1.12.12.m1.1.1.1.1.cmml" xref="algorithm1.12.12.m1.1.1.1.1">ğ‘–</ci></apply><apply id="algorithm1.12.12.m1.2.3.3.cmml" xref="algorithm1.12.12.m1.2.3.3"><plus id="algorithm1.12.12.m1.2.3.3.1.cmml" xref="algorithm1.12.12.m1.2.3.3.1"></plus><apply id="algorithm1.12.12.m1.2.3.3.2.cmml" xref="algorithm1.12.12.m1.2.3.3.2"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.2.3.3.2.1.cmml" xref="algorithm1.12.12.m1.2.3.3.2">superscript</csymbol><ci id="algorithm1.12.12.m1.2.3.3.2.2.cmml" xref="algorithm1.12.12.m1.2.3.3.2.2">ğœ‹</ci><ci id="algorithm1.12.12.m1.2.2.1.1.cmml" xref="algorithm1.12.12.m1.2.2.1.1">ğ‘–</ci></apply><ci id="algorithm1.12.12.m1.2.3.3.3.cmml" xref="algorithm1.12.12.m1.2.3.3.3">italic-Ïµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.12.12.m1.2c">a^{(i)}\sim\pi^{(i)}+\epsilon</annotation></semantics></math> 
</div>
<div id="algorithm1.14.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Get Rewards <math id="algorithm1.13.13.m1.2" class="ltx_Math" alttext="R(a^{(i)})" display="inline"><semantics id="algorithm1.13.13.m1.2a"><mrow id="algorithm1.13.13.m1.2.2" xref="algorithm1.13.13.m1.2.2.cmml"><mi id="algorithm1.13.13.m1.2.2.3" xref="algorithm1.13.13.m1.2.2.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="algorithm1.13.13.m1.2.2.2" xref="algorithm1.13.13.m1.2.2.2.cmml">â€‹</mo><mrow id="algorithm1.13.13.m1.2.2.1.1" xref="algorithm1.13.13.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="algorithm1.13.13.m1.2.2.1.1.2" xref="algorithm1.13.13.m1.2.2.1.1.1.cmml">(</mo><msup id="algorithm1.13.13.m1.2.2.1.1.1" xref="algorithm1.13.13.m1.2.2.1.1.1.cmml"><mi id="algorithm1.13.13.m1.2.2.1.1.1.2" xref="algorithm1.13.13.m1.2.2.1.1.1.2.cmml">a</mi><mrow id="algorithm1.13.13.m1.1.1.1.3" xref="algorithm1.13.13.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="algorithm1.13.13.m1.1.1.1.3.1" xref="algorithm1.13.13.m1.2.2.1.1.1.cmml">(</mo><mi id="algorithm1.13.13.m1.1.1.1.1" xref="algorithm1.13.13.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.13.13.m1.1.1.1.3.2" xref="algorithm1.13.13.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo stretchy="false" id="algorithm1.13.13.m1.2.2.1.1.3" xref="algorithm1.13.13.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.13.13.m1.2b"><apply id="algorithm1.13.13.m1.2.2.cmml" xref="algorithm1.13.13.m1.2.2"><times id="algorithm1.13.13.m1.2.2.2.cmml" xref="algorithm1.13.13.m1.2.2.2"></times><ci id="algorithm1.13.13.m1.2.2.3.cmml" xref="algorithm1.13.13.m1.2.2.3">ğ‘…</ci><apply id="algorithm1.13.13.m1.2.2.1.1.1.cmml" xref="algorithm1.13.13.m1.2.2.1.1"><csymbol cd="ambiguous" id="algorithm1.13.13.m1.2.2.1.1.1.1.cmml" xref="algorithm1.13.13.m1.2.2.1.1">superscript</csymbol><ci id="algorithm1.13.13.m1.2.2.1.1.1.2.cmml" xref="algorithm1.13.13.m1.2.2.1.1.1.2">ğ‘</ci><ci id="algorithm1.13.13.m1.1.1.1.1.cmml" xref="algorithm1.13.13.m1.1.1.1.1">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.13.13.m1.2c">R(a^{(i)})</annotation></semantics></math> by generating a synthetic dataset with parameters <math id="algorithm1.14.14.m2.1" class="ltx_Math" alttext="a^{(i)}" display="inline"><semantics id="algorithm1.14.14.m2.1a"><msup id="algorithm1.14.14.m2.1.2" xref="algorithm1.14.14.m2.1.2.cmml"><mi id="algorithm1.14.14.m2.1.2.2" xref="algorithm1.14.14.m2.1.2.2.cmml">a</mi><mrow id="algorithm1.14.14.m2.1.1.1.3" xref="algorithm1.14.14.m2.1.2.cmml"><mo stretchy="false" id="algorithm1.14.14.m2.1.1.1.3.1" xref="algorithm1.14.14.m2.1.2.cmml">(</mo><mi id="algorithm1.14.14.m2.1.1.1.1" xref="algorithm1.14.14.m2.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.14.14.m2.1.1.1.3.2" xref="algorithm1.14.14.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="algorithm1.14.14.m2.1b"><apply id="algorithm1.14.14.m2.1.2.cmml" xref="algorithm1.14.14.m2.1.2"><csymbol cd="ambiguous" id="algorithm1.14.14.m2.1.2.1.cmml" xref="algorithm1.14.14.m2.1.2">superscript</csymbol><ci id="algorithm1.14.14.m2.1.2.2.cmml" xref="algorithm1.14.14.m2.1.2.2">ğ‘</ci><ci id="algorithm1.14.14.m2.1.1.1.1.cmml" xref="algorithm1.14.14.m2.1.1.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.14.14.m2.1c">a^{(i)}</annotation></semantics></math>, pre-training a backbone on it, and getting the 5-NN downstream accuracy using this backbone 
</div>
<div id="algorithm1.16.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Update <math id="algorithm1.15.15.m1.1" class="ltx_Math" alttext="a_{max}^{(i)}" display="inline"><semantics id="algorithm1.15.15.m1.1a"><msubsup id="algorithm1.15.15.m1.1.2" xref="algorithm1.15.15.m1.1.2.cmml"><mi id="algorithm1.15.15.m1.1.2.2.2" xref="algorithm1.15.15.m1.1.2.2.2.cmml">a</mi><mrow id="algorithm1.15.15.m1.1.2.2.3" xref="algorithm1.15.15.m1.1.2.2.3.cmml"><mi id="algorithm1.15.15.m1.1.2.2.3.2" xref="algorithm1.15.15.m1.1.2.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="algorithm1.15.15.m1.1.2.2.3.1" xref="algorithm1.15.15.m1.1.2.2.3.1.cmml">â€‹</mo><mi id="algorithm1.15.15.m1.1.2.2.3.3" xref="algorithm1.15.15.m1.1.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.15.15.m1.1.2.2.3.1a" xref="algorithm1.15.15.m1.1.2.2.3.1.cmml">â€‹</mo><mi id="algorithm1.15.15.m1.1.2.2.3.4" xref="algorithm1.15.15.m1.1.2.2.3.4.cmml">x</mi></mrow><mrow id="algorithm1.15.15.m1.1.1.1.3" xref="algorithm1.15.15.m1.1.2.cmml"><mo stretchy="false" id="algorithm1.15.15.m1.1.1.1.3.1" xref="algorithm1.15.15.m1.1.2.cmml">(</mo><mi id="algorithm1.15.15.m1.1.1.1.1" xref="algorithm1.15.15.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.15.15.m1.1.1.1.3.2" xref="algorithm1.15.15.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.15.15.m1.1b"><apply id="algorithm1.15.15.m1.1.2.cmml" xref="algorithm1.15.15.m1.1.2"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.2.1.cmml" xref="algorithm1.15.15.m1.1.2">superscript</csymbol><apply id="algorithm1.15.15.m1.1.2.2.cmml" xref="algorithm1.15.15.m1.1.2"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.2.2.1.cmml" xref="algorithm1.15.15.m1.1.2">subscript</csymbol><ci id="algorithm1.15.15.m1.1.2.2.2.cmml" xref="algorithm1.15.15.m1.1.2.2.2">ğ‘</ci><apply id="algorithm1.15.15.m1.1.2.2.3.cmml" xref="algorithm1.15.15.m1.1.2.2.3"><times id="algorithm1.15.15.m1.1.2.2.3.1.cmml" xref="algorithm1.15.15.m1.1.2.2.3.1"></times><ci id="algorithm1.15.15.m1.1.2.2.3.2.cmml" xref="algorithm1.15.15.m1.1.2.2.3.2">ğ‘š</ci><ci id="algorithm1.15.15.m1.1.2.2.3.3.cmml" xref="algorithm1.15.15.m1.1.2.2.3.3">ğ‘</ci><ci id="algorithm1.15.15.m1.1.2.2.3.4.cmml" xref="algorithm1.15.15.m1.1.2.2.3.4">ğ‘¥</ci></apply></apply><ci id="algorithm1.15.15.m1.1.1.1.1.cmml" xref="algorithm1.15.15.m1.1.1.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.15.15.m1.1c">a_{max}^{(i)}</annotation></semantics></math> if <math id="algorithm1.16.16.m2.4" class="ltx_Math" alttext="R(a^{(i)})&gt;R(a_{max}^{(i)})" display="inline"><semantics id="algorithm1.16.16.m2.4a"><mrow id="algorithm1.16.16.m2.4.4" xref="algorithm1.16.16.m2.4.4.cmml"><mrow id="algorithm1.16.16.m2.3.3.1" xref="algorithm1.16.16.m2.3.3.1.cmml"><mi id="algorithm1.16.16.m2.3.3.1.3" xref="algorithm1.16.16.m2.3.3.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="algorithm1.16.16.m2.3.3.1.2" xref="algorithm1.16.16.m2.3.3.1.2.cmml">â€‹</mo><mrow id="algorithm1.16.16.m2.3.3.1.1.1" xref="algorithm1.16.16.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="algorithm1.16.16.m2.3.3.1.1.1.2" xref="algorithm1.16.16.m2.3.3.1.1.1.1.cmml">(</mo><msup id="algorithm1.16.16.m2.3.3.1.1.1.1" xref="algorithm1.16.16.m2.3.3.1.1.1.1.cmml"><mi id="algorithm1.16.16.m2.3.3.1.1.1.1.2" xref="algorithm1.16.16.m2.3.3.1.1.1.1.2.cmml">a</mi><mrow id="algorithm1.16.16.m2.1.1.1.3" xref="algorithm1.16.16.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="algorithm1.16.16.m2.1.1.1.3.1" xref="algorithm1.16.16.m2.3.3.1.1.1.1.cmml">(</mo><mi id="algorithm1.16.16.m2.1.1.1.1" xref="algorithm1.16.16.m2.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.16.16.m2.1.1.1.3.2" xref="algorithm1.16.16.m2.3.3.1.1.1.1.cmml">)</mo></mrow></msup><mo stretchy="false" id="algorithm1.16.16.m2.3.3.1.1.1.3" xref="algorithm1.16.16.m2.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="algorithm1.16.16.m2.4.4.3" xref="algorithm1.16.16.m2.4.4.3.cmml">&gt;</mo><mrow id="algorithm1.16.16.m2.4.4.2" xref="algorithm1.16.16.m2.4.4.2.cmml"><mi id="algorithm1.16.16.m2.4.4.2.3" xref="algorithm1.16.16.m2.4.4.2.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="algorithm1.16.16.m2.4.4.2.2" xref="algorithm1.16.16.m2.4.4.2.2.cmml">â€‹</mo><mrow id="algorithm1.16.16.m2.4.4.2.1.1" xref="algorithm1.16.16.m2.4.4.2.1.1.1.cmml"><mo stretchy="false" id="algorithm1.16.16.m2.4.4.2.1.1.2" xref="algorithm1.16.16.m2.4.4.2.1.1.1.cmml">(</mo><msubsup id="algorithm1.16.16.m2.4.4.2.1.1.1" xref="algorithm1.16.16.m2.4.4.2.1.1.1.cmml"><mi id="algorithm1.16.16.m2.4.4.2.1.1.1.2.2" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.2.cmml">a</mi><mrow id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.cmml"><mi id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.2" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.1" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.1.cmml">â€‹</mo><mi id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.3" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.1a" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.1.cmml">â€‹</mo><mi id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.4" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.4.cmml">x</mi></mrow><mrow id="algorithm1.16.16.m2.2.2.1.3" xref="algorithm1.16.16.m2.4.4.2.1.1.1.cmml"><mo stretchy="false" id="algorithm1.16.16.m2.2.2.1.3.1" xref="algorithm1.16.16.m2.4.4.2.1.1.1.cmml">(</mo><mi id="algorithm1.16.16.m2.2.2.1.1" xref="algorithm1.16.16.m2.2.2.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.16.16.m2.2.2.1.3.2" xref="algorithm1.16.16.m2.4.4.2.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="algorithm1.16.16.m2.4.4.2.1.1.3" xref="algorithm1.16.16.m2.4.4.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.16.16.m2.4b"><apply id="algorithm1.16.16.m2.4.4.cmml" xref="algorithm1.16.16.m2.4.4"><gt id="algorithm1.16.16.m2.4.4.3.cmml" xref="algorithm1.16.16.m2.4.4.3"></gt><apply id="algorithm1.16.16.m2.3.3.1.cmml" xref="algorithm1.16.16.m2.3.3.1"><times id="algorithm1.16.16.m2.3.3.1.2.cmml" xref="algorithm1.16.16.m2.3.3.1.2"></times><ci id="algorithm1.16.16.m2.3.3.1.3.cmml" xref="algorithm1.16.16.m2.3.3.1.3">ğ‘…</ci><apply id="algorithm1.16.16.m2.3.3.1.1.1.1.cmml" xref="algorithm1.16.16.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="algorithm1.16.16.m2.3.3.1.1.1.1.1.cmml" xref="algorithm1.16.16.m2.3.3.1.1.1">superscript</csymbol><ci id="algorithm1.16.16.m2.3.3.1.1.1.1.2.cmml" xref="algorithm1.16.16.m2.3.3.1.1.1.1.2">ğ‘</ci><ci id="algorithm1.16.16.m2.1.1.1.1.cmml" xref="algorithm1.16.16.m2.1.1.1.1">ğ‘–</ci></apply></apply><apply id="algorithm1.16.16.m2.4.4.2.cmml" xref="algorithm1.16.16.m2.4.4.2"><times id="algorithm1.16.16.m2.4.4.2.2.cmml" xref="algorithm1.16.16.m2.4.4.2.2"></times><ci id="algorithm1.16.16.m2.4.4.2.3.cmml" xref="algorithm1.16.16.m2.4.4.2.3">ğ‘…</ci><apply id="algorithm1.16.16.m2.4.4.2.1.1.1.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1"><csymbol cd="ambiguous" id="algorithm1.16.16.m2.4.4.2.1.1.1.1.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1">superscript</csymbol><apply id="algorithm1.16.16.m2.4.4.2.1.1.1.2.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1"><csymbol cd="ambiguous" id="algorithm1.16.16.m2.4.4.2.1.1.1.2.1.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1">subscript</csymbol><ci id="algorithm1.16.16.m2.4.4.2.1.1.1.2.2.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.2">ğ‘</ci><apply id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3"><times id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.1.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.1"></times><ci id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.2.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.2">ğ‘š</ci><ci id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.3.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.3">ğ‘</ci><ci id="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.4.cmml" xref="algorithm1.16.16.m2.4.4.2.1.1.1.2.3.4">ğ‘¥</ci></apply></apply><ci id="algorithm1.16.16.m2.2.2.1.1.cmml" xref="algorithm1.16.16.m2.2.2.1.1">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.16.16.m2.4c">R(a^{(i)})&gt;R(a_{max}^{(i)})</annotation></semantics></math> 
</div>
<div id="algorithm1.17.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Get point estimates of reward gradients <math id="algorithm1.17.17.m1.1" class="ltx_Math" alttext="dr^{(i)}" display="inline"><semantics id="algorithm1.17.17.m1.1a"><mrow id="algorithm1.17.17.m1.1.2" xref="algorithm1.17.17.m1.1.2.cmml"><mi id="algorithm1.17.17.m1.1.2.2" xref="algorithm1.17.17.m1.1.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.17.17.m1.1.2.1" xref="algorithm1.17.17.m1.1.2.1.cmml">â€‹</mo><msup id="algorithm1.17.17.m1.1.2.3" xref="algorithm1.17.17.m1.1.2.3.cmml"><mi id="algorithm1.17.17.m1.1.2.3.2" xref="algorithm1.17.17.m1.1.2.3.2.cmml">r</mi><mrow id="algorithm1.17.17.m1.1.1.1.3" xref="algorithm1.17.17.m1.1.2.3.cmml"><mo stretchy="false" id="algorithm1.17.17.m1.1.1.1.3.1" xref="algorithm1.17.17.m1.1.2.3.cmml">(</mo><mi id="algorithm1.17.17.m1.1.1.1.1" xref="algorithm1.17.17.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.17.17.m1.1.1.1.3.2" xref="algorithm1.17.17.m1.1.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.17.17.m1.1b"><apply id="algorithm1.17.17.m1.1.2.cmml" xref="algorithm1.17.17.m1.1.2"><times id="algorithm1.17.17.m1.1.2.1.cmml" xref="algorithm1.17.17.m1.1.2.1"></times><ci id="algorithm1.17.17.m1.1.2.2.cmml" xref="algorithm1.17.17.m1.1.2.2">ğ‘‘</ci><apply id="algorithm1.17.17.m1.1.2.3.cmml" xref="algorithm1.17.17.m1.1.2.3"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.1.2.3.1.cmml" xref="algorithm1.17.17.m1.1.2.3">superscript</csymbol><ci id="algorithm1.17.17.m1.1.2.3.2.cmml" xref="algorithm1.17.17.m1.1.2.3.2">ğ‘Ÿ</ci><ci id="algorithm1.17.17.m1.1.1.1.1.cmml" xref="algorithm1.17.17.m1.1.1.1.1">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.17.17.m1.1c">dr^{(i)}</annotation></semantics></math> for each task in minibatch using <a href="#S3.E4" title="In 3.3 Task2Sim Training â€£ 3 Proposed Approach â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> 
</div>
<div id="algorithm1.18.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<math id="algorithm1.18.18.m1.4" class="ltx_Math" alttext="\theta_{t,0}\leftarrow\theta_{t-1}+\frac{\sum_{i\in[n]}dr^{(i)}}{n}" display="inline"><semantics id="algorithm1.18.18.m1.4a"><mrow id="algorithm1.18.18.m1.4.5" xref="algorithm1.18.18.m1.4.5.cmml"><msub id="algorithm1.18.18.m1.4.5.2" xref="algorithm1.18.18.m1.4.5.2.cmml"><mi id="algorithm1.18.18.m1.4.5.2.2" xref="algorithm1.18.18.m1.4.5.2.2.cmml">Î¸</mi><mrow id="algorithm1.18.18.m1.2.2.2.4" xref="algorithm1.18.18.m1.2.2.2.3.cmml"><mi id="algorithm1.18.18.m1.1.1.1.1" xref="algorithm1.18.18.m1.1.1.1.1.cmml">t</mi><mo id="algorithm1.18.18.m1.2.2.2.4.1" xref="algorithm1.18.18.m1.2.2.2.3.cmml">,</mo><mn id="algorithm1.18.18.m1.2.2.2.2" xref="algorithm1.18.18.m1.2.2.2.2.cmml">0</mn></mrow></msub><mo stretchy="false" id="algorithm1.18.18.m1.4.5.1" xref="algorithm1.18.18.m1.4.5.1.cmml">â†</mo><mrow id="algorithm1.18.18.m1.4.5.3" xref="algorithm1.18.18.m1.4.5.3.cmml"><msub id="algorithm1.18.18.m1.4.5.3.2" xref="algorithm1.18.18.m1.4.5.3.2.cmml"><mi id="algorithm1.18.18.m1.4.5.3.2.2" xref="algorithm1.18.18.m1.4.5.3.2.2.cmml">Î¸</mi><mrow id="algorithm1.18.18.m1.4.5.3.2.3" xref="algorithm1.18.18.m1.4.5.3.2.3.cmml"><mi id="algorithm1.18.18.m1.4.5.3.2.3.2" xref="algorithm1.18.18.m1.4.5.3.2.3.2.cmml">t</mi><mo id="algorithm1.18.18.m1.4.5.3.2.3.1" xref="algorithm1.18.18.m1.4.5.3.2.3.1.cmml">âˆ’</mo><mn id="algorithm1.18.18.m1.4.5.3.2.3.3" xref="algorithm1.18.18.m1.4.5.3.2.3.3.cmml">1</mn></mrow></msub><mo id="algorithm1.18.18.m1.4.5.3.1" xref="algorithm1.18.18.m1.4.5.3.1.cmml">+</mo><mfrac id="algorithm1.18.18.m1.4.4" xref="algorithm1.18.18.m1.4.4.cmml"><mrow id="algorithm1.18.18.m1.4.4.2" xref="algorithm1.18.18.m1.4.4.2.cmml"><mstyle displaystyle="false" id="algorithm1.18.18.m1.4.4.2.3" xref="algorithm1.18.18.m1.4.4.2.3.cmml"><msub id="algorithm1.18.18.m1.4.4.2.3a" xref="algorithm1.18.18.m1.4.4.2.3.cmml"><mo id="algorithm1.18.18.m1.4.4.2.3.2" xref="algorithm1.18.18.m1.4.4.2.3.2.cmml">âˆ‘</mo><mrow id="algorithm1.18.18.m1.3.3.1.1.1" xref="algorithm1.18.18.m1.3.3.1.1.1.cmml"><mi id="algorithm1.18.18.m1.3.3.1.1.1.3" xref="algorithm1.18.18.m1.3.3.1.1.1.3.cmml">i</mi><mo id="algorithm1.18.18.m1.3.3.1.1.1.2" xref="algorithm1.18.18.m1.3.3.1.1.1.2.cmml">âˆˆ</mo><mrow id="algorithm1.18.18.m1.3.3.1.1.1.4.2" xref="algorithm1.18.18.m1.3.3.1.1.1.4.1.cmml"><mo stretchy="false" id="algorithm1.18.18.m1.3.3.1.1.1.4.2.1" xref="algorithm1.18.18.m1.3.3.1.1.1.4.1.1.cmml">[</mo><mi id="algorithm1.18.18.m1.3.3.1.1.1.1" xref="algorithm1.18.18.m1.3.3.1.1.1.1.cmml">n</mi><mo stretchy="false" id="algorithm1.18.18.m1.3.3.1.1.1.4.2.2" xref="algorithm1.18.18.m1.3.3.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></msub></mstyle><mrow id="algorithm1.18.18.m1.4.4.2.4" xref="algorithm1.18.18.m1.4.4.2.4.cmml"><mi id="algorithm1.18.18.m1.4.4.2.4.2" xref="algorithm1.18.18.m1.4.4.2.4.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.18.18.m1.4.4.2.4.1" xref="algorithm1.18.18.m1.4.4.2.4.1.cmml">â€‹</mo><msup id="algorithm1.18.18.m1.4.4.2.4.3" xref="algorithm1.18.18.m1.4.4.2.4.3.cmml"><mi id="algorithm1.18.18.m1.4.4.2.4.3.2" xref="algorithm1.18.18.m1.4.4.2.4.3.2.cmml">r</mi><mrow id="algorithm1.18.18.m1.4.4.2.2.1.3" xref="algorithm1.18.18.m1.4.4.2.4.3.cmml"><mo stretchy="false" id="algorithm1.18.18.m1.4.4.2.2.1.3.1" xref="algorithm1.18.18.m1.4.4.2.4.3.cmml">(</mo><mi id="algorithm1.18.18.m1.4.4.2.2.1.1" xref="algorithm1.18.18.m1.4.4.2.2.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.18.18.m1.4.4.2.2.1.3.2" xref="algorithm1.18.18.m1.4.4.2.4.3.cmml">)</mo></mrow></msup></mrow></mrow><mi id="algorithm1.18.18.m1.4.4.4" xref="algorithm1.18.18.m1.4.4.4.cmml">n</mi></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.18.18.m1.4b"><apply id="algorithm1.18.18.m1.4.5.cmml" xref="algorithm1.18.18.m1.4.5"><ci id="algorithm1.18.18.m1.4.5.1.cmml" xref="algorithm1.18.18.m1.4.5.1">â†</ci><apply id="algorithm1.18.18.m1.4.5.2.cmml" xref="algorithm1.18.18.m1.4.5.2"><csymbol cd="ambiguous" id="algorithm1.18.18.m1.4.5.2.1.cmml" xref="algorithm1.18.18.m1.4.5.2">subscript</csymbol><ci id="algorithm1.18.18.m1.4.5.2.2.cmml" xref="algorithm1.18.18.m1.4.5.2.2">ğœƒ</ci><list id="algorithm1.18.18.m1.2.2.2.3.cmml" xref="algorithm1.18.18.m1.2.2.2.4"><ci id="algorithm1.18.18.m1.1.1.1.1.cmml" xref="algorithm1.18.18.m1.1.1.1.1">ğ‘¡</ci><cn type="integer" id="algorithm1.18.18.m1.2.2.2.2.cmml" xref="algorithm1.18.18.m1.2.2.2.2">0</cn></list></apply><apply id="algorithm1.18.18.m1.4.5.3.cmml" xref="algorithm1.18.18.m1.4.5.3"><plus id="algorithm1.18.18.m1.4.5.3.1.cmml" xref="algorithm1.18.18.m1.4.5.3.1"></plus><apply id="algorithm1.18.18.m1.4.5.3.2.cmml" xref="algorithm1.18.18.m1.4.5.3.2"><csymbol cd="ambiguous" id="algorithm1.18.18.m1.4.5.3.2.1.cmml" xref="algorithm1.18.18.m1.4.5.3.2">subscript</csymbol><ci id="algorithm1.18.18.m1.4.5.3.2.2.cmml" xref="algorithm1.18.18.m1.4.5.3.2.2">ğœƒ</ci><apply id="algorithm1.18.18.m1.4.5.3.2.3.cmml" xref="algorithm1.18.18.m1.4.5.3.2.3"><minus id="algorithm1.18.18.m1.4.5.3.2.3.1.cmml" xref="algorithm1.18.18.m1.4.5.3.2.3.1"></minus><ci id="algorithm1.18.18.m1.4.5.3.2.3.2.cmml" xref="algorithm1.18.18.m1.4.5.3.2.3.2">ğ‘¡</ci><cn type="integer" id="algorithm1.18.18.m1.4.5.3.2.3.3.cmml" xref="algorithm1.18.18.m1.4.5.3.2.3.3">1</cn></apply></apply><apply id="algorithm1.18.18.m1.4.4.cmml" xref="algorithm1.18.18.m1.4.4"><divide id="algorithm1.18.18.m1.4.4.3.cmml" xref="algorithm1.18.18.m1.4.4"></divide><apply id="algorithm1.18.18.m1.4.4.2.cmml" xref="algorithm1.18.18.m1.4.4.2"><apply id="algorithm1.18.18.m1.4.4.2.3.cmml" xref="algorithm1.18.18.m1.4.4.2.3"><csymbol cd="ambiguous" id="algorithm1.18.18.m1.4.4.2.3.1.cmml" xref="algorithm1.18.18.m1.4.4.2.3">subscript</csymbol><sum id="algorithm1.18.18.m1.4.4.2.3.2.cmml" xref="algorithm1.18.18.m1.4.4.2.3.2"></sum><apply id="algorithm1.18.18.m1.3.3.1.1.1.cmml" xref="algorithm1.18.18.m1.3.3.1.1.1"><in id="algorithm1.18.18.m1.3.3.1.1.1.2.cmml" xref="algorithm1.18.18.m1.3.3.1.1.1.2"></in><ci id="algorithm1.18.18.m1.3.3.1.1.1.3.cmml" xref="algorithm1.18.18.m1.3.3.1.1.1.3">ğ‘–</ci><apply id="algorithm1.18.18.m1.3.3.1.1.1.4.1.cmml" xref="algorithm1.18.18.m1.3.3.1.1.1.4.2"><csymbol cd="latexml" id="algorithm1.18.18.m1.3.3.1.1.1.4.1.1.cmml" xref="algorithm1.18.18.m1.3.3.1.1.1.4.2.1">delimited-[]</csymbol><ci id="algorithm1.18.18.m1.3.3.1.1.1.1.cmml" xref="algorithm1.18.18.m1.3.3.1.1.1.1">ğ‘›</ci></apply></apply></apply><apply id="algorithm1.18.18.m1.4.4.2.4.cmml" xref="algorithm1.18.18.m1.4.4.2.4"><times id="algorithm1.18.18.m1.4.4.2.4.1.cmml" xref="algorithm1.18.18.m1.4.4.2.4.1"></times><ci id="algorithm1.18.18.m1.4.4.2.4.2.cmml" xref="algorithm1.18.18.m1.4.4.2.4.2">ğ‘‘</ci><apply id="algorithm1.18.18.m1.4.4.2.4.3.cmml" xref="algorithm1.18.18.m1.4.4.2.4.3"><csymbol cd="ambiguous" id="algorithm1.18.18.m1.4.4.2.4.3.1.cmml" xref="algorithm1.18.18.m1.4.4.2.4.3">superscript</csymbol><ci id="algorithm1.18.18.m1.4.4.2.4.3.2.cmml" xref="algorithm1.18.18.m1.4.4.2.4.3.2">ğ‘Ÿ</ci><ci id="algorithm1.18.18.m1.4.4.2.2.1.1.cmml" xref="algorithm1.18.18.m1.4.4.2.2.1.1">ğ‘–</ci></apply></apply></apply><ci id="algorithm1.18.18.m1.4.4.4.cmml" xref="algorithm1.18.18.m1.4.4.4">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.18.18.m1.4c">\theta_{t,0}\leftarrow\theta_{t-1}+\frac{\sum_{i\in[n]}dr^{(i)}}{n}</annotation></semantics></math>  
</div>
<div id="algorithm1.19.19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span id="algorithm1.19.19.2" class="ltx_text ltx_font_bold">for</span>Â <em id="algorithm1.19.19.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.19.19.1.m1.1" class="ltx_Math" alttext="j\in[T_{si}]" display="inline"><semantics id="algorithm1.19.19.1.m1.1a"><mrow id="algorithm1.19.19.1.m1.1.1" xref="algorithm1.19.19.1.m1.1.1.cmml"><mi id="algorithm1.19.19.1.m1.1.1.3" xref="algorithm1.19.19.1.m1.1.1.3.cmml">j</mi><mo id="algorithm1.19.19.1.m1.1.1.2" xref="algorithm1.19.19.1.m1.1.1.2.cmml">âˆˆ</mo><mrow id="algorithm1.19.19.1.m1.1.1.1.1" xref="algorithm1.19.19.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="algorithm1.19.19.1.m1.1.1.1.1.2" xref="algorithm1.19.19.1.m1.1.1.1.2.1.cmml">[</mo><msub id="algorithm1.19.19.1.m1.1.1.1.1.1" xref="algorithm1.19.19.1.m1.1.1.1.1.1.cmml"><mi id="algorithm1.19.19.1.m1.1.1.1.1.1.2" xref="algorithm1.19.19.1.m1.1.1.1.1.1.2.cmml">T</mi><mrow id="algorithm1.19.19.1.m1.1.1.1.1.1.3" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3.cmml"><mi id="algorithm1.19.19.1.m1.1.1.1.1.1.3.2" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="algorithm1.19.19.1.m1.1.1.1.1.1.3.1" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="algorithm1.19.19.1.m1.1.1.1.1.1.3.3" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo stretchy="false" id="algorithm1.19.19.1.m1.1.1.1.1.3" xref="algorithm1.19.19.1.m1.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.19.19.1.m1.1b"><apply id="algorithm1.19.19.1.m1.1.1.cmml" xref="algorithm1.19.19.1.m1.1.1"><in id="algorithm1.19.19.1.m1.1.1.2.cmml" xref="algorithm1.19.19.1.m1.1.1.2"></in><ci id="algorithm1.19.19.1.m1.1.1.3.cmml" xref="algorithm1.19.19.1.m1.1.1.3">ğ‘—</ci><apply id="algorithm1.19.19.1.m1.1.1.1.2.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1"><csymbol cd="latexml" id="algorithm1.19.19.1.m1.1.1.1.2.1.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="algorithm1.19.19.1.m1.1.1.1.1.1.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.19.19.1.m1.1.1.1.1.1.1.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.19.19.1.m1.1.1.1.1.1.2.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.1.2">ğ‘‡</ci><apply id="algorithm1.19.19.1.m1.1.1.1.1.1.3.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3"><times id="algorithm1.19.19.1.m1.1.1.1.1.1.3.1.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3.1"></times><ci id="algorithm1.19.19.1.m1.1.1.1.1.1.3.2.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3.2">ğ‘ </ci><ci id="algorithm1.19.19.1.m1.1.1.1.1.1.3.3.cmml" xref="algorithm1.19.19.1.m1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.19.19.1.m1.1c">j\in[T_{si}]</annotation></semantics></math></em>Â <span id="algorithm1.19.19.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.24.25" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span id="algorithm1.24.25.1" class="ltx_text ltx_font_typewriter">// </span><span id="algorithm1.24.25.2" class="ltx_text ltx_font_typewriter">Self Imitation </span>
</div>
<div id="algorithm1.21.21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
Get reward gradient estimates <math id="algorithm1.20.20.m1.1" class="ltx_Math" alttext="dr_{si}^{(i)}" display="inline"><semantics id="algorithm1.20.20.m1.1a"><mrow id="algorithm1.20.20.m1.1.2" xref="algorithm1.20.20.m1.1.2.cmml"><mi id="algorithm1.20.20.m1.1.2.2" xref="algorithm1.20.20.m1.1.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.20.20.m1.1.2.1" xref="algorithm1.20.20.m1.1.2.1.cmml">â€‹</mo><msubsup id="algorithm1.20.20.m1.1.2.3" xref="algorithm1.20.20.m1.1.2.3.cmml"><mi id="algorithm1.20.20.m1.1.2.3.2.2" xref="algorithm1.20.20.m1.1.2.3.2.2.cmml">r</mi><mrow id="algorithm1.20.20.m1.1.2.3.2.3" xref="algorithm1.20.20.m1.1.2.3.2.3.cmml"><mi id="algorithm1.20.20.m1.1.2.3.2.3.2" xref="algorithm1.20.20.m1.1.2.3.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="algorithm1.20.20.m1.1.2.3.2.3.1" xref="algorithm1.20.20.m1.1.2.3.2.3.1.cmml">â€‹</mo><mi id="algorithm1.20.20.m1.1.2.3.2.3.3" xref="algorithm1.20.20.m1.1.2.3.2.3.3.cmml">i</mi></mrow><mrow id="algorithm1.20.20.m1.1.1.1.3" xref="algorithm1.20.20.m1.1.2.3.cmml"><mo stretchy="false" id="algorithm1.20.20.m1.1.1.1.3.1" xref="algorithm1.20.20.m1.1.2.3.cmml">(</mo><mi id="algorithm1.20.20.m1.1.1.1.1" xref="algorithm1.20.20.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.20.20.m1.1.1.1.3.2" xref="algorithm1.20.20.m1.1.2.3.cmml">)</mo></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.20.20.m1.1b"><apply id="algorithm1.20.20.m1.1.2.cmml" xref="algorithm1.20.20.m1.1.2"><times id="algorithm1.20.20.m1.1.2.1.cmml" xref="algorithm1.20.20.m1.1.2.1"></times><ci id="algorithm1.20.20.m1.1.2.2.cmml" xref="algorithm1.20.20.m1.1.2.2">ğ‘‘</ci><apply id="algorithm1.20.20.m1.1.2.3.cmml" xref="algorithm1.20.20.m1.1.2.3"><csymbol cd="ambiguous" id="algorithm1.20.20.m1.1.2.3.1.cmml" xref="algorithm1.20.20.m1.1.2.3">superscript</csymbol><apply id="algorithm1.20.20.m1.1.2.3.2.cmml" xref="algorithm1.20.20.m1.1.2.3"><csymbol cd="ambiguous" id="algorithm1.20.20.m1.1.2.3.2.1.cmml" xref="algorithm1.20.20.m1.1.2.3">subscript</csymbol><ci id="algorithm1.20.20.m1.1.2.3.2.2.cmml" xref="algorithm1.20.20.m1.1.2.3.2.2">ğ‘Ÿ</ci><apply id="algorithm1.20.20.m1.1.2.3.2.3.cmml" xref="algorithm1.20.20.m1.1.2.3.2.3"><times id="algorithm1.20.20.m1.1.2.3.2.3.1.cmml" xref="algorithm1.20.20.m1.1.2.3.2.3.1"></times><ci id="algorithm1.20.20.m1.1.2.3.2.3.2.cmml" xref="algorithm1.20.20.m1.1.2.3.2.3.2">ğ‘ </ci><ci id="algorithm1.20.20.m1.1.2.3.2.3.3.cmml" xref="algorithm1.20.20.m1.1.2.3.2.3.3">ğ‘–</ci></apply></apply><ci id="algorithm1.20.20.m1.1.1.1.1.cmml" xref="algorithm1.20.20.m1.1.1.1.1">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.20.20.m1.1c">dr_{si}^{(i)}</annotation></semantics></math> from <a href="#S3.E4" title="In 3.3 Task2Sim Training â€£ 3 Proposed Approach â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> for <math id="algorithm1.21.21.m2.1" class="ltx_Math" alttext="a\leftarrow a_{max}^{(i)}" display="inline"><semantics id="algorithm1.21.21.m2.1a"><mrow id="algorithm1.21.21.m2.1.2" xref="algorithm1.21.21.m2.1.2.cmml"><mi id="algorithm1.21.21.m2.1.2.2" xref="algorithm1.21.21.m2.1.2.2.cmml">a</mi><mo stretchy="false" id="algorithm1.21.21.m2.1.2.1" xref="algorithm1.21.21.m2.1.2.1.cmml">â†</mo><msubsup id="algorithm1.21.21.m2.1.2.3" xref="algorithm1.21.21.m2.1.2.3.cmml"><mi id="algorithm1.21.21.m2.1.2.3.2.2" xref="algorithm1.21.21.m2.1.2.3.2.2.cmml">a</mi><mrow id="algorithm1.21.21.m2.1.2.3.2.3" xref="algorithm1.21.21.m2.1.2.3.2.3.cmml"><mi id="algorithm1.21.21.m2.1.2.3.2.3.2" xref="algorithm1.21.21.m2.1.2.3.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="algorithm1.21.21.m2.1.2.3.2.3.1" xref="algorithm1.21.21.m2.1.2.3.2.3.1.cmml">â€‹</mo><mi id="algorithm1.21.21.m2.1.2.3.2.3.3" xref="algorithm1.21.21.m2.1.2.3.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.21.21.m2.1.2.3.2.3.1a" xref="algorithm1.21.21.m2.1.2.3.2.3.1.cmml">â€‹</mo><mi id="algorithm1.21.21.m2.1.2.3.2.3.4" xref="algorithm1.21.21.m2.1.2.3.2.3.4.cmml">x</mi></mrow><mrow id="algorithm1.21.21.m2.1.1.1.3" xref="algorithm1.21.21.m2.1.2.3.cmml"><mo stretchy="false" id="algorithm1.21.21.m2.1.1.1.3.1" xref="algorithm1.21.21.m2.1.2.3.cmml">(</mo><mi id="algorithm1.21.21.m2.1.1.1.1" xref="algorithm1.21.21.m2.1.1.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.21.21.m2.1.1.1.3.2" xref="algorithm1.21.21.m2.1.2.3.cmml">)</mo></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.21.21.m2.1b"><apply id="algorithm1.21.21.m2.1.2.cmml" xref="algorithm1.21.21.m2.1.2"><ci id="algorithm1.21.21.m2.1.2.1.cmml" xref="algorithm1.21.21.m2.1.2.1">â†</ci><ci id="algorithm1.21.21.m2.1.2.2.cmml" xref="algorithm1.21.21.m2.1.2.2">ğ‘</ci><apply id="algorithm1.21.21.m2.1.2.3.cmml" xref="algorithm1.21.21.m2.1.2.3"><csymbol cd="ambiguous" id="algorithm1.21.21.m2.1.2.3.1.cmml" xref="algorithm1.21.21.m2.1.2.3">superscript</csymbol><apply id="algorithm1.21.21.m2.1.2.3.2.cmml" xref="algorithm1.21.21.m2.1.2.3"><csymbol cd="ambiguous" id="algorithm1.21.21.m2.1.2.3.2.1.cmml" xref="algorithm1.21.21.m2.1.2.3">subscript</csymbol><ci id="algorithm1.21.21.m2.1.2.3.2.2.cmml" xref="algorithm1.21.21.m2.1.2.3.2.2">ğ‘</ci><apply id="algorithm1.21.21.m2.1.2.3.2.3.cmml" xref="algorithm1.21.21.m2.1.2.3.2.3"><times id="algorithm1.21.21.m2.1.2.3.2.3.1.cmml" xref="algorithm1.21.21.m2.1.2.3.2.3.1"></times><ci id="algorithm1.21.21.m2.1.2.3.2.3.2.cmml" xref="algorithm1.21.21.m2.1.2.3.2.3.2">ğ‘š</ci><ci id="algorithm1.21.21.m2.1.2.3.2.3.3.cmml" xref="algorithm1.21.21.m2.1.2.3.2.3.3">ğ‘</ci><ci id="algorithm1.21.21.m2.1.2.3.2.3.4.cmml" xref="algorithm1.21.21.m2.1.2.3.2.3.4">ğ‘¥</ci></apply></apply><ci id="algorithm1.21.21.m2.1.1.1.1.cmml" xref="algorithm1.21.21.m2.1.1.1.1">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.21.21.m2.1c">a\leftarrow a_{max}^{(i)}</annotation></semantics></math> 
</div>
<div id="algorithm1.22.22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<math id="algorithm1.22.22.m1.6" class="ltx_Math" alttext="\theta_{t,j}\leftarrow\theta_{t,j-1}+\frac{\sum_{i\in[n]}dr_{si}^{(i)}}{n}" display="inline"><semantics id="algorithm1.22.22.m1.6a"><mrow id="algorithm1.22.22.m1.6.7" xref="algorithm1.22.22.m1.6.7.cmml"><msub id="algorithm1.22.22.m1.6.7.2" xref="algorithm1.22.22.m1.6.7.2.cmml"><mi id="algorithm1.22.22.m1.6.7.2.2" xref="algorithm1.22.22.m1.6.7.2.2.cmml">Î¸</mi><mrow id="algorithm1.22.22.m1.2.2.2.4" xref="algorithm1.22.22.m1.2.2.2.3.cmml"><mi id="algorithm1.22.22.m1.1.1.1.1" xref="algorithm1.22.22.m1.1.1.1.1.cmml">t</mi><mo id="algorithm1.22.22.m1.2.2.2.4.1" xref="algorithm1.22.22.m1.2.2.2.3.cmml">,</mo><mi id="algorithm1.22.22.m1.2.2.2.2" xref="algorithm1.22.22.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo stretchy="false" id="algorithm1.22.22.m1.6.7.1" xref="algorithm1.22.22.m1.6.7.1.cmml">â†</mo><mrow id="algorithm1.22.22.m1.6.7.3" xref="algorithm1.22.22.m1.6.7.3.cmml"><msub id="algorithm1.22.22.m1.6.7.3.2" xref="algorithm1.22.22.m1.6.7.3.2.cmml"><mi id="algorithm1.22.22.m1.6.7.3.2.2" xref="algorithm1.22.22.m1.6.7.3.2.2.cmml">Î¸</mi><mrow id="algorithm1.22.22.m1.4.4.2.2" xref="algorithm1.22.22.m1.4.4.2.3.cmml"><mi id="algorithm1.22.22.m1.3.3.1.1" xref="algorithm1.22.22.m1.3.3.1.1.cmml">t</mi><mo id="algorithm1.22.22.m1.4.4.2.2.2" xref="algorithm1.22.22.m1.4.4.2.3.cmml">,</mo><mrow id="algorithm1.22.22.m1.4.4.2.2.1" xref="algorithm1.22.22.m1.4.4.2.2.1.cmml"><mi id="algorithm1.22.22.m1.4.4.2.2.1.2" xref="algorithm1.22.22.m1.4.4.2.2.1.2.cmml">j</mi><mo id="algorithm1.22.22.m1.4.4.2.2.1.1" xref="algorithm1.22.22.m1.4.4.2.2.1.1.cmml">âˆ’</mo><mn id="algorithm1.22.22.m1.4.4.2.2.1.3" xref="algorithm1.22.22.m1.4.4.2.2.1.3.cmml">1</mn></mrow></mrow></msub><mo id="algorithm1.22.22.m1.6.7.3.1" xref="algorithm1.22.22.m1.6.7.3.1.cmml">+</mo><mfrac id="algorithm1.22.22.m1.6.6" xref="algorithm1.22.22.m1.6.6.cmml"><mrow id="algorithm1.22.22.m1.6.6.2" xref="algorithm1.22.22.m1.6.6.2.cmml"><mstyle displaystyle="false" id="algorithm1.22.22.m1.6.6.2.3" xref="algorithm1.22.22.m1.6.6.2.3.cmml"><msub id="algorithm1.22.22.m1.6.6.2.3a" xref="algorithm1.22.22.m1.6.6.2.3.cmml"><mo id="algorithm1.22.22.m1.6.6.2.3.2" xref="algorithm1.22.22.m1.6.6.2.3.2.cmml">âˆ‘</mo><mrow id="algorithm1.22.22.m1.5.5.1.1.1" xref="algorithm1.22.22.m1.5.5.1.1.1.cmml"><mi id="algorithm1.22.22.m1.5.5.1.1.1.3" xref="algorithm1.22.22.m1.5.5.1.1.1.3.cmml">i</mi><mo id="algorithm1.22.22.m1.5.5.1.1.1.2" xref="algorithm1.22.22.m1.5.5.1.1.1.2.cmml">âˆˆ</mo><mrow id="algorithm1.22.22.m1.5.5.1.1.1.4.2" xref="algorithm1.22.22.m1.5.5.1.1.1.4.1.cmml"><mo stretchy="false" id="algorithm1.22.22.m1.5.5.1.1.1.4.2.1" xref="algorithm1.22.22.m1.5.5.1.1.1.4.1.1.cmml">[</mo><mi id="algorithm1.22.22.m1.5.5.1.1.1.1" xref="algorithm1.22.22.m1.5.5.1.1.1.1.cmml">n</mi><mo stretchy="false" id="algorithm1.22.22.m1.5.5.1.1.1.4.2.2" xref="algorithm1.22.22.m1.5.5.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></msub></mstyle><mrow id="algorithm1.22.22.m1.6.6.2.4" xref="algorithm1.22.22.m1.6.6.2.4.cmml"><mi id="algorithm1.22.22.m1.6.6.2.4.2" xref="algorithm1.22.22.m1.6.6.2.4.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.22.22.m1.6.6.2.4.1" xref="algorithm1.22.22.m1.6.6.2.4.1.cmml">â€‹</mo><msubsup id="algorithm1.22.22.m1.6.6.2.4.3" xref="algorithm1.22.22.m1.6.6.2.4.3.cmml"><mi id="algorithm1.22.22.m1.6.6.2.4.3.2.2" xref="algorithm1.22.22.m1.6.6.2.4.3.2.2.cmml">r</mi><mrow id="algorithm1.22.22.m1.6.6.2.4.3.2.3" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3.cmml"><mi id="algorithm1.22.22.m1.6.6.2.4.3.2.3.2" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="algorithm1.22.22.m1.6.6.2.4.3.2.3.1" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3.1.cmml">â€‹</mo><mi id="algorithm1.22.22.m1.6.6.2.4.3.2.3.3" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3.3.cmml">i</mi></mrow><mrow id="algorithm1.22.22.m1.6.6.2.2.1.3" xref="algorithm1.22.22.m1.6.6.2.4.3.cmml"><mo stretchy="false" id="algorithm1.22.22.m1.6.6.2.2.1.3.1" xref="algorithm1.22.22.m1.6.6.2.4.3.cmml">(</mo><mi id="algorithm1.22.22.m1.6.6.2.2.1.1" xref="algorithm1.22.22.m1.6.6.2.2.1.1.cmml">i</mi><mo stretchy="false" id="algorithm1.22.22.m1.6.6.2.2.1.3.2" xref="algorithm1.22.22.m1.6.6.2.4.3.cmml">)</mo></mrow></msubsup></mrow></mrow><mi id="algorithm1.22.22.m1.6.6.4" xref="algorithm1.22.22.m1.6.6.4.cmml">n</mi></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.22.22.m1.6b"><apply id="algorithm1.22.22.m1.6.7.cmml" xref="algorithm1.22.22.m1.6.7"><ci id="algorithm1.22.22.m1.6.7.1.cmml" xref="algorithm1.22.22.m1.6.7.1">â†</ci><apply id="algorithm1.22.22.m1.6.7.2.cmml" xref="algorithm1.22.22.m1.6.7.2"><csymbol cd="ambiguous" id="algorithm1.22.22.m1.6.7.2.1.cmml" xref="algorithm1.22.22.m1.6.7.2">subscript</csymbol><ci id="algorithm1.22.22.m1.6.7.2.2.cmml" xref="algorithm1.22.22.m1.6.7.2.2">ğœƒ</ci><list id="algorithm1.22.22.m1.2.2.2.3.cmml" xref="algorithm1.22.22.m1.2.2.2.4"><ci id="algorithm1.22.22.m1.1.1.1.1.cmml" xref="algorithm1.22.22.m1.1.1.1.1">ğ‘¡</ci><ci id="algorithm1.22.22.m1.2.2.2.2.cmml" xref="algorithm1.22.22.m1.2.2.2.2">ğ‘—</ci></list></apply><apply id="algorithm1.22.22.m1.6.7.3.cmml" xref="algorithm1.22.22.m1.6.7.3"><plus id="algorithm1.22.22.m1.6.7.3.1.cmml" xref="algorithm1.22.22.m1.6.7.3.1"></plus><apply id="algorithm1.22.22.m1.6.7.3.2.cmml" xref="algorithm1.22.22.m1.6.7.3.2"><csymbol cd="ambiguous" id="algorithm1.22.22.m1.6.7.3.2.1.cmml" xref="algorithm1.22.22.m1.6.7.3.2">subscript</csymbol><ci id="algorithm1.22.22.m1.6.7.3.2.2.cmml" xref="algorithm1.22.22.m1.6.7.3.2.2">ğœƒ</ci><list id="algorithm1.22.22.m1.4.4.2.3.cmml" xref="algorithm1.22.22.m1.4.4.2.2"><ci id="algorithm1.22.22.m1.3.3.1.1.cmml" xref="algorithm1.22.22.m1.3.3.1.1">ğ‘¡</ci><apply id="algorithm1.22.22.m1.4.4.2.2.1.cmml" xref="algorithm1.22.22.m1.4.4.2.2.1"><minus id="algorithm1.22.22.m1.4.4.2.2.1.1.cmml" xref="algorithm1.22.22.m1.4.4.2.2.1.1"></minus><ci id="algorithm1.22.22.m1.4.4.2.2.1.2.cmml" xref="algorithm1.22.22.m1.4.4.2.2.1.2">ğ‘—</ci><cn type="integer" id="algorithm1.22.22.m1.4.4.2.2.1.3.cmml" xref="algorithm1.22.22.m1.4.4.2.2.1.3">1</cn></apply></list></apply><apply id="algorithm1.22.22.m1.6.6.cmml" xref="algorithm1.22.22.m1.6.6"><divide id="algorithm1.22.22.m1.6.6.3.cmml" xref="algorithm1.22.22.m1.6.6"></divide><apply id="algorithm1.22.22.m1.6.6.2.cmml" xref="algorithm1.22.22.m1.6.6.2"><apply id="algorithm1.22.22.m1.6.6.2.3.cmml" xref="algorithm1.22.22.m1.6.6.2.3"><csymbol cd="ambiguous" id="algorithm1.22.22.m1.6.6.2.3.1.cmml" xref="algorithm1.22.22.m1.6.6.2.3">subscript</csymbol><sum id="algorithm1.22.22.m1.6.6.2.3.2.cmml" xref="algorithm1.22.22.m1.6.6.2.3.2"></sum><apply id="algorithm1.22.22.m1.5.5.1.1.1.cmml" xref="algorithm1.22.22.m1.5.5.1.1.1"><in id="algorithm1.22.22.m1.5.5.1.1.1.2.cmml" xref="algorithm1.22.22.m1.5.5.1.1.1.2"></in><ci id="algorithm1.22.22.m1.5.5.1.1.1.3.cmml" xref="algorithm1.22.22.m1.5.5.1.1.1.3">ğ‘–</ci><apply id="algorithm1.22.22.m1.5.5.1.1.1.4.1.cmml" xref="algorithm1.22.22.m1.5.5.1.1.1.4.2"><csymbol cd="latexml" id="algorithm1.22.22.m1.5.5.1.1.1.4.1.1.cmml" xref="algorithm1.22.22.m1.5.5.1.1.1.4.2.1">delimited-[]</csymbol><ci id="algorithm1.22.22.m1.5.5.1.1.1.1.cmml" xref="algorithm1.22.22.m1.5.5.1.1.1.1">ğ‘›</ci></apply></apply></apply><apply id="algorithm1.22.22.m1.6.6.2.4.cmml" xref="algorithm1.22.22.m1.6.6.2.4"><times id="algorithm1.22.22.m1.6.6.2.4.1.cmml" xref="algorithm1.22.22.m1.6.6.2.4.1"></times><ci id="algorithm1.22.22.m1.6.6.2.4.2.cmml" xref="algorithm1.22.22.m1.6.6.2.4.2">ğ‘‘</ci><apply id="algorithm1.22.22.m1.6.6.2.4.3.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3"><csymbol cd="ambiguous" id="algorithm1.22.22.m1.6.6.2.4.3.1.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3">superscript</csymbol><apply id="algorithm1.22.22.m1.6.6.2.4.3.2.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3"><csymbol cd="ambiguous" id="algorithm1.22.22.m1.6.6.2.4.3.2.1.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3">subscript</csymbol><ci id="algorithm1.22.22.m1.6.6.2.4.3.2.2.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3.2.2">ğ‘Ÿ</ci><apply id="algorithm1.22.22.m1.6.6.2.4.3.2.3.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3"><times id="algorithm1.22.22.m1.6.6.2.4.3.2.3.1.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3.1"></times><ci id="algorithm1.22.22.m1.6.6.2.4.3.2.3.2.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3.2">ğ‘ </ci><ci id="algorithm1.22.22.m1.6.6.2.4.3.2.3.3.cmml" xref="algorithm1.22.22.m1.6.6.2.4.3.2.3.3">ğ‘–</ci></apply></apply><ci id="algorithm1.22.22.m1.6.6.2.2.1.1.cmml" xref="algorithm1.22.22.m1.6.6.2.2.1.1">ğ‘–</ci></apply></apply></apply><ci id="algorithm1.22.22.m1.6.6.4.cmml" xref="algorithm1.22.22.m1.6.6.4">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.22.22.m1.6c">\theta_{t,j}\leftarrow\theta_{t,j-1}+\frac{\sum_{i\in[n]}dr_{si}^{(i)}}{n}</annotation></semantics></math>

</div>
<div id="algorithm1.24.26" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">16</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â  end for
</div>
<div id="algorithm1.23.23" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">17</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â <math id="algorithm1.23.23.m1.2" class="ltx_Math" alttext="\theta_{t}\leftarrow\theta_{t,T_{si}}" display="inline"><semantics id="algorithm1.23.23.m1.2a"><mrow id="algorithm1.23.23.m1.2.3" xref="algorithm1.23.23.m1.2.3.cmml"><msub id="algorithm1.23.23.m1.2.3.2" xref="algorithm1.23.23.m1.2.3.2.cmml"><mi id="algorithm1.23.23.m1.2.3.2.2" xref="algorithm1.23.23.m1.2.3.2.2.cmml">Î¸</mi><mi id="algorithm1.23.23.m1.2.3.2.3" xref="algorithm1.23.23.m1.2.3.2.3.cmml">t</mi></msub><mo stretchy="false" id="algorithm1.23.23.m1.2.3.1" xref="algorithm1.23.23.m1.2.3.1.cmml">â†</mo><msub id="algorithm1.23.23.m1.2.3.3" xref="algorithm1.23.23.m1.2.3.3.cmml"><mi id="algorithm1.23.23.m1.2.3.3.2" xref="algorithm1.23.23.m1.2.3.3.2.cmml">Î¸</mi><mrow id="algorithm1.23.23.m1.2.2.2.2" xref="algorithm1.23.23.m1.2.2.2.3.cmml"><mi id="algorithm1.23.23.m1.1.1.1.1" xref="algorithm1.23.23.m1.1.1.1.1.cmml">t</mi><mo id="algorithm1.23.23.m1.2.2.2.2.2" xref="algorithm1.23.23.m1.2.2.2.3.cmml">,</mo><msub id="algorithm1.23.23.m1.2.2.2.2.1" xref="algorithm1.23.23.m1.2.2.2.2.1.cmml"><mi id="algorithm1.23.23.m1.2.2.2.2.1.2" xref="algorithm1.23.23.m1.2.2.2.2.1.2.cmml">T</mi><mrow id="algorithm1.23.23.m1.2.2.2.2.1.3" xref="algorithm1.23.23.m1.2.2.2.2.1.3.cmml"><mi id="algorithm1.23.23.m1.2.2.2.2.1.3.2" xref="algorithm1.23.23.m1.2.2.2.2.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="algorithm1.23.23.m1.2.2.2.2.1.3.1" xref="algorithm1.23.23.m1.2.2.2.2.1.3.1.cmml">â€‹</mo><mi id="algorithm1.23.23.m1.2.2.2.2.1.3.3" xref="algorithm1.23.23.m1.2.2.2.2.1.3.3.cmml">i</mi></mrow></msub></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.23.23.m1.2b"><apply id="algorithm1.23.23.m1.2.3.cmml" xref="algorithm1.23.23.m1.2.3"><ci id="algorithm1.23.23.m1.2.3.1.cmml" xref="algorithm1.23.23.m1.2.3.1">â†</ci><apply id="algorithm1.23.23.m1.2.3.2.cmml" xref="algorithm1.23.23.m1.2.3.2"><csymbol cd="ambiguous" id="algorithm1.23.23.m1.2.3.2.1.cmml" xref="algorithm1.23.23.m1.2.3.2">subscript</csymbol><ci id="algorithm1.23.23.m1.2.3.2.2.cmml" xref="algorithm1.23.23.m1.2.3.2.2">ğœƒ</ci><ci id="algorithm1.23.23.m1.2.3.2.3.cmml" xref="algorithm1.23.23.m1.2.3.2.3">ğ‘¡</ci></apply><apply id="algorithm1.23.23.m1.2.3.3.cmml" xref="algorithm1.23.23.m1.2.3.3"><csymbol cd="ambiguous" id="algorithm1.23.23.m1.2.3.3.1.cmml" xref="algorithm1.23.23.m1.2.3.3">subscript</csymbol><ci id="algorithm1.23.23.m1.2.3.3.2.cmml" xref="algorithm1.23.23.m1.2.3.3.2">ğœƒ</ci><list id="algorithm1.23.23.m1.2.2.2.3.cmml" xref="algorithm1.23.23.m1.2.2.2.2"><ci id="algorithm1.23.23.m1.1.1.1.1.cmml" xref="algorithm1.23.23.m1.1.1.1.1">ğ‘¡</ci><apply id="algorithm1.23.23.m1.2.2.2.2.1.cmml" xref="algorithm1.23.23.m1.2.2.2.2.1"><csymbol cd="ambiguous" id="algorithm1.23.23.m1.2.2.2.2.1.1.cmml" xref="algorithm1.23.23.m1.2.2.2.2.1">subscript</csymbol><ci id="algorithm1.23.23.m1.2.2.2.2.1.2.cmml" xref="algorithm1.23.23.m1.2.2.2.2.1.2">ğ‘‡</ci><apply id="algorithm1.23.23.m1.2.2.2.2.1.3.cmml" xref="algorithm1.23.23.m1.2.2.2.2.1.3"><times id="algorithm1.23.23.m1.2.2.2.2.1.3.1.cmml" xref="algorithm1.23.23.m1.2.2.2.2.1.3.1"></times><ci id="algorithm1.23.23.m1.2.2.2.2.1.3.2.cmml" xref="algorithm1.23.23.m1.2.2.2.2.1.3.2">ğ‘ </ci><ci id="algorithm1.23.23.m1.2.2.2.2.1.3.3.cmml" xref="algorithm1.23.23.m1.2.2.2.2.1.3.3">ğ‘–</ci></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.23.23.m1.2c">\theta_{t}\leftarrow\theta_{t,T_{si}}</annotation></semantics></math>

</div>
<div id="algorithm1.24.27" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">18</span> end for
</div>
<div id="algorithm1.24.24" class="ltx_listingline">
<span id="algorithm1.24.24.1" class="ltx_text ltx_font_bold">Output</span>: Trained model with parameters <math id="algorithm1.24.24.m1.1" class="ltx_Math" alttext="\theta_{T}" display="inline"><semantics id="algorithm1.24.24.m1.1a"><msub id="algorithm1.24.24.m1.1.1" xref="algorithm1.24.24.m1.1.1.cmml"><mi id="algorithm1.24.24.m1.1.1.2" xref="algorithm1.24.24.m1.1.1.2.cmml">Î¸</mi><mi id="algorithm1.24.24.m1.1.1.3" xref="algorithm1.24.24.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.24.24.m1.1b"><apply id="algorithm1.24.24.m1.1.1.cmml" xref="algorithm1.24.24.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.24.24.m1.1.1.1.cmml" xref="algorithm1.24.24.m1.1.1">subscript</csymbol><ci id="algorithm1.24.24.m1.1.1.2.cmml" xref="algorithm1.24.24.m1.1.1.2">ğœƒ</ci><ci id="algorithm1.24.24.m1.1.1.3.cmml" xref="algorithm1.24.24.m1.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.24.24.m1.1c">\theta_{T}</annotation></semantics></math>.


</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.26.1.1" class="ltx_text ltx_font_bold">AlgorithmÂ 1</span> </span>Training Task2Sim</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Details</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Downstream Tasks.</span> We use a set of 20 classification tasks with
12 tasks from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as the set of â€œseenâ€ tasks for our model and a separate set of 8 tasks as the â€œunseenâ€ set.
All our tasks can be broadly categorized into the following 6 classes (S:seen, U:unseen):</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Natural Images: CropDisease(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, Flowers102(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, DeepWeeds(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, CUB(U)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Aerial Images: EuroSAT(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, Resisc45(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, AID(U)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, CactusAerial(U)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Symbolic Images: SVHN(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, Omniglot(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, USPS(U)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Medical Images: ISIC(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, ChestX(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, ChestXPneumonia(U)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Illustrative Images: Kaokore(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, Sketch(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, Pacs-C(U), Pacs-S(U)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">Texture Images: DTD(S)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, FMD(U)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite></p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.3" class="ltx_p"><span id="S4.SS1.p2.3.1" class="ltx_text ltx_font_bold">Task2Sim Details.</span> We used a Resnet-18 probe network to generate 9600-dimensional Task2Vec representations of downstream tasks. The Task2Sim model is a multi-layer perceptron with 2 hidden layers, having ReLU activations. The model shares its first two layers for all <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">M</annotation></semantics></math> heads, and branches after that. It is trained for 1000 epochs on seen tasks, with a batch-size 4 and 5 self-imitation steps (<em id="S4.SS1.p2.3.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.3.3" class="ltx_text"></span> <math id="S4.SS1.p2.2.m2.2" class="ltx_Math" alttext="n=4,T_{si}=5" display="inline"><semantics id="S4.SS1.p2.2.m2.2a"><mrow id="S4.SS1.p2.2.m2.2.2.2" xref="S4.SS1.p2.2.m2.2.2.3.cmml"><mrow id="S4.SS1.p2.2.m2.1.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.1.1.2" xref="S4.SS1.p2.2.m2.1.1.1.1.2.cmml">n</mi><mo id="S4.SS1.p2.2.m2.1.1.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.2.m2.1.1.1.1.3" xref="S4.SS1.p2.2.m2.1.1.1.1.3.cmml">4</mn></mrow><mo id="S4.SS1.p2.2.m2.2.2.2.3" xref="S4.SS1.p2.2.m2.2.2.3a.cmml">,</mo><mrow id="S4.SS1.p2.2.m2.2.2.2.2" xref="S4.SS1.p2.2.m2.2.2.2.2.cmml"><msub id="S4.SS1.p2.2.m2.2.2.2.2.2" xref="S4.SS1.p2.2.m2.2.2.2.2.2.cmml"><mi id="S4.SS1.p2.2.m2.2.2.2.2.2.2" xref="S4.SS1.p2.2.m2.2.2.2.2.2.2.cmml">T</mi><mrow id="S4.SS1.p2.2.m2.2.2.2.2.2.3" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3.cmml"><mi id="S4.SS1.p2.2.m2.2.2.2.2.2.3.2" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.2.2.2.2.2.3.1" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.SS1.p2.2.m2.2.2.2.2.2.3.3" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3.3.cmml">i</mi></mrow></msub><mo id="S4.SS1.p2.2.m2.2.2.2.2.1" xref="S4.SS1.p2.2.m2.2.2.2.2.1.cmml">=</mo><mn id="S4.SS1.p2.2.m2.2.2.2.2.3" xref="S4.SS1.p2.2.m2.2.2.2.2.3.cmml">5</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.2b"><apply id="S4.SS1.p2.2.m2.2.2.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.2.2.3a.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.SS1.p2.2.m2.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1"><eq id="S4.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.1"></eq><ci id="S4.SS1.p2.2.m2.1.1.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.2">ğ‘›</ci><cn type="integer" id="S4.SS1.p2.2.m2.1.1.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.3">4</cn></apply><apply id="S4.SS1.p2.2.m2.2.2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2"><eq id="S4.SS1.p2.2.m2.2.2.2.2.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.1"></eq><apply id="S4.SS1.p2.2.m2.2.2.2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.2.2.2.2.2.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p2.2.m2.2.2.2.2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.2.2">ğ‘‡</ci><apply id="S4.SS1.p2.2.m2.2.2.2.2.2.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3"><times id="S4.SS1.p2.2.m2.2.2.2.2.2.3.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3.1"></times><ci id="S4.SS1.p2.2.m2.2.2.2.2.2.3.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3.2">ğ‘ </ci><ci id="S4.SS1.p2.2.m2.2.2.2.2.2.3.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.2.3.3">ğ‘–</ci></apply></apply><cn type="integer" id="S4.SS1.p2.2.m2.2.2.2.2.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.2c">n=4,T_{si}=5</annotation></semantics></math> and <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="T=1000" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">T</mi><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></eq><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">ğ‘‡</ci><cn type="integer" id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">T=1000</annotation></semantics></math>). We used a Resnet-50 model for pre-training and downstream evaluation for Task2Simâ€™s rewards. Details of all datasets, pre-training and evaluation procedures are included in Appendix <a href="#A5" title="Appendix E Training and Evaluation â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Synthetic Data Generation.</span> We use Three-D-World (TDW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for synthetic image generation. The platform provides 2322 different object models from 237 different classes, 57 of which overlap with Imagenet. Using TDW, we generate synthetic images of single objects from the aforementioned set (see FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for examples).</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.2" class="ltx_p">In this paper, we experiment with a parameterization of the pretraining dataset where <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="M=8" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">M</mi><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><eq id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></eq><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">ğ‘€</ci><cn type="integer" id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">M=8</annotation></semantics></math> and <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="k_{i}=2~{}\forall~{}i\in[M]" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.2" xref="S4.SS1.p4.2.m2.1.2.cmml"><msub id="S4.SS1.p4.2.m2.1.2.2" xref="S4.SS1.p4.2.m2.1.2.2.cmml"><mi id="S4.SS1.p4.2.m2.1.2.2.2" xref="S4.SS1.p4.2.m2.1.2.2.2.cmml">k</mi><mi id="S4.SS1.p4.2.m2.1.2.2.3" xref="S4.SS1.p4.2.m2.1.2.2.3.cmml">i</mi></msub><mo id="S4.SS1.p4.2.m2.1.2.3" xref="S4.SS1.p4.2.m2.1.2.3.cmml">=</mo><mrow id="S4.SS1.p4.2.m2.1.2.4" xref="S4.SS1.p4.2.m2.1.2.4.cmml"><mn id="S4.SS1.p4.2.m2.1.2.4.2" xref="S4.SS1.p4.2.m2.1.2.4.2.cmml">2</mn><mo lspace="0.497em" rspace="0em" id="S4.SS1.p4.2.m2.1.2.4.1" xref="S4.SS1.p4.2.m2.1.2.4.1.cmml">â€‹</mo><mrow id="S4.SS1.p4.2.m2.1.2.4.3" xref="S4.SS1.p4.2.m2.1.2.4.3.cmml"><mo rspace="0.497em" id="S4.SS1.p4.2.m2.1.2.4.3.1" xref="S4.SS1.p4.2.m2.1.2.4.3.1.cmml">âˆ€</mo><mi id="S4.SS1.p4.2.m2.1.2.4.3.2" xref="S4.SS1.p4.2.m2.1.2.4.3.2.cmml">i</mi></mrow></mrow><mo id="S4.SS1.p4.2.m2.1.2.5" xref="S4.SS1.p4.2.m2.1.2.5.cmml">âˆˆ</mo><mrow id="S4.SS1.p4.2.m2.1.2.6.2" xref="S4.SS1.p4.2.m2.1.2.6.1.cmml"><mo stretchy="false" id="S4.SS1.p4.2.m2.1.2.6.2.1" xref="S4.SS1.p4.2.m2.1.2.6.1.1.cmml">[</mo><mi id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">M</mi><mo stretchy="false" id="S4.SS1.p4.2.m2.1.2.6.2.2" xref="S4.SS1.p4.2.m2.1.2.6.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.2.cmml" xref="S4.SS1.p4.2.m2.1.2"><and id="S4.SS1.p4.2.m2.1.2a.cmml" xref="S4.SS1.p4.2.m2.1.2"></and><apply id="S4.SS1.p4.2.m2.1.2b.cmml" xref="S4.SS1.p4.2.m2.1.2"><eq id="S4.SS1.p4.2.m2.1.2.3.cmml" xref="S4.SS1.p4.2.m2.1.2.3"></eq><apply id="S4.SS1.p4.2.m2.1.2.2.cmml" xref="S4.SS1.p4.2.m2.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.2.2.1.cmml" xref="S4.SS1.p4.2.m2.1.2.2">subscript</csymbol><ci id="S4.SS1.p4.2.m2.1.2.2.2.cmml" xref="S4.SS1.p4.2.m2.1.2.2.2">ğ‘˜</ci><ci id="S4.SS1.p4.2.m2.1.2.2.3.cmml" xref="S4.SS1.p4.2.m2.1.2.2.3">ğ‘–</ci></apply><apply id="S4.SS1.p4.2.m2.1.2.4.cmml" xref="S4.SS1.p4.2.m2.1.2.4"><times id="S4.SS1.p4.2.m2.1.2.4.1.cmml" xref="S4.SS1.p4.2.m2.1.2.4.1"></times><cn type="integer" id="S4.SS1.p4.2.m2.1.2.4.2.cmml" xref="S4.SS1.p4.2.m2.1.2.4.2">2</cn><apply id="S4.SS1.p4.2.m2.1.2.4.3.cmml" xref="S4.SS1.p4.2.m2.1.2.4.3"><csymbol cd="latexml" id="S4.SS1.p4.2.m2.1.2.4.3.1.cmml" xref="S4.SS1.p4.2.m2.1.2.4.3.1">for-all</csymbol><ci id="S4.SS1.p4.2.m2.1.2.4.3.2.cmml" xref="S4.SS1.p4.2.m2.1.2.4.3.2">ğ‘–</ci></apply></apply></apply><apply id="S4.SS1.p4.2.m2.1.2c.cmml" xref="S4.SS1.p4.2.m2.1.2"><in id="S4.SS1.p4.2.m2.1.2.5.cmml" xref="S4.SS1.p4.2.m2.1.2.5"></in><share href="#S4.SS1.p4.2.m2.1.2.4.cmml" id="S4.SS1.p4.2.m2.1.2d.cmml" xref="S4.SS1.p4.2.m2.1.2"></share><apply id="S4.SS1.p4.2.m2.1.2.6.1.cmml" xref="S4.SS1.p4.2.m2.1.2.6.2"><csymbol cd="latexml" id="S4.SS1.p4.2.m2.1.2.6.1.1.cmml" xref="S4.SS1.p4.2.m2.1.2.6.2.1">delimited-[]</csymbol><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">ğ‘€</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">k_{i}=2~{}\forall~{}i\in[M]</annotation></semantics></math> (using terminology from Section <a href="#S3" title="3 Proposed Approach â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The 8 parameters are:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Object Rotation : If <math id="S4.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i1.p1.1.m1.1a"><mn id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b"><cn type="integer" id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">1</annotation></semantics></math>, multiple poses of an object are shown in the dataset, else, an object appears in a canonical pose in each image.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Object Distance (from camera) : If <math id="S4.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i2.p1.1.m1.1a"><mn id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><cn type="integer" id="S4.I2.i2.p1.1.m1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">1</annotation></semantics></math>, object distance from the camera is varied randomly within a certain range, else, it is kept fixed.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Lighting Intensity : If <math id="S4.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i3.p1.1.m1.1a"><mn id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><cn type="integer" id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">1</annotation></semantics></math>, intensity of the main lighting source (sun-like point light source at a distance) is varied, else it is fixed.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">Lighting Color : If <math id="S4.I2.i4.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i4.p1.1.m1.1a"><mn id="S4.I2.i4.p1.1.m1.1.1" xref="S4.I2.i4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p1.1.m1.1b"><cn type="integer" id="S4.I2.i4.p1.1.m1.1.1.cmml" xref="S4.I2.i4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p1.1.m1.1c">1</annotation></semantics></math>, RGB color of the main lighting source is varied, else it is fixed.</p>
</div>
</li>
<li id="S4.I2.i5" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i5.p1" class="ltx_para">
<p id="S4.I2.i5.p1.1" class="ltx_p">Lighting Direction : If <math id="S4.I2.i5.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i5.p1.1.m1.1a"><mn id="S4.I2.i5.p1.1.m1.1.1" xref="S4.I2.i5.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i5.p1.1.m1.1b"><cn type="integer" id="S4.I2.i5.p1.1.m1.1.1.cmml" xref="S4.I2.i5.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i5.p1.1.m1.1c">1</annotation></semantics></math>, the direction of the main light source is varied, else it is a constant.</p>
</div>
</li>
<li id="S4.I2.i6" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i6.p1" class="ltx_para">
<p id="S4.I2.i6.p1.1" class="ltx_p">Focus Blur : If <math id="S4.I2.i6.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i6.p1.1.m1.1a"><mn id="S4.I2.i6.p1.1.m1.1.1" xref="S4.I2.i6.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i6.p1.1.m1.1b"><cn type="integer" id="S4.I2.i6.p1.1.m1.1.1.cmml" xref="S4.I2.i6.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i6.p1.1.m1.1c">1</annotation></semantics></math>, camera focus point and aperture are randomly perturbed to induce blurriness in the image, else, all image contents are always in focus.</p>
</div>
</li>
<li id="S4.I2.i7" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i7.p1" class="ltx_para">
<p id="S4.I2.i7.p1.1" class="ltx_p">Background : If <math id="S4.I2.i7.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i7.p1.1.m1.1a"><mn id="S4.I2.i7.p1.1.m1.1.1" xref="S4.I2.i7.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i7.p1.1.m1.1b"><cn type="integer" id="S4.I2.i7.p1.1.m1.1.1.cmml" xref="S4.I2.i7.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i7.p1.1.m1.1c">1</annotation></semantics></math>, background of the object changes in each image, else it is held fixed.</p>
</div>
</li>
<li id="S4.I2.i8" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i8.p1" class="ltx_para">
<p id="S4.I2.i8.p1.1" class="ltx_p">Materials : If <math id="S4.I2.i8.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i8.p1.1.m1.1a"><mn id="S4.I2.i8.p1.1.m1.1.1" xref="S4.I2.i8.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i8.p1.1.m1.1b"><cn type="integer" id="S4.I2.i8.p1.1.m1.1.1.cmml" xref="S4.I2.i8.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i8.p1.1.m1.1c">1</annotation></semantics></math>, in each image, each component of an object is given a random material out of 140 different materials, else objects have their default materials.</p>
</div>
</li>
</ul>
<p id="S4.SS1.p4.3" class="ltx_p">Hence in our experiments, for each of the above 8 parameters, Task2Sim decided whether or not different variations of it, would exhibit in the dataset. For speed of dataset generation while training Task2Sim, we used a subset of 780 objects with simple meshes, from 100 different categories and generated 400 images per category for pre-training.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Task2SimÂ Results</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Baselines.</span> We compared Task2Simâ€™s downstream performance with the following baselines (pre-training datasets):
(1) Random : For each downstream dataset, chooses a random 8-length bit string as the set of simulation parameters.
(2) Domain Randomization : Uses a 1 in each simulation parameter, thus using all variations from simulation in each image.
(3) Imagenet : Uses a subset of Imagenet with equal number of classes and images as other baselines<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span> We also compared pre-training using Imagenet with 1K classes and an equal number of images, but this was poorer on average in downstream performance than the subset with fewer classes. <a href="#S4.T2" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tables</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span> andÂ <a href="#S4.T3" title="Table 3 â€£ 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4.F3" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span> andÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> do not include it for succinctness.</span></span></span>.
(4)Â Scratch : Does not involve any pre-training of the classifierâ€™s feature extractor, training a randomly initialized classifier, with only downstream task data.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:489.6pt;height:170.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<td id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span></td>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center" colspan="5"><span id="S4.T2.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Average Downstream Accuracy â€” <span id="S4.T2.1.1.2.1.2.1.1" class="ltx_text ltx_font_typewriter">Seen Tasks</span></span></td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S4.T2.1.1.3.2.2.1" class="ltx_text ltx_font_bold">100 classes / 40k images</span></td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T2.1.1.3.2.3.1" class="ltx_text ltx_font_bold">237 classes / 100k images</span></td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Pretraining Dataset</span></td>
<th id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T2.1.1.4.3.2.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.3.2.2" class="ltx_text">
<span id="S4.T2.1.1.4.3.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.3.2.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.1.4.3.2.2.1.1.1.1" class="ltx_text ltx_font_bold">5NN</span></span></span>
</span></span><span id="S4.T2.1.1.4.3.2.3" class="ltx_text"></span></th>
<th id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T2.1.1.4.3.3.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.3.3.2" class="ltx_text">
<span id="S4.T2.1.1.4.3.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.3.3.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.1.4.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Linear Probing</span></span></span>
</span></span><span id="S4.T2.1.1.4.3.3.3" class="ltx_text"></span></th>
<th id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.4.3.4.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.3.4.2" class="ltx_text">
<span id="S4.T2.1.1.4.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.3.4.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.3.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.1.4.3.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Finetuning</span></span></span>
</span></span><span id="S4.T2.1.1.4.3.4.3" class="ltx_text"></span></th>
<th id="S4.T2.1.1.4.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T2.1.1.4.3.5.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.3.5.2" class="ltx_text">
<span id="S4.T2.1.1.4.3.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.3.5.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.3.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.1.4.3.5.2.1.1.1.1" class="ltx_text ltx_font_bold">Linear Probing</span></span></span>
</span></span><span id="S4.T2.1.1.4.3.5.3" class="ltx_text"></span></th>
<th id="S4.T2.1.1.4.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T2.1.1.4.3.6.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.3.6.2" class="ltx_text">
<span id="S4.T2.1.1.4.3.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.3.6.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.3.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.1.4.3.6.2.1.1.1.1" class="ltx_text ltx_font_bold">Finetuning</span></span></span>
</span></span><span id="S4.T2.1.1.4.3.6.3" class="ltx_text"></span></th>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span>
Scratch</td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">64.85</td>
<td id="S4.T2.1.1.5.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.5.4.6" class="ltx_td ltx_align_center">64.85</td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<td id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">Random</td>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_center">25.30</td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_center">54.06</td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">70.77</td>
<td id="S4.T2.1.1.6.5.5" class="ltx_td ltx_align_center">55.14</td>
<td id="S4.T2.1.1.6.5.6" class="ltx_td ltx_align_center">72.18</td>
</tr>
<tr id="S4.T2.1.1.7.6" class="ltx_tr">
<td id="S4.T2.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">Domain Randomization</td>
<td id="S4.T2.1.1.7.6.2" class="ltx_td ltx_align_center">19.42</td>
<td id="S4.T2.1.1.7.6.3" class="ltx_td ltx_align_center">35.31</td>
<td id="S4.T2.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">62.96</td>
<td id="S4.T2.1.1.7.6.5" class="ltx_td ltx_align_center">45.31</td>
<td id="S4.T2.1.1.7.6.6" class="ltx_td ltx_align_center">68.51</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r">Imagenet<sup id="S4.T2.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>
</td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_framed ltx_framed_underline">28.91</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">63.12</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_framed ltx_framed_underline">74.26</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">68.44</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">77.61</span></td>
</tr>
<tr id="S4.T2.1.1.8.7" class="ltx_tr">
<td id="S4.T2.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.1.1.8.7.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Task2Sim</span></td>
<td id="S4.T2.1.1.8.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.8.7.2.1" class="ltx_text ltx_font_bold">30.46</span></td>
<td id="S4.T2.1.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.8.7.3.1" class="ltx_text ltx_framed ltx_framed_underline">62.70</span></td>
<td id="S4.T2.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.8.7.4.1" class="ltx_text ltx_font_bold">75.34</span></td>
<td id="S4.T2.1.1.8.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.8.7.5.1" class="ltx_text ltx_framed ltx_framed_underline">62.71</span></td>
<td id="S4.T2.1.1.8.7.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.8.7.6.1" class="ltx_text ltx_framed ltx_framed_underline">76.87</span></td>
</tr>
<tr id="S4.T2.1.1.9.8" class="ltx_tr">
<td id="S4.T2.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span></td>
<td id="S4.T2.1.1.9.8.2" class="ltx_td"></td>
<td id="S4.T2.1.1.9.8.3" class="ltx_td"></td>
<td id="S4.T2.1.1.9.8.4" class="ltx_td"></td>
<td id="S4.T2.1.1.9.8.5" class="ltx_td"></td>
<td id="S4.T2.1.1.9.8.6" class="ltx_td"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.8.3.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text" style="font-size:90%;">Comparing the downstream accuracy on seen tasks for the Task2Sim chosen pretraining dataset and other baselines. Simulation parameters found on seen tasks by Task2Sim generates synthetic pretraining data that is better for downstream tasks than other approaches like using Random simulation parameters or Domain Randomization. Pre-training with Task2Simâ€™s data is also competitive with pre-training on images from Imagenet. <sup id="S4.T2.5.2.1" class="ltx_sup"><span id="S4.T2.5.2.1.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>Imagenet has been subsampled to the same number of classes and images as indicated at the top of the column. boldface=highest, underline=<math id="S4.T2.5.2.m2.1" class="ltx_Math" alttext="2^{nd}" display="inline"><semantics id="S4.T2.5.2.m2.1b"><msup id="S4.T2.5.2.m2.1.1" xref="S4.T2.5.2.m2.1.1.cmml"><mn id="S4.T2.5.2.m2.1.1.2" xref="S4.T2.5.2.m2.1.1.2.cmml">2</mn><mrow id="S4.T2.5.2.m2.1.1.3" xref="S4.T2.5.2.m2.1.1.3.cmml"><mi id="S4.T2.5.2.m2.1.1.3.2" xref="S4.T2.5.2.m2.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.T2.5.2.m2.1.1.3.1" xref="S4.T2.5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.T2.5.2.m2.1.1.3.3" xref="S4.T2.5.2.m2.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T2.5.2.m2.1c"><apply id="S4.T2.5.2.m2.1.1.cmml" xref="S4.T2.5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T2.5.2.m2.1.1.1.cmml" xref="S4.T2.5.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.T2.5.2.m2.1.1.2.cmml" xref="S4.T2.5.2.m2.1.1.2">2</cn><apply id="S4.T2.5.2.m2.1.1.3.cmml" xref="S4.T2.5.2.m2.1.1.3"><times id="S4.T2.5.2.m2.1.1.3.1.cmml" xref="S4.T2.5.2.m2.1.1.3.1"></times><ci id="S4.T2.5.2.m2.1.1.3.2.cmml" xref="S4.T2.5.2.m2.1.1.3.2">ğ‘›</ci><ci id="S4.T2.5.2.m2.1.1.3.3.cmml" xref="S4.T2.5.2.m2.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.2.m2.1d">2^{nd}</annotation></semantics></math> highest in column.</span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:489.6pt;height:170.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<td id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span></td>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center" colspan="5"><span id="S4.T3.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Average Downstream Accuracy â€” <span id="S4.T3.1.1.2.1.2.1.1" class="ltx_text ltx_font_typewriter">Unseen Tasks</span></span></td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<td id="S4.T3.1.1.3.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S4.T3.1.1.3.2.2.1" class="ltx_text ltx_font_bold">100 classes / 40k images</span></td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T3.1.1.3.2.3.1" class="ltx_text ltx_font_bold">237 classes / 100k images</span></td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<td id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Pretraining Dataset</span></td>
<th id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T3.1.1.4.3.2.1" class="ltx_text"></span> <span id="S4.T3.1.1.4.3.2.2" class="ltx_text">
<span id="S4.T3.1.1.4.3.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.4.3.2.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.4.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.1.4.3.2.2.1.1.1.1" class="ltx_text ltx_font_bold">5NN</span></span></span>
</span></span><span id="S4.T3.1.1.4.3.2.3" class="ltx_text"></span></th>
<th id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T3.1.1.4.3.3.1" class="ltx_text"></span> <span id="S4.T3.1.1.4.3.3.2" class="ltx_text">
<span id="S4.T3.1.1.4.3.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.4.3.3.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.4.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.1.4.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Linear Probing</span></span></span>
</span></span><span id="S4.T3.1.1.4.3.3.3" class="ltx_text"></span></th>
<th id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.4.3.4.1" class="ltx_text"></span> <span id="S4.T3.1.1.4.3.4.2" class="ltx_text">
<span id="S4.T3.1.1.4.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.4.3.4.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.4.3.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.1.4.3.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Finetuning</span></span></span>
</span></span><span id="S4.T3.1.1.4.3.4.3" class="ltx_text"></span></th>
<th id="S4.T3.1.1.4.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T3.1.1.4.3.5.1" class="ltx_text"></span> <span id="S4.T3.1.1.4.3.5.2" class="ltx_text">
<span id="S4.T3.1.1.4.3.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.4.3.5.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.4.3.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.1.4.3.5.2.1.1.1.1" class="ltx_text ltx_font_bold">Linear Probing</span></span></span>
</span></span><span id="S4.T3.1.1.4.3.5.3" class="ltx_text"></span></th>
<th id="S4.T3.1.1.4.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T3.1.1.4.3.6.1" class="ltx_text"></span> <span id="S4.T3.1.1.4.3.6.2" class="ltx_text">
<span id="S4.T3.1.1.4.3.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.4.3.6.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.4.3.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.1.4.3.6.2.1.1.1.1" class="ltx_text ltx_font_bold">Finetuning</span></span></span>
</span></span><span id="S4.T3.1.1.4.3.6.3" class="ltx_text"></span></th>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<td id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span>
Scratch</td>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">76.86</td>
<td id="S4.T3.1.1.5.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.4.6" class="ltx_td ltx_align_center">76.86</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<td id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">Random</td>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center">51.80</td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center">74.68</td>
<td id="S4.T3.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">83.97</td>
<td id="S4.T3.1.1.6.5.5" class="ltx_td ltx_align_center">74.11</td>
<td id="S4.T3.1.1.6.5.6" class="ltx_td ltx_align_center">83.49</td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<td id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">Domain Randomization</td>
<td id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center">45.06</td>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center">56.96</td>
<td id="S4.T3.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">72.64</td>
<td id="S4.T3.1.1.7.6.5" class="ltx_td ltx_align_center">69.12</td>
<td id="S4.T3.1.1.7.6.6" class="ltx_td ltx_align_center">78.15</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r">Imagenet<sup id="S4.T3.1.1.1.1.1" class="ltx_sup">âˆ—</sup>
</td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">54.12</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">75.47</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_framed ltx_framed_underline">84.78</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_framed ltx_framed_underline">81.33</span></td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.6.1" class="ltx_text ltx_framed ltx_framed_underline">87.84</span></td>
</tr>
<tr id="S4.T3.1.1.8.7" class="ltx_tr">
<td id="S4.T3.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.1.1.8.7.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Task2Sim</span></td>
<td id="S4.T3.1.1.8.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.8.7.2.1" class="ltx_text ltx_framed ltx_framed_underline">53.06</span></td>
<td id="S4.T3.1.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.8.7.3.1" class="ltx_text ltx_font_bold">79.25</span></td>
<td id="S4.T3.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.8.7.4.1" class="ltx_text ltx_font_bold">87.05</span></td>
<td id="S4.T3.1.1.8.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.8.7.5.1" class="ltx_text ltx_font_bold">82.05</span></td>
<td id="S4.T3.1.1.8.7.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.8.7.6.1" class="ltx_text ltx_font_bold">88.77</span></td>
</tr>
<tr id="S4.T3.1.1.9.8" class="ltx_tr">
<td id="S4.T3.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span></td>
<td id="S4.T3.1.1.9.8.2" class="ltx_td"></td>
<td id="S4.T3.1.1.9.8.3" class="ltx_td"></td>
<td id="S4.T3.1.1.9.8.4" class="ltx_td"></td>
<td id="S4.T3.1.1.9.8.5" class="ltx_td"></td>
<td id="S4.T3.1.1.9.8.6" class="ltx_td"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.8.3.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text" style="font-size:90%;">Comparing the downstream accuracy on unseen tasks for the Task2Sim chosen pretraining dataset and other baselines. Task2Sim also generalizes well to â€œunseenâ€ tasks, not encountered during training, maintaining an edge over other synthetic data, while still being competitive with Imagenet. <sup id="S4.T3.5.2.1" class="ltx_sup"><span id="S4.T3.5.2.1.1" class="ltx_text ltx_font_italic">âˆ—</span></sup>Imagenet subsampled as in <a href="#S4.T2" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. boldface=highest, underline=<math id="S4.T3.5.2.m2.1" class="ltx_Math" alttext="2^{nd}" display="inline"><semantics id="S4.T3.5.2.m2.1b"><msup id="S4.T3.5.2.m2.1.1" xref="S4.T3.5.2.m2.1.1.cmml"><mn id="S4.T3.5.2.m2.1.1.2" xref="S4.T3.5.2.m2.1.1.2.cmml">2</mn><mrow id="S4.T3.5.2.m2.1.1.3" xref="S4.T3.5.2.m2.1.1.3.cmml"><mi id="S4.T3.5.2.m2.1.1.3.2" xref="S4.T3.5.2.m2.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.T3.5.2.m2.1.1.3.1" xref="S4.T3.5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.T3.5.2.m2.1.1.3.3" xref="S4.T3.5.2.m2.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T3.5.2.m2.1c"><apply id="S4.T3.5.2.m2.1.1.cmml" xref="S4.T3.5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T3.5.2.m2.1.1.1.cmml" xref="S4.T3.5.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.T3.5.2.m2.1.1.2.cmml" xref="S4.T3.5.2.m2.1.1.2">2</cn><apply id="S4.T3.5.2.m2.1.1.3.cmml" xref="S4.T3.5.2.m2.1.1.3"><times id="S4.T3.5.2.m2.1.1.3.1.cmml" xref="S4.T3.5.2.m2.1.1.3.1"></times><ci id="S4.T3.5.2.m2.1.1.3.2.cmml" xref="S4.T3.5.2.m2.1.1.3.2">ğ‘›</ci><ci id="S4.T3.5.2.m2.1.1.3.3.cmml" xref="S4.T3.5.2.m2.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.2.m2.1d">2^{nd}</annotation></semantics></math> highest in column.</span></figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2112.00054/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Performance of Task2Sim vs baselines on 12 seen tasks for 237 class / 100k image pre-training datasets evaluated with full-network finetuning. Best viewed in color.</span></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2112.00054/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Performance of Task2Sim vs baselines on 8 unseen tasks for 237 class / 100k image pre-training datasets evaluated with full-network finetuning. Best viewed in color.</span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Performance on Seen Tasks.</span> <a href="#S4.T2" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> shows accuracies averaged over 12 seen downstream tasks for Task2Sim and all baselines using different evaluation methods for a Resnet-50 backbone. For the last two columns, we included all objects of TDW from 237 categories, and kept the number of images at roughly 400 per class, resulting in about 100k images total, regenerating a new dataset with the simulation parameters corresponding to the different synthetic image generation methods. On average, over the 12 seen tasks, simulation parameters that Task2Sim finds are better than Domain Randomization and Random selection and are competitive with Imagenet pre-training, both for the subset of classes that Task2Sim is trained using, and when a larger set of classes is used. <a href="#S4.F3" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> shows accuracies for the 12 seen datasets for different methods, on the 237 category 100k image pre-training set.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Performance on Unseen Tasks.</span> <a href="#S4.T3" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> shows average downstream accuracy over 8 unseen datasets, of a Resnet-50 pretrained on different datasets.
We see that Task2Sim generalizes well, and is still better than Domain Randomization and Random simulation parameter selection. Moreover, it is marginally better on average than Imagenet pretraining for these tasks. <a href="#S4.F4" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> shows the accuracies from the last column of <a href="#S4.T3" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> over the 8 individual unseen tasks.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Analysis</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Task2Sim Outputs.</span>
<a href="#S4.F5" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> shows the output distribution from the trained Task2Sim model for different seen and unseen tasks. Each output shows the probability assigned by the model to the output 1 in that particular simulation parameter. From the outputs, we see the model determines that in general for the set of tasks considered, it is better to see a single pose of objects rather than multiple poses, and that it is better to have scene lighting intensity variations in different images than have lighting of constant intensity in all images. In general, adding material variability was determined to be worse for most datasets, except for SVHN. Comparing predictions for seen vs unseen tasks, we see that Task2Sim does its best to generalize to unseen tasks by relating them to the seen tasks.
For <em id="S4.SS3.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS3.p1.1.3" class="ltx_text"></span>, outputs for ChestXPneumonia are similar to ChestX, while outputs of CactusAerial are similar to those of EuroSAT, both being aerial/satellite image datasets. A similar trend is also seen in PacsS and Sketch both of which contain hand-sketches, and for CUB and CropDisease, both natural image datasets.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2112.00054/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="249" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Task2Sim outputs for different seen and unseen tasks. Values shown are predicted probability of value 1 in the specific simulation parameters. Best viewed in color.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Another inspection shows Task2Sim makes decisions that are quite logical for certain tasks. For instance, Task2Sim turns off the â€œLight Colorâ€ parameter for CUB. Here, color plays a major role in distinguishing different birds, thus needing a classifier representation that should not be invariant to color changes. Indeed, from <a href="#S4.F9" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">9</span></a>, we see that the neighbors of Task2Sim are of similar colors.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2112.00054/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Avg performance over 12 seen tasks at different number of classes for pre-training. All methods improve performance at similar rates with the addition of more classes.</span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2112.00054/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Avg performance over 12 seen tasks at different number of object meshes used per category for generating synthetic pretraining data. Both methods of synthetic data generation improve performance with addition of more objects with Domain Randomization improving at a slightly higher rate.</span></figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2112.00054/assets/x8.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="250" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Task2Sim performance (avg over 12 seen tasks) vs other methods at different number of images for pretraining. Task2Sim is highly effective at fewer images. Increasing the number of images improves performance for all methods, reaching a saturation at a high enough number. See <a href="#S4.SS3" title="4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.3</span></a> for more discussion.</span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2112.00054/assets/x9.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">3 nearest neighbors of three test examples from the CUB dataset based on different pretrained feature representations (top: Task2Sim, middle: domain randomization, bottom: ImageNet*). Neighbors with a blue box share the same class with the anchor image on the left. For Task2Sim, similar to Imagenet, the neighbors are of similar color which suggests that the pretrained representation captures color similarity which can be crucial for identifying different bird species. Best viewed in color.</span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Effect of Number of Pretraining Classes.</span>
In <a href="#S4.F6" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, we plot the average accuracy with full network finetuning on the 12 seen downstream tasks. On the x-axis, we vary the number of classes used for pre-training, with 1000 images per class on average (200 classes=200k images). We see all pre-training methods improve with more classes (and correspondingly more images) at about similar rates. Task2Sim stays better than Domain Randomization and competitive with (about 2% shy of) pre-training with an equivalent subset (in number of classes and images) of Imagenet.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Effect of Number of Different Objects per Class.</span> In TDW, we have 2322 object meshes from 237 different categories. In <a href="#S4.F7" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">7</span></a>, we vary the number of object meshes used per category. The point right-most on the x-axis has 200k images with all objects used, and moving to the left, the number of images reduces proportionately as a fraction of these objects are used (the number of categories being the same). We find that with increasing number of different objects used for each category, Domain Randomization improves downstream performance at a slightly higher rate than our proposed Task2Sim.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Effect of Number of Pretraining Images.</span> In <a href="#S4.F8" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">8</span></a>, we show average downstream task accuracy, for the 12 seen tasks, with different number of images used for pretraining. All methods, except Imagenet-1K and Scratch, use 237 image categories, with synthetic datasets using all available object models. Imagenet-237 is a subset of Imagenet-1K containing 237 categories that were randomly picked.
We see Task2Sim is highly effective in the regime where fewer images are available for pre-training, and is even slightly better than pre-training with Imagenet at 50k images. It maintains its advantage over non-adaptive pretraining up to a significant extrapolation of 500k images, having only trained using smaller datasets (of 100 classes and 40k images). At 1M images, it is still competitive with Imagenet pre-training and is much better than training from scratch.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">We also observe that all methods improve when more pre-training images are available, although the rate of improvement decreases as we move along positive X-direction.
Initially, Domain Randomization improves at a higher rate than Task2Sim and at 1M pretraining images, matches its performance. This is likely because at a higher number of images, even when there are all variations possible from simulation in each image (corresponding to Domain Randomization), the deep feature extractor grows robust to the variations which may not add any value to the representation for specific downstream tasks.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">Our hypothesis is that at a fixed number of categories there may exist some point in number of pre-training images when the above robustness can be good enough to match our Task2Simâ€™s downstream performance. With a 237-category limit from TDW and using the set of variations from our 8 chosen parameters, 1M images seems to be this point. However as the number of classes increases, this point shifts towards higher number of images. As evidence, consider <a href="#S4.F6" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, where we see that as more classes of objects are added with more data, different methods improve at similar rates. Moving further along positive X, if this holds with more classes, Task2Sim maintains its edge over Domain Randomization even at higher numbers of images. This suggests a non-adaptive pre-training method like Domain Randomization has potential to be as effective on average as Task2Sim, but only at the cost of more pre-training images. However, this cost would keep increasing as pre-training data encompasses more object categories, and would be unknown without experimentation.</p>
</div>
<div id="S4.SS3.p8" class="ltx_para">
<p id="S4.SS3.p8.1" class="ltx_p">For additional results and discussions, we refer readers to the Appendix.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We saw the approach that is optimal for downstream performance in using synthetic data for pre-training is to specifically adapt the synthetic data to different downstream tasks. In this paper, we parameterized our synthetic data via different simulation parameters from graphics engines, and introduced Task2Sim, which learns to map downstream task representations to optimal simulation parameters for synthetic pretraining data for the task. We showed Task2Sim can be trained on a set of â€œseenâ€ tasks and can then generalize to novel â€œunseenâ€ tasks predicting parameters for them in one-shot, making it highly practical for synthetic pre-training data generation. While a large portion of contemporary representation learning research focuses on self-supervision to avoid using labels, we hope our demonstration with Task2Sim motivates further research in using simulated data from graphics engines for this purpose, with focus on adaptive generation for downstream application.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements.</span> This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8750-19-C-1001. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA). This work was also supported by Army Research Office Grant W911NF2110246, National Science Foundation grants CCF-2007350 and CCF-1955981, and the Hariri institute at Boston University. We would also like to thank the developers of TDW: Seth Alter, Abhishek Bhandwaldar and Jeremy Schwartz, for their assistance with the platform and its use.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu
Maji, CharlessÂ C Fowlkes, Stefano Soatto, and Pietro Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Task2vec: Task embedding for meta-learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh,
Dhruv Batra, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Sim-to-real transfer for vision-and-language navigation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Manel Baradad, Jonas Wulff, Tongzhou Wang, Phillip Isola, and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Learning to see by looking at noise.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.05963</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
HarkiratÂ Singh Behl, AtilimÂ GÃ¼neÅŸ Baydin, Ran Gal, PhilipÂ HS Torr,
and Vibhav Vineet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Autosimulate:(quickly) learning synthetic data generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Gong Cheng, Junwei Han, and Xiaoqiang Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Remote sensing image scene classification: Benchmark and state of the
art.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 105(10):1865â€“1883, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
FranÃ§ois Chollet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Xception: Deep learning with depthwise separable convolutions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 1251â€“1258, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Describing textures in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 3606â€“3613, 2014.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Noel Codella, Veronica Rotemberg, Philipp Tschandl, MÂ Emre Celebi, Stephen
Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael
Marchetti, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Skin lesion analysis toward melanoma detection 2018: A challenge
hosted by the international skin imaging collaboration (isic).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1902.03368</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Gabriela Csurka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Domain adaptation for visual applications: A comprehensive survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1702.05374</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
EkinÂ D Cubuk, Barret Zoph, Jonathon Shlens, and QuocÂ V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Randaugment: Practical automated data augmentation with a reduced
search space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 702â€“703, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 248â€“255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Jeevan Devaranjan, Amlan Kar, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Meta-sim2: Unsupervised learning of scene structure for synthetic
data generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian
DeÂ Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano,
etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Threedworld: A platform for interactive multi-modal physical
simulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS, Datasets Track</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SMÂ Ali Eslami, and Oriol
Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Synthesizing programs for images using reinforced adversarial
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, FranÃ§ois Laviolette, Mario Marchand, and Victor Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Domain-adversarial training of neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The Journal of Machine Learning Research</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 17(1):2096â€“2030,
2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and
Rogerio Feris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Spottune: transfer learning through adaptive fine-tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Dynamic neural networks: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2102.04906</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 770â€“778, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Eurosat: A novel dataset and deep learning benchmark for land use and
land cover classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 12(7):2217â€“2226, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Distilling the knowledge in a neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1503.02531</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
Saenko, Alexei Efros, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Cycada: Cycle-consistent adversarial domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
JonathanÂ J. Hull.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">A database for handwritten text recognition research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on pattern analysis and machine intelligence</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">,
16(5):550â€“554, 1994.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky, Richard Radke,
and Rogerio Feris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">A broad study on the transferability of visual representations with
contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.13517</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens Van DerÂ Maaten, Li Fei-Fei, C
LawrenceÂ Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt
Rusiniak, David Acuna, Antonio Torralba, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Meta-sim: Learning to generate synthetic datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
DanielÂ S Kermany, Michael Goldbaum, Wenjia Cai, CarolinaÂ CS Valentim, Huiying
Liang, SallyÂ L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan,
etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Identifying medical diagnoses and treatable diseases by image-based
deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Cell</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 172(5):1122â€“1131, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Rawal Khirodkar, Donghyun Yoo, and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Domain randomization for scene-specific car detection and pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">WACV</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
SeungÂ Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Drivegan: Towards a controllable high-quality neural simulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 5820â€“5829, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro
Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Ai2-thor: An interactive 3d environment for visual ai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1712.05474</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Similarity of neural network representations revisited.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages
3519â€“3529. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
BrendenÂ M Lake, Ruslan Salakhutdinov, and JoshuaÂ B Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Human-level concept learning through probabilistic program induction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Science</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 350(6266):1332â€“1338, 2015.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
John Langford and Tong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Epoch-greedy algorithm for multi-armed bandits with side information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NIPS 2007)</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">,
20:1, 2007.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Da Li, Yongxin Yang, Yi-Zhe Song, and TimothyÂ M Hospedales.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Deeper, broader and artier domain generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 5542â€“5550, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
JamesÂ J Little and Alessandro Verri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Analysis of differential and matching methods for optical flow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">1988.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Efren LÃ³pez-JimÃ©nez, JuanÂ Irving Vasquez-Gomez, MiguelÂ Angel
Sanchez-Acevedo, JuanÂ Carlos Herrera-Lozada, and AbrilÂ Valeria Uriarte-Arcia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Columnar cactus recognition in aerial images using a deep learning
approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Ecological Informatics</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 52:131â€“138, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Gilles Louppe, Joeri Hermans, and Kyle Cranmer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Adversarial variational optimization of non-differentiable
simulators.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AISTATS</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van DerÂ Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Exploring the limits of weakly supervised pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 181â€“196, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid
Karlinsky, Aude Oliva, Kate Saenko, and Rogerio Feris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Ar-net: Adaptive frame resolution for efficient action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Hiroaki Mikami, Kenji Fukumizu, Shogo Murai, Shuji Suzuki, Yuta Kikuchi, Taiji
Suzuki, Shin-ichi Maeda, and Kohei Hayashi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">A scaling law for synthetic-to-real transfer: How much is your
pre-training effective?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.11018</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
SharadaÂ P Mohanty, DavidÂ P Hughes, and Marcel SalathÃ©.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Using deep learning for image-based plant disease detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Frontiers in plant science</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 7:1419, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Reading digits in natural images with unsupervised feature learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS Workshop on Deep Learning and Unsupervised Feature Learning
2011</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, pages 722â€“729, 2011.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Ramakant Nevatia and ThomasÂ O Binford.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Description and recognition of curved objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artificial intelligence</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 8(1):77â€“98, 1977.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Maria-Elena Nilsback and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Automated flower classification over a large number of classes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2008 Sixth Indian Conference on Computer Vision, Graphics &amp;
Image Processing</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 722â€“729. IEEE, 2008.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Self-imitation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages
3878â€“3887. PMLR, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Alex Olsen, DmitryÂ A. Konovalov, Bronson Philippa, Peter Ridd, JakeÂ C. Wood,
Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James Whinney,
Brendan Calvert, Mostafa Rahimi Azghadi, and RonaldÂ D. White.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">DeepWeeds: A Multiclass Weed Species Image Dataset for Deep
Learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Scientific Reports</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 9(2058), 2 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Learning deep object detectors from 3d models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci,
Gavriel State, Omer Shapira, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Structured domain randomization: Bridging the reality gap by
context-aware synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages
8748â€“8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Zhongzheng Ren and YongÂ Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Cross-domain self-supervised multi-task feature learning using
synthetic imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
StevenÂ J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava
Goel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Self-critical sequence training for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 7008â€“7024, 2017.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
StephanÂ R Richter, HassanÂ Abu AlHaija, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Enhancing photorealism enhancement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2105.04619</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
StephanÂ R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 102â€“118.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Cesar RobertoÂ de Souza, Adrien Gaidon, Yohann Cabon, and Antonio ManuelÂ Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Procedural generation of videos to train deep action recognition
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and AntonioÂ M
Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Artem Rozantsev, Mathieu Salzmann, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Beyond sharing weights for deep domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">,
41(4), 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Nataniel Ruiz, Samuel Schulter, and Manmohan Chandraker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Learning to simulate.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans,
Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Habitat: A platform for embodied ai research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang,
and Russell Webb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Learning from simulated and unsupervised images through adversarial
training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Hao Su, CharlesÂ R Qi, Yangyan Li, and LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Render for cnn: Viewpoint estimation in images using cnns trained
with rendered 3d model views.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages 2686â€“2694, 2015.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Revisiting unreasonable effectiveness of data in deep learning era.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, pages 843â€“852, 2017.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Adashare: Learning what to share for efficient deep multi-task
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Yingtao Tian, Chikahiko Suzuki, Tarin Clanuwat, Mikel Bober-Irizar, Alex Lamb,
and Asanobu Kitamoto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Kaokore: A pre-modern japanese art facial expression dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.08595</span><span id="bib.bib62.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Domain randomization for transferring deep neural networks from
simulation to the real world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Adversarial discriminative domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
GÃ¼l Varol, Ivan Laptev, Cordelia Schmid, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Synthetic humans for action recognition from unseen viewpoints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib65.4.2" class="ltx_text" style="font-size:90%;">, 129(7):2264â€“2287,
2021.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Andreas Veit and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Convolutional networks with adaptive inference graphs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">The Caltech-UCSD Birds-200-2011 Dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">Technical report, 2011.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Haohan Wang, Songwei Ge, EricÂ P. Xing, and ZacharyÂ C. Lipton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Learning robust global representations by penalizing local predictive
power.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.13549</span><span id="bib.bib68.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and
RonaldÂ M Summers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax diseases.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib69.5.3" class="ltx_text" style="font-size:90%;">, pages 2097â€“2106, 2017.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and JosephÂ E Gonzalez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Skipnet: Learning dynamic routing in convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu,
ThomasÂ S Huang, and Honghui Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Differential treatment for stuff and things: A simple unsupervised
domain adaptation method for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib71.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Ross Wightman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Pytorch image models.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/rwightman/pytorch-image-models" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/rwightman/pytorch-image-models</a><span id="bib.bib72.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
RonaldÂ J Williams.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Simple statistical gradient-following algorithms for connectionist
reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Machine learning</span><span id="bib.bib73.4.2" class="ltx_text" style="font-size:90%;">, 8(3):229â€“256, 1992.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, LarryÂ S Davis,
Kristen Grauman, and Rogerio Feris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Blockdrop: Dynamic inference paths in residual networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and LarryÂ S Davis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">Adaframe: Adaptive frame selection for fast video recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib75.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib75.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
Fei Xia, AmirÂ R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio
Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.2.1" class="ltx_text" style="font-size:90%;">Gibson env: Real-world perception for embodied agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib76.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib76.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong,
Liangpei Zhang, and Xiaoqiang Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.2.1" class="ltx_text" style="font-size:90%;">Aid: A benchmark data set for performance evaluation of aerial scene
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</span><span id="bib.bib77.4.2" class="ltx_text" style="font-size:90%;">,
55(7):3965â€“3981, 2017.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Dawei Yang and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.2.1" class="ltx_text" style="font-size:90%;">Learning to generate 3d training data through hybrid gradient.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib78.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib78.5.3" class="ltx_text" style="font-size:90%;">, pages 779â€“789, 2020.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt
Keutzer, and Boqing Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.2.1" class="ltx_text" style="font-size:90%;">Domain randomization and pyramid consistency: Simulation-to-real
generalization without accessing target domain data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib79.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib79.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
Sangdoo Yun, Dongyoon Han, SeongÂ Joon Oh, Sanghyuk Chun, Junsuk Choe, and
Youngjoon Yoo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.2.1" class="ltx_text" style="font-size:90%;">Cutmix: Regularization strategy to train strong classifiers with
localizable features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib80.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib80.5.3" class="ltx_text" style="font-size:90%;">, pages 6023â€“6032, 2019.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text" style="font-size:90%;">
Hongyi Zhang, Moustapha Cisse, YannÂ N Dauphin, and David Lopez-Paz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.2.1" class="ltx_text" style="font-size:90%;">mixup: Beyond empirical risk minimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.09412</span><span id="bib.bib81.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text" style="font-size:90%;">
Yang Zhang, Philip David, Hassan Foroosh, and Boqing Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.2.1" class="ltx_text" style="font-size:90%;">A curriculum domain adaptation approach to the semantic segmentation
of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib82.4.2" class="ltx_text" style="font-size:90%;">,
42(8):1823â€“1841, 2019.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text" style="font-size:90%;">
Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith,
and Scott Howard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.2.1" class="ltx_text" style="font-size:90%;">A poisson-gaussian denoising dataset with real fluorescence
microscopy images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib83.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib83.5.3" class="ltx_text" style="font-size:90%;">, pages 11710â€“11718, 2019.
</span>
</span>
</li>
</ul>
</section>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendices</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Task2Vec</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We used Task2VecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> representations for downstream tasks for our Task2Sim model. Task2Vec of a task consists of diagonal elements of the Fisher information matrix (FIM) of the outputs with respect to the network parameters over the distribution of downstream task examples (Refer to Section 2 of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> for more details). For this purpose, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we used a single Imagenet pre-trained probe network with only the classifier layer trained on specific tasks (using the training set of examples for that task). In our experiments, a Resnet-18 probe network was used, resulting in a 9600-dimensional Task2Vec task representation.</p>
</div>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">How much downstream data do we need access to?</span> In the case of models pre-trained using an approach that is not task-adaptive, there is no need to access any downstream data while pre-training. Given that task-adaptive approaches need a downstream task representation we used Task2Vec. Here, followingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> we used all labeled examples from the training set of the downstream task to represent its distribution (in computing the FIM). However, we show that the FIM can be estimated by using fewer examples from the downstream task and the resulting Task2Vec vectors can be used to train Task2Sim with no degradation in performance (see <a href="#A1.T4" title="In Appendix A Task2Vec â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>). This property also makes Task2Sim more practical since a user need not wait to collect labels for all data pertaining to their downstream application in order to generate pre-training data using Task2Sim.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<table id="A1.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T4.2.1.1" class="ltx_tr">
<th id="A1.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" rowspan="2"><span id="A1.T4.2.1.1.1.1" class="ltx_text"><span id="A1.T4.2.1.1.1.1.1" class="ltx_text"></span> <span id="A1.T4.2.1.1.1.1.2" class="ltx_text">
<span id="A1.T4.2.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T4.2.1.1.1.1.2.1.1" class="ltx_tr">
<span id="A1.T4.2.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A1.T4.2.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Fraction of data</span></span></span>
<span id="A1.T4.2.1.1.1.1.2.1.2" class="ltx_tr">
<span id="A1.T4.2.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A1.T4.2.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">used for Task2Vec</span></span></span>
</span></span> <span id="A1.T4.2.1.1.1.1.3" class="ltx_text"></span></span></th>
<td id="A1.T4.2.1.1.2" class="ltx_td ltx_align_center" colspan="2"><span id="A1.T4.2.1.1.2.1" class="ltx_text ltx_font_bold">Avg Downstream Acc.</span></td>
</tr>
<tr id="A1.T4.2.2.2" class="ltx_tr">
<td id="A1.T4.2.2.2.1" class="ltx_td ltx_align_center">Seen Tasks</td>
<td id="A1.T4.2.2.2.2" class="ltx_td ltx_align_center">Unseen Tasks</td>
</tr>
<tr id="A1.T4.2.3.3" class="ltx_tr">
<td id="A1.T4.2.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100%</td>
<td id="A1.T4.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t">30.46</td>
<td id="A1.T4.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t">53.06</td>
</tr>
<tr id="A1.T4.2.4.4" class="ltx_tr">
<td id="A1.T4.2.4.4.1" class="ltx_td ltx_align_center ltx_border_r">50%</td>
<td id="A1.T4.2.4.4.2" class="ltx_td ltx_align_center">30.69</td>
<td id="A1.T4.2.4.4.3" class="ltx_td ltx_align_center">52.70</td>
</tr>
<tr id="A1.T4.2.5.5" class="ltx_tr">
<td id="A1.T4.2.5.5.1" class="ltx_td ltx_align_center ltx_border_r">20%</td>
<td id="A1.T4.2.5.5.2" class="ltx_td ltx_align_center">30.72</td>
<td id="A1.T4.2.5.5.3" class="ltx_td ltx_align_center">53.11</td>
</tr>
<tr id="A1.T4.2.6.6" class="ltx_tr">
<td id="A1.T4.2.6.6.1" class="ltx_td ltx_align_center ltx_border_r">10%</td>
<td id="A1.T4.2.6.6.2" class="ltx_td ltx_align_center">31.18</td>
<td id="A1.T4.2.6.6.3" class="ltx_td ltx_align_center">53.57</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A1.T4.4.2" class="ltx_text" style="font-size:90%;">Average downstream performance (evaluated with 5NN classifier and using 40k images from 100 classes for pre-training) over seen and unseen tasks using different fractions of downstream training data (randomly subsampled) used to compute Task2Vec task representations for Task2Sim model. Task2Sim performance does not degrade when fewer downstream examples are used for computing Task2Vec.</span></figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Similarity between Learned Features</h2>

<figure id="A2.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F10.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x10.png" id="A2.F10.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="126" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F10.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x11.png" id="A2.F10.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="126" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F10.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x12.png" id="A2.F10.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="126" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F10.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x13.png" id="A2.F10.4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="126" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F10.6.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A2.F10.7.2" class="ltx_text" style="font-size:90%;">CKA similarities between features from backbones trained on different pre-training datasets (with 100k images from 237 classes). Similarities have been computed using features output at different stages of the Resnet-50 model. We notice that features at earlier stages across all methods of pre-training are quite similar and only later in the Resnet, do they start differentiating. We also observe that Task2Simâ€™s features are more similar to Imagenet than those produced by pre-training with Domain Randomization.</span></figcaption>
</figure>
<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We used centered kernel alignment (CKA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> to find the similarity between features learned by the Resnet-50 backbone pre-trained on different image sets containing 100k images from 237 classes. <a href="#A2.F10" title="In Appendix B Similarity between Learned Features â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">10</span></a>, shows these similarities computed using the output features at different stages of the backbone (Stages 1-4 are intermediate outputs after different convolutional blocks in the resnet).</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">A few interesting phenomena surface: Task2Sim features (<em id="A2.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="A2.p2.1.2" class="ltx_text"></span> features produced by a model pre-trained on Task2Sim generated dataset) are more similar to Imagenet features, than Domain Randomization. Thus Task2Sim in some manner, mimics features learned on real images better.
We can also see that features early on in the network are largely similar across all kinds of pre-training and they only start differentiating at later stages, suggesting high similarity in lower level features (<em id="A2.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A2.p2.1.4" class="ltx_text"></span> edges, curves, textures, <em id="A2.p2.1.5" class="ltx_emph ltx_font_italic">etc</em>.<span id="A2.p2.1.6" class="ltx_text"></span>) across different pre-training datasets. Also, as might be expected, features post downstream finetuning are more similar to each other than before, while still quite far away from being identical.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Results</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Effect of Different Backbones</h3>

<figure id="A3.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F11.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x14.png" id="A3.F11.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="251" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F11.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x15.png" id="A3.F11.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="251" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F11.4.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A3.F11.5.2" class="ltx_text" style="font-size:90%;">Effect of different backbones on average seen task performance (237 classes, 100k pre-training images). Best viewed in color.</span></figcaption>
</figure>
<figure id="A3.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F12.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x16.png" id="A3.F12.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="251" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F12.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x17.png" id="A3.F12.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="245" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F12.4.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A3.F12.5.2" class="ltx_text" style="font-size:90%;">Effect of different backbones on average unseen task performance (237 classes, 100k pre-training images). Best viewed in color.</span></figcaption>
</figure>
<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">In <a href="#A3.F11" title="In C.1 Effect of Different Backbones â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span>Â <span class="ltx_text ltx_ref_tag">11</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span> andÂ <a href="#A3.F12" title="Figure 12 â€£ C.1 Effect of Different Backbones â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, we show the average downstream performance over the seen and unseen tasks respectively, using different Resnet backbones (of different sizes). For this study, we used the same pre-training procedure across all backbones. We see that results are largely consistent with different backbones and for all of them Task2Sim performance is competitive with Imagenet pre-training and is much better than Domain Randomization. We also see that typically methods improve average downstream performance with the use of a larger backbone in the classifier. Moving from Resnet-50 to Resnet-101, Task2Sim performance breaks this trend and is lower indicating that the larger backbone could overfit in this case. This might be expected since Task2Sim was trained to optimize the performance of a Resnet-50 backbone.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Task2Sim Resultsâ€”Linear Probing</h3>

<figure id="A3.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F13.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x18.png" id="A3.F13.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="291" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F13.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x19.png" id="A3.F13.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="287" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F13.4.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A3.F13.5.2" class="ltx_text" style="font-size:90%;">Performance of Task2Sim vs baselines on 12 seen tasks and 8 unseen tasks for 237 class / 100k image pre-training datasets evaluated with linear probing. Best viewed in color.</span></figcaption>
</figure>
<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p"><a href="#A3.F13" title="In C.2 Task2Sim Resultsâ€”Linear Probing â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">13</span></a> shows the downstream accuracy with linear probing for different seen and unseen datasets where pre-training dataset has 100k images from all 237 classes. These complement <a href="#S4.F3" title="In 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span> andÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Task2Sim Results â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where downstream evaluation used full network finetuning.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Varying Pre-training Data Size</h3>

<figure id="A3.F14" class="ltx_figure"><img src="/html/2112.00054/assets/x20.png" id="A3.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="A3.F14.3.2" class="ltx_text" style="font-size:90%;">Avg performance with linear probing over 12 seen tasks at different number of classes for pre-training. All methods improve performance at similar rates with the addition of more classes.</span></figcaption>
</figure>
<figure id="A3.F15" class="ltx_figure"><img src="/html/2112.00054/assets/x21.png" id="A3.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="A3.F15.3.2" class="ltx_text" style="font-size:90%;">Avg performance with linear probing over 12 seen tasks at different number of object meshes used per category for generating synthetic pretraining data. Both methods of synthetic data generation improve performance with addition of more objects with Domain Randomization improving at a slightly higher rate.</span></figcaption>
</figure>
<figure id="A3.F16" class="ltx_figure"><img src="/html/2112.00054/assets/x22.png" id="A3.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F16.2.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="A3.F16.3.2" class="ltx_text" style="font-size:90%;">Task2Sim performance (avg over 12 seen tasks) vs other methods using linear probing for evaluation at different number of images for pretraining. Task2Sim is highly effective at fewer images. Increasing the number of images improves performance for all methods. Towards higher number of images in the case of linear probing we see methods not only reach a saturation but also exhibit some overfitting to pre-training data. Also, Domain Randomization stops improving in this case (evaluation with linear probing) before it can match Task2Sim performance.</span></figcaption>
</figure>
<div id="A3.SS3.p1" class="ltx_para ltx_noindent">
<p id="A3.SS3.p1.1" class="ltx_p"><span id="A3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Linear Probing.</span> <a href="#A3.F14" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span>Â <span class="ltx_text ltx_ref_tag">14</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span>, <a href="#A3.F15" title="Figure 15 â€£ C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span> andÂ <a href="#A3.F16" title="Figure 16 â€£ C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> are counterparts (with downstream evaluation done with linear probing) of <a href="#S4.F6" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span>, <a href="#S4.F7" title="Figure 7 â€£ 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span> andÂ <a href="#S4.F8" title="Figure 8 â€£ 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> respectively. We see that primarily similar findings as the main paper hold and in <a href="#A3.F14" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">14</span></a>, different backbones improve at a similar rate with more classes (and images for pre-training). In <a href="#A3.F15" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">15</span></a>, we see that both methods of synthetic pre-training improve their features with more object models, with Domain Randomization improving at a slightly higher rate.</p>
</div>
<div id="A3.SS3.p2" class="ltx_para">
<p id="A3.SS3.p2.1" class="ltx_p">In <a href="#A3.F16" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">16</span></a> we see some differences: There is a more severe saturating behavior of downstream performance, which even decreases by a little after a certain point for the synthetic pre-training data. This is likely because the feature extractor overfits to the pre-training task and a linear classifier on these features cannot perform as well. Both from <a href="#A3.F14" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">14</span></a> and from the curve for Imagenet-1K in <a href="#A3.F16" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">16</span></a> we see that this saturating/overfitting behavior is somewhat alleviated by more classes in pre-training data. Another observation of note in <a href="#A3.F16" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">16</span></a> is that the feature extractor pre-trained on Domain Randomization starts overfitting <em id="A3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">before</em> it matches the performance of Task2Sim.
With <a href="#S4.F8" title="In 4.3 Analysis â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">8</span></a>, we mentioned that with more images a non-adaptive approach like domain randomization could improve its performance faster and sometimes equal a task-adaptive approach like Task2Sim. <a href="#A3.F16" title="In C.3 Varying Pre-training Data Size â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">16</span></a> shows that although a non-adaptive approach may improve faster, it may not always match performance of its adaptive counterpart.</p>
</div>
<div id="A3.SS3.p3" class="ltx_para ltx_noindent">
<p id="A3.SS3.p3.1" class="ltx_p"><span id="A3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Unseen Tasks.</span> <a href="#A6.F18" title="In Appendix F Details of Downstream Tasks â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span>Â <span class="ltx_text ltx_ref_tag">18</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span>, <a href="#A6.F19" title="Figure 19 â€£ Appendix F Details of Downstream Tasks â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:</span> andÂ <a href="#A6.F20" title="Figure 20 â€£ Appendix F Details of Downstream Tasks â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> show effect of above variations averaged over unseen tasks. We can see that similar trends hold in this case, as in case of seen datasets.</p>
</div>
</section>
<section id="A3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Comparison with Large scale Pre-training (CLIP)</h3>

<div id="A3.SS4.p1" class="ltx_para">
<p id="A3.SS4.p1.1" class="ltx_p">CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> pre-trains on Â 400M image-text pairs. Such large datasets when curated from the web, are bound to have privacy and other ethical concerns, as discussed in the paper. CLIP pre-training is also much more expensive than its counterparts using our synthetic data. We conducted an experiment finetuning a Resnet-50 model using pre-trained weights from CLIP on our tasks, while noting that this CLIP pre-trained Resnet-50 is different from the standard model used by us and uses more parameters (38M in CLIP Resnet50 vs 25M in standard Resnet50). The result was 77.33% avg. accuracy on seen tasks and 91.56% avg. accuracy on unseen tasks, which is comparable to the best Task2Sim performance (79.10% over seen tasks and 91.50% over unseen tasks).</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Synthetic Image Generation</h2>

<figure id="A4.F17" class="ltx_figure"><img src="/html/2112.00054/assets/x23.png" id="A4.F17.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="315" height="433" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F17.2.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="A4.F17.3.2" class="ltx_text" style="font-size:90%;">Examples of variations using different simulation parameters. Best viewed in color and under zoom.</span></figcaption>
</figure>
<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We used Three-D-World (TDW)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for synthetic image generation. It is a platform built using the Unity3D engine, and besides a python interface, provides asset bundles which include 3D object models, interactive 3D scenes, and HDRI skyboxes (360<sup id="A4.p1.1.1" class="ltx_sup">âˆ˜</sup> images of real scenes accompanied with lighting information). TDW is available under a BSD 2-Clause â€Simplifiedâ€ License.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.2" class="ltx_p">For our implementation, we used all 2322 object models from 237 different classes available in TDW. We use a generator that imports one object into a simple scene with an HDRI-skybox background. It then, changes different properties of the scene/object/camera based on 8 simulation parameters as mentioned in <a href="#S4.SS1" title="4.1 Details â€£ 4 Experiments â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.1</span></a>. Whenever different variations corresponding to a simulation parameter are to be included, values are chosen uniformly at random within an appropriate range (via a careful choice of the extremes). <a href="#A4.F17" title="In Appendix D Synthetic Image Generation â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">17</span></a> has 8 rows corresponding to each of the simulation parameters used for Task2Sim. Each row shows using 5 images, the variations corresponding to its specific simulation parameter.
Generating 1M images using our generator with all 2322 objects, takes around 12 hours on an Nvidia Tesla-V100 GPU. Given the number of objects we used in our implementation, a bottleneck in image generation is the speed of loading object meshes into Unity3D. Hence, we used a subset of 780 objects from 100 classes with relatively simpler meshes, for generating the data used for training Task2Sim. The 8 parameters we used result in a total of <math id="A4.p2.1.m1.1" class="ltx_Math" alttext="2^{8}=256" display="inline"><semantics id="A4.p2.1.m1.1a"><mrow id="A4.p2.1.m1.1.1" xref="A4.p2.1.m1.1.1.cmml"><msup id="A4.p2.1.m1.1.1.2" xref="A4.p2.1.m1.1.1.2.cmml"><mn id="A4.p2.1.m1.1.1.2.2" xref="A4.p2.1.m1.1.1.2.2.cmml">2</mn><mn id="A4.p2.1.m1.1.1.2.3" xref="A4.p2.1.m1.1.1.2.3.cmml">8</mn></msup><mo id="A4.p2.1.m1.1.1.1" xref="A4.p2.1.m1.1.1.1.cmml">=</mo><mn id="A4.p2.1.m1.1.1.3" xref="A4.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p2.1.m1.1b"><apply id="A4.p2.1.m1.1.1.cmml" xref="A4.p2.1.m1.1.1"><eq id="A4.p2.1.m1.1.1.1.cmml" xref="A4.p2.1.m1.1.1.1"></eq><apply id="A4.p2.1.m1.1.1.2.cmml" xref="A4.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A4.p2.1.m1.1.1.2.1.cmml" xref="A4.p2.1.m1.1.1.2">superscript</csymbol><cn type="integer" id="A4.p2.1.m1.1.1.2.2.cmml" xref="A4.p2.1.m1.1.1.2.2">2</cn><cn type="integer" id="A4.p2.1.m1.1.1.2.3.cmml" xref="A4.p2.1.m1.1.1.2.3">8</cn></apply><cn type="integer" id="A4.p2.1.m1.1.1.3.cmml" xref="A4.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.1.m1.1c">2^{8}=256</annotation></semantics></math> different possibilities and so we pre-generated these 256 sets of 40k images each for faster and smoother training of the Task2Sim model. Each of these 256 sets took <math id="A4.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A4.p2.2.m2.1a"><mo id="A4.p2.2.m2.1.1" xref="A4.p2.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="A4.p2.2.m2.1b"><csymbol cd="latexml" id="A4.p2.2.m2.1.1.cmml" xref="A4.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.2.m2.1c">\sim</annotation></semantics></math>30 mins to generate on a Tesla-V100 GPU.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Training and Evaluation</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">We based our implementation of different classifiers for pre-training and downstream evaluation on pytorch-image-modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. For all experiments except those in <a href="#A3.SS1" title="C.1 Effect of Different Backbones â€£ Appendix C Additional Results â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">C.1</span></a>, we used a Resnet-50 backbone for our classifier. For all datasets while pre-training, we used the following parameters: we trained for 100 epochs using an AdamW optimizer, using a learning rate 0.001 and a batch size of 1024. The learning rate used a linear warmup for 20 epochs and a cosine annealing schedule following warmup. We use regularization methods like label-smoothing, cutmixÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> and mixupÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> following a training strategy from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. We used image augmentation in the form of RandAugmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> while pre-training.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.3" class="ltx_p">For downstream evaluation, we followed a procedure similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. For both evaluations using linear probing and full-network finetuning, we used 50 epochs of training using an SGD optimizer with learning rate decayed by a tenth at 25 and 37 epochs. No additional regularizers or data augmentation approaches were used. For each downstream task, we did a coarse hyperparameter grid-search over learning rate <math id="A5.p2.1.m1.4" class="ltx_Math" alttext="\in\{10^{-5},10^{-4},10^{-3},10^{-2}\}" display="inline"><semantics id="A5.p2.1.m1.4a"><mrow id="A5.p2.1.m1.4.4" xref="A5.p2.1.m1.4.4.cmml"><mi id="A5.p2.1.m1.4.4.6" xref="A5.p2.1.m1.4.4.6.cmml"></mi><mo id="A5.p2.1.m1.4.4.5" xref="A5.p2.1.m1.4.4.5.cmml">âˆˆ</mo><mrow id="A5.p2.1.m1.4.4.4.4" xref="A5.p2.1.m1.4.4.4.5.cmml"><mo stretchy="false" id="A5.p2.1.m1.4.4.4.4.5" xref="A5.p2.1.m1.4.4.4.5.cmml">{</mo><msup id="A5.p2.1.m1.1.1.1.1.1" xref="A5.p2.1.m1.1.1.1.1.1.cmml"><mn id="A5.p2.1.m1.1.1.1.1.1.2" xref="A5.p2.1.m1.1.1.1.1.1.2.cmml">10</mn><mrow id="A5.p2.1.m1.1.1.1.1.1.3" xref="A5.p2.1.m1.1.1.1.1.1.3.cmml"><mo id="A5.p2.1.m1.1.1.1.1.1.3a" xref="A5.p2.1.m1.1.1.1.1.1.3.cmml">âˆ’</mo><mn id="A5.p2.1.m1.1.1.1.1.1.3.2" xref="A5.p2.1.m1.1.1.1.1.1.3.2.cmml">5</mn></mrow></msup><mo id="A5.p2.1.m1.4.4.4.4.6" xref="A5.p2.1.m1.4.4.4.5.cmml">,</mo><msup id="A5.p2.1.m1.2.2.2.2.2" xref="A5.p2.1.m1.2.2.2.2.2.cmml"><mn id="A5.p2.1.m1.2.2.2.2.2.2" xref="A5.p2.1.m1.2.2.2.2.2.2.cmml">10</mn><mrow id="A5.p2.1.m1.2.2.2.2.2.3" xref="A5.p2.1.m1.2.2.2.2.2.3.cmml"><mo id="A5.p2.1.m1.2.2.2.2.2.3a" xref="A5.p2.1.m1.2.2.2.2.2.3.cmml">âˆ’</mo><mn id="A5.p2.1.m1.2.2.2.2.2.3.2" xref="A5.p2.1.m1.2.2.2.2.2.3.2.cmml">4</mn></mrow></msup><mo id="A5.p2.1.m1.4.4.4.4.7" xref="A5.p2.1.m1.4.4.4.5.cmml">,</mo><msup id="A5.p2.1.m1.3.3.3.3.3" xref="A5.p2.1.m1.3.3.3.3.3.cmml"><mn id="A5.p2.1.m1.3.3.3.3.3.2" xref="A5.p2.1.m1.3.3.3.3.3.2.cmml">10</mn><mrow id="A5.p2.1.m1.3.3.3.3.3.3" xref="A5.p2.1.m1.3.3.3.3.3.3.cmml"><mo id="A5.p2.1.m1.3.3.3.3.3.3a" xref="A5.p2.1.m1.3.3.3.3.3.3.cmml">âˆ’</mo><mn id="A5.p2.1.m1.3.3.3.3.3.3.2" xref="A5.p2.1.m1.3.3.3.3.3.3.2.cmml">3</mn></mrow></msup><mo id="A5.p2.1.m1.4.4.4.4.8" xref="A5.p2.1.m1.4.4.4.5.cmml">,</mo><msup id="A5.p2.1.m1.4.4.4.4.4" xref="A5.p2.1.m1.4.4.4.4.4.cmml"><mn id="A5.p2.1.m1.4.4.4.4.4.2" xref="A5.p2.1.m1.4.4.4.4.4.2.cmml">10</mn><mrow id="A5.p2.1.m1.4.4.4.4.4.3" xref="A5.p2.1.m1.4.4.4.4.4.3.cmml"><mo id="A5.p2.1.m1.4.4.4.4.4.3a" xref="A5.p2.1.m1.4.4.4.4.4.3.cmml">âˆ’</mo><mn id="A5.p2.1.m1.4.4.4.4.4.3.2" xref="A5.p2.1.m1.4.4.4.4.4.3.2.cmml">2</mn></mrow></msup><mo stretchy="false" id="A5.p2.1.m1.4.4.4.4.9" xref="A5.p2.1.m1.4.4.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p2.1.m1.4b"><apply id="A5.p2.1.m1.4.4.cmml" xref="A5.p2.1.m1.4.4"><in id="A5.p2.1.m1.4.4.5.cmml" xref="A5.p2.1.m1.4.4.5"></in><csymbol cd="latexml" id="A5.p2.1.m1.4.4.6.cmml" xref="A5.p2.1.m1.4.4.6">absent</csymbol><set id="A5.p2.1.m1.4.4.4.5.cmml" xref="A5.p2.1.m1.4.4.4.4"><apply id="A5.p2.1.m1.1.1.1.1.1.cmml" xref="A5.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="A5.p2.1.m1.1.1.1.1.1.1.cmml" xref="A5.p2.1.m1.1.1.1.1.1">superscript</csymbol><cn type="integer" id="A5.p2.1.m1.1.1.1.1.1.2.cmml" xref="A5.p2.1.m1.1.1.1.1.1.2">10</cn><apply id="A5.p2.1.m1.1.1.1.1.1.3.cmml" xref="A5.p2.1.m1.1.1.1.1.1.3"><minus id="A5.p2.1.m1.1.1.1.1.1.3.1.cmml" xref="A5.p2.1.m1.1.1.1.1.1.3"></minus><cn type="integer" id="A5.p2.1.m1.1.1.1.1.1.3.2.cmml" xref="A5.p2.1.m1.1.1.1.1.1.3.2">5</cn></apply></apply><apply id="A5.p2.1.m1.2.2.2.2.2.cmml" xref="A5.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="A5.p2.1.m1.2.2.2.2.2.1.cmml" xref="A5.p2.1.m1.2.2.2.2.2">superscript</csymbol><cn type="integer" id="A5.p2.1.m1.2.2.2.2.2.2.cmml" xref="A5.p2.1.m1.2.2.2.2.2.2">10</cn><apply id="A5.p2.1.m1.2.2.2.2.2.3.cmml" xref="A5.p2.1.m1.2.2.2.2.2.3"><minus id="A5.p2.1.m1.2.2.2.2.2.3.1.cmml" xref="A5.p2.1.m1.2.2.2.2.2.3"></minus><cn type="integer" id="A5.p2.1.m1.2.2.2.2.2.3.2.cmml" xref="A5.p2.1.m1.2.2.2.2.2.3.2">4</cn></apply></apply><apply id="A5.p2.1.m1.3.3.3.3.3.cmml" xref="A5.p2.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="A5.p2.1.m1.3.3.3.3.3.1.cmml" xref="A5.p2.1.m1.3.3.3.3.3">superscript</csymbol><cn type="integer" id="A5.p2.1.m1.3.3.3.3.3.2.cmml" xref="A5.p2.1.m1.3.3.3.3.3.2">10</cn><apply id="A5.p2.1.m1.3.3.3.3.3.3.cmml" xref="A5.p2.1.m1.3.3.3.3.3.3"><minus id="A5.p2.1.m1.3.3.3.3.3.3.1.cmml" xref="A5.p2.1.m1.3.3.3.3.3.3"></minus><cn type="integer" id="A5.p2.1.m1.3.3.3.3.3.3.2.cmml" xref="A5.p2.1.m1.3.3.3.3.3.3.2">3</cn></apply></apply><apply id="A5.p2.1.m1.4.4.4.4.4.cmml" xref="A5.p2.1.m1.4.4.4.4.4"><csymbol cd="ambiguous" id="A5.p2.1.m1.4.4.4.4.4.1.cmml" xref="A5.p2.1.m1.4.4.4.4.4">superscript</csymbol><cn type="integer" id="A5.p2.1.m1.4.4.4.4.4.2.cmml" xref="A5.p2.1.m1.4.4.4.4.4.2">10</cn><apply id="A5.p2.1.m1.4.4.4.4.4.3.cmml" xref="A5.p2.1.m1.4.4.4.4.4.3"><minus id="A5.p2.1.m1.4.4.4.4.4.3.1.cmml" xref="A5.p2.1.m1.4.4.4.4.4.3"></minus><cn type="integer" id="A5.p2.1.m1.4.4.4.4.4.3.2.cmml" xref="A5.p2.1.m1.4.4.4.4.4.3.2">2</cn></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.1.m1.4c">\in\{10^{-5},10^{-4},10^{-3},10^{-2}\}</annotation></semantics></math>, optimizer weight decay <math id="A5.p2.2.m2.2" class="ltx_Math" alttext="\in\{0,10^{-5}\}" display="inline"><semantics id="A5.p2.2.m2.2a"><mrow id="A5.p2.2.m2.2.2" xref="A5.p2.2.m2.2.2.cmml"><mi id="A5.p2.2.m2.2.2.3" xref="A5.p2.2.m2.2.2.3.cmml"></mi><mo id="A5.p2.2.m2.2.2.2" xref="A5.p2.2.m2.2.2.2.cmml">âˆˆ</mo><mrow id="A5.p2.2.m2.2.2.1.1" xref="A5.p2.2.m2.2.2.1.2.cmml"><mo stretchy="false" id="A5.p2.2.m2.2.2.1.1.2" xref="A5.p2.2.m2.2.2.1.2.cmml">{</mo><mn id="A5.p2.2.m2.1.1" xref="A5.p2.2.m2.1.1.cmml">0</mn><mo id="A5.p2.2.m2.2.2.1.1.3" xref="A5.p2.2.m2.2.2.1.2.cmml">,</mo><msup id="A5.p2.2.m2.2.2.1.1.1" xref="A5.p2.2.m2.2.2.1.1.1.cmml"><mn id="A5.p2.2.m2.2.2.1.1.1.2" xref="A5.p2.2.m2.2.2.1.1.1.2.cmml">10</mn><mrow id="A5.p2.2.m2.2.2.1.1.1.3" xref="A5.p2.2.m2.2.2.1.1.1.3.cmml"><mo id="A5.p2.2.m2.2.2.1.1.1.3a" xref="A5.p2.2.m2.2.2.1.1.1.3.cmml">âˆ’</mo><mn id="A5.p2.2.m2.2.2.1.1.1.3.2" xref="A5.p2.2.m2.2.2.1.1.1.3.2.cmml">5</mn></mrow></msup><mo stretchy="false" id="A5.p2.2.m2.2.2.1.1.4" xref="A5.p2.2.m2.2.2.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p2.2.m2.2b"><apply id="A5.p2.2.m2.2.2.cmml" xref="A5.p2.2.m2.2.2"><in id="A5.p2.2.m2.2.2.2.cmml" xref="A5.p2.2.m2.2.2.2"></in><csymbol cd="latexml" id="A5.p2.2.m2.2.2.3.cmml" xref="A5.p2.2.m2.2.2.3">absent</csymbol><set id="A5.p2.2.m2.2.2.1.2.cmml" xref="A5.p2.2.m2.2.2.1.1"><cn type="integer" id="A5.p2.2.m2.1.1.cmml" xref="A5.p2.2.m2.1.1">0</cn><apply id="A5.p2.2.m2.2.2.1.1.1.cmml" xref="A5.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="A5.p2.2.m2.2.2.1.1.1.1.cmml" xref="A5.p2.2.m2.2.2.1.1.1">superscript</csymbol><cn type="integer" id="A5.p2.2.m2.2.2.1.1.1.2.cmml" xref="A5.p2.2.m2.2.2.1.1.1.2">10</cn><apply id="A5.p2.2.m2.2.2.1.1.1.3.cmml" xref="A5.p2.2.m2.2.2.1.1.1.3"><minus id="A5.p2.2.m2.2.2.1.1.1.3.1.cmml" xref="A5.p2.2.m2.2.2.1.1.1.3"></minus><cn type="integer" id="A5.p2.2.m2.2.2.1.1.1.3.2.cmml" xref="A5.p2.2.m2.2.2.1.1.1.3.2">5</cn></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.2.m2.2c">\in\{0,10^{-5}\}</annotation></semantics></math> and training batch size <math id="A5.p2.3.m3.2" class="ltx_Math" alttext="\in\{32,128\}" display="inline"><semantics id="A5.p2.3.m3.2a"><mrow id="A5.p2.3.m3.2.3" xref="A5.p2.3.m3.2.3.cmml"><mi id="A5.p2.3.m3.2.3.2" xref="A5.p2.3.m3.2.3.2.cmml"></mi><mo id="A5.p2.3.m3.2.3.1" xref="A5.p2.3.m3.2.3.1.cmml">âˆˆ</mo><mrow id="A5.p2.3.m3.2.3.3.2" xref="A5.p2.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="A5.p2.3.m3.2.3.3.2.1" xref="A5.p2.3.m3.2.3.3.1.cmml">{</mo><mn id="A5.p2.3.m3.1.1" xref="A5.p2.3.m3.1.1.cmml">32</mn><mo id="A5.p2.3.m3.2.3.3.2.2" xref="A5.p2.3.m3.2.3.3.1.cmml">,</mo><mn id="A5.p2.3.m3.2.2" xref="A5.p2.3.m3.2.2.cmml">128</mn><mo stretchy="false" id="A5.p2.3.m3.2.3.3.2.3" xref="A5.p2.3.m3.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p2.3.m3.2b"><apply id="A5.p2.3.m3.2.3.cmml" xref="A5.p2.3.m3.2.3"><in id="A5.p2.3.m3.2.3.1.cmml" xref="A5.p2.3.m3.2.3.1"></in><csymbol cd="latexml" id="A5.p2.3.m3.2.3.2.cmml" xref="A5.p2.3.m3.2.3.2">absent</csymbol><set id="A5.p2.3.m3.2.3.3.1.cmml" xref="A5.p2.3.m3.2.3.3.2"><cn type="integer" id="A5.p2.3.m3.1.1.cmml" xref="A5.p2.3.m3.1.1">32</cn><cn type="integer" id="A5.p2.3.m3.2.2.cmml" xref="A5.p2.3.m3.2.2">128</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.3.m3.2c">\in\{32,128\}</annotation></semantics></math>. We found by comparing backbones pre-trained on Imagenet and a large synthetic set generated with Domain Randomization, that with the above grid, for each specific downstream task and evaluation method, a particular set of hyperparameters worked best irrespective of the pre-training data. This was found using a separate validation split created from the downstream training set with 30% of the examples. Given this finding, we fixed these hyperparameters for a given downstream task and evaluation method for all remaining experiments.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Details of Downstream Tasks</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p"><a href="#A6.T5" title="In Appendix F Details of Downstream Tasks â€£ Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> shows the number of classes in each of the 20 downstream tasks we used. It also shows the number of images in the training and test splits for each.</p>
</div>
<figure id="A6.T5" class="ltx_table">
<div id="A6.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:382.4pt;height:302.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-47.8pt,37.8pt) scale(0.8,0.8) ;">
<table id="A6.T5.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A6.T5.2.1.1" class="ltx_tr">
<td id="A6.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Category</td>
<td id="A6.T5.2.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Dataset</td>
<td id="A6.T5.2.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">Train Size</td>
<td id="A6.T5.2.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">Test Size</td>
<td id="A6.T5.2.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">Classes</td>
</tr>
<tr id="A6.T5.2.1.2" class="ltx_tr">
<td id="A6.T5.2.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="3"><span id="A6.T5.2.1.2.1.1" class="ltx_text">Natural</span></td>
<td id="A6.T5.2.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CropDiseaseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="A6.T5.2.1.2.3" class="ltx_td ltx_align_right ltx_border_t">43456</td>
<td id="A6.T5.2.1.2.4" class="ltx_td ltx_align_right ltx_border_t">10849</td>
<td id="A6.T5.2.1.2.5" class="ltx_td ltx_align_right ltx_border_t">38</td>
</tr>
<tr id="A6.T5.2.1.3" class="ltx_tr">
<td id="A6.T5.2.1.3.1" class="ltx_td ltx_align_left ltx_border_r">FlowersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="A6.T5.2.1.3.2" class="ltx_td ltx_align_right">1020</td>
<td id="A6.T5.2.1.3.3" class="ltx_td ltx_align_right">6149</td>
<td id="A6.T5.2.1.3.4" class="ltx_td ltx_align_right">102</td>
</tr>
<tr id="A6.T5.2.1.4" class="ltx_tr">
<td id="A6.T5.2.1.4.1" class="ltx_td ltx_align_left ltx_border_r">DeepWeedsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="A6.T5.2.1.4.2" class="ltx_td ltx_align_right">12252</td>
<td id="A6.T5.2.1.4.3" class="ltx_td ltx_align_right">5257</td>
<td id="A6.T5.2.1.4.4" class="ltx_td ltx_align_right">9</td>
</tr>
<tr id="A6.T5.2.1.5" class="ltx_tr">
<td id="A6.T5.2.1.5.1" class="ltx_td ltx_border_r"></td>
<td id="A6.T5.2.1.5.2" class="ltx_td ltx_align_left ltx_border_r">CUBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="A6.T5.2.1.5.3" class="ltx_td ltx_align_right">5994</td>
<td id="A6.T5.2.1.5.4" class="ltx_td ltx_align_right">5794</td>
<td id="A6.T5.2.1.5.5" class="ltx_td ltx_align_right">200</td>
</tr>
<tr id="A6.T5.2.1.6" class="ltx_tr">
<td id="A6.T5.2.1.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="A6.T5.2.1.6.1.1" class="ltx_text">Satellite</span></td>
<td id="A6.T5.2.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">EuroSATÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="A6.T5.2.1.6.3" class="ltx_td ltx_align_right ltx_border_t">18900</td>
<td id="A6.T5.2.1.6.4" class="ltx_td ltx_align_right ltx_border_t">8100</td>
<td id="A6.T5.2.1.6.5" class="ltx_td ltx_align_right ltx_border_t">10</td>
</tr>
<tr id="A6.T5.2.1.7" class="ltx_tr">
<td id="A6.T5.2.1.7.1" class="ltx_td ltx_align_left ltx_border_r">Resisc45Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="A6.T5.2.1.7.2" class="ltx_td ltx_align_right">22005</td>
<td id="A6.T5.2.1.7.3" class="ltx_td ltx_align_right">9495</td>
<td id="A6.T5.2.1.7.4" class="ltx_td ltx_align_right">45</td>
</tr>
<tr id="A6.T5.2.1.8" class="ltx_tr">
<td id="A6.T5.2.1.8.1" class="ltx_td ltx_border_r"></td>
<td id="A6.T5.2.1.8.2" class="ltx_td ltx_align_left ltx_border_r">AIDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="A6.T5.2.1.8.3" class="ltx_td ltx_align_right">6993</td>
<td id="A6.T5.2.1.8.4" class="ltx_td ltx_align_right">3007</td>
<td id="A6.T5.2.1.8.5" class="ltx_td ltx_align_right">30</td>
</tr>
<tr id="A6.T5.2.1.9" class="ltx_tr">
<td id="A6.T5.2.1.9.1" class="ltx_td ltx_border_r"></td>
<td id="A6.T5.2.1.9.2" class="ltx_td ltx_align_left ltx_border_r">CactusAerialÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="A6.T5.2.1.9.3" class="ltx_td ltx_align_right">17500</td>
<td id="A6.T5.2.1.9.4" class="ltx_td ltx_align_right">4000</td>
<td id="A6.T5.2.1.9.5" class="ltx_td ltx_align_right">2</td>
</tr>
<tr id="A6.T5.2.1.10" class="ltx_tr">
<td id="A6.T5.2.1.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="A6.T5.2.1.10.1.1" class="ltx_text">Symbolic</span></td>
<td id="A6.T5.2.1.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">OmniglotÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="A6.T5.2.1.10.3" class="ltx_td ltx_align_right ltx_border_t">9226</td>
<td id="A6.T5.2.1.10.4" class="ltx_td ltx_align_right ltx_border_t">3954</td>
<td id="A6.T5.2.1.10.5" class="ltx_td ltx_align_right ltx_border_t">1623</td>
</tr>
<tr id="A6.T5.2.1.11" class="ltx_tr">
<td id="A6.T5.2.1.11.1" class="ltx_td ltx_align_left ltx_border_r">SVHNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="A6.T5.2.1.11.2" class="ltx_td ltx_align_right">73257</td>
<td id="A6.T5.2.1.11.3" class="ltx_td ltx_align_right">26032</td>
<td id="A6.T5.2.1.11.4" class="ltx_td ltx_align_right">10</td>
</tr>
<tr id="A6.T5.2.1.12" class="ltx_tr">
<td id="A6.T5.2.1.12.1" class="ltx_td ltx_border_r"></td>
<td id="A6.T5.2.1.12.2" class="ltx_td ltx_align_left ltx_border_r">USPSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="A6.T5.2.1.12.3" class="ltx_td ltx_align_right">7291</td>
<td id="A6.T5.2.1.12.4" class="ltx_td ltx_align_right">2007</td>
<td id="A6.T5.2.1.12.5" class="ltx_td ltx_align_right">10</td>
</tr>
<tr id="A6.T5.2.1.13" class="ltx_tr">
<td id="A6.T5.2.1.13.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="A6.T5.2.1.13.1.1" class="ltx_text">Medical</span></td>
<td id="A6.T5.2.1.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ISICÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="A6.T5.2.1.13.3" class="ltx_td ltx_align_right ltx_border_t">7007</td>
<td id="A6.T5.2.1.13.4" class="ltx_td ltx_align_right ltx_border_t">3008</td>
<td id="A6.T5.2.1.13.5" class="ltx_td ltx_align_right ltx_border_t">7</td>
</tr>
<tr id="A6.T5.2.1.14" class="ltx_tr">
<td id="A6.T5.2.1.14.1" class="ltx_td ltx_align_left ltx_border_r">ChestXÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="A6.T5.2.1.14.2" class="ltx_td ltx_align_right">18090</td>
<td id="A6.T5.2.1.14.3" class="ltx_td ltx_align_right">7758</td>
<td id="A6.T5.2.1.14.4" class="ltx_td ltx_align_right">7</td>
</tr>
<tr id="A6.T5.2.1.15" class="ltx_tr">
<td id="A6.T5.2.1.15.1" class="ltx_td ltx_border_r"></td>
<td id="A6.T5.2.1.15.2" class="ltx_td ltx_align_left ltx_border_r">ChestXPneumoniaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="A6.T5.2.1.15.3" class="ltx_td ltx_align_right">5216</td>
<td id="A6.T5.2.1.15.4" class="ltx_td ltx_align_right">624</td>
<td id="A6.T5.2.1.15.5" class="ltx_td ltx_align_right">2</td>
</tr>
<tr id="A6.T5.2.1.16" class="ltx_tr">
<td id="A6.T5.2.1.16.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="A6.T5.2.1.16.1.1" class="ltx_text">Illustrative</span></td>
<td id="A6.T5.2.1.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">KaokoreÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</td>
<td id="A6.T5.2.1.16.3" class="ltx_td ltx_align_right ltx_border_t">6568</td>
<td id="A6.T5.2.1.16.4" class="ltx_td ltx_align_right ltx_border_t">821</td>
<td id="A6.T5.2.1.16.5" class="ltx_td ltx_align_right ltx_border_t">8</td>
</tr>
<tr id="A6.T5.2.1.17" class="ltx_tr">
<td id="A6.T5.2.1.17.1" class="ltx_td ltx_align_left ltx_border_r">SketchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="A6.T5.2.1.17.2" class="ltx_td ltx_align_right">35000</td>
<td id="A6.T5.2.1.17.3" class="ltx_td ltx_align_right">15889</td>
<td id="A6.T5.2.1.17.4" class="ltx_td ltx_align_right">1000</td>
</tr>
<tr id="A6.T5.2.1.18" class="ltx_tr">
<td id="A6.T5.2.1.18.1" class="ltx_td ltx_border_r"></td>
<td id="A6.T5.2.1.18.2" class="ltx_td ltx_align_left ltx_border_r">PACS-CÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="A6.T5.2.1.18.3" class="ltx_td ltx_align_right">2107</td>
<td id="A6.T5.2.1.18.4" class="ltx_td ltx_align_right">237</td>
<td id="A6.T5.2.1.18.5" class="ltx_td ltx_align_right">7</td>
</tr>
<tr id="A6.T5.2.1.19" class="ltx_tr">
<td id="A6.T5.2.1.19.1" class="ltx_td ltx_border_r"></td>
<td id="A6.T5.2.1.19.2" class="ltx_td ltx_align_left ltx_border_r">PACS-SÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="A6.T5.2.1.19.3" class="ltx_td ltx_align_right">3531</td>
<td id="A6.T5.2.1.19.4" class="ltx_td ltx_align_right">398</td>
<td id="A6.T5.2.1.19.5" class="ltx_td ltx_align_right">7</td>
</tr>
<tr id="A6.T5.2.1.20" class="ltx_tr">
<td id="A6.T5.2.1.20.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="A6.T5.2.1.20.1.1" class="ltx_text">Texture</span></td>
<td id="A6.T5.2.1.20.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DTDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="A6.T5.2.1.20.3" class="ltx_td ltx_align_right ltx_border_t">3760</td>
<td id="A6.T5.2.1.20.4" class="ltx_td ltx_align_right ltx_border_t">1880</td>
<td id="A6.T5.2.1.20.5" class="ltx_td ltx_align_right ltx_border_t">47</td>
</tr>
<tr id="A6.T5.2.1.21" class="ltx_tr">
<td id="A6.T5.2.1.21.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">FMDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
</td>
<td id="A6.T5.2.1.21.2" class="ltx_td ltx_align_right ltx_border_bb">1400</td>
<td id="A6.T5.2.1.21.3" class="ltx_td ltx_align_right ltx_border_bb">600</td>
<td id="A6.T5.2.1.21.4" class="ltx_td ltx_align_right ltx_border_bb">10</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A6.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="A6.T5.4.2" class="ltx_text" style="font-size:90%;">Number of classes in each downstream task and number of images in each training and test split.</span></figcaption>
</figure>
<figure id="A6.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F18.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x24.png" id="A6.F18.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="256" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F18.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x25.png" id="A6.F18.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="256" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F18.4.1.1" class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span id="A6.F18.5.2" class="ltx_text" style="font-size:90%;">Downstream performance (avg over 8 unseen tasks) with different number of classes for pre-training. Best viewed in color.</span></figcaption>
</figure>
<figure id="A6.F19" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F19.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x26.png" id="A6.F19.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="256" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F19.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x27.png" id="A6.F19.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="256" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F19.4.1.1" class="ltx_text" style="font-size:90%;">Figure 19</span>: </span><span id="A6.F19.5.2" class="ltx_text" style="font-size:90%;">Downstream performance (avg over 8 unseen tasks) with different number of objects for pre-training. Best viewed in color.</span></figcaption>
</figure>
<figure id="A6.F20" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F20.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x28.png" id="A6.F20.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="250" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F20.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.00054/assets/x29.png" id="A6.F20.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="256" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F20.4.1.1" class="ltx_text" style="font-size:90%;">Figure 20</span>: </span><span id="A6.F20.5.2" class="ltx_text" style="font-size:90%;">Downstream performance (avg over 8 unseen tasks) with different number of images for pre-training. Best viewed in color.</span></figcaption>
</figure>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Limitations</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">In this paper, we constrained our demonstration to a relatively low number of datasets and simulation parameters, limited by data generation, pre-training and evaluation speed. If these processes can be made more efficient, in future work, we can expect to use more simulation parameters (with possibly more discrete options or even real-valued ranges), and use more datasets for training Task2Sim, allowing it to be more effective in deployment as a practical application.</p>
</div>
<div id="A7.p2" class="ltx_para">
<p id="A7.p2.1" class="ltx_p">While a large portion of contemporary representation learning research focuses on self-supervision to avoid using labels, we hope our demonstration with Task2Sim motivates further research in using simulated data from graphics engines for this purpose, with focus on adaptive generation for downstream application.</p>
</div>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Societal Impact</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.1" class="ltx_p">In the introduction, we discussed model pre-training using large real image datasets was what paved the way for a gamut of transfer learning research. Using real images is however riddled with curation costs and others concerns around privacy, copyright, ethical usage, etc. The fact that downstream performance on average correlates positively with the size of pre-training data, created a race for curating bigger datasets. Corporations with large resources are able to invest in such large-scale curation and create datasets for their exclusive use (<em id="A8.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A8.p1.1.2" class="ltx_text"></span> JFT-300MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, or Instagram-3.5BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>), which are unavailable to a range of research on downstream applications.</p>
</div>
<div id="A8.p2" class="ltx_para">
<p id="A8.p2.1" class="ltx_p">Using synthetic data for pre-training can drastically reduce these costs, because potentially infinite images can be rendered once 3D models and scenes are available, by varying various simulation parameters. In this paper, we demonstrated that the optimal use of such a simulation engine can be found in restricting certain variations, and that different restrictions benefit different downstream tasks. Our Task2Sim approach, can be used as the basis for a pre-training data generator, which as an end-user application can allow research on a wide range of downstream applications to have access to the benefits of pre-training on large-scale scale data. This does not create any direct impacts on average individuals, but could do so through the advancement in downstream applications. One particular case, as an example, could be the advancement in visual recognition systems in the medical domain, possibly making the diagnosis of illnesses faster and cheaper.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.00052" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.00054" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.00054">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.00054" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.00055" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 02:37:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
