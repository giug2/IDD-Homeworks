<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1909.00713] Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data</title><meta property="og:description" content="This paper addresses the problem of scale estimation in monocular SLAM by estimating absolute distances between camera centers of consecutive image frames. These estimates would improve the overall performance of class…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1909.00713">

<!--Generated on Sat Mar 16 10:09:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Danila Rukhovich
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">drk@zurich.ibm.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Mouritzen
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">dmo@zurich.ibm.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ralf Kaestner
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">alf@zurich.ibm.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Rufli
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">mru@zurich.ibm.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Velizhev
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ave@zurich.ibm.com
<br class="ltx_break"></span>IBM Research - Zurich, Switzerland
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">This paper addresses the problem of scale estimation in monocular SLAM by estimating absolute distances between camera centers of consecutive image frames. These estimates would improve the overall performance of classical (not deep) SLAM systems and allow metric feature locations to be recovered from a single monocular camera. We propose several network architectures that lead to an improvement of scale estimation accuracy over the state of the art. In addition, we exploit a possibility to train the neural network only with synthetic data derived from a computer graphics simulator. Our key insight is that, using only synthetic training inputs, we can achieve similar scale estimation accuracy as that obtained from real data. This fact indicates that fully annotated simulated data is a viable alternative to existing deep-learning-based SLAM systems trained on real (unlabeled) data. Our experiments with unsupervised domain adaptation also show that the difference in visual appearance between simulated and real data does not affect scale estimation results. Our method operates with low-resolution images (0.03 MP), which makes it practical for real-time SLAM applications with a monocular camera.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Monocular visual SLAM allows the translation and rotation of a moving camera and a sparse map representation to be determined, but only up to scale. This paper targets the problem of absolute scale estimation using sequences of images captured by a monocular camera. We consider the most general formulation of this problem but make no assumptions about the observed scene and its objects.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Estimation of scale is important for several reasons. First, it helps recover metric feature locations, which is valuable for numerous real-world applications. Other applications that do not require absolute scale also benefit from its estimation because negative effects caused by scale drift are reduced. For example, scale drift makes it more difficult to detect loops, and selecting key frames might not take into account actual camera displacements. In addition, scale drift does not allow scale correction by optimizing just one global scale parameter.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Additional sensors might be used to overcome the scale estimation problem, including IMU, GPS, LiDAR, stereo or depth cameras. However, they raise the cost, complexity, power consumption and weight of the entire system and thus reduce the number of possible applications. Therefore, the ability to estimate scale using but a single camera would be very beneficial.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The success of deep learning has opened new options for SLAM systems. For example, single image depth prediction makes it possible to retrieve absolute depth maps from color images. This might solve the scale estimation problem as well and improves SLAM systems in general <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. However, this approach solves a much more complex problem, requires a lot of training data and therefore might be considered too computationally expensive for a given scale estimation problem. End-to-end deep learning approaches for SLAM were also recently introduced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. In this case, absolute scale might be estimated as well if relative camera poses with absolute scale are used for training. For the time being, complete end-to-end SLAM approaches require a modern GPU unit, which limits applications of these methods.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we focus only on estimating the scale, which might then be integrated into classical SLAM systems. Comparison of classical and deep learning-based SLAM systems or solving the full six-degrees-of-freedom (DOF) pose estimation problem is beyond the scope of this work.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">An elegant and lightweight approach for scale estimation using CNNs is described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The absolute distance (or speed) between two consecutive images is estimated independently for each pair. This helps reduce the scale drift effect significantly. The approach is very general (no explicit assumptions are made about the observed scene or objects inside it), is applicable to monocular SLAM and can be easily integrated into existing SLAM systems. This paper considers the method established in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> as a baseline.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our detailed analysis of results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> on the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> highlighted several issues. First, we observed considerable systematic errors as the camera turned, see Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Evaluation of scale estimators ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, which might be attributed to a strong domination of pure forward camera movements in the training set. Another issue is related to the sensor configuration. Specifically, a camera placed at an offset to the vehicle’s center of rotation constrains the possible combinations of rotation and translation values. For example, a front camera would never experience a pure rotation around the camera center because the global rotational center of a conventional vehicle is located on the rear wheel axis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. This means that actual camera rotation will always be combined with a certain translational movement. Adapting to new sensor configurations will require new data collection efforts. The last issue pertains to absolute scale estimation accuracy. There are often significant changes in the estimated distances between consecutive image pairs, even when the speed is relatively constant. Based on these arguments, the following goals of this paper can be formulated as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Improve the state-of-the-art accuracy of absolute scale estimation</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Improve estimates for camera turning movements</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Improve robustness against variations of the sensor configuration and environmental conditions.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Our solution to the first and the second problems is to modify the neural network architecture with respect to the baseline approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> by increasing complexity of the network and adding a recurrent neural layer. These network modifications lead to significant reduction of the training loss to the very low values. This fact makes unnecessary further investigation of the network architecture, so we do not focus on this topic in the paper. The third issue is solved by using autonomous driving simulators, with which we can easily model any type of sensor configuration and generate a large amount of training data with wide variations of lighting and weather conditions. Recent works in similar domains (depth prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> or optical flow generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>) show promising results when synthetic data is used. Although our simulated training environment differs significantly from the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> (for example, there are no parked cars), the trained network is still able to generalize to new and unseen environments. We will show that training on synthetic data yields comparable results to models trained on real data. In addition, we evaluate the influence of image photorealism for the problem at hand.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The paper is organized as follows. Section <a href="#S2" title="2 Related work ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents an overview of existing methods, and
Section <a href="#S3" title="3 Proposed method ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes proposed network architectures, datasets and domain adaptation strategies.
Section <a href="#S4" title="4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents evaluation results and compares different methods.
Concluding remarks and discussion are available in Section <a href="#S5" title="5 Conclusion ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section describes previous work on the scale estimation problem for monocular SLAM. Our discussion covers the following approaches: (i) using explicit knowledge about the scene, (ii) using explicit soft assumptions about the scene, (iii) general, assumption-free approaches.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The first approach explicitly uses information about the scene, <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.2" class="ltx_text"></span> a 3D model with absolute scale. The scale of image-based reconstruction is estimated using correspondences between images and the 3D model. A known 3D model is a very strong assumption, which strongly limits the applicability of these approaches. These methods were introduced more than 30 years ago <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and are mentioned primarily for completeness.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The second approach uses soft assumptions about the observed scene. This might be a known height of the camera above the ground plane <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> or information about absolute sizes of objects presented in a scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In the first case, a position of the ground plane is estimated from space 3D points of the reconstructed scene, and the distance between camera center and this plane is constrained.
In the second case, a pretrained object detector for a defined set of object classes (<em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p3.1.2" class="ltx_text"></span> cars) is employed to incorporate general knowledge of object sizes into the optimization problem.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The main disadvantage of both these approaches are the assumptions themselves. For instance, the constraint on the camera height is applicable only for cameras mounted on vehicles and assumes this height is known beforehand and is constant during image recording. Analogously, relying on having certain classes of objects in a scene fails when none of these objects are present. Moreover, observed objects might be only partially visible, which makes their size estimates inaccurate. Objects might also have intra-class variations in size (<em id="S2.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p4.1.2" class="ltx_text"></span> different types of cars), which additionally decreases the accuracy of the scale estimates. In order to mitigate those issues, different flexible schemes using object detection have been introduced. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> introduces so-called object bundle adjustment, which optimises 3D landmark positions associated with objects of known size. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> fuses single detections from a generic object detector within a Bayesian framework. Using the nonholonomic constraints of wheeled vehicles (<em id="S2.p4.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p4.1.4" class="ltx_text"></span> cars, bikes or differential drive robots) was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to estimate the absolute scale from a single vehicle-mounted camera. This approach uses no assumptions about the scene, but it works only for cameras mounted on wheeled vehicles and only when the vehicle is turning. This limitation makes it difficult to recover absolute scale for long, linear trajectories.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">The last type of methods is more generic and does not use explicit assumptions about the scene. The most common idea is to recover scale using absolute depth maps constructed from single images using deep-learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. These approaches are similar to 3D reconstruction using RGB-D sensors, which directly output metric depth maps. The scale is recovered natively by integrating the absolute depth maps into the 3D reconstruction. Recent progress in depth prediction from single images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> has made it possible to apply these methods to monocular scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Another approach, introduced by Frost <em id="S2.p6.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p6.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, trains the network to predict the absolute distance between camera centers from a pair of images with significant visual overlap. This approach is fairly generic because no explicit assumptions about the scene are made. Intuitively this approach learns how similar image regions are shifted between two frames. Given intrinsic camera parameters, these shifts would be proportional to the camera displacement. The distance predictions are directly included in the bundle adjustment, which improves scale accuracy significantly. The method is applied to images with a relatively low resolution of <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="240\times 120" display="inline"><semantics id="S2.p6.1.m1.1a"><mrow id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml"><mn id="S2.p6.1.m1.1.1.2" xref="S2.p6.1.m1.1.1.2.cmml">240</mn><mo lspace="0.222em" rspace="0.222em" id="S2.p6.1.m1.1.1.1" xref="S2.p6.1.m1.1.1.1.cmml">×</mo><mn id="S2.p6.1.m1.1.1.3" xref="S2.p6.1.m1.1.1.3.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><apply id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1"><times id="S2.p6.1.m1.1.1.1.cmml" xref="S2.p6.1.m1.1.1.1"></times><cn type="integer" id="S2.p6.1.m1.1.1.2.cmml" xref="S2.p6.1.m1.1.1.2">240</cn><cn type="integer" id="S2.p6.1.m1.1.1.3.cmml" xref="S2.p6.1.m1.1.1.3">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">240\times 120</annotation></semantics></math> pixels and can be executed in real time. Frost <em id="S2.p6.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p6.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> also present benefits of integrating the scale estimator into a full monocular SLAM system. From our point of view this integration is straightforward and we are focusing only on improving the scale estimator itself and leaving the integration for future work.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">End-to-end SLAM approaches such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> provide an alternative to the classical feature-based SLAM approach. These methods also provide full camera poses in absolute scale. We consider these approaches to be overcomplicated (in terms of the number of parameters) for dealing with the scale estimation problem alone. For example, the DeepVO method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> introduces a fully connected layer with 122M trainable parameters and thus significantly increases computational complexity with strong implications to real-time applications.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">The idea to use synthetic data for scale estimation raises the following question: How important is the visual similarity between simulated and real environments? To answer this question, one could apply domain adaptation methods, which help improve the visual realism of synthetic data and train the scale estimator with more realistic synthetic data. Numerous unsupervised image domain adaptation methods have been introduced recently (<em id="S2.p8.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p8.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>) and show impressive results with regard to changing the visual similarity between different image domains (<em id="S2.p8.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p8.1.4" class="ltx_text"></span> synthetic and real). The scale estimation problem takes as input a pair of images and implicitly operates on image changes so the role of photorealism itself is unclear. To evaluate this influence, we test two modern domain adaptation methods (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>), which are trained in an end-to-end fashion including the target problem (in our case: scale estimation). This approach allows us to optimize the visual image appearance and scale estimation within the same network. Theoretically, this allows us to apply only those image transformations that are relevant for the target problem instead of targeting visually appealing image photorealism for humans. Our implementation of both domain adaptation methods is similar to that proposed in the original papers, see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> for technical details.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We will first introduce the synthetic data collection pipeline, then propose network architectures and finally describe the domain adaptation methods we used.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span> Synthetic data collection</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/1909.00713/assets/examples_of_images.png" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="423" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of real images from KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> (top row) and CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (bottom three rows) with different daytime and weather conditions.</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1909.00713/assets/cnn_block.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="89" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Baseline CNN architecture used in all experiments of this paper.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Using real data (such as the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) for training imposes certain limitations: the variation of weather, illumination and time of day might be limited, diversity of scene appearance (<em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.1.2" class="ltx_text"></span> urban, rural) requires significant data collection efforts. In addition, sensor configuration and vehicle movement type determine the variability of observed data in the parameter space. A change of the camera position in the vehicle could lead to the significant systematic errors because certain combinations of rotation and translation are not presented in the training set. One way to overcome these issues is the use of synthetic data collected from autonomous driving simulators like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. A simulator allows to attach one or multiple cameras at specific locations on the vehicle, vary weather, time and lighting conditions, and define custom camera intrinsic parameters. An autopilot mode enables automatic driving in the scene combined with training data collection. In our case the collected training data includes color images and ground truth camera trajectories. The synthetic data collection pipeline and a network training procedure form one complete framework which is executed fully automatically. Basically this solves the problem of sensor configuration and, as we show later, allows to reach scale estimation accuracy comparable to accuracy produced by the model trained on real data.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Network architectures</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To start, we selected the method from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> as a baseline implementation. The key idea is to concatenate a pair of RGB images into one 6-channel image, pass it through three convolutional layers with max-pooling (window size 2x2, stride 2), followed by two fully connected layers. As mentioned above, our goal is to improve the absolute distance estimation accuracy, so our first step is to increase the complexity of the network. We added two more convolutional layers, which improves the accuracy remarkably, see Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic data collection ‣ 3 Proposed method ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for details. In contrast to [11], we use exponential linear units (ELUs) [5] as activation functions instead of tanh(·) because it leads to a faster convergence of the training. Dropout layers are used between all convolutional and fully connected layers. A detailed configuration of the network is presented in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Network architectures ‣ 3 Proposed method ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We use the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and the mean squared error as our loss function. For simplicity, the following sections refer to this as a “CNN” architecture.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Layer</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">
<table id="S3.T1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Kernel</td>
</tr>
<tr id="S3.T1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Size</td>
</tr>
</table>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Padding</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Number of</td>
</tr>
<tr id="S3.T1.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Filters</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">conv 1</th>
<th id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">11x11</th>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">32</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">conv 2</th>
<th id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">9x9</th>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">conv 3</th>
<th id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">7x7</th>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">128</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">conv 4</th>
<th id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">5x5</th>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">256</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">conv 5</th>
<th id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">3x3</th>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">512</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Configuration of CNN layers.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1909.00713/assets/lstm_block.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="384" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Proposed bidirectional LSTM architecture. The CNN block corresponds to the green block of Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic data collection ‣ 3 Proposed method ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.3" class="ltx_p">As described in Section <a href="#S1" title="1 Introduction ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, there are often significant changes in the estimated distances between consecutive image pairs. This is not surprising because distances are estimated independently for all image pairs, which means that vehicle dymanics are ignored. Instead of adding explicit constraints to possible vehicle movements, we propose that the actual dynamics be learned from data. For this purpose, we use a recurrent neural network (RNN), in particular a many-to-one LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. This allows the network to learn how previously observed image pairs influence the current distance estimate. The key concept of our architecture is inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which uses an LSTM for action recognition in videos. A similar approach was also used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to predict full 6DOF camera pose. In contrast to the previous approaches, we also evaluate a bidirectional LSTM version, see Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 Network architectures ‣ 3 Proposed method ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). In this case, we propagate information from <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">N</annotation></semantics></math> past and <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">N</annotation></semantics></math> future frames. Our algorithm outputs distance estimates with a small delay, but this limitation is not critical because this step can be done in parallel with other SLAM steps such as feature detection and descriptor computation. In addition, a convolutional part of the network (the green block) is executed only once per image pair, so we need to evaluate only relatively lightweight LSTM layers. Given the low resolution of images in our experiments (<math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="280\times 120" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mn id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">280</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">280</cn><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">280\times 120</annotation></semantics></math> pixels) this delay has a minor impact on computational performance. For comparison, we also evaluated a similar unidirectional LSTM version. We will refer to this as an “LSTM” architecture.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Our changes of the baseline architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> lead to the convergence of the training loss to the values comparable to the accuracy of training data itself. So we conclude that complexity of the network is enough for the considered problem.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">We use the same image normalization procedure as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which helps make the trained model invariant to different intrinsic camera parameters. For both synthetic and real data, we use the following image augmentation steps:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Random image contrast and brightness adjustment</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-1.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Random image horizontal flip</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-1.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Constrained random image rotation and translation</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:-1.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Using all consecutive or non-consecutive image pairs as long as the distance between camera poses is not greater than <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="D\textsubscript{max}" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mrow id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><mi id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.1.m1.1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.1.cmml">​</mo><mtext id="S3.I1.i4.p1.1.m1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.3b.cmml"><sub id="S3.I1.i4.p1.1.m1.1.1.3.1nest" class="ltx_sub">max</sub></mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><times id="S3.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1"></times><ci id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.I1.i4.p1.1.m1.1.1.3b.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3"><mtext id="S3.I1.i4.p1.1.m1.1.1.3.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3"><sub id="S3.I1.i4.p1.1.m1.1.1.3.1anest" class="ltx_sub">max</sub></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">D\textsubscript{max}</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:-1.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Duplication of image pairs recorded while vehicle is turning.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Domain adaptation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Images collected from autonomous driving simulators have quite a different appearance compared with real data, see Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Synthetic data collection ‣ 3 Proposed method ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. However, for the task at hand, the network takes a pair of images and, in principle, basically has to look at the geometric scene changes between the images. This raises the question of whether the difference in appearance has a significant impact on the quality of the distance estimation. To answer this question, we reimplemented and applied two state-of-the-art frameworks for unsupervised domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Both approaches show promising results for semantic segmentation and single-image depth prediction. We trained these frameworks using a full training set of synthetic data with ground-truth camera locations and unlabeled images from the real training sequences. We tested these approaches out-of-the-box without fine-tuning them, which is beyond the scope of this paper.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment overview</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">This section is organized as follows. First we describe real and synthetic datasets as well as training details for all proposed network architectures. Then, in order to evaluate the proposed methodology, we run the following series of experiments:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Comparison of absolute scale accuracy of the baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> with our proposed CNN and LSTM architectures trained on real or/and synthetic data</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-1.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Evaluation of LSTM length</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-1.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Evaluation of synthetic data diversity</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:-1.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Evaluation of domain adaptation.</p>
</div>
</li>
</ul>
<p id="S4.SS1.p1.2" class="ltx_p">These results are followed by a detailed analysis of errors and their distribution for our best model.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.4" class="ltx_p">We observed that a large part of the image is cropped when the original intrinsic camera parameters are used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. To use the full image, we take a focal length <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">250</annotation></semantics></math> px and a principal point <math id="S4.SS2.p1.2.m2.2" class="ltx_Math" alttext="(140,60)" display="inline"><semantics id="S4.SS2.p1.2.m2.2a"><mrow id="S4.SS2.p1.2.m2.2.3.2" xref="S4.SS2.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p1.2.m2.2.3.2.1" xref="S4.SS2.p1.2.m2.2.3.1.cmml">(</mo><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">140</mn><mo id="S4.SS2.p1.2.m2.2.3.2.2" xref="S4.SS2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.2.m2.2.2" xref="S4.SS2.p1.2.m2.2.2.cmml">60</mn><mo stretchy="false" id="S4.SS2.p1.2.m2.2.3.2.3" xref="S4.SS2.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.2b"><interval closure="open" id="S4.SS2.p1.2.m2.2.3.1.cmml" xref="S4.SS2.p1.2.m2.2.3.2"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">140</cn><cn type="integer" id="S4.SS2.p1.2.m2.2.2.cmml" xref="S4.SS2.p1.2.m2.2.2">60</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.2c">(140,60)</annotation></semantics></math> for the target camera within the image normalization procedure. This change enlarges image resolution slightly from <math id="S4.SS2.p1.3.m3.2" class="ltx_Math" alttext="(240,120)" display="inline"><semantics id="S4.SS2.p1.3.m3.2a"><mrow id="S4.SS2.p1.3.m3.2.3.2" xref="S4.SS2.p1.3.m3.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p1.3.m3.2.3.2.1" xref="S4.SS2.p1.3.m3.2.3.1.cmml">(</mo><mn id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">240</mn><mo id="S4.SS2.p1.3.m3.2.3.2.2" xref="S4.SS2.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.3.m3.2.2" xref="S4.SS2.p1.3.m3.2.2.cmml">120</mn><mo stretchy="false" id="S4.SS2.p1.3.m3.2.3.2.3" xref="S4.SS2.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.2b"><interval closure="open" id="S4.SS2.p1.3.m3.2.3.1.cmml" xref="S4.SS2.p1.3.m3.2.3.2"><cn type="integer" id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">240</cn><cn type="integer" id="S4.SS2.p1.3.m3.2.2.cmml" xref="S4.SS2.p1.3.m3.2.2">120</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.2c">(240,120)</annotation></semantics></math> to <math id="S4.SS2.p1.4.m4.2" class="ltx_Math" alttext="(280,120)" display="inline"><semantics id="S4.SS2.p1.4.m4.2a"><mrow id="S4.SS2.p1.4.m4.2.3.2" xref="S4.SS2.p1.4.m4.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p1.4.m4.2.3.2.1" xref="S4.SS2.p1.4.m4.2.3.1.cmml">(</mo><mn id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">280</mn><mo id="S4.SS2.p1.4.m4.2.3.2.2" xref="S4.SS2.p1.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.4.m4.2.2" xref="S4.SS2.p1.4.m4.2.2.cmml">120</mn><mo stretchy="false" id="S4.SS2.p1.4.m4.2.3.2.3" xref="S4.SS2.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.2b"><interval closure="open" id="S4.SS2.p1.4.m4.2.3.1.cmml" xref="S4.SS2.p1.4.m4.2.3.2"><cn type="integer" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">280</cn><cn type="integer" id="S4.SS2.p1.4.m4.2.2.cmml" xref="S4.SS2.p1.4.m4.2.2">120</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.2c">(280,120)</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.4" class="ltx_p">Contrast, brightness, rotation and translation are randomized with the same parameters for both images within a pair. Images are augmented and models are trained and evaluated using TensorFlow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, version 1.12. Random image rotation is applied with an angle in the range <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\pm 10^{\circ}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mo id="S4.SS2.p2.1.m1.1.1a" xref="S4.SS2.p2.1.m1.1.1.cmml">±</mo><msup id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2.2" xref="S4.SS2.p2.1.m1.1.1.2.2.cmml">10</mn><mo id="S4.SS2.p2.1.m1.1.1.2.3" xref="S4.SS2.p2.1.m1.1.1.2.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.p2.1.m1.1.1.2">superscript</csymbol><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2.2">10</cn><compose id="S4.SS2.p2.1.m1.1.1.2.3.cmml" xref="S4.SS2.p2.1.m1.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\pm 10^{\circ}</annotation></semantics></math>, and random image translation is applied with a shift in the range <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\pm 10\%" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mo id="S4.SS2.p2.2.m2.1.1a" xref="S4.SS2.p2.2.m2.1.1.cmml">±</mo><mrow id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2.2" xref="S4.SS2.p2.2.m2.1.1.2.2.cmml">10</mn><mo id="S4.SS2.p2.2.m2.1.1.2.1" xref="S4.SS2.p2.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">plus-or-minus</csymbol><apply id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2.1">percent</csymbol><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\pm 10\%</annotation></semantics></math> of image size. We choose <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="D\textsubscript{max}=1.7m" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mrow id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2.2" xref="S4.SS2.p2.3.m3.1.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.2.1" xref="S4.SS2.p2.3.m3.1.1.2.1.cmml">​</mo><mtext id="S4.SS2.p2.3.m3.1.1.2.3" xref="S4.SS2.p2.3.m3.1.1.2.3b.cmml"><sub id="S4.SS2.p2.3.m3.1.1.2.3.1nest" class="ltx_sub">max</sub></mtext></mrow><mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml"><mn id="S4.SS2.p2.3.m3.1.1.3.2" xref="S4.SS2.p2.3.m3.1.1.3.2.cmml">1.7</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.3.1" xref="S4.SS2.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><eq id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></eq><apply id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2"><times id="S4.SS2.p2.3.m3.1.1.2.1.cmml" xref="S4.SS2.p2.3.m3.1.1.2.1"></times><ci id="S4.SS2.p2.3.m3.1.1.2.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2.2">𝐷</ci><ci id="S4.SS2.p2.3.m3.1.1.2.3b.cmml" xref="S4.SS2.p2.3.m3.1.1.2.3"><mtext id="S4.SS2.p2.3.m3.1.1.2.3.cmml" xref="S4.SS2.p2.3.m3.1.1.2.3"><sub id="S4.SS2.p2.3.m3.1.1.2.3.1anest" class="ltx_sub">max</sub></mtext></ci></apply><apply id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3"><times id="S4.SS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.1"></times><cn type="float" id="S4.SS2.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.2">1.7</cn><ci id="S4.SS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">D\textsubscript{max}=1.7m</annotation></semantics></math> to constrain the distance between nonconsecutive frames within a single training image pair, accounting for the maximum distance between frames in the testing set with an additional margin of <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mn id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><cn type="integer" id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">20</annotation></semantics></math>cm.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We follow the strategy of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and use the KITTI outdoor dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to train and evaluate our approach using real data. Specifically, we use image sequences 01, 03, 04, 05, 06, 07, 09 and 10 for training and sequences 00, 02 and 08 for testing. Ground-truth distances between camera centers are estimated separately for the left and right-hand cameras while taking into account the different camera offsets. For the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the total number of input real training image pairs before random augmentation is approximately 100 K. During evaluation, we use a total of approximately 26 K consecutive testing image pairs both the left and right-hand cameras.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.68.68" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.68.68.69.1" class="ltx_tr">
<th id="S4.T2.68.68.69.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T2.68.68.69.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T2.68.68.69.1.3" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<td id="S4.T2.68.68.69.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Seq #00</td>
<td id="S4.T2.68.68.69.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Seq #02</td>
<td id="S4.T2.68.68.69.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Seq #08</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<th id="S4.T2.6.6.6.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">#</th>
<th id="S4.T2.6.6.6.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Method</th>
<th id="S4.T2.6.6.6.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Training Data</th>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\mu</annotation></semantics></math></td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mi id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\sigma</annotation></semantics></math></td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mi id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\mu</annotation></semantics></math></td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T2.4.4.4.4.m1.1a"><mi id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">\sigma</annotation></semantics></math></td>
<td id="S4.T2.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.5.5.5.5.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T2.5.5.5.5.m1.1a"><mi id="S4.T2.5.5.5.5.m1.1.1" xref="S4.T2.5.5.5.5.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.5.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.m1.1c">\mu</annotation></semantics></math></td>
<td id="S4.T2.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.6.6.6.6.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T2.6.6.6.6.m1.1a"><mi id="S4.T2.6.6.6.6.m1.1.1" xref="S4.T2.6.6.6.6.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.6.m1.1b"><ci id="S4.T2.6.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.6.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.6.m1.1c">\sigma</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.12.12.12" class="ltx_tr">
<th id="S4.T2.12.12.12.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">1</th>
<th id="S4.T2.12.12.12.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Fixed height <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<th id="S4.T2.12.12.12.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">KITTI only</th>
<td id="S4.T2.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.7.7.7.1.m1.1" class="ltx_Math" alttext="0.072" display="inline"><semantics id="S4.T2.7.7.7.1.m1.1a"><mn id="S4.T2.7.7.7.1.m1.1.1" xref="S4.T2.7.7.7.1.m1.1.1.cmml">0.072</mn><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.1b"><cn type="float" id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1">0.072</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.1c">0.072</annotation></semantics></math></td>
<td id="S4.T2.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.8.8.8.2.m1.1" class="ltx_Math" alttext="0.252" display="inline"><semantics id="S4.T2.8.8.8.2.m1.1a"><mn id="S4.T2.8.8.8.2.m1.1.1" xref="S4.T2.8.8.8.2.m1.1.1.cmml">0.252</mn><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.2.m1.1b"><cn type="float" id="S4.T2.8.8.8.2.m1.1.1.cmml" xref="S4.T2.8.8.8.2.m1.1.1">0.252</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.2.m1.1c">0.252</annotation></semantics></math></td>
<td id="S4.T2.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.9.9.9.3.m1.1" class="ltx_Math" alttext="-0.012" display="inline"><semantics id="S4.T2.9.9.9.3.m1.1a"><mrow id="S4.T2.9.9.9.3.m1.1.1" xref="S4.T2.9.9.9.3.m1.1.1.cmml"><mo id="S4.T2.9.9.9.3.m1.1.1a" xref="S4.T2.9.9.9.3.m1.1.1.cmml">−</mo><mn id="S4.T2.9.9.9.3.m1.1.1.2" xref="S4.T2.9.9.9.3.m1.1.1.2.cmml">0.012</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.3.m1.1b"><apply id="S4.T2.9.9.9.3.m1.1.1.cmml" xref="S4.T2.9.9.9.3.m1.1.1"><minus id="S4.T2.9.9.9.3.m1.1.1.1.cmml" xref="S4.T2.9.9.9.3.m1.1.1"></minus><cn type="float" id="S4.T2.9.9.9.3.m1.1.1.2.cmml" xref="S4.T2.9.9.9.3.m1.1.1.2">0.012</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.3.m1.1c">-0.012</annotation></semantics></math></td>
<td id="S4.T2.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.10.10.10.4.m1.1" class="ltx_Math" alttext="0.160" display="inline"><semantics id="S4.T2.10.10.10.4.m1.1a"><mn id="S4.T2.10.10.10.4.m1.1.1" xref="S4.T2.10.10.10.4.m1.1.1.cmml">0.160</mn><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.4.m1.1b"><cn type="float" id="S4.T2.10.10.10.4.m1.1.1.cmml" xref="S4.T2.10.10.10.4.m1.1.1">0.160</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.4.m1.1c">0.160</annotation></semantics></math></td>
<td id="S4.T2.11.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.11.11.11.5.m1.1" class="ltx_Math" alttext="0.154" display="inline"><semantics id="S4.T2.11.11.11.5.m1.1a"><mn id="S4.T2.11.11.11.5.m1.1.1" xref="S4.T2.11.11.11.5.m1.1.1.cmml">0.154</mn><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.5.m1.1b"><cn type="float" id="S4.T2.11.11.11.5.m1.1.1.cmml" xref="S4.T2.11.11.11.5.m1.1.1">0.154</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.5.m1.1c">0.154</annotation></semantics></math></td>
<td id="S4.T2.12.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.12.12.12.6.m1.1" class="ltx_Math" alttext="0.349" display="inline"><semantics id="S4.T2.12.12.12.6.m1.1a"><mn id="S4.T2.12.12.12.6.m1.1.1" xref="S4.T2.12.12.12.6.m1.1.1.cmml">0.349</mn><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.6.m1.1b"><cn type="float" id="S4.T2.12.12.12.6.m1.1.1.cmml" xref="S4.T2.12.12.12.6.m1.1.1">0.349</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.6.m1.1c">0.349</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.18.18.18" class="ltx_tr">
<th id="S4.T2.18.18.18.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2</th>
<th id="S4.T2.18.18.18.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Frost <em id="S4.T2.18.18.18.8.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.T2.18.18.18.8.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, CNN alone</th>
<th id="S4.T2.18.18.18.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">KITTI only</th>
<td id="S4.T2.13.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.13.13.13.1.m1.1" class="ltx_Math" alttext="-0.014" display="inline"><semantics id="S4.T2.13.13.13.1.m1.1a"><mrow id="S4.T2.13.13.13.1.m1.1.1" xref="S4.T2.13.13.13.1.m1.1.1.cmml"><mo id="S4.T2.13.13.13.1.m1.1.1a" xref="S4.T2.13.13.13.1.m1.1.1.cmml">−</mo><mn id="S4.T2.13.13.13.1.m1.1.1.2" xref="S4.T2.13.13.13.1.m1.1.1.2.cmml">0.014</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.13.1.m1.1b"><apply id="S4.T2.13.13.13.1.m1.1.1.cmml" xref="S4.T2.13.13.13.1.m1.1.1"><minus id="S4.T2.13.13.13.1.m1.1.1.1.cmml" xref="S4.T2.13.13.13.1.m1.1.1"></minus><cn type="float" id="S4.T2.13.13.13.1.m1.1.1.2.cmml" xref="S4.T2.13.13.13.1.m1.1.1.2">0.014</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.13.1.m1.1c">-0.014</annotation></semantics></math></td>
<td id="S4.T2.14.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.14.14.14.2.m1.1" class="ltx_Math" alttext="0.177" display="inline"><semantics id="S4.T2.14.14.14.2.m1.1a"><mn id="S4.T2.14.14.14.2.m1.1.1" xref="S4.T2.14.14.14.2.m1.1.1.cmml">0.177</mn><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.14.2.m1.1b"><cn type="float" id="S4.T2.14.14.14.2.m1.1.1.cmml" xref="S4.T2.14.14.14.2.m1.1.1">0.177</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.14.2.m1.1c">0.177</annotation></semantics></math></td>
<td id="S4.T2.15.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.15.15.15.3.m1.1" class="ltx_Math" alttext="-0.018" display="inline"><semantics id="S4.T2.15.15.15.3.m1.1a"><mrow id="S4.T2.15.15.15.3.m1.1.1" xref="S4.T2.15.15.15.3.m1.1.1.cmml"><mo id="S4.T2.15.15.15.3.m1.1.1a" xref="S4.T2.15.15.15.3.m1.1.1.cmml">−</mo><mn id="S4.T2.15.15.15.3.m1.1.1.2" xref="S4.T2.15.15.15.3.m1.1.1.2.cmml">0.018</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.15.3.m1.1b"><apply id="S4.T2.15.15.15.3.m1.1.1.cmml" xref="S4.T2.15.15.15.3.m1.1.1"><minus id="S4.T2.15.15.15.3.m1.1.1.1.cmml" xref="S4.T2.15.15.15.3.m1.1.1"></minus><cn type="float" id="S4.T2.15.15.15.3.m1.1.1.2.cmml" xref="S4.T2.15.15.15.3.m1.1.1.2">0.018</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.15.3.m1.1c">-0.018</annotation></semantics></math></td>
<td id="S4.T2.16.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.16.16.16.4.m1.1" class="ltx_Math" alttext="0.203" display="inline"><semantics id="S4.T2.16.16.16.4.m1.1a"><mn id="S4.T2.16.16.16.4.m1.1.1" xref="S4.T2.16.16.16.4.m1.1.1.cmml">0.203</mn><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.16.4.m1.1b"><cn type="float" id="S4.T2.16.16.16.4.m1.1.1.cmml" xref="S4.T2.16.16.16.4.m1.1.1">0.203</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.16.4.m1.1c">0.203</annotation></semantics></math></td>
<td id="S4.T2.17.17.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.17.17.17.5.m1.1" class="ltx_Math" alttext="-0.004" display="inline"><semantics id="S4.T2.17.17.17.5.m1.1a"><mrow id="S4.T2.17.17.17.5.m1.1.1" xref="S4.T2.17.17.17.5.m1.1.1.cmml"><mo id="S4.T2.17.17.17.5.m1.1.1a" xref="S4.T2.17.17.17.5.m1.1.1.cmml">−</mo><mn id="S4.T2.17.17.17.5.m1.1.1.2" xref="S4.T2.17.17.17.5.m1.1.1.2.cmml">0.004</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.17.5.m1.1b"><apply id="S4.T2.17.17.17.5.m1.1.1.cmml" xref="S4.T2.17.17.17.5.m1.1.1"><minus id="S4.T2.17.17.17.5.m1.1.1.1.cmml" xref="S4.T2.17.17.17.5.m1.1.1"></minus><cn type="float" id="S4.T2.17.17.17.5.m1.1.1.2.cmml" xref="S4.T2.17.17.17.5.m1.1.1.2">0.004</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.17.5.m1.1c">-0.004</annotation></semantics></math></td>
<td id="S4.T2.18.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.18.18.18.6.m1.1" class="ltx_Math" alttext="0.165" display="inline"><semantics id="S4.T2.18.18.18.6.m1.1a"><mn id="S4.T2.18.18.18.6.m1.1.1" xref="S4.T2.18.18.18.6.m1.1.1.cmml">0.165</mn><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.18.6.m1.1b"><cn type="float" id="S4.T2.18.18.18.6.m1.1.1.cmml" xref="S4.T2.18.18.18.6.m1.1.1">0.165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.18.6.m1.1c">0.165</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.25.25.25" class="ltx_tr">
<th id="S4.T2.25.25.25.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">3</th>
<th id="S4.T2.19.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Frost <em id="S4.T2.19.19.19.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.T2.19.19.19.1.2" class="ltx_text"></span> (our impl., <math id="S4.T2.19.19.19.1.m1.1" class="ltx_Math" alttext="240x120" display="inline"><semantics id="S4.T2.19.19.19.1.m1.1a"><mrow id="S4.T2.19.19.19.1.m1.1.1" xref="S4.T2.19.19.19.1.m1.1.1.cmml"><mn id="S4.T2.19.19.19.1.m1.1.1.2" xref="S4.T2.19.19.19.1.m1.1.1.2.cmml">240</mn><mo lspace="0em" rspace="0em" id="S4.T2.19.19.19.1.m1.1.1.1" xref="S4.T2.19.19.19.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.19.19.19.1.m1.1.1.3" xref="S4.T2.19.19.19.1.m1.1.1.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.T2.19.19.19.1.m1.1.1.1a" xref="S4.T2.19.19.19.1.m1.1.1.1.cmml">​</mo><mn id="S4.T2.19.19.19.1.m1.1.1.4" xref="S4.T2.19.19.19.1.m1.1.1.4.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.19.1.m1.1b"><apply id="S4.T2.19.19.19.1.m1.1.1.cmml" xref="S4.T2.19.19.19.1.m1.1.1"><times id="S4.T2.19.19.19.1.m1.1.1.1.cmml" xref="S4.T2.19.19.19.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.19.19.19.1.m1.1.1.2.cmml" xref="S4.T2.19.19.19.1.m1.1.1.2">240</cn><ci id="S4.T2.19.19.19.1.m1.1.1.3.cmml" xref="S4.T2.19.19.19.1.m1.1.1.3">𝑥</ci><cn type="integer" id="S4.T2.19.19.19.1.m1.1.1.4.cmml" xref="S4.T2.19.19.19.1.m1.1.1.4">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.19.1.m1.1c">240x120</annotation></semantics></math>)</th>
<th id="S4.T2.25.25.25.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">KITTI only</th>
<td id="S4.T2.20.20.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.20.20.20.2.m1.1" class="ltx_Math" alttext="-0.009" display="inline"><semantics id="S4.T2.20.20.20.2.m1.1a"><mrow id="S4.T2.20.20.20.2.m1.1.1" xref="S4.T2.20.20.20.2.m1.1.1.cmml"><mo id="S4.T2.20.20.20.2.m1.1.1a" xref="S4.T2.20.20.20.2.m1.1.1.cmml">−</mo><mn id="S4.T2.20.20.20.2.m1.1.1.2" xref="S4.T2.20.20.20.2.m1.1.1.2.cmml">0.009</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.20.2.m1.1b"><apply id="S4.T2.20.20.20.2.m1.1.1.cmml" xref="S4.T2.20.20.20.2.m1.1.1"><minus id="S4.T2.20.20.20.2.m1.1.1.1.cmml" xref="S4.T2.20.20.20.2.m1.1.1"></minus><cn type="float" id="S4.T2.20.20.20.2.m1.1.1.2.cmml" xref="S4.T2.20.20.20.2.m1.1.1.2">0.009</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.20.2.m1.1c">-0.009</annotation></semantics></math></td>
<td id="S4.T2.21.21.21.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.21.21.21.3.m1.1" class="ltx_Math" alttext="0.177" display="inline"><semantics id="S4.T2.21.21.21.3.m1.1a"><mn id="S4.T2.21.21.21.3.m1.1.1" xref="S4.T2.21.21.21.3.m1.1.1.cmml">0.177</mn><annotation-xml encoding="MathML-Content" id="S4.T2.21.21.21.3.m1.1b"><cn type="float" id="S4.T2.21.21.21.3.m1.1.1.cmml" xref="S4.T2.21.21.21.3.m1.1.1">0.177</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.21.21.3.m1.1c">0.177</annotation></semantics></math></td>
<td id="S4.T2.22.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.22.22.22.4.m1.1" class="ltx_Math" alttext="0.013" display="inline"><semantics id="S4.T2.22.22.22.4.m1.1a"><mn id="S4.T2.22.22.22.4.m1.1.1" xref="S4.T2.22.22.22.4.m1.1.1.cmml">0.013</mn><annotation-xml encoding="MathML-Content" id="S4.T2.22.22.22.4.m1.1b"><cn type="float" id="S4.T2.22.22.22.4.m1.1.1.cmml" xref="S4.T2.22.22.22.4.m1.1.1">0.013</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.22.22.4.m1.1c">0.013</annotation></semantics></math></td>
<td id="S4.T2.23.23.23.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.23.23.23.5.m1.1" class="ltx_Math" alttext="0.180" display="inline"><semantics id="S4.T2.23.23.23.5.m1.1a"><mn id="S4.T2.23.23.23.5.m1.1.1" xref="S4.T2.23.23.23.5.m1.1.1.cmml">0.180</mn><annotation-xml encoding="MathML-Content" id="S4.T2.23.23.23.5.m1.1b"><cn type="float" id="S4.T2.23.23.23.5.m1.1.1.cmml" xref="S4.T2.23.23.23.5.m1.1.1">0.180</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.23.23.5.m1.1c">0.180</annotation></semantics></math></td>
<td id="S4.T2.24.24.24.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.24.24.24.6.m1.1" class="ltx_Math" alttext="-0.061" display="inline"><semantics id="S4.T2.24.24.24.6.m1.1a"><mrow id="S4.T2.24.24.24.6.m1.1.1" xref="S4.T2.24.24.24.6.m1.1.1.cmml"><mo id="S4.T2.24.24.24.6.m1.1.1a" xref="S4.T2.24.24.24.6.m1.1.1.cmml">−</mo><mn id="S4.T2.24.24.24.6.m1.1.1.2" xref="S4.T2.24.24.24.6.m1.1.1.2.cmml">0.061</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.24.24.24.6.m1.1b"><apply id="S4.T2.24.24.24.6.m1.1.1.cmml" xref="S4.T2.24.24.24.6.m1.1.1"><minus id="S4.T2.24.24.24.6.m1.1.1.1.cmml" xref="S4.T2.24.24.24.6.m1.1.1"></minus><cn type="float" id="S4.T2.24.24.24.6.m1.1.1.2.cmml" xref="S4.T2.24.24.24.6.m1.1.1.2">0.061</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.24.24.6.m1.1c">-0.061</annotation></semantics></math></td>
<td id="S4.T2.25.25.25.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.25.25.25.7.m1.1" class="ltx_Math" alttext="0.152" display="inline"><semantics id="S4.T2.25.25.25.7.m1.1a"><mn id="S4.T2.25.25.25.7.m1.1.1" xref="S4.T2.25.25.25.7.m1.1.1.cmml">0.152</mn><annotation-xml encoding="MathML-Content" id="S4.T2.25.25.25.7.m1.1b"><cn type="float" id="S4.T2.25.25.25.7.m1.1.1.cmml" xref="S4.T2.25.25.25.7.m1.1.1">0.152</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.25.25.7.m1.1c">0.152</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.32.32.32" class="ltx_tr">
<th id="S4.T2.32.32.32.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">4</th>
<th id="S4.T2.26.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Frost <em id="S4.T2.26.26.26.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.T2.26.26.26.1.2" class="ltx_text"></span> (our impl., <math id="S4.T2.26.26.26.1.m1.1" class="ltx_Math" alttext="280x120" display="inline"><semantics id="S4.T2.26.26.26.1.m1.1a"><mrow id="S4.T2.26.26.26.1.m1.1.1" xref="S4.T2.26.26.26.1.m1.1.1.cmml"><mn id="S4.T2.26.26.26.1.m1.1.1.2" xref="S4.T2.26.26.26.1.m1.1.1.2.cmml">280</mn><mo lspace="0em" rspace="0em" id="S4.T2.26.26.26.1.m1.1.1.1" xref="S4.T2.26.26.26.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.26.26.26.1.m1.1.1.3" xref="S4.T2.26.26.26.1.m1.1.1.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.T2.26.26.26.1.m1.1.1.1a" xref="S4.T2.26.26.26.1.m1.1.1.1.cmml">​</mo><mn id="S4.T2.26.26.26.1.m1.1.1.4" xref="S4.T2.26.26.26.1.m1.1.1.4.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.26.26.26.1.m1.1b"><apply id="S4.T2.26.26.26.1.m1.1.1.cmml" xref="S4.T2.26.26.26.1.m1.1.1"><times id="S4.T2.26.26.26.1.m1.1.1.1.cmml" xref="S4.T2.26.26.26.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.26.26.26.1.m1.1.1.2.cmml" xref="S4.T2.26.26.26.1.m1.1.1.2">280</cn><ci id="S4.T2.26.26.26.1.m1.1.1.3.cmml" xref="S4.T2.26.26.26.1.m1.1.1.3">𝑥</ci><cn type="integer" id="S4.T2.26.26.26.1.m1.1.1.4.cmml" xref="S4.T2.26.26.26.1.m1.1.1.4">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.26.26.1.m1.1c">280x120</annotation></semantics></math>)</th>
<th id="S4.T2.32.32.32.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">KITTI only</th>
<td id="S4.T2.27.27.27.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.27.27.27.2.m1.1" class="ltx_Math" alttext="-0.015" display="inline"><semantics id="S4.T2.27.27.27.2.m1.1a"><mrow id="S4.T2.27.27.27.2.m1.1.1" xref="S4.T2.27.27.27.2.m1.1.1.cmml"><mo id="S4.T2.27.27.27.2.m1.1.1a" xref="S4.T2.27.27.27.2.m1.1.1.cmml">−</mo><mn id="S4.T2.27.27.27.2.m1.1.1.2" xref="S4.T2.27.27.27.2.m1.1.1.2.cmml">0.015</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.27.27.27.2.m1.1b"><apply id="S4.T2.27.27.27.2.m1.1.1.cmml" xref="S4.T2.27.27.27.2.m1.1.1"><minus id="S4.T2.27.27.27.2.m1.1.1.1.cmml" xref="S4.T2.27.27.27.2.m1.1.1"></minus><cn type="float" id="S4.T2.27.27.27.2.m1.1.1.2.cmml" xref="S4.T2.27.27.27.2.m1.1.1.2">0.015</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.27.27.2.m1.1c">-0.015</annotation></semantics></math></td>
<td id="S4.T2.28.28.28.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.28.28.28.3.m1.1" class="ltx_Math" alttext="0.175" display="inline"><semantics id="S4.T2.28.28.28.3.m1.1a"><mn id="S4.T2.28.28.28.3.m1.1.1" xref="S4.T2.28.28.28.3.m1.1.1.cmml">0.175</mn><annotation-xml encoding="MathML-Content" id="S4.T2.28.28.28.3.m1.1b"><cn type="float" id="S4.T2.28.28.28.3.m1.1.1.cmml" xref="S4.T2.28.28.28.3.m1.1.1">0.175</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.28.28.3.m1.1c">0.175</annotation></semantics></math></td>
<td id="S4.T2.29.29.29.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.29.29.29.4.m1.1" class="ltx_Math" alttext="-0.010" display="inline"><semantics id="S4.T2.29.29.29.4.m1.1a"><mrow id="S4.T2.29.29.29.4.m1.1.1" xref="S4.T2.29.29.29.4.m1.1.1.cmml"><mo id="S4.T2.29.29.29.4.m1.1.1a" xref="S4.T2.29.29.29.4.m1.1.1.cmml">−</mo><mn id="S4.T2.29.29.29.4.m1.1.1.2" xref="S4.T2.29.29.29.4.m1.1.1.2.cmml">0.010</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.29.29.29.4.m1.1b"><apply id="S4.T2.29.29.29.4.m1.1.1.cmml" xref="S4.T2.29.29.29.4.m1.1.1"><minus id="S4.T2.29.29.29.4.m1.1.1.1.cmml" xref="S4.T2.29.29.29.4.m1.1.1"></minus><cn type="float" id="S4.T2.29.29.29.4.m1.1.1.2.cmml" xref="S4.T2.29.29.29.4.m1.1.1.2">0.010</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.29.29.4.m1.1c">-0.010</annotation></semantics></math></td>
<td id="S4.T2.30.30.30.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.30.30.30.5.m1.1" class="ltx_Math" alttext="0.178" display="inline"><semantics id="S4.T2.30.30.30.5.m1.1a"><mn id="S4.T2.30.30.30.5.m1.1.1" xref="S4.T2.30.30.30.5.m1.1.1.cmml">0.178</mn><annotation-xml encoding="MathML-Content" id="S4.T2.30.30.30.5.m1.1b"><cn type="float" id="S4.T2.30.30.30.5.m1.1.1.cmml" xref="S4.T2.30.30.30.5.m1.1.1">0.178</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.30.30.5.m1.1c">0.178</annotation></semantics></math></td>
<td id="S4.T2.31.31.31.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.31.31.31.6.m1.1" class="ltx_Math" alttext="-0.057" display="inline"><semantics id="S4.T2.31.31.31.6.m1.1a"><mrow id="S4.T2.31.31.31.6.m1.1.1" xref="S4.T2.31.31.31.6.m1.1.1.cmml"><mo id="S4.T2.31.31.31.6.m1.1.1a" xref="S4.T2.31.31.31.6.m1.1.1.cmml">−</mo><mn id="S4.T2.31.31.31.6.m1.1.1.2" xref="S4.T2.31.31.31.6.m1.1.1.2.cmml">0.057</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.31.31.31.6.m1.1b"><apply id="S4.T2.31.31.31.6.m1.1.1.cmml" xref="S4.T2.31.31.31.6.m1.1.1"><minus id="S4.T2.31.31.31.6.m1.1.1.1.cmml" xref="S4.T2.31.31.31.6.m1.1.1"></minus><cn type="float" id="S4.T2.31.31.31.6.m1.1.1.2.cmml" xref="S4.T2.31.31.31.6.m1.1.1.2">0.057</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.31.31.31.6.m1.1c">-0.057</annotation></semantics></math></td>
<td id="S4.T2.32.32.32.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.32.32.32.7.m1.1" class="ltx_Math" alttext="0.149" display="inline"><semantics id="S4.T2.32.32.32.7.m1.1a"><mn id="S4.T2.32.32.32.7.m1.1.1" xref="S4.T2.32.32.32.7.m1.1.1.cmml">0.149</mn><annotation-xml encoding="MathML-Content" id="S4.T2.32.32.32.7.m1.1b"><cn type="float" id="S4.T2.32.32.32.7.m1.1.1.cmml" xref="S4.T2.32.32.32.7.m1.1.1">0.149</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.32.32.7.m1.1c">0.149</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.38.38.38" class="ltx_tr">
<th id="S4.T2.38.38.38.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">5</th>
<th id="S4.T2.38.38.38.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">CNN</th>
<th id="S4.T2.38.38.38.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">KITTI only</th>
<td id="S4.T2.33.33.33.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.33.33.33.1.m1.1" class="ltx_Math" alttext="0.009" display="inline"><semantics id="S4.T2.33.33.33.1.m1.1a"><mn id="S4.T2.33.33.33.1.m1.1.1" xref="S4.T2.33.33.33.1.m1.1.1.cmml">0.009</mn><annotation-xml encoding="MathML-Content" id="S4.T2.33.33.33.1.m1.1b"><cn type="float" id="S4.T2.33.33.33.1.m1.1.1.cmml" xref="S4.T2.33.33.33.1.m1.1.1">0.009</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.33.33.33.1.m1.1c">0.009</annotation></semantics></math></td>
<td id="S4.T2.34.34.34.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.34.34.34.2.m1.1" class="ltx_Math" alttext="0.107" display="inline"><semantics id="S4.T2.34.34.34.2.m1.1a"><mn id="S4.T2.34.34.34.2.m1.1.1" xref="S4.T2.34.34.34.2.m1.1.1.cmml">0.107</mn><annotation-xml encoding="MathML-Content" id="S4.T2.34.34.34.2.m1.1b"><cn type="float" id="S4.T2.34.34.34.2.m1.1.1.cmml" xref="S4.T2.34.34.34.2.m1.1.1">0.107</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.34.34.34.2.m1.1c">0.107</annotation></semantics></math></td>
<td id="S4.T2.35.35.35.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.35.35.35.3.m1.1" class="ltx_Math" alttext="0.023" display="inline"><semantics id="S4.T2.35.35.35.3.m1.1a"><mn id="S4.T2.35.35.35.3.m1.1.1" xref="S4.T2.35.35.35.3.m1.1.1.cmml">0.023</mn><annotation-xml encoding="MathML-Content" id="S4.T2.35.35.35.3.m1.1b"><cn type="float" id="S4.T2.35.35.35.3.m1.1.1.cmml" xref="S4.T2.35.35.35.3.m1.1.1">0.023</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.35.35.35.3.m1.1c">0.023</annotation></semantics></math></td>
<td id="S4.T2.36.36.36.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.36.36.36.4.m1.1" class="ltx_Math" alttext="0.113" display="inline"><semantics id="S4.T2.36.36.36.4.m1.1a"><mn id="S4.T2.36.36.36.4.m1.1.1" xref="S4.T2.36.36.36.4.m1.1.1.cmml">0.113</mn><annotation-xml encoding="MathML-Content" id="S4.T2.36.36.36.4.m1.1b"><cn type="float" id="S4.T2.36.36.36.4.m1.1.1.cmml" xref="S4.T2.36.36.36.4.m1.1.1">0.113</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.36.36.36.4.m1.1c">0.113</annotation></semantics></math></td>
<td id="S4.T2.37.37.37.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.37.37.37.5.m1.1" class="ltx_Math" alttext="-0.017" display="inline"><semantics id="S4.T2.37.37.37.5.m1.1a"><mrow id="S4.T2.37.37.37.5.m1.1.1" xref="S4.T2.37.37.37.5.m1.1.1.cmml"><mo id="S4.T2.37.37.37.5.m1.1.1a" xref="S4.T2.37.37.37.5.m1.1.1.cmml">−</mo><mn id="S4.T2.37.37.37.5.m1.1.1.2" xref="S4.T2.37.37.37.5.m1.1.1.2.cmml">0.017</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.37.37.37.5.m1.1b"><apply id="S4.T2.37.37.37.5.m1.1.1.cmml" xref="S4.T2.37.37.37.5.m1.1.1"><minus id="S4.T2.37.37.37.5.m1.1.1.1.cmml" xref="S4.T2.37.37.37.5.m1.1.1"></minus><cn type="float" id="S4.T2.37.37.37.5.m1.1.1.2.cmml" xref="S4.T2.37.37.37.5.m1.1.1.2">0.017</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.37.37.37.5.m1.1c">-0.017</annotation></semantics></math></td>
<td id="S4.T2.38.38.38.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.38.38.38.6.m1.1" class="ltx_Math" alttext="0.092" display="inline"><semantics id="S4.T2.38.38.38.6.m1.1a"><mn id="S4.T2.38.38.38.6.m1.1.1" xref="S4.T2.38.38.38.6.m1.1.1.cmml">0.092</mn><annotation-xml encoding="MathML-Content" id="S4.T2.38.38.38.6.m1.1b"><cn type="float" id="S4.T2.38.38.38.6.m1.1.1.cmml" xref="S4.T2.38.38.38.6.m1.1.1">0.092</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.38.38.38.6.m1.1c">0.092</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.44.44.44" class="ltx_tr">
<th id="S4.T2.44.44.44.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">6</th>
<th id="S4.T2.44.44.44.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">CNN</th>
<th id="S4.T2.44.44.44.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">CARLA only</th>
<td id="S4.T2.39.39.39.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.39.39.39.1.m1.1" class="ltx_Math" alttext="-0.017" display="inline"><semantics id="S4.T2.39.39.39.1.m1.1a"><mrow id="S4.T2.39.39.39.1.m1.1.1" xref="S4.T2.39.39.39.1.m1.1.1.cmml"><mo id="S4.T2.39.39.39.1.m1.1.1a" xref="S4.T2.39.39.39.1.m1.1.1.cmml">−</mo><mn id="S4.T2.39.39.39.1.m1.1.1.2" xref="S4.T2.39.39.39.1.m1.1.1.2.cmml">0.017</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.39.39.39.1.m1.1b"><apply id="S4.T2.39.39.39.1.m1.1.1.cmml" xref="S4.T2.39.39.39.1.m1.1.1"><minus id="S4.T2.39.39.39.1.m1.1.1.1.cmml" xref="S4.T2.39.39.39.1.m1.1.1"></minus><cn type="float" id="S4.T2.39.39.39.1.m1.1.1.2.cmml" xref="S4.T2.39.39.39.1.m1.1.1.2">0.017</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.39.39.39.1.m1.1c">-0.017</annotation></semantics></math></td>
<td id="S4.T2.40.40.40.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.40.40.40.2.m1.1" class="ltx_Math" alttext="0.111" display="inline"><semantics id="S4.T2.40.40.40.2.m1.1a"><mn id="S4.T2.40.40.40.2.m1.1.1" xref="S4.T2.40.40.40.2.m1.1.1.cmml">0.111</mn><annotation-xml encoding="MathML-Content" id="S4.T2.40.40.40.2.m1.1b"><cn type="float" id="S4.T2.40.40.40.2.m1.1.1.cmml" xref="S4.T2.40.40.40.2.m1.1.1">0.111</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.40.40.40.2.m1.1c">0.111</annotation></semantics></math></td>
<td id="S4.T2.41.41.41.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.41.41.41.3.m1.1" class="ltx_Math" alttext="0.023" display="inline"><semantics id="S4.T2.41.41.41.3.m1.1a"><mn id="S4.T2.41.41.41.3.m1.1.1" xref="S4.T2.41.41.41.3.m1.1.1.cmml">0.023</mn><annotation-xml encoding="MathML-Content" id="S4.T2.41.41.41.3.m1.1b"><cn type="float" id="S4.T2.41.41.41.3.m1.1.1.cmml" xref="S4.T2.41.41.41.3.m1.1.1">0.023</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.41.41.41.3.m1.1c">0.023</annotation></semantics></math></td>
<td id="S4.T2.42.42.42.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.42.42.42.4.m1.1" class="ltx_Math" alttext="0.105" display="inline"><semantics id="S4.T2.42.42.42.4.m1.1a"><mn id="S4.T2.42.42.42.4.m1.1.1" xref="S4.T2.42.42.42.4.m1.1.1.cmml">0.105</mn><annotation-xml encoding="MathML-Content" id="S4.T2.42.42.42.4.m1.1b"><cn type="float" id="S4.T2.42.42.42.4.m1.1.1.cmml" xref="S4.T2.42.42.42.4.m1.1.1">0.105</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.42.42.42.4.m1.1c">0.105</annotation></semantics></math></td>
<td id="S4.T2.43.43.43.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.43.43.43.5.m1.1" class="ltx_Math" alttext="-0.029" display="inline"><semantics id="S4.T2.43.43.43.5.m1.1a"><mrow id="S4.T2.43.43.43.5.m1.1.1" xref="S4.T2.43.43.43.5.m1.1.1.cmml"><mo id="S4.T2.43.43.43.5.m1.1.1a" xref="S4.T2.43.43.43.5.m1.1.1.cmml">−</mo><mn id="S4.T2.43.43.43.5.m1.1.1.2" xref="S4.T2.43.43.43.5.m1.1.1.2.cmml">0.029</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.43.43.43.5.m1.1b"><apply id="S4.T2.43.43.43.5.m1.1.1.cmml" xref="S4.T2.43.43.43.5.m1.1.1"><minus id="S4.T2.43.43.43.5.m1.1.1.1.cmml" xref="S4.T2.43.43.43.5.m1.1.1"></minus><cn type="float" id="S4.T2.43.43.43.5.m1.1.1.2.cmml" xref="S4.T2.43.43.43.5.m1.1.1.2">0.029</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.43.43.43.5.m1.1c">-0.029</annotation></semantics></math></td>
<td id="S4.T2.44.44.44.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.44.44.44.6.m1.1" class="ltx_Math" alttext="0.121" display="inline"><semantics id="S4.T2.44.44.44.6.m1.1a"><mn id="S4.T2.44.44.44.6.m1.1.1" xref="S4.T2.44.44.44.6.m1.1.1.cmml">0.121</mn><annotation-xml encoding="MathML-Content" id="S4.T2.44.44.44.6.m1.1b"><cn type="float" id="S4.T2.44.44.44.6.m1.1.1.cmml" xref="S4.T2.44.44.44.6.m1.1.1">0.121</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.44.44.44.6.m1.1c">0.121</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.50.50.50" class="ltx_tr">
<th id="S4.T2.50.50.50.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">7</th>
<th id="S4.T2.50.50.50.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">CNN</th>
<th id="S4.T2.50.50.50.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">CARLA and KITTI</th>
<td id="S4.T2.45.45.45.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.45.45.45.1.m1.1" class="ltx_Math" alttext="0.036" display="inline"><semantics id="S4.T2.45.45.45.1.m1.1a"><mn id="S4.T2.45.45.45.1.m1.1.1" xref="S4.T2.45.45.45.1.m1.1.1.cmml">0.036</mn><annotation-xml encoding="MathML-Content" id="S4.T2.45.45.45.1.m1.1b"><cn type="float" id="S4.T2.45.45.45.1.m1.1.1.cmml" xref="S4.T2.45.45.45.1.m1.1.1">0.036</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.45.45.45.1.m1.1c">0.036</annotation></semantics></math></td>
<td id="S4.T2.46.46.46.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.46.46.46.2.m1.1" class="ltx_Math" alttext="0.079" display="inline"><semantics id="S4.T2.46.46.46.2.m1.1a"><mn id="S4.T2.46.46.46.2.m1.1.1" xref="S4.T2.46.46.46.2.m1.1.1.cmml">0.079</mn><annotation-xml encoding="MathML-Content" id="S4.T2.46.46.46.2.m1.1b"><cn type="float" id="S4.T2.46.46.46.2.m1.1.1.cmml" xref="S4.T2.46.46.46.2.m1.1.1">0.079</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.46.46.46.2.m1.1c">0.079</annotation></semantics></math></td>
<td id="S4.T2.47.47.47.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.47.47.47.3.m1.1" class="ltx_Math" alttext="0.029" display="inline"><semantics id="S4.T2.47.47.47.3.m1.1a"><mn id="S4.T2.47.47.47.3.m1.1.1" xref="S4.T2.47.47.47.3.m1.1.1.cmml">0.029</mn><annotation-xml encoding="MathML-Content" id="S4.T2.47.47.47.3.m1.1b"><cn type="float" id="S4.T2.47.47.47.3.m1.1.1.cmml" xref="S4.T2.47.47.47.3.m1.1.1">0.029</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.47.47.47.3.m1.1c">0.029</annotation></semantics></math></td>
<td id="S4.T2.48.48.48.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.48.48.48.4.m1.1" class="ltx_Math" alttext="0.084" display="inline"><semantics id="S4.T2.48.48.48.4.m1.1a"><mn id="S4.T2.48.48.48.4.m1.1.1" xref="S4.T2.48.48.48.4.m1.1.1.cmml">0.084</mn><annotation-xml encoding="MathML-Content" id="S4.T2.48.48.48.4.m1.1b"><cn type="float" id="S4.T2.48.48.48.4.m1.1.1.cmml" xref="S4.T2.48.48.48.4.m1.1.1">0.084</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.48.48.48.4.m1.1c">0.084</annotation></semantics></math></td>
<td id="S4.T2.49.49.49.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.49.49.49.5.m1.1" class="ltx_Math" alttext="0.013" display="inline"><semantics id="S4.T2.49.49.49.5.m1.1a"><mn id="S4.T2.49.49.49.5.m1.1.1" xref="S4.T2.49.49.49.5.m1.1.1.cmml">0.013</mn><annotation-xml encoding="MathML-Content" id="S4.T2.49.49.49.5.m1.1b"><cn type="float" id="S4.T2.49.49.49.5.m1.1.1.cmml" xref="S4.T2.49.49.49.5.m1.1.1">0.013</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.49.49.49.5.m1.1c">0.013</annotation></semantics></math></td>
<td id="S4.T2.50.50.50.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.50.50.50.6.m1.1" class="ltx_Math" alttext="0.081" display="inline"><semantics id="S4.T2.50.50.50.6.m1.1a"><mn id="S4.T2.50.50.50.6.m1.1.1" xref="S4.T2.50.50.50.6.m1.1.1.cmml">0.081</mn><annotation-xml encoding="MathML-Content" id="S4.T2.50.50.50.6.m1.1b"><cn type="float" id="S4.T2.50.50.50.6.m1.1.1.cmml" xref="S4.T2.50.50.50.6.m1.1.1">0.081</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.50.50.50.6.m1.1c">0.081</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.56.56.56" class="ltx_tr">
<th id="S4.T2.56.56.56.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">8</th>
<th id="S4.T2.56.56.56.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">CNN, LSTM (B, 19)</th>
<th id="S4.T2.56.56.56.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">KITTI only</th>
<td id="S4.T2.51.51.51.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.51.51.51.1.m1.1" class="ltx_Math" alttext="0.019" display="inline"><semantics id="S4.T2.51.51.51.1.m1.1a"><mn id="S4.T2.51.51.51.1.m1.1.1" xref="S4.T2.51.51.51.1.m1.1.1.cmml">0.019</mn><annotation-xml encoding="MathML-Content" id="S4.T2.51.51.51.1.m1.1b"><cn type="float" id="S4.T2.51.51.51.1.m1.1.1.cmml" xref="S4.T2.51.51.51.1.m1.1.1">0.019</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.51.51.51.1.m1.1c">0.019</annotation></semantics></math></td>
<td id="S4.T2.52.52.52.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.52.52.52.2.m1.1" class="ltx_Math" alttext="\boldsymbol{0.069}" display="inline"><semantics id="S4.T2.52.52.52.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T2.52.52.52.2.m1.1.1" xref="S4.T2.52.52.52.2.m1.1.1.cmml">0.069</mn><annotation-xml encoding="MathML-Content" id="S4.T2.52.52.52.2.m1.1b"><cn type="float" id="S4.T2.52.52.52.2.m1.1.1.cmml" xref="S4.T2.52.52.52.2.m1.1.1">0.069</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.52.52.52.2.m1.1c">\boldsymbol{0.069}</annotation></semantics></math></td>
<td id="S4.T2.53.53.53.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.53.53.53.3.m1.1" class="ltx_Math" alttext="0.033" display="inline"><semantics id="S4.T2.53.53.53.3.m1.1a"><mn id="S4.T2.53.53.53.3.m1.1.1" xref="S4.T2.53.53.53.3.m1.1.1.cmml">0.033</mn><annotation-xml encoding="MathML-Content" id="S4.T2.53.53.53.3.m1.1b"><cn type="float" id="S4.T2.53.53.53.3.m1.1.1.cmml" xref="S4.T2.53.53.53.3.m1.1.1">0.033</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.53.53.53.3.m1.1c">0.033</annotation></semantics></math></td>
<td id="S4.T2.54.54.54.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.54.54.54.4.m1.1" class="ltx_Math" alttext="\boldsymbol{0.083}" display="inline"><semantics id="S4.T2.54.54.54.4.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T2.54.54.54.4.m1.1.1" xref="S4.T2.54.54.54.4.m1.1.1.cmml">0.083</mn><annotation-xml encoding="MathML-Content" id="S4.T2.54.54.54.4.m1.1b"><cn type="float" id="S4.T2.54.54.54.4.m1.1.1.cmml" xref="S4.T2.54.54.54.4.m1.1.1">0.083</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.54.54.54.4.m1.1c">\boldsymbol{0.083}</annotation></semantics></math></td>
<td id="S4.T2.55.55.55.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.55.55.55.5.m1.1" class="ltx_Math" alttext="-0.017" display="inline"><semantics id="S4.T2.55.55.55.5.m1.1a"><mrow id="S4.T2.55.55.55.5.m1.1.1" xref="S4.T2.55.55.55.5.m1.1.1.cmml"><mo id="S4.T2.55.55.55.5.m1.1.1a" xref="S4.T2.55.55.55.5.m1.1.1.cmml">−</mo><mn id="S4.T2.55.55.55.5.m1.1.1.2" xref="S4.T2.55.55.55.5.m1.1.1.2.cmml">0.017</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.55.55.55.5.m1.1b"><apply id="S4.T2.55.55.55.5.m1.1.1.cmml" xref="S4.T2.55.55.55.5.m1.1.1"><minus id="S4.T2.55.55.55.5.m1.1.1.1.cmml" xref="S4.T2.55.55.55.5.m1.1.1"></minus><cn type="float" id="S4.T2.55.55.55.5.m1.1.1.2.cmml" xref="S4.T2.55.55.55.5.m1.1.1.2">0.017</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.55.55.55.5.m1.1c">-0.017</annotation></semantics></math></td>
<td id="S4.T2.56.56.56.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.56.56.56.6.m1.1" class="ltx_Math" alttext="\boldsymbol{0.064}" display="inline"><semantics id="S4.T2.56.56.56.6.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T2.56.56.56.6.m1.1.1" xref="S4.T2.56.56.56.6.m1.1.1.cmml">0.064</mn><annotation-xml encoding="MathML-Content" id="S4.T2.56.56.56.6.m1.1b"><cn type="float" id="S4.T2.56.56.56.6.m1.1.1.cmml" xref="S4.T2.56.56.56.6.m1.1.1">0.064</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.56.56.56.6.m1.1c">\boldsymbol{0.064}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.62.62.62" class="ltx_tr">
<th id="S4.T2.62.62.62.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">9</th>
<th id="S4.T2.62.62.62.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">CNN, LSTM (B, 19)</th>
<th id="S4.T2.62.62.62.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">CARLA only</th>
<td id="S4.T2.57.57.57.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.57.57.57.1.m1.1" class="ltx_Math" alttext="-0.004" display="inline"><semantics id="S4.T2.57.57.57.1.m1.1a"><mrow id="S4.T2.57.57.57.1.m1.1.1" xref="S4.T2.57.57.57.1.m1.1.1.cmml"><mo id="S4.T2.57.57.57.1.m1.1.1a" xref="S4.T2.57.57.57.1.m1.1.1.cmml">−</mo><mn id="S4.T2.57.57.57.1.m1.1.1.2" xref="S4.T2.57.57.57.1.m1.1.1.2.cmml">0.004</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.57.57.57.1.m1.1b"><apply id="S4.T2.57.57.57.1.m1.1.1.cmml" xref="S4.T2.57.57.57.1.m1.1.1"><minus id="S4.T2.57.57.57.1.m1.1.1.1.cmml" xref="S4.T2.57.57.57.1.m1.1.1"></minus><cn type="float" id="S4.T2.57.57.57.1.m1.1.1.2.cmml" xref="S4.T2.57.57.57.1.m1.1.1.2">0.004</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.57.57.57.1.m1.1c">-0.004</annotation></semantics></math></td>
<td id="S4.T2.58.58.58.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.58.58.58.2.m1.1" class="ltx_Math" alttext="0.102" display="inline"><semantics id="S4.T2.58.58.58.2.m1.1a"><mn id="S4.T2.58.58.58.2.m1.1.1" xref="S4.T2.58.58.58.2.m1.1.1.cmml">0.102</mn><annotation-xml encoding="MathML-Content" id="S4.T2.58.58.58.2.m1.1b"><cn type="float" id="S4.T2.58.58.58.2.m1.1.1.cmml" xref="S4.T2.58.58.58.2.m1.1.1">0.102</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.58.58.58.2.m1.1c">0.102</annotation></semantics></math></td>
<td id="S4.T2.59.59.59.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.59.59.59.3.m1.1" class="ltx_Math" alttext="-0.003" display="inline"><semantics id="S4.T2.59.59.59.3.m1.1a"><mrow id="S4.T2.59.59.59.3.m1.1.1" xref="S4.T2.59.59.59.3.m1.1.1.cmml"><mo id="S4.T2.59.59.59.3.m1.1.1a" xref="S4.T2.59.59.59.3.m1.1.1.cmml">−</mo><mn id="S4.T2.59.59.59.3.m1.1.1.2" xref="S4.T2.59.59.59.3.m1.1.1.2.cmml">0.003</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.59.59.59.3.m1.1b"><apply id="S4.T2.59.59.59.3.m1.1.1.cmml" xref="S4.T2.59.59.59.3.m1.1.1"><minus id="S4.T2.59.59.59.3.m1.1.1.1.cmml" xref="S4.T2.59.59.59.3.m1.1.1"></minus><cn type="float" id="S4.T2.59.59.59.3.m1.1.1.2.cmml" xref="S4.T2.59.59.59.3.m1.1.1.2">0.003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.59.59.59.3.m1.1c">-0.003</annotation></semantics></math></td>
<td id="S4.T2.60.60.60.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.60.60.60.4.m1.1" class="ltx_Math" alttext="0.132" display="inline"><semantics id="S4.T2.60.60.60.4.m1.1a"><mn id="S4.T2.60.60.60.4.m1.1.1" xref="S4.T2.60.60.60.4.m1.1.1.cmml">0.132</mn><annotation-xml encoding="MathML-Content" id="S4.T2.60.60.60.4.m1.1b"><cn type="float" id="S4.T2.60.60.60.4.m1.1.1.cmml" xref="S4.T2.60.60.60.4.m1.1.1">0.132</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.60.60.60.4.m1.1c">0.132</annotation></semantics></math></td>
<td id="S4.T2.61.61.61.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.61.61.61.5.m1.1" class="ltx_Math" alttext="-0.049" display="inline"><semantics id="S4.T2.61.61.61.5.m1.1a"><mrow id="S4.T2.61.61.61.5.m1.1.1" xref="S4.T2.61.61.61.5.m1.1.1.cmml"><mo id="S4.T2.61.61.61.5.m1.1.1a" xref="S4.T2.61.61.61.5.m1.1.1.cmml">−</mo><mn id="S4.T2.61.61.61.5.m1.1.1.2" xref="S4.T2.61.61.61.5.m1.1.1.2.cmml">0.049</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.61.61.61.5.m1.1b"><apply id="S4.T2.61.61.61.5.m1.1.1.cmml" xref="S4.T2.61.61.61.5.m1.1.1"><minus id="S4.T2.61.61.61.5.m1.1.1.1.cmml" xref="S4.T2.61.61.61.5.m1.1.1"></minus><cn type="float" id="S4.T2.61.61.61.5.m1.1.1.2.cmml" xref="S4.T2.61.61.61.5.m1.1.1.2">0.049</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.61.61.61.5.m1.1c">-0.049</annotation></semantics></math></td>
<td id="S4.T2.62.62.62.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.62.62.62.6.m1.1" class="ltx_Math" alttext="0.128" display="inline"><semantics id="S4.T2.62.62.62.6.m1.1a"><mn id="S4.T2.62.62.62.6.m1.1.1" xref="S4.T2.62.62.62.6.m1.1.1.cmml">0.128</mn><annotation-xml encoding="MathML-Content" id="S4.T2.62.62.62.6.m1.1b"><cn type="float" id="S4.T2.62.62.62.6.m1.1.1.cmml" xref="S4.T2.62.62.62.6.m1.1.1">0.128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.62.62.62.6.m1.1c">0.128</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.68.68.68" class="ltx_tr">
<th id="S4.T2.68.68.68.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">10</th>
<th id="S4.T2.68.68.68.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">CNN, LSTM (B, 19)</th>
<th id="S4.T2.68.68.68.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">CARLA and KITTI</th>
<td id="S4.T2.63.63.63.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.63.63.63.1.m1.1" class="ltx_Math" alttext="0.039" display="inline"><semantics id="S4.T2.63.63.63.1.m1.1a"><mn id="S4.T2.63.63.63.1.m1.1.1" xref="S4.T2.63.63.63.1.m1.1.1.cmml">0.039</mn><annotation-xml encoding="MathML-Content" id="S4.T2.63.63.63.1.m1.1b"><cn type="float" id="S4.T2.63.63.63.1.m1.1.1.cmml" xref="S4.T2.63.63.63.1.m1.1.1">0.039</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.63.63.63.1.m1.1c">0.039</annotation></semantics></math></td>
<td id="S4.T2.64.64.64.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.64.64.64.2.m1.1" class="ltx_Math" alttext="0.076" display="inline"><semantics id="S4.T2.64.64.64.2.m1.1a"><mn id="S4.T2.64.64.64.2.m1.1.1" xref="S4.T2.64.64.64.2.m1.1.1.cmml">0.076</mn><annotation-xml encoding="MathML-Content" id="S4.T2.64.64.64.2.m1.1b"><cn type="float" id="S4.T2.64.64.64.2.m1.1.1.cmml" xref="S4.T2.64.64.64.2.m1.1.1">0.076</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.64.64.64.2.m1.1c">0.076</annotation></semantics></math></td>
<td id="S4.T2.65.65.65.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.65.65.65.3.m1.1" class="ltx_Math" alttext="0.030" display="inline"><semantics id="S4.T2.65.65.65.3.m1.1a"><mn id="S4.T2.65.65.65.3.m1.1.1" xref="S4.T2.65.65.65.3.m1.1.1.cmml">0.030</mn><annotation-xml encoding="MathML-Content" id="S4.T2.65.65.65.3.m1.1b"><cn type="float" id="S4.T2.65.65.65.3.m1.1.1.cmml" xref="S4.T2.65.65.65.3.m1.1.1">0.030</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.65.65.65.3.m1.1c">0.030</annotation></semantics></math></td>
<td id="S4.T2.66.66.66.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.66.66.66.4.m1.1" class="ltx_Math" alttext="0.084" display="inline"><semantics id="S4.T2.66.66.66.4.m1.1a"><mn id="S4.T2.66.66.66.4.m1.1.1" xref="S4.T2.66.66.66.4.m1.1.1.cmml">0.084</mn><annotation-xml encoding="MathML-Content" id="S4.T2.66.66.66.4.m1.1b"><cn type="float" id="S4.T2.66.66.66.4.m1.1.1.cmml" xref="S4.T2.66.66.66.4.m1.1.1">0.084</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.66.66.66.4.m1.1c">0.084</annotation></semantics></math></td>
<td id="S4.T2.67.67.67.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.67.67.67.5.m1.1" class="ltx_Math" alttext="0.002" display="inline"><semantics id="S4.T2.67.67.67.5.m1.1a"><mn id="S4.T2.67.67.67.5.m1.1.1" xref="S4.T2.67.67.67.5.m1.1.1.cmml">0.002</mn><annotation-xml encoding="MathML-Content" id="S4.T2.67.67.67.5.m1.1b"><cn type="float" id="S4.T2.67.67.67.5.m1.1.1.cmml" xref="S4.T2.67.67.67.5.m1.1.1">0.002</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.67.67.67.5.m1.1c">0.002</annotation></semantics></math></td>
<td id="S4.T2.68.68.68.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.68.68.68.6.m1.1" class="ltx_Math" alttext="0.084" display="inline"><semantics id="S4.T2.68.68.68.6.m1.1a"><mn id="S4.T2.68.68.68.6.m1.1.1" xref="S4.T2.68.68.68.6.m1.1.1.cmml">0.084</mn><annotation-xml encoding="MathML-Content" id="S4.T2.68.68.68.6.m1.1b"><cn type="float" id="S4.T2.68.68.68.6.m1.1.1.cmml" xref="S4.T2.68.68.68.6.m1.1.1">0.084</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.68.68.68.6.m1.1c">0.084</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of absolute scale estimation results for the baseline method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and proposed architectures. Means and standard deviations of the difference between ground truth and predicted values are provided separately for each testing sequence. LSTM (B, 19) stands for bidirectional LSTM with a sequence length 19. Models are trained on the KITTI dataset (KITTI only), synthetic data from the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (CARLA only) or both datasets CARLA and KITTI. All values are in meters.</figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">We chose the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to generate synthetic training data out of convenience as it facilitates an autopilot mode. Other simulators (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> might be applicable instead or in addition to achieve an ever larger variability of the data. The CARLA simulator allows the time of day and weather conditions to be changed—in particular, we can add puddles and vary rain intensity—and provides six different maps. These maps have different visual appearances and road networks. To collect data, we randomly initialize the vehicle’s position and the weather conditions. We save all images and their respective camera poses while driving for a short distance (<em id="S4.SS2.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.p4.1.2" class="ltx_text"></span> 100 meters) in autopilot mode, then resume driving with a new vehicle position and weather settings. For performance reasons, we change maps only after 100 K image pairs have been collected. We model both the left and right RGB camera using the same extrinsic parameters, i.e., the same offsets to the vehicle’s center of rotation, as those provided for the KITTI dataset. The full synthetic training set includes <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="~{}800" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mn id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">800</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><cn type="integer" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">800</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">~{}800</annotation></semantics></math> K image pairs from all six virtual maps available in the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">For the CNN architecture, the learning rate of the Adam Optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is set to 0.0001 and decreased by a factor of 2 after every 10 K iterations. We add a dropout layer with a probability rate of 15%. The batch size is 75 and number of iterations is 100 K.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">For the network with an LSTM part, we use slightly different parameters: initial learning rate is 0.00002 with a decay factor of 2 after every 2500 iterations. The batch size is set to 16. The augmentation scheme is as described above. The convolutional part of the network is initialized from the model trained without LSTM layers, which allows for limiting the total number of training iterations to 15 K.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation of scale estimators</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Training details ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> contains the main results of this paper, including a comparison with previous results. For each method we compute standard deviations <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\sigma</annotation></semantics></math> of the difference between ground truth and predicted values. For most of the experiments means are close to zero which indicates an absence of systematic errors. We do not use more advanced metrics (like Absolute Trajectory Error (ATE)) because we estimate only distances between consecutive pairs and not full camera poses.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1909.00713/assets/result_trajectories_with_bar.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Trajectories colored by the absolute distance estimation errors (KITTI sequences 00, 02, 08). Left: State-of-the-art results of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> (reimplementation). Right: Best results of this paper. Color bar units are meters.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The first two results come from the literature. We use slightly different settings for dropout and image augmentation than those used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, so we evaluate the influence of these changes by reimplementing that architecture. As reported within the 3rd row of Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Training details ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our results are slightly better. The change of intrinsic parameters has a minor effect on the evaluation results (see rows 3 and 4), which indicates that high accuracy may be reached even with a smaller field of view.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Result 5 corresponds to our findings regarding the proposed CNN network trained using only the KITTI data, for which we observe a significant improvement of <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mi id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><ci id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\sigma</annotation></semantics></math> compared to the results of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Row 6 of Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Training details ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents results of the proposed CNN network trained only on synthetic data. An important conclusion comes from comparing the numbers provided in rows 5 and 6, which are quite similar. This proves the ability of the model trained on synthetic data to generalize on a completely unseen set of real images. We also trained the model using a combined dataset from KITTI and CARLA images (row 7). Clearly, these results outperform those trained using a single input modality, which means KITTI and CARLA datasets are complementary.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Our best results are obtained using a bidirectional LSTM with a length of 19, see row 8 in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Training details ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This confirms the importance of taking vehicle dynamics into account. However, in the case of CARLA-generated images, our LSTM does not improve accuracy, see rows 9–10. We interpret these results as a special property of synthetic trajectories: they are very smooth and regular, while real trajectories are more noisy and less linear. This gives rise to the assumption that our virtual vehicle dynamics do not generalize well to the real dynamics. Updating the autopilot mode of the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> might help solving this problem.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Training data</th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\mu</annotation></semantics></math></th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mi id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\sigma</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.4.4" class="ltx_tr">
<th id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">CNN</th>
<th id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">CARLA only</th>
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="-0.009" display="inline"><semantics id="S4.T3.3.3.3.1.m1.1a"><mrow id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml"><mo id="S4.T3.3.3.3.1.m1.1.1a" xref="S4.T3.3.3.3.1.m1.1.1.cmml">−</mo><mn id="S4.T3.3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.3.1.m1.1.1.2.cmml">0.009</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"><minus id="S4.T3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"></minus><cn type="float" id="S4.T3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T3.3.3.3.1.m1.1.1.2">0.009</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">-0.009</annotation></semantics></math></td>
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T3.4.4.4.2.m1.1" class="ltx_Math" alttext="0.109" display="inline"><semantics id="S4.T3.4.4.4.2.m1.1a"><mn id="S4.T3.4.4.4.2.m1.1.1" xref="S4.T3.4.4.4.2.m1.1.1.cmml">0.109</mn><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.2.m1.1b"><cn type="float" id="S4.T3.4.4.4.2.m1.1.1.cmml" xref="S4.T3.4.4.4.2.m1.1.1">0.109</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.2.m1.1c">0.109</annotation></semantics></math></td>
</tr>
<tr id="S4.T3.6.6.6" class="ltx_tr">
<th id="S4.T3.6.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">CNN, LSTM (B, 19)</th>
<th id="S4.T3.6.6.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">CARLA only</th>
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="0.006" display="inline"><semantics id="S4.T3.5.5.5.1.m1.1a"><mn id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml">0.006</mn><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><cn type="float" id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1">0.006</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">0.006</annotation></semantics></math></td>
<td id="S4.T3.6.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T3.6.6.6.2.m1.1" class="ltx_Math" alttext="0.118" display="inline"><semantics id="S4.T3.6.6.6.2.m1.1a"><mn id="S4.T3.6.6.6.2.m1.1.1" xref="S4.T3.6.6.6.2.m1.1.1.cmml">0.118</mn><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.2.m1.1b"><cn type="float" id="S4.T3.6.6.6.2.m1.1.1.cmml" xref="S4.T3.6.6.6.2.m1.1.1">0.118</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.2.m1.1c">0.118</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation of the models trained exclusively on CARLA data on <span id="S4.T3.10.1" class="ltx_text ltx_font_bold">all available</span> KITTI sequences ( 20K image pairs). All <math id="S4.T3.8.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T3.8.m1.1b"><mi id="S4.T3.8.m1.1.1" xref="S4.T3.8.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.8.m1.1c"><ci id="S4.T3.8.m1.1.1.cmml" xref="S4.T3.8.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.m1.1d">\sigma</annotation></semantics></math> values are in meters.</figcaption>
</figure>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">In addition we evaluated models trained exclusively on synthetic CARLA data on all KITTI sequences as none of them is used for training. Results are presented in the Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Evaluation of scale estimators ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and show that the trained models are able to generalize very well to the full corpus of KITTI data.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation of LSTM length</h3>

<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.14.14" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.14.14.15.1" class="ltx_tr">
<th id="S4.T4.14.14.15.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<td id="S4.T4.14.14.15.1.2" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S4.T4.14.14.15.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">KITTI Seq #00, #02, #08</td>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<th id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">
<table id="S4.T4.2.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.2.2.3.1.1" class="ltx_tr">
<td id="S4.T4.2.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">LSTM</td>
</tr>
<tr id="S4.T4.2.2.2.3.1.2" class="ltx_tr">
<td id="S4.T4.2.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Version</td>
</tr>
</table>
</th>
<td id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">
<table id="S4.T4.2.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.2.2.4.1.1" class="ltx_tr">
<td id="S4.T4.2.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">LSTM</td>
</tr>
<tr id="S4.T4.2.2.2.4.1.2" class="ltx_tr">
<td id="S4.T4.2.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Length</td>
</tr>
</table>
</td>
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mi id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\mu</annotation></semantics></math></td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mi id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\sigma</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.4.4.4" class="ltx_tr">
<th id="S4.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">U</th>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5</td>
<td id="S4.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="0.003" display="inline"><semantics id="S4.T4.3.3.3.1.m1.1a"><mn id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml">0.003</mn><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><cn type="float" id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">0.003</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">0.003</annotation></semantics></math></td>
<td id="S4.T4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T4.4.4.4.2.m1.1" class="ltx_Math" alttext="0.087" display="inline"><semantics id="S4.T4.4.4.4.2.m1.1a"><mn id="S4.T4.4.4.4.2.m1.1.1" xref="S4.T4.4.4.4.2.m1.1.1.cmml">0.087</mn><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.2.m1.1b"><cn type="float" id="S4.T4.4.4.4.2.m1.1.1.cmml" xref="S4.T4.4.4.4.2.m1.1.1">0.087</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.2.m1.1c">0.087</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.6.6.6" class="ltx_tr">
<th id="S4.T4.6.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">U</th>
<td id="S4.T4.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11</td>
<td id="S4.T4.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="0.002" display="inline"><semantics id="S4.T4.5.5.5.1.m1.1a"><mn id="S4.T4.5.5.5.1.m1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.cmml">0.002</mn><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><cn type="float" id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1">0.002</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">0.002</annotation></semantics></math></td>
<td id="S4.T4.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.6.6.6.2.m1.1" class="ltx_Math" alttext="0.085" display="inline"><semantics id="S4.T4.6.6.6.2.m1.1a"><mn id="S4.T4.6.6.6.2.m1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.cmml">0.085</mn><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.2.m1.1b"><cn type="float" id="S4.T4.6.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.6.2.m1.1.1">0.085</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.2.m1.1c">0.085</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.8.8.8" class="ltx_tr">
<th id="S4.T4.8.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">U</th>
<td id="S4.T4.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19</td>
<td id="S4.T4.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.7.7.7.1.m1.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.T4.7.7.7.1.m1.1a"><mn id="S4.T4.7.7.7.1.m1.1.1" xref="S4.T4.7.7.7.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.1.m1.1b"><cn type="float" id="S4.T4.7.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.7.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.1.m1.1c">0.001</annotation></semantics></math></td>
<td id="S4.T4.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.8.8.8.2.m1.1" class="ltx_Math" alttext="0.088" display="inline"><semantics id="S4.T4.8.8.8.2.m1.1a"><mn id="S4.T4.8.8.8.2.m1.1.1" xref="S4.T4.8.8.8.2.m1.1.1.cmml">0.088</mn><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.2.m1.1b"><cn type="float" id="S4.T4.8.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.8.2.m1.1.1">0.088</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.2.m1.1c">0.088</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.10.10.10" class="ltx_tr">
<th id="S4.T4.10.10.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">B</th>
<td id="S4.T4.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5</td>
<td id="S4.T4.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="0.019" display="inline"><semantics id="S4.T4.9.9.9.1.m1.1a"><mn id="S4.T4.9.9.9.1.m1.1.1" xref="S4.T4.9.9.9.1.m1.1.1.cmml">0.019</mn><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.1.m1.1b"><cn type="float" id="S4.T4.9.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.9.1.m1.1.1">0.019</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.1.m1.1c">0.019</annotation></semantics></math></td>
<td id="S4.T4.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T4.10.10.10.2.m1.1" class="ltx_Math" alttext="0.084" display="inline"><semantics id="S4.T4.10.10.10.2.m1.1a"><mn id="S4.T4.10.10.10.2.m1.1.1" xref="S4.T4.10.10.10.2.m1.1.1.cmml">0.084</mn><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.2.m1.1b"><cn type="float" id="S4.T4.10.10.10.2.m1.1.1.cmml" xref="S4.T4.10.10.10.2.m1.1.1">0.084</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.2.m1.1c">0.084</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.12.12.12" class="ltx_tr">
<th id="S4.T4.12.12.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">B</th>
<td id="S4.T4.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11</td>
<td id="S4.T4.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.11.11.11.1.m1.1" class="ltx_Math" alttext="0.005" display="inline"><semantics id="S4.T4.11.11.11.1.m1.1a"><mn id="S4.T4.11.11.11.1.m1.1.1" xref="S4.T4.11.11.11.1.m1.1.1.cmml">0.005</mn><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.1.m1.1b"><cn type="float" id="S4.T4.11.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.11.1.m1.1.1">0.005</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.1.m1.1c">0.005</annotation></semantics></math></td>
<td id="S4.T4.12.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.12.12.12.2.m1.1" class="ltx_Math" alttext="0.077" display="inline"><semantics id="S4.T4.12.12.12.2.m1.1a"><mn id="S4.T4.12.12.12.2.m1.1.1" xref="S4.T4.12.12.12.2.m1.1.1.cmml">0.077</mn><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.2.m1.1b"><cn type="float" id="S4.T4.12.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.12.2.m1.1.1">0.077</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.2.m1.1c">0.077</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.14.14.14" class="ltx_tr">
<th id="S4.T4.14.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">B</th>
<td id="S4.T4.14.14.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">19</td>
<td id="S4.T4.13.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T4.13.13.13.1.m1.1" class="ltx_Math" alttext="0.012" display="inline"><semantics id="S4.T4.13.13.13.1.m1.1a"><mn id="S4.T4.13.13.13.1.m1.1.1" xref="S4.T4.13.13.13.1.m1.1.1.cmml">0.012</mn><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.1.m1.1b"><cn type="float" id="S4.T4.13.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.13.1.m1.1.1">0.012</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.1.m1.1c">0.012</annotation></semantics></math></td>
<td id="S4.T4.14.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T4.14.14.14.2.m1.1" class="ltx_Math" alttext="\boldsymbol{0.076}" display="inline"><semantics id="S4.T4.14.14.14.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T4.14.14.14.2.m1.1.1" xref="S4.T4.14.14.14.2.m1.1.1.cmml">0.076</mn><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.2.m1.1b"><cn type="float" id="S4.T4.14.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.14.2.m1.1.1">0.076</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.2.m1.1c">\boldsymbol{0.076}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Evaluation of different sequence lengths within the unidirectional (U) vs bidirectional (B) LSTM arhitectures. Training and evaluation are performed on the KITTI dataset. All <math id="S4.T4.16.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T4.16.m1.1b"><mi id="S4.T4.16.m1.1.1" xref="S4.T4.16.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.16.m1.1c"><ci id="S4.T4.16.m1.1.1.cmml" xref="S4.T4.16.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.m1.1d">\sigma</annotation></semantics></math> values are in meters.</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Evaluation of different lengths of LSTM sequences and comparison of unidirectional and bidirectional LSTM versions are shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.4 Evaluation of LSTM length ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. This experiment shows that the length of sequences plays a minor role, both for unidirectional LSTM (U) and bidirectional LSTM (B).</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Evaluation of synthetic data diversity</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4.5 Evaluation of synthetic data diversity ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents evaluation results illustrating the importance of diversity in the synthetic training data. The numbers show a clear improvement as soon as a virtual scene (or map) with appearances very different from those of the previously considered maps is added to the training set. Town 1 and 2 are quite similar to each other, so adding the data from town 2 does not yield a noticeable improvement. Town 3 contains roads with multiple lanes and thus helps to reduce <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mi id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><ci id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\sigma</annotation></semantics></math> values by about 1 cm. Town 6 contains two highways and improves results even further. All maps available in CARLA are not very large, and the amount of effort involved in adding more maps is small compared to the cost of launching a campaign to collect real data. This is the benefit of using synthetic data for training.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.14.14" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.14.14.15.1" class="ltx_tr">
<th id="S4.T5.14.14.15.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T5.14.14.15.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">KITTI Seq #00, #02, #08</th>
</tr>
<tr id="S4.T5.2.2.2" class="ltx_tr">
<th id="S4.T5.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r">Training CARLA Maps</th>
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><mi id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">\mu</annotation></semantics></math></th>
<th id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T5.2.2.2.2.m1.1a"><mi id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">\sigma</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.4.4" class="ltx_tr">
<th id="S4.T5.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">1</th>
<td id="S4.T5.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T5.3.3.3.1.m1.1" class="ltx_Math" alttext="0.004" display="inline"><semantics id="S4.T5.3.3.3.1.m1.1a"><mn id="S4.T5.3.3.3.1.m1.1.1" xref="S4.T5.3.3.3.1.m1.1.1.cmml">0.004</mn><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.1.m1.1b"><cn type="float" id="S4.T5.3.3.3.1.m1.1.1.cmml" xref="S4.T5.3.3.3.1.m1.1.1">0.004</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.1.m1.1c">0.004</annotation></semantics></math></td>
<td id="S4.T5.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T5.4.4.4.2.m1.1" class="ltx_Math" alttext="0.139" display="inline"><semantics id="S4.T5.4.4.4.2.m1.1a"><mn id="S4.T5.4.4.4.2.m1.1.1" xref="S4.T5.4.4.4.2.m1.1.1.cmml">0.139</mn><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.2.m1.1b"><cn type="float" id="S4.T5.4.4.4.2.m1.1.1.cmml" xref="S4.T5.4.4.4.2.m1.1.1">0.139</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.2.m1.1c">0.139</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.6.6.6" class="ltx_tr">
<th id="S4.T5.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1 and 2</th>
<td id="S4.T5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.5.5.5.1.m1.1" class="ltx_Math" alttext="-0.025" display="inline"><semantics id="S4.T5.5.5.5.1.m1.1a"><mrow id="S4.T5.5.5.5.1.m1.1.1" xref="S4.T5.5.5.5.1.m1.1.1.cmml"><mo id="S4.T5.5.5.5.1.m1.1.1a" xref="S4.T5.5.5.5.1.m1.1.1.cmml">−</mo><mn id="S4.T5.5.5.5.1.m1.1.1.2" xref="S4.T5.5.5.5.1.m1.1.1.2.cmml">0.025</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.1.m1.1b"><apply id="S4.T5.5.5.5.1.m1.1.1.cmml" xref="S4.T5.5.5.5.1.m1.1.1"><minus id="S4.T5.5.5.5.1.m1.1.1.1.cmml" xref="S4.T5.5.5.5.1.m1.1.1"></minus><cn type="float" id="S4.T5.5.5.5.1.m1.1.1.2.cmml" xref="S4.T5.5.5.5.1.m1.1.1.2">0.025</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.1.m1.1c">-0.025</annotation></semantics></math></td>
<td id="S4.T5.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.6.6.6.2.m1.1" class="ltx_Math" alttext="0.137" display="inline"><semantics id="S4.T5.6.6.6.2.m1.1a"><mn id="S4.T5.6.6.6.2.m1.1.1" xref="S4.T5.6.6.6.2.m1.1.1.cmml">0.137</mn><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.2.m1.1b"><cn type="float" id="S4.T5.6.6.6.2.m1.1.1.cmml" xref="S4.T5.6.6.6.2.m1.1.1">0.137</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.2.m1.1c">0.137</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.8.8.8" class="ltx_tr">
<th id="S4.T5.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1, 2 and 3</th>
<td id="S4.T5.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.7.7.7.1.m1.1" class="ltx_Math" alttext="-0.008" display="inline"><semantics id="S4.T5.7.7.7.1.m1.1a"><mrow id="S4.T5.7.7.7.1.m1.1.1" xref="S4.T5.7.7.7.1.m1.1.1.cmml"><mo id="S4.T5.7.7.7.1.m1.1.1a" xref="S4.T5.7.7.7.1.m1.1.1.cmml">−</mo><mn id="S4.T5.7.7.7.1.m1.1.1.2" xref="S4.T5.7.7.7.1.m1.1.1.2.cmml">0.008</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.7.1.m1.1b"><apply id="S4.T5.7.7.7.1.m1.1.1.cmml" xref="S4.T5.7.7.7.1.m1.1.1"><minus id="S4.T5.7.7.7.1.m1.1.1.1.cmml" xref="S4.T5.7.7.7.1.m1.1.1"></minus><cn type="float" id="S4.T5.7.7.7.1.m1.1.1.2.cmml" xref="S4.T5.7.7.7.1.m1.1.1.2">0.008</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.7.1.m1.1c">-0.008</annotation></semantics></math></td>
<td id="S4.T5.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.8.8.8.2.m1.1" class="ltx_Math" alttext="0.128" display="inline"><semantics id="S4.T5.8.8.8.2.m1.1a"><mn id="S4.T5.8.8.8.2.m1.1.1" xref="S4.T5.8.8.8.2.m1.1.1.cmml">0.128</mn><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.8.2.m1.1b"><cn type="float" id="S4.T5.8.8.8.2.m1.1.1.cmml" xref="S4.T5.8.8.8.2.m1.1.1">0.128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.8.2.m1.1c">0.128</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.10.10.10" class="ltx_tr">
<th id="S4.T5.10.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1, 2, 3 and 4</th>
<td id="S4.T5.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.9.9.9.1.m1.1" class="ltx_Math" alttext="-0.012" display="inline"><semantics id="S4.T5.9.9.9.1.m1.1a"><mrow id="S4.T5.9.9.9.1.m1.1.1" xref="S4.T5.9.9.9.1.m1.1.1.cmml"><mo id="S4.T5.9.9.9.1.m1.1.1a" xref="S4.T5.9.9.9.1.m1.1.1.cmml">−</mo><mn id="S4.T5.9.9.9.1.m1.1.1.2" xref="S4.T5.9.9.9.1.m1.1.1.2.cmml">0.012</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.9.1.m1.1b"><apply id="S4.T5.9.9.9.1.m1.1.1.cmml" xref="S4.T5.9.9.9.1.m1.1.1"><minus id="S4.T5.9.9.9.1.m1.1.1.1.cmml" xref="S4.T5.9.9.9.1.m1.1.1"></minus><cn type="float" id="S4.T5.9.9.9.1.m1.1.1.2.cmml" xref="S4.T5.9.9.9.1.m1.1.1.2">0.012</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.9.1.m1.1c">-0.012</annotation></semantics></math></td>
<td id="S4.T5.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.10.10.10.2.m1.1" class="ltx_Math" alttext="0.129" display="inline"><semantics id="S4.T5.10.10.10.2.m1.1a"><mn id="S4.T5.10.10.10.2.m1.1.1" xref="S4.T5.10.10.10.2.m1.1.1.cmml">0.129</mn><annotation-xml encoding="MathML-Content" id="S4.T5.10.10.10.2.m1.1b"><cn type="float" id="S4.T5.10.10.10.2.m1.1.1.cmml" xref="S4.T5.10.10.10.2.m1.1.1">0.129</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.10.10.2.m1.1c">0.129</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.12.12.12" class="ltx_tr">
<th id="S4.T5.12.12.12.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1, 2, 3, 4 and 5</th>
<td id="S4.T5.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.11.11.11.1.m1.1" class="ltx_Math" alttext="-0.011" display="inline"><semantics id="S4.T5.11.11.11.1.m1.1a"><mrow id="S4.T5.11.11.11.1.m1.1.1" xref="S4.T5.11.11.11.1.m1.1.1.cmml"><mo id="S4.T5.11.11.11.1.m1.1.1a" xref="S4.T5.11.11.11.1.m1.1.1.cmml">−</mo><mn id="S4.T5.11.11.11.1.m1.1.1.2" xref="S4.T5.11.11.11.1.m1.1.1.2.cmml">0.011</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.11.11.11.1.m1.1b"><apply id="S4.T5.11.11.11.1.m1.1.1.cmml" xref="S4.T5.11.11.11.1.m1.1.1"><minus id="S4.T5.11.11.11.1.m1.1.1.1.cmml" xref="S4.T5.11.11.11.1.m1.1.1"></minus><cn type="float" id="S4.T5.11.11.11.1.m1.1.1.2.cmml" xref="S4.T5.11.11.11.1.m1.1.1.2">0.011</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.11.11.11.1.m1.1c">-0.011</annotation></semantics></math></td>
<td id="S4.T5.12.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.12.12.12.2.m1.1" class="ltx_Math" alttext="0.129" display="inline"><semantics id="S4.T5.12.12.12.2.m1.1a"><mn id="S4.T5.12.12.12.2.m1.1.1" xref="S4.T5.12.12.12.2.m1.1.1.cmml">0.129</mn><annotation-xml encoding="MathML-Content" id="S4.T5.12.12.12.2.m1.1b"><cn type="float" id="S4.T5.12.12.12.2.m1.1.1.cmml" xref="S4.T5.12.12.12.2.m1.1.1">0.129</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.12.12.12.2.m1.1c">0.129</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.14.14.14" class="ltx_tr">
<th id="S4.T5.14.14.14.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">1, 2, 3, 4, 5 and 6</th>
<td id="S4.T5.13.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T5.13.13.13.1.m1.1" class="ltx_Math" alttext="-0.007" display="inline"><semantics id="S4.T5.13.13.13.1.m1.1a"><mrow id="S4.T5.13.13.13.1.m1.1.1" xref="S4.T5.13.13.13.1.m1.1.1.cmml"><mo id="S4.T5.13.13.13.1.m1.1.1a" xref="S4.T5.13.13.13.1.m1.1.1.cmml">−</mo><mn id="S4.T5.13.13.13.1.m1.1.1.2" xref="S4.T5.13.13.13.1.m1.1.1.2.cmml">0.007</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.13.13.13.1.m1.1b"><apply id="S4.T5.13.13.13.1.m1.1.1.cmml" xref="S4.T5.13.13.13.1.m1.1.1"><minus id="S4.T5.13.13.13.1.m1.1.1.1.cmml" xref="S4.T5.13.13.13.1.m1.1.1"></minus><cn type="float" id="S4.T5.13.13.13.1.m1.1.1.2.cmml" xref="S4.T5.13.13.13.1.m1.1.1.2">0.007</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.13.13.1.m1.1c">-0.007</annotation></semantics></math></td>
<td id="S4.T5.14.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T5.14.14.14.2.m1.1" class="ltx_Math" alttext="\boldsymbol{0.115}" display="inline"><semantics id="S4.T5.14.14.14.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T5.14.14.14.2.m1.1.1" xref="S4.T5.14.14.14.2.m1.1.1.cmml">0.115</mn><annotation-xml encoding="MathML-Content" id="S4.T5.14.14.14.2.m1.1b"><cn type="float" id="S4.T5.14.14.14.2.m1.1.1.cmml" xref="S4.T5.14.14.14.2.m1.1.1">0.115</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.14.14.2.m1.1c">\boldsymbol{0.115}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Scale estimation results for the CNN architecture trained using synthetic data with different numbers of virtual maps. Evaluation is performed on the testing part of the KITTI dataset. All <math id="S4.T5.16.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T5.16.m1.1b"><mi id="S4.T5.16.m1.1.1" xref="S4.T5.16.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T5.16.m1.1c"><ci id="S4.T5.16.m1.1.1.cmml" xref="S4.T5.16.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.16.m1.1d">\sigma</annotation></semantics></math> values are in meters.</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Results of domain adaptation</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">This subsection addresses the question regarding how photorealism of synthetic data influences absolute scale estimation.
Table <a href="#S4.T6" title="Table 6 ‣ 4.6 Results of domain adaptation ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> contains results of distance estimations for three use cases: (i) no domain adaptation, (ii) domain adaptation using approaches T2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and (iii) CyCADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Our experiments indicate almost no improvement or even minor degradation of the results if domain adaptation is applied. The difficulties may be related to the problem of changing both input images in a similar way. It seems challenging to constrain the generator of an adversarial network such that realism is equally improved on multiple images, whilst those features important for the task remain preserved. Hence, our insights are aligned with the conclusions drawn in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, i.e., that photorealism of synthetic data is less important than diversity.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.8.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.8.8.9.1" class="ltx_tr">
<th id="S4.T6.8.8.9.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T6.8.8.9.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">KITTI Seq #00, #02, #08</th>
</tr>
<tr id="S4.T6.2.2.2" class="ltx_tr">
<th id="S4.T6.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r">Method</th>
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><mi id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">\mu</annotation></semantics></math></th>
<th id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T6.2.2.2.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T6.2.2.2.2.m1.1a"><mi id="S4.T6.2.2.2.2.m1.1.1" xref="S4.T6.2.2.2.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.2.m1.1b"><ci id="S4.T6.2.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.2.m1.1c">\sigma</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.4.4.4" class="ltx_tr">
<th id="S4.T6.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">No domain adaptation</th>
<td id="S4.T6.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T6.3.3.3.1.m1.1" class="ltx_Math" alttext="-0.007" display="inline"><semantics id="S4.T6.3.3.3.1.m1.1a"><mrow id="S4.T6.3.3.3.1.m1.1.1" xref="S4.T6.3.3.3.1.m1.1.1.cmml"><mo id="S4.T6.3.3.3.1.m1.1.1a" xref="S4.T6.3.3.3.1.m1.1.1.cmml">−</mo><mn id="S4.T6.3.3.3.1.m1.1.1.2" xref="S4.T6.3.3.3.1.m1.1.1.2.cmml">0.007</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.1.m1.1b"><apply id="S4.T6.3.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.3.1.m1.1.1"><minus id="S4.T6.3.3.3.1.m1.1.1.1.cmml" xref="S4.T6.3.3.3.1.m1.1.1"></minus><cn type="float" id="S4.T6.3.3.3.1.m1.1.1.2.cmml" xref="S4.T6.3.3.3.1.m1.1.1.2">0.007</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.1.m1.1c">-0.007</annotation></semantics></math></td>
<td id="S4.T6.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T6.4.4.4.2.m1.1" class="ltx_Math" alttext="\boldsymbol{0.115}" display="inline"><semantics id="S4.T6.4.4.4.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T6.4.4.4.2.m1.1.1" xref="S4.T6.4.4.4.2.m1.1.1.cmml">0.115</mn><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.2.m1.1b"><cn type="float" id="S4.T6.4.4.4.2.m1.1.1.cmml" xref="S4.T6.4.4.4.2.m1.1.1">0.115</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.2.m1.1c">\boldsymbol{0.115}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.6.6.6" class="ltx_tr">
<th id="S4.T6.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">T2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S4.T6.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T6.5.5.5.1.m1.1" class="ltx_Math" alttext="-0.011" display="inline"><semantics id="S4.T6.5.5.5.1.m1.1a"><mrow id="S4.T6.5.5.5.1.m1.1.1" xref="S4.T6.5.5.5.1.m1.1.1.cmml"><mo id="S4.T6.5.5.5.1.m1.1.1a" xref="S4.T6.5.5.5.1.m1.1.1.cmml">−</mo><mn id="S4.T6.5.5.5.1.m1.1.1.2" xref="S4.T6.5.5.5.1.m1.1.1.2.cmml">0.011</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.5.1.m1.1b"><apply id="S4.T6.5.5.5.1.m1.1.1.cmml" xref="S4.T6.5.5.5.1.m1.1.1"><minus id="S4.T6.5.5.5.1.m1.1.1.1.cmml" xref="S4.T6.5.5.5.1.m1.1.1"></minus><cn type="float" id="S4.T6.5.5.5.1.m1.1.1.2.cmml" xref="S4.T6.5.5.5.1.m1.1.1.2">0.011</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.5.1.m1.1c">-0.011</annotation></semantics></math></td>
<td id="S4.T6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T6.6.6.6.2.m1.1" class="ltx_Math" alttext="0.117" display="inline"><semantics id="S4.T6.6.6.6.2.m1.1a"><mn id="S4.T6.6.6.6.2.m1.1.1" xref="S4.T6.6.6.6.2.m1.1.1.cmml">0.117</mn><annotation-xml encoding="MathML-Content" id="S4.T6.6.6.6.2.m1.1b"><cn type="float" id="S4.T6.6.6.6.2.m1.1.1.cmml" xref="S4.T6.6.6.6.2.m1.1.1">0.117</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.6.6.2.m1.1c">0.117</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.8.8.8" class="ltx_tr">
<th id="S4.T6.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">CyCADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S4.T6.7.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T6.7.7.7.1.m1.1" class="ltx_Math" alttext="0.011" display="inline"><semantics id="S4.T6.7.7.7.1.m1.1a"><mn id="S4.T6.7.7.7.1.m1.1.1" xref="S4.T6.7.7.7.1.m1.1.1.cmml">0.011</mn><annotation-xml encoding="MathML-Content" id="S4.T6.7.7.7.1.m1.1b"><cn type="float" id="S4.T6.7.7.7.1.m1.1.1.cmml" xref="S4.T6.7.7.7.1.m1.1.1">0.011</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.7.7.7.1.m1.1c">0.011</annotation></semantics></math></td>
<td id="S4.T6.8.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T6.8.8.8.2.m1.1" class="ltx_Math" alttext="0.120" display="inline"><semantics id="S4.T6.8.8.8.2.m1.1a"><mn id="S4.T6.8.8.8.2.m1.1.1" xref="S4.T6.8.8.8.2.m1.1.1.cmml">0.120</mn><annotation-xml encoding="MathML-Content" id="S4.T6.8.8.8.2.m1.1b"><cn type="float" id="S4.T6.8.8.8.2.m1.1.1.cmml" xref="S4.T6.8.8.8.2.m1.1.1">0.120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.8.8.2.m1.1c">0.120</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results of domain adaptation for the CNN architecture. The training set consists of a fully annotated synthetic part and unlabeled training images from KITTI. Evaluations are performed on the testing part of the KITTI dataset. All <math id="S4.T6.10.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.T6.10.m1.1b"><mi id="S4.T6.10.m1.1.1" xref="S4.T6.10.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.T6.10.m1.1c"><ci id="S4.T6.10.m1.1.1.cmml" xref="S4.T6.10.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.10.m1.1d">\sigma</annotation></semantics></math> values are in meters.</figcaption>
</figure>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Error analysis</h3>

<figure id="S4.F5" class="ltx_figure"><img src="/html/1909.00713/assets/difference_to_gt_seq_00_2.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="374" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of recovered distances (blue) and ground truth (red) per frame for the part of the KITTI sequence 00 from the (a)  CNN alone; (b) LSTM (B), length 19. Training data: KITTI only. X-axis: frame index. </figcaption>
</figure>
<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">An overview of all testing trajectories for the baseline method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and our best model (LSTM, length 19) is presented in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Evaluation of scale estimators ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The overview gives a clear understanding of our improvements: (a) absolute scale estimations are more accurate (b) random noise is reduced, and (c) large errors during vehicle turns are nearly eliminated.</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<p id="S4.SS7.p2.1" class="ltx_p">Figure <a href="#S4.F5" title="Figure 5 ‣ 4.7 Error analysis ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the proposed LSTM architecture with the proposed CNN architecture. Clearly, the LSTM results are smoother, which confirms the value of adding LSTM layers. However, vehicle rotations remain the most difficult challenge for the proposed method.</p>
</div>
<div id="S4.SS7.p3" class="ltx_para">
<p id="S4.SS7.p3.1" class="ltx_p">Figure <a href="#S4.F6" title="Figure 6 ‣ 4.7 Error analysis ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the cumulated error distribution for all testing sequences. From the histogram, we observe a minor tendency of overestimating the “true” distances.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1909.00713/assets/histogram_of_errors2.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="280" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Histogram of distance estimation errors for LSTM (B, length 19) on all testing KITTI sequences. Training data: KITTI only.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/1909.00713/assets/ImagesWithLargestErrors2.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="88" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Images with largest errors for LSTM (B, length 19). Training data: KITTI only.</figcaption>
</figure>
<div id="S4.SS7.p4" class="ltx_para">
<p id="S4.SS7.p4.1" class="ltx_p">Figure <a href="#S4.F7" title="Figure 7 ‣ 4.7 Error analysis ‣ 4 Experiments ‣ Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> provides example images where relatively large errors are still observed (LSTM (B), length 19). The areas with large amounts of vegetation are the most difficult ones.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper addresses the problem of scale estimation in monocular SLAM by estimating the distance between camera centers of consecutive image frames. These estimates would improve the overall performance of classical (not deep) SLAM systems and cast the entire 3D reconstruction from a monocular camera in metric values. The proposed solution estimates scale for each pair independently (or with soft-constrained LSTM network), which makes it insensitive to long-term drift effects. Our work introduced several network architectures, which lead to an improvement of scale estimation accuracy over the state of the art. With respect to the baseline method, our results show significant improvements of the estimates for camera rotations. In addition, we exploit the possibility to train the neural network only with synthetic data derived from a computer graphics simulator. Our experiments indicate that, using only synthetic training inputs, we can achieve similar scale estimation accuracy as that obtained from real data. This provides a practical solution to the sensor reconfiguration problem. Our experiments with unsupervised domain adaptation also demonstrate that differences in visual appearance (photorealism) between simulated and real data does not affect scale estimation results. The proposed methods operate with low-resolution images (0.03 MP), which makes them practical for real-time SLAM applications with a monocular camera.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgment</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 731993 and No 688652.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Cycada: Cycle consistent adversarial domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.3.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib1.4.3" class="ltx_text" style="font-size:90%;">, pages
1994–2003, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">TensorFlow: Large-scale machine learning on heterogeneous systems,
2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A. Atapour-Abarghouei and T.P. Breckon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Real-time monocular depth estimation using synthetic data with domain
adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 1–8, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
R. O. Castle, D. J. Gawley, G. Klein, and D. W. Murray.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Towards simultaneous recognition, localization and mapping for
hand-held and wearable cameras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">”International Conference on Robotics and Automation
(ICRA)”</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 4102–4107, 2007.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach,
Subhashini Venugopalan, Kate Saenko, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Long-term recurrent convolutional networks for visual recognition and
description.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 2625–2634, 2015.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
A. Dosovitskiy, P. Fischer, E. Ilg, P. Häusser, C. Hazırbaş, V.
Golkov, P. v.d. Smagt, D. Cremers, and T. Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Flownet: Learning optical flow with convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 2758–2766, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">CARLA: An open urban driving simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 1st Annual Conference on Robot Learning</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">,
pages 1–16, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
David Eigen, Christian Puhrsch, and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Depth map prediction from a single image using a multi-scale deep
network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages
2366–2374. 2014.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Duncan P. Frost, Olaf Kähler, and David W. Murray.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Object-aware bundle adjustment for correcting monocular scale drift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">”International Conference on Robotics and Automation
(ICRA)”</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 4770–4776, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Duncan P. Frost, David W. Murray, and Victor Adrian Prisacariu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Using learning of speed to stabilize scale in monocular localization
and mapping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on 3D Vision (3DV)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 527–536,
2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deep ordinal regression network for monocular depth estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 2002–2011, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 3354–3361, 2012.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Martin Lauer, Christian Wojek, Christoph Stiller, and Raquel
Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">3d traffic scene understanding from movable platforms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">,
36(5):1012–1025, 2013.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Julius Ziegler, and Christoph Stiller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Stereoscan: Dense 3d reconstruction in real-time.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Intelligent Vehicles Symposium (IV)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 963–968,
2011.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Johannes Gräter, Tobias Schwarze, and Martin Lauer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Robust scale estimation for monocular visual odometry using structure
from motion and vanishing points.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Intelligent Vehicles Symposium (IV)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 475–480,
2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Sepp Hochreiter and Jürgen Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Long short-term memory.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Computation</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 9(8):1735–1780, 1997.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Multimodal unsupervised image-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages
172–189, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe,
Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
and Andrew Fitzgibbon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Kinectfusion: Real-time 3d reconstruction and interaction using a
moving depth camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 24th Annual ACM Symposium on User
Interface Software and Technology</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 559–568, 2011.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
C. Kerl, J. Sturm, and D. Cremers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Robust odometry estimation for rgb-d cameras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">”International Conference on Robotics and Automation
(ICRA)”</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 3748–3754, 2013.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Bernd Kitt, Andreas Geiger, and Henning Lategahn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Visual odometry based on stereo image sequences with ransac-based
outlier rejection scheme.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Intelligent Vehicles Symposium (IV)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 486–492,
2010.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Qing Li, Jiasong Zhu, Rui Cao, Ke Sun, Jonathan M Garibaldi, Qingquan Li, Bozhi
Liu, and Guoping Qiu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Relative geometry-aware siamese neural network for 6dof camera
relocalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1901.01049</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers,
Alexey Dosovitskiy, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">What makes good synthetic training data for learning disparity and
optical flow estimation?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 126(9):942–960,
2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Craig Quiter and Maik Ernst.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">deepdrive/deepdrive: 2.0, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Dan Rosenholm and Kenner! Torlegard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Three-dimensional absolute orientation of stereo models using digital
elevation models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Photogrammetric Engineering and Remote Sensing</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
54(10):4102–4107, 1988.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Davide Scaramuzza, Friedrich Fraundorfer, Marc Pollefeys, and Roland Siegwart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Absolute scale in structure from motion from a single vehicle mounted
camera by exploiting nonholonomic constraints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 1413–1419, 2009.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Davide Scaramuzza, Friedrich Fraundorfer, Marc Pollefeys, and Roland Siegwart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Absolute scale in structure from motion from a single vehicle mounted
camera by exploiting nonholonomic constraints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision (ICCV)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 1413 – 1419, 2009.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Airsim: High-fidelity visual and physical simulation for autonomous
vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Field and Service Robotics</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 621–635, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Shiyu Song and Manmohan Chandraker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Robust scale estimation in real-time monocular SFM for autonomous
driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 1566–1573, 2014.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Edgar Sucar and Jean-Bernard Hayet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Bayesian scale estimation for monocular SLAM based on generic
object detection for correcting scale drift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">”International Conference on Robotics and Automation
(ICRA)”</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 1–7, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
K. Tateno, F. Tombari, I. Laina, and N. Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Cnn-slam: Real-time dense monocular slam with learned depth
prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 6243–6252, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
S. Wang, R. Clark, H. Wen, and N. Trigoni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Deepvo: Towards end-to-end visual odometry with deep recurrent
convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">pages 2043–2050, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
N. Yang, R. Wang, J. Stueckler, and D. Cremers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Deep virtual stereo odometry: Leveraging deep depth prediction for
monocular direct sparse odometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE European Conference on Computer
Vision (ECCV)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 817–833, 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">T2net: Synthetic-to-realistic translation for solving single-image
depth estimation tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE European Conference on Computer
Vision (ECCV)</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 767–783, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Dingfu Zhou, Yuchao Dai, and Hongdong Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Ground plane based absolute scale estimation for monocular visual
odometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1903.00912</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Unpaired image-to-image translation using cycle-consistent
adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">,
pages 2223–2232, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1909.00711" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1909.00713" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1909.00713">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1909.00713" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1909.00714" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 10:09:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
