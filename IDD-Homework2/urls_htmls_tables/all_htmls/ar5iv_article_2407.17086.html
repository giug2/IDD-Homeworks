<article class="ltx_document ltx_authors_1line ltx_leqno">
 <h1 class="ltx_title ltx_title_document">
  AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Yijie Guo
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id2.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id3.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:guoyijie.sh@gmail.com">
      guoyijie.sh@gmail.com
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhenhan Huang
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">
      University of Tsukuba
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id5.2.id2">
      Tsukuba
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id6.3.id3">
      Japan
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:zhenhan.email.jp@gmail.com">
      zhenhan.email.jp@gmail.com
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Ruhan Wang
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id8.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id9.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:wangrh22@mails.tsinghua.edu.cn">
      wangrh22@mails.tsinghua.edu.cn
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhihao Yao
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_streetaddress" id="id11.2.id2">
      30 Shuangqing Rd
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id12.3.id3">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id13.4.id4">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:yaozh%CB%99h@outlook.com">
      yaozh˙h@outlook.com
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Tianyu Yu
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id14.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id15.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id16.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:yty21@mails.tsinghua.edu.cn">
      yty21@mails.tsinghua.edu.cn
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhiling Xu
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id18.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id19.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:xzl23@mails.tsinghua.edu.cn">
      xzl23@mails.tsinghua.edu.cn
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Xinyu Zhao
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id20.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id21.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id22.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:xyzhao23@mails.tsinghua.edu.cn">
      xyzhao23@mails.tsinghua.edu.cn
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Xueqing Li
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id23.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id24.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id25.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:li-xq23@mails.tsinghua.edu.cn">
      li-xq23@mails.tsinghua.edu.cn
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   and
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Haipeng Mi
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id26.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id27.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id28.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:mhp@tsinghua.edu.cn">
      mhp@tsinghua.edu.cn
     </a>
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_dates">
  (2018; 20 February 2007; 12 March 2009; 5 June 2009)
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract.
  </h6>
  <p class="ltx_p" id="id29.id1">
   While Swarm User Interfaces (SUIs) have succeeded in enriching tangible interaction experiences, their limitations in autonomous action planning have hindered the potential for personalized and dynamic interaction generation in tabletop games. Based on the AI-Gadget Kit we developed, this paper explores how to integrate LLM-driven agents within tabletop games to enable SUIs to execute complex interaction tasks. After defining the design space of this kit, we elucidate the method for designing agents that can extend the meta-actions of SUIs to complex motion planning. Furthermore, we introduce an add-on prompt method that simplifies the design process for four interaction behaviors and four interaction relationships in tabletop games. Lastly, we present several application scenarios that illustrate the potential of AI-Gadget Kit to construct personalized interaction in SUI tabletop games. We expect to use our work as a case study to inspire research on multi-agent-driven SUI for other scenarios with complex interaction tasks.
  </p>
 </div>
 <div class="ltx_keywords">
  Personalization; Tangible UIs; LLM-Based Agent; Tabletop Game; Swarm User Interface
 </div>
 <span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     copyright:
    </span>
    acmcopyright
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     journalyear:
    </span>
    2018
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     doi:
    </span>
    XXXXXXX.XXXXXXX
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     conference:
    </span>
    Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id5">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     price:
    </span>
    15.00
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     isbn:
    </span>
    978-1-4503-XXXX-X/18/06
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     ccs:
    </span>
    Human-centered computing Interaction devices
   </span>
  </span>
 </span>
 <figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
  <div class="ltx_flex_figure">
   <div class="ltx_flex_cell ltx_flex_size_2">
    <img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="173" id="S0.F1.g1" src="/html/2407.17086/assets/figure/title.jpg" width="598"/>
   </div>
  </div>
  <figcaption class="ltx_caption">
   <span class="ltx_tag ltx_tag_figure">
    Figure 1.
   </span>
   Building a tabletop game with a multi-agent system enabled by AI-Gaget Kit. a) Extend the meta actions of SUIs to gadgets’ complex motion planning, b) interaction behavior generation, c) interaction relationship management. d) Robotic gadget plays a turn-based strategy game with a human player.
  </figcaption>
  <div class="ltx_flex_figure">
   <div class="ltx_flex_cell ltx_flex_size_1">
    <span class="ltx_ERROR ltx_figure_panel undefined" id="S0.F1.1">
     \Description
    </span>
   </div>
   <div class="ltx_flex_break">
   </div>
   <div class="ltx_flex_cell ltx_flex_size_1">
    <p class="ltx_p ltx_figure_panel" id="S0.F1.2">
     Building a tabletop game with a multi-agent system enabled by AI-Gaget Kit. a) Extend the meta actions of SUIs to gadgets’ complex motion planning, b) interaction behavior generation, c) interaction relationship management. d) Robotic gadget plays a turn-based strategy game with a human player.
    </p>
   </div>
  </div>
 </figure>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1.
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Swarm User Interface (SUI) as an emerging interface has garnered significant attention from researchers.
Researchers have explored multiple application scenarios of SUI, such as data physicalization
    <cite class="ltx_cite ltx_citemacro_citep">
     (Suzuki et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2019
     </a>
     )
    </cite>
    , remote collaboration
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ihara et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2023
     </a>
     )
    </cite>
    , and educational purposes
    <cite class="ltx_cite ltx_citemacro_citep">
     (Kaimoto et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib13" title="">
      2022
     </a>
     )
    </cite>
    , based on the unique tangible interaction behaviors of SUI, such as collaborative motion, objects actuation, or self-shape changes.
However, most existing SUIs utilize a pre-programmed set of action planning rules to execute different interaction tasks during use, i.e., users need to program the action each time they face a new interaction task.
This approach struggles to adapt to real-world tasks that are usually more complex, dynamic, and possibly changeable beyond the pre-programmed scope.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    In recent years, Large Language Models (LLM) have shown benefits for robotic motion control and planning.
Based on the embedded knowledge of LLMs, they are capable of understanding and reasoning about the complex contexts in different tasks, thus dynamically generating responses, e.g., motion planning, based on the contexts in the tasks.
Traditional methods for robotic motion planning included using one-decision models, which relied on the prediction of every step of the robot’s movement
    <cite class="ltx_cite ltx_citemacro_citep">
     (Gan et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2020
     </a>
     ; Wang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2020
     </a>
     )
    </cite>
    .
Recent research has explored using LLMs, particularly LLM-driven agents, to conduct robotic motion planning for more complex interaction tasks.
For instance, DiscussNav
    <cite class="ltx_cite ltx_citemacro_citep">
     (Long et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    has used multiple LLM agents with different expertise to make decisions for robotic navigation in a complex interior scenario.
Nevertheless, most of these works focused on single-robot systems.
Research on multiple-robot systems, e.g., SUI, and how to use LLMs to assist the motion planning of these systems for complex interaction tasks still remains unexplored.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Among the various applications of SUI, the tabletop game is a typical scenario that contains versatile complex interaction tasks.
Existing research has explored the application of tangible and swarm user interfaces in tabletop games to enhance interactivity and enjoyment, such as using robots to facilitate embodied AI players
    <cite class="ltx_cite ltx_citemacro_citep">
     (Matuszek et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib19" title="">
      2011
     </a>
     ; van Breemen et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib30" title="">
      2005
     </a>
     )
    </cite>
    , robotic game masters
    <cite class="ltx_cite ltx_citemacro_citep">
     (Gillet et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib6" title="">
      2020
     </a>
     )
    </cite>
    , and automated gadgets
    <cite class="ltx_cite ltx_citemacro_citep">
     (Brock et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib2" title="">
      2021
     </a>
     ; Jariyavajee et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib11" title="">
      2018
     </a>
     )
    </cite>
    .
However, the action planning for robots in these studies still relies on pre-programmed rules, which makes it challenging to execute the complex interaction tasks in tabletop games, such as understanding and reacting to complex game narratives, improvised decisions of players, or emotional expressions from players.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    In this paper, we aimed to use the tabletop game as a case study to explore the application of LLMs on action planning of SUI in scenarios with complex interaction tasks.
We proposed AI-gadget Kit, a multi-agent SUI tabletop gaming system, which is designed to facilitate dynamic and complex interaction tasks in tabletop games.
We first introduced the system architecture of the AI-gadget Kit, which includes a set of swarm robots based on existing platforms to perform the gadget behaviors, and a multi-agent system responsible for executing the game and generating action plans for the swarm robots.
We then elaborated the design of the multi-agent system, comprising a series of meta-motions for individual robots, two LLM-based agents for complex action planning, and a set of add-on prompts aimed at reinforcing the understanding and reacting capabilities of the agents.
At last, we demonstrate four application examples using AI-gadget Kit to showcase the effect of the multi-agent-driven SUI on executing complex interaction tasks in tabletop games.
Through this work, we aim to inspire the research of multi-agent-driven SUI on other scenarios with complex interaction tasks.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    In summary, the contribution of this paper includes:
   </p>
   <ol class="ltx_enumerate" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      (1)
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       AI-gadget Kit, a multi-agent SUI tabletop gaming system, which consists of a series of meta-motions for individual robots, two LLM-based agents for complex action planning, and a series of add-on prompts tailored to the tabletop gaming scenario to enhance the understanding capability of the multi-agent system.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      (2)
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       A set of application examples using AI-gadget Kit on tabletop games, which demonstrates the effect of agent-driven swarm robots as gadgets in tabletop games for complex interaction tasks.
      </p>
     </div>
    </li>
   </ol>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2.
   </span>
   Related work
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1.
    </span>
    Swarm User Interface
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     Compared to traditional Tangible User Interfaces (TUI), Swarm User Interfaces (SUI) introduce multiple moving robots that enable collaborative motion, providing a flexible and extensive physical interaction space and multi-modal interaction experiences.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     Researchers in Human-Computer Interaction (HCI) have explored various applications of SUI.
For instance, SwarmHaptic
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kim and Follmer,
      <a class="ltx_ref" href="#bib.bib14" title="">
       2019
      </a>
      )
     </cite>
     utilized small wheeled swarm robots moving on a flat surface to construct a novel tactile interface.
Rovables
     <cite class="ltx_cite ltx_citemacro_citep">
      (Dementyev et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib3" title="">
       2016
      </a>
      )
     </cite>
     provided a series of robots that were capable of autonomous movement on wearable clothing, proposing an interaction space for sensing and actuation on wearables.
ShapeBots
     <cite class="ltx_cite ltx_citemacro_citep">
      (Suzuki et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2019
      </a>
      )
     </cite>
     enabled a group of self-deforming robots to individually or collectively change their configuration to display information in physical space.
Additionally, Holobots
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ihara et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2023
      </a>
      )
     </cite>
     proposed a mixed-reality remote collaboration system augmenting holographic telepresence with synchronized mobile robots.
Out of academia, SUIs have also demonstrated extensive application prospects in education and entertainment scenarios.
Sony employs programmable small robots called Toio
     <span class="ltx_note ltx_role_footnote" id="footnote1">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         1
        </sup>
        <span class="ltx_tag ltx_tag_note">
         1
        </span>
        https://toio.io/
       </span>
      </span>
     </span>
     to engage learners from elementary to adult in logical thinking and learning programming
Thymo
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        https://www.thymio.org/
       </span>
      </span>
     </span>
     robots provide STEM education for learners of all ages.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     However, most existing SUIs utilize a pre-programmed set of action planning rules to execute different interaction tasks during use.
For example, although Holobots creatively proposed six interaction types for remote collaboration, constrained by predetermined programming, they struggled to dynamically generate personalized tactile feedback based on users’ flexible needs during actual usage.
Thus, a system that is capable of understanding and reacting to complex interaction tasks will significantly improve the generalizability of interaction behaviors of SUIs in these works.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2.
    </span>
    Agents for Action Planning
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     In the domain of robotics, researchers aim to issue high-level instructions to robotic agents. These agents automatically translate the instructions into low-level actions for execution by robots, eliminating the need for humans to manually program. The Skill Transformer
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2023a
      </a>
      )
     </cite>
     , leveraging a neural network model based on the Transformer architecture
     <cite class="ltx_cite ltx_citemacro_citep">
      (Vaswani et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2017
      </a>
      )
     </cite>
     , predicts low-level actions for robots, enabling them to accomplish embodied tasks of moving objects to specified targets and locations in complex environments. With the advent of Large Language Models (LLMs), researchers have sought to harness LLMs’ robust natural language understanding capabilities to process generalized, natural language-based embodied instructions. For example, March in Chat
     <cite class="ltx_cite ltx_citemacro_citep">
      (Qiao et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      )
     </cite>
     interacts with agents, LLMs, and VLMs to navigate daily activity scenes based on vague natural language instructions. VoxPoser
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023b
      </a>
      )
     </cite>
     estimates the potential benefits and losses of objects in a scene towards fulfilling an instruction using LLMs, generating a 3D value map of the scene to derive the robot’s trajectory.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     Researchers have also recognized that collaborative decision-making among multiple agents for a robot’s actions can enable adaptation to more complex scenarios compared to decisions made by a single agent alone. DiscussNav
     <cite class="ltx_cite ltx_citemacro_citep">
      (Long et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     is used to address navigation problems in complex scenes, where robots take each step with the involvement of multiple LLM/VLM agents with different specialties, enhancing the robots’ generalization ability in navigation.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p3">
    <p class="ltx_p" id="S2.SS2.p3.1">
     Despite existing research has shown feasible performance in action planning and robotic control for embodied agents, their validation of task planning for embodied tasks has been primarily confined to operational tasks in daily routine scenes
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2023a
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib8" title="">
       b
      </a>
      )
     </cite>
     and navigation tasks
     <cite class="ltx_cite ltx_citemacro_citep">
      (Long et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     with less emphasis on other certain genres of task scenarios, e.g., fictional narrative settings. Moreover, its interaction planning capabilities in user interfaces driven by multiple robots also require validation. Exploring the integration of LLM-based agents with SUI will broaden the application scenarios and interaction cases for related work in robotics.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3.
    </span>
    Robots in Tabletop Game
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     In recent years, researchers have explored the application of tangible and swarm user interfaces in tabletop games to enhance interactivity and enjoyment.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS3.p2">
    <p class="ltx_p" id="S2.SS3.p2.1">
     With the emerging trend of robots being small and versatile, more and more robots are used as embodied AI players or gadgets in tabletop games.
For example, Brock et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Brock et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib2" title="">
       2021
      </a>
      )
     </cite>
     utilized the robot Haru to simulate the behavior of remote human players.
Researchers also used robots to serve as robotic gadgets in the game, such as creating chess that can move automatically to enable novel and compelling interaction experiences
     <cite class="ltx_cite ltx_citemacro_citep">
      (Jariyavajee et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib11" title="">
       2018
      </a>
      )
     </cite>
     .
Sparkybot
     <cite class="ltx_cite ltx_citemacro_citep">
      (Guo et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      )
     </cite>
     allowed children to use mobile robots as different actors in storytelling games to enhance children’s creativity.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS3.p3">
    <p class="ltx_p" id="S2.SS3.p3.1">
     These works, that use SUI for tabletop games, provide rich user interaction spaces.
However, the action planning for robots in these studies still relies on pre-programmed rules, which makes it challenging to execute the complex interaction tasks in tabletop games, such as understanding and reacting to complex game narratives, improvised decisions of players, or emotional expressions from players.
In this work, we aimed to leverage LLM-based agents to assist action planning for SUI, in order to execute complex interaction behaviors of the gadgets in tabletop games.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3.
   </span>
   System Architecture of AI-Gaget Kit
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    The system architecture of AI-Gadget Kit, shown in Figure
    <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3. System Architecture of AI-Gaget Kit ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    , consists of an SUI platform, a localization system, a server, and a LLM-based multi-agent system.
The SUI platform includes multiple independent robots, which are used to actuate various gadget behaviors in tabletop games.
The localization system comprises a camera and multiple on-robot markers, used to obtain the position and orientation of each robot.
The server obtains users’ text or verbal input commands, as well as robots’ position and orientation data, and then sends them to the LLM-based multi-agent system for information processing.
The server also receives the actuations generated by the multi-agent system, playing sound effects, or sending action sequences to the robots in the SUI platform.
The LLM-based multi-agent system is responsible for the execution of the core game interactions.
Based on the game’s rules and knowledge, the multi-agent system responds to user commands, reasons the gadget behaviors in the game, and then generates the action sequences to control the SUI robots.
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    In this project, we utilized a 1m*1m tabletop as the interaction space.
The space was divided into a 30*30 coordinate system, with the east and north directions serving as the positive directions of x and y axes, respectively.
We used Sony’s Toio robots as the SUI platform. Each individual robot has dimensions of 3.2*3.2*2.5cm.
We used a PC as the server, which communicates with Toio robots via Bluetooth and communicates with the LLM via WiFi.
In our localization system, we employed an imx415 network IP camera, positioned 1m above the tabletop, and we used ArUco codes as the localization markers on the robots.
The server retrieves the video stream from the camera using the RTSP protocol and uses the Python OpenCV to track the position and orientation of each robot.
We developed the multi-agent system based on GPT4 LLM. The design of the multi-agent system is introduced as follows.
   </p>
  </div>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="461" id="S3.F2.g1" src="/html/2407.17086/assets/figure/architecture.png" width="598"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2.
    </span>
    System Architecture of AI-Gadget Kit.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4.
   </span>
   Multi-agent system
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In the AI-Gadget Kit, we utilized a multi-agent system to compute the core interaction of the game.
Based on the rules, and knowledge of the game, the system responds to the user’s command, reasons the gadget behaviors within the game, and then generates the action sequences to control the SUI robots.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    To build the multi-agent system, we first defined a set of meta-actions for each single robot, and then designed a two-agent system, including a
    <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">
     Coordinator
    </span>
    agent and a
    <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">
     Controller
    </span>
    agent, to learn and use those meta-actions for complex motion planning.
We also designed a set of add-on prompts, including prompts for
    <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">
     Interaction behavior planning
    </span>
    and
    <span class="ltx_text ltx_font_italic" id="S4.p2.1.4">
     Interaction relationship planning
    </span>
    , to enhance the agents to understand and react to complex interactions during the game.
   </p>
  </div>
  <figure class="ltx_figure" id="S4.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="401" id="S4.F3.g1" src="/html/2407.17086/assets/figure/design_space.png" width="598"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3.
    </span>
    Design space for SUI action planning in tabletop game scenarios through our kit. The kit includes a two-agent system(d) that can generate action sequences for gadgets to complete interaction tasks based on eight meta-actions (a), four types of interaction behaviors (b), and four types of interaction relationships (c).
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1.
    </span>
    Meta-action for Individual Robot
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     To leverage the LLM-based agents for planning and generating complex actions for our SUI, we first defined the primitive movement patterns of an individual robot, i.e., meta-actions (Figure
     <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     a).
An individual Toio robot moves by controlling the motors of the two wheels.
Thus, in this work, we controlled the robot’s meta-action by controlling the rotation direction (clockwise by default or anti-clockwise), speed (three levels - 10, 20, or 30
     <span class="ltx_note ltx_role_footnote" id="footnote3">
      <sup class="ltx_note_mark">
       3
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         3
        </sup>
        <span class="ltx_tag ltx_tag_note">
         3
        </span>
        Speed values in Toio platform
       </span>
      </span>
     </span>
     ), and the duration (x seconds) of each motor.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     We categorized the robot’s meta-actions into two types of movements:
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">
      translation
     </span>
     and
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">
      rotation
     </span>
     , defined as follows:
    </p>
    <ul class="ltx_itemize" id="S4.I1">
     <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i1.p1">
       <p class="ltx_p" id="S4.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">
         Rotation:
        </span>
       </p>
       <ul class="ltx_itemize" id="S4.I1.i1.I1">
        <li class="ltx_item" id="S4.I1.i1.I1.i1" style="list-style-type:none;">
         <span class="ltx_tag ltx_tag_item">
          <span class="ltx_text ltx_font_bold" id="S4.I1.i1.I1.i1.1.1.1">
           –
          </span>
         </span>
         <div class="ltx_para" id="S4.I1.i1.I1.i1.p1">
          <p class="ltx_p" id="S4.I1.i1.I1.i1.p1.1">
           <span class="ltx_text ltx_font_bold" id="S4.I1.i1.I1.i1.p1.1.1">
            Rotation A:
           </span>
           The robot rotates around its center, by spinning its wheels in opposite directions at the same speed for a specified seconds, achieving a precise angle of rotation.
          </p>
         </div>
        </li>
        <li class="ltx_item" id="S4.I1.i1.I1.i2" style="list-style-type:none;">
         <span class="ltx_tag ltx_tag_item">
          <span class="ltx_text ltx_font_bold" id="S4.I1.i1.I1.i2.1.1.1">
           –
          </span>
         </span>
         <div class="ltx_para" id="S4.I1.i1.I1.i2.p1">
          <p class="ltx_p" id="S4.I1.i1.I1.i2.p1.1">
           <span class="ltx_text ltx_font_bold" id="S4.I1.i1.I1.i2.p1.1.1">
            Rotation B:
           </span>
           The robot rotates around one side of itself, by spinning only one wheel for a specified seconds, achieving a specific angle of rotation.
          </p>
         </div>
        </li>
       </ul>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i2.p1">
       <p class="ltx_p" id="S4.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">
         Translation:
        </span>
        The robot adjusts its orientation to the desired direction through Rotation A, followed by spinning both wheels in the same direction and speed for a few seconds to achieve linear translation over a specific distance.
       </p>
      </div>
     </li>
    </ul>
    <p class="ltx_p" id="S4.SS1.p2.2">
     During each movement, the server determines the duration based on a simple calculation of expected translation or rotation displacement and official kinematic data
     <span class="ltx_note ltx_role_footnote" id="footnote4">
      <sup class="ltx_note_mark">
       4
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         4
        </sup>
        <span class="ltx_tag ltx_tag_note">
         4
        </span>
        https://toio.github.io/toio-spec/en/docs/hardware_components
       </span>
      </span>
     </span>
     .
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     Furthermore, considering interactions between multiple robots, we designed another type of meta-action to adjust their relative orientations, including face-to-face, back-to-back, face-to-back, parallel, and counter-parallel, defined and illustrated in Figure
     <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     a.
The two robots conducting one of these meta-actions adjust their orientations by executing Rotation A for a time.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     By repeatedly calling and combining these three types of meta-actions with custom parameters, the agents in the AI-Gadget kit could generate multiple actions in sequences to facilitate the complex motion planning of these robots in SUI.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2.
    </span>
    Complex Motion Planning
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     To facilitate the complex motion planning for SUI to execute the gadget behaviors in tabletop games, we developed an LLM-based two-agent system, that aims to understand and react to the game contexts and then generate the action sequences for the robots (Figure
     <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     d).
Specifically, we proposed two agents in the system with expert prompts:
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">
      Coordinator
     </span>
     and
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">
      Controller
     </span>
     :
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">
      (1) Coordinator Agent.
     </span>
     The operation in a tabletop game typically involves the user’s commands and the processing of context information.
Taking chess as an example, if the user inputs a command of ”move the queen to A1”, the execution of this command involves the actual movement of the queen, and processing of context information such as the dimensions of the chessboard, the queen’s movement rules, and whether an opponent’s piece can be captured.
Hence, we designed a Coordinator agent to respond by processing the players’ commands and the game context information, then reasoning the commands for each gadget within this interaction step.
In the prompts for Coordinator, we asked the agent to act as administrator, coordinator, and referee in the game.
We added the description of a series of duties to the prompts, such as explaining rules, coordinating actions, and updating game states.
We also inputted the game’s environmental settings to the prompts, including the size of the map and the coordinate system.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     Note that, to allow the Coordinator agent to focus on game operations, we employed a ”reality-agnostic” approach.
Specifically, the Coordinator only facilitates the execution of the game, without dealing with the physical parameters of the SUI robots, such as their locations or next movements.
We aim to use this method to enhance the Coordinator’s understanding and reaction capabilities through efficient prompting of LLM.
The prompts of the Coordinator are detailed in Supplementary Material.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">
      (2) Controller Agent.
     </span>
     To use SUI in tabletop games, we require the robots to plan their motion according to the gadget behaviors in the game.
To this end, we proposed a Controller agent in our system, which is responsible for embodying characters and generating the action sequence of each robot that represents these characters.
The Controller agent is designed to gather information from two sources: the gadget commands outputted by the Coordinator agent, along with the physical location data of the robots at the given time.
Next, we asked the Controller to generate the action sequences for the robots using the meta-actions, while simultaneously considering the logical flow of the game and the gameplay experience.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p5">
    <p class="ltx_p" id="S4.SS2.p5.1">
     To ensure these action sequences are properly formatted for the robot actuation, we designed a Chain-of-Thought (CoT) prompting
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wei et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib33" title="">
       2022
      </a>
      )
     </cite>
     for the Controller.
The CoT prompting of the Controller unfolds as follows: (1) Output a textual description of the action sequence of the robots that will move, along with the current location information of these robots; (2) Generate an action sequence based on the aforementioned textural description in the form of a Python dictionary. This Python dictionary encompasses ID for the robots, as well as details for each subsequent meta-action, including destination locations/angles, speeds, types of movement, etc.
Furthermore, we utilized in-context learning and few-shot learning approaches
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2021
      </a>
      )
     </cite>
     , which provide specific examples in the prompts to demonstrate the process of the aforementioned CoT prompting, to assist the agent in effectively learning how to generate and plan the motion sequences.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p6">
    <p class="ltx_p" id="S4.SS2.p6.1">
     During use, users first input the game’s description and rules to the system, in order to declare the game to play.
The system then understands and initiates the game based on the inherent knowledge of LLMs.
Then, users continuously input the game commands to the system to engage with the game.
The two agents in the system analyze these commands and the ongoing contextual information of the game, reasoning the gadget behaviors in the game, and then generating the action sequences to actuate the motion of SUI robots, embodying the interactions of the gadgets with users.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="S4.F4.g1" src="/html/2407.17086/assets/figure/design_space_example.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4.
     </span>
     An example using our kit: (a) Users can declare the game they are playing through an introduction and rules entry. (b) Then, they input game commands based on the scenario to interact with the gadget. The gadget generates corresponding action sequences based on a two-agent system within the kit.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS2.p7">
    <p class="ltx_p" id="S4.SS2.p7.1">
     Here we presented a specific case of using the two-agent system to play a chess game.
The comprehensive process of the interaction was illustrated in Figure
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2. Complex Motion Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , including the initiation step of the game and a typical step during one round of the game.
In the initiation step, the user first inputted the game name and a piece of introduction to the system (see Supplementary Material).
The Coordinator then initialized the game based on the user’s input and outputted the response as shown in Figure
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2. Complex Motion Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     a.
Note that in this step, since the user did not input any specific game commands, the Controller agent did not generate specific action sequences, either.
Subsequently, as the game started, the user continuously gave commands to the system.
For instance, in the first round, the user inputted a command: ”Move the pawn from d2 to d4”.
Next, the Coordinator responded to the command and informed the Controller of the gadget behavior of the pawn actuated by the user, as shown in the middle block in
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2. Complex Motion Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     b.
The Controlled then received the gadget behavior of the pawn, along with the actual location data (shown in
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2. Complex Motion Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     c) of the robot that represented the pawn gadget, proceeding to generate the corresponding action sequence for the pawn robot through the CoT process.
The output action sequence of the pawn robot is shown in the right block in
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2. Complex Motion Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     b.
The server in the system then decoded the action sequence from the Controller, sent the motion commands to the pawn gadget, and then actuated the movement of the pawn gadget, shown in
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2. Complex Motion Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     d.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3.
    </span>
    Interactive Behavior Planning
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     Generally, in the context of utilizing SUI for tabletop games, depending on the specific game scenario, SUI often needs to control the movements of robots and translate various game operations into tangible interactive behaviors
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nakagaki et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2020
      </a>
      )
     </cite>
     .
However, in practice, we have found that due to the complexity and specificity of various interactive behaviors, the agents in our system, particularly the Controller, may not be able to rely solely on the generic prompts to realize all of these tangible interactive behaviors.
To address this challenge, we have enhanced the Controller’s capability by incorporating several sets of additional prompts (add-on prompts) following a one/few-shot learning scheme. This approach aids the Controller in comprehending certain specific robotic operations (abilities) applied to different contexts. We anticipate that these add-on prompts will optimize the system’s ability to generate action sequences for Gadgets across various game scenarios.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     To determine which interactive behaviors required the design of additional prompts, we referenced past work in SUI
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nakagaki et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2020
      </a>
      ; Suzuki et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2019
      </a>
      ; Ihara et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2023
      </a>
      ; Guo et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      ; Peng et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2020
      </a>
      ; Yu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023
      </a>
      ; Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2022
      </a>
      )
     </cite>
     and recruited five designers with a background in Human-Computer Interaction (HCI) and experience in tabletop games for an informal interview and brainstorming session. During this process, we identified four common types of interaction behavior related to Gadget operations in tabletop games, including Objective Actuation, Symbol Visualization, Non-verbal Expression, and Scene Interaction. After that, we designed additional prompts for the Controller based on each type of interaction behavior, as detailed below.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S4.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.1.
     </span>
     Object Actuation
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS1.p1">
     <p class="ltx_p" id="S4.SS3.SSS1.p1.1">
      In tabletop games based on SUI, the ability of the system to automatically manipulate objects on the table using robotic gadgets
      <cite class="ltx_cite ltx_citemacro_citep">
       (Ihara et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib10" title="">
        2023
       </a>
       )
      </cite>
      enables the automation of game prop operations or simulates interactions between players and AI opponents or remote players. To this end, we added a set of additional prompts for the controller to assist in generating action sequences for our Gadgets to ”actuate objects”. In these prompts, we specify that the Controller should focus on the speed and trajectory of robot movement to achieve object movement along a prescribed path and simulate the properties of objects, such as weight. We also included a specific example in the prompts, which involves the task of pushing a heavy box to a designated location and the expected action sequence generated by the Controller.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS1.p2">
     <p class="ltx_p" id="S4.SS3.SSS1.p2.1">
      After incorporating the add-on prompt for object actuation into the Controller’s original prompt, we test the after-modified effectiveness by using the Controller to actuate the Gadgets in scenarios involving pushing heavy objects.
We tested the effectiveness of using the controller to actuate the Gadgets in pushing light objects. For instance, it was stipulated that the Gadgets start at a position (1, 1) and need to kick a light plastic soccer ball located at (3, 3). The outcome of the Gadget’s movement and the action sequences generated by the controller are depicted in Figure
      <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.3.1. Object Actuation ‣ 4.3. Interactive Behavior Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      a-b.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS1.p3">
     <p class="ltx_p" id="S4.SS3.SSS1.p3.1">
      Also, in another example, two gadgets are initially located at (1, 1) and (3, 1), and we require the gadgets to push the two very heavy doors to (1, 4) (3, 4) from (1, 3) and (3, 3).
The demonstration of the movements of the Gadgets and the action sequence generated by the Controller is shown in Figure
      <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.3.1. Object Actuation ‣ 4.3. Interactive Behavior Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      c-d.
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F5">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="427" id="S4.F5.g1" src="/html/2407.17086/assets/figure/4.3.1_actuation.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 5.
      </span>
      The gadget starts at position (1,1) and needs to kick a light plastic soccer ball located at (3,3). a) Gadget: Moves northeast at speed 3 to approach the soccer ball’s location b)Kicks the soccer ball, simulating the action by moving east to (3,3). c-d) Two gadgets open a door together by moving towards it.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.2.
     </span>
     Symbol Visualization
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS2.p1">
     <p class="ltx_p" id="S4.SS3.SSS2.p1.1">
      In SUI-based tabletop games, the system’s ability to use robotic gadgets to visualize certain graphic symbols can enhance the presentation of textual or graphical information in the game [74]. For this purpose, we added a set of add-on prompts for the Controller to assist in generating action sequences for swarm robots (the Gadgets) for Symbol Visualization.
Specifically, we designed two methods for visualizing graphic symbols: ”trajectory tracing” and ”robotic formation,” along with corresponding prompts for each method. To make this visualization function more consistent, we also added certain rules, such as ”not inverting the symbols vertically” and ”using uppercase letters for English alphabets”. We also provided an example for each visualization method in the add-on prompt, including a description of a natural language task for visualizing the letters ”HCI,” along with the expected action sequence the Controller should generate. Detailed prompts are provided in the Supplementary Material.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS2.p2">
     <p class="ltx_p" id="S4.SS3.SSS2.p2.1">
      After incorporating the add-on prompt for symbol visualization into the Controller, we tested its effectiveness by using the Controller to drive certain Gadgets in visualizing ”UIST.” For the ”tracing” method, the Controller successfully determined to utilize four Gadgets and generated appropriate trajectories of movement for each Gadget. The trajectory patterns and the action sequences that formed these trajectories are illustrated in Figure
      <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.3.2. Symbol Visualization ‣ 4.3. Interactive Behavior Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      a-g.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS2.p3">
     <p class="ltx_p" id="S4.SS3.SSS2.p3.1">
      For the ”formation” method, the Controller determined to utilize multiple robots to separately form the shapes of the letters ”H” and ”I.” The effectiveness of the robotic forming these letters and the action sequences used are illustrated in Figure
      <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.3.2. Symbol Visualization ‣ 4.3. Interactive Behavior Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      h-i.
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F6">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="437" id="S4.F6.g1" src="/html/2407.17086/assets/figure/4.3.2_information.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 6.
      </span>
      The gadgets were asked to present the word ’UIST’ or ’HI’. a-d) Agents generated trajectories of four gadgets, which were arranged to form ’U’, ’I’, ’S’. e-f) A detailed demonstration of how one of the gadgets presents the letter ”T” through its movement trajectory. h-i) Agents make each letter composed of multiple gadgets, using 7 and 3 gadgets to form the letters ’H’ and ’I’ respectively.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS3.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.3.
     </span>
     Non-Verbal Expression
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS3.p1">
     <p class="ltx_p" id="S4.SS3.SSS3.p1.1">
      In SUI-based tabletop games, the ability of the system to use robotic gadgets for non-verbal expressions, such as displaying characters’ emotions, can significantly enhance the narrative expressiveness of tabletop games
      <cite class="ltx_cite ltx_citemacro_citep">
       (Peng et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib21" title="">
        2020
       </a>
       )
      </cite>
      . To this end, we incorporate a set of add-on prompts for the Controller to assist in generating action sequences for swarm robots for ”non-verbal expression.” Specifically, we designed two methods of non-verbal expression: ”mood expression” and ”social expression,” along with corresponding prompts for each method. We also provided an example for each method in the prompts, along with the expected action sequences the Controller should generate. The examples include asking the Gadget to express sadness and a greeting between two Gadgets. Related prompts are detailed in the Supplementary Material.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS3.p2">
     <p class="ltx_p" id="S4.SS3.SSS3.p2.1">
      After incorporating the add-on prompt for non-verbal expression into the Controller, we first tested the effectiveness of using the controller to drive a single Gadget to express excitement. The demonstration of this expression and the corresponding action sequences are illustrated in the Figure
      <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.3.3. Non-Verbal Expression ‣ 4.3. Interactive Behavior Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        7
       </span>
      </a>
      a-b.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS3.p3">
     <p class="ltx_p" id="S4.SS3.SSS3.p3.1">
      Next, we tested the effectiveness of using the controller to drive two Gadgets to express a disputing social behavior. The demonstration of this expression and the corresponding action sequences are illustrated in the Figure
      <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.3.3. Non-Verbal Expression ‣ 4.3. Interactive Behavior Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        7
       </span>
      </a>
      c-e.
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F7">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="301" id="S4.F7.g1" src="/html/2407.17086/assets/figure/4.3.3_non-verbal.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 7.
      </span>
      a-b) A single gadget expresses excitement, the gadgets move upward a little and rotate for a circle. c-e) Two gadgets express an argument, they move toward each other, each rotates for a circle during the argument, and each retreats when the argument is over.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS3.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.4.
     </span>
     Scene Interaction
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS4.p1">
     <p class="ltx_p" id="S4.SS3.SSS4.p1.1">
      In SUI-based tabletop games, as it is expected that a robotic gadget should be able to perform actions such as storytelling within a scene, the gadget’s behavior may need to interact with the settings of map environment
      <cite class="ltx_cite ltx_citemacro_citep">
       (Guo et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib7" title="">
        2023
       </a>
       )
      </cite>
      . To this end, we incorporate a set of add-on prompts for the Controller to assist in generating action sequences for swarm robots for ”scene interaction.” Specifically, we proposed that the Controller possesses a ”global perspective” in the Gadget’s action planning, taking into account additional map information, scene props, and even the positions or statuses of other Gadgets among other factors. We also provided an example for each method in the prompts, along with the expected action sequences the Controller should generate. The add-on prompt includes an example demonstrating a Gadget navigating around an obstacle of a certain length to reach the other side of the obstacle. Related prompts are specified in the Supplementary Material.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS4.p2">
     <p class="ltx_p" id="S4.SS3.SSS4.p2.1">
      After incorporating the add-on prompt for scene interaction into the Controller, we tested it with an example of navigating a Gadget around an obstacle formed by three other Gadgets. The demonstration of this expression and the corresponding action sequences are illustrated in Figure
      <a class="ltx_ref" href="#S4.F8" title="Figure 8 ‣ 4.3.4. Scene Interaction ‣ 4.3. Interactive Behavior Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        8
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F8">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S4.F8.g1" src="/html/2407.17086/assets/figure/4.3.4_environment.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 8.
      </span>
      The gadget goes around a wall consisting of three other gadgets, approaching the obstacle before moving to the south side of the obstacle and moving east to get over the obstacle.
     </figcaption>
    </figure>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4.
    </span>
    Interaction Relationship Planning
   </h3>
   <div class="ltx_para" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     Within the multiple interaction rounds in a game, it is crucial for agents to reflect upon the relational structure underpinning their engagements with players. This encompasses scenarios where agents may either confront or collaborate with players, or independently modulate their responses in alignment with the unfolding narrative, thereby contributing to the thematic ambiance. Drawing from this premise, researchers’ investigations have distilled the potential stances an agent might assume into four distinct categories: Apprentice, Competitor, Teammate, or Designer
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2021
      </a>
      )
     </cite>
     .
To enable agents to grasp the relationship between human-computer interaction relationships and the generation of interactive behaviors within the game, we have augmented the Controller with the additional prompt (add-on prompt) to understand the varying interaction relationships.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S4.SS4.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.4.1.
     </span>
     Apprentice
    </h4>
    <div class="ltx_para" id="S4.SS4.SSS1.p1">
     <p class="ltx_p" id="S4.SS4.SSS1.p1.1">
      Within the context of most tabletop gaming environments, Gadgets in our system typically embody the player’s avatar or operate as player-controlled entities
      <span class="ltx_note ltx_role_footnote" id="footnote5">
       <sup class="ltx_note_mark">
        5
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          5
         </sup>
         <span class="ltx_tag ltx_tag_note">
          5
         </span>
         http://scruffygrognard.com/
        </span>
       </span>
      </span>
      .
We believe that the ability of the system to act as an ”apprentice,” autonomously and precisely modifying the actions of robotic gadgets in response to users’ suggestions, would significantly enhance personalized interactive behaviors.
To achieve this, we established a set of additional prompts for the Controller, enabling it to refer to and adjust its action planning as much as possible according to the user’s guidance. The specific prompts are detailed in the Supplementary Material.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS4.SSS1.p2">
     <p class="ltx_p" id="S4.SS4.SSS1.p2.1">
      After incorporating the add-on prompt for ”apprentice” into the Controller, we tested it with an example of requesting to ”speed up Gadgets”. The following is an example of an action sequence generated by the Controller controlling a Gadget to move from (5,5) to (10,10) on the grid map. The translation (movement) speed is set to 2.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS4.SSS1.p3">
     <p class="ltx_p" id="S4.SS4.SSS1.p3.1">
      Following this, if we then request the Gadget to move faster, the Controller updates and generates the following sequence of actions as Figure
      <a class="ltx_ref" href="#S4.F9" title="Figure 9 ‣ 4.4.1. Apprentice ‣ 4.4. Interaction Relationship Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        9
       </span>
      </a>
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F9">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S4.F9.g1" src="/html/2407.17086/assets/figure/4.4.1_apprentice.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 9.
      </span>
      The gadget was asked to move from (5, 5) to (10, 10), and after being asked to speed up, the gadget moved faster.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS4.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.4.2.
     </span>
     Opponent
    </h4>
    <div class="ltx_para" id="S4.SS4.SSS2.p1">
     <p class="ltx_p" id="S4.SS4.SSS2.p1.1">
      Many tabletop games feature the roles of opponents not controlled by players, engaging in competitive activities with the players. We believe it can enrich the gaming experience of tabletop games if our system comprehends the concept of ”opponent”, establishes non-player-controlled opponent roles, and uses the Gadgets to materialize their interactions. To facilitate this, we have established a set of additional prompts for the Controller, helping it generate and enact the roles of single or multiple opponents for competition or matches in games.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS4.SSS2.p2">
     <p class="ltx_p" id="S4.SS4.SSS2.p2.1">
      We specify in the prompt the planning about the principles and duties that the Controller should adhere to when generating ”Opponent” roles
      <cite class="ltx_cite ltx_citemacro_citep">
       (Raman et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib23" title="">
        2022
       </a>
       )
      </cite>
      . For example, we set the goal of the opponent ”… to challenge the opponent characters through strategy and decision-making while keeping the game fair and enjoyable.” Furthermore, we have defined a mechanism for the Controller how to analyze player behavior and dynamically formulate challenging interaction strategies. For example, we specify in the prompt that the Controller ”… formulate challenging strategies based on the current state of the game and the behaviors of opponent characters.” The prompts are specified in the Supplementary Material.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS4.SSS2.p3">
     <p class="ltx_p" id="S4.SS4.SSS2.p3.1">
      After incorporating the add-on prompt for ”opponent” into the Controller, we tested it with an example of using Gadgets for combat behavior. In this example, one gadget embodies the role of a Monster1 commanded by the user (with action sequences generated by the Controller), while another gadget was controlled by the system, acting as an opponent Monster2.
After the Monster1 is commanded to use Thunderbolt to attack, the Controller generates the action sequence of their battle as Figure
      <a class="ltx_ref" href="#S4.F10" title="Figure 10 ‣ 4.4.2. Opponent ‣ 4.4. Interaction Relationship Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        10
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F10">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S4.F10.g1" src="/html/2407.17086/assets/figure/4.4.2_opponent.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 10.
      </span>
      Monster1 is asked to attack Monster2, and Monster2 responds being attacked.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS4.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.4.3.
     </span>
     Teammate
    </h4>
    <div class="ltx_para" id="S4.SS4.SSS3.p1">
     <p class="ltx_p" id="S4.SS4.SSS3.p1.1">
      Similar to opponents, in tabletop gaming, there exist numerous non-player controlled characters that support the player by collaborating to accomplish tasks or combat adversaries. Enriching the gaming experience becomes more feasible if our system can comprehend the ”teammate (ally)” relationship, autonomously establish these non-player controlled teammate roles, and materialize interactions using the Gadgets. To achieve this, we have also developed a set of add-on prompts for the Controller, fostering it in generating and embodying one or multiple teammate roles. We specify in the prompt the planning about the principles and duties that the Controller should adhere to when generating ”Teammate” roles
      <cite class="ltx_cite ltx_citemacro_citep">
       (Raman et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib23" title="">
        2022
       </a>
       )
      </cite>
      . For example, we set a goal for the teammate ”… is to support teammate characters through effective collaboration and strategy, ensuring the success of the entire team and the enjoyment of the game…” Furthermore, we have established guidelines in the prompts for how Controllers can support teammate roles through effective collaboration and strategic planning, such as ”providing necessary support to help teammate characters overcome challenges while ensuring not to overshadow them, maintaining the game’s balance and interest.”, to ensure the success of the entire team and enhance the enjoyment of the game. The prompts are specified in the Supplementary Material.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS4.SSS3.p2">
     <p class="ltx_p" id="S4.SS4.SSS3.p2.1">
      After incorporating the add-on prompt for ”teammate” into the Controller, we tested it with an example of using Gadgets for collaborative combat behavior as Figure
      <a class="ltx_ref" href="#S4.F11" title="Figure 11 ‣ 4.4.3. Teammate ‣ 4.4. Interaction Relationship Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        11
       </span>
      </a>
      . In this scenario, one Gadget embodies the role of a Monster1 commanded by the user (with action sequences generated by the Controller), while two other Gadgets, generated and governed by the Controller, take on the roles of a teammate Monster2 and an opponent Monster3, respectively.
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F11">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="S4.F11.g1" src="/html/2407.17086/assets/figure/4.4.3_teammate.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 11.
      </span>
      Gadgets that can generate collaborative behavior act as a teammate of the user and attack the enemy together.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS4.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.4.4.
     </span>
     Designer
    </h4>
    <div class="ltx_para" id="S4.SS4.SSS4.p1">
     <p class="ltx_p" id="S4.SS4.SSS4.p1.1">
      In tabletop games, more roles that serve to enhance the narrative, such as NPCs (Non-Player Characters) or props, may also be present. For instance, upon the players and their teammates defeating adversaries, villager NPCs can be configured to celebrate players’ victory. If the Controller can act as a ”designer,” generating these roles based on the game’s narrative and materializing their actions through gadgets, then it would elevate the dramatic effect and overall experience of the game. To facilitate this, we have developed a set of add-on prompts for the Controller, enabling it to ”… spontaneously generate new characters, NPCs, items, or plots, as well as the corresponding robotic action sequences, to advance the game storyline.” Related prompts are detailed in the Supplementary Material.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS4.SSS4.p2">
     <p class="ltx_p" id="S4.SS4.SSS4.p2.1">
      After incorporating the add-on prompt for ”designer” into the Controller, we tested it with an example focusing on a pivotal narrative moment following a battle victory. Specifically, when the user-controlled Monster1 and the teammate Monster2 defeated Monster3, the Controller generated a storyline in which villager NPCs appeared to celebrate the victory. To bring this celebratory behavior to life, the Controller then invoked three new Gadgets designed for this purpose as Figure
      <a class="ltx_ref" href="#S4.F12" title="Figure 12 ‣ 4.4.4. Designer ‣ 4.4. Interaction Relationship Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
       <span class="ltx_text ltx_ref_tag">
        12
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F12">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S4.F12.g1" src="/html/2407.17086/assets/figure/4.4.4_designer.png" width="598"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 12.
      </span>
      Three new NPCs generated by the agent celebrate the user’s victory together, they surround the player, each taking a step forward and expressing excitement by spinning.
     </figcaption>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5.
   </span>
   Example Application
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    We designed several different tabletop games using the AI-Gadget Kit to demonstrate its capabilities with a two-agent system and the effectiveness of the eight add-on prompts (comprising four Interaction Behaviors and four Interaction Relationships). For each game played, we followed the procedure outlined in Figure
    <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2. Complex Motion Planning ‣ 4. Multi-agent system ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    , beginning with giving the game’s introduction and rules to the AI-Gadget Kit (details provided in the Supplementary Material). This initial step involved declaring the game being played. Subsequently, based on the game scenario, we provide game commands to interact with the Gadgets, engaging in an interactive gameplay experience.
   </p>
  </div>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1.
    </span>
    Soccer-Ball-Shooting Game
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     Using the AI-Gadget Kit, we implemented a soccer-ball-shooting game, as illustrated in the Figure
     <a class="ltx_ref" href="#S5.F13" title="Figure 13 ‣ 5.1. Soccer-Ball-Shooting Game ‣ 5. Example Application ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       13
      </span>
     </a>
     . The setup includes a stationary goal on the tabletop, with players and a Gadget taking turns to shoot the ball. Players propel the ball using two fingers, while the Gadget executes its shots by striking the ball.
We began by providing the game rules into the system (details in the Supplementary Material). Once the Coordinator comprehended the rules, it assumed the roles of both host and referee within the game. It then tasked the Controller with deploying an opponent role to control the action sequences of the Gadget.
The Kit shall leverage the capability through the ”object actuation” add-on prompt of the Controller to simulate and execute collision planning based on the position of the goal and the ball. When it is the Gadget turn to take a shot, Controller generates the opponent’s direction and speed by considering factors such as the distance of the ball from the goal, the ball’s orientation relative to the goal, and the speed required to propel the ball.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     Figure
     <a class="ltx_ref" href="#S5.F13" title="Figure 13 ‣ 5.1. Soccer-Ball-Shooting Game ‣ 5. Example Application ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       13
      </span>
     </a>
     demonstrates the performance of the system in this gaming scenario, highlighting the Gadget acting as the opponent. After the player takes a shot, the Gadget autonomously aims for the kickoff spot, moves towards the ball, and pushes it towards the goal.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F13">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="391" id="S5.F13.g1" src="/html/2407.17086/assets/figure/5.1_soccer.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 13.
     </span>
     Human player and AI player play soccer games together.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS1.p3">
    <p class="ltx_p" id="S5.SS1.p3.1">
     Additionally, the Controller monitors past scores to adjust strategies to enhance gameplay and competitiveness. For instance, in a single game session, if the player fails to score consecutively over multiple attempts, the opponent’s accuracy will be dynamically lowered in subsequent rounds. This adjustment is designed to enrich the users’ gaming experience by maintaining a competitive balance and ensuring that the game remains engaging and challenging without becoming discouragingly difficult.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2.
    </span>
    Turn-Based Strategy Game
   </h3>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     Within the Turn-Based Strategy (TBS) game scenarios, the system is designed to support battles between players whose commands are embodied through Gadget controlling (Player vs. Player, or PVP) as well as battles where the Gadget itself acts as the opponent (Player vs. Environment, or PVE).
During gameplay, robotic gadgets assume roles as combatants, generating robotic action commands based on the attack targets and skills released as designated by the players. Following the game rules outlined, the Coordinator updates and transmits game information during interactions, such as the status and capabilities of the gadgets. The Controller then synthesizes this information to produce corresponding sequences of actions. In combat scenarios, our Kit primarily relies on an apprentice, opponent, and non-verbal expression add-on prompts. It adheres to the players’ interaction commands (such as attack/defense, skill deployment, etc.) to generate corresponding interactive behaviors.
As illustrated in the figure
     <a class="ltx_ref" href="#S5.F14" title="Figure 14 ‣ 5.2. Turn-Based Strategy Game ‣ 5. Example Application ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       14
      </span>
     </a>
     , when a player commands their character to ”attack the enemy with the power of thunderbolt,” the kit generates an attack action directed at the opposing Gadget.
Considering the characteristics of ”thunderbolt,” the kit is designed with actions for the attacking gadget that simulate the buildup of electrical charge followed by a swift charge forward, and for the opponent gadget, it simulates the motion of being electrocuted with on-the-spot swaying movements. Moreover, through the add-on prompts of teammates and designers within our kit, TBS games can accommodate a larger number of gadgets for combat or interaction. This can complement the combat theater in existing Dungeons and Dragons (D&amp;D) type games.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F14">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S5.F14.g1" src="/html/2407.17086/assets/figure/5.2_TBS.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 14.
     </span>
     Robotic gadget plays turn-based strategy games with human players.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3.
    </span>
    Yes Or No?
   </h3>
   <div class="ltx_para" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     In many games, there are moments where a system provides Yes or No responses, such as ability checks and answering questions within the narrative in Dungeons and Dragons (D&amp;D) games. Our kit enhances the presentation of Yes and No answers in these instances, making them more engaging.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     For instance, we have designed a quiz game
     <a class="ltx_ref" href="#S5.F15" title="Figure 15 ‣ 5.3. Yes Or No? ‣ 5. Example Application ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       15
      </span>
     </a>
     (its rules are detailed in the Supplementary Material) where the Coordinator accepts questions raised by users and provides corresponding answers.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p3">
    <p class="ltx_p" id="S5.SS3.p3.1">
     The Controller, equipped with designer capabilities, utilizes add-on prompts for symbol visualization to control a Gadget. This Gadget writes out Y (Yes) or N (No) on paper, responding affirmatively or negatively to the users’ queries.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p4">
    <p class="ltx_p" id="S5.SS3.p4.1">
     To mitigate the impact of friction between the pen tip and paper on the robotic gadget’s movement, we employed two gadgets to hold a pen jointly for drawing. This design is intended to offer users a more suspenseful and immersive gaming experience.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F15">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="378" id="S5.F15.g1" src="/html/2407.17086/assets/figure/5.3_Y_and_N.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 15.
     </span>
     Robotic gadgets answer ’Yes’ or ’No’ to user questions by driving a pen to write on paper.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.4.
    </span>
    Improvisational Theater
   </h3>
   <div class="ltx_para" id="S5.SS4.p1">
    <p class="ltx_p" id="S5.SS4.p1.1">
     Improvisational theater is a form of group drama without a fixed script, where most of the content is spontaneously created by the performers during the performance
     <cite class="ltx_cite ltx_citemacro_citep">
      (Johnstone,
      <a class="ltx_ref" href="#bib.bib12" title="">
       2012
      </a>
      )
     </cite>
     . Integrating AI-Gadget Kit, we have designed an application for users to perform improvisational theater in collaboration with the Gadgets. In this setup, Gadgets act as performers, with one of the Gadgets representing the user. We require the Gadgets to adhere to the core principle of improvisational theater, ”Yes, and,” which means freely generating subsequent performance content, including voice and action sequences, based on the previous performer’s contribution and randomly selecting the next performer from among the users and other Gadgets. The Coordinator is responsible for assigning roles to all parties, recording and transmitting the content of the performance, and coordinating the turns of the performance.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p2">
    <p class="ltx_p" id="S5.SS4.p2.1">
     After integrating non-verbal expression add-on prompts, the Controller is capable of generating dialogue that fits the characters’ identities and the plot development, while also endowing robots with non-verbal ”mood” expressions to convey the characters’ emotional states. Moreover, by assigning specific scene information to different areas of the venue, the scene interaction capability add-on prompt enables the Controller to consider information such as the performance location when generating action sequences. In one of our improvisational theater tests set in the world of ”Hamlet” (detailed game rules can be found in the Supplementary Material), our Kit initially assigns identities to a cluster of Gadgets(Figure
     <a class="ltx_ref" href="#S5.F16" title="Figure 16 ‣ 5.4. Improvisational Theater ‣ 5. Example Application ‣ AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications">
      <span class="ltx_text ltx_ref_tag">
       16
      </span>
     </a>
     ), several following the original characters of the drama and others being original characters based on user commands.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p3">
    <p class="ltx_p" id="S5.SS4.p3.1">
     Once the performance starts, the Controller coordinates with the user, taking into account the pre-set information of the performance site, to continue the act. For example, when the robot playing Hamlet utters ”To be or not to be, that is the question,” in complete contrast to the original story, the user, who is consoled by a robot as Hamlet’s friend (an original character), expresses his comfort and informs Hamlet that Ophelia is waiting on the terrace. Based on the lines entered by the user for the performance, the Controller Agent directs the user’s robot to express emotion through a non-verbal sequence of actions. Hamlet then moved to the area representing the terrace based on the venue information and engaged in a dialog and performance with Ophelia.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p4">
    <p class="ltx_p" id="S5.SS4.p4.1">
     It is not difficult to see that AI-Gadget has the ability to respond to high degree-of-freedom user input in performance scenarios. We also tested completely original story contexts in which AI-Gadget similarly demonstrated the ability to receive feedback on improvised content. As the art form of improvisational theater is gaining traction in the areas of imagination and creativity stimulation, mental health, and social-emotional education
     <cite class="ltx_cite ltx_citemacro_citep">
      (Sawyer,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2000
      </a>
      )
     </cite>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Schwenke et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2021
      </a>
      )
     </cite>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Felsman et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib4" title="">
       2019
      </a>
      )
     </cite>
     , combined with a more specialized Emergent Story Generation strategy, AI-Gadget has the potential to provide users with a richer interactive and emotional experience.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F16">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S5.F16.g1" src="/html/2407.17086/assets/figure/5.4_Hamlet.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 16.
     </span>
     In Hamlet-themed improvisational theater, the gadget playing Hamlet flexibly reacts to the user’s words outside of the script.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6.
   </span>
   Discussion
  </h2>
  <section class="ltx_subsection" id="S6.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.1.
    </span>
    Limitation and Future Works
   </h3>
   <div class="ltx_para" id="S6.SS1.p1">
    <p class="ltx_p" id="S6.SS1.p1.1">
     In this study, we employed the Sony Toio robots as our Robotic Gadget. However, due to the Toio robots’ movement being reliant on motor differential speed control, the performance of the motors themselves can impact the robots’ action performance. This imposes limitations on the AI-Gadget Kit’s functionality. For instance, in practice, the Toio robots exhibited random deviations when executing users’ movement commands, a situation exacerbated by a shift in the center of gravity due to additional attachments to the casing.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS1.p2">
    <p class="ltx_p" id="S6.SS1.p2.1">
     To mitigate the challenges faced by the Gadget due to the aforementioned factors, we are considering several optimizations to the system architecture. Firstly, a more precise movement correction algorithm on the robot side could support the robots’ movement performance. With the anticipated action sequences of the Gadget, coupled with real-time positioning information based on ArUCo from cameras, the robots are capable of precise closed-loop control during action, enabling real-time adjustments. Furthermore, inspired by works such as Swarm Haptics
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kim and Follmer,
      <a class="ltx_ref" href="#bib.bib14" title="">
       2019
      </a>
      )
     </cite>
     and Hermits
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nakagaki et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2020
      </a>
      )
     </cite>
     , mechanical modifications to the robots (like increasing tire friction) could address the limitations in the Gadget’s linear driving capability and performance due to the mass and speed of swarm robots in scenarios such as object propulsion.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS1.p3">
    <p class="ltx_p" id="S6.SS1.p3.1">
     Beyond optimizing the robots’ movement capabilities, we also recognize that the current provision of meta-actions by the robot side is insufficient to meet the demand for a broader range of complex movements. For the Gadget, diverse movement modes (such as curved motion) are expected to improve the robots’ interactive experience and expressiveness. Therefore, the encapsulation and provision of more movement modes will assist the Robotic Gadget in achieving interactions with complex and rich semantics in tabletop games.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS1.p4">
    <p class="ltx_p" id="S6.SS1.p4.1">
     On the other hand, in this study, when confronted with complex board game scenarios, such as ”Improvisational Theater,” the LLM (i.e., GPT-4) which the agents rely on, may encounter performance bottlenecks due to the utilization of multiple add-ons prompts and numerous game rounds, leading to complications in processing lengthy contexts. This can result in inaccuracies and errors or the so-called ”hallucination” phenomenon of LLM, where the model generates content with inaccuracies related to the game’s rules and scenario descriptions. In such cases, the model’s output may require additional testing (e.g., The Needle In a Haystack Test) to verify its accuracy and reliability.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS1.p5">
    <p class="ltx_p" id="S6.SS1.p5.1">
     To address this challenge and enhance the agents’ performance in complex scenarios, several strategies can be considered for the future. Firstly, introducing a more powerful LLM represents a direct solution. By enhancing the model’s capability to understand and retrieve long contexts, we can directly improve agents’ performance in complex interaction scenarios. However, this approach depends on external technological advancements.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS1.p6">
    <p class="ltx_p" id="S6.SS1.p6.1">
     Furthermore, drawing inspiration from AutoGen
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2023
      </a>
      )
     </cite>
     , we can introduce multiple specialized agents to handle specific contextual challenges. These specialized agents could be designed to perform distinct functions, such as generating game rules and action sequences and analyzing the consequences of players’ moves. By isolating specific contexts, these specialized agents facilitate interaction among the main agents. They can work together to provide more accurate, efficient, and coherent content generation. For example, by distributing specific parts of the context and isolating the others, a certain agent can specialize in tracking the logical relationship between game states and player actions, while other agents may focus on generating descriptions and reactions that align with the specific game environment or scenario and action sequences of the Gadgets.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS1.p7">
    <p class="ltx_p" id="S6.SS1.p7.1">
     Through such a two-agent collaboration system, we can not only enhance the agents’ performance in complex game scenarios, improve content generation speed, and reduce token consumption to lower inference costs but also allow each agent to provide deep and precise processing within their areas of expertise. This enables the entire SUI system to better understand and reflect the complex logic of games and player interactions.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.2.
    </span>
    Beyond Action Planning
   </h3>
   <div class="ltx_para" id="S6.SS2.p1">
    <p class="ltx_p" id="S6.SS2.p1.1">
     This paper primarily focuses on the integration of SUI (Spatial User Interface) with LLM-based (Large Language Model-based) agents in tabletop game scenarios. However, the HCI (Human-Computer Interaction) field has already introduced many instances where SUI is combined with other interactive scenarios, including AR (Augmented Reality) games
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kaimoto et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2022
      </a>
      )
     </cite>
     , serious games
     <cite class="ltx_cite ltx_citemacro_citep">
      (Peng et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2020
      </a>
      )
     </cite>
     , and remote interactions
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ihara et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2023
      </a>
      )
     </cite>
     . This reveals the potential for our AI-Gadget Kit to inspire further research and applications of SUI in scenarios with complex interaction tasks. In this paper, we take the first step by incorporating LLM-based agents for automated action planning in SUI. Future expansions of our kit could enhance understanding and generation of SUI’s other interactive modalities. For instance, by integrating Add-ons like the Mechanical Shell
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nakagaki et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2020
      </a>
      )
     </cite>
     , our kit could transform SUI action planning into planning for other interactivities (e.g., Multi-DoF, Aggregation). Similarly, by generating virtual information, our kit could extend SUI action planning into dynamic interactions that blend virtual and real elements
     <cite class="ltx_cite ltx_citemacro_citep">
      (Suzuki et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2020
      </a>
      )
     </cite>
     . Moving forward, we aim to explore more LLM-generated SUI interaction modalities, advancing research and application of SUI in scenarios with complex interaction tasks.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.3.
    </span>
    Availability and Applicability
   </h3>
   <div class="ltx_para" id="S6.SS3.p1">
    <p class="ltx_p" id="S6.SS3.p1.1">
     Our proposed AI-Gadget Kit aims to assist researchers, board game designers, and players in creatively designing interactive experiences with automated gadgets using natural language. By leveraging the two-agent system and various add-on prompts included in the kit, users can easily modify the rules for generating sequences of interactive actions (for example, testing can be conducted directly on the OpenAI GPT-4 web client).
    </p>
   </div>
   <div class="ltx_para" id="S6.SS3.p2">
    <p class="ltx_p" id="S6.SS3.p2.1">
     However, we have identified obstacles encountered when attempting to rapidly debug the dynamic effects of action sequences generated by the Kit. Users must either conduct live tests with a corresponding number of gadgets for different scenarios or plot the trajectories on a coordinate system based on the content of the action sequences to evaluate the agent-generated content. This impedes the visualization of creative interactive ideas of the users.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS3.p3">
    <p class="ltx_p" id="S6.SS3.p3.1">
     Recently, WRLKits
     <cite class="ltx_cite ltx_citemacro_citep">
      (Saberpour Abadian et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023
      </a>
      )
     </cite>
     demonstrated a tool based on the interactive computational design approach, which visually assists designers in rapidly building personalized prototypes. Similarly, works like Habitat-Sim
     <cite class="ltx_cite ltx_citemacro_citep">
      (Savva et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2019
      </a>
      )
     </cite>
     and AI2-THOR
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kolve et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2022
      </a>
      )
     </cite>
     have proven the viability of simulation platforms for robot interaction design. In the future, we plan to develop simulation and design tools for the AI-Gadget Kit, enabling visualizations on web or client platforms for designing personalized interactions, thus making it more accessible and usable by many.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS3.p4">
    <p class="ltx_p" id="S6.SS3.p4.1">
     In addition, we have noted that the hardware costs associated with SUI capable of driving gadgets for interaction may hinder the widespread adoption of this work in board game scenarios. This is because most board game café may find it challenging to afford the costs of bulk purchasing expensive hardware platforms, such as the Sony Toio platform. To further promote our work, we will explore the development of lower-cost hardware platforms for the AI-Gadget Kit in the future.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7.
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    While Swarm User Interfaces (SUIs) have succeeded in enriching tangible interaction experiences, their limitations in autonomous action planning have hindered the potential for personalized and dynamic interaction generation in tabletop games.
In this paper, We proposed an AI-Gadget Kit, a multi-agent SUI tabletop gaming system, which is designed to facilitate dynamic and complex interaction tasks in tabletop games.
We first introduced the system architecture of the AI-gadget Kit, which includes a set of swarm robots to perform the gadget behaviors, and a multi-agent system responsible for executing the game and generating action plans for the swarm robots.
We then elaborated the design of the multi-agent system, comprising a series of meta-motions for individual robots, two LLM-based agents for complex action planning, and a set of add-on prompts aimed at reinforcing the understanding and reacting capabilities of the agents.
At last, we demonstrate four application examples using AI-gadget Kit to showcase the effect of the multi-agent-driven SUI on executing complex interaction tasks in tabletop games.
We aimed to use our work as a case study to explore and inspire the application of LLMs on action planning of SUI in multiple scenarios with complex interaction tasks.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     (1)
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brock et al
     <span class="ltx_text" id="bib.bib2.2.2.1">
      .
     </span>
     (2021)
    </span>
    <span class="ltx_bibblock">
     Heike Brock, Selma Šabanović, and Randy Gomez. 2021.
    </span>
    <span class="ltx_bibblock">
     Remote You, Haru and Me: Exploring Social Interaction in Telepresence Gaming With a Robotic Agent. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">
      Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction
     </em>
     (Boulder, CO, USA)
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.4.2">
      (HRI ’21 Companion)
     </em>
     . Association for Computing Machinery, New York, NY, USA, 283–287.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3434074.3447177" target="_blank" title="">
      https://doi.org/10.1145/3434074.3447177
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dementyev et al
     <span class="ltx_text" id="bib.bib3.2.2.1">
      .
     </span>
     (2016)
    </span>
    <span class="ltx_bibblock">
     Artem Dementyev, Hsin-Liu (Cindy) Kao, Inrak Choi, Deborah Ajilo, Maggie Xu, Joseph A. Paradiso, Chris Schmandt, and Sean Follmer. 2016.
    </span>
    <span class="ltx_bibblock">
     Rovables: Miniature On-Body Robots as Mobile Wearables. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">
      Proceedings of the 29th Annual Symposium on User Interface Software and Technology
     </em>
     (Tokyo, Japan)
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.2">
      (UIST ’16)
     </em>
     . Association for Computing Machinery, New York, NY, USA, 111–120.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2984511.2984531" target="_blank" title="">
      https://doi.org/10.1145/2984511.2984531
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Felsman et al
     <span class="ltx_text" id="bib.bib4.2.2.1">
      .
     </span>
     (2019)
    </span>
    <span class="ltx_bibblock">
     Peter Felsman, Colleen M. Seifert, and Joseph A. Himle. 2019.
    </span>
    <span class="ltx_bibblock">
     The use of improvisational theater training to reduce social anxiety in adolescents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">
      The Arts in Psychotherapy
     </em>
     63 (2019), 111–117.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.aip.2018.12.001" target="_blank" title="">
      https://doi.org/10.1016/j.aip.2018.12.001
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gan et al
     <span class="ltx_text" id="bib.bib5.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Chuang Gan, Yiwei Zhang, Jiajun Wu, Boqing Gong, and Joshua B Tenenbaum. 2020.
    </span>
    <span class="ltx_bibblock">
     Look, listen, and act: Towards audio-visual embodied navigation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">
      2020 IEEE International Conference on Robotics and Automation (ICRA)
     </em>
     . IEEE, 9701–9707.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gillet et al
     <span class="ltx_text" id="bib.bib6.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Sarah Gillet, Wouter van den Bos, Iolanda Leite, et al
     <span class="ltx_text" id="bib.bib6.3.1">
      .
     </span>
     2020.
    </span>
    <span class="ltx_bibblock">
     A social robot mediator to foster collaboration and inclusion among children.. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.1">
      Robotics: Science and Systems
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al
     <span class="ltx_text" id="bib.bib7.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yijie Guo, Zhenhan Huang, Ruhan Wang, Chih-Heng Li, Ruoyu Wu, Qirui Sun, Zhihao Yao, Haipeng Mi, and Yu Peng. 2023.
    </span>
    <span class="ltx_bibblock">
     Sparkybot:An Embodied AI Agent-Powered Robot with Customizable Characters andInteraction Behavior for Children. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">
      Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology
     </em>
     (¡conf-loc¿, ¡city¿San Francisco¡/city¿, ¡state¿CA¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿)
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.2">
      (UIST ’23 Adjunct)
     </em>
     . Association for Computing Machinery, New York, NY, USA, Article 90, 3 pages.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3586182.3615804" target="_blank" title="">
      https://doi.org/10.1145/3586182.3615804
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al
     <span class="ltx_text" id="bib.bib8.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. 2023b.
    </span>
    <span class="ltx_bibblock">
     Voxposer: Composable 3d value maps for robotic manipulation with language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">
      arXiv preprint arXiv:2307.05973
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al
     <span class="ltx_text" id="bib.bib9.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Xiaoyu Huang, Dhruv Batra, Akshara Rai, and Andrew Szot. 2023a.
    </span>
    <span class="ltx_bibblock">
     Skill transformer: A monolithic policy for mobile manipulation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     . 10852–10862.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ihara et al
     <span class="ltx_text" id="bib.bib10.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Keiichi Ihara, Mehrad Faridan, Ayumi Ichikawa, Ikkaku Kawaguchi, and Ryo Suzuki. 2023.
    </span>
    <span class="ltx_bibblock">
     HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">
      Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology
     </em>
     (¡conf-loc¿, ¡city¿San Francisco¡/city¿, ¡state¿CA¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿)
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.4.2">
      (UIST ’23)
     </em>
     . Association for Computing Machinery, New York, NY, USA, Article 119, 12 pages.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3586183.3606727" target="_blank" title="">
      https://doi.org/10.1145/3586183.3606727
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jariyavajee et al
     <span class="ltx_text" id="bib.bib11.2.2.1">
      .
     </span>
     (2018)
    </span>
    <span class="ltx_bibblock">
     Chattriya Jariyavajee, Arnon Visavakitcharoen, Preeyaphond Sirimaha, Booncharoen Sirinaovakul, and Jumpol Polvichai. 2018.
    </span>
    <span class="ltx_bibblock">
     A Practical interactive chess board with automatic movement control. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">
      2018 Global Wireless Summit (GWS)
     </em>
     . IEEE, 246–250.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Johnstone (2012)
    </span>
    <span class="ltx_bibblock">
     Keith Johnstone. 2012.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Impro: Improvisation and the theatre
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Routledge.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kaimoto et al
     <span class="ltx_text" id="bib.bib13.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Hiroki Kaimoto, Kyzyl Monteiro, Mehrad Faridan, Jiatong Li, Samin Farajian, Yasuaki Kakehi, Ken Nakagaki, and Ryo Suzuki. 2022.
    </span>
    <span class="ltx_bibblock">
     Sketched reality: Sketching bi-directional interactions between virtual and physical worlds with ar and actuated tangible ui. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">
      Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology
     </em>
     . 1–12.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim and Follmer (2019)
    </span>
    <span class="ltx_bibblock">
     Lawrence H. Kim and Sean Follmer. 2019.
    </span>
    <span class="ltx_bibblock">
     SwarmHaptics: Haptic Display with Swarm Robots. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
     </em>
     (Glasgow, Scotland Uk)
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">
      (CHI ’19)
     </em>
     . Association for Computing Machinery, New York, NY, USA, 1–13.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3290605.3300918" target="_blank" title="">
      https://doi.org/10.1145/3290605.3300918
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kolve et al
     <span class="ltx_text" id="bib.bib15.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, and Ali Farhadi. 2022.
    </span>
    <span class="ltx_bibblock">
     AI2-THOR: An Interactive 3D Environment for Visual AI.
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:1712.05474 [cs.CV]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib16.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Jiannan Li, Maurício Sousa, Chu Li, Jessie Liu, Yan Chen, Ravin Balakrishnan, and Tovi Grossman. 2022.
    </span>
    <span class="ltx_bibblock">
     ASTEROIDS: Exploring Swarms of Mini-Telepresence Robots for Physical Skill Demonstration. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">
      Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems
     </em>
     (¡conf-loc¿, ¡city¿New Orleans¡/city¿, ¡state¿LA¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿)
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.4.2">
      (CHI ’22)
     </em>
     . Association for Computing Machinery, New York, NY, USA, Article 111, 14 pages.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3491102.3501927" target="_blank" title="">
      https://doi.org/10.1145/3491102.3501927
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al
     <span class="ltx_text" id="bib.bib17.3.2.1">
      .
     </span>
     (2021)
    </span>
    <span class="ltx_bibblock">
     Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021.
    </span>
    <span class="ltx_bibblock">
     What Makes Good In-Context Examples for GPT-
     <math alttext="3" class="ltx_Math" display="inline" id="bib.bib17.1.m1.1">
      <semantics id="bib.bib17.1.m1.1a">
       <mn id="bib.bib17.1.m1.1.1" xref="bib.bib17.1.m1.1.1.cmml">
        3
       </mn>
       <annotation-xml encoding="MathML-Content" id="bib.bib17.1.m1.1b">
        <cn id="bib.bib17.1.m1.1.1.cmml" type="integer" xref="bib.bib17.1.m1.1.1">
         3
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="bib.bib17.1.m1.1c">
        3
       </annotation>
      </semantics>
     </math>
     ?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">
      arXiv preprint arXiv:2101.06804
     </em>
     (2021).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Long et al
     <span class="ltx_text" id="bib.bib18.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yuxing Long, Xiaoqi Li, Wenzhe Cai, and Hao Dong. 2023.
    </span>
    <span class="ltx_bibblock">
     Discuss before moving: Visual language navigation via multi-expert discussions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">
      arXiv preprint arXiv:2309.11382
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Matuszek et al
     <span class="ltx_text" id="bib.bib19.2.2.1">
      .
     </span>
     (2011)
    </span>
    <span class="ltx_bibblock">
     Cynthia Matuszek, Brian Mayton, Roberto Aimi, Marc Peter Deisenroth, Liefeng Bo, Robert Chu, Mike Kung, Louis LeGrand, Joshua R Smith, and Dieter Fox. 2011.
    </span>
    <span class="ltx_bibblock">
     Gambit: An autonomous chess-playing robotic system. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">
      2011 IEEE international conference on robotics and automation
     </em>
     . IEEE, 4291–4297.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakagaki et al
     <span class="ltx_text" id="bib.bib20.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Ken Nakagaki, Joanne Leong, Jordan L. Tappa, João Wilbert, and Hiroshi Ishii. 2020.
    </span>
    <span class="ltx_bibblock">
     HERMITS: Dynamically Reconfiguring the Interactivity of Self-propelled TUIs with Mechanical Shell Add-ons. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">
      Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
     </em>
     (Virtual Event, USA)
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.4.2">
      (UIST ’20)
     </em>
     . Association for Computing Machinery, New York, NY, USA, 882–896.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3379337.3415831" target="_blank" title="">
      https://doi.org/10.1145/3379337.3415831
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Peng et al
     <span class="ltx_text" id="bib.bib21.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Yu Peng, Yuan-Ling Feng, Nan Wang, and Haipeng Mi. 2020.
    </span>
    <span class="ltx_bibblock">
     How children interpret robots’ contextual behaviors in live theatre: Gaining insights for multi-robot theatre design. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">
      2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
     </em>
     . IEEE, 327–334.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qiao et al
     <span class="ltx_text" id="bib.bib22.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, and Qi Wu. 2023.
    </span>
    <span class="ltx_bibblock">
     March in chat: Interactive prompting for remote embodied referring expression. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     . 15758–15767.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Raman et al
     <span class="ltx_text" id="bib.bib23.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. 2022.
    </span>
    <span class="ltx_bibblock">
     Planning with large language models via corrective re-prompting. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">
      NeurIPS 2022 Foundation Models for Decision Making Workshop
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Saberpour Abadian et al
     <span class="ltx_text" id="bib.bib24.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Artin Saberpour Abadian, Ata Otaran, Martin Schmitz, Marie Muehlhaus, Rishabh Dabral, Diogo Luvizon, Azumi Maekawa, Masahiko Inami, Christian Theobalt, and Jürgen Steimle. 2023.
    </span>
    <span class="ltx_bibblock">
     Computational Design of Personalized Wearable Robotic Limbs. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">
      Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology
     </em>
     (¡conf-loc¿, ¡city¿San Francisco¡/city¿, ¡state¿CA¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿)
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.4.2">
      (UIST ’23)
     </em>
     . Association for Computing Machinery, New York, NY, USA, Article 68, 13 pages.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3586183.3606748" target="_blank" title="">
      https://doi.org/10.1145/3586183.3606748
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Savva et al
     <span class="ltx_text" id="bib.bib25.2.2.1">
      .
     </span>
     (2019)
    </span>
    <span class="ltx_bibblock">
     Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al
     <span class="ltx_text" id="bib.bib25.3.1">
      .
     </span>
     2019.
    </span>
    <span class="ltx_bibblock">
     Habitat: A platform for embodied ai research. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.4.1">
      Proceedings of the IEEE/CVF international conference on computer vision
     </em>
     . 9339–9347.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sawyer (2000)
    </span>
    <span class="ltx_bibblock">
     R. Keith Sawyer. 2000.
    </span>
    <span class="ltx_bibblock">
     Improvisational Cultures: Collaborative Emergence and Creativity in Improvisation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      Mind, Culture, and Activity
     </em>
     7, 3 (2000), 180–185.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1207/S15327884MCA0703_05" target="_blank" title="">
      https://doi.org/10.1207/S15327884MCA0703_05
     </a>
     arXiv:https://doi.org/10.1207/S15327884MCA0703_05
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwenke et al
     <span class="ltx_text" id="bib.bib27.2.2.1">
      .
     </span>
     (2021)
    </span>
    <span class="ltx_bibblock">
     Diana Schwenke, Maja Dshemuchadse, Lisa Rasehorn, Dominik Klarholter, and Stefan Scherbaum. 2021.
    </span>
    <span class="ltx_bibblock">
     Improv to Improve: The Impact of Improvisational Theater on Creativity, Acceptance, and Psychological Well-Being.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">
      Journal of Creativity in Mental Health
     </em>
     16, 1 (2021), 31–48.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1080/15401383.2020.1754987" target="_blank" title="">
      https://doi.org/10.1080/15401383.2020.1754987
     </a>
     arXiv:https://doi.org/10.1080/15401383.2020.1754987
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Suzuki et al
     <span class="ltx_text" id="bib.bib28.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Ryo Suzuki, Rubaiat Habib Kazi, Li-yi Wei, Stephen DiVerdi, Wilmot Li, and Daniel Leithinger. 2020.
    </span>
    <span class="ltx_bibblock">
     RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">
      Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
     </em>
     (Virtual Event, USA)
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.4.2">
      (UIST ’20)
     </em>
     . Association for Computing Machinery, New York, NY, USA, 166–181.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3379337.3415892" target="_blank" title="">
      https://doi.org/10.1145/3379337.3415892
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Suzuki et al
     <span class="ltx_text" id="bib.bib29.2.2.1">
      .
     </span>
     (2019)
    </span>
    <span class="ltx_bibblock">
     Ryo Suzuki, Clement Zheng, Yasuaki Kakehi, Tom Yeh, Ellen Yi-Luen Do, Mark D. Gross, and Daniel Leithinger. 2019.
    </span>
    <span class="ltx_bibblock">
     ShapeBots: Shape-changing Swarm Robots. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">
      Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
     </em>
     (New Orleans, LA, USA)
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.4.2">
      (UIST ’19)
     </em>
     . Association for Computing Machinery, New York, NY, USA, 493–505.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3332165.3347911" target="_blank" title="">
      https://doi.org/10.1145/3332165.3347911
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     van Breemen et al
     <span class="ltx_text" id="bib.bib30.2.2.1">
      .
     </span>
     (2005)
    </span>
    <span class="ltx_bibblock">
     Albert van Breemen, Xue Yan, and Bernt Meerbeek. 2005.
    </span>
    <span class="ltx_bibblock">
     iCat: an animated user-interface robot with personality. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">
      Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems
     </em>
     (The Netherlands)
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.4.2">
      (AAMAS ’05)
     </em>
     . Association for Computing Machinery, New York, NY, USA, 143–144.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1082473.1082823" target="_blank" title="">
      https://doi.org/10.1145/1082473.1082823
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vaswani et al
     <span class="ltx_text" id="bib.bib31.2.2.1">
      .
     </span>
     (2017)
    </span>
    <span class="ltx_bibblock">
     Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.
    </span>
    <span class="ltx_bibblock">
     Attention is all you need.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">
      Advances in neural information processing systems
     </em>
     30 (2017).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al
     <span class="ltx_text" id="bib.bib32.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Xin Eric Wang, Vihan Jain, Eugene Ie, William Yang Wang, Zornitsa Kozareva, and Sujith Ravi. 2020.
    </span>
    <span class="ltx_bibblock">
     Environment-agnostic multitask learning for natural language grounded navigation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">
      Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV 16
     </em>
     . Springer, 413–430.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al
     <span class="ltx_text" id="bib.bib33.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al
     <span class="ltx_text" id="bib.bib33.3.1">
      .
     </span>
     2022.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.4.1">
      Advances in neural information processing systems
     </em>
     35 (2022), 24824–24837.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al
     <span class="ltx_text" id="bib.bib34.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023.
    </span>
    <span class="ltx_bibblock">
     Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">
      arXiv preprint arXiv:2308.08155
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al
     <span class="ltx_text" id="bib.bib35.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Lilith Yu, Chenfeng Gao, David Wu, and Ken Nakagaki. 2023.
    </span>
    <span class="ltx_bibblock">
     AeroRigUI: Actuated TUIs for Spatial Interaction using Rigging Swarm Robots on Ceilings in Everyday Space. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">
      Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems
     </em>
     . 1–18.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al
     <span class="ltx_text" id="bib.bib36.2.2.1">
      .
     </span>
     (2021)
    </span>
    <span class="ltx_bibblock">
     Jichen Zhu, Jennifer Villareale, Nithesh Javvaji, Sebastian Risi, Mathias Löwe, Rush Weigelt, and Casper Harteveld. 2021.
    </span>
    <span class="ltx_bibblock">
     Player-AI interaction: What neural network games reveal about AI as play. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">
      Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
     </em>
     . 1–17.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
  </ul>
 </section>
</article>
