<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.06777] Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data</title><meta property="og:description" content="One of the most pressing problems in the automated analysis of historical documents is the availability of annotated training data.
The problem is that labeling samples is a time-consuming task because it requires huma…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.06777">

<!--Generated on Tue Mar 19 13:03:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<section id="id4" class="ltx_glossary ltx_acronym ltx_list_acronym">
<dl id="id4.4" class="ltx_glossarylist">
<dt id="id1.1.id1" class="ltx_glossaryentry">OCR</dt>
<dd>Optical Character Recognition</dd>
<dt id="id2.2.id2" class="ltx_glossaryentry">GAN</dt>
<dd>Generative Adversarial Network</dd>
<dt id="id3.3.id3" class="ltx_glossaryentry">IoU</dt>
<dd>Intersection over Union</dd>
<dt id="id4.4.id4" class="ltx_glossaryentry">WPI</dt>
<dd>Wildenstein Plattner Institute</dd>
</dl>
</section>
<h1 class="ltx_title ltx_title_document">Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Christian Bartz<sup id="id5.1.id1" class="ltx_sup">*</sup> Hendrik Raetz<sup id="id6.2.id2" class="ltx_sup">*</sup>, Jona Otholt, Christoph Meinel, Haojin Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Hasso Plattner Institute, University of Potsdam 
<br class="ltx_break">Potsdam, Germany 
<br class="ltx_break">{firstname.lastname}@hpi.de
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">One of the most pressing problems in the automated analysis of historical documents is the availability of annotated training data.
The problem is that labeling samples is a time-consuming task because it requires human expertise and thus, cannot be automated well.
In this work, we propose a novel method to construct synthetic labeled datasets for historical documents where no annotations are available.
We train a StyleGAN model to synthesize document images that capture the core features of the original documents.
While originally, the StyleGAN architecture was not intended to produce labels, it indirectly learns the underlying semantics to generate realistic images.
Using our approach, we can extract the semantic information from the intermediate feature maps and use it to generate ground truth labels.
To investigate if our synthetic dataset can be used to segment the text in historical documents, we use it to train multiple supervised segmentation models and evaluate their performance.
We also train these models on another dataset created by a state-of-the-art synthesis approach to show that the models trained on our dataset achieve better results while requiring even less human annotation effort.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Equal contribution</span></span></span>
<section id="S1" class="ltx_section ltx_pruned_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">For the majority of history, humanity gathered its information in analog form and stored them in archives.
With the emergence of digital methods, more and more of these archives digitize their documents to preserve them for generations to come.
An additional benefit of this digitization is better indexing, which can help historians in their research.
However, the large amount of digitized documents (sometimes millions of scanned pages per archive) makes manual analysis impractical and calls for automation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With the introduction of deep learning into the area of document analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, it is becoming more and more feasible to analyze large quantities of documents of digitized archives effectively.
In this paper, we focus on semantic segmentation of documents and try to extract three classes: background, printed text, and handwritten text.
Extracting these classes from a document is useful for two reasons:
It may help archivists identify pages where potentially valuable, handwritten annotations are located.
Additionally, it serves as a helpful preprocessing step before applying character recognition algorithms.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Many existing state-of-the-art approaches train machine learning models with annotated real-world data (more on related work in <a href="#S2" title="II Related Work ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section II</span></a>).
However, annotating real-world data is a highly time-consuming and thus cost-intensive task.
Therefore, adapting these methods to a new unlabeled dataset becomes inefficient due to the high expenses for gathering annotations.
The goal of our work is to perform semantic segmentation on document images without the need to label large amounts of images manually.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Synthetic data can be a way to circumvent this requirement.
However, while synthetic image generation is a well-researched topic, generating the corresponding labels is a problem that is far from solved.
In some subdomains, such as scene text detection, synthetic data has already been in use for years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Still, the generated samples look artificial, and these approaches do not generalize to other applications.
DatasetGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, a more generalized approach relying on <a href="#id2.2.id2"><span href="#id2.2.id2" title="Generative Adversarial Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Generative Adversarial Networks</span></span></a>, creates realistic samples but requires a relatively high amount of manual labeling at pixel level.
To further reduce the manual labeling effort, we propose a method that only requires high-level annotations, reducing the annotation time by eight times.
In addition, we show that synthetic data generated with our approach is better suited for training semantic segmentation models.
On the compiled datasets, our method outperforms DatasetGAN on all three segmentation models that were evaluated by us.
Similarly to DatasetGAN, our approach is not limited to a specific domain but can be applied to every field where enough data is available to train a functioning StyleGAN network.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our proposed pipeline consists of several steps:
First, we directly use scanned documents to train a <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Generative Adversarial Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GAN</span></abbr></a> to synthesize artificial documents that look as realistic as possible.
Additionally, we use the knowledge of the trained model for generating a label image that contains the semantic class for each pixel of the synthesized image.
Specifically, we utilize the generative capabilities of a StyleGAN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and the observation that intermediate layers of a StyleGAN model might encode the semantic class of pixels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
By applying an unsupervised clustering algorithm to these intermediate layers, we can create clusters that represent the semantic classes of the corresponding pixels.
At this point, human intervention is required to classify these clusters.
However, using the interface provided by us, this task does not take more than <math id="S1.p6.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S1.p6.1.m1.1a"><mn id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><cn type="integer" id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">30</annotation></semantics></math> minutes per trained StyleGAN model.
Thus, we can use our system to synthesize a large, annotated dataset, which we use to train an additional semantic segmentation model.
A more detailed description of our proposed pipeline can be found in <a href="#S3" title="III Method ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section III</span></a>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In <a href="#S4" title="IV Experiments ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section IV</span></a>, we analyze the capabilities and weaknesses of our synthesis approach.
For this, we create an artificial dataset that we use for the training of multiple recently-proposed segmentation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In <a href="#S5" title="V Conclusion and Future Work ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section V</span></a>, we conclude the findings of this paper and provide a short intro into further extensions of our proposed idea.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">In summary, the contributions of this paper are as follows:

<span id="S1.I1" class="ltx_inline-enumerate">
<span id="S1.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span id="S1.I1.i1.1" class="ltx_text">We propose a novel approach for the semi-automatic synthesis of training data for the semantic analysis of documents based on the intermediate features of generative models and show that it can be used to create fine-grained semantic label images.
</span></span>
<span id="S1.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span id="S1.I1.i2.1" class="ltx_text">For evaluation purposes, we provide a labeled dataset of high-resolution historical documents that includes detailed annotations.
</span></span>
<span id="S1.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">3)</span> <span id="S1.I1.i3.1" class="ltx_text">In our experiments, we train multiple segmentation models entirely on artificial datasets and show that they can successfully segment real-world documents, outperforming previous work.
</span></span>
<span id="S1.I1.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">4)</span> <span id="S1.I1.i4.1" class="ltx_text">We provide our code and dataset to the community for further experimentation<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/hendraet/synthesis-in-style" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hendraet/synthesis-in-style</a></span></span></span>.
</span></span>
</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The field of semantic structure analysis of historical documents features a wide variety of research topics, e.g.<span id="S2.p1.1.1" class="ltx_text"></span>, page detection, page segmentation, layout analysis and line segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
However, these methods rely on annotated training data that is tailored to their specific use case.
Thus, it is not possible to easily apply them to datasets where no suitable annotations are available because acquiring labeled data is usually a very costly process that requires a lot of human work.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Looking at other fields, we can see that using synthetic data could be the key to solve the problem at hand.
Synthetic data has been successfully employed in the field of scene text detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and scene text recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, where artificial text was rendered on different natural backgrounds in plausible positions.
Recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> also shows that artificial data can be utilized in the area of historical document analysis.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The idea to extract semantic information from the intermediate feature maps of a StyleGAN network was also shown in a concurrent work by Zhang et al.<span id="S2.p3.1.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
This method is denoted <em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">DatasetGAN</em> and can be used to synthesize labels for natural images, such as faces, cars, and animals.
To simultaneously generate labels and artificial images, first, StyleGAN’s latent codes are recorded while creating a set of samples.
In a second step, they let a human annotator manually generate labels for these samples, which they use to train an ensemble of small classification networks to predict label images based on latent codes.
Subsequently, when using StyleGAN to generate new samples, the trained ensemble automatically creates labels based on the latent codes.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">A similar approach was later introduced by Li et al.<span id="S2.p4.1.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and successfully used for the segmentation of medical images (CT/MRT scans) and images of human faces.
They add another branch to a StyleGAN2 network that is trained to output the label image together with a synthesized image.
Additionally, an encoder network is trained, which embeds a target image as noise that can be used by the StyleGAN network to reproduce the original image.
Thus, the label image for the encoded image is produced as well.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Both approaches significantly reduce the time needed to create labeled datasets.
However, human intervention is still required to annotate images.
Depending on the complexity of the problem, this might still take several hours and might be challenging for inexperienced annotators.
Additionally, the examined natural images have different properties than scanned documents.
Images of text feature specific semantics because they contain important details, such as fine strokes or punctuation.
Thus, it is worth investigating if StyleGAN can learn these characteristics so that it can produce artificial datasets for historical document analysis.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In contrast to recent document analysis methods, we focus on training a segmentation model entirely on synthetic data.
This data is generated by a generative model that is trained to closely replicate the real distribution.
Using synthetic data allows us to use well-established supervised learning methods for the semantic segmentation of document images.
The small domain gap between the synthetic and real data allows us to apply our trained models directly to our target data distribution.
In summary, our method leverages synthetic data to obtain a segmentation model that is tailored to the real data, even though no matching annotations are available for it.
In this section, we introduce our StyleGAN-based data synthesis pipeline.
Furthermore, we introduce the semantic segmentation networks that we use in our experiments.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Data Synthesis Pipeline</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our proposed data synthesis pipeline consists of three basic steps.
In the first step, we train a StyleGAN model on the samples of the database we want to analyze.
We chose to use StyleGAN because it reaches state-of-the-art results in unconditional image generation and exhibits many interesting properties that allow us to control the image generation, as shown in several prior works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In our second step, we make use of the observation that intermediate layers of StyleGAN can encode semantic information about the class of each pixel in the resulting image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Based on this idea, we create an algorithm that uses the output of these intermediate layers for the synthesis of label images.
In the last step, we use our trained StyleGAN model and our analysis algorithm to synthesize a large-scale, fully annotated dataset for document segmentation that we then use for the training of an off-the-shelf semantic segmentation network.
In the following, we explain our pipeline using the example of segmenting and classifying printed and handwritten text in document images.
In <a href="#S3.F1" title="Figure 1 ‣ III-A Data Synthesis Pipeline ‣ III Method ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, we show how data synthesis is utilized in our approach.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2107.06777/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Depiction of our proposed pipeline.
Our pipeline consists of the following steps:
0. We gather scans of documents.
1. We train a StyleGAN model to generate document patches that look as similar, as possible, to real patches extracted from our document corpus.
2. We use an unsupervised clustering algorithm on the intermediate outputs of the synthesis network of our trained StyleGAN model and annotate the found clusters manually
3. We use the StyleGAN model from step 2 and the classified clusters to synthesize a training dataset.
4. We use the synthesized training data to train an off-the-shelf segmentation network on patches of documents.
5. We apply the trained segmentation network on the real document images
</figcaption>
</figure>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Training of StyleGAN</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.4" class="ltx_p">First, we train a <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Generative Adversarial Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GAN</span></abbr></a> based on the StyleGAN architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> using the original document images.
The StyleGAN architecture proposed by Karras et al.<span id="S3.SS1.SSS1.p1.4.1" class="ltx_text"></span> consists of three main components.
The core component of the model is the synthesis network, which uses progressive growing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, enabling it to synthesize high-resolution images of high quality.
It receives style guidance from the mapping network that maps a latent vector <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="z\in\mathcal{Z}" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">z</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml">𝒵</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><in id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1"></in><ci id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">𝑧</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">𝒵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">z\in\mathcal{Z}</annotation></semantics></math> with <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{Z}\in\mathbb{R}^{n}" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mrow id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">𝒵</mi><mo id="S3.SS1.SSS1.p1.2.m2.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><in id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1"></in><ci id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">𝒵</ci><apply id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2">ℝ</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">\mathcal{Z}\in\mathbb{R}^{n}</annotation></semantics></math> into an intermediate latent space <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="r\in\mathcal{R}" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mrow id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml">r</mi><mo id="S3.SS1.SSS1.p1.3.m3.1.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p1.3.m3.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml">ℛ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><apply id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1"><in id="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.1"></in><ci id="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2">𝑟</ci><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3">ℛ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">r\in\mathcal{R}</annotation></semantics></math> with <math id="S3.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{R}\in\mathbb{R}^{n}" display="inline"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><mrow id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p1.4.m4.1.1.2" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml">ℛ</mi><mo id="S3.SS1.SSS1.p1.4.m4.1.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p1.4.m4.1.1.3" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.4.m4.1.1.3.2" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.SSS1.p1.4.m4.1.1.3.3" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><apply id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1"><in id="S3.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.1"></in><ci id="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.2">ℛ</ci><apply id="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.2">ℝ</ci><ci id="S3.SS1.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">\mathcal{R}\in\mathbb{R}^{n}</annotation></semantics></math>.
These vectors are then fed into the synthesis network, allowing it to generate diverse images.
The last component is another input to the synthesis network called <em id="S3.SS1.SSS1.p1.4.2" class="ltx_emph ltx_font_italic">stochastic noise</em>, which helps to generate stochastic details.
In the case of face generation, these details can be freckles or hair, whereas, in the case of document generation, this noise can, e.g.<span id="S3.SS1.SSS1.p1.4.3" class="ltx_text"></span>, influence the design of characters.
We use StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, an improved version of StyleGAN that addresses several weaknesses of the original model.
For further information about StyleGAN and StyleGAN2, please refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">We do not use entire document images as input to StyleGAN.
Instead, we divide each document image into multiple patches and train our StyleGAN model to synthesize patches of images.
We choose to synthesize patches because it is simpler to create a realistic-looking patch than to generate a full document that resembles real ones.
Using patches allows us to analyze documents at a high-resolution without compromising the granularity of the results.
Additionally, this approach allows us to work on documents of varying sizes without having to account for different aspect ratios.
Furthermore, documents combine various aspects, e.g., areas with printed or handwritten text, images, text decorations, and scanning margins.
Patches allow the generator to concentrate on specific ones because it does not have to include many variations or combinations of these aspects simultaneously.
However, using patches adds extra computational work during inference and might also cause inconsistencies when assembling the patches after performing semantic segmentation with a segmentation network.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Analysis of the Trained StyleGAN Model</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Once we have trained the StyleGAN model, we use it to synthesize annotated training data for training a semantic segmentation network.
StyleGAN was designed to synthesize realistic RGB images, i.e.<span id="S3.SS1.SSS2.p1.1.1" class="ltx_text"></span>, similar to the provided input data.
Thus, at first glance, StyleGAN seems unsuitable to synthesize realistic-looking samples and the corresponding label images. However, we can deduce the label information from the feature maps within the internal layers of StyleGAN.
During the generation of a sample, StyleGAN encodes the semantic class of pixels in the intermediate layers of the synthesis network.
This behavior was first described by Collins et al.<span id="S3.SS1.SSS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, where they used this property to perform semantically meaningful local edits of faces.
They found that if an unsupervised clustering algorithm, such as spherical k-Means clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, is applied to the activations of each StyleGAN block in the synthesis network, semantic classes of each pixel can be determined.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2107.06777/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="408" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Visualization of the clustered intermediate layers and their corresponding labels for two synthesized samples (best viewed digitally and in color).
The first four columns show the output of spherical k-means (20 clusters) on the intermediate layers of the synthesis network.
The top row for each sample shows the original clusters, and the labeled counterparts are displayed directly below.
The rightmost images depict the final image (top) and the matching label, which was created by combining the intermediate labels (bottom).
In the label images, the blue color represents handwritten text, the orange color corresponds to printed text, and black stands for background.
Since the last intermediate layer only contains structural information, the orange color only indicates the location of text and not the textual class.
In practice, we have two layers for each size and layers that are smaller than <math id="S3.F2.2.m1.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S3.F2.2.m1.1b"><mrow id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml"><mn id="S3.F2.2.m1.1.1.2" xref="S3.F2.2.m1.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F2.2.m1.1.1.1" xref="S3.F2.2.m1.1.1.1.cmml">×</mo><mn id="S3.F2.2.m1.1.1.3" xref="S3.F2.2.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><apply id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1"><times id="S3.F2.2.m1.1.1.1.cmml" xref="S3.F2.2.m1.1.1.1"></times><cn type="integer" id="S3.F2.2.m1.1.1.2.cmml" xref="S3.F2.2.m1.1.1.2">32</cn><cn type="integer" id="S3.F2.2.m1.1.1.3.cmml" xref="S3.F2.2.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">32\times 32</annotation></semantics></math>.
(These were omitted due to space limitations.)
</figcaption>
</figure>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">We follow the approach of Collins et al.<span id="S3.SS1.SSS2.p2.1.1" class="ltx_text"></span> and examine the clusters of a StyleGAN model trained on patches of document images that contain handwritten and printed text (Step 2 in <a href="#S3.F1" title="Figure 1 ‣ III-A Data Synthesis Pipeline ‣ III Method ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>).
We provide some samples and the results of clustering with spherical k-Means in <a href="#S3.F2" title="Figure 2 ‣ III-A2 Analysis of the Trained StyleGAN Model ‣ III-A Data Synthesis Pipeline ‣ III Method ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.
The provided samples clearly show that certain intermediate layers of StyleGAN’s synthesis network encode the semantic class of printed or handwritten text very well.
However, these semantic clusters cannot be used directly for the synthesis of label images, because multiple clusters belong to a single class.
Also, if we were to use only one of these intermediate images for deciding the semantic class of each pixel, we would encounter several problems:

<span id="S3.I1" class="ltx_inline-enumerate">
<span id="S3.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(1)</span> <span id="S3.I1.i1.1" class="ltx_text">If we were to use an output of an early layer in the synthesis network, the resolution of our identified text regions in the resulting label image could be very low.
</span></span>
<span id="S3.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(2)</span> <span id="S3.I1.i2.1" class="ltx_text">The classes found within one layer of the synthesis network are not always accurate, thus, we have to rely on the output of multiple layers of the synthesis network.
</span></span>
</span></p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.3" class="ltx_p">To remedy these problems, we require a human annotator to examine the clusters found by spherical k-Means.
First, the last layers of the network (size <math id="S3.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S3.SS1.SSS2.p3.1.m1.1a"><mrow id="S3.SS1.SSS2.p3.1.m1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.cmml"><mn id="S3.SS1.SSS2.p3.1.m1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p3.1.m1.1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS2.p3.1.m1.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.1b"><apply id="S3.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1"><times id="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.2">256</cn><cn type="integer" id="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.1c">256\times 256</annotation></semantics></math>) have to be analyzed by the annotator.
In this step, the main task is to separate the text clusters from background clusters to extract detailed structural information about the synthesized text.
However, at this stage of the network, the model only focuses on the texture and not on the shape <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, thus distinction between handwritten and printed text is not possible.
Therefore, the second task of the annotator is to identify the feature maps that contain semantic information (usually layers of size <math id="S3.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><mrow id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml"><mn id="S3.SS1.SSS2.p3.2.m2.1.1.2" xref="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p3.2.m2.1.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS2.p3.2.m2.1.1.3" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><apply id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1"><times id="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.2">64</cn><cn type="integer" id="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">64\times 64</annotation></semantics></math> or <math id="S3.SS1.SSS2.p3.3.m3.1" class="ltx_Math" alttext="128\times 128" display="inline"><semantics id="S3.SS1.SSS2.p3.3.m3.1a"><mrow id="S3.SS1.SSS2.p3.3.m3.1.1" xref="S3.SS1.SSS2.p3.3.m3.1.1.cmml"><mn id="S3.SS1.SSS2.p3.3.m3.1.1.2" xref="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p3.3.m3.1.1.1" xref="S3.SS1.SSS2.p3.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS2.p3.3.m3.1.1.3" xref="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.3.m3.1b"><apply id="S3.SS1.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1"><times id="S3.SS1.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.2">128</cn><cn type="integer" id="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.3.m3.1c">128\times 128</annotation></semantics></math>) and determine the corresponding class for each of the detected clusters.
In very low-resolution feature maps, the clusters do not correspond to text regions, so they are not useful for creating label images.
<a href="#S3.F2" title="Figure 2 ‣ III-A2 Analysis of the Trained StyleGAN Model ‣ III-A Data Synthesis Pipeline ‣ III Method ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> shows how the final annotated intermediate feature maps might look like.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.1" class="ltx_p">We provide an algorithm that uses the annotations to combine the semantic information of the lower layers with the structural information of the last layers to create a label image of sufficient detail.
It might be necessary to adapt this algorithm for usage in different scenarios depending on the corresponding StyleGAN output.</p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p">We found that examining a maximum of <math id="S3.SS1.SSS2.p5.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS1.SSS2.p5.1.m1.1a"><mn id="S3.SS1.SSS2.p5.1.m1.1.1" xref="S3.SS1.SSS2.p5.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p5.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.p5.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p5.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p5.1.m1.1c">100</annotation></semantics></math> images suffices to accurately determine the classes of clusters.
To speed up the labeling process and keep the annotation time minimal, we provide a simple labeling tool in the code repository belonging to this publication.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.5.1.1" class="ltx_text">III-A</span>3 </span>Synthesis of a Large Scale Dataset</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Once the clusters are labeled, we can run the StyleGAN model in combination with our algorithm to create a large-scale training dataset.
Here, we draw vectors from a uniform distribution and pass them to our StyleGAN model to produce synthetic document patches and label images.
The resulting dataset might be imbalanced because it could contain more images only depicting background information than images displaying text.
This happens because it is highly probable that most patches used for the training of our StyleGAN model do not contain any text.
However, we can later balance the dataset by using the synthesized annotations.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Semantic Segmentation Network</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The second to last step of our pipeline is the training of semantic segmentation networks using our synthetic training data.
In our experiments, we decided to use three different segmentation networks.
The first is Doc-UFCN, a semantic segmentation network that reaches state-of-the-art results in the line segmentation task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
The network by Boillet et al.<span id="S3.SS2.p1.1.1" class="ltx_text"></span> is based on a U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> architecture, uses custom dilated convolution blocks and has less trainable parameters than other state-of-the-art networks.
EMANet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the second network, was originally designed for the semantic segmentation of natural image datasets, such as PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
The backbone of this model is a ResNet-101 that incorporates the eponymous <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic">EMAUnits</em>, which introduce a special attention mechanism.
As third network, we choose TransUNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which was developed for the segmentation of medical images.
As the name implies, it employs the U-Net architecture while using a Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as an encoder.
We chose these networks because they reach state-of-the-art results in their respective field and exhibit different strengths that we want to evaluate.
Doc-UFCN is specifically designed for segmentation tasks on historical documents, EMANet reaches state-of-the-art performance on widely researched benchmark datasets, and TransUNet can produce detailed segmentations required in the field of medical segmentation.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To enhance the performance of the segmentation networks, we assert that input images are grayscale before feeding them into the training pipeline.
We enhance the diversity of the training data by randomly augmenting the input images.
These augmentation operations include: cropping, shearing, shifting, slight distortion, rotation, contrast change and color inversion.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In principle, we can apply our proposed pipeline to any unlabeled dataset and create semantic segmentation models that are custom-fit to the dataset.
We show in our experiments that this applies to real-world images of auction catalogs that we extracted from an art-historical dataset.
In this section, we first introduce the dataset we use to evaluate our pipeline.
Afterwards, we provide a detailed description of our experimental setup.
We finish this section by presenting our results and discussing the possibilities and limitations of our proposed method.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Benchmark Dataset</span>
</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2107.06777/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Image cutouts of a sample from the in-domain split of the evaluation dataset, the corresponding ground truth and our prediction made with TransUNet on the right. More examples can be found in the supplementary material.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To the best of our knowledge, there are no datasets that include fine-grained, pixel-level annotations of historical documents.
Existing datasets were built for less detailed segmentation tasks, such as line segmentation or layout analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
However, we want to evaluate if the trained segmentation models can generate even more precise segmentations.
Thus, we manually labeled 32 high-resolution images of scanned document pages obtained from the archive of the  <a href="#id4.4.id4"><span href="#id4.4.id4" title="Wildenstein Plattner Institute" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Wildenstein Plattner Institute</span></span></a> (<a href="#id4.4.id4"><abbr href="#id4.4.id4" title="Wildenstein Plattner Institute" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WPI</span></abbr></a>).
These documents are pages of auction catalogs that contain mainly printed text but also handwritten letters and annotations that are especially interesting for researchers.
In addition to the printed and handwritten text, which comes in various fonts, sizes, and orientations, the catalogs also contain images of paintings.
<a href="#S4.F3" title="Figure 3 ‣ IV-A Benchmark Dataset ‣ IV Experiments ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> shows an example image and the corresponding ground truth.
For better evaluation, we decided to split the dataset in two parts: in-domain and out-of-domain.
The images that are considered in-domain are representative for the majority of the samples in the dataset we used to train our generative models.
The out-of-domain images feature more diverse text and page layouts that may not be as well reflected by the learned distribution of the generative models.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We use our pipeline to synthesize <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="100\,000" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">100 000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">100000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">100\,000</annotation></semantics></math> training images with the corresponding ground truth.
Afterwards, this dataset is balanced so that it contains an equal amount of images containing handwriting and images containing printed text.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We perform post-processing on the segmented image patches to remove noise and improve the confidence of the predictions.
For this, we use two hyperparameters that are used by Doc-UFCN: a threshold for keeping a text prediction (<em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">minimum confidence</em>) and a threshold for discarding small connected components of text (<em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">minimum contour area</em>).
Although these hyperparameters are also used during the training of Doc-UFCN, we found that we can improve our results by applying them to all segmentation models during evaluation.
An additional hyperparameter is called <em id="S4.SS2.p2.1.3" class="ltx_emph ltx_font_italic">patch overlap factor</em>, which steers the fragmentation of the original images.
If the overlap factor is larger than <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="0.0" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">0.0</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="float" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">0.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">0.0</annotation></semantics></math>, pixels will be classified multiple times because they are contained in different patches.
During the reassembling process, the most confident of these predictions determines the class of a pixel.
We find the optimal hyperparameters with an exhaustive grid search.
The resulting parameters we used for our experiments can be found in the appendix.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Results and Discussion</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In our experiments, we want to evaluate the following:

<span id="S4.I1" class="ltx_inline-enumerate">
<span id="S4.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(1)</span> <span id="S4.I1.i1.1" class="ltx_text">which segmentation network is best suited for the segmentation of our benchmark dataset, and
</span></span>
<span id="S4.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(2)</span> <span id="S4.I1.i2.1" class="ltx_text">the performance of our segmentation method compared to DatasetGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
</span></span>
</span></p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Performance of Segmentation Networks</h4>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Performance of the segmentation models on the in-domain split of the benchmark dataset.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S4.T1.1.1.1.1" class="ltx_text"><span id="S4.T1.1.1.1.1.1" class="ltx_text"></span> <span id="S4.T1.1.1.1.1.2" class="ltx_text">
<span id="S4.T1.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Segmentation</span></span>
<span id="S4.T1.1.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T1.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Model</span></span>
</span></span> <span id="S4.T1.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">Printed Text</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">Handwritten Text</td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">mIoU</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="S4.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
<td id="S4.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="S4.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="S4.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.66</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.70</td>
<td id="S4.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.83</td>
<td id="S4.T1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.82</td>
<td id="S4.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.28</td>
<td id="S4.T1.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.32</td>
<td id="S4.T1.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.3.8.1" class="ltx_text ltx_font_bold">0.70</span></td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.4.2.1" class="ltx_text ltx_font_bold">0.72</span></td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.4.3.1" class="ltx_text ltx_font_bold">0.71</span></td>
<td id="S4.T1.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.4.4.1" class="ltx_text ltx_font_bold">0.84</span></td>
<td id="S4.T1.1.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.82</td>
<td id="S4.T1.1.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.4.6.1" class="ltx_text ltx_font_bold">0.46</span></td>
<td id="S4.T1.1.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.67</td>
<td id="S4.T1.1.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.60</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.66</td>
<td id="S4.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.52</td>
<td id="S4.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.56</td>
<td id="S4.T1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.5.5.1" class="ltx_text ltx_font_bold">0.87</span></td>
<td id="S4.T1.1.5.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.46</td>
<td id="S4.T1.1.5.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.5.7.1" class="ltx_text ltx_font_bold">0.71</span></td>
<td id="S4.T1.1.5.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.57</td>
</tr>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Performance of the segmentation models on the out-of-domain split of the benchmark dataset.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S4.T2.1.1.1.1" class="ltx_text"><span id="S4.T2.1.1.1.1.1" class="ltx_text"></span> <span id="S4.T2.1.1.1.1.2" class="ltx_text">
<span id="S4.T2.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Segmentation</span></span>
<span id="S4.T2.1.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Model</span></span>
</span></span> <span id="S4.T2.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">Printed Text</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">Handwritten Text</td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">mIoU</td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="S4.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
<td id="S4.T2.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="S4.T2.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="S4.T2.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.49</td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.37</td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.3.4.1" class="ltx_text ltx_font_bold">0.87</span></td>
<td id="S4.T2.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.40</td>
<td id="S4.T2.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.11</td>
<td id="S4.T2.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.15</td>
<td id="S4.T2.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.25</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.54</td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.47</td>
<td id="S4.T2.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.4.4.1" class="ltx_text ltx_font_bold">0.87</span></td>
<td id="S4.T2.1.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.50</td>
<td id="S4.T2.1.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.4.6.1" class="ltx_text ltx_font_bold">0.17</span></td>
<td id="S4.T2.1.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.4.7.1" class="ltx_text ltx_font_bold">0.38</span></td>
<td id="S4.T2.1.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.24</td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.2.1" class="ltx_text ltx_font_bold">0.56</span></td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.3.1" class="ltx_text ltx_font_bold">0.52</span></td>
<td id="S4.T2.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.67</td>
<td id="S4.T2.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.5.1" class="ltx_text ltx_font_bold">0.70</span></td>
<td id="S4.T2.1.5.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.6.1" class="ltx_text ltx_font_bold">0.17</span></td>
<td id="S4.T2.1.5.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.27</td>
<td id="S4.T2.1.5.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.8.1" class="ltx_text ltx_font_bold">0.30</span></td>
</tr>
</table>
</figure>
<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p"><a href="#S4.T1" title="TABLE I ‣ IV-C1 Performance of Segmentation Networks ‣ IV-C Results and Discussion ‣ IV Experiments ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table I</span></a> shows the performance of the segmentation models on the in-domain split of our benchmark dataset.
Besides the widely used mean <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Intersection over Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IoU</span></abbr></a> (mIoU), we report the class-wise  <a href="#id3.3.id3"><span href="#id3.3.id3" title="Intersection over Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Intersection over Union</span></span></a> (<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Intersection over Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IoU</span></abbr></a>), precision, and recall for handwritten and printed text<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
We have decided to leave out the class scores for background because it does not provide any added value for the evaluation.
However, detailed results for all experiments can be found in the supplementary material.
</span></span></span>.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">Overall, the results are satisfying.
TransUNet shows the best performance while the other models do not trail far behind.
However, EMANet and Doc-UFCN achieve higher recall values for printed and handwritten text, respectively.
Taking a closer look at class-wise metrics reveals that it is much easier for the models to correctly identify printed text compared to handwritten text.
A reason for this significant difference might be the underlying dataset used for the training of StyleGAN.
The original images contain a lot of printed text and only occasionally feature handwritten annotations.
Thus, StyleGAN will produce more samples depicting printed text.
Balancing the synthetic training dataset so that there are as many samples containing handwritten text as samples showing printed text mitigates this issue to a certain degree.
Overall, the dataset still contains significantly more pixels belonging to the printed text class.
Thus, during training, the segmentation models learn to detect printed text more reliably.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">The results on the out-of-domain split are shown in <a href="#S4.T2" title="TABLE II ‣ IV-C1 Performance of Segmentation Networks ‣ IV-C Results and Discussion ‣ IV Experiments ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table II</span></a>.
As expected, the <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Intersection over Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IoU</span></abbr></a> is lower across the board, with handwritten text showing a more pronounced drop than printed text.
This is mostly due to a lower precision of the handwritten text classifications compared to the in-domain data, whereas for printed text the precision is almost unchanged.
A possible explanation is that the out-of-domain samples contain more objects that are easily misclassified as handwritten text, for example drawings, paintings, or unusual fonts.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Comparison to DatasetGAN</h4>

<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison between our data synthesis approach and DatasetGAN for different segmentation models.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S4.T3.1.1.1.1" class="ltx_text"><span id="S4.T3.1.1.1.1.1" class="ltx_text"></span> <span id="S4.T3.1.1.1.1.2" class="ltx_text">
<span id="S4.T3.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Segmentation</span></span>
<span id="S4.T3.1.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Model</span></span>
</span></span> <span id="S4.T3.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S4.T3.1.1.2.1" class="ltx_text"><span id="S4.T3.1.1.2.1.1" class="ltx_text"></span> <span id="S4.T3.1.1.2.1.2" class="ltx_text">
<span id="S4.T3.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Synthesis</span></span>
<span id="S4.T3.1.1.2.1.2.1.2" class="ltx_tr">
<span id="S4.T3.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Method</span></span>
</span></span> <span id="S4.T3.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2">mIoU</td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">In-Domain</td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Out-of-Domain</td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">DatsetGAN</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.65</td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.45</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.66</td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.49</td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DatsetGAN</td>
<td id="S4.T3.1.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.61</td>
<td id="S4.T3.1.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.48</td>
</tr>
<tr id="S4.T3.1.6" class="ltx_tr">
<td id="S4.T3.1.6.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T3.1.6.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</td>
<td id="S4.T3.1.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.6.3.1" class="ltx_text ltx_font_bold">0.72</span></td>
<td id="S4.T3.1.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.54</td>
</tr>
<tr id="S4.T3.1.7" class="ltx_tr">
<td id="S4.T3.1.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="S4.T3.1.7.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DatsetGAN</td>
<td id="S4.T3.1.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.52</td>
<td id="S4.T3.1.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.43</td>
</tr>
<tr id="S4.T3.1.8" class="ltx_tr">
<td id="S4.T3.1.8.1" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T3.1.8.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</td>
<td id="S4.T3.1.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.66</td>
<td id="S4.T3.1.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.8.4.1" class="ltx_text ltx_font_bold">0.56</span></td>
</tr>
</table>
</figure>
<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In <a href="#S4.T3" title="TABLE III ‣ IV-C2 Comparison to DatasetGAN ‣ IV-C Results and Discussion ‣ IV Experiments ‣ Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table III</span></a>, we evaluate the performance of our dataset synthesis approach.
For this, we trained the segmentation models on a dataset synthesized using the DatasetGAN method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
When comparing the mIoUs, it can be seen that the models trained using our method achieve better results on both the in-domain and the out-of-domain samples.
We believe that this is the case because DatasetGAN was developed for a completely different domain.
Natural images often depict larger coherent structures, whereas the text in document images is a set of detailed characters where minor variations can change the semantic class.
As a result, the synthesized data produced by StyleGAN is noisier and contains more ambiguities, such as partially drawn letters that make it harder to establish a consistent segmentation.
Our approach, which labels more samples but with a straightforward, cluster-based labeling procedure, can achieve a more consistent segmentation with less labeling effort.
To put labeling time in perspective: annotating an adequate dataset required by DatasetGAN (10 images) took us around four hours, whereas labeling using our method only takes about 30 minutes.
It is possible that using more labeled samples would further improve the performance of DatasetGAN, but this improvement would probably be disproportionate to the required labeling effort.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we have proposed a novel approach for synthesizing large-scale training datasets, which is suited (but not limited) to the analysis of historical document images.
Our approach works directly on scans of documents without the need to annotate large amounts of individual images.
However, we still require human intervention for the labeling of clusters, for which we provide an easy-to-use interface to keep the annotation time minimal.
This enables us to train fully-supervised machine learning models on datasets that do not have any annotations available.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In our experiments, we have shown that state-of-the-art segmentation models that have been trained on our synthesized datasets can produce satisfactory segmentations.
In addition, our proposed method outperforms the similar DatasetGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> due to its easy and consistent labeling process, while requiring less labeling effort.
These results prove that our approach can help to make machine learning more accessible by avoiding the high labeling cost associated with traditional supervised methods.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In the future, we wish to improve our pipeline and use it to analyze different datasets where no annotations are available.
To improve our pipeline, we are especially interested in removing the need for human intervention, reducing the amount of computational resources necessary for data synthesis, and improving the generalizability of models created with our proposed system.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. A. Oliveira, B. Seguin, and F. Kaplan, “dhSegment: A generic
deep-learning approach for document segmentation,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">16th
International Conference on Frontiers in Handwriting Recognition</em>.   IEEE Computer Society, 2018, pp. 7–12.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Boillet, C. Kermorvant, and T. Paquet, “Multiple document datasets
pre-training improves text line detection with deep neural networks,” in
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">25th International Conference on Pattern Recognition</em>.   IEEE, 2020, pp. 2134–2141.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
C. Bartz, L. Seidel, D. Nguyen, J. Bethge, H. Yang, and C. Meinel, “Synthetic
data for the analysis of archival documents: Handwriting determination,” in
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Digital Image Computing: Techniques and Applications</em>.   IEEE, 2020, pp. 1–8.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. Moysset, C. Kermorvant, and C. Wolf, “Full-page text recognition: Learning
where to start and when to stop,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">14th IAPR International
Conference on Document Analysis and Recognition</em>.   IEEE, 2017, pp. 871–876.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Gupta, A. Vedaldi, and A. Zisserman, “Synthetic data for text localisation
in natural images,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and
Pattern Recognition</em>.   IEEE Computer
Society, 2016, pp. 2315–2324.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Long and C. Yao, “UnrealText: Synthesizing realistic scene text images
from the unreal world,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2003.10608, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K. Wang, B. Babenko, and S. J. Belongie, “End-to-end scene text recognition,”
in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision,</em>, D. N. Metaxas,
L. Quan, A. Sanfeliu, and L. V. Gool, Eds.   IEEE Computer Society, 2011, pp. 1457–1464.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Zhang, H. Ling, J. Gao, K. Yin, J. Lafleche, A. Barriuso, A. Torralba, and
S. Fidler, “DatasetGAN: Efficient labeled data factory with minimal human
effort,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition</em>.   Computer Vision
Foundation / IEEE, 2021, pp. 10 145–10 155.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for
generative adversarial networks,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer
Vision and Pattern Recognition</em>.   Computer Vision Foundation / IEEE, 2019, pp. 4401–4410.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,
“Analyzing and improving the image quality of StyleGAN,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.   Computer Vision Foundation / IEEE, 2020, pp.
8107–8116.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
E. Collins, R. Bala, B. Price, and S. Süsstrunk, “Editing in style:
Uncovering the local semantics of GANs,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.   Computer Vision Foundation / IEEE, 2020, pp. 5770–5779.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, “Expectation-maximization
attention networks for semantic segmentation,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF
International Conference on Computer Vision</em>.   IEEE, 2019, pp. 9166–9175.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and
Y. Zhou, “TransUNet: Transformers make strong encoders for medical image
segmentation,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2102.04306, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Stewart and B. Barrett, “Document image page segmentation and character
recognition as semantic segmentation,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th
International Workshop on Historical Document Imaging and Processing</em>.   ACM, 2017, pp. 101–106.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, “Reading text in the
wild with convolutional neural networks,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vis.</em>, vol.
116, no. 1, pp. 1–20, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. Li, J. Yang, K. Kreis, A. Torralba, and S. Fidler, “Semantic segmentation
with generative models: Semi-supervised learning and strong out-of-domain
generalization,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition</em>.   Computer Vision
Foundation / IEEE, 2021, pp. 8300–8311.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
R. Abdal, Y. Qin, and P. Wonka, “Image2StyleGAN: How to embed images into
the StyleGAN latent space?” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF International
Conference on Computer Vision</em>.   IEEE, 2019, pp. 4431–4440.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
——, “Image2StyleGAN++: How to edit the embedded images?” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.   Computer Vision Foundation / IEEE, 2020, pp.
8293–8302.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Pidhorskyi, D. A. Adjeroh, and G. Doretto, “Adversarial latent
autoencoders,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition</em>.   Computer Vision
Foundation / IEEE, 2020, pp. 14 092–14 101.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
W. Nie, T. Karras, A. Garg, S. Debnath, A. Patney, A. B. Patel, and
A. Anandkumar, “Semi-supervised StyleGAN for disentanglement learning,”
in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine
Learning</em>, ser. Proceedings of Machine Learning Research, vol. 119.   PMLR, 2020, pp. 7360–7369.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
C. Bartz, J. Bethge, H. Yang, and C. Meinel, “One model to reconstruct them
all: A novel way to use the stochastic noise in StyleGAN,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2010.11113, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of GANs
for improved quality, stability, and variation,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">6th International
Conference on Learning Representations</em>.   OpenReview.net, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. Hornik, I. Feinerer, M. Kober, and C. Buchta, “Spherical k-means
clustering,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Journal of Statistical Software</em>, vol. 50, no. 10, p.
1–22, 2012.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and
W. Brendel, “ImageNet-trained CNNs are biased towards texture;
increasing shape bias improves accuracy and robustness,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">7th
International Conference on Learning Representations</em>.   OpenReview.net, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks for
biomedical image segmentation,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and
Computer-Assisted Intervention - 18th International Conference, Proceedings,
Part III</em>, ser. Lecture Notes in Computer Science, N. Navab, J. Hornegger,
W. M. W. III, and A. F. Frangi, Eds., vol. 9351.   Springer, 2015, pp. 234–241.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Everingham, L. V. Gool, C. K. I. Williams, J. M. Winn, and A. Zisserman,
“The Pascal visual object classes (VOC) challenge,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Int. J.
Comput. Vis.</em>, vol. 88, no. 2, pp. 303–338, 2010.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017</em>, I. Guyon, U. von Luxburg,
S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett,
Eds., 2017, pp. 5998–6008.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
T. Gruning, R. Labahn, M. Diem, F. Kleber, and S. Fiel, “READ-BAD: A new
dataset and evaluation scheme for baseline detection in archival documents,”
in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">13th IAPR International Workshop on Document Analysis
Systems</em>.   IEEE Computer Society,
2018, pp. 351–356.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Boillet, M. Bonhomme, D. Stutzmann, and C. Kermorvant, “HORAE: an
annotated dataset of books of hours,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th
International Workshop on Historical Document Imaging and Processing</em>.   ACM, 2019, pp. 7–12.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Baloun, P. Král, and L. Lenc, “ChronSeg: Novel dataset for
segmentation of handwritten historical chronicles,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 13th International Conference on Agents and Artificial Intelligence,
Volume 2</em>, A. P. Rocha, L. Steels, and H. J. van den Herik, Eds.   SCITEPRESS, 2021, pp. 314–322.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
I. Loshchilov and F. Hutter, “SGDR: stochastic gradient descent with warm
restarts,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">5th International Conference on Learning
Representations</em>.   OpenReview.net,
2017.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A0.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">More Information on Experimental Setup</h3>

<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="A0.SSx1.p1" class="ltx_para ltx_noindent">
<p id="A0.SSx1.p1.4" class="ltx_p">Additional information on the experimental setup:
We performed our experiments on systems with a GPU that has at least <math id="A0.SSx1.p1.1.m1.3" class="ltx_Math" alttext="11\text{\,}\mathrm{GB}" display="inline"><semantics id="A0.SSx1.p1.1.m1.3a"><mrow id="A0.SSx1.p1.1.m1.3.3" xref="A0.SSx1.p1.1.m1.3.3.cmml"><mn id="A0.SSx1.p1.1.m1.1.1.1.1.1.1" xref="A0.SSx1.p1.1.m1.1.1.1.1.1.1.cmml">11</mn><mtext id="A0.SSx1.p1.1.m1.2.2.2.2.2.2" xref="A0.SSx1.p1.1.m1.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="A0.SSx1.p1.1.m1.3.3.3.3.3.3" xref="A0.SSx1.p1.1.m1.3.3.3.3.3.3.cmml">GB</mi></mrow><annotation-xml encoding="MathML-Content" id="A0.SSx1.p1.1.m1.3b"><apply id="A0.SSx1.p1.1.m1.3.3.cmml" xref="A0.SSx1.p1.1.m1.3.3"><csymbol cd="latexml" id="A0.SSx1.p1.1.m1.2.2.2.2.2.2.cmml" xref="A0.SSx1.p1.1.m1.2.2.2.2.2.2">times</csymbol><cn type="integer" id="A0.SSx1.p1.1.m1.1.1.1.1.1.1.cmml" xref="A0.SSx1.p1.1.m1.1.1.1.1.1.1">11</cn><csymbol cd="latexml" id="A0.SSx1.p1.1.m1.3.3.3.3.3.3.cmml" xref="A0.SSx1.p1.1.m1.3.3.3.3.3.3">gigabyte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p1.1.m1.3c">11\text{\,}\mathrm{GB}</annotation></semantics></math> of RAM (e.g., using a Geforce GTX 1080Ti).
For the training of our StyleGAN model, we follow the hyperparameters set by Karras et al.<span id="A0.SSx1.p1.4.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, but we set the initial learning rate to <math id="A0.SSx1.p1.2.m2.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="A0.SSx1.p1.2.m2.1a"><mn id="A0.SSx1.p1.2.m2.1.1" xref="A0.SSx1.p1.2.m2.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p1.2.m2.1b"><cn type="float" id="A0.SSx1.p1.2.m2.1.1.cmml" xref="A0.SSx1.p1.2.m2.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p1.2.m2.1c">0.001</annotation></semantics></math>, the number of iterations to <math id="A0.SSx1.p1.3.m3.1" class="ltx_Math" alttext="100\,000" display="inline"><semantics id="A0.SSx1.p1.3.m3.1a"><mn id="A0.SSx1.p1.3.m3.1.1" xref="A0.SSx1.p1.3.m3.1.1.cmml">100 000</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p1.3.m3.1b"><cn type="integer" id="A0.SSx1.p1.3.m3.1.1.cmml" xref="A0.SSx1.p1.3.m3.1.1">100000</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p1.3.m3.1c">100\,000</annotation></semantics></math> and we use cosine annealing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for updating the learning rate during training.
We set the image size of StyleGAN to <math id="A0.SSx1.p1.4.m4.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="A0.SSx1.p1.4.m4.1a"><mrow id="A0.SSx1.p1.4.m4.1.1" xref="A0.SSx1.p1.4.m4.1.1.cmml"><mn id="A0.SSx1.p1.4.m4.1.1.2" xref="A0.SSx1.p1.4.m4.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="A0.SSx1.p1.4.m4.1.1.1" xref="A0.SSx1.p1.4.m4.1.1.1.cmml">×</mo><mn id="A0.SSx1.p1.4.m4.1.1.3" xref="A0.SSx1.p1.4.m4.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A0.SSx1.p1.4.m4.1b"><apply id="A0.SSx1.p1.4.m4.1.1.cmml" xref="A0.SSx1.p1.4.m4.1.1"><times id="A0.SSx1.p1.4.m4.1.1.1.cmml" xref="A0.SSx1.p1.4.m4.1.1.1"></times><cn type="integer" id="A0.SSx1.p1.4.m4.1.1.2.cmml" xref="A0.SSx1.p1.4.m4.1.1.2">256</cn><cn type="integer" id="A0.SSx1.p1.4.m4.1.1.3.cmml" xref="A0.SSx1.p1.4.m4.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p1.4.m4.1c">256\times 256</annotation></semantics></math> pixels for all of our experiments.</p>
</div>
<div id="A0.SSx1.p2" class="ltx_para">
<p id="A0.SSx1.p2.10" class="ltx_p">During the training of Doc-UFCN for segmentation, we follow the hyperparameters of Boillet et al.<span id="A0.SSx1.p2.10.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> by setting the initial learning rate to <math id="A0.SSx1.p2.1.m1.1" class="ltx_Math" alttext="0.005" display="inline"><semantics id="A0.SSx1.p2.1.m1.1a"><mn id="A0.SSx1.p2.1.m1.1.1" xref="A0.SSx1.p2.1.m1.1.1.cmml">0.005</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.1.m1.1b"><cn type="float" id="A0.SSx1.p2.1.m1.1.1.cmml" xref="A0.SSx1.p2.1.m1.1.1">0.005</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.1.m1.1c">0.005</annotation></semantics></math>, the threshold for keeping a text prediction (<em id="A0.SSx1.p2.10.2" class="ltx_emph ltx_font_italic">minimum confidence</em>) to <math id="A0.SSx1.p2.2.m2.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="A0.SSx1.p2.2.m2.1a"><mn id="A0.SSx1.p2.2.m2.1.1" xref="A0.SSx1.p2.2.m2.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.2.m2.1b"><cn type="float" id="A0.SSx1.p2.2.m2.1.1.cmml" xref="A0.SSx1.p2.2.m2.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.2.m2.1c">0.7</annotation></semantics></math>, and we discard connected components of text with an area of less than <math id="A0.SSx1.p2.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="A0.SSx1.p2.3.m3.1a"><mn id="A0.SSx1.p2.3.m3.1.1" xref="A0.SSx1.p2.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.3.m3.1b"><cn type="integer" id="A0.SSx1.p2.3.m3.1.1.cmml" xref="A0.SSx1.p2.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.3.m3.1c">50</annotation></semantics></math> pixels (<em id="A0.SSx1.p2.10.3" class="ltx_emph ltx_font_italic">minimum contour area</em>).
EMANet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is also trained using the proposed hyperparameters, namely an initial learning rate of <math id="A0.SSx1.p2.4.m4.1" class="ltx_Math" alttext="0.009" display="inline"><semantics id="A0.SSx1.p2.4.m4.1a"><mn id="A0.SSx1.p2.4.m4.1.1" xref="A0.SSx1.p2.4.m4.1.1.cmml">0.009</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.4.m4.1b"><cn type="float" id="A0.SSx1.p2.4.m4.1.1.cmml" xref="A0.SSx1.p2.4.m4.1.1">0.009</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.4.m4.1c">0.009</annotation></semantics></math>.
Weight decay coefficients are set to <math id="A0.SSx1.p2.5.m5.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A0.SSx1.p2.5.m5.1a"><mn id="A0.SSx1.p2.5.m5.1.1" xref="A0.SSx1.p2.5.m5.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.5.m5.1b"><cn type="float" id="A0.SSx1.p2.5.m5.1.1.cmml" xref="A0.SSx1.p2.5.m5.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.5.m5.1c">0.9</annotation></semantics></math> and <math id="A0.SSx1.p2.6.m6.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="A0.SSx1.p2.6.m6.1a"><mn id="A0.SSx1.p2.6.m6.1.1" xref="A0.SSx1.p2.6.m6.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.6.m6.1b"><cn type="float" id="A0.SSx1.p2.6.m6.1.1.cmml" xref="A0.SSx1.p2.6.m6.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.6.m6.1c">0.0001</annotation></semantics></math>.
The hyperparameter configuration for TransUNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is also taken from the referenced paper: learning rate of <math id="A0.SSx1.p2.7.m7.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="A0.SSx1.p2.7.m7.1a"><mn id="A0.SSx1.p2.7.m7.1.1" xref="A0.SSx1.p2.7.m7.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.7.m7.1b"><cn type="float" id="A0.SSx1.p2.7.m7.1.1.cmml" xref="A0.SSx1.p2.7.m7.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.7.m7.1c">0.01</annotation></semantics></math>, momentum of <math id="A0.SSx1.p2.8.m8.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A0.SSx1.p2.8.m8.1a"><mn id="A0.SSx1.p2.8.m8.1.1" xref="A0.SSx1.p2.8.m8.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.8.m8.1b"><cn type="float" id="A0.SSx1.p2.8.m8.1.1.cmml" xref="A0.SSx1.p2.8.m8.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.8.m8.1c">0.9</annotation></semantics></math> and weight decay of <math id="A0.SSx1.p2.9.m9.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="A0.SSx1.p2.9.m9.1a"><mn id="A0.SSx1.p2.9.m9.1.1" xref="A0.SSx1.p2.9.m9.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.9.m9.1b"><cn type="float" id="A0.SSx1.p2.9.m9.1.1.cmml" xref="A0.SSx1.p2.9.m9.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.9.m9.1c">0.0001</annotation></semantics></math>.
For the training of all our segmentation models, we use a batch size of <math id="A0.SSx1.p2.10.m10.1" class="ltx_Math" alttext="8" display="inline"><semantics id="A0.SSx1.p2.10.m10.1a"><mn id="A0.SSx1.p2.10.m10.1.1" xref="A0.SSx1.p2.10.m10.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A0.SSx1.p2.10.m10.1b"><cn type="integer" id="A0.SSx1.p2.10.m10.1.1.cmml" xref="A0.SSx1.p2.10.m10.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.SSx1.p2.10.m10.1c">8</annotation></semantics></math> and cosine annealing for updating our learning rate during training.</p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<figure id="A0.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Performance of the on the in-domain split across all generation models and segmentation models</figcaption>
<table id="A0.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A0.T4.1.1" class="ltx_tr">
<td id="A0.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A0.T4.1.1.1.1" class="ltx_text"><span id="A0.T4.1.1.1.1.1" class="ltx_text"></span> <span id="A0.T4.1.1.1.1.2" class="ltx_text">
<span id="A0.T4.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A0.T4.1.1.1.1.2.1.1" class="ltx_tr">
<span id="A0.T4.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Synthesis</span></span>
<span id="A0.T4.1.1.1.1.2.1.2" class="ltx_tr">
<span id="A0.T4.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Method</span></span>
</span></span> <span id="A0.T4.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="A0.T4.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A0.T4.1.1.2.1" class="ltx_text"><span id="A0.T4.1.1.2.1.1" class="ltx_text"></span> <span id="A0.T4.1.1.2.1.2" class="ltx_text">
<span id="A0.T4.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A0.T4.1.1.2.1.2.1.1" class="ltx_tr">
<span id="A0.T4.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Segmentation</span></span>
<span id="A0.T4.1.1.2.1.2.1.2" class="ltx_tr">
<span id="A0.T4.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Model</span></span>
</span></span> <span id="A0.T4.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="A0.T4.1.1.3" class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">printed text</td>
<td id="A0.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">handwritten text</td>
<td id="A0.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">background</td>
</tr>
<tr id="A0.T4.1.2" class="ltx_tr">
<td id="A0.T4.1.2.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">mIoU</td>
<td id="A0.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="A0.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="A0.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
<td id="A0.T4.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="A0.T4.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="A0.T4.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
<td id="A0.T4.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="A0.T4.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="A0.T4.1.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
</tr>
<tr id="A0.T4.1.3" class="ltx_tr">
<td id="A0.T4.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">DatasetGAN</td>
<td id="A0.T4.1.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="A0.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.655</td>
<td id="A0.T4.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.643</td>
<td id="A0.T4.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.827</td>
<td id="A0.T4.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.743</td>
<td id="A0.T4.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.326</td>
<td id="A0.T4.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.624</td>
<td id="A0.T4.1.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.406</td>
<td id="A0.T4.1.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.995</td>
<td id="A0.T4.1.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.997</td>
<td id="A0.T4.1.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.3.12.1" class="ltx_text ltx_font_bold">0.998</span></td>
</tr>
<tr id="A0.T4.1.4" class="ltx_tr">
<td id="A0.T4.1.4.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T4.1.4.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="A0.T4.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.615</td>
<td id="A0.T4.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.609</td>
<td id="A0.T4.1.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.832</td>
<td id="A0.T4.1.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.695</td>
<td id="A0.T4.1.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.239</td>
<td id="A0.T4.1.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.294</td>
<td id="A0.T4.1.4.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.562</td>
<td id="A0.T4.1.4.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.996</td>
<td id="A0.T4.1.4.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.997</td>
<td id="A0.T4.1.4.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.4.12.1" class="ltx_text ltx_font_bold">0.998</span></td>
</tr>
<tr id="A0.T4.1.5" class="ltx_tr">
<td id="A0.T4.1.5.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T4.1.5.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="A0.T4.1.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.521</td>
<td id="A0.T4.1.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.307</td>
<td id="A0.T4.1.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.707</td>
<td id="A0.T4.1.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.351</td>
<td id="A0.T4.1.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.265</td>
<td id="A0.T4.1.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.409</td>
<td id="A0.T4.1.5.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.429</td>
<td id="A0.T4.1.5.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.991</td>
<td id="A0.T4.1.5.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.993</td>
<td id="A0.T4.1.5.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.5.12.1" class="ltx_text ltx_font_bold">0.998</span></td>
</tr>
<tr id="A0.T4.1.6" class="ltx_tr">
<td id="A0.T4.1.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</td>
<td id="A0.T4.1.6.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="A0.T4.1.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.658</td>
<td id="A0.T4.1.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.696</td>
<td id="A0.T4.1.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.825</td>
<td id="A0.T4.1.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.817</td>
<td id="A0.T4.1.6.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.281</td>
<td id="A0.T4.1.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.320</td>
<td id="A0.T4.1.6.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.6.9.1" class="ltx_text ltx_font_bold">0.700</span></td>
<td id="A0.T4.1.6.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.996</td>
<td id="A0.T4.1.6.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.998</td>
<td id="A0.T4.1.6.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.6.12.1" class="ltx_text ltx_font_bold">0.998</span></td>
</tr>
<tr id="A0.T4.1.7" class="ltx_tr">
<td id="A0.T4.1.7.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T4.1.7.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="A0.T4.1.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.7.3.1" class="ltx_text ltx_font_bold">0.725</span></td>
<td id="A0.T4.1.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.7.4.1" class="ltx_text ltx_font_bold">0.714</span></td>
<td id="A0.T4.1.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.7.5.1" class="ltx_text ltx_font_bold">0.843</span></td>
<td id="A0.T4.1.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.824</td>
<td id="A0.T4.1.7.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.7.7.1" class="ltx_text ltx_font_bold">0.463</span></td>
<td id="A0.T4.1.7.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.7.8.1" class="ltx_text ltx_font_bold">0.672</span></td>
<td id="A0.T4.1.7.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.598</td>
<td id="A0.T4.1.7.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.7.10.1" class="ltx_text ltx_font_bold">0.997</span></td>
<td id="A0.T4.1.7.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.998</td>
<td id="A0.T4.1.7.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.7.12.1" class="ltx_text ltx_font_bold">0.998</span></td>
</tr>
<tr id="A0.T4.1.8" class="ltx_tr">
<td id="A0.T4.1.8.1" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T4.1.8.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="A0.T4.1.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.656</td>
<td id="A0.T4.1.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.518</td>
<td id="A0.T4.1.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.560</td>
<td id="A0.T4.1.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.8.6.1" class="ltx_text ltx_font_bold">0.875</span></td>
<td id="A0.T4.1.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.460</td>
<td id="A0.T4.1.8.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.707</td>
<td id="A0.T4.1.8.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.568</td>
<td id="A0.T4.1.8.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.991</td>
<td id="A0.T4.1.8.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T4.1.8.11.1" class="ltx_text ltx_font_bold">0.999</span></td>
<td id="A0.T4.1.8.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.993</td>
</tr>
</table>
</figure>
<figure id="A0.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Performance of the on the out-of-domain split across all generation models and segmentation models</figcaption>
<table id="A0.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A0.T5.1.1" class="ltx_tr">
<td id="A0.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A0.T5.1.1.1.1" class="ltx_text"><span id="A0.T5.1.1.1.1.1" class="ltx_text"></span> <span id="A0.T5.1.1.1.1.2" class="ltx_text">
<span id="A0.T5.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A0.T5.1.1.1.1.2.1.1" class="ltx_tr">
<span id="A0.T5.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Synthesis</span></span>
<span id="A0.T5.1.1.1.1.2.1.2" class="ltx_tr">
<span id="A0.T5.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Method</span></span>
</span></span> <span id="A0.T5.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="A0.T5.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A0.T5.1.1.2.1" class="ltx_text"><span id="A0.T5.1.1.2.1.1" class="ltx_text"></span> <span id="A0.T5.1.1.2.1.2" class="ltx_text">
<span id="A0.T5.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A0.T5.1.1.2.1.2.1.1" class="ltx_tr">
<span id="A0.T5.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Segmentation</span></span>
<span id="A0.T5.1.1.2.1.2.1.2" class="ltx_tr">
<span id="A0.T5.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Model</span></span>
</span></span> <span id="A0.T5.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="A0.T5.1.1.3" class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">printed text</td>
<td id="A0.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">handwritten text</td>
<td id="A0.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="3">background</td>
</tr>
<tr id="A0.T5.1.2" class="ltx_tr">
<td id="A0.T5.1.2.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">mIoU</td>
<td id="A0.T5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="A0.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="A0.T5.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
<td id="A0.T5.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="A0.T5.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="A0.T5.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
<td id="A0.T5.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IoU</td>
<td id="A0.T5.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Prec.</td>
<td id="A0.T5.1.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Recall</td>
</tr>
<tr id="A0.T5.1.3" class="ltx_tr">
<td id="A0.T5.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">DatasetGAN</td>
<td id="A0.T5.1.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="A0.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.455</td>
<td id="A0.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.325</td>
<td id="A0.T5.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.802</td>
<td id="A0.T5.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.353</td>
<td id="A0.T5.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.059</td>
<td id="A0.T5.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.261</td>
<td id="A0.T5.1.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.070</td>
<td id="A0.T5.1.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.980</td>
<td id="A0.T5.1.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.982</td>
<td id="A0.T5.1.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.998</td>
</tr>
<tr id="A0.T5.1.4" class="ltx_tr">
<td id="A0.T5.1.4.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T5.1.4.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="A0.T5.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.482</td>
<td id="A0.T5.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.336</td>
<td id="A0.T5.1.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.863</td>
<td id="A0.T5.1.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.355</td>
<td id="A0.T5.1.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.125</td>
<td id="A0.T5.1.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.224</td>
<td id="A0.T5.1.4.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.220</td>
<td id="A0.T5.1.4.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.984</td>
<td id="A0.T5.1.4.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.985</td>
<td id="A0.T5.1.4.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.998</td>
</tr>
<tr id="A0.T5.1.5" class="ltx_tr">
<td id="A0.T5.1.5.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T5.1.5.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="A0.T5.1.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.431</td>
<td id="A0.T5.1.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.241</td>
<td id="A0.T5.1.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.844</td>
<td id="A0.T5.1.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.252</td>
<td id="A0.T5.1.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.072</td>
<td id="A0.T5.1.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.195</td>
<td id="A0.T5.1.5.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.101</td>
<td id="A0.T5.1.5.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.979</td>
<td id="A0.T5.1.5.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.981</td>
<td id="A0.T5.1.5.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.5.12.1" class="ltx_text ltx_font_bold">0.999</span></td>
</tr>
<tr id="A0.T5.1.6" class="ltx_tr">
<td id="A0.T5.1.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</td>
<td id="A0.T5.1.6.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="A0.T5.1.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.488</td>
<td id="A0.T5.1.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.375</td>
<td id="A0.T5.1.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.6.5.1" class="ltx_text ltx_font_bold">0.875</span></td>
<td id="A0.T5.1.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.396</td>
<td id="A0.T5.1.6.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.106</td>
<td id="A0.T5.1.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.154</td>
<td id="A0.T5.1.6.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.255</td>
<td id="A0.T5.1.6.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.983</td>
<td id="A0.T5.1.6.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.987</td>
<td id="A0.T5.1.6.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.996</td>
</tr>
<tr id="A0.T5.1.7" class="ltx_tr">
<td id="A0.T5.1.7.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T5.1.7.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="A0.T5.1.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.541</td>
<td id="A0.T5.1.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.466</td>
<td id="A0.T5.1.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.866</td>
<td id="A0.T5.1.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.502</td>
<td id="A0.T5.1.7.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.7.7.1" class="ltx_text ltx_font_bold">0.173</span></td>
<td id="A0.T5.1.7.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.7.8.1" class="ltx_text ltx_font_bold">0.384</span></td>
<td id="A0.T5.1.7.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.240</td>
<td id="A0.T5.1.7.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.7.10.1" class="ltx_text ltx_font_bold">0.985</span></td>
<td id="A0.T5.1.7.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.987</td>
<td id="A0.T5.1.7.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.998</td>
</tr>
<tr id="A0.T5.1.8" class="ltx_tr">
<td id="A0.T5.1.8.1" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T5.1.8.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="A0.T5.1.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.8.3.1" class="ltx_text ltx_font_bold">0.557</span></td>
<td id="A0.T5.1.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.8.4.1" class="ltx_text ltx_font_bold">0.523</span></td>
<td id="A0.T5.1.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.675</td>
<td id="A0.T5.1.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.8.6.1" class="ltx_text ltx_font_bold">0.699</span></td>
<td id="A0.T5.1.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.167</td>
<td id="A0.T5.1.8.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.270</td>
<td id="A0.T5.1.8.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.8.9.1" class="ltx_text ltx_font_bold">0.303</span></td>
<td id="A0.T5.1.8.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.983</td>
<td id="A0.T5.1.8.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A0.T5.1.8.11.1" class="ltx_text ltx_font_bold">0.992</span></td>
<td id="A0.T5.1.8.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.991</td>
</tr>
</table>
</figure>
<figure id="A0.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Post-processing hyperparameters used for evaluation. The grid search was performed for the patch_overlap_factor <math id="A0.T6.9.m1.1" class="ltx_Math" alttext="0.0" display="inline"><semantics id="A0.T6.9.m1.1b"><mn id="A0.T6.9.m1.1.1" xref="A0.T6.9.m1.1.1.cmml">0.0</mn><annotation-xml encoding="MathML-Content" id="A0.T6.9.m1.1c"><cn type="float" id="A0.T6.9.m1.1.1.cmml" xref="A0.T6.9.m1.1.1">0.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.9.m1.1d">0.0</annotation></semantics></math> and <math id="A0.T6.10.m2.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="A0.T6.10.m2.1b"><mn id="A0.T6.10.m2.1.1" xref="A0.T6.10.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="A0.T6.10.m2.1c"><cn type="float" id="A0.T6.10.m2.1.1.cmml" xref="A0.T6.10.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.10.m2.1d">0.5</annotation></semantics></math>, the min_confidence values <math id="A0.T6.11.m3.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="A0.T6.11.m3.1b"><mn id="A0.T6.11.m3.1.1" xref="A0.T6.11.m3.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="A0.T6.11.m3.1c"><cn type="float" id="A0.T6.11.m3.1.1.cmml" xref="A0.T6.11.m3.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.11.m3.1d">0.3</annotation></semantics></math>, <math id="A0.T6.12.m4.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="A0.T6.12.m4.1b"><mn id="A0.T6.12.m4.1.1" xref="A0.T6.12.m4.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="A0.T6.12.m4.1c"><cn type="float" id="A0.T6.12.m4.1.1.cmml" xref="A0.T6.12.m4.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.12.m4.1d">0.7</annotation></semantics></math>, <math id="A0.T6.13.m5.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A0.T6.13.m5.1b"><mn id="A0.T6.13.m5.1.1" xref="A0.T6.13.m5.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A0.T6.13.m5.1c"><cn type="float" id="A0.T6.13.m5.1.1.cmml" xref="A0.T6.13.m5.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.13.m5.1d">0.9</annotation></semantics></math> and the min_contour_area values <math id="A0.T6.14.m6.1" class="ltx_Math" alttext="15" display="inline"><semantics id="A0.T6.14.m6.1b"><mn id="A0.T6.14.m6.1.1" xref="A0.T6.14.m6.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="A0.T6.14.m6.1c"><cn type="integer" id="A0.T6.14.m6.1.1.cmml" xref="A0.T6.14.m6.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.14.m6.1d">15</annotation></semantics></math>, <math id="A0.T6.15.m7.1" class="ltx_Math" alttext="30" display="inline"><semantics id="A0.T6.15.m7.1b"><mn id="A0.T6.15.m7.1.1" xref="A0.T6.15.m7.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="A0.T6.15.m7.1c"><cn type="integer" id="A0.T6.15.m7.1.1.cmml" xref="A0.T6.15.m7.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.15.m7.1d">30</annotation></semantics></math>, and <math id="A0.T6.16.m8.1" class="ltx_Math" alttext="55" display="inline"><semantics id="A0.T6.16.m8.1b"><mn id="A0.T6.16.m8.1.1" xref="A0.T6.16.m8.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="A0.T6.16.m8.1c"><cn type="integer" id="A0.T6.16.m8.1.1.cmml" xref="A0.T6.16.m8.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="A0.T6.16.m8.1d">55</annotation></semantics></math></figcaption>
<table id="A0.T6.17" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A0.T6.17.1" class="ltx_tr">
<td id="A0.T6.17.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Synthesis Method</td>
<td id="A0.T6.17.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Segmentation Model</td>
<td id="A0.T6.17.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Minimum Confidence</td>
<td id="A0.T6.17.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Minimum Contour Area</td>
<td id="A0.T6.17.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Patch Overlap Factor</td>
</tr>
<tr id="A0.T6.17.2" class="ltx_tr">
<td id="A0.T6.17.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">DatasetGAN</td>
<td id="A0.T6.17.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="A0.T6.17.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.3</td>
<td id="A0.T6.17.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">15</td>
<td id="A0.T6.17.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.0</td>
</tr>
<tr id="A0.T6.17.3" class="ltx_tr">
<td id="A0.T6.17.3.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T6.17.3.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="A0.T6.17.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.7</td>
<td id="A0.T6.17.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">55</td>
<td id="A0.T6.17.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.5</td>
</tr>
<tr id="A0.T6.17.4" class="ltx_tr">
<td id="A0.T6.17.4.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T6.17.4.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="A0.T6.17.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.3</td>
<td id="A0.T6.17.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15</td>
<td id="A0.T6.17.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.5</td>
</tr>
<tr id="A0.T6.17.5" class="ltx_tr">
<td id="A0.T6.17.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</td>
<td id="A0.T6.17.5.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Doc-UFCN</td>
<td id="A0.T6.17.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.3</td>
<td id="A0.T6.17.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15</td>
<td id="A0.T6.17.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.5</td>
</tr>
<tr id="A0.T6.17.6" class="ltx_tr">
<td id="A0.T6.17.6.1" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T6.17.6.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TransUNet</td>
<td id="A0.T6.17.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.9</td>
<td id="A0.T6.17.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15</td>
<td id="A0.T6.17.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.5</td>
</tr>
<tr id="A0.T6.17.7" class="ltx_tr">
<td id="A0.T6.17.7.1" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="A0.T6.17.7.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">EMANet</td>
<td id="A0.T6.17.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.3</td>
<td id="A0.T6.17.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">15</td>
<td id="A0.T6.17.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.5</td>
</tr>
</table>
</figure>
<figure id="A0.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x4.png" id="A0.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="249" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x5.png" id="A0.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="249" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x6.png" id="A0.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="249" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Demonstration of our approach applied to an in-domain sample. Bottom: RGB image, Middle: Ground truth segmentation, Top: Prediction of TransUNet trained on synthetic data generated with our approach. The prediction is done on image patches that are then reassembled into the segmentation of the full image</figcaption>
</figure>
<figure id="A0.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x7.png" id="A0.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="233" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x8.png" id="A0.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="233" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x9.png" id="A0.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="233" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Demonstration of our approach applied to an in-domain sample. Bottom: RGB image, Middle: Ground truth segmentation, Top: Prediction of TransUNet trained on synthetic data generated with our approach. The prediction is done on image patches that are then reassembled into the segmentation of the full image</figcaption>
</figure>
<figure id="A0.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x10.png" id="A0.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="212" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x11.png" id="A0.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="212" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x12.png" id="A0.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="212" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Demonstration of our approach applied to an out-of-domain sample. Bottom: RGB image, Middle: Ground truth segmentation, Top: Prediction of TransUNet trained on synthetic data generated with our approach. The prediction is done on image patches that are then reassembled into the segmentation of the full image</figcaption>
</figure>
<figure id="A0.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x13.png" id="A0.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="213" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x14.png" id="A0.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="213" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A0.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.06777/assets/x15.png" id="A0.F7.sf3.g1" class="ltx_graphics ltx_img_landscape" width="213" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Demonstration of our approach applied to an out-of-domain sample. Bottom: RGB image, Middle: Ground truth segmentation, Top: Prediction of TransUNet trained on synthetic data generated with our approach. The prediction is done on image patches that are then reassembled into the segmentation of the full image</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.06776" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.06777" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.06777">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.06777" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.06778" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 13:03:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
