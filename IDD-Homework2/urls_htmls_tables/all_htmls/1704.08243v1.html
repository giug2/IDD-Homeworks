<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1704.08243] C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset</title><meta property="og:description" content="Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown [1, 2, 3, 4] that these model…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1704.08243">

<!--Generated on Sat Mar 16 06:12:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">C-VQA: A Compositional Split of the 
<br class="ltx_break">Visual Question Answering (VQA) v1.0 Dataset</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aishwarya Agrawal<sup id="id8.8.id1" class="ltx_sup">∗</sup>, Aniruddha Kembhavi<sup id="id9.9.id2" class="ltx_sup">†</sup>, Dhruv Batra<sup id="id10.10.id3" class="ltx_sup">‡</sup>, Devi Parikh<sup id="id11.11.id4" class="ltx_sup">‡</sup> 
<br class="ltx_break"><sup id="id12.12.id5" class="ltx_sup">∗</sup>Virginia Tech, <sup id="id13.13.id6" class="ltx_sup">†</sup>Allen Institute for Artificial Intelligence, <sup id="id14.14.id7" class="ltx_sup">‡</sup>Georgia Institute of Technology
<br class="ltx_break"><span id="id15.15.id8" class="ltx_text ltx_font_typewriter">aish@vt.edu, anik@allenai.org, {dbatra, parikh}@gatech.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> that these models are heavily driven by superficial correlations in the training data and lack <em id="id16.id1.1" class="ltx_emph ltx_font_italic">compositionality</em> – the ability to answer questions about <em id="id16.id1.2" class="ltx_emph ltx_font_italic">unseen compositions</em> of <em id="id16.id1.3" class="ltx_emph ltx_font_italic">seen concepts</em>. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Automatically answering questions about visual content is considered to be one of the holy grails of artificial intelligence research. Visual Question Answering (VQA) poses a rich set of challenges spanning various domains such as computer vision, natural language processing, knowledge representation and reasoning. VQA is a stepping stone to visually grounded dialog and intelligent agents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. In the past couple of years, VQA has received a lot of attention. Various VQA datasets have been proposed by different groups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and a number of deep-learning models have been developed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">However, it has been shown that despite recent progress, today’s VQA models are heavily driven by superficial correlations in the training data and lack compositionality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> – the ability to answer questions about <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">unseen compositions</em> of <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">seen concepts</em>. For instance, a model is said to be compositional if it can correctly answer [“What <span id="S1.p2.1.3" class="ltx_text" style="color:#808080;">color</span> are the <span id="S1.p2.1.4" class="ltx_text" style="color:#FF8000;">safety</span> <span id="S1.p2.1.5" class="ltx_text" style="color:#0000FF;">cones</span>?”, “<span id="S1.p2.1.6" class="ltx_text" style="color:#00FF00;">green</span>”] without seeing this question-answer (QA) pair during training, but perhaps having seen [“What <span id="S1.p2.1.7" class="ltx_text" style="color:#808080;">color</span> are the <span id="S1.p2.1.8" class="ltx_text" style="color:#FF8000;">safety</span> <span id="S1.p2.1.9" class="ltx_text" style="color:#0000FF;">cones</span>?”, “orange”] and [“What <span id="S1.p2.1.10" class="ltx_text" style="color:#808080;">color</span> are the plates?”, “<span id="S1.p2.1.11" class="ltx_text" style="color:#00FF00;">green</span>”] during training.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1704.08243/assets/figures/compositionality.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="440" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples from our Compositional VQA (C-VQA) splits. Words belonging to same concepts are highlighted with same color to show the training instances from which the model can learn those concepts.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In order to evaluate the extent to which existing VQA models are compositional, we create a compositional split of the VQA v1.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, called <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Compositional VQA (C-VQA)</span>. This new dataset is created by re-arranging the train and val splits of the VQA v1.0 dataset in such a way that the question-answer (QA) pairs in C-VQA test split are compositionally novel with respect to those in C-VQA train split, i.e., QA pairs in C-VQA test split are not present in C-VQA train splits but most concepts constituting the QA pairs in test split are present in the train split. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows some examples from our C-VQA splits. Since, C-VQA test split contains the QA pair [“What is the color of the plate?”, “red”], similar QA pairs such as [“What color is the plate?”, “red”] are not present in C-VQA train split. But C-VQA train split contains other QA pairs consisting of the concepts “plate”, “red” and “color” such as [“What color is the plate?”, “green”] and [“What color are stop lights?”, “red”].<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>It should be noted that in the VQA v1.0 splits, a given Image, Question, Answer (IQA) triplet is not shared across splits but a given QA pair could be shared across splits.</span></span></span></p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Evaluating a VQA model under such setting helps in testing – 1) whether the model is capable of learning disentangled representations for different concepts (<em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.2" class="ltx_text"></span>, “plate”, “green”, “stop light”, “red”), 2) whether the model can compose the concepts learned during training to correctly answer questions about novel compositions at test time. Please see Section <a href="#S3" title="3 Compositional Visual Question Answering (C-VQA) ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for more details about C-VQA splits.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on our C-VQA splits. Our experiments show that the performance of the VQA models drops significantly (with performance drop being smaller for models which are compositional by design such as the Neural Module Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>) when trained and evaluated on train and test splits (respectively) of C-VQA, compared to when these models are trained and evaluated on train and val splits (respectively) of the original VQA v1.0. Please see Section <a href="#S4" title="4 Baselines ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for more details about these experiments.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Question Answering.</span> Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Over the span of time, the size of VQA datasets has become larger and questions have becomes more free form and open-ended. For instance, one of the earliest VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> considers questions generated using templates and consists of fixed vocabulary of objects, attributes, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> also consider questions whose answers come from a closed world. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> generate questions automatically using image captions and their answers belong to one of the following four types – object, number, color, location. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> consist of free form open-ended questions. Of these datasets, the VQA v1.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> has been used widely to train deep models. Performance of such models has increased steadily over the past two years on the test set of VQA v1.0 which has a similar distribution of data points as its training set. However, careful examination of the behaviors of such models reveals that these models are heavily driven by superficial correlations in the training data and lack compositionality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This is partly because the training set of VQA v1.0 contains strong language priors which data-driven models can learn easily and can perform well on the test set which consists of similar priors as the training set, without truly understanding the visual content in images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, because it is easier to learn the biases of the data (or even our world) than to truly understand images.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">In order to counter the language priors, Goyal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> balance every question in the VQA v1.0 dataset by collecting complementary images for every question. Thus, for every question in the VQA v2.0 dataset, there are two similar images that have different answers to the question. Clearly, language priors are significantly weaker in the VQA v2.0 dataset.
However, such balancing does not test for compositionality because the train and test distributions are similar. So, in order to test whether models can learn each concept individually irrespective of the correlations in the data and can perform well on a test set which has a different distribution of correlations compared to the training set, we propose a compositional split of the VQA v1.0 dataset, which we call Compositional-VQA (C-VQA).</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Compositionality.</span> The ability to generalize to novel compositions of concepts learned during training is desirable from any intelligent system. Compositionality has been studied in various forms in the vision community. Zero-shot object recognition using attributes is based on the idea of composing attributes to detect novel object categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> have studied compositionality in the domain of image captioning by focusing on structured representations (subject-relation-object triplets). We study compositionality for visual question answering where the questions and answers are open-ended and in free-form natural language. The work closest to us is <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> where they study compositionality in the domain of VQA. However, their dataset (images as well as questions) is synthetic and has only limited number of objects and attributes. On the contrary, our C-VQA splits consist of real images and questions (asked by humans) and hence involve a variety of objects and attributes, as well as activities, scenes, etc. Andreas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> have developed compositional models for VQA that consist of different modules each specialized for a particular task. These modules can be composed together based on the question structure to create a model architecture for the given question. Although, compositional by design, these models have not been evaluated specifically for compositionality. Our C-VQA splits can be used to evaluate such models to test the degree of compositionality. In fact, we report the performance of Neural Module Networks on VQA v1.0 and C-VQA splits (Section <a href="#S4" title="4 Baselines ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">The compositionality setting we are proposing is one type of zero-shot VQA where test QA pairs are novel. Other types of zero-shot VQA have also been explored. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> propose a setting for VQA where the test questions (the question string itself or the multiple choices) contain atleast one unseen word. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> propose answering questions about unknown objects (<em id="S2.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p4.1.2" class="ltx_text"></span>, “Is the dog black and white?” where “dog” is never seen in training (neither in questions, nor in answers)).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Compositional Visual Question Answering (C-VQA)</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>C-VQA Creation</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The C-VQA splits are created by re-arranging the training and validation splits of the VQA v1.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We can not use the test splits from VQA v1.0 because creation of C-VQA splits requires access to test annotations which are not publicly available.</span></span></span>. These splits are created such that the question-answer (QA) pairs in the C-VQA test split (<em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.1.2" class="ltx_text"></span>, Question: “What color is the plate?”, Answer: “green”) are not seen in the C-VQA train split, but in most cases, the concepts that compose the C-VQA test QA pairs (<em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.1.4" class="ltx_text"></span>, “plate”, “green”) have been seen in the C-VQA train split (<em id="S3.SS1.p1.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.1.6" class="ltx_text"></span>, Question: “What color is the apple?”, Answer: “Green”, Question: “How many plates are on the table?”, Answer: “4”).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">The C-VQA splits are created using the following procedure –</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Question Reduction:</span> Every question is reduced to a list of concepts needed to answer the question. For instance,</p>
<p id="S3.SS1.p3.2" class="ltx_p ltx_align_center">“What color are the cones?” is reduced to [“what”, “color”, “cone”].</p>
<p id="S3.SS1.p3.3" class="ltx_p">We do this in order to reduce similar questions to the same form. For instance, “What color are the cones?” and “What is the color of the cones?” both get reduced to the same form – [“what”, “color”, “cone”]’. This reduction is achieved using simple text processing such as removal of stop words and lemmatization.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Reduced QA Grouping:</span> Questions having the same reduced form and the same ground truth answer are grouped together. For instance,</p>
<p id="S3.SS1.p4.2" class="ltx_p ltx_align_center">[“What color are the cones?”, “orange”] and [“What are the color of the cones?”, “orange”] are grouped together whereas [“What color are the cones?”, “green”] is put in a different group.</p>
<p id="S3.SS1.p4.3" class="ltx_p">This grouping is done after merging the QA pairs from the VQA v1.0 train and val splits.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Greedily Re-splitting:</span> A greedy approach is used to redistribute data points (image, question, answer) to the C-VQA train and test splits so as to maximize the coverage of the test concepts in the C-VQA train split while making sure QA pairs are not repeated between test and train splits. In this procedure, we loop through all the groups created above, and in every iteration, we add the current group to the C-VQA test split unless the group has already been assigned to the C-VQA train split. We always maintain a set of concepts<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For a given group, concepts are the set of all unique words present in the reduced question and the ground truth answer belonging to that group</span></span></span> belonging to the groups in the C-VQA test split that have not yet been covered by the groups belonging to the C-VQA train split. From the groups that have not yet been assigned to either of the splits, we find the group that covers majority of the concepts (in the list) and add that group to the C-VQA train split.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.1" class="ltx_p">The above approach results in 73.5% of the unique C-VQA test concepts to be covered in the C-VQA train split. The coverage is 98.8% when taking into account the frequency of occurrence of each concept in C-VQA test split.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.1 C-VQA Creation ‣ 3 Compositional Visual Question Answering (C-VQA) ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the number of questions, images and answers in the train and val splits of the VQA v1.0 dataset and those in train and test splits of the C-VQA dataset. We can see that the number of questions and the number of answers in the C-VQA splits is similar to that in the VQA v1.0 splits. However, the number of images in the C-VQA splits is more than that in the VQA v1.0 splits. This is because in the C-VQA splits, the same image can be present in both the train and the test sets. Note that there are three questions for every image in the VQA v1.0 dataset and the splitting for C-VQA is done based on QA pairs, not based on images. Consider the following two questions associated with the same image in VQA v1.0 – “What color are the cones?” (with the answer “orange”) and “What time of day is it?” (with the answer “afternoon”). It is possible that “What color are the cones?” (along with the image and the ground-truth answers) gets assigned to C-VQA train split and “What time of day is it?” gets assigned to C-VQA test split. As a result, the image corresponding to these questions would be present in both the train and test splits of C-VQA.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>To verify that sharing of images across splits does not make the problem easier, we randomly split the VQA v1.0 train+val into random-train and random-val. We then trained and evaluated the deeper LSTM Q + norm I model from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> on these new splits. We saw that the this new setup leads to only <math id="footnote4.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="footnote4.m1.1b"><mo id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><csymbol cd="latexml" id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">\sim</annotation></semantics></math>1% increase in the model performance compared to the VQA v1.0 train and val setup.</span></span></span></p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Dataset</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Split</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">#Questions</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">#Images</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">#Answers</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">Train</th>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">248,349</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">82,783</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">2,483,490</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">VQA (v1.0) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<th id="S3.T1.1.3.2.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<td id="S3.T1.1.3.2.3" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S3.T1.1.3.2.4" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_nopad_r" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">Val</th>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">121,512</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">40,504</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">1,215,120</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S3.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">Train</th>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">246,574</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">118,663</td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">2,465,740</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">C-VQA (proposed)</th>
<th id="S3.T1.1.6.5.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<td id="S3.T1.1.6.5.3" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S3.T1.1.6.5.4" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S3.T1.1.6.5.5" class="ltx_td ltx_nopad_r" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S3.T1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">Test</th>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">123,287</td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">86,700</td>
<td id="S3.T1.1.7.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">1,232,870</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Statistics of the VQA v1.0 and our C-VQA splits.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>C-VQA Analysis</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">In this section we analyze how the distributions of questions and answers in the C-VQA train and test splits differ from those in the VQA v1.0 train and val splits.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Question Distribution</span>. Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 C-VQA Analysis ‣ 3 Compositional Visual Question Answering (C-VQA) ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the distribution of questions based on the first four words of the questions for the train (left) and test (right) splits of the C-VQA dataset. We can see that splitting the dataset compositionally (as in C-VQA) does not lead to significant differences in the distribution of questions across splits, keeping the distributions qualitatively similar to VQA v1.0 splits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Quantitatively, 46.06% of the question strings in the VQA v1.0 val split are also present in the VQA v1.0 train split, whereas this percentage is 37.76 for the C-VQA splits.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1704.08243/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of questions by their first four words for a random sample of 60K questions for <span id="S3.F2.3.1" class="ltx_text ltx_font_bold">C-VQA train split</span> (left) and <span id="S3.F2.4.2" class="ltx_text ltx_font_bold">C-VQA test split</span> (right). The ordering of the words starts towards the center and radiates outwards. The arc length is proportional to the number of questions containing the word. White areas are words with contributions too small to show.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Answer Distribution.</span> Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 C-VQA Analysis ‣ 3 Compositional Visual Question Answering (C-VQA) ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the distribution of answers for several question types such as “what color”, “what sport”, “how many”, etc. for the train (top) and test (bottom) splits of the C-VQA dataset. We can see that the distributions of answers for a given question type is significantly different. However, for VQA v1.0 dataset, the distribution for a given question type is similar across train and val splits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. For instance, “tennis” is the most frequent answer for the question type “what sport” in C-VQA train split whereas “skiing” is the most frequent answer for the same question type in C-VQA test split. However, for the VQA v1.0 splits, “tennis” is the most frequent answer for both the train and val splits. Similar differences can be seen for other question types as well – “what animal”, “what brand”, “what kind”, “what type”, “what are”. Quantitatively, 32.49% of the QA pairs in the VQA v1.0 val split are also present in the VQA v1.0 train split, whereas this percentage is 0 for the C-VQA splits (by construction).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1704.08243/assets/figures/comp_ans.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="483" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of answers per question type for a random sample of 60K questions for <span id="S3.F3.3.1" class="ltx_text ltx_font_bold">C-VQA train split</span> (top) and <span id="S3.F3.4.2" class="ltx_text ltx_font_bold">C-VQA test split</span> (bottom).</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Baselines</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We report the performances of the following VQA models when trained on C-VQA train split and evaluated on C-VQA test split and compare this with the setting when these models are trained on VQA v1.0 train split and evaluated on VQA v1.0 val split (Table <a href="#S4.T2" title="Table 2 ‣ 4 Baselines ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Deeper LSTM Question + normalized Image (deeper LSTM Q + norm I)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>: This model was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. It is a two channel model – one channel processes the image and the other channel processes the question. For each image, the image channel extracts the activations (4096-dim) of the last hidden layer of the VGGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and normalizes them. For each question, the question channel extracts the hidden state and cell state activations of the last hidden layers of 2-layered LSTM, resulting in a 2048-dim encoding of the question. The image features (4096-dim) obtained from the image channel and the question features (2048-dim) obtained from the question channel are linearly transformed to 1024 dimensions each and fused together via element-wise multiplication. This fused vector is then passed through one more fully-connected layer in a Multi-Layered Perceptron (MLP), which finally outputs a 1000-way softmax score over the 1000 most frequent answers from the training set. The entire model, except the CNN (which is not fine-tuned) is learned end-to-end with a cross-entropy loss.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Neural Module Networks (NMN)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>: This model is designed to be compositional in nature. The model consists of composable modules where each module has a specific role (such as detecting a dog in the image, counting the number of dogs in the image, etc.). Given an image and the natural language question about the image, NMN first decomposes the question into its linguistic substructures using a parser. These structures determine which modules need to be composed together in what layout to create the network for answering the question. The resulting compound networks are jointly trained. At test time, the image and the question are forward propagated through the dynamically composed network which outputs a distribution over answers. In addition to the network composed using different modules, NMN also uses an LSTM to encode the question which is then added elementwise to the representation produced by the last module of the NMN. This combined representation is passed through a fully-connected layer to output a softmax distribution over answers. The LSTM encodes priors in the training data and models syntactic regularities such as singular vs. plural (“what is flying?” should be answered with “kite” whereas “what are flying?” should be answered with “kites”).</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Stacked Attention Networks (SAN)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>: This is one of the widely used models for VQA. This model is different from other VQA models in that it uses multiple hops of attention over the image. Given an image and the natural language question, SAN uses the question to obtain an attention map over the image. The attended image is combined with the encoded question vector which becomes the new query vector. This new query vector is used again to obtain a second round of attention over the image. The query vector obtained from the second round of attention is passed through a fully-connected layer to obtain a distribution over answers.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We use a torch implementation of SAN, available at <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/abhshkdz/neural-vqa-attention</a>, for our experiments.</span></span></span></p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Hierarchical Question-Image Co-attention Networks (HieCoAtt)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>: This is one of the top performing models for VQA. In addition to modeling attention over image, this model also models attention over question. Both image and question attention are computed in a hierarchical fashion. The attended image and question features obtained from different levels of the hierarchy are combined and passed through a fully-connected layer to obtain a softmax distribution over the space of answers.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Multimodal Compact Bilinear Pooling (MCB)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>: This model won the real image track of the VQA Challenge 2016. MCB uses multimodal compact bilinear pooling to predict attention over image features and also to combine the attended image features with the question features. These combined features are passed through a fully-connected layer to obtain a softmax distribution over the space of answers.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Model</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Dataset</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Yes/No</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Number</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Other</th>
<th id="S4.T2.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.2pt;padding-right:3.2pt;">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">VQA v1.0 val</th>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">79.81</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">33.26</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">40.35</td>
<td id="S4.T2.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">54.23</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">deeper LSTM Q + norm I <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<th id="S4.T2.1.3.2.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<td id="S4.T2.1.3.2.3" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.3.2.4" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.3.2.5" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.3.2.6" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">C-VQA test</th>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">70.60</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">29.76</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">31.83</td>
<td id="S4.T2.1.4.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">46.69</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">VQA v1.0 val</th>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">80.39</td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">33.45</td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">41.07</td>
<td id="S4.T2.1.5.4.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">54.83</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">NMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<th id="S4.T2.1.6.5.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<td id="S4.T2.1.6.5.3" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.6.5.4" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.6.5.5" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.6.5.6" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<th id="S4.T2.1.7.6.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">C-VQA test</th>
<td id="S4.T2.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">72.96</td>
<td id="S4.T2.1.7.6.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">31.02</td>
<td id="S4.T2.1.7.6.5" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">34.49</td>
<td id="S4.T2.1.7.6.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">49.05</td>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<th id="S4.T2.1.8.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">VQA v1.0 val</th>
<td id="S4.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">78.54</td>
<td id="S4.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">33.46</td>
<td id="S4.T2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">44.51</td>
<td id="S4.T2.1.8.7.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">55.86</td>
</tr>
<tr id="S4.T2.1.9.8" class="ltx_tr">
<th id="S4.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<th id="S4.T2.1.9.8.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<td id="S4.T2.1.9.8.3" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.9.8.4" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.9.8.5" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.9.8.6" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
</tr>
<tr id="S4.T2.1.10.9" class="ltx_tr">
<th id="S4.T2.1.10.9.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.10.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">C-VQA test</th>
<td id="S4.T2.1.10.9.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">66.96</td>
<td id="S4.T2.1.10.9.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">24.30</td>
<td id="S4.T2.1.10.9.5" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">33.19</td>
<td id="S4.T2.1.10.9.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">45.25</td>
</tr>
<tr id="S4.T2.1.11.10" class="ltx_tr">
<th id="S4.T2.1.11.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.11.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">VQA v1.0 val</th>
<td id="S4.T2.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">79.81</td>
<td id="S4.T2.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">34.93</td>
<td id="S4.T2.1.11.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">45.64</td>
<td id="S4.T2.1.11.10.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">57.09</td>
</tr>
<tr id="S4.T2.1.12.11" class="ltx_tr">
<th id="S4.T2.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">HieCoAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<th id="S4.T2.1.12.11.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<td id="S4.T2.1.12.11.3" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.12.11.4" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.12.11.5" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.12.11.6" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
</tr>
<tr id="S4.T2.1.13.12" class="ltx_tr">
<th id="S4.T2.1.13.12.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.13.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">C-VQA test</th>
<td id="S4.T2.1.13.12.3" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">71.11</td>
<td id="S4.T2.1.13.12.4" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">30.48</td>
<td id="S4.T2.1.13.12.5" class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">38.31</td>
<td id="S4.T2.1.13.12.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;">50.12</td>
</tr>
<tr id="S4.T2.1.14.13" class="ltx_tr">
<th id="S4.T2.1.14.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.14.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">VQA v1.0 val</th>
<td id="S4.T2.1.14.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">81.62</td>
<td id="S4.T2.1.14.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">34.56</td>
<td id="S4.T2.1.14.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">52.16</td>
<td id="S4.T2.1.14.13.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">60.97</td>
</tr>
<tr id="S4.T2.1.15.14" class="ltx_tr">
<th id="S4.T2.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;">MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</th>
<th id="S4.T2.1.15.14.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<td id="S4.T2.1.15.14.3" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.15.14.4" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.15.14.5" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
<td id="S4.T2.1.15.14.6" class="ltx_td" style="padding-left:3.2pt;padding-right:3.2pt;"></td>
</tr>
<tr id="S4.T2.1.16.15" class="ltx_tr">
<th id="S4.T2.1.16.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"></th>
<th id="S4.T2.1.16.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">C-VQA test</th>
<td id="S4.T2.1.16.15.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">71.33</td>
<td id="S4.T2.1.16.15.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">24.90</td>
<td id="S4.T2.1.16.15.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">47.84</td>
<td id="S4.T2.1.16.15.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;">54.15</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracies of existing VQA models on the VQA v1.0 val split when trained on VQA v1.0 train split and those on C-VQA test split when trained on C-VQA train split.</figcaption>
</figure>
<div id="S4.p7" class="ltx_para ltx_noindent">
<p id="S4.p7.1" class="ltx_p">From Table <a href="#S4.T2" title="Table 2 ‣ 4 Baselines ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can see that the performance of all the existing VQA models drops significantly in the C-VQA setting compared to the VQA v1.0 setting. Note that even though the Neural Module Networks architecture is compositional by design, their performance suffers on C-VQA. We posit this may be because they use an additional LSTM encoding of the question to encode priors in the dataset. In C-VQA, the priors learned from the train set are unlikely to generalize to the test set. Also note that other models suffer a larger drop in performance compared to Neural Module Networks.</p>
</div>
<div id="S4.p8" class="ltx_para ltx_noindent">
<p id="S4.p8.1" class="ltx_p">Another interesting observation from Table <a href="#S4.T2" title="Table 2 ‣ 4 Baselines ‣ C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is that the ranking of the models based on overall performance changes from VQA v1.0 to C-VQA. For VQA v1.0, SAN outperforms deeper LSTM Q + norm I and NMN, whereas for C-VQA, these two models outperform SAN. Also note the change in ranking of the models for different types of answers (“yes/no”, “number”, “other”). For instance, for “number” questions, MCB outperforms all the models except HieCoAtt for VQA v1.0. However, for C-VQA, all the models except SAN outperform MCB.</p>
</div>
<div id="S4.p9" class="ltx_para ltx_noindent">
<p id="S4.p9.1" class="ltx_p">Examining the accuracies of these models for different question types shows that the performance drop from VQA v1.0 to C-VQA is larger for some question types than the others. For Neural Module Networks (NMN), Stacked Attention Networks (SAN) and Hierarchical Question-Image Co-attention Networks (HieCoAtt), questions starting with “what room is” (such as “What room is this?”) have the largest drop – 33.28% drop for NMN, 40.73% drop for SAN and 32.56% drop for HieCoAtt. For such questions in the C-VQA test split, one of the correct answers is “living room” which is not one of the correct answers to such questions in the C-VQA train split (the correct answers in the C-VQA train split are “kitchen”, “bedroom”, etc.). So, models tend to answer the C-VQA test questions with what they have seen during training (such as “kitchen”). Note that “living room” is seen during training for questions such as “Which room is this?”. For deeper LSTM + norm I model and Multimodal Compact Bilinear Pooling (MCB) model, the largest drop is for “is it” questions (such as “Is it daytime?”) – 29.52% drop for deeper LSTM Q + norm I and 30.77% drop for MCB model. For such questions in the C-VQA test split, the correct answer is “yes” whereas the correct answer for such questions in C-VQA train split is “no”. Again, models tend to answer the C-VQA test questions with “no”. Other question types resulting in significant drop in performance (more than 10%) for all the models are – “what is the color of the”, “how many people are in”, “are there”, “is this a”.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In conclusion, we introduce a novel setting for Visual Question Answering – Compositional Visual Question Answering. Under this setting, the question-answer pairs in the test set are compositionally novel compared to the question-answer pairs in the training set. We create a compositional split of the VQA (v1.0) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, called C-VQA, which facilitates training compositional VQA models. We show the similarities and differences between the VQA v1.0 and C-VQA splits. Finally, we report performances of several existing VQA models on the C-VQA splits and show that the performance of all the models drops significantly compared to the original VQA v1.0 setting. This suggests that today’s VQA models do not handle compositionality well and that C-VQA splits can be used as a benchmark for building and evaluating compositional VQA models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Analyzing the behavior of visual question answering models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Yin and Yang: Balancing and answering binary visual questions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.00837</span>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.06890</span>, 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav,
José M.F. Moura, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Visual Dialog.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Abhishek Das, Satwik Kottur, José M.F. Moura, Stefan Lee, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Learning cooperative visual dialog agents with deep reinforcement
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1703.06585</span>, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo
Larochelle, and Aaron Courville.

</span>
<span class="ltx_bibblock">Guesswhat?! visual object discovery through multi-modal dialogue.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.08481</span>, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao,
Georgios P. Spithourakis, and Lucy Vanderwende.

</span>
<span class="ltx_bibblock">Image-grounded conversations: Multimodal context for natural question
and response generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1701.08251, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.07332</span>, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4995–5004, 2016.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes.

</span>
<span class="ltx_bibblock">Visual turing test for computer vision systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the National Academy of Sciences</span>,
112(12):3618–3623, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
1682–1690, 2014.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu.

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual
image question.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
2296–2304, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
2953–2961, 2015.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia.

</span>
<span class="ltx_bibblock">ABC-CNN: an attention based convolutional neural network for visual
question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1511.05960, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alexander J. Smola.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Huijuan Xu and Kate Saenko.

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Aiwen Jiang, Fang Wang, Fatih Porikli, and Yi Li.

</span>
<span class="ltx_bibblock">Compositional memory for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1511.05676, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.

</span>
<span class="ltx_bibblock">Deep compositional question answering with neural module networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony R. Dick.

</span>
<span class="ltx_bibblock">Explicit knowledge-based reasoning for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1511.02570, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan.

</span>
<span class="ltx_bibblock">Answer-type prediction for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.

</span>
<span class="ltx_bibblock">Learning to compose neural networks for question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">NAACL</span>, 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kevin J. Shih, Saurabh Singh, and Derek Hoiem.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
Ha, and Byoung-Tak Zhang.

</span>
<span class="ltx_bibblock">Multimodal residual learning for visual QA.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2016.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Hyeonwoo Noh and Bohyung Han.

</span>
<span class="ltx_bibblock">Training recurrent answering units with joint loss minimization for
vqa.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1606.03647, 2016.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ilija Ilievski, Shuicheng Yan, and Jiashi Feng.

</span>
<span class="ltx_bibblock">A focused dynamic attention model for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1604.01485, 2016.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Qi Wu, Peng Wang, Chunhua Shen, Anton van den Hengel, and Anthony R. Dick.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Caiming Xiong, Stephen Merity, and Richard Socher.

</span>
<span class="ltx_bibblock">Dynamic memory networks for visual and textual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2016.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Xiang Sean Zhou and Thomas S. Huang.

</span>
<span class="ltx_bibblock">Relevance feedback in image retrieval: A comprehensive review.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proceedings of ACM Multimedia Systems</span>, 2003.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada.

</span>
<span class="ltx_bibblock">Dualnet: Domain-invariant network for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1606.06108, 2016.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Jiasen Lu, Xiao Lin, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Deeper lstm and normalized cnn visual question answering model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/VT-vision-lab/VQA_LSTM_CNN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/VT-vision-lab/VQA_LSTM_CNN</a>, 2015.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling.

</span>
<span class="ltx_bibblock">Learning to detect unseen object classes by between-class attribute
transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on</span>, pages 951–958. IEEE, 2009.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Dinesh Jayaraman, Fei Sha, and Kristen Grauman.

</span>
<span class="ltx_bibblock">Decorrelating semantic visual attributes by resisting the urge to
share.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 1629–1636, 2014.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson, and Gal Chechik.

</span>
<span class="ltx_bibblock">Learning to generalize to new compositions in image understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1608.07639</span>, 2016.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Damien Teney and Anton van den Hengel.

</span>
<span class="ltx_bibblock">Zero-shot visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.05546</span>, 2016.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Santhosh K Ramakrishnan, Ambar Pal, Gaurav Sharma, and Anurag Mittal.

</span>
<span class="ltx_bibblock">An empirical evaluation of visual question answering for novel
objects.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.02516</span>, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1409.1556, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1704.08242" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1704.08243" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1704.08243">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1704.08243" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1704.08244" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 06:12:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
