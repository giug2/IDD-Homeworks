<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  dIR–Discrete Information Retrieval:
  <br class="ltx_break"/>
  Conversational Search over Unstructured (and Structured) Data
  <br class="ltx_break"/>
  with Large Language Models
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Pablo Rodriguez Bertorello
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Jean Rodmond Junior Laguerre
    <br class="ltx_break"/>
    Computer Science Department
    <br class="ltx_break"/>
    Stanford University
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id1.id1">
   Data is stored in both structured and unstructured form. Querying both, to power natural language conversations, is a challenge.
  </p>
  <p class="ltx_p" id="id2.id2">
   This paper introduces dIR, Discrete Information Retrieval, providing a unified interface to query both free text and structured knowledge. Specifically, a Large Language Model (LLM) transforms text into expressive representation. After the text is extracted into columnar form, it can then be queried via a text-to-SQL Semantic Parser, with an LLM converting natural language into SQL. Where desired, such conversation may be effected by a multi-step reasoning conversational agent.
  </p>
  <p class="ltx_p" id="id3.id3">
   We validate our approach via a proprietary question/answer data set, concluding that dIR makes a whole new class of queries on free text possible when compared to traditionally fine-tuned dense-embedding-model- based Information Retrieval (IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIR can succeed where no other method stands a chance.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Many data are stored in both structured and unstructured form: product databases, patient records, and customer reviews, to name a few. This paper sets forth a novel approach for conversational agents to interact with such data, particularly taking advantage of free text information.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Products table row with both structured and unstructured data
   </figcaption>
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="453" id="S1.F1.g1" src="/html/2312.13264/assets/img/motivating-tall.png" width="434"/>
  </figure>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    shows a data row to motivate this paper. It describes a product, with information such as title, price, and description. To answer a question like "Do you have a non-black 15-liter backpack under $400?", an agent needs to take into account both the structured column price, and the unstructured free text column description.
   </p>
  </div>
  <section class="ltx_subsection" id="S1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     1.1
    </span>
    Current Approaches
   </h3>
   <div class="ltx_para" id="S1.SS1.p1">
    <p class="ltx_p" id="S1.SS1.p1.1">
     Prior work has focused on question-answering on structured databases, via SQL, which is limited to basic pattern matching on strings. Thus, largely applicable to searches not needing to rely on free text information (
     <cite class="ltx_cite ltx_citemacro_citep">
      Guo and Gao,
      <a class="ltx_ref" href="#bib.bib11" title="">
       2019
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Wang et al.,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2020
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Scholak et al.,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2021
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Zhong et al.,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2017
      </a>
     </cite>
     ). Recent approaches are based on LLMs (
     <cite class="ltx_cite ltx_citemacro_citep">
      Hu et al.,
      <a class="ltx_ref" href="#bib.bib12" title="">
       2022
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      An et al.,
      <a class="ltx_ref" href="#bib.bib1" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Nan et al.,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Poesia et al.,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2022
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Arora et al.,
      <a class="ltx_ref" href="#bib.bib2" title="">
       2023
      </a>
     </cite>
     )
    </p>
   </div>
   <div class="ltx_para" id="S1.SS1.p2">
    <p class="ltx_p" id="S1.SS1.p2.1">
     A separate research direction focuses on information retrieval over free text corpus (
     <cite class="ltx_cite ltx_citemacro_citep">
      Yang et al.,
      <a class="ltx_ref" href="#bib.bib38" title="">
       2019
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Semnani and Pandey,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2020
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Nie et al.,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2019
      </a>
     </cite>
     ). More recently investigation seeks to leverage LLMs (
     <cite class="ltx_cite ltx_citemacro_citep">
      Jiang et al.,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Semnani et al.,
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Gao et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Khattab et al.,
      <a class="ltx_ref" href="#bib.bib14" title="">
       2023
      </a>
     </cite>
     ).
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     1.2
    </span>
    Contributions
   </h3>
   <div class="ltx_para" id="S1.SS2.p1">
    <p class="ltx_p" id="S1.SS2.p1.1">
     First,
     <span class="ltx_text ltx_font_bold" id="S1.SS2.p1.1.1">
      We make it possible for queries to take full advantage of hybrid sources
     </span>
     . That is, generate responses from free text in addition to structured conversational knowledge. At data preparation time, free text sources are used to generate structured columnar representations, via few-shot LLM prompting, as shown in Figure
     <a class="ltx_ref" href="#A1.F4" title="Figure 4 ‣ A.2 LLM Prompt ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     . At query time, natural language questions are converted via few-shot LLM text-to-SQL Semantic Parsing, as shown in Figure
     <a class="ltx_ref" href="#A1.F5" title="Figure 5 ‣ A.2 LLM Prompt ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . Thus, no model fine-tuning is required, and neither is any modification of the SQL language standard.
    </p>
   </div>
   <div class="ltx_para" id="S1.SS2.p2">
    <p class="ltx_p" id="S1.SS2.p2.1">
     Second,
     <span class="ltx_text ltx_font_bold" id="S1.SS2.p2.1.1">
      We validate our dIR approach via a cross-domain proprietary question/answer data set
     </span>
     . This includes exploratory queries, as in Figure
     <a class="ltx_ref" href="#A1.F2" title="Figure 2 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , and complex direct queries, as in Figure
     <a class="ltx_ref" href="#A1.F3" title="Figure 3 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     , as well as multi-hop queries. Given the arbitrary complexity of the queries studied, SQL stores string functions and vector databases can’t but produce both low Query Recall and Query Precision.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Conversational Agents
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     Recent progress in conversational agents leverages LLMs to reason and act over data (
     <cite class="ltx_cite ltx_citemacro_citep">
      Yao et al.,
      <a class="ltx_ref" href="#bib.bib39" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Lei et al.,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Kumar et al.,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Lee et al.,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
     </cite>
     ). Such reasoner modules are capable of generating a final answer after multiple data consideration steps.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     A prior line of work utilized hybrid data selectors. However, they focused on single-turn question-answering, see Section
     <a class="ltx_ref" href="#S2.SS3" title="2.3 Hybrid Question Answering ‣ 2 Related Work ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       2.3
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Information Retrieval
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     The Information Retrieval (IR) field has seen the emergence of many neural ranking models that operate on dense embeddings (
     <cite class="ltx_cite ltx_citemacro_citep">
      Khattab and Zaharia,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2020
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Guo et al.,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2017
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Xiong et al.,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2017
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Mitra and Craswell,
      <a class="ltx_ref" href="#bib.bib23" title="">
       2019
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Guo et al.,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2019
      </a>
     </cite>
     ). Previously, ranking models relied on hand-crafted features. Still today, search matching employs embedding-based representations of queries and documents.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     Recent progress in Natural Language Understanding highlights the importance of pre-training language representation models in an unsupervised fashion, before subsequently fine-tuning them on downstream tasks such as ranking. (
     <cite class="ltx_cite ltx_citemacro_citep">
      Devlin et al.,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2018
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      MacAvaney et al.,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2019
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Nogueira and Cho,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2019
      </a>
     </cite>
     ).
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Hybrid Question Answering
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     Hybrid data querying often follows the retriever-reasoner paradigm, where retriever modules are trained to identify relevant subsets of structured or unstructured data.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS3.p2">
    <p class="ltx_p" id="S2.SS3.p2.1">
     <cite class="ltx_cite ltx_citemacro_citep">
      Liu et al.,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023
      </a>
     </cite>
     proposes a formal augmentation of the SQL language standard with free-text primitives. This augmentation allows the query language compiler to generate temporary tables, where columnar fields are extracted from text. Then LLM-based Text-to-SQL semantic parsing is utilized to answer queries.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS3.p3">
    <p class="ltx_p" id="S2.SS3.p3.1">
     <cite class="ltx_cite ltx_citemacro_citep">
      Chen et al.,
      <a class="ltx_ref" href="#bib.bib4" title="">
       2020
      </a>
     </cite>
     focuses on single-turn question-answering, with the Hybrid QA data set for evaluation, with only 16 rows per table on average.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.4
    </span>
    Text-to-SQL Semantic Parsing
   </h3>
   <div class="ltx_para" id="S2.SS4.p1">
    <p class="ltx_p" id="S2.SS4.p1.1">
     Semantic parsing aims to translate natural language utterances into executable programs, an active research (
     <cite class="ltx_cite ltx_citemacro_citep">
      Zettlemoyer and Collins,
      <a class="ltx_ref" href="#bib.bib42" title="">
       2012
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Kwiatkowski et al.,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2011
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Pasupat and Liang,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2015
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Wang et al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2015
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Xu et al.,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2020
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Yu et al.,
      <a class="ltx_ref" href="#bib.bib41" title="">
       2019b
      </a>
     </cite>
     ).
    </p>
   </div>
   <div class="ltx_para" id="S2.SS4.p2">
    <p class="ltx_p" id="S2.SS4.p2.1">
     Relational database SQL has been widely adopted as the target executable by previous works due to its popularity. Text-to-SQL may be used for single-turn question-answering (
     <cite class="ltx_cite ltx_citemacro_citep">
      Chen et al.,
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Guo et al.,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Wang et al.,
      <a class="ltx_ref" href="#bib.bib33" title="">
       2019
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Scholak et al.,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2021
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Zhong et al.,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2017
      </a>
     </cite>
     ) or multi-turn conversations (
     <cite class="ltx_cite ltx_citemacro_citep">
      Yu et al.,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2019a
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Wang et al.,
      <a class="ltx_ref" href="#bib.bib33" title="">
       2019
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Liu et al.,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2022
      </a>
     </cite>
     ).
    </p>
   </div>
   <div class="ltx_para" id="S2.SS4.p3">
    <p class="ltx_p" id="S2.SS4.p3.1">
     Recently, LLMs have become capable text-to-SQL semantic parsers via in-context learning (
     <cite class="ltx_cite ltx_citemacro_citep">
      Brown et al.,
      <a class="ltx_ref" href="#bib.bib3" title="">
       2020
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Guo et al.,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
     </cite>
     ), following a variety of prompts (
     <cite class="ltx_cite ltx_citemacro_citep">
      Hu et al.,
      <a class="ltx_ref" href="#bib.bib12" title="">
       2022
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Poesia et al.,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2022
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      An et al.,
      <a class="ltx_ref" href="#bib.bib1" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Nan et al.,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Arora et al.,
      <a class="ltx_ref" href="#bib.bib2" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Guo et al.,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Sun et al.,
      <a class="ltx_ref" href="#bib.bib32" title="">
       2023
      </a>
     </cite>
     ;
     <cite class="ltx_cite ltx_citemacro_citep">
      Zhang et al.,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
     </cite>
     ).
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Core Ideas
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Underlying traditional Information Retrieval per Section
    <a class="ltx_ref" href="#S2.SS2" title="2.2 Information Retrieval ‣ 2 Related Work ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      2.2
     </span>
    </a>
    , often the Approximate Nearest Neighbors algorithm is utilized. They arrive at a dense embedding representation text in aggregate, for the entire query or document chunk. Thus, it fails to take advantage of implicit regularities in the text, where they exist.
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    These regularities often exist in unstructured text. Consider product databases as exemplified in Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , patient records, and restaurants, to name a few. For a backpack product table, the description text field likely indicates the backpack size or handle type, among several other facts. For a patient record, a description field likely utilizes anatomy terminology. For a restaurant, there is some likelihood that the customer review field likely indicates whether it is pet-friendly.
   </p>
  </div>
  <div class="ltx_para" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    Hybrid question-answering approaches per Section
    <a class="ltx_ref" href="#S2.SS3" title="2.3 Hybrid Question Answering ‣ 2 Related Work ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      2.3
     </span>
    </a>
    , often powered by Information Retrieval, inherit IR’s weaknesses. Not only do they require domain fine-tuning embedding models. They also result in low complex query recall and low complex query precision. While IR may be able to tell apart a backpack in a table of watches, it is most likely unable to rank the correct backpack rows for a search requiring that the backpack’s size and strap type be considered, as that may be a small part of the text description field.
   </p>
  </div>
  <div class="ltx_para" id="S3.p4">
   <p class="ltx_p" id="S3.p4.1">
    This paper sets forth a novel approach for conversational agents to interact with unstructured in addition to structured data:
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Taking advantage of free text information regularity
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <ul class="ltx_itemize" id="S3.I1">
     <li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i1.p1">
       <p class="ltx_p" id="S3.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">
         Context:
        </span>
        consider a table like the one in Figure
        <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
         <span class="ltx_text ltx_ref_tag">
          1
         </span>
        </a>
        , containing a primary key, in this case product_id
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i2.p1">
       <p class="ltx_p" id="S3.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">
         Inference:
        </span>
        let a new table, with the same primary key
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i3.p1">
       <p class="ltx_p" id="S3.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">
         Collect:
        </span>
        identify the text fields from the Context that should be utilized in the Discretize step
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i4.p1">
       <p class="ltx_p" id="S3.I1.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">
         Discretize:
        </span>
        for the text from the Collect step, run LLM inference, per the prompt in Figure
        <a class="ltx_ref" href="#A1.F4" title="Figure 4 ‣ A.2 LLM Prompt ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
         <span class="ltx_text ltx_ref_tag">
          4
         </span>
        </a>
        . The result is a list of key-value tuples, where the key is a categorization, and the value is the enumerated value for that category. For example (’product_size’, ’15 liter’), (’handle_type’, ’strap’)
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i5.p1">
       <p class="ltx_p" id="S3.I1.i5.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">
         Enumerate:
        </span>
        reduce the categorization from the Discretize step into enumerated types like { ’product_size’: [’15 liter’, ’22 liter’] }
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i6.p1">
       <p class="ltx_p" id="S3.I1.i6.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i6.p1.1.1">
         Generate table:
        </span>
        for each key from the Enumerate step, generate a column in the Inference table.
       </p>
      </div>
     </li>
    </ul>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     Several of these steps are objectively optimizable, which is outlined as future work in Section
     <a class="ltx_ref" href="#S6" title="6 Future Work ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Providing a unified SQL interface
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Once the process in Section
     <a class="ltx_ref" href="#S3.SS1" title="3.1 Taking advantage of free text information regularity ‣ 3 Core Ideas ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     is completed:
    </p>
    <ul class="ltx_itemize" id="S3.I2">
     <li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I2.i1.p1">
       <p class="ltx_p" id="S3.I2.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">
         (Optional) Execution:
        </span>
        let a ReAct
        <cite class="ltx_cite ltx_citemacro_cite">
         Yao et al. (
         <a class="ltx_ref" href="#bib.bib39" title="">
          2023
         </a>
         )
        </cite>
        reasoner, which in response to a user question executes a cycle of:
       </p>
      </div>
      <div class="ltx_para" id="S3.I2.i1.p2">
       <p class="ltx_p" id="S3.I2.i1.p2.1">
        <span class="ltx_text ltx_font_bold" id="S3.I2.i1.p2.1.1">
         Thought:
        </span>
        chain-of-thought prompting
       </p>
      </div>
      <div class="ltx_para" id="S3.I2.i1.p3">
       <p class="ltx_p" id="S3.I2.i1.p3.1">
        <span class="ltx_text ltx_font_bold" id="S3.I2.i1.p3.1.1">
         Action:
        </span>
        action plan generation, for instance converting the user’s intention into a tool call and query arguments, to fulfill the following
       </p>
      </div>
      <div class="ltx_para" id="S3.I2.i1.p4">
       <p class="ltx_p" id="S3.I2.i1.p4.1">
        <span class="ltx_text ltx_font_bold" id="S3.I2.i1.p4.1.1">
         Observation:
        </span>
        returning the tool’s hybrid structured/unstructured query results for further thought/action to conclusion by the agent
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I2.i2.p1">
       <p class="ltx_p" id="S3.I2.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">
         Text-to-SQL:
        </span>
        a user or tool’s query is converted to SQL, which is run against the JOIN of the Context table and Inference tables created above
       </p>
      </div>
     </li>
    </ul>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     As the user query is converted into SQL queries over a table that includes pre-existing structure and fields extracted from text, it returns maximum insight from the data.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experimental Results
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    The evaluation was conducted on a proprietary data set consisting of 33 different sub-domains, totaling 11,967 products (over 362 products on average for each). That is over 20 times more rows per table than in the HybridQA dataset.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    The main types of evaluations were:
   </p>
   <ul class="ltx_itemize" id="S4.I1">
    <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S4.I1.i1.p1">
      <p class="ltx_p" id="S4.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">
        Direct Query:
       </span>
       where the natural language question expresses very specific user interest, as in Figure
       <a class="ltx_ref" href="#A1.F3" title="Figure 3 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
        <span class="ltx_text ltx_ref_tag">
         3
        </span>
       </a>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S4.I1.i2.p1">
      <p class="ltx_p" id="S4.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">
        Exploratory Query:
       </span>
       where the natural language question expresses a vague user interest, as in Figure
       <a class="ltx_ref" href="#A1.F2" title="Figure 2 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
        <span class="ltx_text ltx_ref_tag">
         2
        </span>
       </a>
       . The level of complexity in queries simply escapes what is possible with unstructured Information Retrieval or structured SQL on text fields
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S4.I1.i3.p1">
      <p class="ltx_p" id="S4.I1.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">
        Multi-hop Query:
       </span>
       where the natural language question requires reasoning over multiple hybrid query responses, which will be published in a separate paper
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    We estimated the relative performance of different LLMs:
   </p>
   <ul class="ltx_itemize" id="S4.I2">
    <li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S4.I2.i1.p1">
      <p class="ltx_p" id="S4.I2.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">
        GPT 3.5:
       </span>
       which was sufficiently effective for the Discretize step
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S4.I2.i2.p1">
      <p class="ltx_p" id="S4.I2.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">
        GPT 4.0:
       </span>
       which we found most effective for the text-to-SQL step, while being unnecessarily expensive for the Discretize step
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S4.I2.i3.p1">
      <p class="ltx_p" id="S4.I2.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">
        Palm2:
       </span>
       which was neither the best at Discretize nor at text-to-SQL
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S4.p4">
   <p class="ltx_p" id="S4.p4.1">
    Per our analysis, one of the critical trade-offs was whether to put all 33 sub-domains of product data into a single Inference database table. Since we sought to answer the most complex natural language questions possible that needed to be resolved via fields extracted from free text, we opted to put each sub-domain in its separate table.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Limitations
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    The Discretize step could generate thousands of columns, even for a data sub-domain, which is to be optimized:
   </p>
   <ul class="ltx_itemize" id="S5.I1">
    <li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I1.i1.p1">
      <p class="ltx_p" id="S5.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">
        SQL system limitations:
       </span>
       databases support a maximum number of columns per table. At present that ranges from 2048 columns for SQL Lite, to 4096 columns for MySQL, to 20480 for Spark
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I1.i2.p1">
      <p class="ltx_p" id="S5.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">
        LLM inference limitations:
       </span>
       the greater the number of column-value pairs identified in Enumerate, the larger the input context that needs to be passed at text-to-SQL inference time. At present, input character limits range from 4096 tokens for GPT 3.5, to 32k tokens for GPT 4.0, to 128k tokens for GPT 4 Turbo. Even if enumerations could fit in the context for a cross-domain query, LLM inference cost is on a per token basis and thus should be minimized
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    In our experiments, we capped the number of fields extracted by keeping column name complexity to a minimum. For example, product_brand was kept, while number_of_pockets was discarded. This was sufficient for enumeration to fit within the column limit of SQL Lite and the token limit of GPT 4.0.
   </p>
  </div>
  <div class="ltx_para" id="S5.p3">
   <p class="ltx_p" id="S5.p3.1">
    Importantly, we noted the importance of cross-domain grounding in generated SQL, which we accomplished by ensuring that some enumerated fields were extracted in every sub-domain, for example: product_type
   </p>
  </div>
  <div class="ltx_para" id="S5.p4">
   <p class="ltx_p" id="S5.p4.1">
    At scale, several of these aspects may be rigorously optimized, which we leave for future work per Section
    <a class="ltx_ref" href="#S6" title="6 Future Work ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      6
     </span>
    </a>
    .
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Future Work
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    First, housing each sub-domain of data in separate tables creates the complexity that a conversational agent then needs to be able to take a query Action that is specific to a particular table. The alternative would be too expensive, to run separate LLM inferences in each domain. A possible path forward would be to rely on a separate model to direct the conversational agent’s query to the best data source table. This may be where ReAct should be combined with Reinforcement Learning to further unlock the conversational potential of Large Language Models.
   </p>
  </div>
  <div class="ltx_para" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    Second, the following algorithmic steps may be objectively optimizable:
   </p>
   <ul class="ltx_itemize" id="S6.I1">
    <li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S6.I1.i1.p1">
      <p class="ltx_p" id="S6.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">
        Grounding:
       </span>
       when natural language text is converted to SQL, the few-shot prompt provided to the LLM includes Enumerate information, per Figure
       <a class="ltx_ref" href="#A1.F5" title="Figure 5 ‣ A.2 LLM Prompt ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
        <span class="ltx_text ltx_ref_tag">
         5
        </span>
       </a>
       . However, a question about backpacks directed to a table about perfumes could result in undesired results, unless cross-table/domain grounding is provided
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S6.I1.i2.p1">
      <p class="ltx_p" id="S6.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">
        Discretize:
       </span>
       it may be possible to optimize the few-shot prompt provided to the LLM, such that the extracted database fields provide the best coverage of all data sub-domains, as opposed to just being good for each one row individually
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S6.I1.i3.p1">
      <p class="ltx_p" id="S6.I1.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">
        Enumerate:
       </span>
       leveraging LLMs, it may be possible to best linguistically reduce different enumerated keys. For example, these two fields could be consolidated into one: no_of_pockets, and number_of_pockets
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S6.I1.i4.p1">
      <p class="ltx_p" id="S6.I1.i4.p1.1">
       <span class="ltx_text ltx_font_bold" id="S6.I1.i4.p1.1.1">
        Domains:
       </span>
       whether different data domains could or should be stored in shared tables, as a function of the depth of querying desired, while constrained by the maximum number of columns per table that is supported by the database system of choice
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S6.p3">
   <p class="ltx_p" id="S6.p3.1">
    Finally, we demonstrated that query semantic parsing may be utilized to establish a Dialog State. Further research should be conducted on a conversational agent utilizing this state information to best understand a user’s intentions as it evolves over multi-step dialog. See Figure
    <a class="ltx_ref" href="#A1.F2" title="Figure 2 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    and Figure
    <a class="ltx_ref" href="#A1.F3" title="Figure 3 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    .
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    We introduce dIR, Discrete Information Retrieval, a method and process providing a unified interface to query both free text and structured knowledge. It leverages LLMs to transform free text into an expressive structured representation, which can be queried in natural language, via a text-to-SQL Semantic Parser, with an LLM converting natural language into SQL. Where desired, such conversation may be effected by a multi-step reasoning conversational agent. Our validation with a proprietary cross-domain dataset reveals that dIR makes a whole new class of arbitrarily complex queries possible on information that may originally be stored as free text. While several aspects should be optimized in the future, it is performant at this onset, without requiring LLM fine-tuning nor a modification of the SQL language standard.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     An et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Weizhu Chen, and Jian-Guang Lou. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14210" target="_blank" title="">
      Skill-based few-shot selection for in-context learning
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Arora et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Aseem Arora, Shabbirhussain Bhaisaheb, Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.02582" target="_blank" title="">
      Adapt and decompose: Efficient generalization of text-to-sql via domain adapted least-to-most prompting
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.14165" target="_blank" title="">
      Language models are few-shot learners
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      CoRR
     </em>
     , abs/2005.14165.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.findings-emnlp.91" target="_blank" title="">
      HybridQA: A dataset of multi-hop question answering over tabular and textual data
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2020
     </em>
     , pages 1026–1036, Online. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ziru Chen, Shijie Chen, Michael White, Raymond Mooney, Ali Payani, Jayanth Srinivasa, Yu Su, and Huan Sun. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.13073" target="_blank" title="">
      Text-to-sql error correction with language models of code
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Devlin et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1810.04805" target="_blank" title="">
      BERT: pre-training of deep bidirectional transformers for language understanding
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      CoRR
     </em>
     , abs/1810.04805.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14627" target="_blank" title="">
      Enabling large language models to generate text with citations
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chunxi Guo, Zhiliang Tian, Jintao Tang, Pancheng Wang, Zhihua Wen, Kang Yang, and Ting Wang. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.13301" target="_blank" title="">
      Prompting gpt-3.5 for text-to-sql with de-semanticization and skeleton retrieval
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2017.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1711.08611" target="_blank" title="">
      A deep relevance matching model for ad-hoc retrieval
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      CoRR
     </em>
     , abs/1711.08611.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W. Bruce Croft, and Xueqi Cheng. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1903.06902" target="_blank" title="">
      A deep look into neural ranking models for information retrieval
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      CoRR
     </em>
     , abs/1903.06902.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo and Gao (2019)
    </span>
    <span class="ltx_bibblock">
     Tong Guo and Huilin Gao. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1910.07179" target="_blank" title="">
      Content enhanced bert-based text-to-sql generation
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      CoRR
     </em>
     , abs/1910.07179.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2203.08568" target="_blank" title="">
      In-context learning for few-shot dialogue state tracking
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.06983" target="_blank" title="">
      Active retrieval augmented generation
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Khattab et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.14024" target="_blank" title="">
      Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Khattab and Zaharia (2020)
    </span>
    <span class="ltx_bibblock">
     Omar Khattab and Matei Zaharia. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2004.12832" target="_blank" title="">
      Colbert: Efficient and effective passage search via contextualized late interaction over BERT
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      CoRR
     </em>
     , abs/2004.12832.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kumar et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Vishwajeet Kumar, Yash Gupta, Saneem Chemmengath, Jaydeep Sen, Soumen Chakrabarti, Samarth Bharadwaj, and Feifei Pan. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.449" target="_blank" title="">
      Multi-row, multi-span distant supervision for Table+Text question answering
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
     </em>
     , pages 8080–8094, Toronto, Canada. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kwiatkowski et al. (2011)
    </span>
    <span class="ltx_bibblock">
     Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://aclanthology.org/D11-1140" target="_blank" title="">
      Lexical generalization in CCG grammar induction for semantic parsing
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 1512–1523, Edinburgh, Scotland, UK. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lee et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sung-Min Lee, Eunhwan Park, Daeryong Seo, Donghyeon Jeon, Inho Kang, and Seung-Hoon Na. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-eacl.177" target="_blank" title="">
      MAFiD: Moving average equipped fusion-in-decoder for question answering over tabular and textual data
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      Findings of the Association for Computational Linguistics: EACL 2023
     </em>
     , pages 2337–2344, Dubrovnik, Croatia. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lei et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, and Kang Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.11725" target="_blank" title="">
      S
      <sup class="ltx_sup" id="bib.bib19.2.2.1">
       3
      </sup>
      hqa: A three-stage approach for multi-hop text-table hybrid question answering
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Qi Liu, Zihuiwen Ye, Tao Yu, Linfeng Song, and Phil Blunsom. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-emnlp.411" target="_blank" title="">
      Augmenting multi-turn text-to-SQL datasets with self-play
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2022
     </em>
     , pages 5608–5620, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Gui Dávid, and Monica S. Lam. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.09818" target="_blank" title="">
      Suql: Conversational search over structured and unstructured data with large language models
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     MacAvaney et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1904.07094" target="_blank" title="">
      CEDR: contextualized embeddings for document ranking
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      CoRR
     </em>
     , abs/1904.07094.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mitra and Craswell (2019)
    </span>
    <span class="ltx_bibblock">
     Bhaskar Mitra and Nick Craswell. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1903.07666" target="_blank" title="">
      An updated duet model for passage re-ranking
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      CoRR
     </em>
     , abs/1903.07666.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, and Dragomir Radev. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.12586" target="_blank" title="">
      Enhancing few-shot text-to-sql capabilities of large language models: A study on prompt design strategies
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nie et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Yixin Nie, Songhe Wang, and Mohit Bansal. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1258" target="_blank" title="">
      Revealing the importance of semantic retrieval for machine reading at scale
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
     </em>
     , pages 2553–2566, Hong Kong, China. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nogueira and Cho (2019)
    </span>
    <span class="ltx_bibblock">
     Rodrigo Frassetto Nogueira and Kyunghyun Cho. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1901.04085" target="_blank" title="">
      Passage re-ranking with BERT
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      CoRR
     </em>
     , abs/1901.04085.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pasupat and Liang (2015)
    </span>
    <span class="ltx_bibblock">
     Panupong Pasupat and Percy Liang. 2015.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P15-1142" target="_blank" title="">
      Compositional semantic parsing on semi-structured tables
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
     </em>
     , pages 1470–1480, Beijing, China. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Poesia et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2201.11227" target="_blank" title="">
      Synchromesh: Reliable code generation from pre-trained language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      CoRR
     </em>
     , abs/2201.11227.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Scholak et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Torsten Scholak, Raymond Li, Dzmitry Bahdanau, Harm de Vries, and Chris Pal. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.103" target="_blank" title="">
      DuoRAT: Towards simpler text-to-SQL models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
     </em>
     , pages 1313–1321, Online. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Semnani et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.findings-emnlp.157" target="_blank" title="">
      WikiChat: Stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2023
     </em>
     , pages 2387–2413, Singapore. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Semnani and Pandey (2020)
    </span>
    <span class="ltx_bibblock">
     Sina J. Semnani and Manish Pandey. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2009.00914" target="_blank" title="">
      Revisiting the open-domain question answering pipeline
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      CoRR
     </em>
     , abs/2009.00914.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ruoxi Sun, Sercan Ö. Arik, Rajarishi Sinha, Hootan Nakhost, Hanjun Dai, Pengcheng Yin, and Tomas Pfister. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.02883" target="_blank" title="">
      Sqlprompt: In-context text-to-sql with minimal labeled data
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1911.04942" target="_blank" title="">
      RAT-SQL: relation-aware schema encoding and linking for text-to-sql parsers
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      CoRR
     </em>
     , abs/1911.04942.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.677" target="_blank" title="">
      RAT-SQL: Relation-aware schema encoding and linking for text-to-SQL parsers
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
     </em>
     , pages 7567–7578, Online. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2015)
    </span>
    <span class="ltx_bibblock">
     Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P15-1129" target="_blank" title="">
      Building a semantic parser overnight
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
     </em>
     , pages 1332–1342, Beijing, China. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xiong et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2017.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:5771862" target="_blank" title="">
      Convolutional neural networks for so-matching n-grams in ad-hoc search zhuyun dai
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      Proceedings of the eleventh ACM international conference on web search and data mining
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Silei Xu, Sina Semnani, Giovanni Campagna, and Monica Lam. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.31" target="_blank" title="">
      AutoQA: From databases to QA semantic parsers with only synthetic training data
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
     </em>
     , pages 422–434, Online. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-4013" target="_blank" title="">
      End-to-end open-domain question answering with BERTserini
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)
     </em>
     , pages 72–77, Minneapolis, Minnesota. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.03629" target="_blank" title="">
      React: Synergizing reasoning and acting in language models
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al. (2019a)
    </span>
    <span class="ltx_bibblock">
     Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S. Lasecki, and Dragomir R. Radev. 2019a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1909.05378" target="_blank" title="">
      Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      CoRR
     </em>
     , abs/1909.05378.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al. (2019b)
    </span>
    <span class="ltx_bibblock">
     Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev. 2019b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1443" target="_blank" title="">
      SParC: Cross-domain semantic parsing in context
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
     </em>
     , pages 4511–4523, Florence, Italy. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zettlemoyer and Collins (2012)
    </span>
    <span class="ltx_bibblock">
     Luke S. Zettlemoyer and Michael Collins. 2012.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1207.1420" target="_blank" title="">
      Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      CoRR
     </em>
     , abs/1207.1420.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen Deep, and Jignesh M. Patel. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.00815" target="_blank" title="">
      Reactable: Enhancing react for table question answering
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhong et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1709.00103" target="_blank" title="">
      Seq2sql: Generating structured queries from natural language using reinforcement learning
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      CoRR
     </em>
     , abs/1709.00103.
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Appendix
  </h2>
  <section class="ltx_subsection" id="A1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.1
    </span>
    Query Complexity
   </h3>
   <div class="ltx_para" id="A1.SS1.p1">
    <p class="ltx_p" id="A1.SS1.p1.1">
     The key queries studied are Exploratory as in Figure
     <a class="ltx_ref" href="#A1.F2" title="Figure 2 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , and Direct as in Figure
     <a class="ltx_ref" href="#A1.F3" title="Figure 3 ‣ A.1 Query Complexity ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="A1.F2">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 2:
     </span>
     Exploratory Query: Response and Dialog State
    </figcaption>
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="527" id="A1.F2.g1" src="/html/2312.13264/assets/img/query-exploratory.png" width="424"/>
   </figure>
   <figure class="ltx_figure" id="A1.F3">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Direct Query: Response and Dialog State
    </figcaption>
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="532" id="A1.F3.g1" src="/html/2312.13264/assets/img/query-direct.png" width="418"/>
   </figure>
  </section>
  <section class="ltx_subsection" id="A1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.2
    </span>
    LLM Prompt
   </h3>
   <div class="ltx_para" id="A1.SS2.p1">
    <p class="ltx_p" id="A1.SS2.p1.1">
     The fundamental LLM prompts developed are Generate as in Figure
     <a class="ltx_ref" href="#A1.F4" title="Figure 4 ‣ A.2 LLM Prompt ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , and Text-toSQL as in Figure
     <a class="ltx_ref" href="#A1.F5" title="Figure 5 ‣ A.2 LLM Prompt ‣ Appendix A Appendix ‣ dIR–Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="A1.F4">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     LLM Prompt to Generate Structured Columnar Representations from Free Text
    </figcaption>
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="519" id="A1.F4.g1" src="/html/2312.13264/assets/img/fewshot-classify.png" width="396"/>
   </figure>
   <figure class="ltx_figure" id="A1.F5">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     LLM Prompt for Text-to-SQL Semantic Parsing
    </figcaption>
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="420" id="A1.F5.g1" src="/html/2312.13264/assets/img/fewshot-semantic-parsing.png" width="392"/>
   </figure>
  </section>
 </section>
</article>
