<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.01596] Maze Discovery using Multiple Robots via Federated Learning</title><meta property="og:description" content="This work presents a use case of federated learning (FL) applied to discovering a maze with LiDAR sensors-equipped robots. Goal here is to train classification models to accurately identify the shapes of grid areas wit‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Maze Discovery using Multiple Robots via Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Maze Discovery using Multiple Robots via Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.01596">

<!--Generated on Mon Aug  5 18:23:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  Maze Discovery,  LiDAR
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Maze Discovery using Multiple Robots via Federated Learning
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Kalpana¬†Ranasinghe1,
H.P.¬†Madushanka1,¬†
Rafaela¬†Scaciota12,¬†
<br class="ltx_break">Sumudu¬†Samarakoon12,¬†, and
Mehdi Bennis1
</span><span class="ltx_author_notes">
This work was supported by NSF-AKA CRUISE (GA 24304406), VERGE (GA 101096034), 6G-INTENSE (GA 101139266), and Infotech-R2D2. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text" style="font-size:90%;">1Centre for Wireless Communication, University of Oulu, Finland 
<br class="ltx_break">2Infortech Oulu, University of Oulu, Finland 
<br class="ltx_break">email: {kalpana.ranasinghe, madushanka.hewapathiranage, rafaela.scaciotatimoesdasilva, sumudu.samarakoon, mehdi.bennis}@oulu.fi
</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">This work presents a use case of <span title="" class="ltx_glossaryref">federated learning (FL)</span> applied to discovering a maze with LiDAR sensors-equipped robots. Goal here is to train classification models to accurately identify the shapes of grid areas within two different square mazes made up with irregular shaped walls.
Due to the use of different shapes for the walls, a classification model trained in one maze that captures its structure does not generalize for the other.
This issue is resolved by adopting <span title="" class="ltx_glossaryref">FL</span> framework between the robots that explore only one maze so that the collective knowledge allows them to operate accurately in the unseen maze.
This illustrates the effectiveness of <span title="" class="ltx_glossaryref">FL</span> in real-world applications in terms of enhancing classification accuracy and robustness in maze discovery tasks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, Maze Discovery, LiDAR

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Robotic navigation in complex environments requires accurate perception and understanding of the surroundings.
However, when multiple robots operate in different environments independently, their ability to generalize for unseen environments can be limited due to the lack of shared information based on constraints such as communication, storage, privacy, etc.
<span title="" class="ltx_glossaryref">Federated learning (FL)</span> has emerged as a promising approach to address this challenge by enabling learning agents to collaboratively train machine learning models without sharing raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
In this work, we demonstrate the effectiveness of <span title="" class="ltx_glossaryref">FL</span> in the context of maze discovery and navigation using two autonomous robots in two maze environments with distinct structural builds.
As oppose to individual learning that fails robots to identify the features of the unseen maze, the <span title="" class="ltx_glossaryref">FL</span>-based approach allows robots to train generalized models that can be accurately used to identify the features in both seen and unseen mazes.
In this view, our results highlight the importance of <span title="" class="ltx_glossaryref">FL</span> in real-world applications in terms of improving the collective knowledge and performance of multiple robots in diverse environments without exchanging the raw data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Features of Square Grid Mazes</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">A maze consists of smaller grids, each representing a specific shape within the overall structure.
In a square-type maze, there are 15 predefined shapes, including open areas as shown in Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ II Features of Square Grid Mazes ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2407.01596/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Block types present in a maze with respect to the orientation of the observer and their corresponding label.
</figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">By accurately classifying the block types observed from the middle of a grid using integrated sensory data, robots can navigate and explore the maze.
To achieve this, a robot needs an accurate classification model.
The nature of the maze, such as the irregularities in wall shapes, can be different from one maze to another.
Therefore, robots need data from various types of environments to train a robust classification model.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">System Architecture &amp; Hardware</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.13" class="ltx_p">The demo setup consists of two <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="4\times 4" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mn id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><times id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">4</cn><cn type="integer" id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">4\times 4</annotation></semantics></math> square grid mazes: Maze <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\alpha</annotation></semantics></math> (<math id="S3.p1.3.m3.1" class="ltx_Math" alttext="M_{\alpha}" display="inline"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">M</mi><mi id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ùëÄ</ci><ci id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">M_{\alpha}</annotation></semantics></math>) and Maze <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\alpha</annotation></semantics></math> (<math id="S3.p1.5.m5.1" class="ltx_Math" alttext="M_{\beta}" display="inline"><semantics id="S3.p1.5.m5.1a"><msub id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">M</mi><mi id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">ùëÄ</ci><ci id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">M_{\beta}</annotation></semantics></math>), each constructed with unique irregular-shaped walls: type <math id="S3.p1.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">\alpha</annotation></semantics></math> and type <math id="S3.p1.7.m7.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.p1.7.m7.1a"><mi id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">\beta</annotation></semantics></math>, two robots namely <math id="S3.p1.8.m8.1" class="ltx_Math" alttext="R_{\alpha}" display="inline"><semantics id="S3.p1.8.m8.1a"><msub id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><mi id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml">R</mi><mi id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1">subscript</csymbol><ci id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2">ùëÖ</ci><ci id="S3.p1.8.m8.1.1.3.cmml" xref="S3.p1.8.m8.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">R_{\alpha}</annotation></semantics></math> and <math id="S3.p1.9.m9.1" class="ltx_Math" alttext="R_{\beta}" display="inline"><semantics id="S3.p1.9.m9.1a"><msub id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml"><mi id="S3.p1.9.m9.1.1.2" xref="S3.p1.9.m9.1.1.2.cmml">R</mi><mi id="S3.p1.9.m9.1.1.3" xref="S3.p1.9.m9.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><apply id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p1.9.m9.1.1.1.cmml" xref="S3.p1.9.m9.1.1">subscript</csymbol><ci id="S3.p1.9.m9.1.1.2.cmml" xref="S3.p1.9.m9.1.1.2">ùëÖ</ci><ci id="S3.p1.9.m9.1.1.3.cmml" xref="S3.p1.9.m9.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">R_{\beta}</annotation></semantics></math> that use LiDAR sensors to perceive the environment, and two slave edge servers referred to as <math id="S3.p1.10.m10.1" class="ltx_Math" alttext="S_{\alpha}" display="inline"><semantics id="S3.p1.10.m10.1a"><msub id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml"><mi id="S3.p1.10.m10.1.1.2" xref="S3.p1.10.m10.1.1.2.cmml">S</mi><mi id="S3.p1.10.m10.1.1.3" xref="S3.p1.10.m10.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><apply id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p1.10.m10.1.1.1.cmml" xref="S3.p1.10.m10.1.1">subscript</csymbol><ci id="S3.p1.10.m10.1.1.2.cmml" xref="S3.p1.10.m10.1.1.2">ùëÜ</ci><ci id="S3.p1.10.m10.1.1.3.cmml" xref="S3.p1.10.m10.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">S_{\alpha}</annotation></semantics></math> and <math id="S3.p1.11.m11.1" class="ltx_Math" alttext="S_{\beta}" display="inline"><semantics id="S3.p1.11.m11.1a"><msub id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml"><mi id="S3.p1.11.m11.1.1.2" xref="S3.p1.11.m11.1.1.2.cmml">S</mi><mi id="S3.p1.11.m11.1.1.3" xref="S3.p1.11.m11.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><apply id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p1.11.m11.1.1.1.cmml" xref="S3.p1.11.m11.1.1">subscript</csymbol><ci id="S3.p1.11.m11.1.1.2.cmml" xref="S3.p1.11.m11.1.1.2">ùëÜ</ci><ci id="S3.p1.11.m11.1.1.3.cmml" xref="S3.p1.11.m11.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">S_{\beta}</annotation></semantics></math> as shown in the Fig.¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ III System Architecture &amp; Hardware ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Wall type <math id="S3.p1.12.m12.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p1.12.m12.1a"><mi id="S3.p1.12.m12.1.1" xref="S3.p1.12.m12.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p1.12.m12.1b"><ci id="S3.p1.12.m12.1.1.cmml" xref="S3.p1.12.m12.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m12.1c">\alpha</annotation></semantics></math> consists of a base with two cylindrical shapes attached to it, while wall type <math id="S3.p1.13.m13.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.p1.13.m13.1a"><mi id="S3.p1.13.m13.1.1" xref="S3.p1.13.m13.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S3.p1.13.m13.1b"><ci id="S3.p1.13.m13.1.1.cmml" xref="S3.p1.13.m13.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.13.m13.1c">\beta</annotation></semantics></math> is characterized by a base with three cylindrical shapes.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.13" class="ltx_p">Operating as <span title="" class="ltx_glossaryref">robot operating system (ROS)</span> masters, <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="R_{\alpha}" display="inline"><semantics id="S3.p2.1.m1.1a"><msub id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">R</mi><mi id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">ùëÖ</ci><ci id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">R_{\alpha}</annotation></semantics></math> and <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="R_{\beta}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">R</mi><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">ùëÖ</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">R_{\beta}</annotation></semantics></math> are assigned the tasks of navigation, data collection, and maze discovery within their respective mazes <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="M_{\alpha}" display="inline"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">M</mi><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">ùëÄ</ci><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">M_{\alpha}</annotation></semantics></math> and <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="M_{\beta}" display="inline"><semantics id="S3.p2.4.m4.1a"><msub id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">M</mi><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">ùëÄ</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">M_{\beta}</annotation></semantics></math>.
Therein, the robots are positioned at the starting point <math id="S3.p2.5.m5.2" class="ltx_Math" alttext="(0,0)" display="inline"><semantics id="S3.p2.5.m5.2a"><mrow id="S3.p2.5.m5.2.3.2" xref="S3.p2.5.m5.2.3.1.cmml"><mo stretchy="false" id="S3.p2.5.m5.2.3.2.1" xref="S3.p2.5.m5.2.3.1.cmml">(</mo><mn id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">0</mn><mo id="S3.p2.5.m5.2.3.2.2" xref="S3.p2.5.m5.2.3.1.cmml">,</mo><mn id="S3.p2.5.m5.2.2" xref="S3.p2.5.m5.2.2.cmml">0</mn><mo stretchy="false" id="S3.p2.5.m5.2.3.2.3" xref="S3.p2.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.2b"><interval closure="open" id="S3.p2.5.m5.2.3.1.cmml" xref="S3.p2.5.m5.2.3.2"><cn type="integer" id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">0</cn><cn type="integer" id="S3.p2.5.m5.2.2.cmml" xref="S3.p2.5.m5.2.2">0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.2c">(0,0)</annotation></semantics></math> in each maze.
As for mobile robots, we use open-source robots based on NVIDIA Jetson Nano known as ‚ÄùJetBot <span title="" class="ltx_glossaryref">ROS</span> <span title="" class="ltx_glossaryref">artificial intelligence (AI)</span> kit‚Äù <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Each robot is equipped with a <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S3.p2.6.m6.1a"><msup id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mn id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">360</mn><mo id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1">superscript</csymbol><cn type="integer" id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">360</cn><compose id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">360^{\circ}</annotation></semantics></math> laser ranging LiDAR that is used to observe the surrounding from the middle of the grid, and a <math id="S3.p2.7.m7.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.p2.7.m7.1a"><mn id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><cn type="integer" id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">8</annotation></semantics></math>‚ÄâMP <math id="S3.p2.8.m8.1" class="ltx_Math" alttext="160^{\circ}" display="inline"><semantics id="S3.p2.8.m8.1a"><msup id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mn id="S3.p2.8.m8.1.1.2" xref="S3.p2.8.m8.1.1.2.cmml">160</mn><mo id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1">superscript</csymbol><cn type="integer" id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1.2">160</cn><compose id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">160^{\circ}</annotation></semantics></math> field of view camera.
To achieve precise navigation between grid centers, robots use a line-following technique.
Utilizing cameras mounted on the robots, they track white lines marked on the floor, ensuring accurate movement along the paths.
Additionally, blue lines are marked in front of each grid center, enabling the robots to stop precisely at the center. The LiDAR model is RPLIDAR A1, which has a scanning frequency of <math id="S3.p2.9.m9.1" class="ltx_Math" alttext="5.5" display="inline"><semantics id="S3.p2.9.m9.1a"><mn id="S3.p2.9.m9.1.1" xref="S3.p2.9.m9.1.1.cmml">5.5</mn><annotation-xml encoding="MathML-Content" id="S3.p2.9.m9.1b"><cn type="float" id="S3.p2.9.m9.1.1.cmml" xref="S3.p2.9.m9.1.1">5.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.9.m9.1c">5.5</annotation></semantics></math>‚ÄâHz, a ranging distance of <math id="S3.p2.10.m10.1" class="ltx_Math" alttext="0.15\sim 12" display="inline"><semantics id="S3.p2.10.m10.1a"><mrow id="S3.p2.10.m10.1.1" xref="S3.p2.10.m10.1.1.cmml"><mn id="S3.p2.10.m10.1.1.2" xref="S3.p2.10.m10.1.1.2.cmml">0.15</mn><mo id="S3.p2.10.m10.1.1.1" xref="S3.p2.10.m10.1.1.1.cmml">‚àº</mo><mn id="S3.p2.10.m10.1.1.3" xref="S3.p2.10.m10.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.10.m10.1b"><apply id="S3.p2.10.m10.1.1.cmml" xref="S3.p2.10.m10.1.1"><csymbol cd="latexml" id="S3.p2.10.m10.1.1.1.cmml" xref="S3.p2.10.m10.1.1.1">similar-to</csymbol><cn type="float" id="S3.p2.10.m10.1.1.2.cmml" xref="S3.p2.10.m10.1.1.2">0.15</cn><cn type="integer" id="S3.p2.10.m10.1.1.3.cmml" xref="S3.p2.10.m10.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.10.m10.1c">0.15\sim 12</annotation></semantics></math>‚Äâm with an accuracy of <math id="S3.p2.11.m11.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S3.p2.11.m11.1a"><mrow id="S3.p2.11.m11.1.1" xref="S3.p2.11.m11.1.1.cmml"><mn id="S3.p2.11.m11.1.1.2" xref="S3.p2.11.m11.1.1.2.cmml">1</mn><mo id="S3.p2.11.m11.1.1.1" xref="S3.p2.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.11.m11.1b"><apply id="S3.p2.11.m11.1.1.cmml" xref="S3.p2.11.m11.1.1"><csymbol cd="latexml" id="S3.p2.11.m11.1.1.1.cmml" xref="S3.p2.11.m11.1.1.1">percent</csymbol><cn type="integer" id="S3.p2.11.m11.1.1.2.cmml" xref="S3.p2.11.m11.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.11.m11.1c">1\%</annotation></semantics></math> for distances less than <math id="S3.p2.12.m12.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.p2.12.m12.1a"><mn id="S3.p2.12.m12.1.1" xref="S3.p2.12.m12.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.p2.12.m12.1b"><cn type="integer" id="S3.p2.12.m12.1.1.cmml" xref="S3.p2.12.m12.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.12.m12.1c">3</annotation></semantics></math>‚Äâm, and <math id="S3.p2.13.m13.1" class="ltx_Math" alttext="1147" display="inline"><semantics id="S3.p2.13.m13.1a"><mn id="S3.p2.13.m13.1.1" xref="S3.p2.13.m13.1.1.cmml">1147</mn><annotation-xml encoding="MathML-Content" id="S3.p2.13.m13.1b"><cn type="integer" id="S3.p2.13.m13.1.1.cmml" xref="S3.p2.13.m13.1.1">1147</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.13.m13.1c">1147</annotation></semantics></math> sample points per one sweep.
To obtain accurate odometry information, the robot utilizes two high-power encoded motors, an IMU sensor, and an odometer.
These sensors work together to provide reliable odometry data, which is then represented in the <span title="" class="ltx_glossaryref">ROS</span> framework.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.4" class="ltx_p">The slave servers <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="S_{\alpha}" display="inline"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">S</mi><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">ùëÜ</ci><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">S_{\alpha}</annotation></semantics></math> and <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="S_{\beta}" display="inline"><semantics id="S3.p3.2.m2.1a"><msub id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">S</mi><mi id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">ùëÜ</ci><ci id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">S_{\beta}</annotation></semantics></math> are used to facilitate visualization and monitoring of the maze discovery process.
A Lenovo Thinkpad with <span title="" class="ltx_glossaryref">ROS</span> is used as <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="S_{\alpha}" display="inline"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">S</mi><mi id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">ùëÜ</ci><ci id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">S_{\alpha}</annotation></semantics></math> and a Jetson Nano with <span title="" class="ltx_glossaryref">ROS</span> is used as <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="S_{\beta}" display="inline"><semantics id="S3.p3.4.m4.1a"><msub id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml"><mi id="S3.p3.4.m4.1.1.2" xref="S3.p3.4.m4.1.1.2.cmml">S</mi><mi id="S3.p3.4.m4.1.1.3" xref="S3.p3.4.m4.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><apply id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p3.4.m4.1.1.1.cmml" xref="S3.p3.4.m4.1.1">subscript</csymbol><ci id="S3.p3.4.m4.1.1.2.cmml" xref="S3.p3.4.m4.1.1.2">ùëÜ</ci><ci id="S3.p3.4.m4.1.1.3.cmml" xref="S3.p3.4.m4.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">S_{\beta}</annotation></semantics></math>.
The slave servers and the robots use <span title="" class="ltx_glossaryref">ROS</span> topics for seamless and efficient communication.
The robots publish their sensor readings and navigation commands, while the slave servers subscribe to these topics to receive the data.
These data includes position and direction of the robot, identified block type, and flag messages for synchronization between <span title="" class="ltx_glossaryref">ROS</span> nodes, enabling the slave servers to visualize the robots‚Äô progress, block identifications, and navigation decisions over a graphical user interface.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2407.01596/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Robotic platform of two mazes.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">System Implementation</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This demo is composed of several components including the navigation, data collection, training of classification models, and inference and maze discovery as discussed next.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Navigation between adjacent grids</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">A vision-based line-following system is implemented on the robots to facilitate successful navigation between the grids using the camera serial interface camera module.
The camera, along with the Jetson Nano, supports image processing at resolutions of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="480\times 640" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">480</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">480</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">480\times 640</annotation></semantics></math> pixels, with a maximum frame rate of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">30</annotation></semantics></math> frames per second.
The navigation process consists of two repeated steps: i) the robot follows the white line, and ii) the robot stops at the center of each grid by detecting the blue line.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.3" class="ltx_p">We choose a <span title="" class="ltx_glossaryref">region-of-interest (RoI)</span> from the bottom portion of the raw camera image to prioritise the detection of nearby points during line following.
The <span title="" class="ltx_glossaryref">RoI</span> is then converted from RGB to HSV color space, due to the effectiveness of filtering out white lines based on the luminosity.
Subsequently, the contour with the largest area is chosen to accurately identify the white line.
The coordinates of the center point of this contour is extracted and used to determine the error, which represents the displacement of the path‚Äôs center from the center of the <span title="" class="ltx_glossaryref">RoI</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
This error value is used as an input for a <span title="" class="ltx_glossaryref">propotional-integral-derivative (PID)</span> controller, ensuring precise control commands and smooth navigation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
The coefficients of the proportional, derivative, and integral terms in the <span title="" class="ltx_glossaryref">PID</span> controller are set to <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">3</annotation></semantics></math>, <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="1.8" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">1.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="float" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">1.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">1.8</annotation></semantics></math>, and <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn type="integer" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">0</cn></annotation-xml></semantics></math>, respectively.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Detection of the blue line is used to stop the robot precisely at the center of each grid.
The blue line detection process is similar to the white line detection, with exceptions of the use of a different <span title="" class="ltx_glossaryref">RoI</span> and the HSV adjustments to filter out the blue lines.
The robot stops once the detected area of the blue line exceeded a predetermined threshold value, thereby achieving precise positioning at the center of the grid.
</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.2" class="ltx_p">Further, we use the odometer frame information from the <span id="S4.SS1.p4.2.1" class="ltx_text ltx_font_typewriter">/odom</span> rostopic to make precise <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="90^{\circ}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><msup id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">90</mn><mo id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">90</cn><compose id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">90^{\circ}</annotation></semantics></math> and <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="180^{\circ}" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><msup id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mn id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">180</mn><mo id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">180</cn><compose id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">180^{\circ}</annotation></semantics></math> rotations while navigating by feeding the error obtained from the odometry information and target angle to a <span title="" class="ltx_glossaryref">PID</span> controller¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Data Collection</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">During the data collection phase, we feed a predefined path and the map information as an array to the robot assigned for the maze.
While following the path, at each grid, the robot performs four <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="90^{\circ}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msup id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">90</mn><mo id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">90</cn><compose id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">90^{\circ}</annotation></semantics></math> rotations and collects <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">200</annotation></semantics></math> LiDAR sweeps at each position by reading the <span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_typewriter">LaserSacn</span> message from <span id="S4.SS2.p1.2.2" class="ltx_text ltx_font_typewriter">/scan</span> rostopic.
These LiDAR readings are then saved into a numpy array with the corresponding label as shown in the Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ II Features of Square Grid Mazes ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
To enhance the diversity of the dataset within a short time frame,
some noises to the movements are manually fed to the robot via a remote controller
while it collects data from a single block type.
These variations aid to generate a comprehensive local dataset capturing a wide range of environmental configurations and scenarios, enhancing the overall robustness of the trained model.
</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Training the classification model</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.5" class="ltx_p">We use two training modes to train the classification model: <em id="S4.SS3.p1.5.1" class="ltx_emph ltx_font_italic">local training</em> and <em id="S4.SS3.p1.5.2" class="ltx_emph ltx_font_italic"><span title="" class="ltx_glossaryref">FL</span></em>.
During the local training mode, both robots train their classification models locally using the datasets obtained from respective mazes with supervised learning.
The classification models are based on a feed-forward neural network architecture, consisting of a <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="1147\times 1" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">1147</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">1147</cn><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1147\times 1</annotation></semantics></math> input reflecting LiDAR reading, one hidden layer with a size of 256 neurons, and a <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="15\times 1" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">15</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">15</cn><cn type="integer" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">15\times 1</annotation></semantics></math> output layer corresponding one-hot encoded label with ReLu activation.
To prevent over-fitting and improve generalization, a learning rate of <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mn id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><cn type="float" id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">0.001</annotation></semantics></math> and the L2 regularization with a weight decay of <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><cn type="float" id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">0.001</annotation></semantics></math> are applied in the training.
After the training process, both models achieve a classification accuracy of <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="99\%" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mn id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">99</mn><mo id="S4.SS3.p1.5.m5.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">99\%</annotation></semantics></math> on their local testing data.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.6" class="ltx_p">During the <span title="" class="ltx_glossaryref">FL</span> mode, <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="R_{\alpha}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">R</mi><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">ùëÖ</ci><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">R_{\alpha}</annotation></semantics></math> and <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="R_{\beta}" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><msub id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">R</mi><mi id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">ùëÖ</ci><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">R_{\beta}</annotation></semantics></math> act as clients, while <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="S_{\beta}" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><msub id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">S</mi><mi id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">ùëÜ</ci><ci id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">S_{\beta}</annotation></semantics></math> acts as the parameter server, and <em id="S4.SS3.p2.6.1" class="ltx_emph ltx_font_italic">FedAvg</em> algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is used for the training.
In the <span title="" class="ltx_glossaryref">FL</span> mode, we used the same model architecture and learning rate as in the local training mode, with a minibatch size of <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn type="integer" id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">16</annotation></semantics></math>, two local iterations prior to model averaging, and <math id="S4.SS3.p2.5.m5.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S4.SS3.p2.5.m5.1a"><mn id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><cn type="integer" id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">15</annotation></semantics></math> global iterations.
Under <span title="" class="ltx_glossaryref">FL</span>, the global model used in both robots achieve a test accuracy of about <math id="S4.SS3.p2.6.m6.1" class="ltx_Math" alttext="99\%" display="inline"><semantics id="S4.SS3.p2.6.m6.1a"><mrow id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml"><mn id="S4.SS3.p2.6.m6.1.1.2" xref="S4.SS3.p2.6.m6.1.1.2.cmml">99</mn><mo id="S4.SS3.p2.6.m6.1.1.1" xref="S4.SS3.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><apply id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS3.p2.6.m6.1.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.6.m6.1.1.2.cmml" xref="S4.SS3.p2.6.m6.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">99\%</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Maze discovery</span>
</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2407.01596/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="396" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Discovering mazes (top) using locally trained models: inference output of the own maze (middle) vs the other maze (bottom). </figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.4" class="ltx_p">The inference of the classification models obtained by local training and <span title="" class="ltx_glossaryref">FL</span> modes is used within the discovery phase as two different scenarios.
Therein, each robot self navigates within the mazes by taking different actions with the objective of discovering the maze layout.
The action space includes four actions correspond to moving forward after a clockwise rotation of <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="0^{\circ}" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><msup id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">0</mn><mo id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">0</cn><compose id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">0^{\circ}</annotation></semantics></math>, <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="90^{\circ}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><msup id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mn id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">90</mn><mo id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">90</cn><compose id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">90^{\circ}</annotation></semantics></math>, <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="-90^{\circ}" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mo id="S4.SS4.p1.3.m3.1.1a" xref="S4.SS4.p1.3.m3.1.1.cmml">‚àí</mo><msup id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml"><mn id="S4.SS4.p1.3.m3.1.1.2.2" xref="S4.SS4.p1.3.m3.1.1.2.2.cmml">90</mn><mo id="S4.SS4.p1.3.m3.1.1.2.3" xref="S4.SS4.p1.3.m3.1.1.2.3.cmml">‚àò</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><minus id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"></minus><apply id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.p1.3.m3.1.1.2.1.cmml" xref="S4.SS4.p1.3.m3.1.1.2">superscript</csymbol><cn type="integer" id="S4.SS4.p1.3.m3.1.1.2.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2.2">90</cn><compose id="S4.SS4.p1.3.m3.1.1.2.3.cmml" xref="S4.SS4.p1.3.m3.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">-90^{\circ}</annotation></semantics></math>, or <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="180^{\circ}" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><msup id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mn id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">180</mn><mo id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">‚àò</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">180</cn><compose id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">180^{\circ}</annotation></semantics></math>.
To ensure efficient maze exploration, a prioritized clockwise turn strategy is employed to avoid unnecessary looping.
Once the maze is discovered, the robot automatically stops at the adjacent grid.
The navigation techniques described in Section <a href="#S4.SS1" title="IV-A Navigation between adjacent grids ‚Ä£ IV System Implementation ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a> are utilized to navigate between grid centers.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.10" class="ltx_p">Fig.¬†<a href="#S4.F3" title="Figure 3 ‚Ä£ IV-D Maze discovery ‚Ä£ IV System Implementation ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the maze discovery using the locally trained models.
It can be noted that <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="R_{\alpha}" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><msub id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mi id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">R</mi><mi id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">ùëÖ</ci><ci id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">R_{\alpha}</annotation></semantics></math> and <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="R_{\beta}" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><msub id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">R</mi><mi id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">ùëÖ</ci><ci id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">R_{\beta}</annotation></semantics></math> can correctly infer their corresponding mazes <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="M_{\alpha}" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><msub id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mi id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">M</mi><mi id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">ùëÄ</ci><ci id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">M_{\alpha}</annotation></semantics></math> and <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="M_{\beta}" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><msub id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mi id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2.cmml">M</mi><mi id="S4.SS4.p2.4.m4.1.1.3" xref="S4.SS4.p2.4.m4.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p2.4.m4.1.1.2.cmml" xref="S4.SS4.p2.4.m4.1.1.2">ùëÄ</ci><ci id="S4.SS4.p2.4.m4.1.1.3.cmml" xref="S4.SS4.p2.4.m4.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">M_{\beta}</annotation></semantics></math>, respectively validating the test accuracy obtained during the training phase.
However, once the robots are swapped, they no longer able to correctly identify the unseen wall types, in which, maze discovery is inaccurate as shown in the bottom row of Fig.¬†<a href="#S4.F3" title="Figure 3 ‚Ä£ IV-D Maze discovery ‚Ä£ IV System Implementation ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
With further experiments, it is observed that <math id="S4.SS4.p2.5.m5.1" class="ltx_Math" alttext="R_{\alpha}" display="inline"><semantics id="S4.SS4.p2.5.m5.1a"><msub id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml"><mi id="S4.SS4.p2.5.m5.1.1.2" xref="S4.SS4.p2.5.m5.1.1.2.cmml">R</mi><mi id="S4.SS4.p2.5.m5.1.1.3" xref="S4.SS4.p2.5.m5.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><apply id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.5.m5.1.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS4.p2.5.m5.1.1.2.cmml" xref="S4.SS4.p2.5.m5.1.1.2">ùëÖ</ci><ci id="S4.SS4.p2.5.m5.1.1.3.cmml" xref="S4.SS4.p2.5.m5.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">R_{\alpha}</annotation></semantics></math> exhibits about <math id="S4.SS4.p2.6.m6.1" class="ltx_Math" alttext="48\%" display="inline"><semantics id="S4.SS4.p2.6.m6.1a"><mrow id="S4.SS4.p2.6.m6.1.1" xref="S4.SS4.p2.6.m6.1.1.cmml"><mn id="S4.SS4.p2.6.m6.1.1.2" xref="S4.SS4.p2.6.m6.1.1.2.cmml">48</mn><mo id="S4.SS4.p2.6.m6.1.1.1" xref="S4.SS4.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.6.m6.1b"><apply id="S4.SS4.p2.6.m6.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS4.p2.6.m6.1.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.p2.6.m6.1.1.2.cmml" xref="S4.SS4.p2.6.m6.1.1.2">48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.6.m6.1c">48\%</annotation></semantics></math> of classification accuracy on the data from <math id="S4.SS4.p2.7.m7.1" class="ltx_Math" alttext="M_{\beta}" display="inline"><semantics id="S4.SS4.p2.7.m7.1a"><msub id="S4.SS4.p2.7.m7.1.1" xref="S4.SS4.p2.7.m7.1.1.cmml"><mi id="S4.SS4.p2.7.m7.1.1.2" xref="S4.SS4.p2.7.m7.1.1.2.cmml">M</mi><mi id="S4.SS4.p2.7.m7.1.1.3" xref="S4.SS4.p2.7.m7.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.7.m7.1b"><apply id="S4.SS4.p2.7.m7.1.1.cmml" xref="S4.SS4.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.7.m7.1.1.1.cmml" xref="S4.SS4.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS4.p2.7.m7.1.1.2.cmml" xref="S4.SS4.p2.7.m7.1.1.2">ùëÄ</ci><ci id="S4.SS4.p2.7.m7.1.1.3.cmml" xref="S4.SS4.p2.7.m7.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.7.m7.1c">M_{\beta}</annotation></semantics></math> while <math id="S4.SS4.p2.8.m8.1" class="ltx_Math" alttext="R_{\beta}" display="inline"><semantics id="S4.SS4.p2.8.m8.1a"><msub id="S4.SS4.p2.8.m8.1.1" xref="S4.SS4.p2.8.m8.1.1.cmml"><mi id="S4.SS4.p2.8.m8.1.1.2" xref="S4.SS4.p2.8.m8.1.1.2.cmml">R</mi><mi id="S4.SS4.p2.8.m8.1.1.3" xref="S4.SS4.p2.8.m8.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.8.m8.1b"><apply id="S4.SS4.p2.8.m8.1.1.cmml" xref="S4.SS4.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.8.m8.1.1.1.cmml" xref="S4.SS4.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS4.p2.8.m8.1.1.2.cmml" xref="S4.SS4.p2.8.m8.1.1.2">ùëÖ</ci><ci id="S4.SS4.p2.8.m8.1.1.3.cmml" xref="S4.SS4.p2.8.m8.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.8.m8.1c">R_{\beta}</annotation></semantics></math> achieves merely <math id="S4.SS4.p2.9.m9.1" class="ltx_Math" alttext="29\%" display="inline"><semantics id="S4.SS4.p2.9.m9.1a"><mrow id="S4.SS4.p2.9.m9.1.1" xref="S4.SS4.p2.9.m9.1.1.cmml"><mn id="S4.SS4.p2.9.m9.1.1.2" xref="S4.SS4.p2.9.m9.1.1.2.cmml">29</mn><mo id="S4.SS4.p2.9.m9.1.1.1" xref="S4.SS4.p2.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.9.m9.1b"><apply id="S4.SS4.p2.9.m9.1.1.cmml" xref="S4.SS4.p2.9.m9.1.1"><csymbol cd="latexml" id="S4.SS4.p2.9.m9.1.1.1.cmml" xref="S4.SS4.p2.9.m9.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.p2.9.m9.1.1.2.cmml" xref="S4.SS4.p2.9.m9.1.1.2">29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.9.m9.1c">29\%</annotation></semantics></math> accuracy on <math id="S4.SS4.p2.10.m10.1" class="ltx_Math" alttext="M_{\alpha}" display="inline"><semantics id="S4.SS4.p2.10.m10.1a"><msub id="S4.SS4.p2.10.m10.1.1" xref="S4.SS4.p2.10.m10.1.1.cmml"><mi id="S4.SS4.p2.10.m10.1.1.2" xref="S4.SS4.p2.10.m10.1.1.2.cmml">M</mi><mi id="S4.SS4.p2.10.m10.1.1.3" xref="S4.SS4.p2.10.m10.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.10.m10.1b"><apply id="S4.SS4.p2.10.m10.1.1.cmml" xref="S4.SS4.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.10.m10.1.1.1.cmml" xref="S4.SS4.p2.10.m10.1.1">subscript</csymbol><ci id="S4.SS4.p2.10.m10.1.1.2.cmml" xref="S4.SS4.p2.10.m10.1.1.2">ùëÄ</ci><ci id="S4.SS4.p2.10.m10.1.1.3.cmml" xref="S4.SS4.p2.10.m10.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.10.m10.1c">M_{\alpha}</annotation></semantics></math>‚Äôs data.
This indicates that the local training does not generalize for unseen environments.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2407.01596/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="396" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Discovering mazes (top) using FL models: inference output of the own maze (middle) vs the other maze (bottom). </figcaption>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.3" class="ltx_p">The inference using the <span title="" class="ltx_glossaryref">FL</span> models are illustrated in Fig. <a href="#S4.F4" title="Figure 4 ‚Ä£ IV-D Maze discovery ‚Ä£ IV System Implementation ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
In contrast to local training alone, with <span title="" class="ltx_glossaryref">FL</span>, the robots are capable of sharing their local knowledge with one another, in which, the models are generalized for unseen wall types.
This is exhibited by the accurate maze discovery of both mazes by the both robots as shown in the inferred outputs (middle and bottom) of Fig. <a href="#S4.F4" title="Figure 4 ‚Ä£ IV-D Maze discovery ‚Ä£ IV System Implementation ‚Ä£ Maze Discovery using Multiple Robots via Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Our experiments show that the <span title="" class="ltx_glossaryref">FL</span>-based model achieves about <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="99\%" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mrow id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mn id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">99</mn><mo id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">99\%</annotation></semantics></math> classification accuracy over the data from both <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="M_{\alpha}" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><msub id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">M</mi><mi id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml">Œ±</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">ùëÄ</ci><ci id="S4.SS4.p3.2.m2.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.3">ùõº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">M_{\alpha}</annotation></semantics></math> and <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="M_{\beta}" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><msub id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mi id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">M</mi><mi id="S4.SS4.p3.3.m3.1.1.3" xref="S4.SS4.p3.3.m3.1.1.3.cmml">Œ≤</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">ùëÄ</ci><ci id="S4.SS4.p3.3.m3.1.1.3.cmml" xref="S4.SS4.p3.3.m3.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">M_{\beta}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Demo, Resources, and Future Developments</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The software related to robots and servers is available at <a target="_blank" href="https://github.com/ICONgroupCWC/MapDiscoveryDemo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ICONgroupCWC/MapDiscoveryDemo</a>.
This demo in action can be seen from <a target="_blank" href="https://youtu.be/K2M8MCLn1po?si=TzIeUhHuSVCVeZRl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://youtu.be/K2M8MCLn1po?si=TzIeUhHuSVCVeZRl</a>
The future developments are focused on exploiting the topological signatures of LiDAR point-cloud data to train models over the symmetries, deformations, and irregularities in the raw observations with improved communication and computation efficiency.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H.¬†B. McMahan <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúCommunication-efficient learning of deep
networks from decentralized data. arxiv,‚Äù <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1602.05629</em>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Waveshare, ‚ÄúJetbot ros ai robot,‚Äù Available at
<a target="_blank" href="https://www.waveshare.com/jetbot-ros-ai-kit.htm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.waveshare.com/jetbot-ros-ai-kit.htm</a> (2023/07/10).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
OpenCV, ‚ÄúContours in opencv,‚Äù Available at
<a target="_blank" href="https://docs.opencv.org/4.0.0/dd/d49/tutorial_py_contour_features.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.opencv.org/4.0.0/dd/d49/tutorial_py_contour_features.html</a>
(2023/07/12).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S.¬†Bennett, ‚ÄúDevelopment of the PID controller,‚Äù <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE
Control Systems Magazine</em>, vol.¬†13, no.¬†6, pp. 58‚Äì62, 1993.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.01595" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.01596" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.01596">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.01596" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.01599" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:23:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
