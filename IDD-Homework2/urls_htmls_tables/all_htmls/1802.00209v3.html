<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1802.00209] DRAU: Dual Recurrent Attention Units for Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) requires AI models to comprehend data in two domains, vision and text. Current state-of-the-art models use learned attention mechanisms to extract relevant information from the input domâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DRAU: Dual Recurrent Attention Units for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DRAU: Dual Recurrent Attention Units for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1802.00209">

<!--Generated on Tue Mar 19 10:53:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">DRAU: Dual Recurrent Attention Units for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ahmed <span id="id11.1.id1" class="ltx_text" style="color:#FF0000;">Osman</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ahmed.osman@hhi.fraunhofer.de">ahmed.osman@hhi.fraunhofer.de</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wojciech <span id="id12.1.id1" class="ltx_text" style="color:#FF0000;">Samek</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wojciech.samek@hhi.fraunhofer.de">wojciech.samek@hhi.fraunhofer.de</a>
</span>
<span class="ltx_contact ltx_role_address">Fraunhofer Heinrich Hertz Institute, Einsteinufer 37, Berlin 10587, Germany
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p"><a href="#id1.1.id1"><span href="#id1.1.id1" title="Visual Question Answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Visual Question Answering</span></span></a> (<a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Visual Question Answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr></a>) requires AI models to comprehend data in two domains, vision and text. Current state-of-the-art models use learned attention mechanisms to extract relevant information from the input domains to answer a certain question. Thus, robust attention mechanisms are essential for powerful VQA models. In this paper, we propose a recurrent attention mechanism and show its benefits compared to the traditional convolutional approach. We perform two ablation studies to evaluate recurrent attention. First, we introduce a baseline <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Visual Question Answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr></a> model with visual attention and test the performance difference between convolutional and recurrent attention on the VQA 2.0 dataset. Secondly, we design an architecture for VQA which utilizes dual (textual and visual) <a href="#id2.2.id2"><span href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Recurrent Attention Units</span></span></a>. Using this model, we show the effect of all possible combinations of recurrent and convolutional dual attention. Our single model outperforms the first place winner on the VQA 2016 challenge and to the best of our knowledge, it is the second best performing single model on the VQA 1.0 dataset. Furthermore, our model noticeably improves upon the winner of the VQA 2017 challenge. Moreover, we experiment replacing attention mechanisms in state-of-the-art models with our <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a> and show increased performance.</p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journal: </span>Computer Vision and Image Understanding</span></span></span>
<section id="id10" class="ltx_glossary ltx_acronym ltx_list_acronym">
<dl id="id10.10" class="ltx_glossarylist">
<dt id="id1.1.id1" class="ltx_glossaryentry">VQA</dt>
<dd>Visual Question Answering</dd>
<dt id="id2.2.id2" class="ltx_glossaryentry">RAU</dt>
<dd>Recurrent Attention Unit</dd>
<dt id="id3.3.id3" class="ltx_glossaryentry">DRAU</dt>
<dd>Dual Recurrent Attention Units</dd>
<dt id="id4.4.id4" class="ltx_glossaryentry">RTAU</dt>
<dd>Recurrent Textual Attention Unit</dd>
<dt id="id5.5.id5" class="ltx_glossaryentry">RVAU</dt>
<dd>Recurrent Visual Attention Unit</dd>
<dt id="id6.6.id6" class="ltx_glossaryentry">CNN</dt>
<dd>convolutional neural network</dd>
<dt id="id7.7.id7" class="ltx_glossaryentry">RNN</dt>
<dd>recurrent neural network</dd>
<dt id="id8.8.id8" class="ltx_glossaryentry">GRU</dt>
<dd>gated recurrent unit</dd>
<dt id="id9.9.id9" class="ltx_glossaryentry">LSTM</dt>
<dd>long short-term memory</dd>
<dt id="id10.10.id10" class="ltx_glossaryentry">DCA</dt>
<dd>Dual Convolution Attention</dd>
</dl>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Although <a href="#id6.6.id6"><span href="#id6.6.id6" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">convolutional neural networks</span></span></a> and <a href="#id7.7.id7"><span href="#id7.7.id7" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">recurrent neural networks</span></span></a>
have been successfully applied to various image and natural language processing tasks (cf.Â <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>); Bosse etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>); Bahdanau etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>); Nallapati etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite>), these breakthroughs only slowly translate to multimodal tasks such as <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Visual Question Answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr></a> where the model needs to create a joint understanding of the image and question. Such multimodal tasks require designing highly expressive joint visual and textual representations. On the other hand, a highly discriminative multi-modal feature fusion method is not sufficient for all VQA questions, since global features can contain noisy information for answering questions pertaining to certain local parts of the input. This motivation has led to the use of attention mechanisms in VQA.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Attention mechanisms have been extensively used in VQA recently <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2017</a>; Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>; Kim etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>. They attempt to make the model selectively predict based on segments of the spatial or lingual context. However, most attention mechanisms used in VQA models are rather simple, consisting of two convolutional layers and a softmax function to generate the attention weights which are summed over the input features. These shallow attention mechanisms could fail to select the relevant information from the joint representation of the question and image for complex questions. According to the literature in human cognition, humans process cognitive attention both spatially and temporally <cite class="ltx_cite ltx_citemacro_citep">(Rensink, <a href="#bib.bib32" title="" class="ltx_ref">2000</a>)</cite>. Consequently, recurrent attention mechanisms come to mind. Creating attention for complex questions, particularly sequential or relational reasoning questions, requires processing information in a sequential manner which recurrent layers are better suited due to their intrinsic ability to capture relevant information over an input sequence.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RNN</span></abbr></a>-based attention mechanism for visual and textual attention. We argue that embedding an RNN in the attention mechanism helps the model process information in a sequential manner and determine what is relevant to solve the task. We refer to the combination of RNN embedding and attention as  <a href="#id4.4.id4"><span href="#id4.4.id4" title="Recurrent Textual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Recurrent Textual Attention Unit</span></span></a> (<a href="#id4.4.id4"><abbr href="#id4.4.id4" title="Recurrent Textual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RTAU</span></abbr></a>) and  <a href="#id5.5.id5"><span href="#id5.5.id5" title="Recurrent Visual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Recurrent Visual Attention Unit</span></span></a> (<a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Recurrent Visual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RVAU</span></abbr></a>) respective of their purpose. Furthermore, we employ these units in a fairly simple network, referred to as  <a href="#id3.3.id3"><span href="#id3.3.id3" title="Dual Recurrent Attention Units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Dual Recurrent Attention Units</span></span></a> (<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Dual Recurrent Attention Units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DRAU</span></abbr></a>) network, and show competitive results compared to state-of-the-art models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our main contributions are the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a novel approach to generate soft attention for VQA. To the best of our knowledge, this is the first attempt to generate attention maps using recurrent neural networks in the VQA domain.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We conduct a direct comparison between two identical models except for their attention mechanism. In this controlled environment, the recurrent attention outperforms the convolutional attention significantly (4% absolute difference).</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We propose a network that utilizes two <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a> to co-attend the multi-modal inputs. We perform an ablation study to further test the effect of recurrent attention in our model. The results show a significant improvement when using both <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a> compared to using convolutional attention.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a> are modular, thus, they can substitute existing attention mechanisms in most models fairly easily. We show that state-of-the-art models with RVAU or RTAU â€œplugged-inâ€ perform consistently better than their standard counterparts.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">We show that our network outperforms the VQA 2016 and 2017 challenge winners and performs close to the current state-of-the-art single models. Additionally, we provide qualitative results showing subjective improvements over the default attention used in most VQA models.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In Section 2, we review related work for recurrent attention and VQA methods. In Section 3, we break down the components of the DRAU network and explain the details of a <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAU</span></abbr></a>. In Section 4, we compare convolutional and recurrent attention in a baseline model, conduct ablation experiments using <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Dual Recurrent Attention Units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DRAU</span></abbr></a>, and report the results of substituting attention mechanisms of state-of-the-art models with <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a> on the VQA 2.0 dataset <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>. Then we compare our model against the state-of-the-art on the VQA 1.0 <cite class="ltx_cite ltx_citemacro_citep">(Antol etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite> and 2.0 datasets. In Section 5, we compare the difference in attention maps between standard and recurrent attention with qualitative examples to illustrate the effect of RAUs. Finally, we conclude the paper in Section 6 and discuss future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section discusses related recurrent attention mechanisms and common methods that have been explored in the past for VQA.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Recurrent Attention</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p"><a href="#id7.7.id7"><abbr href="#id7.7.id7" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RNN</span></abbr></a>-based attention models have been used outside of the VQA domain.
<cite class="ltx_cite ltx_citemacro_cite">Mnih etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2014</a>)</cite> train a recurrent neural network model for digit recognition. Their model selects a
sequence of regions of an image and processes the selected regions at high
resolutions to save computational power. However, the whole architecture is a monolithic
recurrent model that limits its use in existing VQA models since it can not easily substitute VQA attention mechanisms. Moreover, the model
is not differentiable and requires training with reinforcement learning which is highly inefficient to train in
practice due to training instability <cite class="ltx_cite ltx_citemacro_citep">(Lanctot etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> and complexity of reward function
design <cite class="ltx_cite ltx_citemacro_citep">(Paulus etÂ al., <a href="#bib.bib29" title="" class="ltx_ref">2017</a>)</cite>.
Closer to our work, <cite class="ltx_cite ltx_citemacro_cite">Homayounfar etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite> uses a recurrent attention mechanism to attend to lane boundaries for traffic lane counting. There exists some differences compared to our attention unit. Their attention mechanism uses multi-step training to train
the attention unit while <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a> do not require any attention-specific training. It is worth noting that <cite class="ltx_cite ltx_citemacro_cite">Homayounfar etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite> use a vanilla Convolutional RNN for attention. Furthermore, they sample the input feature maps from different scales to provide information from different granularites. Trying different recurrent architectures like GRU <cite class="ltx_cite ltx_citemacro_citep">(Cho etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2014</a>)</cite>,
Grid-LSTM <cite class="ltx_cite ltx_citemacro_citep">(Kalchbrenner etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite>, and Conv-LSTM <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al., <a href="#bib.bib34" title="" class="ltx_ref">2015</a>)</cite> and their effect on attention could be an interesting research direction in the future.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Bilinear pooling representations</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Fukui etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> use compact bilinear pooling to attend over the image features and combine it with the language representation. The basic concept behind compact bilinear pooling is approximating
the outer product by randomly projecting the embeddings to a higher
dimensional space using Count Sketch projection <cite class="ltx_cite ltx_citemacro_citep">(Charikar etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2004</a>)</cite> and then exploiting
Fast Fourier Transforms to compute an efficient convolution. An ensemble model using MCB won first place in VQA (1.0) 2016 challenge.
<cite class="ltx_cite ltx_citemacro_citet">Kim etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite> argues that compact bilinear pooling is still expensive
to compute and shows that it can be replaced by element-wise product (Hadamard product) and a linear mapping (i.e.Â fully-connected layer) which gives a lower dimensional representation and also improves the model accuracy.
<cite class="ltx_cite ltx_citemacro_citet">Ben-younes etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> proposed using Tucker decomposition <cite class="ltx_cite ltx_citemacro_citep">(Tucker, <a href="#bib.bib36" title="" class="ltx_ref">1966</a>)</cite> with a low-rank matrix constraint as a bilinear representation. <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2017a</a>)</cite> utilize matrix factorization tricks to create a multi-modal factorized bilinear pooling method (MFB). Later, <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2017b</a>)</cite> generalizes the factorization for higher-order factorized pooling (MFH).</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Attention-based</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Xu and Saenko (<a href="#bib.bib38" title="" class="ltx_ref">2016</a>)</cite> propose a memory network that predicts visual attention based on a dot product between image features and word embedding. They utilize a separate embedding that predicts visual â€œevidenceâ€ related to the question. The evidence embedding can be computed iteratively (2-hops) to collect more evidence.
<cite class="ltx_cite ltx_citemacro_citet">Lu etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite> were the first to feature
a co-attention mechanism that applies attention to both the question
and image. <cite class="ltx_cite ltx_citemacro_citet">Nam etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite> use a Dual Attention Network (DAN) that
employs attention on both text and visual features iteratively to predict the result. The goal behind this is to allow the image and question attentions to guide each other in a co-dependent manner. <cite class="ltx_cite ltx_citemacro_cite">Schwartz etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> use high-order correlations between the multimodal input and multiple-choice answers to guide the modelâ€™s attention. It is worth noting that this model processes attention not only for both the question and image, but also for the answer. However, their VQA model utility is gravely weakened by their dependence on multiple-choice answer embedding which renders their model computationally infeasible for standard open-ended VQA tasks.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">
<a href="#id7.7.id7"><abbr href="#id7.7.id7" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RNNs</span></abbr></a> for VQA</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Using <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RNNs</span></abbr></a> for VQA models has been explored in the past, but, to the best of our knowledge, has never been used as an attention mechanism. <cite class="ltx_cite ltx_citemacro_citet">Xiong etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2016</a>)</cite> build upon the dynamic memory network from <cite class="ltx_cite ltx_citemacro_citet">Kumar and Varaiya (<a href="#bib.bib20" title="" class="ltx_ref">2015</a>)</cite> and proposes DMN+. DMN+ uses episodic modules which contain attention-based GRUs. Note that this is not the same as what we propose; <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib37" title="" class="ltx_ref">Xiong etÂ al.</a></cite> generate soft attention using <span id="S2.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_italic">convolutional layers</span> and uses the output to substitute the update gate of the GRU. In contrast, our approach uses the <span id="S2.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_italic">recurrent layers</span> to explicitly generate the attention. <cite class="ltx_cite ltx_citemacro_citet">Noh and Han (<a href="#bib.bib28" title="" class="ltx_ref">2016</a>)</cite> propose recurrent answering units in which each unit is a complete module that can answer a question about an image. They use joint loss minimization to train the units. However during testing, they use the first answering unit which was trained from other units through backpropagation.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dual Recurrent Attention in VQA</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we define our attention mechanism. Then, we describe the components of our VQA model in this section. All modules are annotated in <a href="#S3.F2" title="In 3 Dual Recurrent Attention in VQA â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> for reference.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1802.00209/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 1: </span>Recurrent Attention Unit.</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1802.00209/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 2: </span>The proposed network. <math id="S3.F2.2.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S3.F2.2.m1.1b"><mo id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">âŠ•</mo><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><csymbol cd="latexml" id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">\oplus</annotation></semantics></math> denotes concatenation.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span> <a href="#id2.2.id2"><span href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Recurrent Attention Units</span></span></a> (<a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a>)</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">The <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAU</span></abbr></a> receives a multi-modal multi-channel representation of the inputs, <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">X</annotation></semantics></math>. To scale down the input representation, a RAU starts with a <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">1\times 1</annotation></semantics></math> convolution and PReLU activation <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite>:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E1.m1.17" class="ltx_Math" alttext="\begin{gathered}c_{a}=\mathrm{PReLU}\ \big{(}W_{a}\ X\big{)},\\
X\in\mathbb{R}^{K\times\phi}\end{gathered}" display="block"><semantics id="S3.E1.m1.17a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.17.17.3"><mtr id="S3.E1.m1.17.17.3a"><mtd id="S3.E1.m1.17.17.3b"><mrow id="S3.E1.m1.17.17.3.15.11.11.11"><mrow id="S3.E1.m1.17.17.3.15.11.11.11.1"><msub id="S3.E1.m1.17.17.3.15.11.11.11.1.2"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">c</mi><mi id="S3.E1.m1.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.1.cmml">a</mi></msub><mo id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.E1.m1.17.17.3.15.11.11.11.1.1"><mi id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.4.4.4.4.4.4.cmml">PReLU</mi><mo lspace="0.500em" rspace="0em" id="S3.E1.m1.17.17.3.15.11.11.11.1.1.2" xref="S3.E1.m1.16.16.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.17.17.3.15.11.11.11.1.1.1.1"><mo maxsize="120%" minsize="120%" id="S3.E1.m1.5.5.5.5.5.5" xref="S3.E1.m1.16.16.2.3.cmml">(</mo><mrow id="S3.E1.m1.17.17.3.15.11.11.11.1.1.1.1.1"><msub id="S3.E1.m1.17.17.3.15.11.11.11.1.1.1.1.1.2"><mi id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.6.6.6.6.6.6.cmml">W</mi><mi id="S3.E1.m1.7.7.7.7.7.7.1" xref="S3.E1.m1.7.7.7.7.7.7.1.cmml">a</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.17.17.3.15.11.11.11.1.1.1.1.1.1" xref="S3.E1.m1.16.16.2.3.cmml">â€‹</mo><mi id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.8.8.8.8.8.8.cmml">X</mi></mrow><mo maxsize="120%" minsize="120%" id="S3.E1.m1.9.9.9.9.9.9" xref="S3.E1.m1.16.16.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.10.10.10.10.10.10" xref="S3.E1.m1.16.16.2.3.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.17.17.3c"><mtd id="S3.E1.m1.17.17.3d"><mrow id="S3.E1.m1.14.14.14.4.4"><mi id="S3.E1.m1.11.11.11.1.1.1" xref="S3.E1.m1.11.11.11.1.1.1.cmml">X</mi><mo id="S3.E1.m1.12.12.12.2.2.2" xref="S3.E1.m1.12.12.12.2.2.2.cmml">âˆˆ</mo><msup id="S3.E1.m1.14.14.14.4.4.5"><mi id="S3.E1.m1.13.13.13.3.3.3" xref="S3.E1.m1.13.13.13.3.3.3.cmml">â„</mi><mrow id="S3.E1.m1.14.14.14.4.4.4.1" xref="S3.E1.m1.14.14.14.4.4.4.1.cmml"><mi id="S3.E1.m1.14.14.14.4.4.4.1.2" xref="S3.E1.m1.14.14.14.4.4.4.1.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.14.14.14.4.4.4.1.1" xref="S3.E1.m1.14.14.14.4.4.4.1.1.cmml">Ã—</mo><mi id="S3.E1.m1.14.14.14.4.4.4.1.3" xref="S3.E1.m1.14.14.14.4.4.4.1.3.cmml">Ï•</mi></mrow></msup></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.17b"><apply id="S3.E1.m1.16.16.2.3.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.16.16.2.3a.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2">formulae-sequence</csymbol><apply id="S3.E1.m1.15.15.1.1.1.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><eq id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></eq><apply id="S3.E1.m1.15.15.1.1.1.3.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.15.15.1.1.1.3.1.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">ğ‘</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1">ğ‘</ci></apply><apply id="S3.E1.m1.15.15.1.1.1.1.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><times id="S3.E1.m1.15.15.1.1.1.1.2.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"></times><ci id="S3.E1.m1.4.4.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4.4.4">PReLU</ci><apply id="S3.E1.m1.15.15.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><times id="S3.E1.m1.15.15.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"></times><apply id="S3.E1.m1.15.15.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.15.15.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2">subscript</csymbol><ci id="S3.E1.m1.6.6.6.6.6.6.cmml" xref="S3.E1.m1.6.6.6.6.6.6">ğ‘Š</ci><ci id="S3.E1.m1.7.7.7.7.7.7.1.cmml" xref="S3.E1.m1.7.7.7.7.7.7.1">ğ‘</ci></apply><ci id="S3.E1.m1.8.8.8.8.8.8.cmml" xref="S3.E1.m1.8.8.8.8.8.8">ğ‘‹</ci></apply></apply></apply><apply id="S3.E1.m1.16.16.2.2.2.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><in id="S3.E1.m1.12.12.12.2.2.2.cmml" xref="S3.E1.m1.12.12.12.2.2.2"></in><ci id="S3.E1.m1.11.11.11.1.1.1.cmml" xref="S3.E1.m1.11.11.11.1.1.1">ğ‘‹</ci><apply id="S3.E1.m1.16.16.2.2.2.3.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.16.16.2.2.2.3.1.cmml" xref="S3.E1.m1.17.17.3.15.11.11.11.1.1.2">superscript</csymbol><ci id="S3.E1.m1.13.13.13.3.3.3.cmml" xref="S3.E1.m1.13.13.13.3.3.3">â„</ci><apply id="S3.E1.m1.14.14.14.4.4.4.1.cmml" xref="S3.E1.m1.14.14.14.4.4.4.1"><times id="S3.E1.m1.14.14.14.4.4.4.1.1.cmml" xref="S3.E1.m1.14.14.14.4.4.4.1.1"></times><ci id="S3.E1.m1.14.14.14.4.4.4.1.2.cmml" xref="S3.E1.m1.14.14.14.4.4.4.1.2">ğ¾</ci><ci id="S3.E1.m1.14.14.14.4.4.4.1.3.cmml" xref="S3.E1.m1.14.14.14.4.4.4.1.3">italic-Ï•</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.17c">\begin{gathered}c_{a}=\mathrm{PReLU}\ \big{(}W_{a}\ X\big{)},\\
X\in\mathbb{R}^{K\times\phi}\end{gathered}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.7" class="ltx_p">where <math id="S3.SS1.p1.3.m1.1" class="ltx_Math" alttext="W_{a}" display="inline"><semantics id="S3.SS1.p1.3.m1.1a"><msub id="S3.SS1.p1.3.m1.1.1" xref="S3.SS1.p1.3.m1.1.1.cmml"><mi id="S3.SS1.p1.3.m1.1.1.2" xref="S3.SS1.p1.3.m1.1.1.2.cmml">W</mi><mi id="S3.SS1.p1.3.m1.1.1.3" xref="S3.SS1.p1.3.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m1.1b"><apply id="S3.SS1.p1.3.m1.1.1.cmml" xref="S3.SS1.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m1.1.1.1.cmml" xref="S3.SS1.p1.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m1.1.1.2.cmml" xref="S3.SS1.p1.3.m1.1.1.2">ğ‘Š</ci><ci id="S3.SS1.p1.3.m1.1.1.3.cmml" xref="S3.SS1.p1.3.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m1.1c">W_{a}</annotation></semantics></math> is the <math id="S3.SS1.p1.4.m2.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.p1.4.m2.1a"><mrow id="S3.SS1.p1.4.m2.1.1" xref="S3.SS1.p1.4.m2.1.1.cmml"><mn id="S3.SS1.p1.4.m2.1.1.2" xref="S3.SS1.p1.4.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.4.m2.1.1.1" xref="S3.SS1.p1.4.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p1.4.m2.1.1.3" xref="S3.SS1.p1.4.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m2.1b"><apply id="S3.SS1.p1.4.m2.1.1.cmml" xref="S3.SS1.p1.4.m2.1.1"><times id="S3.SS1.p1.4.m2.1.1.1.cmml" xref="S3.SS1.p1.4.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p1.4.m2.1.1.2.cmml" xref="S3.SS1.p1.4.m2.1.1.2">1</cn><cn type="integer" id="S3.SS1.p1.4.m2.1.1.3.cmml" xref="S3.SS1.p1.4.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m2.1c">1\times 1</annotation></semantics></math> convolution weights, <math id="S3.SS1.p1.5.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p1.5.m3.1a"><mi id="S3.SS1.p1.5.m3.1.1" xref="S3.SS1.p1.5.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m3.1b"><ci id="S3.SS1.p1.5.m3.1.1.cmml" xref="S3.SS1.p1.5.m3.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m3.1c">X</annotation></semantics></math> is the multimodal input to the RAU, <math id="S3.SS1.p1.6.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p1.6.m4.1a"><mi id="S3.SS1.p1.6.m4.1.1" xref="S3.SS1.p1.6.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m4.1b"><ci id="S3.SS1.p1.6.m4.1.1.cmml" xref="S3.SS1.p1.6.m4.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m4.1c">K</annotation></semantics></math> is the shape of the target attention (e.g. image pixels/number of visual objects or question length), and <math id="S3.SS1.p1.7.m5.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS1.p1.7.m5.1a"><mi id="S3.SS1.p1.7.m5.1.1" xref="S3.SS1.p1.7.m5.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m5.1b"><ci id="S3.SS1.p1.7.m5.1.1.cmml" xref="S3.SS1.p1.7.m5.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m5.1c">\phi</annotation></semantics></math> is the number of channels in the input.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p">Furthermore, we feed the previous output into a unidirectional LSTM:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E2.m1.5" class="ltx_Math" alttext="h_{a,n}=\mathrm{LSTM}\ \big{(}c_{a,n}\big{)}" display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml"><msub id="S3.E2.m1.5.5.3" xref="S3.E2.m1.5.5.3.cmml"><mi id="S3.E2.m1.5.5.3.2" xref="S3.E2.m1.5.5.3.2.cmml">h</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">a</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">n</mi></mrow></msub><mo id="S3.E2.m1.5.5.2" xref="S3.E2.m1.5.5.2.cmml">=</mo><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.cmml"><mi id="S3.E2.m1.5.5.1.3" xref="S3.E2.m1.5.5.1.3.cmml">LSTM</mi><mo lspace="0.500em" rspace="0em" id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mo maxsize="120%" minsize="120%" id="S3.E2.m1.5.5.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">c</mi><mrow id="S3.E2.m1.4.4.2.4" xref="S3.E2.m1.4.4.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml">a</mi><mo id="S3.E2.m1.4.4.2.4.1" xref="S3.E2.m1.4.4.2.3.cmml">,</mo><mi id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml">n</mi></mrow></msub><mo maxsize="120%" minsize="120%" id="S3.E2.m1.5.5.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"><eq id="S3.E2.m1.5.5.2.cmml" xref="S3.E2.m1.5.5.2"></eq><apply id="S3.E2.m1.5.5.3.cmml" xref="S3.E2.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.3.1.cmml" xref="S3.E2.m1.5.5.3">subscript</csymbol><ci id="S3.E2.m1.5.5.3.2.cmml" xref="S3.E2.m1.5.5.3.2">â„</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ‘</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">ğ‘›</ci></list></apply><apply id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.5.1"><times id="S3.E2.m1.5.5.1.2.cmml" xref="S3.E2.m1.5.5.1.2"></times><ci id="S3.E2.m1.5.5.1.3.cmml" xref="S3.E2.m1.5.5.1.3">LSTM</ci><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2">ğ‘</ci><list id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.4"><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">ğ‘</ci><ci id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2">ğ‘›</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">h_{a,n}=\mathrm{LSTM}\ \big{(}c_{a,n}\big{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.2" class="ltx_p">where <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="h_{a,n}" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><msub id="S3.SS1.p2.1.m1.2.3" xref="S3.SS1.p2.1.m1.2.3.cmml"><mi id="S3.SS1.p2.1.m1.2.3.2" xref="S3.SS1.p2.1.m1.2.3.2.cmml">h</mi><mrow id="S3.SS1.p2.1.m1.2.2.2.4" xref="S3.SS1.p2.1.m1.2.2.2.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.cmml">a</mi><mo id="S3.SS1.p2.1.m1.2.2.2.4.1" xref="S3.SS1.p2.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.1.m1.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><apply id="S3.SS1.p2.1.m1.2.3.cmml" xref="S3.SS1.p2.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3">subscript</csymbol><ci id="S3.SS1.p2.1.m1.2.3.2.cmml" xref="S3.SS1.p2.1.m1.2.3.2">â„</ci><list id="S3.SS1.p2.1.m1.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.2.4"><ci id="S3.SS1.p2.1.m1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1">ğ‘</ci><ci id="S3.SS1.p2.1.m1.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">h_{a,n}</annotation></semantics></math> is the hidden state at time <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">n</annotation></semantics></math>. Each hidden state processes the joint features at each location/word of the input and decides which information should be kept and propagated forward and which information should be ignored.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">To generate the attention weights, we feed all the hidden states of the previous LSTM to a <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">1\times 1</annotation></semantics></math> convolution layer followed by a softmax function. The <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mn id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">1\times 1</annotation></semantics></math> convolution layer could be interpreted as the <span id="S3.SS1.p3.2.1" class="ltx_text ltx_font_italic">number of glimpses</span> the model sees.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E3.m1.5" class="ltx_Math" alttext="W_{att,n}=\mathrm{softmax}\ \Big{(}\mathrm{PReLU}\ \big{(}W_{g}\ h_{a,n}\big{)}\Big{)}" display="block"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml"><msub id="S3.E3.m1.5.5.3" xref="S3.E3.m1.5.5.3.cmml"><mi id="S3.E3.m1.5.5.3.2" xref="S3.E3.m1.5.5.3.2.cmml">W</mi><mrow id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.3.cmml"><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.1a" xref="S3.E3.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.2.2.2.2.1.4" xref="S3.E3.m1.2.2.2.2.1.4.cmml">t</mi></mrow><mo id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">n</mi></mrow></msub><mo id="S3.E3.m1.5.5.2" xref="S3.E3.m1.5.5.2.cmml">=</mo><mrow id="S3.E3.m1.5.5.1" xref="S3.E3.m1.5.5.1.cmml"><mi id="S3.E3.m1.5.5.1.3" xref="S3.E3.m1.5.5.1.3.cmml">softmax</mi><mo lspace="0.500em" rspace="0em" id="S3.E3.m1.5.5.1.2" xref="S3.E3.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.5.5.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.cmml"><mo maxsize="160%" minsize="160%" id="S3.E3.m1.5.5.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.5.5.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.3.cmml">PReLU</mi><mo lspace="0.500em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><mo maxsize="120%" minsize="120%" id="S3.E3.m1.5.5.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.2.cmml">W</mi><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.3.cmml">g</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2.cmml">h</mi><mrow id="S3.E3.m1.4.4.2.4" xref="S3.E3.m1.4.4.2.3.cmml"><mi id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml">a</mi><mo id="S3.E3.m1.4.4.2.4.1" xref="S3.E3.m1.4.4.2.3.cmml">,</mo><mi id="S3.E3.m1.4.4.2.2" xref="S3.E3.m1.4.4.2.2.cmml">n</mi></mrow></msub></mrow><mo maxsize="120%" minsize="120%" id="S3.E3.m1.5.5.1.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="160%" minsize="160%" id="S3.E3.m1.5.5.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"><eq id="S3.E3.m1.5.5.2.cmml" xref="S3.E3.m1.5.5.2"></eq><apply id="S3.E3.m1.5.5.3.cmml" xref="S3.E3.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.3.1.cmml" xref="S3.E3.m1.5.5.3">subscript</csymbol><ci id="S3.E3.m1.5.5.3.2.cmml" xref="S3.E3.m1.5.5.3.2">ğ‘Š</ci><list id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2"><apply id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1"><times id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"></times><ci id="S3.E3.m1.2.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2">ğ‘</ci><ci id="S3.E3.m1.2.2.2.2.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.3">ğ‘¡</ci><ci id="S3.E3.m1.2.2.2.2.1.4.cmml" xref="S3.E3.m1.2.2.2.2.1.4">ğ‘¡</ci></apply><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ğ‘›</ci></list></apply><apply id="S3.E3.m1.5.5.1.cmml" xref="S3.E3.m1.5.5.1"><times id="S3.E3.m1.5.5.1.2.cmml" xref="S3.E3.m1.5.5.1.2"></times><ci id="S3.E3.m1.5.5.1.3.cmml" xref="S3.E3.m1.5.5.1.3">softmax</ci><apply id="S3.E3.m1.5.5.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1"><times id="S3.E3.m1.5.5.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2"></times><ci id="S3.E3.m1.5.5.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3">PReLU</ci><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1"><times id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1"></times><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.2">ğ‘Š</ci><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.3">ğ‘”</ci></apply><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2">â„</ci><list id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.4"><ci id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1">ğ‘</ci><ci id="S3.E3.m1.4.4.2.2.cmml" xref="S3.E3.m1.4.4.2.2">ğ‘›</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">W_{att,n}=\mathrm{softmax}\ \Big{(}\mathrm{PReLU}\ \big{(}W_{g}\ h_{a,n}\big{)}\Big{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.4" class="ltx_p">where <math id="S3.SS1.p3.3.m1.1" class="ltx_Math" alttext="W_{g}" display="inline"><semantics id="S3.SS1.p3.3.m1.1a"><msub id="S3.SS1.p3.3.m1.1.1" xref="S3.SS1.p3.3.m1.1.1.cmml"><mi id="S3.SS1.p3.3.m1.1.1.2" xref="S3.SS1.p3.3.m1.1.1.2.cmml">W</mi><mi id="S3.SS1.p3.3.m1.1.1.3" xref="S3.SS1.p3.3.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m1.1b"><apply id="S3.SS1.p3.3.m1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m1.1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m1.1.1.2.cmml" xref="S3.SS1.p3.3.m1.1.1.2">ğ‘Š</ci><ci id="S3.SS1.p3.3.m1.1.1.3.cmml" xref="S3.SS1.p3.3.m1.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m1.1c">W_{g}</annotation></semantics></math> is the glimpsesâ€™ weights and <math id="S3.SS1.p3.4.m2.2" class="ltx_Math" alttext="W_{att,n}" display="inline"><semantics id="S3.SS1.p3.4.m2.2a"><msub id="S3.SS1.p3.4.m2.2.3" xref="S3.SS1.p3.4.m2.2.3.cmml"><mi id="S3.SS1.p3.4.m2.2.3.2" xref="S3.SS1.p3.4.m2.2.3.2.cmml">W</mi><mrow id="S3.SS1.p3.4.m2.2.2.2.2" xref="S3.SS1.p3.4.m2.2.2.2.3.cmml"><mrow id="S3.SS1.p3.4.m2.2.2.2.2.1" xref="S3.SS1.p3.4.m2.2.2.2.2.1.cmml"><mi id="S3.SS1.p3.4.m2.2.2.2.2.1.2" xref="S3.SS1.p3.4.m2.2.2.2.2.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m2.2.2.2.2.1.1" xref="S3.SS1.p3.4.m2.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.SS1.p3.4.m2.2.2.2.2.1.3" xref="S3.SS1.p3.4.m2.2.2.2.2.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m2.2.2.2.2.1.1a" xref="S3.SS1.p3.4.m2.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.SS1.p3.4.m2.2.2.2.2.1.4" xref="S3.SS1.p3.4.m2.2.2.2.2.1.4.cmml">t</mi></mrow><mo id="S3.SS1.p3.4.m2.2.2.2.2.2" xref="S3.SS1.p3.4.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p3.4.m2.1.1.1.1" xref="S3.SS1.p3.4.m2.1.1.1.1.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m2.2b"><apply id="S3.SS1.p3.4.m2.2.3.cmml" xref="S3.SS1.p3.4.m2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m2.2.3.1.cmml" xref="S3.SS1.p3.4.m2.2.3">subscript</csymbol><ci id="S3.SS1.p3.4.m2.2.3.2.cmml" xref="S3.SS1.p3.4.m2.2.3.2">ğ‘Š</ci><list id="S3.SS1.p3.4.m2.2.2.2.3.cmml" xref="S3.SS1.p3.4.m2.2.2.2.2"><apply id="S3.SS1.p3.4.m2.2.2.2.2.1.cmml" xref="S3.SS1.p3.4.m2.2.2.2.2.1"><times id="S3.SS1.p3.4.m2.2.2.2.2.1.1.cmml" xref="S3.SS1.p3.4.m2.2.2.2.2.1.1"></times><ci id="S3.SS1.p3.4.m2.2.2.2.2.1.2.cmml" xref="S3.SS1.p3.4.m2.2.2.2.2.1.2">ğ‘</ci><ci id="S3.SS1.p3.4.m2.2.2.2.2.1.3.cmml" xref="S3.SS1.p3.4.m2.2.2.2.2.1.3">ğ‘¡</ci><ci id="S3.SS1.p3.4.m2.2.2.2.2.1.4.cmml" xref="S3.SS1.p3.4.m2.2.2.2.2.1.4">ğ‘¡</ci></apply><ci id="S3.SS1.p3.4.m2.1.1.1.1.cmml" xref="S3.SS1.p3.4.m2.1.1.1.1">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m2.2c">W_{att,n}</annotation></semantics></math> is the attention weight vector.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.5" class="ltx_p">Next, we use the attention weights to compute a weighted average of the image and question features.</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E4.m1.4" class="ltx_Math" alttext="att_{a,n}=\sum_{n=1}^{N}W_{att,n}\ f_{n}" display="block"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.5" xref="S3.E4.m1.4.5.cmml"><mrow id="S3.E4.m1.4.5.2" xref="S3.E4.m1.4.5.2.cmml"><mi id="S3.E4.m1.4.5.2.2" xref="S3.E4.m1.4.5.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.5.2.1" xref="S3.E4.m1.4.5.2.1.cmml">â€‹</mo><mi id="S3.E4.m1.4.5.2.3" xref="S3.E4.m1.4.5.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.5.2.1a" xref="S3.E4.m1.4.5.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.4.5.2.4" xref="S3.E4.m1.4.5.2.4.cmml"><mi id="S3.E4.m1.4.5.2.4.2" xref="S3.E4.m1.4.5.2.4.2.cmml">t</mi><mrow id="S3.E4.m1.2.2.2.4" xref="S3.E4.m1.2.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">a</mi><mo id="S3.E4.m1.2.2.2.4.1" xref="S3.E4.m1.2.2.2.3.cmml">,</mo><mi id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml">n</mi></mrow></msub></mrow><mo rspace="0.111em" id="S3.E4.m1.4.5.1" xref="S3.E4.m1.4.5.1.cmml">=</mo><mrow id="S3.E4.m1.4.5.3" xref="S3.E4.m1.4.5.3.cmml"><munderover id="S3.E4.m1.4.5.3.1" xref="S3.E4.m1.4.5.3.1.cmml"><mo movablelimits="false" id="S3.E4.m1.4.5.3.1.2.2" xref="S3.E4.m1.4.5.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.4.5.3.1.2.3" xref="S3.E4.m1.4.5.3.1.2.3.cmml"><mi id="S3.E4.m1.4.5.3.1.2.3.2" xref="S3.E4.m1.4.5.3.1.2.3.2.cmml">n</mi><mo id="S3.E4.m1.4.5.3.1.2.3.1" xref="S3.E4.m1.4.5.3.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.4.5.3.1.2.3.3" xref="S3.E4.m1.4.5.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.4.5.3.1.3" xref="S3.E4.m1.4.5.3.1.3.cmml">N</mi></munderover><mrow id="S3.E4.m1.4.5.3.2" xref="S3.E4.m1.4.5.3.2.cmml"><msub id="S3.E4.m1.4.5.3.2.2" xref="S3.E4.m1.4.5.3.2.2.cmml"><mi id="S3.E4.m1.4.5.3.2.2.2" xref="S3.E4.m1.4.5.3.2.2.2.cmml">W</mi><mrow id="S3.E4.m1.4.4.2.2" xref="S3.E4.m1.4.4.2.3.cmml"><mrow id="S3.E4.m1.4.4.2.2.1" xref="S3.E4.m1.4.4.2.2.1.cmml"><mi id="S3.E4.m1.4.4.2.2.1.2" xref="S3.E4.m1.4.4.2.2.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.2.1.1" xref="S3.E4.m1.4.4.2.2.1.1.cmml">â€‹</mo><mi id="S3.E4.m1.4.4.2.2.1.3" xref="S3.E4.m1.4.4.2.2.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.2.1.1a" xref="S3.E4.m1.4.4.2.2.1.1.cmml">â€‹</mo><mi id="S3.E4.m1.4.4.2.2.1.4" xref="S3.E4.m1.4.4.2.2.1.4.cmml">t</mi></mrow><mo id="S3.E4.m1.4.4.2.2.2" xref="S3.E4.m1.4.4.2.3.cmml">,</mo><mi id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml">n</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.5.3.2.1" xref="S3.E4.m1.4.5.3.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.4.5.3.2.3" xref="S3.E4.m1.4.5.3.2.3.cmml"><mi id="S3.E4.m1.4.5.3.2.3.2" xref="S3.E4.m1.4.5.3.2.3.2.cmml">f</mi><mi id="S3.E4.m1.4.5.3.2.3.3" xref="S3.E4.m1.4.5.3.2.3.3.cmml">n</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.5.cmml" xref="S3.E4.m1.4.5"><eq id="S3.E4.m1.4.5.1.cmml" xref="S3.E4.m1.4.5.1"></eq><apply id="S3.E4.m1.4.5.2.cmml" xref="S3.E4.m1.4.5.2"><times id="S3.E4.m1.4.5.2.1.cmml" xref="S3.E4.m1.4.5.2.1"></times><ci id="S3.E4.m1.4.5.2.2.cmml" xref="S3.E4.m1.4.5.2.2">ğ‘</ci><ci id="S3.E4.m1.4.5.2.3.cmml" xref="S3.E4.m1.4.5.2.3">ğ‘¡</ci><apply id="S3.E4.m1.4.5.2.4.cmml" xref="S3.E4.m1.4.5.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.2.4.1.cmml" xref="S3.E4.m1.4.5.2.4">subscript</csymbol><ci id="S3.E4.m1.4.5.2.4.2.cmml" xref="S3.E4.m1.4.5.2.4.2">ğ‘¡</ci><list id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.4"><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">ğ‘</ci><ci id="S3.E4.m1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2">ğ‘›</ci></list></apply></apply><apply id="S3.E4.m1.4.5.3.cmml" xref="S3.E4.m1.4.5.3"><apply id="S3.E4.m1.4.5.3.1.cmml" xref="S3.E4.m1.4.5.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.1.1.cmml" xref="S3.E4.m1.4.5.3.1">superscript</csymbol><apply id="S3.E4.m1.4.5.3.1.2.cmml" xref="S3.E4.m1.4.5.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.1.2.1.cmml" xref="S3.E4.m1.4.5.3.1">subscript</csymbol><sum id="S3.E4.m1.4.5.3.1.2.2.cmml" xref="S3.E4.m1.4.5.3.1.2.2"></sum><apply id="S3.E4.m1.4.5.3.1.2.3.cmml" xref="S3.E4.m1.4.5.3.1.2.3"><eq id="S3.E4.m1.4.5.3.1.2.3.1.cmml" xref="S3.E4.m1.4.5.3.1.2.3.1"></eq><ci id="S3.E4.m1.4.5.3.1.2.3.2.cmml" xref="S3.E4.m1.4.5.3.1.2.3.2">ğ‘›</ci><cn type="integer" id="S3.E4.m1.4.5.3.1.2.3.3.cmml" xref="S3.E4.m1.4.5.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.4.5.3.1.3.cmml" xref="S3.E4.m1.4.5.3.1.3">ğ‘</ci></apply><apply id="S3.E4.m1.4.5.3.2.cmml" xref="S3.E4.m1.4.5.3.2"><times id="S3.E4.m1.4.5.3.2.1.cmml" xref="S3.E4.m1.4.5.3.2.1"></times><apply id="S3.E4.m1.4.5.3.2.2.cmml" xref="S3.E4.m1.4.5.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.2.2.1.cmml" xref="S3.E4.m1.4.5.3.2.2">subscript</csymbol><ci id="S3.E4.m1.4.5.3.2.2.2.cmml" xref="S3.E4.m1.4.5.3.2.2.2">ğ‘Š</ci><list id="S3.E4.m1.4.4.2.3.cmml" xref="S3.E4.m1.4.4.2.2"><apply id="S3.E4.m1.4.4.2.2.1.cmml" xref="S3.E4.m1.4.4.2.2.1"><times id="S3.E4.m1.4.4.2.2.1.1.cmml" xref="S3.E4.m1.4.4.2.2.1.1"></times><ci id="S3.E4.m1.4.4.2.2.1.2.cmml" xref="S3.E4.m1.4.4.2.2.1.2">ğ‘</ci><ci id="S3.E4.m1.4.4.2.2.1.3.cmml" xref="S3.E4.m1.4.4.2.2.1.3">ğ‘¡</ci><ci id="S3.E4.m1.4.4.2.2.1.4.cmml" xref="S3.E4.m1.4.4.2.2.1.4">ğ‘¡</ci></apply><ci id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1.1">ğ‘›</ci></list></apply><apply id="S3.E4.m1.4.5.3.2.3.cmml" xref="S3.E4.m1.4.5.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.2.3.1.cmml" xref="S3.E4.m1.4.5.3.2.3">subscript</csymbol><ci id="S3.E4.m1.4.5.3.2.3.2.cmml" xref="S3.E4.m1.4.5.3.2.3.2">ğ‘“</ci><ci id="S3.E4.m1.4.5.3.2.3.3.cmml" xref="S3.E4.m1.4.5.3.2.3.3">ğ‘›</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">att_{a,n}=\sum_{n=1}^{N}W_{att,n}\ f_{n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.2" class="ltx_p">where <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="f_{n}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">ğ‘“</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">f_{n}</annotation></semantics></math> is the input representation and <math id="S3.SS1.p4.2.m2.2" class="ltx_Math" alttext="att_{a,n}" display="inline"><semantics id="S3.SS1.p4.2.m2.2a"><mrow id="S3.SS1.p4.2.m2.2.3" xref="S3.SS1.p4.2.m2.2.3.cmml"><mi id="S3.SS1.p4.2.m2.2.3.2" xref="S3.SS1.p4.2.m2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.2.3.1" xref="S3.SS1.p4.2.m2.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p4.2.m2.2.3.3" xref="S3.SS1.p4.2.m2.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.2.3.1a" xref="S3.SS1.p4.2.m2.2.3.1.cmml">â€‹</mo><msub id="S3.SS1.p4.2.m2.2.3.4" xref="S3.SS1.p4.2.m2.2.3.4.cmml"><mi id="S3.SS1.p4.2.m2.2.3.4.2" xref="S3.SS1.p4.2.m2.2.3.4.2.cmml">t</mi><mrow id="S3.SS1.p4.2.m2.2.2.2.4" xref="S3.SS1.p4.2.m2.2.2.2.3.cmml"><mi id="S3.SS1.p4.2.m2.1.1.1.1" xref="S3.SS1.p4.2.m2.1.1.1.1.cmml">a</mi><mo id="S3.SS1.p4.2.m2.2.2.2.4.1" xref="S3.SS1.p4.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p4.2.m2.2.2.2.2" xref="S3.SS1.p4.2.m2.2.2.2.2.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.2b"><apply id="S3.SS1.p4.2.m2.2.3.cmml" xref="S3.SS1.p4.2.m2.2.3"><times id="S3.SS1.p4.2.m2.2.3.1.cmml" xref="S3.SS1.p4.2.m2.2.3.1"></times><ci id="S3.SS1.p4.2.m2.2.3.2.cmml" xref="S3.SS1.p4.2.m2.2.3.2">ğ‘</ci><ci id="S3.SS1.p4.2.m2.2.3.3.cmml" xref="S3.SS1.p4.2.m2.2.3.3">ğ‘¡</ci><apply id="S3.SS1.p4.2.m2.2.3.4.cmml" xref="S3.SS1.p4.2.m2.2.3.4"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.2.3.4.1.cmml" xref="S3.SS1.p4.2.m2.2.3.4">subscript</csymbol><ci id="S3.SS1.p4.2.m2.2.3.4.2.cmml" xref="S3.SS1.p4.2.m2.2.3.4.2">ğ‘¡</ci><list id="S3.SS1.p4.2.m2.2.2.2.3.cmml" xref="S3.SS1.p4.2.m2.2.2.2.4"><ci id="S3.SS1.p4.2.m2.1.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1.1.1">ğ‘</ci><ci id="S3.SS1.p4.2.m2.2.2.2.2.cmml" xref="S3.SS1.p4.2.m2.2.2.2.2">ğ‘›</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.2c">att_{a,n}</annotation></semantics></math> is the attention applied on the input.
Finally, the attention maps are fed into a fully-connected layer and a PReLU activation. <a href="#S3.F1" title="In 3 Dual Recurrent Attention in VQA â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> illustrates the structure of a RAU.</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E5.m1.5" class="ltx_Math" alttext="y_{att,n}=\mathrm{PReLU}\ \big{(}W_{out}\ att_{a,n}\big{)}" display="block"><semantics id="S3.E5.m1.5a"><mrow id="S3.E5.m1.5.5" xref="S3.E5.m1.5.5.cmml"><msub id="S3.E5.m1.5.5.3" xref="S3.E5.m1.5.5.3.cmml"><mi id="S3.E5.m1.5.5.3.2" xref="S3.E5.m1.5.5.3.2.cmml">y</mi><mrow id="S3.E5.m1.2.2.2.2" xref="S3.E5.m1.2.2.2.3.cmml"><mrow id="S3.E5.m1.2.2.2.2.1" xref="S3.E5.m1.2.2.2.2.1.cmml"><mi id="S3.E5.m1.2.2.2.2.1.2" xref="S3.E5.m1.2.2.2.2.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.2.2.1.1" xref="S3.E5.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E5.m1.2.2.2.2.1.3" xref="S3.E5.m1.2.2.2.2.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.2.2.1.1a" xref="S3.E5.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E5.m1.2.2.2.2.1.4" xref="S3.E5.m1.2.2.2.2.1.4.cmml">t</mi></mrow><mo id="S3.E5.m1.2.2.2.2.2" xref="S3.E5.m1.2.2.2.3.cmml">,</mo><mi id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml">n</mi></mrow></msub><mo id="S3.E5.m1.5.5.2" xref="S3.E5.m1.5.5.2.cmml">=</mo><mrow id="S3.E5.m1.5.5.1" xref="S3.E5.m1.5.5.1.cmml"><mi id="S3.E5.m1.5.5.1.3" xref="S3.E5.m1.5.5.1.3.cmml">PReLU</mi><mo lspace="0.500em" rspace="0em" id="S3.E5.m1.5.5.1.2" xref="S3.E5.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S3.E5.m1.5.5.1.1.1" xref="S3.E5.m1.5.5.1.1.1.1.cmml"><mo maxsize="120%" minsize="120%" id="S3.E5.m1.5.5.1.1.1.2" xref="S3.E5.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.5.5.1.1.1.1" xref="S3.E5.m1.5.5.1.1.1.1.cmml"><msub id="S3.E5.m1.5.5.1.1.1.1.2" xref="S3.E5.m1.5.5.1.1.1.1.2.cmml"><mi id="S3.E5.m1.5.5.1.1.1.1.2.2" xref="S3.E5.m1.5.5.1.1.1.1.2.2.cmml">W</mi><mrow id="S3.E5.m1.5.5.1.1.1.1.2.3" xref="S3.E5.m1.5.5.1.1.1.1.2.3.cmml"><mi id="S3.E5.m1.5.5.1.1.1.1.2.3.2" xref="S3.E5.m1.5.5.1.1.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.1.1.2.3.1" xref="S3.E5.m1.5.5.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.5.5.1.1.1.1.2.3.3" xref="S3.E5.m1.5.5.1.1.1.1.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.1.1.2.3.1a" xref="S3.E5.m1.5.5.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.5.5.1.1.1.1.2.3.4" xref="S3.E5.m1.5.5.1.1.1.1.2.3.4.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.1.1.1" xref="S3.E5.m1.5.5.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.E5.m1.5.5.1.1.1.1.3" xref="S3.E5.m1.5.5.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.1.1.1a" xref="S3.E5.m1.5.5.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.E5.m1.5.5.1.1.1.1.4" xref="S3.E5.m1.5.5.1.1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.1.1.1.1b" xref="S3.E5.m1.5.5.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.E5.m1.5.5.1.1.1.1.5" xref="S3.E5.m1.5.5.1.1.1.1.5.cmml"><mi id="S3.E5.m1.5.5.1.1.1.1.5.2" xref="S3.E5.m1.5.5.1.1.1.1.5.2.cmml">t</mi><mrow id="S3.E5.m1.4.4.2.4" xref="S3.E5.m1.4.4.2.3.cmml"><mi id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml">a</mi><mo id="S3.E5.m1.4.4.2.4.1" xref="S3.E5.m1.4.4.2.3.cmml">,</mo><mi id="S3.E5.m1.4.4.2.2" xref="S3.E5.m1.4.4.2.2.cmml">n</mi></mrow></msub></mrow><mo maxsize="120%" minsize="120%" id="S3.E5.m1.5.5.1.1.1.3" xref="S3.E5.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.5b"><apply id="S3.E5.m1.5.5.cmml" xref="S3.E5.m1.5.5"><eq id="S3.E5.m1.5.5.2.cmml" xref="S3.E5.m1.5.5.2"></eq><apply id="S3.E5.m1.5.5.3.cmml" xref="S3.E5.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E5.m1.5.5.3.1.cmml" xref="S3.E5.m1.5.5.3">subscript</csymbol><ci id="S3.E5.m1.5.5.3.2.cmml" xref="S3.E5.m1.5.5.3.2">ğ‘¦</ci><list id="S3.E5.m1.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2.2"><apply id="S3.E5.m1.2.2.2.2.1.cmml" xref="S3.E5.m1.2.2.2.2.1"><times id="S3.E5.m1.2.2.2.2.1.1.cmml" xref="S3.E5.m1.2.2.2.2.1.1"></times><ci id="S3.E5.m1.2.2.2.2.1.2.cmml" xref="S3.E5.m1.2.2.2.2.1.2">ğ‘</ci><ci id="S3.E5.m1.2.2.2.2.1.3.cmml" xref="S3.E5.m1.2.2.2.2.1.3">ğ‘¡</ci><ci id="S3.E5.m1.2.2.2.2.1.4.cmml" xref="S3.E5.m1.2.2.2.2.1.4">ğ‘¡</ci></apply><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">ğ‘›</ci></list></apply><apply id="S3.E5.m1.5.5.1.cmml" xref="S3.E5.m1.5.5.1"><times id="S3.E5.m1.5.5.1.2.cmml" xref="S3.E5.m1.5.5.1.2"></times><ci id="S3.E5.m1.5.5.1.3.cmml" xref="S3.E5.m1.5.5.1.3">PReLU</ci><apply id="S3.E5.m1.5.5.1.1.1.1.cmml" xref="S3.E5.m1.5.5.1.1.1"><times id="S3.E5.m1.5.5.1.1.1.1.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1.1"></times><apply id="S3.E5.m1.5.5.1.1.1.1.2.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.5.5.1.1.1.1.2.2.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2.2">ğ‘Š</ci><apply id="S3.E5.m1.5.5.1.1.1.1.2.3.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2.3"><times id="S3.E5.m1.5.5.1.1.1.1.2.3.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2.3.1"></times><ci id="S3.E5.m1.5.5.1.1.1.1.2.3.2.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2.3.2">ğ‘œ</ci><ci id="S3.E5.m1.5.5.1.1.1.1.2.3.3.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2.3.3">ğ‘¢</ci><ci id="S3.E5.m1.5.5.1.1.1.1.2.3.4.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2.3.4">ğ‘¡</ci></apply></apply><ci id="S3.E5.m1.5.5.1.1.1.1.3.cmml" xref="S3.E5.m1.5.5.1.1.1.1.3">ğ‘</ci><ci id="S3.E5.m1.5.5.1.1.1.1.4.cmml" xref="S3.E5.m1.5.5.1.1.1.1.4">ğ‘¡</ci><apply id="S3.E5.m1.5.5.1.1.1.1.5.cmml" xref="S3.E5.m1.5.5.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E5.m1.5.5.1.1.1.1.5.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1.5">subscript</csymbol><ci id="S3.E5.m1.5.5.1.1.1.1.5.2.cmml" xref="S3.E5.m1.5.5.1.1.1.1.5.2">ğ‘¡</ci><list id="S3.E5.m1.4.4.2.3.cmml" xref="S3.E5.m1.4.4.2.4"><ci id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1.1">ğ‘</ci><ci id="S3.E5.m1.4.4.2.2.cmml" xref="S3.E5.m1.4.4.2.2">ğ‘›</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.5c">y_{att,n}=\mathrm{PReLU}\ \big{(}W_{out}\ att_{a,n}\big{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.4" class="ltx_p">where <math id="S3.SS1.p4.3.m1.1" class="ltx_Math" alttext="W_{out}" display="inline"><semantics id="S3.SS1.p4.3.m1.1a"><msub id="S3.SS1.p4.3.m1.1.1" xref="S3.SS1.p4.3.m1.1.1.cmml"><mi id="S3.SS1.p4.3.m1.1.1.2" xref="S3.SS1.p4.3.m1.1.1.2.cmml">W</mi><mrow id="S3.SS1.p4.3.m1.1.1.3" xref="S3.SS1.p4.3.m1.1.1.3.cmml"><mi id="S3.SS1.p4.3.m1.1.1.3.2" xref="S3.SS1.p4.3.m1.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m1.1.1.3.1" xref="S3.SS1.p4.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p4.3.m1.1.1.3.3" xref="S3.SS1.p4.3.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m1.1.1.3.1a" xref="S3.SS1.p4.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p4.3.m1.1.1.3.4" xref="S3.SS1.p4.3.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m1.1b"><apply id="S3.SS1.p4.3.m1.1.1.cmml" xref="S3.SS1.p4.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m1.1.1.1.cmml" xref="S3.SS1.p4.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.3.m1.1.1.2.cmml" xref="S3.SS1.p4.3.m1.1.1.2">ğ‘Š</ci><apply id="S3.SS1.p4.3.m1.1.1.3.cmml" xref="S3.SS1.p4.3.m1.1.1.3"><times id="S3.SS1.p4.3.m1.1.1.3.1.cmml" xref="S3.SS1.p4.3.m1.1.1.3.1"></times><ci id="S3.SS1.p4.3.m1.1.1.3.2.cmml" xref="S3.SS1.p4.3.m1.1.1.3.2">ğ‘œ</ci><ci id="S3.SS1.p4.3.m1.1.1.3.3.cmml" xref="S3.SS1.p4.3.m1.1.1.3.3">ğ‘¢</ci><ci id="S3.SS1.p4.3.m1.1.1.3.4.cmml" xref="S3.SS1.p4.3.m1.1.1.3.4">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m1.1c">W_{out}</annotation></semantics></math> is a weight vector of the fully connected layer and <math id="S3.SS1.p4.4.m2.2" class="ltx_Math" alttext="y_{att,n}" display="inline"><semantics id="S3.SS1.p4.4.m2.2a"><msub id="S3.SS1.p4.4.m2.2.3" xref="S3.SS1.p4.4.m2.2.3.cmml"><mi id="S3.SS1.p4.4.m2.2.3.2" xref="S3.SS1.p4.4.m2.2.3.2.cmml">y</mi><mrow id="S3.SS1.p4.4.m2.2.2.2.2" xref="S3.SS1.p4.4.m2.2.2.2.3.cmml"><mrow id="S3.SS1.p4.4.m2.2.2.2.2.1" xref="S3.SS1.p4.4.m2.2.2.2.2.1.cmml"><mi id="S3.SS1.p4.4.m2.2.2.2.2.1.2" xref="S3.SS1.p4.4.m2.2.2.2.2.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.4.m2.2.2.2.2.1.1" xref="S3.SS1.p4.4.m2.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.SS1.p4.4.m2.2.2.2.2.1.3" xref="S3.SS1.p4.4.m2.2.2.2.2.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.4.m2.2.2.2.2.1.1a" xref="S3.SS1.p4.4.m2.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.SS1.p4.4.m2.2.2.2.2.1.4" xref="S3.SS1.p4.4.m2.2.2.2.2.1.4.cmml">t</mi></mrow><mo id="S3.SS1.p4.4.m2.2.2.2.2.2" xref="S3.SS1.p4.4.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p4.4.m2.1.1.1.1" xref="S3.SS1.p4.4.m2.1.1.1.1.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m2.2b"><apply id="S3.SS1.p4.4.m2.2.3.cmml" xref="S3.SS1.p4.4.m2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m2.2.3.1.cmml" xref="S3.SS1.p4.4.m2.2.3">subscript</csymbol><ci id="S3.SS1.p4.4.m2.2.3.2.cmml" xref="S3.SS1.p4.4.m2.2.3.2">ğ‘¦</ci><list id="S3.SS1.p4.4.m2.2.2.2.3.cmml" xref="S3.SS1.p4.4.m2.2.2.2.2"><apply id="S3.SS1.p4.4.m2.2.2.2.2.1.cmml" xref="S3.SS1.p4.4.m2.2.2.2.2.1"><times id="S3.SS1.p4.4.m2.2.2.2.2.1.1.cmml" xref="S3.SS1.p4.4.m2.2.2.2.2.1.1"></times><ci id="S3.SS1.p4.4.m2.2.2.2.2.1.2.cmml" xref="S3.SS1.p4.4.m2.2.2.2.2.1.2">ğ‘</ci><ci id="S3.SS1.p4.4.m2.2.2.2.2.1.3.cmml" xref="S3.SS1.p4.4.m2.2.2.2.2.1.3">ğ‘¡</ci><ci id="S3.SS1.p4.4.m2.2.2.2.2.1.4.cmml" xref="S3.SS1.p4.4.m2.2.2.2.2.1.4">ğ‘¡</ci></apply><ci id="S3.SS1.p4.4.m2.1.1.1.1.cmml" xref="S3.SS1.p4.4.m2.1.1.1.1">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m2.2c">y_{att,n}</annotation></semantics></math> is the output of the RAU.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Input Representation</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image representation (IR)</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.4" class="ltx_p">In our baseline and ablation studies, we use two types of image representations. First, a 152-layer â€œResNetâ€ pretrained CNN from <cite class="ltx_cite ltx_citemacro_citet">He etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite> to extract image features. Similar to <cite class="ltx_cite ltx_citemacro_citep">(Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>; Nam etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>, we resize the images to <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="448\times 448" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">448</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">448</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">448</cn><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">448</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">448\times 448</annotation></semantics></math> and extract the last layer before the final pooling layer (res5c) with size <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="2048\times 14\times 14" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">2048</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.4" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.4.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><times id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">2048</cn><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">14</cn><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.4">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">2048\times 14\times 14</annotation></semantics></math>. Finally, we use <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">l</mi><mn id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">ğ‘™</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">l_{2}</annotation></semantics></math> normalization on all dimensions.
Recently, <cite class="ltx_cite ltx_citemacro_citet">Anderson etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite> have shown that object-level features can provide a significant performance uplift compared to global-level features from pretrained CNNs. Therefore, we use Faster R-CNN features <cite class="ltx_cite ltx_citemacro_citep">(Ren etÂ al., <a href="#bib.bib31" title="" class="ltx_ref">2015</a>)</cite> with a fixed number of proposals per image (<math id="S3.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="K=36" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">K</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">36</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><eq id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1"></eq><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2">ğ¾</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3">36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">K=36</annotation></semantics></math>) for the DRAU model and its variants.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Question representation (QR)</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">We use a fairly similar representation as <cite class="ltx_cite ltx_citemacro_cite">Fukui etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>. In short, the question is tokenized and encoded using an embedding layer followed by a <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1a" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.4" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1b" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.5" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2">ğ‘¡</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.4.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.4">ğ‘›</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.5.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.5">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">tanh</annotation></semantics></math> activation. We also exploit pretrained GloVe vectors <cite class="ltx_cite ltx_citemacro_citep">(Pennington etÂ al., <a href="#bib.bib30" title="" class="ltx_ref">2014</a>)</cite> and concatenate them with the output of the embedding layer. The concatenated vector is fed to a two-layer unidirectional LSTM that contains 1024 hidden states each. In contrast to <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib9" title="" class="ltx_ref">Fukui etÂ al.</a></cite>, we use all the hidden states of both LSTMs rather than concatenating the final states to represent the final question representation.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Use of <math id="S3.SS3.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.1.m1.1b"><mrow id="S3.SS3.1.m1.1.1" xref="S3.SS3.1.m1.1.1.cmml"><mn id="S3.SS3.1.m1.1.1.2" xref="S3.SS3.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.1.m1.1.1.1" xref="S3.SS3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.1.m1.1.1.3" xref="S3.SS3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.1.m1.1c"><apply id="S3.SS3.1.m1.1.1.cmml" xref="S3.SS3.1.m1.1.1"><times id="S3.SS3.1.m1.1.1.1.cmml" xref="S3.SS3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.1.m1.1.1.2.cmml" xref="S3.SS3.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS3.1.m1.1.1.3.cmml" xref="S3.SS3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.1.m1.1d">1\times 1</annotation></semantics></math> Convolution and PReLU</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">We apply multiple <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">1\times 1</annotation></semantics></math> convolution layers in the network for mainly two reasons. First, they learn weights from the image and question representations in the early layers. This is important especially for the image representation, since it was originally trained for a different task. This is also true for the question representation to a lesser degree (GloVe vectors are trained on co-occurrence statistics) Second, they are used to generate a common representation size. To obtain a joint representation (JR), we apply <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">1\times 1</annotation></semantics></math> convolutions followed by PReLU activations on both the image and question representations. Through empirical evidence, PReLU activations were found to reduce training time significantly and improve performance compared to ReLU and <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1a" xref="S3.SS3.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p1.3.m3.1.1.4" xref="S3.SS3.p1.3.m3.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1b" xref="S3.SS3.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p1.3.m3.1.1.5" xref="S3.SS3.p1.3.m3.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></times><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">ğ‘¡</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">ğ‘</ci><ci id="S3.SS3.p1.3.m3.1.1.4.cmml" xref="S3.SS3.p1.3.m3.1.1.4">ğ‘›</ci><ci id="S3.SS3.p1.3.m3.1.1.5.cmml" xref="S3.SS3.p1.3.m3.1.1.5">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">tanh</annotation></semantics></math> activations.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Fusion operation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">A fusion operation is used to merge the textual and visual branches. For DRAU, we use MCB <cite class="ltx_cite ltx_citemacro_citep">(Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>; Gao etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Reasoning module (RM)</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The result of the fusion is given to a many-class classifier using the top 3000 frequent answers. We use a single-layer softmax with cross-entropy loss. This can be written as:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E6.m1.1" class="ltx_Math" alttext="P_{a}=\mathrm{softmax}\Big{(}\mathrm{fusion\_op}\big{(}y_{text},y_{vis}\big{)}W_{ans}\Big{)}" display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><msub id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.3.2" xref="S3.E6.m1.1.1.3.2.cmml">P</mi><mi id="S3.E6.m1.1.1.3.3" xref="S3.E6.m1.1.1.3.3.cmml">a</mi></msub><mo id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.1.1.1.3.cmml">softmax</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mo maxsize="160%" minsize="160%" id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.4" xref="S3.E6.m1.1.1.1.1.1.1.4.cmml">fusion</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">â€‹</mo><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.1.1.5" xref="S3.E6.m1.1.1.1.1.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.3a" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.6" xref="S3.E6.m1.1.1.1.1.1.1.6.cmml">op</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.3b" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">â€‹</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml"><mo maxsize="120%" minsize="120%" id="S3.E6.m1.1.1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1a" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.4" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1b" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.5" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.5.cmml">t</mi></mrow></msub><mo id="S3.E6.m1.1.1.1.1.1.1.2.2.4" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E6.m1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.2.cmml">y</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.1" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.1a" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.4" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.4.cmml">s</mi></mrow></msub><mo maxsize="120%" minsize="120%" id="S3.E6.m1.1.1.1.1.1.1.2.2.5" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.3c" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">â€‹</mo><msub id="S3.E6.m1.1.1.1.1.1.1.7" xref="S3.E6.m1.1.1.1.1.1.1.7.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.7.2" xref="S3.E6.m1.1.1.1.1.1.1.7.2.cmml">W</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.7.3" xref="S3.E6.m1.1.1.1.1.1.1.7.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.7.3.2" xref="S3.E6.m1.1.1.1.1.1.1.7.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.7.3.1" xref="S3.E6.m1.1.1.1.1.1.1.7.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.7.3.3" xref="S3.E6.m1.1.1.1.1.1.1.7.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.7.3.1a" xref="S3.E6.m1.1.1.1.1.1.1.7.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.7.3.4" xref="S3.E6.m1.1.1.1.1.1.1.7.3.4.cmml">s</mi></mrow></msub></mrow><mo maxsize="160%" minsize="160%" id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"></eq><apply id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.3.2">ğ‘ƒ</ci><ci id="S3.E6.m1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.3.3">ğ‘</ci></apply><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><times id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"></times><ci id="S3.E6.m1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.3">softmax</ci><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3"></times><ci id="S3.E6.m1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.4">fusion</ci><ci id="S3.E6.m1.1.1.1.1.1.1.5.cmml" xref="S3.E6.m1.1.1.1.1.1.1.5">_</ci><ci id="S3.E6.m1.1.1.1.1.1.1.6.cmml" xref="S3.E6.m1.1.1.1.1.1.1.6">op</ci><interval closure="open" id="S3.E6.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2"><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¡</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.3">ğ‘’</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.4">ğ‘¥</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.5">ğ‘¡</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.2">ğ‘¦</ci><apply id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3"><times id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.2">ğ‘£</ci><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.3">ğ‘–</ci><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.3.4">ğ‘ </ci></apply></apply></interval><apply id="S3.E6.m1.1.1.1.1.1.1.7.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.7.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.7.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7.2">ğ‘Š</ci><apply id="S3.E6.m1.1.1.1.1.1.1.7.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7.3"><times id="S3.E6.m1.1.1.1.1.1.1.7.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.1.7.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7.3.2">ğ‘</ci><ci id="S3.E6.m1.1.1.1.1.1.1.7.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7.3.3">ğ‘›</ci><ci id="S3.E6.m1.1.1.1.1.1.1.7.3.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.7.3.4">ğ‘ </ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">P_{a}=\mathrm{softmax}\Big{(}\mathrm{fusion\_op}\big{(}y_{text},y_{vis}\big{)}W_{ans}\Big{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.4" class="ltx_p">where <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="y_{text}" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><msub id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">y</mi><mrow id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3.cmml"><mi id="S3.SS5.p2.1.m1.1.1.3.2" xref="S3.SS5.p2.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.1" xref="S3.SS5.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.1.m1.1.1.3.3" xref="S3.SS5.p2.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.1a" xref="S3.SS5.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.1.m1.1.1.3.4" xref="S3.SS5.p2.1.m1.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.1b" xref="S3.SS5.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.1.m1.1.1.3.5" xref="S3.SS5.p2.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">ğ‘¦</ci><apply id="S3.SS5.p2.1.m1.1.1.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3"><times id="S3.SS5.p2.1.m1.1.1.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3.1"></times><ci id="S3.SS5.p2.1.m1.1.1.3.2.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2">ğ‘¡</ci><ci id="S3.SS5.p2.1.m1.1.1.3.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3">ğ‘’</ci><ci id="S3.SS5.p2.1.m1.1.1.3.4.cmml" xref="S3.SS5.p2.1.m1.1.1.3.4">ğ‘¥</ci><ci id="S3.SS5.p2.1.m1.1.1.3.5.cmml" xref="S3.SS5.p2.1.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">y_{text}</annotation></semantics></math> and <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="y_{vis}" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><msub id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">y</mi><mrow id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml"><mi id="S3.SS5.p2.2.m2.1.1.3.2" xref="S3.SS5.p2.2.m2.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.2.m2.1.1.3.3" xref="S3.SS5.p2.2.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.3.1a" xref="S3.SS5.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.2.m2.1.1.3.4" xref="S3.SS5.p2.2.m2.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">ğ‘¦</ci><apply id="S3.SS5.p2.2.m2.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3"><times id="S3.SS5.p2.2.m2.1.1.3.1.cmml" xref="S3.SS5.p2.2.m2.1.1.3.1"></times><ci id="S3.SS5.p2.2.m2.1.1.3.2.cmml" xref="S3.SS5.p2.2.m2.1.1.3.2">ğ‘£</ci><ci id="S3.SS5.p2.2.m2.1.1.3.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS5.p2.2.m2.1.1.3.4.cmml" xref="S3.SS5.p2.2.m2.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">y_{vis}</annotation></semantics></math> are the outputs of the RAUs, <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="W_{ans}" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><msub id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml"><mi id="S3.SS5.p2.3.m3.1.1.2" xref="S3.SS5.p2.3.m3.1.1.2.cmml">W</mi><mrow id="S3.SS5.p2.3.m3.1.1.3" xref="S3.SS5.p2.3.m3.1.1.3.cmml"><mi id="S3.SS5.p2.3.m3.1.1.3.2" xref="S3.SS5.p2.3.m3.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.1" xref="S3.SS5.p2.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.3.m3.1.1.3.3" xref="S3.SS5.p2.3.m3.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.3.1a" xref="S3.SS5.p2.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.3.m3.1.1.3.4" xref="S3.SS5.p2.3.m3.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.3.m3.1.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p2.3.m3.1.1.2.cmml" xref="S3.SS5.p2.3.m3.1.1.2">ğ‘Š</ci><apply id="S3.SS5.p2.3.m3.1.1.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3"><times id="S3.SS5.p2.3.m3.1.1.3.1.cmml" xref="S3.SS5.p2.3.m3.1.1.3.1"></times><ci id="S3.SS5.p2.3.m3.1.1.3.2.cmml" xref="S3.SS5.p2.3.m3.1.1.3.2">ğ‘</ci><ci id="S3.SS5.p2.3.m3.1.1.3.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3.3">ğ‘›</ci><ci id="S3.SS5.p2.3.m3.1.1.3.4.cmml" xref="S3.SS5.p2.3.m3.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">W_{ans}</annotation></semantics></math> represents the weights of the multi-way classifier, and <math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="P_{a}" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><msub id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml"><mi id="S3.SS5.p2.4.m4.1.1.2" xref="S3.SS5.p2.4.m4.1.1.2.cmml">P</mi><mi id="S3.SS5.p2.4.m4.1.1.3" xref="S3.SS5.p2.4.m4.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><apply id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.p2.4.m4.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1.2">ğ‘ƒ</ci><ci id="S3.SS5.p2.4.m4.1.1.3.cmml" xref="S3.SS5.p2.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">P_{a}</annotation></semantics></math> is the probability of the top 3000 frequent answers.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">The final answer <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="\hat{a}" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mover accent="true" id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml">a</mi><mo id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><ci id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1">^</ci><ci id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\hat{a}</annotation></semantics></math> is chosen according to the following:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\hat{a}=\operatorname*{arg\,max}P_{a}" display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mover accent="true" id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml"><mi id="S3.E7.m1.1.1.2.2" xref="S3.E7.m1.1.1.2.2.cmml">a</mi><mo id="S3.E7.m1.1.1.2.1" xref="S3.E7.m1.1.1.2.1.cmml">^</mo></mover><mo id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mrow id="S3.E7.m1.1.1.3.1" xref="S3.E7.m1.1.1.3.1.cmml"><mi id="S3.E7.m1.1.1.3.1.2" xref="S3.E7.m1.1.1.3.1.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E7.m1.1.1.3.1.1" xref="S3.E7.m1.1.1.3.1.1.cmml">â€‹</mo><mi id="S3.E7.m1.1.1.3.1.3" xref="S3.E7.m1.1.1.3.1.3.cmml">max</mi></mrow><mo id="S3.E7.m1.1.1.3a" xref="S3.E7.m1.1.1.3.cmml">â¡</mo><msub id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><mi id="S3.E7.m1.1.1.3.2.2" xref="S3.E7.m1.1.1.3.2.2.cmml">P</mi><mi id="S3.E7.m1.1.1.3.2.3" xref="S3.E7.m1.1.1.3.2.3.cmml">a</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"></eq><apply id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"><ci id="S3.E7.m1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.2.1">^</ci><ci id="S3.E7.m1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.2.2">ğ‘</ci></apply><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><apply id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3.1"><times id="S3.E7.m1.1.1.3.1.1.cmml" xref="S3.E7.m1.1.1.3.1.1"></times><ci id="S3.E7.m1.1.1.3.1.2.cmml" xref="S3.E7.m1.1.1.3.1.2">arg</ci><ci id="S3.E7.m1.1.1.3.1.3.cmml" xref="S3.E7.m1.1.1.3.1.3">max</ci></apply><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2">ğ‘ƒ</ci><ci id="S3.E7.m1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.3.2.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\hat{a}=\operatorname*{arg\,max}P_{a}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Experiments are performed on the VQA 1.0 and 2.0 datasets <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>; Antol etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>. These datasets use images from the MS-COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite> and generate questions and labels (10 labels per question) using Amazonâ€™s Mechanical Turk. Compared to VQA 1.0, VQA 2.0 adds more image-question pairs to balance the language prior present in the VQA 1.0 dataset <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>. The ground truth answers in the VQA dataset are evaluated using human consensus:</p>
<table id="S4.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E8.m1.4" class="ltx_Math" alttext="\mathrm{Acc}(a)=\min\bigg{(}\frac{\sum{a\text{ is in human annotation}}}{3},1\bigg{)}" display="block"><semantics id="S4.E8.m1.4a"><mrow id="S4.E8.m1.4.5" xref="S4.E8.m1.4.5.cmml"><mrow id="S4.E8.m1.4.5.2" xref="S4.E8.m1.4.5.2.cmml"><mi id="S4.E8.m1.4.5.2.2" xref="S4.E8.m1.4.5.2.2.cmml">Acc</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.4.5.2.1" xref="S4.E8.m1.4.5.2.1.cmml">â€‹</mo><mrow id="S4.E8.m1.4.5.2.3.2" xref="S4.E8.m1.4.5.2.cmml"><mo stretchy="false" id="S4.E8.m1.4.5.2.3.2.1" xref="S4.E8.m1.4.5.2.cmml">(</mo><mi id="S4.E8.m1.1.1" xref="S4.E8.m1.1.1.cmml">a</mi><mo stretchy="false" id="S4.E8.m1.4.5.2.3.2.2" xref="S4.E8.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="S4.E8.m1.4.5.1" xref="S4.E8.m1.4.5.1.cmml">=</mo><mrow id="S4.E8.m1.4.5.3.2" xref="S4.E8.m1.4.5.3.1.cmml"><mi id="S4.E8.m1.2.2" xref="S4.E8.m1.2.2.cmml">min</mi><mo id="S4.E8.m1.4.5.3.2a" xref="S4.E8.m1.4.5.3.1.cmml">â¡</mo><mrow id="S4.E8.m1.4.5.3.2.1" xref="S4.E8.m1.4.5.3.1.cmml"><mo maxsize="210%" minsize="210%" id="S4.E8.m1.4.5.3.2.1.1" xref="S4.E8.m1.4.5.3.1.cmml">(</mo><mfrac id="S4.E8.m1.3.3" xref="S4.E8.m1.3.3.cmml"><mrow id="S4.E8.m1.3.3.2" xref="S4.E8.m1.3.3.2.cmml"><mo id="S4.E8.m1.3.3.2.1" xref="S4.E8.m1.3.3.2.1.cmml">âˆ‘</mo><mrow id="S4.E8.m1.3.3.2.2" xref="S4.E8.m1.3.3.2.2.cmml"><mi id="S4.E8.m1.3.3.2.2.2" xref="S4.E8.m1.3.3.2.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.3.3.2.2.1" xref="S4.E8.m1.3.3.2.2.1.cmml">â€‹</mo><mtext id="S4.E8.m1.3.3.2.2.3" xref="S4.E8.m1.3.3.2.2.3a.cmml">Â is in human annotation</mtext></mrow></mrow><mn id="S4.E8.m1.3.3.3" xref="S4.E8.m1.3.3.3.cmml">3</mn></mfrac><mo id="S4.E8.m1.4.5.3.2.1.2" xref="S4.E8.m1.4.5.3.1.cmml">,</mo><mn id="S4.E8.m1.4.4" xref="S4.E8.m1.4.4.cmml">1</mn><mo maxsize="210%" minsize="210%" id="S4.E8.m1.4.5.3.2.1.3" xref="S4.E8.m1.4.5.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E8.m1.4b"><apply id="S4.E8.m1.4.5.cmml" xref="S4.E8.m1.4.5"><eq id="S4.E8.m1.4.5.1.cmml" xref="S4.E8.m1.4.5.1"></eq><apply id="S4.E8.m1.4.5.2.cmml" xref="S4.E8.m1.4.5.2"><times id="S4.E8.m1.4.5.2.1.cmml" xref="S4.E8.m1.4.5.2.1"></times><ci id="S4.E8.m1.4.5.2.2.cmml" xref="S4.E8.m1.4.5.2.2">Acc</ci><ci id="S4.E8.m1.1.1.cmml" xref="S4.E8.m1.1.1">ğ‘</ci></apply><apply id="S4.E8.m1.4.5.3.1.cmml" xref="S4.E8.m1.4.5.3.2"><min id="S4.E8.m1.2.2.cmml" xref="S4.E8.m1.2.2"></min><apply id="S4.E8.m1.3.3.cmml" xref="S4.E8.m1.3.3"><divide id="S4.E8.m1.3.3.1.cmml" xref="S4.E8.m1.3.3"></divide><apply id="S4.E8.m1.3.3.2.cmml" xref="S4.E8.m1.3.3.2"><sum id="S4.E8.m1.3.3.2.1.cmml" xref="S4.E8.m1.3.3.2.1"></sum><apply id="S4.E8.m1.3.3.2.2.cmml" xref="S4.E8.m1.3.3.2.2"><times id="S4.E8.m1.3.3.2.2.1.cmml" xref="S4.E8.m1.3.3.2.2.1"></times><ci id="S4.E8.m1.3.3.2.2.2.cmml" xref="S4.E8.m1.3.3.2.2.2">ğ‘</ci><ci id="S4.E8.m1.3.3.2.2.3a.cmml" xref="S4.E8.m1.3.3.2.2.3"><mtext id="S4.E8.m1.3.3.2.2.3.cmml" xref="S4.E8.m1.3.3.2.2.3">Â is in human annotation</mtext></ci></apply></apply><cn type="integer" id="S4.E8.m1.3.3.3.cmml" xref="S4.E8.m1.3.3.3">3</cn></apply><cn type="integer" id="S4.E8.m1.4.4.cmml" xref="S4.E8.m1.4.4">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8.m1.4c">\mathrm{Acc}(a)=\min\bigg{(}\frac{\sum{a\text{ is in human annotation}}}{3},1\bigg{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We evaluate our results on the <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">validation</span>, <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">test-dev</span>, <span id="S4.p2.1.3" class="ltx_text ltx_font_italic">test-std</span> splits of each dataset. We test on multiple splits due to the fact that submissions to the test server are limited (for test and test-dev splits). Thus, we mainly evaluate models on the validation split unless comparing results with the state-of-the-art.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.3" class="ltx_p">To train our model, we use Adam <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib18" title="" class="ltx_ref">2014</a>)</cite> for optimization with <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9\text{, }\beta_{2}=0.999" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><msub id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml"><mi id="S4.p3.1.m1.1.1.2.2" xref="S4.p3.1.m1.1.1.2.2.cmml">Î²</mi><mn id="S4.p3.1.m1.1.1.2.3" xref="S4.p3.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">=</mo><mrow id="S4.p3.1.m1.1.1.4" xref="S4.p3.1.m1.1.1.4.cmml"><mn id="S4.p3.1.m1.1.1.4.2" xref="S4.p3.1.m1.1.1.4.2.cmml">0.9</mn><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.4.1" xref="S4.p3.1.m1.1.1.4.1.cmml">â€‹</mo><mtext id="S4.p3.1.m1.1.1.4.3" xref="S4.p3.1.m1.1.1.4.3a.cmml">,Â </mtext><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.4.1a" xref="S4.p3.1.m1.1.1.4.1.cmml">â€‹</mo><msub id="S4.p3.1.m1.1.1.4.4" xref="S4.p3.1.m1.1.1.4.4.cmml"><mi id="S4.p3.1.m1.1.1.4.4.2" xref="S4.p3.1.m1.1.1.4.4.2.cmml">Î²</mi><mn id="S4.p3.1.m1.1.1.4.4.3" xref="S4.p3.1.m1.1.1.4.4.3.cmml">2</mn></msub></mrow><mo id="S4.p3.1.m1.1.1.5" xref="S4.p3.1.m1.1.1.5.cmml">=</mo><mn id="S4.p3.1.m1.1.1.6" xref="S4.p3.1.m1.1.1.6.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><and id="S4.p3.1.m1.1.1a.cmml" xref="S4.p3.1.m1.1.1"></and><apply id="S4.p3.1.m1.1.1b.cmml" xref="S4.p3.1.m1.1.1"><eq id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"></eq><apply id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.2.1.cmml" xref="S4.p3.1.m1.1.1.2">subscript</csymbol><ci id="S4.p3.1.m1.1.1.2.2.cmml" xref="S4.p3.1.m1.1.1.2.2">ğ›½</ci><cn type="integer" id="S4.p3.1.m1.1.1.2.3.cmml" xref="S4.p3.1.m1.1.1.2.3">1</cn></apply><apply id="S4.p3.1.m1.1.1.4.cmml" xref="S4.p3.1.m1.1.1.4"><times id="S4.p3.1.m1.1.1.4.1.cmml" xref="S4.p3.1.m1.1.1.4.1"></times><cn type="float" id="S4.p3.1.m1.1.1.4.2.cmml" xref="S4.p3.1.m1.1.1.4.2">0.9</cn><ci id="S4.p3.1.m1.1.1.4.3a.cmml" xref="S4.p3.1.m1.1.1.4.3"><mtext id="S4.p3.1.m1.1.1.4.3.cmml" xref="S4.p3.1.m1.1.1.4.3">,Â </mtext></ci><apply id="S4.p3.1.m1.1.1.4.4.cmml" xref="S4.p3.1.m1.1.1.4.4"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.4.4.1.cmml" xref="S4.p3.1.m1.1.1.4.4">subscript</csymbol><ci id="S4.p3.1.m1.1.1.4.4.2.cmml" xref="S4.p3.1.m1.1.1.4.4.2">ğ›½</ci><cn type="integer" id="S4.p3.1.m1.1.1.4.4.3.cmml" xref="S4.p3.1.m1.1.1.4.4.3">2</cn></apply></apply></apply><apply id="S4.p3.1.m1.1.1c.cmml" xref="S4.p3.1.m1.1.1"><eq id="S4.p3.1.m1.1.1.5.cmml" xref="S4.p3.1.m1.1.1.5"></eq><share href="#S4.p3.1.m1.1.1.4.cmml" id="S4.p3.1.m1.1.1d.cmml" xref="S4.p3.1.m1.1.1"></share><cn type="float" id="S4.p3.1.m1.1.1.6.cmml" xref="S4.p3.1.m1.1.1.6">0.999</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\beta_{1}=0.9\text{, }\beta_{2}=0.999</annotation></semantics></math>, and an initial learning rate of
<math id="S4.p3.2.m2.1" class="ltx_Math" alttext="\epsilon=7\times 10^{-4}" display="inline"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">Ïµ</mi><mo id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">=</mo><mrow id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml"><mn id="S4.p3.2.m2.1.1.3.2" xref="S4.p3.2.m2.1.1.3.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p3.2.m2.1.1.3.1" xref="S4.p3.2.m2.1.1.3.1.cmml">Ã—</mo><msup id="S4.p3.2.m2.1.1.3.3" xref="S4.p3.2.m2.1.1.3.3.cmml"><mn id="S4.p3.2.m2.1.1.3.3.2" xref="S4.p3.2.m2.1.1.3.3.2.cmml">10</mn><mrow id="S4.p3.2.m2.1.1.3.3.3" xref="S4.p3.2.m2.1.1.3.3.3.cmml"><mo id="S4.p3.2.m2.1.1.3.3.3a" xref="S4.p3.2.m2.1.1.3.3.3.cmml">âˆ’</mo><mn id="S4.p3.2.m2.1.1.3.3.3.2" xref="S4.p3.2.m2.1.1.3.3.3.2.cmml">4</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><eq id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></eq><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">italic-Ïµ</ci><apply id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3"><times id="S4.p3.2.m2.1.1.3.1.cmml" xref="S4.p3.2.m2.1.1.3.1"></times><cn type="integer" id="S4.p3.2.m2.1.1.3.2.cmml" xref="S4.p3.2.m2.1.1.3.2">7</cn><apply id="S4.p3.2.m2.1.1.3.3.cmml" xref="S4.p3.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.3.3.1.cmml" xref="S4.p3.2.m2.1.1.3.3">superscript</csymbol><cn type="integer" id="S4.p3.2.m2.1.1.3.3.2.cmml" xref="S4.p3.2.m2.1.1.3.3.2">10</cn><apply id="S4.p3.2.m2.1.1.3.3.3.cmml" xref="S4.p3.2.m2.1.1.3.3.3"><minus id="S4.p3.2.m2.1.1.3.3.3.1.cmml" xref="S4.p3.2.m2.1.1.3.3.3"></minus><cn type="integer" id="S4.p3.2.m2.1.1.3.3.3.2.cmml" xref="S4.p3.2.m2.1.1.3.3.3.2">4</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\epsilon=7\times 10^{-4}</annotation></semantics></math>. The models are trained with a small batch size of 32 for 400K iterations. We did not fully explore tuning the batch size which explains the relatively high number of training iterations.
Dropout <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="(p=0.3)" display="inline"><semantics id="S4.p3.3.m3.1a"><mrow id="S4.p3.3.m3.1.1.1" xref="S4.p3.3.m3.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.3.m3.1.1.1.2" xref="S4.p3.3.m3.1.1.1.1.cmml">(</mo><mrow id="S4.p3.3.m3.1.1.1.1" xref="S4.p3.3.m3.1.1.1.1.cmml"><mi id="S4.p3.3.m3.1.1.1.1.2" xref="S4.p3.3.m3.1.1.1.1.2.cmml">p</mi><mo id="S4.p3.3.m3.1.1.1.1.1" xref="S4.p3.3.m3.1.1.1.1.1.cmml">=</mo><mn id="S4.p3.3.m3.1.1.1.1.3" xref="S4.p3.3.m3.1.1.1.1.3.cmml">0.3</mn></mrow><mo stretchy="false" id="S4.p3.3.m3.1.1.1.3" xref="S4.p3.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1"><eq id="S4.p3.3.m3.1.1.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1.1.1"></eq><ci id="S4.p3.3.m3.1.1.1.1.2.cmml" xref="S4.p3.3.m3.1.1.1.1.2">ğ‘</ci><cn type="float" id="S4.p3.3.m3.1.1.1.1.3.cmml" xref="S4.p3.3.m3.1.1.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">(p=0.3)</annotation></semantics></math> is applied after each LSTM and after the fusion operation. All weights are initialized as described in <cite class="ltx_cite ltx_citemacro_citep">(Glorot and Bengio, <a href="#bib.bib11" title="" class="ltx_ref">2010</a>)</cite> except LSTM layers which use an uniform weight distribution.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Since VQA datasets provide 10 answers per image-question pair, we randomly sample one answer at each training iteration.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Simple Net model and ablation study of the DRAU model results on the VQA 2.0 <span id="S4.T1.12.1" class="ltx_text ltx_font_italic">validation</span> split. The number of trainable parameters are shown for each model. IF indicates the type of image features the model uses. VG indicates whether the method uses external data augmentation from the Visual Genome dataset.</figcaption>
<table id="S4.T1.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.10.11.1" class="ltx_tr">
<th id="S4.T1.10.11.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="9">VQA 2.0 Validation Split</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.10.12.1" class="ltx_tr">
<th id="S4.T1.10.12.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Model</th>
<th id="S4.T1.10.12.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"># Parameters</th>
<td id="S4.T1.10.12.1.3" class="ltx_td ltx_align_center ltx_border_t">IF</td>
<td id="S4.T1.10.12.1.4" class="ltx_td ltx_align_center ltx_border_t">Fusion</td>
<td id="S4.T1.10.12.1.5" class="ltx_td ltx_align_center ltx_border_t">VG</td>
<td id="S4.T1.10.12.1.6" class="ltx_td ltx_align_center ltx_border_t">All</td>
<td id="S4.T1.10.12.1.7" class="ltx_td ltx_align_center ltx_border_t">Y/N</td>
<td id="S4.T1.10.12.1.8" class="ltx_td ltx_align_center ltx_border_t">Num.</td>
<td id="S4.T1.10.12.1.9" class="ltx_td ltx_align_center ltx_border_t">Other</td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Simple Net (Conv. Visual Attn.)</th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="37.4\times 10^{9}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mn id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml">37.4</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml"><mn id="S4.T1.1.1.1.m1.1.1.3.2" xref="S4.T1.1.1.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.T1.1.1.1.m1.1.1.3.3" xref="S4.T1.1.1.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><times id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1.1"></times><cn type="float" id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">37.4</cn><apply id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T1.1.1.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.T1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">37.4\times 10^{9}</annotation></semantics></math></th>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">ResNet</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">41.06</td>
<td id="S4.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">66.01</td>
<td id="S4.T1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">28.08</td>
<td id="S4.T1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">25.51</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simple Net w/RVAU</th>
<th id="S4.T1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="37.3\times 10^{9}" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mrow id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><mn id="S4.T1.2.2.1.m1.1.1.2" xref="S4.T1.2.2.1.m1.1.1.2.cmml">37.3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.2.2.1.m1.1.1.1" xref="S4.T1.2.2.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.T1.2.2.1.m1.1.1.3" xref="S4.T1.2.2.1.m1.1.1.3.cmml"><mn id="S4.T1.2.2.1.m1.1.1.3.2" xref="S4.T1.2.2.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.T1.2.2.1.m1.1.1.3.3" xref="S4.T1.2.2.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><times id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1.1"></times><cn type="float" id="S4.T1.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2">37.3</cn><apply id="S4.T1.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.3.1.cmml" xref="S4.T1.2.2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T1.2.2.1.m1.1.1.3.2.cmml" xref="S4.T1.2.2.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.T1.2.2.1.m1.1.1.3.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">37.3\times 10^{9}</annotation></semantics></math></th>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_center">ResNet</td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.2.2.5" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T1.2.2.6" class="ltx_td ltx_align_center">45.12</td>
<td id="S4.T1.2.2.7" class="ltx_td ltx_align_center">66.24</td>
<td id="S4.T1.2.2.8" class="ltx_td ltx_align_center">28.48</td>
<td id="S4.T1.2.2.9" class="ltx_td ltx_align_center">33.46</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<th id="S4.T1.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DCA (Conv. Attn)</th>
<th id="S4.T1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="138.4\times 10^{9}" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mrow id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml"><mn id="S4.T1.3.3.1.m1.1.1.2" xref="S4.T1.3.3.1.m1.1.1.2.cmml">138.4</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.3.3.1.m1.1.1.1" xref="S4.T1.3.3.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.T1.3.3.1.m1.1.1.3" xref="S4.T1.3.3.1.m1.1.1.3.cmml"><mn id="S4.T1.3.3.1.m1.1.1.3.2" xref="S4.T1.3.3.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.T1.3.3.1.m1.1.1.3.3" xref="S4.T1.3.3.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><apply id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"><times id="S4.T1.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1.1"></times><cn type="float" id="S4.T1.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.1.m1.1.1.2">138.4</cn><apply id="S4.T1.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.3.3.1.m1.1.1.3.1.cmml" xref="S4.T1.3.3.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T1.3.3.1.m1.1.1.3.2.cmml" xref="S4.T1.3.3.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.T1.3.3.1.m1.1.1.3.3.cmml" xref="S4.T1.3.3.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">138.4\times 10^{9}</annotation></semantics></math></th>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">FRCNN</td>
<td id="S4.T1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">MCB</td>
<td id="S4.T1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T1.4.4.2.m1.1" class="ltx_Math" alttext="59.42\pm 0.09" display="inline"><semantics id="S4.T1.4.4.2.m1.1a"><mrow id="S4.T1.4.4.2.m1.1.1" xref="S4.T1.4.4.2.m1.1.1.cmml"><mn id="S4.T1.4.4.2.m1.1.1.2" xref="S4.T1.4.4.2.m1.1.1.2.cmml">59.42</mn><mo id="S4.T1.4.4.2.m1.1.1.1" xref="S4.T1.4.4.2.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.4.4.2.m1.1.1.3" xref="S4.T1.4.4.2.m1.1.1.3.cmml">0.09</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.2.m1.1b"><apply id="S4.T1.4.4.2.m1.1.1.cmml" xref="S4.T1.4.4.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.4.4.2.m1.1.1.1.cmml" xref="S4.T1.4.4.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.4.4.2.m1.1.1.2.cmml" xref="S4.T1.4.4.2.m1.1.1.2">59.42</cn><cn type="float" id="S4.T1.4.4.2.m1.1.1.3.cmml" xref="S4.T1.4.4.2.m1.1.1.3">0.09</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.2.m1.1c">59.42\pm 0.09</annotation></semantics></math></td>
<td id="S4.T1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">77.80</td>
<td id="S4.T1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">36.28</td>
<td id="S4.T1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">51.57</td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<th id="S4.T1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">DCA w/RVAU</th>
<th id="S4.T1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="138.4\times 10^{9}" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mrow id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml"><mn id="S4.T1.5.5.1.m1.1.1.2" xref="S4.T1.5.5.1.m1.1.1.2.cmml">138.4</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.5.5.1.m1.1.1.1" xref="S4.T1.5.5.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.T1.5.5.1.m1.1.1.3" xref="S4.T1.5.5.1.m1.1.1.3.cmml"><mn id="S4.T1.5.5.1.m1.1.1.3.2" xref="S4.T1.5.5.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.T1.5.5.1.m1.1.1.3.3" xref="S4.T1.5.5.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><apply id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"><times id="S4.T1.5.5.1.m1.1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1.1"></times><cn type="float" id="S4.T1.5.5.1.m1.1.1.2.cmml" xref="S4.T1.5.5.1.m1.1.1.2">138.4</cn><apply id="S4.T1.5.5.1.m1.1.1.3.cmml" xref="S4.T1.5.5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.5.5.1.m1.1.1.3.1.cmml" xref="S4.T1.5.5.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T1.5.5.1.m1.1.1.3.2.cmml" xref="S4.T1.5.5.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.T1.5.5.1.m1.1.1.3.3.cmml" xref="S4.T1.5.5.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">138.4\times 10^{9}</annotation></semantics></math></th>
<td id="S4.T1.6.6.4" class="ltx_td ltx_align_center">FRCNN</td>
<td id="S4.T1.6.6.5" class="ltx_td ltx_align_center">MCB</td>
<td id="S4.T1.6.6.6" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.6.6.2" class="ltx_td ltx_align_center"><math id="S4.T1.6.6.2.m1.1" class="ltx_Math" alttext="60.90\pm 0.02" display="inline"><semantics id="S4.T1.6.6.2.m1.1a"><mrow id="S4.T1.6.6.2.m1.1.1" xref="S4.T1.6.6.2.m1.1.1.cmml"><mn id="S4.T1.6.6.2.m1.1.1.2" xref="S4.T1.6.6.2.m1.1.1.2.cmml">60.90</mn><mo id="S4.T1.6.6.2.m1.1.1.1" xref="S4.T1.6.6.2.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.6.6.2.m1.1.1.3" xref="S4.T1.6.6.2.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.2.m1.1b"><apply id="S4.T1.6.6.2.m1.1.1.cmml" xref="S4.T1.6.6.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.6.6.2.m1.1.1.1.cmml" xref="S4.T1.6.6.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.6.6.2.m1.1.1.2.cmml" xref="S4.T1.6.6.2.m1.1.1.2">60.90</cn><cn type="float" id="S4.T1.6.6.2.m1.1.1.3.cmml" xref="S4.T1.6.6.2.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.2.m1.1c">60.90\pm 0.02</annotation></semantics></math></td>
<td id="S4.T1.6.6.7" class="ltx_td ltx_align_center">79.04</td>
<td id="S4.T1.6.6.8" class="ltx_td ltx_align_center">39.63</td>
<td id="S4.T1.6.6.9" class="ltx_td ltx_align_center">52.73</td>
</tr>
<tr id="S4.T1.8.8" class="ltx_tr">
<th id="S4.T1.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">DCA w/RTAU</th>
<th id="S4.T1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T1.7.7.1.m1.1" class="ltx_Math" alttext="138.4\times 10^{9}" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mrow id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml"><mn id="S4.T1.7.7.1.m1.1.1.2" xref="S4.T1.7.7.1.m1.1.1.2.cmml">138.4</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.7.7.1.m1.1.1.1" xref="S4.T1.7.7.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.T1.7.7.1.m1.1.1.3" xref="S4.T1.7.7.1.m1.1.1.3.cmml"><mn id="S4.T1.7.7.1.m1.1.1.3.2" xref="S4.T1.7.7.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.T1.7.7.1.m1.1.1.3.3" xref="S4.T1.7.7.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><apply id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1"><times id="S4.T1.7.7.1.m1.1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1.1"></times><cn type="float" id="S4.T1.7.7.1.m1.1.1.2.cmml" xref="S4.T1.7.7.1.m1.1.1.2">138.4</cn><apply id="S4.T1.7.7.1.m1.1.1.3.cmml" xref="S4.T1.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.7.7.1.m1.1.1.3.1.cmml" xref="S4.T1.7.7.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T1.7.7.1.m1.1.1.3.2.cmml" xref="S4.T1.7.7.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.T1.7.7.1.m1.1.1.3.3.cmml" xref="S4.T1.7.7.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">138.4\times 10^{9}</annotation></semantics></math></th>
<td id="S4.T1.8.8.4" class="ltx_td ltx_align_center">FRCNN</td>
<td id="S4.T1.8.8.5" class="ltx_td ltx_align_center">MCB</td>
<td id="S4.T1.8.8.6" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.8.8.2" class="ltx_td ltx_align_center"><math id="S4.T1.8.8.2.m1.1" class="ltx_Math" alttext="58.76\pm 0.08" display="inline"><semantics id="S4.T1.8.8.2.m1.1a"><mrow id="S4.T1.8.8.2.m1.1.1" xref="S4.T1.8.8.2.m1.1.1.cmml"><mn id="S4.T1.8.8.2.m1.1.1.2" xref="S4.T1.8.8.2.m1.1.1.2.cmml">58.76</mn><mo id="S4.T1.8.8.2.m1.1.1.1" xref="S4.T1.8.8.2.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.8.8.2.m1.1.1.3" xref="S4.T1.8.8.2.m1.1.1.3.cmml">0.08</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.2.m1.1b"><apply id="S4.T1.8.8.2.m1.1.1.cmml" xref="S4.T1.8.8.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.8.8.2.m1.1.1.1.cmml" xref="S4.T1.8.8.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.8.8.2.m1.1.1.2.cmml" xref="S4.T1.8.8.2.m1.1.1.2">58.76</cn><cn type="float" id="S4.T1.8.8.2.m1.1.1.3.cmml" xref="S4.T1.8.8.2.m1.1.1.3">0.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.2.m1.1c">58.76\pm 0.08</annotation></semantics></math></td>
<td id="S4.T1.8.8.7" class="ltx_td ltx_align_center">77.56</td>
<td id="S4.T1.8.8.8" class="ltx_td ltx_align_center">35.54</td>
<td id="S4.T1.8.8.9" class="ltx_td ltx_align_center">50.61</td>
</tr>
<tr id="S4.T1.10.10" class="ltx_tr">
<th id="S4.T1.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">DRAU</th>
<th id="S4.T1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><math id="S4.T1.9.9.1.m1.1" class="ltx_Math" alttext="138.4\times 10^{9}" display="inline"><semantics id="S4.T1.9.9.1.m1.1a"><mrow id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml"><mn id="S4.T1.9.9.1.m1.1.1.2" xref="S4.T1.9.9.1.m1.1.1.2.cmml">138.4</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.9.9.1.m1.1.1.1" xref="S4.T1.9.9.1.m1.1.1.1.cmml">Ã—</mo><msup id="S4.T1.9.9.1.m1.1.1.3" xref="S4.T1.9.9.1.m1.1.1.3.cmml"><mn id="S4.T1.9.9.1.m1.1.1.3.2" xref="S4.T1.9.9.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.T1.9.9.1.m1.1.1.3.3" xref="S4.T1.9.9.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><apply id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1"><times id="S4.T1.9.9.1.m1.1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1.1"></times><cn type="float" id="S4.T1.9.9.1.m1.1.1.2.cmml" xref="S4.T1.9.9.1.m1.1.1.2">138.4</cn><apply id="S4.T1.9.9.1.m1.1.1.3.cmml" xref="S4.T1.9.9.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.9.9.1.m1.1.1.3.1.cmml" xref="S4.T1.9.9.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T1.9.9.1.m1.1.1.3.2.cmml" xref="S4.T1.9.9.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.T1.9.9.1.m1.1.1.3.3.cmml" xref="S4.T1.9.9.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">138.4\times 10^{9}</annotation></semantics></math></th>
<td id="S4.T1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb">FRCNN</td>
<td id="S4.T1.10.10.5" class="ltx_td ltx_align_center ltx_border_bb">MCB</td>
<td id="S4.T1.10.10.6" class="ltx_td ltx_align_center ltx_border_bb">âœ—</td>
<td id="S4.T1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T1.10.10.2.m1.1" class="ltx_Math" alttext="61.36\pm 0.02" display="inline"><semantics id="S4.T1.10.10.2.m1.1a"><mrow id="S4.T1.10.10.2.m1.1.1" xref="S4.T1.10.10.2.m1.1.1.cmml"><mn id="S4.T1.10.10.2.m1.1.1.2" xref="S4.T1.10.10.2.m1.1.1.2.cmml">61.36</mn><mo id="S4.T1.10.10.2.m1.1.1.1" xref="S4.T1.10.10.2.m1.1.1.1.cmml">Â±</mo><mn id="S4.T1.10.10.2.m1.1.1.3" xref="S4.T1.10.10.2.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.2.m1.1b"><apply id="S4.T1.10.10.2.m1.1.1.cmml" xref="S4.T1.10.10.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.10.10.2.m1.1.1.1.cmml" xref="S4.T1.10.10.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.10.10.2.m1.1.1.2.cmml" xref="S4.T1.10.10.2.m1.1.1.2">61.36</cn><cn type="float" id="S4.T1.10.10.2.m1.1.1.3.cmml" xref="S4.T1.10.10.2.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.2.m1.1c">61.36\pm 0.02</annotation></semantics></math></td>
<td id="S4.T1.10.10.7" class="ltx_td ltx_align_center ltx_border_bb">79.74</td>
<td id="S4.T1.10.10.8" class="ltx_td ltx_align_center ltx_border_bb">40.03</td>
<td id="S4.T1.10.10.9" class="ltx_td ltx_align_center ltx_border_bb">53.03</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Attention ablation studies</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Convolutional versus recurrent attention</h4>

<figure id="S4.F3" class="ltx_figure"><img src="/html/1802.00209/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 3: </span>Simple Net architecture. This model uses components from the DRAU model (<a href="#S3.F2" title="In 3 Dual Recurrent Attention in VQA â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>) to be used as a baseline for attention evaluation. Multimodal features are concatenated (<math id="S4.F3.2.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S4.F3.2.m1.1b"><mo id="S4.F3.2.m1.1.1" xref="S4.F3.2.m1.1.1.cmml">âŠ•</mo><annotation-xml encoding="MathML-Content" id="S4.F3.2.m1.1c"><csymbol cd="latexml" id="S4.F3.2.m1.1.1.cmml" xref="S4.F3.2.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.m1.1d">\oplus</annotation></semantics></math>) and fed directly to the attention mechanism.</figcaption>
</figure>
<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We compare using convolution against our recurrent attention in a simple baseline model.
This baseline VQA model uses the same question representation as described in the previous section and the ResNet global image features. The input features are simply concatenated and sent to a visual attention mechanism. The processed attention is fed to the reasoning module. We refer to this model as <span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Simple Net</span> (illustrated in <a href="#S4.F3" title="In Convolutional versus recurrent attention â€£ 4.1 Attention ablation studies â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>). Simple Net was trained twice: once with convolutional attention and once with <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Recurrent Visual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RVAU</span></abbr></a>. To avoid any type of parameter advantage, both models were designed to have the same number of parameters approximately. Both baselines use the <span id="S4.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">train</span> split and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> for training, and were evaluated on the VQA 2.0 validation split. The results of Simple Net in <a href="#S4.T1" title="In 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> show a clear advantage of recurrent attention outperforming convolutional attention by over 4% absolute overall accuracy.</p>
</div>
<div id="S4.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p2.1" class="ltx_p">In second ablation study, we aim to examine the effect of different combinations of recurrent attention by training 4 different variants of the <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Dual Recurrent Attention Units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DRAU</span></abbr></a> model that only differ in the type of attention used. The models are the following:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><a href="#id10.10.id10"><span href="#id10.10.id10" title="Dual Convolution Attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Dual Convolution Attention</span></span></a> (<a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Dual Convolution Attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DCA</span></abbr></a>): DRAU with dual convolutional attention, i.e.Â text att.(Convolution) â€“ image att.(Convolution).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Dual Convolution Attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DCA</span></abbr></a> w/<a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Recurrent Visual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RVAU</span></abbr></a>: text att.(Convolution) â€“ image att.(<a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Recurrent Visual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RVAU</span></abbr></a>).</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Dual Convolution Attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DCA</span></abbr></a> w/<a href="#id4.4.id4"><abbr href="#id4.4.id4" title="Recurrent Textual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RTAU</span></abbr></a>: text att.(<a href="#id4.4.id4"><abbr href="#id4.4.id4" title="Recurrent Textual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RTAU</span></abbr></a>) â€“ image att.(Convolution).</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">DRAU: text att.(<a href="#id4.4.id4"><abbr href="#id4.4.id4" title="Recurrent Textual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RTAU</span></abbr></a>) â€“ image att.(<a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Recurrent Visual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RVAU</span></abbr></a>).</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p3.1" class="ltx_p">Similar to the baseline models, we match the number of parameters in the attention units such that all models have roughly the same number of parameters to ensure that differences are not from a higher number of parameters. All models were trained using Faster R-CNN image features on the <span id="S4.SS1.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_italic">train</span> split. Additionally, we train each variant with 3 different initial seeds to
show the effect of parameter initialization.</p>
</div>
<div id="S4.SS1.SSS0.Px1.p4" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p4.1" class="ltx_p">The results in <a href="#S4.T1" title="In 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> show the evaluation results of this ablation study on the VQA 2.0 validation split. We report the mean and standard deviation of the each model. Using RVAU improves
the baseline <a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Dual Convolution Attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DCA</span></abbr></a> model by 1.48%. While swapping convolutional text attention with
RTAU reduces performance by 0.66%, using both RTAU and RVAU in the
same model improves the overall performance by almost 2%. This might
indicate that multimodal recurrent text attention thrives with certain architectural designs since it improves performance in our DRAU model.</p>
</div>
<div id="S4.SS1.SSS0.Px1.p5" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p5.1" class="ltx_p">In conclusion, using <a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Recurrent Visual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RVAU</span></abbr></a> significantly improves network accuracy. <a href="#id4.4.id4"><abbr href="#id4.4.id4" title="Recurrent Textual Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RTAU</span></abbr></a> requires more careful use, but in an appropriate model, improves performance significantly.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Using <a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RAUs</span></abbr></a> in other models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To verify the effectiveness of the recurrent attention units, we replace the convolutional attention layers in MCB <cite class="ltx_cite ltx_citemacro_citep">(Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> and MUTAN <cite class="ltx_cite ltx_citemacro_citep">(Ben-younes etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> with RVAU (visual attention). Additionally, we replace the textual attention in MFH <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2017b</a>)</cite> with recurrent attention.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For MCB we remove all the layers after the first MCB operation until the first 2048-d output and replace them with RVAU. Due to GPU memory constraints, we reduced the size of each hidden unit in RVAUâ€™s LSTM from 2048 to 1024. In the same setting, RVAU significantly helps improve the original MCB modelâ€™s accuracy as shown in <a href="#S4.T2" title="In 4.2 Using in other models â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">Furthermore, we test RVAU in the MUTAN model. The authors use a multimodal vector with dimension size of <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="510" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">510</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">510</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">510</annotation></semantics></math> for the joint representations. For coherence, we change the usual dimension size in RVAU to <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="510" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">510</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn type="integer" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">510</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">510</annotation></semantics></math>. At the time of this writing, the authors have not released results on VQA 2.0 using a single model rather than a model ensemble. Therefore, we train a single-model MUTAN using the authorsâ€™ implementation.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/Cadene/vqa.pytorch" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Cadene/vqa.pytorch</a></span></span></span> The story does not change here, RVAU improves the modelâ€™s overall accuracy.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Finally, we replace the convolution text attention in MFH with RTAU (text attention). We train two networks, the standard MFH network and MFH with RTAU, on the VQA 2.0 train split and test on the validation split. It is apparent that RTAU improves the overall accuracy of MFH from <a href="#S4.T2" title="In 4.2 Using in other models â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. Note that the text attention in MFH is â€œself-attendingâ€ which means that the textual attention does not
interact with visual content in this setting. This is different from our DRAU model where RTAU uses a joint representation of the question and image to predict the textual attention. This gives insight about the difference of performance between the ablation study and the modified MFH results. While the performance improvement might not look large for some models, it is consistent which shows that RAUs can reliably improve existing state-of-the-art models with different architectures.</p>
</div>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of state-of-the-art models with RAUs.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_top">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="5">VQA 2.0 Test-dev Split</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Model</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">All</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Y/N</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">Num.</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">Other</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MCB <cite class="ltx_cite ltx_citemacro_citep">(Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> <sup id="S4.T2.1.3.2.1.1" class="ltx_sup">2</sup>
</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">61.96</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">78.41</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">38.81</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">53.23</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MCB w/RVAU</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">62.33</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center">77.31</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center">40.12</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center">54.64</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MUTAN (Ben. et al.,2017)</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">62.36</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">79.06</td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">38.95</td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">53.46</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MUTAN w/RVAU</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center">62.45</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_center">79.33</td>
<td id="S4.T2.1.6.5.4" class="ltx_td ltx_align_center">39.48</td>
<td id="S4.T2.1.6.5.5" class="ltx_td ltx_align_center">53.28</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<th id="S4.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="5">VQA 2.0 Validation Split</th>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<th id="S4.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MFH <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2017b</a>)</cite>
</th>
<td id="S4.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">64.31</td>
<td id="S4.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">82.26</td>
<td id="S4.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">43.49</td>
<td id="S4.T2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">56.17</td>
</tr>
<tr id="S4.T2.1.9.8" class="ltx_tr">
<th id="S4.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MFH w/RTAU</th>
<td id="S4.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb">64.38</td>
<td id="S4.T2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb">82.35</td>
<td id="S4.T2.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb">43.31</td>
<td id="S4.T2.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb">56.3</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I2" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S4.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span> 
<div id="S4.I2.ix1.p1" class="ltx_para">
<p id="S4.I2.ix1.p1.1" class="ltx_p"><a target="_blank" href="http://www.visualqa.org/roe_2017.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">http://www.visualqa.org/roe_2017.html</a><span id="S4.I2.ix1.p1.1.1" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>DRAU versus the state-of-the-art</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we present the results of our model in the scope of state-of-the-art models. Although our model does not utilize some extra knowledge and performance optimization techniques (Visual Genome augmentation, model ensembles, hyperparameter tuning), we choose to show models that do so in order to present our results from a practical point of view.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA 1.0</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p"><a href="#S4.T3" title="In VQA 1.0 â€£ 4.3 DRAU versus the state-of-the-art â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> shows a comparison between DRAU and other state-of-the-art models. Excluding model ensembles, DRAU performs favorably against other models. To the best of our knowledge, <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2017b</a>)</cite> has the best reported single model performance of <math id="S4.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="67.5\%" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">67.5</mn><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2">67.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">67.5\%</annotation></semantics></math> on the <span id="S4.SS3.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">test-std</span> split. Our single model (DRAU) comes a very close second to the current state-of-the-art single model.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>DRAU compared to the state-of-the-art on the VQA 1.0 dataset. N corresponds to the number of models used for prediction. WE indicates whether the method uses a pre-trained word embedding. VG indicates whether the method uses external data from the Visual Genome dataset.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="12">VQA 1.0 Open Ended Task</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<th id="S4.T3.1.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" colspan="4"></th>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="4">Test-dev</td>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="4">Test-standard</td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<th id="S4.T3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Model</th>
<th id="S4.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">N</th>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_align_center">WE</td>
<td id="S4.T3.1.4.2.4" class="ltx_td ltx_align_center">VG</td>
<td id="S4.T3.1.4.2.5" class="ltx_td ltx_align_center">All</td>
<td id="S4.T3.1.4.2.6" class="ltx_td ltx_align_center">Y/N</td>
<td id="S4.T3.1.4.2.7" class="ltx_td ltx_align_center">Num.</td>
<td id="S4.T3.1.4.2.8" class="ltx_td ltx_align_center">Other</td>
<td id="S4.T3.1.4.2.9" class="ltx_td ltx_align_center">All</td>
<td id="S4.T3.1.4.2.10" class="ltx_td ltx_align_center">Y/N</td>
<td id="S4.T3.1.4.2.11" class="ltx_td ltx_align_center">Num.</td>
<td id="S4.T3.1.4.2.12" class="ltx_td ltx_align_center">Other</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<th id="S4.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DMN+ <cite class="ltx_cite ltx_citemacro_citep">(Xiong etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2016</a>)</cite>
</th>
<th id="S4.T3.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t">60.3</td>
<td id="S4.T3.1.5.3.6" class="ltx_td ltx_align_center ltx_border_t">80.5</td>
<td id="S4.T3.1.5.3.7" class="ltx_td ltx_align_center ltx_border_t">36.8</td>
<td id="S4.T3.1.5.3.8" class="ltx_td ltx_align_center ltx_border_t">48.3</td>
<td id="S4.T3.1.5.3.9" class="ltx_td ltx_align_center ltx_border_t">60.4</td>
<td id="S4.T3.1.5.3.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.5.3.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.5.3.12" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.1.6.4" class="ltx_tr">
<th id="S4.T3.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HieCoAtt <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>
</th>
<th id="S4.T3.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T3.1.6.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.6.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.6.4.5" class="ltx_td ltx_align_center">61.8</td>
<td id="S4.T3.1.6.4.6" class="ltx_td ltx_align_center">79.7</td>
<td id="S4.T3.1.6.4.7" class="ltx_td ltx_align_center">38.7</td>
<td id="S4.T3.1.6.4.8" class="ltx_td ltx_align_center">51.7</td>
<td id="S4.T3.1.6.4.9" class="ltx_td ltx_align_center">62.1</td>
<td id="S4.T3.1.6.4.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.6.4.11" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.6.4.12" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.1.7.5" class="ltx_tr">
<th id="S4.T3.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RAU <cite class="ltx_cite ltx_citemacro_citep">(Noh and Han, <a href="#bib.bib28" title="" class="ltx_ref">2016</a>)</cite>
</th>
<th id="S4.T3.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T3.1.7.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.7.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.7.5.5" class="ltx_td ltx_align_center">63.3</td>
<td id="S4.T3.1.7.5.6" class="ltx_td ltx_align_center">81.9</td>
<td id="S4.T3.1.7.5.7" class="ltx_td ltx_align_center">39.0</td>
<td id="S4.T3.1.7.5.8" class="ltx_td ltx_align_center">53.0</td>
<td id="S4.T3.1.7.5.9" class="ltx_td ltx_align_center">63.2</td>
<td id="S4.T3.1.7.5.10" class="ltx_td ltx_align_center">81.7</td>
<td id="S4.T3.1.7.5.11" class="ltx_td ltx_align_center">38.2</td>
<td id="S4.T3.1.7.5.12" class="ltx_td ltx_align_center">52.8</td>
</tr>
<tr id="S4.T3.1.8.6" class="ltx_tr">
<th id="S4.T3.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DAN <cite class="ltx_cite ltx_citemacro_citep">(Nam etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T3.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T3.1.8.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.8.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.8.6.5" class="ltx_td ltx_align_center">64.3</td>
<td id="S4.T3.1.8.6.6" class="ltx_td ltx_align_center">83.0</td>
<td id="S4.T3.1.8.6.7" class="ltx_td ltx_align_center">39.1</td>
<td id="S4.T3.1.8.6.8" class="ltx_td ltx_align_center">53.9</td>
<td id="S4.T3.1.8.6.9" class="ltx_td ltx_align_center">64.2</td>
<td id="S4.T3.1.8.6.10" class="ltx_td ltx_align_center">82.8</td>
<td id="S4.T3.1.8.6.11" class="ltx_td ltx_align_center">38.1</td>
<td id="S4.T3.1.8.6.12" class="ltx_td ltx_align_center">54.0</td>
</tr>
<tr id="S4.T3.1.9.7" class="ltx_tr">
<th id="S4.T3.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MCB <cite class="ltx_cite ltx_citemacro_citep">(Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>
</th>
<th id="S4.T3.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">7</th>
<td id="S4.T3.1.9.7.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.9.7.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.9.7.5" class="ltx_td ltx_align_center">66.7</td>
<td id="S4.T3.1.9.7.6" class="ltx_td ltx_align_center">83.4</td>
<td id="S4.T3.1.9.7.7" class="ltx_td ltx_align_center">39.8</td>
<td id="S4.T3.1.9.7.8" class="ltx_td ltx_align_center">58.5</td>
<td id="S4.T3.1.9.7.9" class="ltx_td ltx_align_center">66.47</td>
<td id="S4.T3.1.9.7.10" class="ltx_td ltx_align_center">83.24</td>
<td id="S4.T3.1.9.7.11" class="ltx_td ltx_align_center">39.47</td>
<td id="S4.T3.1.9.7.12" class="ltx_td ltx_align_center">58.00</td>
</tr>
<tr id="S4.T3.1.10.8" class="ltx_tr">
<th id="S4.T3.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MLB <cite class="ltx_cite ltx_citemacro_citep">(Kim etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T3.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T3.1.10.8.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.10.8.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T3.1.10.8.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.10.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.10.8.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.10.8.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.10.8.9" class="ltx_td ltx_align_center">65.07</td>
<td id="S4.T3.1.10.8.10" class="ltx_td ltx_align_center">84.02</td>
<td id="S4.T3.1.10.8.11" class="ltx_td ltx_align_center">37.90</td>
<td id="S4.T3.1.10.8.12" class="ltx_td ltx_align_center">54.77</td>
</tr>
<tr id="S4.T3.1.11.9" class="ltx_tr">
<th id="S4.T3.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MLB <cite class="ltx_cite ltx_citemacro_citep">(Kim etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T3.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">7</th>
<td id="S4.T3.1.11.9.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.11.9.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.11.9.5" class="ltx_td ltx_align_center">66.77</td>
<td id="S4.T3.1.11.9.6" class="ltx_td ltx_align_center">84.57</td>
<td id="S4.T3.1.11.9.7" class="ltx_td ltx_align_center">39.21</td>
<td id="S4.T3.1.11.9.8" class="ltx_td ltx_align_center">57.81</td>
<td id="S4.T3.1.11.9.9" class="ltx_td ltx_align_center">66.89</td>
<td id="S4.T3.1.11.9.10" class="ltx_td ltx_align_center">84.61</td>
<td id="S4.T3.1.11.9.11" class="ltx_td ltx_align_center">39.07</td>
<td id="S4.T3.1.11.9.12" class="ltx_td ltx_align_center">57.79</td>
</tr>
<tr id="S4.T3.1.12.10" class="ltx_tr">
<th id="S4.T3.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MUTAN <cite class="ltx_cite ltx_citemacro_citep">(Ben-younes etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>)</th>
<th id="S4.T3.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<td id="S4.T3.1.12.10.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.12.10.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.12.10.5" class="ltx_td ltx_align_center">67.42</td>
<td id="S4.T3.1.12.10.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.10.6.1" class="ltx_text ltx_font_bold">85.14</span></td>
<td id="S4.T3.1.12.10.7" class="ltx_td ltx_align_center">39.81</td>
<td id="S4.T3.1.12.10.8" class="ltx_td ltx_align_center">58.52</td>
<td id="S4.T3.1.12.10.9" class="ltx_td ltx_align_center">67.36</td>
<td id="S4.T3.1.12.10.10" class="ltx_td ltx_align_center">84.91</td>
<td id="S4.T3.1.12.10.11" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.10.11.1" class="ltx_text ltx_font_bold">39.79</span></td>
<td id="S4.T3.1.12.10.12" class="ltx_td ltx_align_center">58.35</td>
</tr>
<tr id="S4.T3.1.13.11" class="ltx_tr">
<th id="S4.T3.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MFH <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2017b</a>)</cite>
</th>
<th id="S4.T3.1.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T3.1.13.11.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.13.11.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T3.1.13.11.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.5.1" class="ltx_text ltx_font_bold">67.7</span></td>
<td id="S4.T3.1.13.11.6" class="ltx_td ltx_align_center">84.9</td>
<td id="S4.T3.1.13.11.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.7.1" class="ltx_text ltx_font_bold">40.2</span></td>
<td id="S4.T3.1.13.11.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.8.1" class="ltx_text ltx_font_bold">59.2</span></td>
<td id="S4.T3.1.13.11.9" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.9.1" class="ltx_text ltx_font_bold">67.5</span></td>
<td id="S4.T3.1.13.11.10" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.10.1" class="ltx_text ltx_font_bold">84.91</span></td>
<td id="S4.T3.1.13.11.11" class="ltx_td ltx_align_center">39.3</td>
<td id="S4.T3.1.13.11.12" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.11.12.1" class="ltx_text ltx_font_bold">58.7</span></td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">DRAU<math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\mbox{FRCNN + MCB fusion}}" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.m1.1.1a" xref="S4.T3.1.1.1.m1.1.1.cmml"></mi><mtext id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1a.cmml">FRCNN + MCB fusion</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><ci id="S4.T3.1.1.1.m1.1.1.1a.cmml" xref="S4.T3.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1.1">FRCNN + MCB fusion</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">{}_{\mbox{FRCNN + MCB fusion}}</annotation></semantics></math>
</th>
<th id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">1</th>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ“</td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ—</td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.5.1" class="ltx_text ltx_font_bold">66.86</span></td>
<td id="S4.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.6.1" class="ltx_text ltx_font_bold">84.92</span></td>
<td id="S4.T3.1.1.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.7.1" class="ltx_text ltx_font_bold">39.16</span></td>
<td id="S4.T3.1.1.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.8.1" class="ltx_text ltx_font_bold">57.70</span></td>
<td id="S4.T3.1.1.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.9.1" class="ltx_text ltx_font_bold">67.16</span></td>
<td id="S4.T3.1.1.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.10.1" class="ltx_text ltx_font_bold">84.87</span></td>
<td id="S4.T3.1.1.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.11.1" class="ltx_text ltx_font_bold">40.02</span></td>
<td id="S4.T3.1.1.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.12.1" class="ltx_text ltx_font_bold">57.91</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA 2.0</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">The first place submission <cite class="ltx_cite ltx_citemacro_cite">Anderson etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite> reports using an ensemble of 30 models. In their report, the best single model that also uses FRCNN features achieves <math id="S4.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="65.67\%" display="inline"><semantics id="S4.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">65.67</mn><mo id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2">65.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p1.1.m1.1c">65.67\%</annotation></semantics></math> on the <span id="S4.SS3.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">test-standard</span> split which is outperformed by our single model (DRAU).</p>
</div>
<div id="S4.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p2.1" class="ltx_p">Recently, the VQA 2018 challenge results have been released. It uses the same dataset as the previous VQA 2017 challenge (VQA 2.0). While we have not participated in this challenge, we include the challenge winners results <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite> for the sake of completeness. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib15" title="" class="ltx_ref">Jiang etÂ al.</a></cite> builds upon the VQA 2017 challenge winners model by proposing a number of modifications. First, they use weight normalization and ReLU instead of gated hyperbolic tangent activation. For the learning schedule, the Adam optimizer was swapped for Adamax with a warm up strategy. Moreover, the Faster-RCNN features have been replaced by the state-of-the-art Feature Pyramid Networks (FPN) object detectors. Lastly, they use more additional training data from the common Visual Genome and the new Visual Dialog (VisDial) datasets.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>DRAU compared to the current submissions on the VQA 2.0 dataset. N corresponds to the number of models used for prediction. WE indicates whether the method uses a pre-trained word embedding. VG indicates whether the method uses external data from the Visual Genome dataset.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="12">VQA 2.0 Open Ended Task</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.3.1" class="ltx_tr">
<th id="S4.T4.1.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" colspan="4"></th>
<td id="S4.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="4">Test-dev</td>
<td id="S4.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="4">Test-standard</td>
</tr>
<tr id="S4.T4.1.4.2" class="ltx_tr">
<th id="S4.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Model</th>
<th id="S4.T4.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">N</th>
<td id="S4.T4.1.4.2.3" class="ltx_td ltx_align_center">WE</td>
<td id="S4.T4.1.4.2.4" class="ltx_td ltx_align_center">VG</td>
<td id="S4.T4.1.4.2.5" class="ltx_td ltx_align_center">All</td>
<td id="S4.T4.1.4.2.6" class="ltx_td ltx_align_center">Y/N</td>
<td id="S4.T4.1.4.2.7" class="ltx_td ltx_align_center">Num.</td>
<td id="S4.T4.1.4.2.8" class="ltx_td ltx_align_center">Other</td>
<td id="S4.T4.1.4.2.9" class="ltx_td ltx_align_center">All</td>
<td id="S4.T4.1.4.2.10" class="ltx_td ltx_align_center">Y/N</td>
<td id="S4.T4.1.4.2.11" class="ltx_td ltx_align_center">Num.</td>
<td id="S4.T4.1.4.2.12" class="ltx_td ltx_align_center">Other</td>
</tr>
<tr id="S4.T4.1.5.3" class="ltx_tr">
<th id="S4.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VQATeam_MCB <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T4.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S4.T4.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S4.T4.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S4.T4.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t">61.96</td>
<td id="S4.T4.1.5.3.6" class="ltx_td ltx_align_center ltx_border_t">78.41</td>
<td id="S4.T4.1.5.3.7" class="ltx_td ltx_align_center ltx_border_t">38.81</td>
<td id="S4.T4.1.5.3.8" class="ltx_td ltx_align_center ltx_border_t">53.23</td>
<td id="S4.T4.1.5.3.9" class="ltx_td ltx_align_center ltx_border_t">62.27</td>
<td id="S4.T4.1.5.3.10" class="ltx_td ltx_align_center ltx_border_t">78.82</td>
<td id="S4.T4.1.5.3.11" class="ltx_td ltx_align_center ltx_border_t">38.28</td>
<td id="S4.T4.1.5.3.12" class="ltx_td ltx_align_center ltx_border_t">53.36</td>
</tr>
<tr id="S4.T4.1.6.4" class="ltx_tr">
<th id="S4.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UPMC-LIP6 <cite class="ltx_cite ltx_citemacro_citep">(Ben-younes etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T4.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<td id="S4.T4.1.6.4.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.6.4.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.6.4.5" class="ltx_td ltx_align_center">65.57</td>
<td id="S4.T4.1.6.4.6" class="ltx_td ltx_align_center">81.96</td>
<td id="S4.T4.1.6.4.7" class="ltx_td ltx_align_center">41.62</td>
<td id="S4.T4.1.6.4.8" class="ltx_td ltx_align_center">57.07</td>
<td id="S4.T4.1.6.4.9" class="ltx_td ltx_align_center">65.71</td>
<td id="S4.T4.1.6.4.10" class="ltx_td ltx_align_center">82.07</td>
<td id="S4.T4.1.6.4.11" class="ltx_td ltx_align_center">41.06</td>
<td id="S4.T4.1.6.4.12" class="ltx_td ltx_align_center">57.12</td>
</tr>
<tr id="S4.T4.1.7.5" class="ltx_tr">
<th id="S4.T4.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HDU-USYD-UNCC <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2017b</a>)</cite>
</th>
<th id="S4.T4.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">9</th>
<td id="S4.T4.1.7.5.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.7.5.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.7.5.5" class="ltx_td ltx_align_center">68.02</td>
<td id="S4.T4.1.7.5.6" class="ltx_td ltx_align_center">84.39</td>
<td id="S4.T4.1.7.5.7" class="ltx_td ltx_align_center">45.76</td>
<td id="S4.T4.1.7.5.8" class="ltx_td ltx_align_center">59.14</td>
<td id="S4.T4.1.7.5.9" class="ltx_td ltx_align_center">68.09</td>
<td id="S4.T4.1.7.5.10" class="ltx_td ltx_align_center">84.5</td>
<td id="S4.T4.1.7.5.11" class="ltx_td ltx_align_center">45.39</td>
<td id="S4.T4.1.7.5.12" class="ltx_td ltx_align_center">59.01</td>
</tr>
<tr id="S4.T4.1.8.6" class="ltx_tr">
<th id="S4.T4.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adelaide-Teney <cite class="ltx_cite ltx_citemacro_citep">(Teney etÂ al., <a href="#bib.bib35" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T4.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T4.1.8.6.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.8.6.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.8.6.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.5.1" class="ltx_text ltx_font_bold">65.32</span></td>
<td id="S4.T4.1.8.6.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.6.1" class="ltx_text ltx_font_bold">81.82</span></td>
<td id="S4.T4.1.8.6.7" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.7.1" class="ltx_text ltx_font_bold">44.21</span></td>
<td id="S4.T4.1.8.6.8" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.8.1" class="ltx_text ltx_font_bold">56.05</span></td>
<td id="S4.T4.1.8.6.9" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.9.1" class="ltx_text ltx_font_bold">65.67</span></td>
<td id="S4.T4.1.8.6.10" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.10.1" class="ltx_text ltx_font_bold">82.20</span></td>
<td id="S4.T4.1.8.6.11" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.11.1" class="ltx_text ltx_font_bold">43.90</span></td>
<td id="S4.T4.1.8.6.12" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.6.12.1" class="ltx_text ltx_font_bold">56.26</span></td>
</tr>
<tr id="S4.T4.1.9.7" class="ltx_tr">
<th id="S4.T4.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adelaide-Teney <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T4.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">30</th>
<td id="S4.T4.1.9.7.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.9.7.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.9.7.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.9.7.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.9.7.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.9.7.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.9.7.9" class="ltx_td ltx_align_center">70.34</td>
<td id="S4.T4.1.9.7.10" class="ltx_td ltx_align_center">86.60</td>
<td id="S4.T4.1.9.7.11" class="ltx_td ltx_align_center">48.64</td>
<td id="S4.T4.1.9.7.12" class="ltx_td ltx_align_center">61.15</td>
</tr>
<tr id="S4.T4.1.10.8" class="ltx_tr">
<th id="S4.T4.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FAIR A-STAR <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>
</th>
<th id="S4.T4.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T4.1.10.8.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.10.8.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.10.8.5" class="ltx_td ltx_align_center">70.01</td>
<td id="S4.T4.1.10.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.10.8.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.10.8.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.10.8.9" class="ltx_td ltx_align_center">70.24</td>
<td id="S4.T4.1.10.8.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.10.8.11" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.10.8.12" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T4.1.11.9" class="ltx_tr">
<th id="S4.T4.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FAIR A-STAR <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>
</th>
<th id="S4.T4.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">30</th>
<td id="S4.T4.1.11.9.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.11.9.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T4.1.11.9.5" class="ltx_td ltx_align_center">72.12</td>
<td id="S4.T4.1.11.9.6" class="ltx_td ltx_align_center">87.82</td>
<td id="S4.T4.1.11.9.7" class="ltx_td ltx_align_center">51.54</td>
<td id="S4.T4.1.11.9.8" class="ltx_td ltx_align_center">63.41</td>
<td id="S4.T4.1.11.9.9" class="ltx_td ltx_align_center">72.25</td>
<td id="S4.T4.1.11.9.10" class="ltx_td ltx_align_center">87.82</td>
<td id="S4.T4.1.11.9.11" class="ltx_td ltx_align_center">51.59</td>
<td id="S4.T4.1.11.9.12" class="ltx_td ltx_align_center">63.43</td>
</tr>
<tr id="S4.T4.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">DRAU<math id="S4.T4.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\mbox{FRCNN + MCB fusion}}" display="inline"><semantics id="S4.T4.1.1.1.m1.1a"><msub id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.m1.1.1a" xref="S4.T4.1.1.1.m1.1.1.cmml"></mi><mtext id="S4.T4.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.m1.1.1.1a.cmml">FRCNN + MCB fusion</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1"><ci id="S4.T4.1.1.1.m1.1.1.1a.cmml" xref="S4.T4.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S4.T4.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1.1">FRCNN + MCB fusion</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">{}_{\mbox{FRCNN + MCB fusion}}</annotation></semantics></math>
</th>
<th id="S4.T4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">1</th>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ“</td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ—</td>
<td id="S4.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.5.1" class="ltx_text ltx_font_bold">66.45</span></td>
<td id="S4.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.6.1" class="ltx_text ltx_font_bold">82.85</span></td>
<td id="S4.T4.1.1.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.7.1" class="ltx_text ltx_font_bold">44.78</span></td>
<td id="S4.T4.1.1.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.8.1" class="ltx_text ltx_font_bold">57.4</span></td>
<td id="S4.T4.1.1.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.9.1" class="ltx_text ltx_font_bold">66.85</span></td>
<td id="S4.T4.1.1.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.10.1" class="ltx_text ltx_font_bold">83.35</span></td>
<td id="S4.T4.1.1.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.11.1" class="ltx_text ltx_font_bold">44.37</span></td>
<td id="S4.T4.1.1.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.12.1" class="ltx_text ltx_font_bold">57.63</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Discussion</h3>

<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">DRAU versus MCB</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">The strength of RAUs is notable in tasks that require sequentially processing the image or relational/multi-step reasoning. <a href="#S4.F4" title="In DRAU versus MCB â€£ 4.4 Discussion â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> shows some qualitative results between DRAU and MCB. For fair comparison we compare the first attention map of MCB with the second attention map of our model. We do so because the authors of MCB <cite class="ltx_cite ltx_citemacro_citep">(Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> visualize the first map in their work<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/akirafukui/vqa-mcb/blob/master/server/server.py#L185" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/akirafukui/vqa-mcb/blob/master/server/server.py#L185</a></span></span></span>. Furthermore, the first glimpse of our model seems to be the complement of the second attention, i.e.Â the model separates the background and the target object(s) into separate attention maps (illustrated in <a href="#S3.F2" title="In 3 Dual Recurrent Attention in VQA â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.9" class="ltx_p"><span id="S4.F4.9.9" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;">

<span id="S4.F4.6.6.6" class="ltx_inline-block ltx_transformed_outer" style="width:288.4pt;height:144.5pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-349.2pt,174.7pt) scale(0.292210647380383,0.292210647380383) ;">
<span id="S4.F4.6.6.6.6" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.F4.6.6.6.6.6" class="ltx_tr">
<span id="S4.F4.6.6.6.6.6.6" class="ltx_td ltx_align_center">
<span id="S4.F4.3.3.3.3.3.3.3" class="ltx_tabular ltx_align_middle">
<span id="S4.F4.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="S4.F4.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/1802.00209/assets/lantern_orig.jpg" id="S4.F4.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption">
</span></span>
<span id="S4.F4.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.2.2.2.2.2.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.2.2.2.2.2.2.2.2.2.1.1" class="ltx_p"><span id="S4.F4.2.2.2.2.2.2.2.2.2.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/lantern_drau1.png" id="S4.F4.2.2.2.2.2.2.2.2.2.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span></span>
<span id="S4.F4.2.2.2.2.2.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.2.2.2.2.2.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1" class="ltx_p">How <span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1.1" class="ltx_text" style="background-color:#FFFFFF;">many</span> <span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1.2" class="ltx_text" style="background-color:#FFADAD;">lanterns</span> <span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1.3" class="ltx_text" style="background-color:#FF8282;">hang</span> <span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1.4" class="ltx_text" style="background-color:#FFDBDB;">off</span> <span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1.5" class="ltx_text" style="background-color:#FFFCFC;">the</span> <span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1.6" class="ltx_text" style="background-color:#FFFCFC;">clock</span> <span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.1.7" class="ltx_text" style="background-color:#FFF7F7;">tower</span>?</span>
<span id="S4.F4.2.2.2.2.2.2.2.2.3.1.1.2" class="ltx_p ltx_align_center">DRAU: 4</span>
</span>
</span></span></span>
<span id="S4.F4.3.3.3.3.3.3.3.3" class="ltx_tr">
<span id="S4.F4.3.3.3.3.3.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.3.3.3.3.3.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.3.3.3.3.3.3.3.3.2.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F4.3.3.3.3.3.3.3.3.2.1.1.1" class="ltx_p"><span id="S4.F4.3.3.3.3.3.3.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">How many lanterns hang off the clock tower?</span></span>
<span id="S4.F4.3.3.3.3.3.3.3.3.2.1.1.2" class="ltx_p ltx_align_center"><span id="S4.F4.3.3.3.3.3.3.3.3.2.1.1.2.1" class="ltx_text ltx_font_bold">GT: 4</span></span>
</span>
</span></span>
<span id="S4.F4.3.3.3.3.3.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.3.3.3.3.3.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.3.3.3.3.3.3.3.3.1.1.1" class="ltx_p"><span id="S4.F4.3.3.3.3.3.3.3.3.1.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/lantern_mcb0.png" id="S4.F4.3.3.3.3.3.3.3.3.1.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span></span>
<span id="S4.F4.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.3.3.3.3.3.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.3.3.3.3.3.3.3.3.3.1.1" class="ltx_p">MCB: 1</span>
</span></span></span>
</span>

<span id="S4.F4.6.6.6.6.6.6.6" class="ltx_tabular ltx_align_middle">
<span id="S4.F4.5.5.5.5.5.5.5.2" class="ltx_tr">
<span id="S4.F4.4.4.4.4.4.4.4.1.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.4.4.4.4.4.4.4.1.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/1802.00209/assets/camel_orig.jpg" id="S4.F4.4.4.4.4.4.4.4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption">
</span></span>
<span id="S4.F4.5.5.5.5.5.5.5.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.5.5.5.5.5.5.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.5.5.5.5.5.5.5.2.2.1.1" class="ltx_p"><span id="S4.F4.5.5.5.5.5.5.5.2.2.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/camel_drau1.png" id="S4.F4.5.5.5.5.5.5.5.2.2.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span></span>
<span id="S4.F4.5.5.5.5.5.5.5.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.5.5.5.5.5.5.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.5.5.5.5.5.5.5.2.3.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F4.5.5.5.5.5.5.5.2.3.1.1.1" class="ltx_p">How many <span id="S4.F4.5.5.5.5.5.5.5.2.3.1.1.1.1" class="ltx_text" style="background-color:#FF5454;">camels</span> <span id="S4.F4.5.5.5.5.5.5.5.2.3.1.1.1.2" class="ltx_text" style="background-color:#FFBFBF;">are</span> <span id="S4.F4.5.5.5.5.5.5.5.2.3.1.1.1.3" class="ltx_text" style="background-color:#FFF5F5;">in</span> <span id="S4.F4.5.5.5.5.5.5.5.2.3.1.1.1.4" class="ltx_text" style="background-color:#FFF7F7;">the</span> photo?</span>
<span id="S4.F4.5.5.5.5.5.5.5.2.3.1.1.2" class="ltx_p ltx_align_center">DRAU: 0</span>
</span>
</span></span></span>
<span id="S4.F4.6.6.6.6.6.6.6.3" class="ltx_tr">
<span id="S4.F4.6.6.6.6.6.6.6.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.6.6.6.6.6.6.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.6.6.6.6.6.6.6.3.2.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F4.6.6.6.6.6.6.6.3.2.1.1.1" class="ltx_p"><span id="S4.F4.6.6.6.6.6.6.6.3.2.1.1.1.1" class="ltx_text ltx_font_bold">How many camels are in the photo?</span></span>
<span id="S4.F4.6.6.6.6.6.6.6.3.2.1.1.2" class="ltx_p ltx_align_center"><span id="S4.F4.6.6.6.6.6.6.6.3.2.1.1.2.1" class="ltx_text ltx_font_bold">GT: 0</span></span>
</span>
</span></span>
<span id="S4.F4.6.6.6.6.6.6.6.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.6.6.6.6.6.6.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.6.6.6.6.6.6.6.3.1.1.1" class="ltx_p"><span id="S4.F4.6.6.6.6.6.6.6.3.1.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/camel_mcb0.png" id="S4.F4.6.6.6.6.6.6.6.3.1.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span></span>
<span id="S4.F4.6.6.6.6.6.6.6.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.6.6.6.6.6.6.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.6.6.6.6.6.6.6.3.3.1.1" class="ltx_p">MCB: 1</span>
</span></span></span>
</span></span></span>
</span>
</span>
</span></span>

<span id="S4.F4.9.9.9" class="ltx_inline-block ltx_transformed_outer" style="width:143.1pt;height:49.6pt;vertical-align:-0.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-414.2pt,143.2pt) scale(0.147296106451405,0.147296106451405) ;">
<span id="S4.F4.9.9.9.3" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.F4.8.8.8.2.2" class="ltx_tr">
<span id="S4.F4.7.7.7.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.7.7.7.1.1.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/1802.00209/assets/racket_orig.jpg" id="S4.F4.7.7.7.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption">
</span></span>
<span id="S4.F4.8.8.8.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.8.8.8.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.2.2.2.1.1" class="ltx_p"><span id="S4.F4.8.8.8.2.2.2.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/racket_drau1.png" id="S4.F4.8.8.8.2.2.2.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span></span>
<span id="S4.F4.8.8.8.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.8.8.8.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.8.8.8.2.2.3.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F4.8.8.8.2.2.3.1.1.1" class="ltx_p">What is on the floor leaning on <span id="S4.F4.8.8.8.2.2.3.1.1.1.1" class="ltx_text" style="background-color:#FFEBEB;">the</span> <span id="S4.F4.8.8.8.2.2.3.1.1.1.2" class="ltx_text" style="background-color:#FF9696;">bench</span> <span id="S4.F4.8.8.8.2.2.3.1.1.1.3" class="ltx_text" style="background-color:#FFE3E3;">in</span> <span id="S4.F4.8.8.8.2.2.3.1.1.1.4" class="ltx_text" style="background-color:#FFFCFC;">between</span> <span id="S4.F4.8.8.8.2.2.3.1.1.1.5" class="ltx_text" style="background-color:#FFEBEB;">the</span> <span id="S4.F4.8.8.8.2.2.3.1.1.1.6" class="ltx_text" style="background-color:#FFB0B0;">people</span>?</span>
<span id="S4.F4.8.8.8.2.2.3.1.1.2" class="ltx_p ltx_align_center">DRAU: racket</span>
</span>
</span></span></span>
<span id="S4.F4.9.9.9.3.3" class="ltx_tr">
<span id="S4.F4.9.9.9.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.9.9.9.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.9.9.9.3.3.2.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F4.9.9.9.3.3.2.1.1.1" class="ltx_p"><span id="S4.F4.9.9.9.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">What is on the floor leaning on the bench in between the people?</span></span>
<span id="S4.F4.9.9.9.3.3.2.1.1.2" class="ltx_p ltx_align_center"><span id="S4.F4.9.9.9.3.3.2.1.1.2.1" class="ltx_text ltx_font_bold">GT: racket</span></span>
</span>
</span></span>
<span id="S4.F4.9.9.9.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.9.9.9.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.9.9.9.3.3.1.1.1" class="ltx_p"><span id="S4.F4.9.9.9.3.3.1.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/racket_mcb0.png" id="S4.F4.9.9.9.3.3.1.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span></span>
<span id="S4.F4.9.9.9.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F4.9.9.9.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F4.9.9.9.3.3.3.1.1" class="ltx_p">MCB: backpack</span>
</span></span></span>
</span>
</span>
</span></span>
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Fig. 4: </span>DRAU vs. MCB Qualitative examples. Attention maps for both models shown. DRAU shows subjectively better attention map quality.
</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1802.00209/assets/horsies.jpg" id="S4.F5.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="99" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F5.4" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:216.8pt;height:154.6pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-168.9pt,120.1pt) scale(0.390902331915362,0.390902331915362) ;">
<table id="S4.F5.4.4" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F5.4.4.4" class="ltx_tr">
<td id="S4.F5.4.4.4.4" class="ltx_td ltx_align_center">
<table id="S4.F5.2.2.2.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.F5.1.1.1.1.1.1" class="ltx_tr">
<td id="S4.F5.1.1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:0.0pt;"></td>
<td id="S4.F5.1.1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.1.1.1.1.1.1.1.1.1" class="ltx_p"><span id="S4.F5.1.1.1.1.1.1.1.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/horsies.png" id="S4.F5.1.1.1.1.1.1.1.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span>
</td>
<td id="S4.F5.1.1.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.1.1.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.1.1.1.1.1.1.3.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F5.1.1.1.1.1.1.3.1.1.1" class="ltx_p">How <span id="S4.F5.1.1.1.1.1.1.3.1.1.1.1" class="ltx_text" style="background-color:#FFB0B0;">many</span> <span id="S4.F5.1.1.1.1.1.1.3.1.1.1.2" class="ltx_text" style="background-color:#FF5757;">horses</span> <span id="S4.F5.1.1.1.1.1.1.3.1.1.1.3" class="ltx_text" style="background-color:#FFC9C9;">are</span> <span id="S4.F5.1.1.1.1.1.1.3.1.1.1.4" class="ltx_text" style="background-color:#FFE6E6;">there</span> ?</span>
<span id="S4.F5.1.1.1.1.1.1.3.1.1.2" class="ltx_p ltx_align_center">DRAU: <span id="S4.F5.1.1.1.1.1.1.3.1.1.2.1" class="ltx_text ltx_font_bold">3</span></span>
</span>
</span>
</td>
</tr>
<tr id="S4.F5.2.2.2.2.2.2" class="ltx_tr">
<td id="S4.F5.2.2.2.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:0.0pt;"></td>
<td id="S4.F5.2.2.2.2.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.2.2.2.2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.2.2.2.2.2.2.1.1.1" class="ltx_p"><span id="S4.F5.2.2.2.2.2.2.1.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/horsies_camels.png" id="S4.F5.2.2.2.2.2.2.1.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span>
</td>
<td id="S4.F5.2.2.2.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.2.2.2.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.2.2.2.2.2.2.3.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F5.2.2.2.2.2.2.3.1.1.1" class="ltx_p">How <span id="S4.F5.2.2.2.2.2.2.3.1.1.1.1" class="ltx_text" style="background-color:#FFB0B0;">many</span> <span id="S4.F5.2.2.2.2.2.2.3.1.1.1.2" class="ltx_text" style="background-color:#FF0303;">dogs</span> <span id="S4.F5.2.2.2.2.2.2.3.1.1.1.3" class="ltx_text" style="background-color:#FFC9C9;">are</span> <span id="S4.F5.2.2.2.2.2.2.3.1.1.1.4" class="ltx_text" style="background-color:#FFE6E6;">there</span> ?</span>
<span id="S4.F5.2.2.2.2.2.2.3.1.1.2" class="ltx_p ltx_align_center">DRAU: <span id="S4.F5.2.2.2.2.2.2.3.1.1.2.1" class="ltx_text ltx_font_bold">0</span></span>
</span>
</span>
</td>
</tr>
</table>

<table id="S4.F5.4.4.4.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.F5.3.3.3.3.3.1" class="ltx_tr">
<td id="S4.F5.3.3.3.3.3.1.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.3.3.3.3.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.3.3.3.3.3.1.1.1.1" class="ltx_p"><span id="S4.F5.3.3.3.3.3.1.1.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/horses_where.png" id="S4.F5.3.3.3.3.3.1.1.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span>
</td>
<td id="S4.F5.3.3.3.3.3.1.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:0.0pt;">
<span id="S4.F5.3.3.3.3.3.1.2.1" class="ltx_inline-block ltx_align_top">
</span>
</td>
<td id="S4.F5.3.3.3.3.3.1.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.3.3.3.3.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.3.3.3.3.3.1.3.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F5.3.3.3.3.3.1.3.1.1.1" class="ltx_p"><span id="S4.F5.3.3.3.3.3.1.3.1.1.1.1" class="ltx_text" style="background-color:#FFB0B0;">Where</span></span>
<span id="S4.F5.3.3.3.3.3.1.3.1.1.2" class="ltx_p ltx_align_center"><span id="S4.F5.3.3.3.3.3.1.3.1.1.2.1" class="ltx_text" style="background-color:#FFF2F2;">are</span></span>
<span id="S4.F5.3.3.3.3.3.1.3.1.1.3" class="ltx_p ltx_align_center"><span id="S4.F5.3.3.3.3.3.1.3.1.1.3.1" class="ltx_text" style="background-color:#FFF2F2;">the</span></span>
<span id="S4.F5.3.3.3.3.3.1.3.1.1.4" class="ltx_p ltx_align_center"><span id="S4.F5.3.3.3.3.3.1.3.1.1.4.1" class="ltx_text" style="background-color:#FF0303;">horses</span></span>
<span id="S4.F5.3.3.3.3.3.1.3.1.1.5" class="ltx_p ltx_align_center">?</span>
<span id="S4.F5.3.3.3.3.3.1.3.1.1.6" class="ltx_p ltx_align_center">DRAU: <span id="S4.F5.3.3.3.3.3.1.3.1.1.6.1" class="ltx_text ltx_font_bold">field</span></span>
</span>
</span>
</td>
</tr>
<tr id="S4.F5.4.4.4.4.4.2" class="ltx_tr">
<td id="S4.F5.4.4.4.4.4.2.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.4.4.4.4.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.4.4.4.4.4.2.1.1.1" class="ltx_p"><span id="S4.F5.4.4.4.4.4.2.1.1.1.1" class="ltx_text"><svg version="1.1" width="598" height="99" overflow="visible"><g transform="translate(0,99) scale(1,-1)"><rect fill="none" height="71.5pt" stroke="#000000" stroke-width="0.4" width="432.2pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignObject width="598" height="99" overflow="visible"><img src="/html/1802.00209/assets/horsies_horizon.png" id="S4.F5.4.4.4.4.4.2.1.1.1.1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="Refer to caption"></foreignObject></g></g></g></svg></span></span>
</span>
</td>
<td id="S4.F5.4.4.4.4.4.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:0.0pt;"></td>
<td id="S4.F5.4.4.4.4.4.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:72.3pt;">
<span id="S4.F5.4.4.4.4.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F5.4.4.4.4.4.2.3.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:71.1pt;">
<span id="S4.F5.4.4.4.4.4.2.3.1.1.1" class="ltx_p"><span id="S4.F5.4.4.4.4.4.2.3.1.1.1.1" class="ltx_text" style="background-color:#FF8C8C;">Can</span></span>
<span id="S4.F5.4.4.4.4.4.2.3.1.1.2" class="ltx_p ltx_align_center"><span id="S4.F5.4.4.4.4.4.2.3.1.1.2.1" class="ltx_text" style="background-color:#FFF7F7;">you</span></span>
<span id="S4.F5.4.4.4.4.4.2.3.1.1.3" class="ltx_p ltx_align_center"><span id="S4.F5.4.4.4.4.4.2.3.1.1.3.1" class="ltx_text" style="background-color:#FFA1A1;">see</span></span>
<span id="S4.F5.4.4.4.4.4.2.3.1.1.4" class="ltx_p ltx_align_center"><span id="S4.F5.4.4.4.4.4.2.3.1.1.4.1" class="ltx_text" style="background-color:#FFE6E6;">the</span></span>
<span id="S4.F5.4.4.4.4.4.2.3.1.1.5" class="ltx_p ltx_align_center"><span id="S4.F5.4.4.4.4.4.2.3.1.1.5.1" class="ltx_text" style="background-color:#FF0303;">horizon</span></span>
<span id="S4.F5.4.4.4.4.4.2.3.1.1.6" class="ltx_p ltx_align_center">?</span>
<span id="S4.F5.4.4.4.4.4.2.3.1.1.7" class="ltx_p ltx_align_center">DRAU: <span id="S4.F5.4.4.4.4.4.2.3.1.1.7.1" class="ltx_text ltx_font_bold">yes</span></span>
</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Fig. 5: </span>Four real example results of our proposed model for a single random image. The visual attention, textual attention, and answer are shown. Even on the same image, our model shows rich reasoning capabilities for different question types. The first column shows that the model is able to do two-hop reasoning, initially identifying the animal in the question and then proceed to correctly count it in the image. The second column results highlights the modelâ€™s ability to shift its attention to the relevant parts of the image and question. It is worth noting that all the keywords in the questions have the highest attention weights. </figcaption>
</figure>
<div id="S4.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p2.1" class="ltx_p">In <a href="#S4.F4" title="In DRAU versus MCB â€£ 4.4 Discussion â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>, it is clear that the recurrence helps the model attend to multiple targets as apparent in the difference of the attention maps between the two models. The first example shows that <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Dual Recurrent Attention Units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DRAU</span></abbr></a> attends to the right object (lanterns) both visually and textually. The model also attends to the carsâ€™ rear lights as possible â€œlanternsâ€, but the text attention attends to the word â€œhangâ€ which disqualifies the car lights and guides the model to count hanging lanterns. Furthermore, DRAU can predict non-existing object(s). The second example in <a href="#S4.F4" title="In DRAU versus MCB â€£ 4.4 Discussion â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> illustrates that DRAU is not easily fooled by counting whatever animal is present in the image but rather the â€œcamelsâ€ that is needed to answer the question. This property also translates to questions that require relational reasoning. The third example in <a href="#S4.F4" title="In DRAU versus MCB â€£ 4.4 Discussion â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> demonstrates how well DRAU can attend to the relative location required to answer the question based on the textual and visual attention maps compared to MCB.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Attention Quality</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p"><a href="#S4.F5" title="In DRAU versus MCB â€£ 4.4 Discussion â€£ 4 Experiments and Results â€£ DRAU: Dual Recurrent Attention Units for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> shows the modelâ€™s prediction as well as its attention maps for four questions on the same image. It highlights how DRAU can shift the attention intelligently based on different multi-step reasoning questions. To answer the two leftmost questions, a VQA model needs to sequentially process the image and question. First, the model filters out the animals in the picture. Then, the animal in the question is matched to the visual features and finally counted. The two right-most attention maps give a glimpse on how the model filters out the irrelevant parts in the input. Interestingly, inspecting the visual attention for the top question might indicate a bias in the VQA model. Even though the question asks about â€œhorsesâ€, the visual attention filters out all objects and leaves out the two different backgrounds: sea and field. Since â€œhorsesâ€ are often found on land, the model predicts â€œfieldâ€ without any direct attention on the horses in the image.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We proposed an architecture for VQA with a recurrent attention mechanism, termed the  <a href="#id2.2.id2"><span href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Recurrent Attention Unit</span></span></a> (<a href="#id2.2.id2"><abbr href="#id2.2.id2" title="Recurrent Attention Unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAU</span></abbr></a>). The recurrent layers help guide the textual and visual attention since the network can reason relations between several parts of the image and question. We provided quantitative and qualitative results indicating the usefulness of a recurrent attention mechanism. Using a simple VQA baseline, we have shown the performance advantage of recurrent attention compared to the traditional convolutional attention used in most VQA models. Furthermore, we performed an ablation study with all possible combinations of recurrent and convolutional attention. The results of the study indicate that recurrent attention can be very beneficial in dual attention VQA models.
Then, we demonstrated that substituting the visual attention mechanism in other networks, MCB <cite class="ltx_cite ltx_citemacro_citep">(Fukui etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>, MUTAN <cite class="ltx_cite ltx_citemacro_citep">(Ben-younes etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>, and MFH <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2017b</a>)</cite>, consistently improves their performance. In VQA 1.0, we come a very close second to the state-of-the-art model. While using the same image features, our DRAU network outperforms the VQA 2017 challenge winner <cite class="ltx_cite ltx_citemacro_cite">Anderson etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite> in a single-model scenario.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In future work we will investigate implicit recurrent attention mechanism using recently proposed explanation methods <cite class="ltx_cite ltx_citemacro_citep">(Arras etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2017</a>; Montavon etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite> and explore different recurrent models for attention <cite class="ltx_cite ltx_citemacro_citep">(Kalchbrenner etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2015</a>; Shi etÂ al., <a href="#bib.bib34" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by the Fraunhofer Society through the MPI-FhG collaboration project â€Theory and Practice for Reduced Learning Machinesâ€. This research was also supported by the German Ministry for Education and Research as Berlin Big Data Center under Grant 01IS14013A and the Berlin Center for Machine Learning under Grant 01IS180371.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson etÂ al. (2017)</span>
<span class="ltx_bibblock">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L., 2017. Bottom-Up and Top-Down Attention for Image
Captioning and VQA. arXiv:1707.07998.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol etÂ al. (2015)</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Zitnick, C.Â L., Batra, D.,
Parikh, D., 2015. VQA: Visual Question Answering. In: CVPR. pp.
2425â€“2433.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arras etÂ al. (2017)</span>
<span class="ltx_bibblock">
Arras, L., Montavon, G., MÃ¼ller, K.-R., Samek, W., 2017. Explaining
Recurrent Neural Network Predictions in Sentiment Analysis. In:
EMNLPâ€™17 Workshop on Computational Approaches to
Subjectivity, Sentiment &amp; Social Media Analysis (WASSA). pp.
159â€“168.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau etÂ al. (2015)</span>
<span class="ltx_bibblock">
Bahdanau, D., Cho, K., Bengio, Y., 2015. Neural Machine Translation by
Jointly Learning to Align and Translate. In: ICLR.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben-younes etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ben-younes, H., Cadene, R., Cord, M., Thome, N., 2017. MUTAN: Multimodal
Tucker Fusion for Visual Question Answering. arXiv:1705.06676.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bosse etÂ al. (2018)</span>
<span class="ltx_bibblock">
Bosse, S., Maniry, D., MÃ¼ller, K.-R., Wiegand, T., Samek, W., 2018. Deep
Neural Networks for No-Reference and Full-Reference Image
Quality Assessment. IEEE Trans. Image Process. 27Â (1), 206â€“219.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charikar etÂ al. (2004)</span>
<span class="ltx_bibblock">
Charikar, M., Chen, K., Farach-Colton, M., 2004. Finding frequent items in data
streams. Theor. Comput. Sci. 312Â (1), 3â€“15.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho etÂ al. (2014)</span>
<span class="ltx_bibblock">
Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
Schwenk, H., Bengio, Y., Jun. 2014. Learning Phrase Representations using
RNN Encoder-Decoder for Statistical Machine Translation.
arXiv:1406.1078 [cs, stat].

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui etÂ al. (2016)</span>
<span class="ltx_bibblock">
Fukui, A., Park, D.Â H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.,
2016. Multimodal Compact Bilinear Pooling for Visual Question
Answering and Visual Grounding. In: EMNLP. pp. 457â€“468.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2016)</span>
<span class="ltx_bibblock">
Gao, Y., Beijbom, O., Zhang, N., Darrell, T., 2016. Compact bilinear pooling.
In: CVPR. pp. 317â€“326.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glorot and Bengio (2010)</span>
<span class="ltx_bibblock">
Glorot, X., Bengio, Y., 2010. Understanding the Difficulty of Training
Deep Feedforward Neural Networks. In: AISTATS. pp. 249â€“256.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017)</span>
<span class="ltx_bibblock">
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D., 2017. Making the
V in VQA Matter: Elevating the Role of Image
Understanding in Visual Question Answering. In: CVPR. pp. 6904â€“6913.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2015)</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J., 2015. Delving Deep into
Rectifiers: Surpassing Human-Level Performance on ImageNet
Classification. In: ICCV. pp. 1026â€“1034.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Homayounfar etÂ al. (2018)</span>
<span class="ltx_bibblock">
Homayounfar, N., Ma, W., Lakshmikanth, S.Â K., Urtasun, R., Jun. 2018.
Hierarchical Recurrent Attention Networks for Structured Online Maps.
In: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 3417â€“3426.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jiang, Y., Natarajan, V., Chen, X., Rohrbach, M., Batra, D., Parikh, D., Jul.
2018. Pythia v0.1: The Winning Entry to the VQA Challenge 2018.
ArXiv180709956 Cs.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalchbrenner etÂ al. (2015)</span>
<span class="ltx_bibblock">
Kalchbrenner, N., Danihelka, I., Graves, A., 2015. Grid long short-term memory.
arXiv:1507.01526.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2017)</span>
<span class="ltx_bibblock">
Kim, J.-H., On, K.-W., Kim, J., Ha, J.-W., Zhang, B.-T., 2017. Hadamard
Product for Low-Rank Bilinear Pooling. In: ICLR.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
Kingma, D.Â P., Ba, J., 2014. Adam: A Method for Stochastic
Optimization. arXiv:1412.6980.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2017)</span>
<span class="ltx_bibblock">
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.-J., Shamma, D.Â A., others, 2017. Visual genome:
Connecting language and vision using crowdsourced dense image
annotations. Int. J. Comput. Vis. 123Â (1), 32â€“73.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar and Varaiya (2015)</span>
<span class="ltx_bibblock">
Kumar, P.Â R., Varaiya, P., 2015. Stochastic Systems: Estimation,
Identification, and Adaptive Control. SIAM.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lanctot etÂ al. (2017)</span>
<span class="ltx_bibblock">
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Perolat, J.,
Silver, D., Graepel, T., Nov. 2017. A Unified Game-Theoretic Approach
to Multiagent Reinforcement Learning. arXiv:1711.00832 [cs].

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
DollÃ¡r, P., Zitnick, C.Â L., 2014. Microsoft Coco: Common Objects
in Context. In: ECCV. pp. 740â€“755.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Lu, J., Yang, J., Batra, D., Parikh, D., 2016. Hierarchical
Question-Image Co-Attention for Visual Question Answering.
In: NIPS. pp. 289â€“297.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mnih etÂ al. (2014)</span>
<span class="ltx_bibblock">
Mnih, V., Heess, N., Graves, A., Kavukcuoglu, K., 2014. Recurrent Models of
Visual Attention. In: Proceedings of the 27th International
Conference on Neural Information Processing Systems - Volume 2.
NIPSâ€™14. MIT Press, Cambridge, MA, USA, pp. 2204â€“2212.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Montavon etÂ al. (2018)</span>
<span class="ltx_bibblock">
Montavon, G., Samek, W., MÃ¼ller, K.-R., 2018. Methods for Interpreting
and Understanding Deep Neural Networks. Digit. Signal Process. 73, 1â€“15.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nallapati etÂ al. (2016)</span>
<span class="ltx_bibblock">
Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., others, 2016. Abstractive
text summarization using sequence-to-sequence rnns and beyond.
arXiv:1602.06023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nam etÂ al. (2017)</span>
<span class="ltx_bibblock">
Nam, H., Ha, J.-W., Kim, J., 2017. Dual Attention Networks for Multimodal
Reasoning and Matching. In: CVPR. pp. 299â€“307.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noh and Han (2016)</span>
<span class="ltx_bibblock">
Noh, H., Han, B., 2016. Training Recurrent Answering Units with Joint
Loss Minimization for VQA. arXiv:1606.03647.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paulus etÂ al. (2017)</span>
<span class="ltx_bibblock">
Paulus, R., Xiong, C., Socher, R., May 2017. A Deep Reinforced Model for
Abstractive Summarization. arXiv:1705.04304 [cs].

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington etÂ al. (2014)</span>
<span class="ltx_bibblock">
Pennington, J., Socher, R., Manning, C.Â D., 2014. GloVe: Global Vectors
for Word Representation. In: EMNLP. pp. 1532â€“1543.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2015)</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J., Jun. 2015. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal Networks.
arXiv:1506.01497.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rensink (2000)</span>
<span class="ltx_bibblock">
Rensink, R.Â A., Jan. 2000. The Dynamic Representation of Scenes. Vis.
Cogn. 7Â (1-3), 17â€“42.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz etÂ al. (2017)</span>
<span class="ltx_bibblock">
Schwartz, I., Schwing, A.Â G., Hazan, T., Nov. 2017. High-Order Attention
Models for Visual Question Answering. arXiv:1711.04323 [cs].

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2015)</span>
<span class="ltx_bibblock">
Shi, X., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-k., Woo, W.-c., Jun. 2015.
Convolutional LSTM Network: A Machine Learning Approach for
Precipitation Nowcasting. arXiv:1506.04214 [cs].

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney etÂ al. (2017)</span>
<span class="ltx_bibblock">
Teney, D., Anderson, P., He, X., van den Hengel, A., 2017. Tips and
Tricks for Visual Question Answering: Learnings from the 2017
Challenge. arXiv:1708.02711.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tucker (1966)</span>
<span class="ltx_bibblock">
Tucker, L.Â R., 1966. Some Mathematical Notes on Three-Mode Factor
Analysis. Psychometrika 31Â (3), 279â€“311.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al. (2016)</span>
<span class="ltx_bibblock">
Xiong, C., Merity, S., Socher, R., 2016. Dynamic Memory Networks for
Visual and Textual Question Answering. In: ICML. pp. 2397â€“2406.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Saenko (2016)</span>
<span class="ltx_bibblock">
Xu, H., Saenko, K., 2016. Ask, Attend and Answer: Exploring
Question-Guided Spatial Attention for Visual Question Answering.
In: ECCV. pp. 451â€“466.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2017a)</span>
<span class="ltx_bibblock">
Yu, Z., Yu, J., Fan, J., Tao, D., Aug. 2017a. Multi-Modal
Factorized Bilinear Pooling with Co-Attention Learning for Visual
Question Answering. ArXiv170801471 Cs.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2017b)</span>
<span class="ltx_bibblock">
Yu, Z., Yu, J., Xiang, C., Fan, J., Tao, D., 2017b. Beyond
Bilinear: Generalized Multi-Modal Factorized High-Order
Pooling for Visual Question Answering. arXiv:1708.03619.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1802.00208" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1802.00209" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1802.00209">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1802.00209" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1802.00210" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 10:53:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
