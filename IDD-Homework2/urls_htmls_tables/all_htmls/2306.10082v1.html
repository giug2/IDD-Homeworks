<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.10082] DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding</title><meta property="og:description" content="The human brain possesses remarkable abilities in visual processing, including image recognition and scene summarization. Efforts have been made to understand the cognitive capacities of the visual brain, but a compreh…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.10082">

<!--Generated on Wed Feb 28 23:43:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Subhrasankar Chatterjee 
<br class="ltx_break">Indian Institute of Technology, Kharagpur
<br class="ltx_break">Kharagpur-721302, West Bengal, India.
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">subhrasankarphd@iitkgp.ac.in</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Debasis Samanta
<br class="ltx_break">Indian Institute of Technology, Kharagpur
<br class="ltx_break">Kharagpur-721302, West Bengal, India.
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">dsamanta@iitkgp.ac.in</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">The human brain possesses remarkable abilities in visual processing, including image recognition and scene summarization. Efforts have been made to understand the cognitive capacities of the visual brain, but a comprehensive understanding of the underlying mechanisms still needs to be discovered. Advancements in brain decoding techniques have led to sophisticated approaches like fMRI-to-Image reconstruction, which has implications for cognitive neuroscience and medical imaging. However, challenges persist in fMRI-to-image reconstruction, such as incorporating global context and contextual information. In this article, we propose fMRI captioning, where captions are generated based on fMRI data to gain insight into the neural correlates of visual perception. This research presents DreamCatcher, a novel framework for fMRI captioning. DreamCatcher consists of the Representation Space Encoder (RSE) and the RevEmbedding Decoder, which transform fMRI vectors into a latent space and generate captions, respectively. We evaluated the framework through visualization, dataset training, and testing on subjects, demonstrating strong performance. fMRI-based captioning has diverse applications, including understanding neural mechanisms, Human-Computer Interaction, and enhancing learning and training processes.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The human brain manifests excellent proficiencies in visual processing, encompassing image recognition and scene summarization. Considerable endeavors have been devoted to elucidating the cognitive capacities inherent in the visual brain. Nevertheless, a comprehensive understanding of the fundamental mechanisms that underlie human visual processing remains an unresolved endeavor.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.10082/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="413" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustrative example of current issues with fMRI-to-Image reconstruction.<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium"> First reconstruction example successfully captures the low-level features however misses the high-level features. Second reconstruction example is adequate at a object-level replication, however misses the context in which the objects are to be placed.</span></span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2306.10082/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">fMRI-Captioning:<span id="S1.F2.4.2.1" class="ltx_text ltx_font_medium"> Subject is presented with an image stimulus and fMRI Neural Responses were captured. Given an fMRI response, the task is to predict captions based on the visual stimulus.</span></span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Functional magnetic resonance imaging (fMRI) has emerged as an invaluable instrument for scrutinizing the neural activity of the human brain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Over time, fMRI-based brain decoding techniques have progressed from rudimentary fMRI classification approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to the more sophisticated realm of fMRI-to-Image reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. This evolutionary trajectory holds significant implications for both the comprehension of neural mechanisms and the practical application of such knowledge. Particularly, domains such as fMRI-to-Image reconstruction have the potential to revolutionize fields, including cognitive neuroscience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and even medical imaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. These advancements paved the way for proposing a more sophisticated approach to brain decoding known as fMRI captioning(Please refer to <a href="#S1.F2" title="In 1 Introduction ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>). In this domain, captions are generated for input stimuli based on fMRI data, affording a deeper understanding of the neural correlates of visual perception.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Deep generative models, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Latent Diffusion Models (LDMs), have significantly advanced visual reconstruction. These models have been widely used to reconstruct complete images by mapping brain signals to latent variables. Successful applications include face reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> , single-object-centered image reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> , and complex scene reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Previous studies have focused on datasets like Generic Object Decoding and Deep Image Reconstruction derived from ImageNet, and demonstrated improvements in reconstruction quality using approaches such as deep generator networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, supervised and unsupervised training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, BigBiGAN-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and dual VAE-GAN models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The Natural Scenes Dataset (NSD), curated by Allen et al. (2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, has emerged as a benchmark for fMRI-based natural scene reconstruction, with studies employing models such as StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, and improved IC-GAN frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to reconstruct images and estimate pose.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite the advancement in architectures for fMRI-to-image reconstruction, specific inherent challenges persist. Firstly, the reconstruction process is typically fragment-based, necessitating understanding the image’s global context within most frameworks. Secondly, while fragment-based reconstructions can effectively capture low-level object features, they often need help to incorporate the contextual information of the image. As illustrated in <a href="#S1.F1" title="In 1 Introduction ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, taken from the work of Gu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the first part of the image demonstrates the reconstruction of an airplane, revealing the successful capture of low-level features but a failure to reproduce high-level features resulting in fragmented reconstruction. Similarly, the human objects are adequately replicated in the second part of the image, but the contextual aspects remain absent. These substantial challenges are addressed through the application of fMRI-captioning techniques.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2306.10082/assets/x3.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S1.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Algorithmic Pipeline:<span id="S1.F3.4.2.1" class="ltx_text ltx_font_medium"> The human subject was presented with visual stimuli while fMRI was being recorded. For each visual stimuli, captions were also produced by the subject. The DreamCatcher Framework has two components: the RSE takes in the fMRI neural response and generates a 1536-D GPT Embedding, the RevEmbedding Decoder uses a pone-to-many sequential model that takes the GPT Embedding and produces a caption. The loss is calculated using the original human caption and predicted fMRI caption.</span></span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this research paper, we introduce a novel framework called DreamCatcher for fMRI captioning. The DreamCatcher framework comprises two key components: the Representation Space Encoder (RSE) and the RevEmbedding Decoder(Please refer to <a href="#S1.F3" title="In 1 Introduction ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>). The RSE is designed as a standard neural network architecture, which takes preprocessed fMRI vectors as input and projects them onto an N-Dimensional Representation space. Our framework adopts a 1536-D GPT Embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> as the representation space, as it is well-suited for language-based transformations. The RSE acts as a transformation function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> that maps the fMRI input space to a latent space based on a pre-trained Large Language Model (LLM). However, existing reverse embedding techniques often rely on approximations unsuitable for our purpose. Hence, the RevEmbedding Decoder, the second part of our model, is implemented as a One-to-many LSTM Decoder. It takes the GPT Embedding as input and generates the desired captions. To evaluate the performance of our framework, we conducted three sets of experiments. Firstly, we visualized the generated representation space to assess its ability to capture adequate representations. Secondly, we trained and tested the entire framework using the Natural Scene Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and MS-COCO Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to establish its feasibility. Since fMRI captioning is still a nascent field, we could not compare our framework directly with state-of-the-art models. Finally, we evaluated the effectiveness of our framework by testing it on two subjects, employing metrics such as METEOR, Sentence, and Perplixity. The results indicate that our DreamCatcher framework exhibits strong performance as an fMRI captioning model.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">fMRI-based captioning finds application in understanding neural mechanisms and domains like Human-Computer Interaction. Researchers can design more intuitive and responsive interfaces that adapt to users’ cognitive states by discerning the brain’s response to diverse visual inputs. The utilization of fMRI-based caption generation has the potential to support learning and training processes, particularly in educational settings. Researchers can discern patterns associated with successful learning or engagement by analyzing neural responses to visual stimuli during educational tasks. This amalgamation of neuroimaging techniques and educational contexts holds promise for enhancing pedagogical practices through a nuanced understanding of the brain’s cognitive responses to visual stimuli.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">DreamCatcher effectively addresses the limitations inherent in fragment-based reconstructions by incorporating contextual information through an LSTM module. This approach enables more comprehensive and coherent reconstructions of visual stimuli, capturing not only low-level object features but also high-level contextual aspects. Notably, the DreamCatcher framework has the potential for versatility by being independent of neural response modality, making it applicable to other modalities such as EEG(Electroencephalogram) or ECoG(Electrocorticogram). This adaptability extends the potential of DreamCatcher in real-time applications.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">To summarize, the contributions of this research are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Introduction of fMRI Captioning:</span> This study introduces the fMRI captioning domain as an alternative approach to traditional fMRI-to-Image Reconstruction for Neural Decoding.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Proposal of the DreamCatcher Framework:</span> The DreamCatcher framework is proposed as a feasibility test for fMRI captioning.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Verification of GPT Embedding Space Applicability:</span> This research validates the use of GPT Embedding space as a brain representation space within the DreamCatcher framework. The effectiveness of this representation space is demonstrated through empirical evaluation, providing evidence for its utility in fMRI captioning tasks.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F4" class="ltx_figure"><img src="/html/2306.10082/assets/x4.png" id="S1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F4.9.4.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S1.F4.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Block Diagram demonstrating the embedding mechanism of GPT Embedding and the Reverse Embedding Mechanism of our Proposed Framework.<span id="S1.F4.6.3.3" class="ltx_text ltx_font_medium"> The GPT Embedding Model uses a Transformer that takes in two texts <math id="S1.F4.4.1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S1.F4.4.1.1.m1.1b"><mi id="S1.F4.4.1.1.m1.1.1" xref="S1.F4.4.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S1.F4.4.1.1.m1.1c"><ci id="S1.F4.4.1.1.m1.1.1.cmml" xref="S1.F4.4.1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F4.4.1.1.m1.1d">x</annotation></semantics></math> and <math id="S1.F4.5.2.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S1.F4.5.2.2.m2.1b"><mi id="S1.F4.5.2.2.m2.1.1" xref="S1.F4.5.2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S1.F4.5.2.2.m2.1c"><ci id="S1.F4.5.2.2.m2.1.1.cmml" xref="S1.F4.5.2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F4.5.2.2.m2.1d">y</annotation></semantics></math> , for each instance <math id="S1.F4.6.3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S1.F4.6.3.3.m3.1b"><mi id="S1.F4.6.3.3.m3.1.1" xref="S1.F4.6.3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S1.F4.6.3.3.m3.1c"><ci id="S1.F4.6.3.3.m3.1.1.cmml" xref="S1.F4.6.3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F4.6.3.3.m3.1d">i</annotation></semantics></math>, and calculates the cosine similarity between them. Hence, training such a language model over a large corpus generates the GPT Embedding Space which adequately captures the contextual relationship among texts.</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Survey</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Deep Brain Reconstruction:</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Deep generative models have been pivotal in advancing visual reconstruction, particularly in deep learning. Prominent models such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Latent Diffusion Models (LDMs) have been extensively employed to reconstruct entire images. The standard approach involves using pre-trained deep generative models to learn mappings reconstructing the corresponding latent variables from brain signals. This approach has successfully reconstructed various images, including faces, single-object-centered images, and complex scenes.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Previous studies in visual stimulus reconstruction have primarily focused on the Generic Object Decoding and Deep Image Reconstruction datasets, which are derived from the ImageNet dataset and involve training and testing images with varying fMRI repetitions. Notable research has emerged from this line of investigation. The initial problem was addressed by Shen et al. (2019) through an optimization method utilizing a deep generator network and fMRI-decoded CNN features<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. They optimized image pixel values to align with fMRI-decoded features. Beliy et al. (2019) introduced supervised and unsupervised training approaches for fMRI-to-image reconstruction networks to address the scarcity of labeled data, allowing training on ”unlabeled” data without fMRI or images<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Mozafari et al. (2020) recognized the issue of unrecognizable objects in reconstructed images due to an emphasis on pixel-level similarity. They proposed the BigBiGAN-based reconstruction model, which focused on preserving object recognition<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Ren et al. (2021) tackled limitations in fMRI data, such as low signal-to-noise ratio and limited spatial resolution, with a Dual VAE-GAN model that learned visually guided latent cognitive representations from fMRI signals and reconstructed image stimuli<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Ozcelik et al. (2022) addressed the challenge of simultaneously reconstructing low-level and high-level image features. They introduced the Instance-Conditioned GAN model, which captured precise semantics and poses information<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Chen et al. (2022) addressed the lack of fMRI-image pairs and effective biological guidance, leading to blurry and semantically meaningless reconstructions<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Their solution involved a sparse masked brain modeling approach and a double-conditioned diffusion model for establishing a precise and generalizable connection between brain activity and visual stimuli.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">In recent years, Allen et al. (2022) introduced the Natural Scenes Dataset (NSD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as a benchmark for fMRI-based natural scene reconstruction. Lin et al. (2022) adapted the StyleGAN2 model using the Lafite framework for text-to-image generation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, while Takagi et al. (2022) and Gu et al. (2022) utilized Stable Diffusion and an improved IC-GAN framework for image reconstruction and pose estimation, respectively<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Conceptual Background</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Word Embedding:</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">Word embedding is a fundamental technique employed in natural language processing (NLP) to represent words in a continuous and low-dimensional vector space(Please refer to <a href="#S1.F4" title="In 1 Introduction ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>), referred to as the latent space. Its purpose is to capture the semantic and syntactic relationships among words based on their contextual usage within a vast text corpus.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p">In contrast to the traditional approach of representing words in NLP using one-hot encoding, where words are represented as sparse binary vectors, word embedding overcomes this limitation by mapping words into a dense vector space. This transformation allows for the capture of meaningful relationships between words, as the spatial proximity of word vectors reflects the semantic similarity between the corresponding words. This proximity arises from the observation that words appearing in similar contexts tend to have similar vector representations.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p3.1" class="ltx_p">Popular algorithms for generating word embeddings include Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and FastText <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which leverage large text datasets to learn word representations. GPT, a prominent language model developed by OpenAI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, utilizes word embeddings as an integral component of its architecture. GPT employs contextual word embeddings, also known as contextualized representations, which capture the context-dependent meaning of words. Unlike traditional word embeddings, which assign fixed vector representations to each word, contextual word embeddings in GPT consider the target word, its neighboring words, and the overall sentence or document to generate word representations. The word embedding mechanism in GPT is built upon the Transformer architecture, a deep neural network model designed explicitly for sequence-to-sequence tasks. GPT employs a multi-layer Transformer encoder to process input text and generate contextualized word representations.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reverse Embedding:</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">Reverse word embedding, also called word decoding or word reconstruction, is a crucial process in converting a numerical representation, typically in the form of a vector, back into its corresponding word or textual representation(Please refer to <a href="#S1.F4" title="In 1 Introduction ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). The inverse operation of word embedding maps continuous vector representations within a high-dimensional space back onto its respective word or text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.1" class="ltx_p">Reverse word embedding plays a significant role in various applications, including language generation, machine translation, and text summarization, where the generated or translated text needs to be transformed back into its original word form. By reconstructing words from their numerical representations, reverse word embedding bridges the continuous vector space and the discrete word space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p3.1" class="ltx_p">The underlying principle of reverse word embedding lies in comprehending how word embeddings are learned and utilized. It relies on the associations and relationships learned within the embedding space to reconstruct the original words. This inversion process entails finding the word closest to a given vector representation in the embedding space. Different approaches can be employed, such as nearest neighbor search or computing the cosine similarity between the vector and all the word embeddings within a pre-trained embedding model.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2306.10082/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustrative example for the RSE model.<span id="S3.F5.4.2.1" class="ltx_text ltx_font_medium"> The RSE model uses a vanilla Neural Network that uses a Mean Squared Error Loss(MSE) to learn the GPT Embedding Space from the input fMRI space.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed Framework</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The details graphical illustration of the DreamCatcher Framework can be found in <a href="#S1.F3" title="In 1 Introduction ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Representation Space Encoder</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The first module of the DreamCatcher framework is the Representation Space Encoder(Please refer to <a href="#S3.F5" title="In Reverse Embedding: ‣ 3 Conceptual Background ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>), which facilitates the conversion of preprocessed fMRI vectors into the GPT Embedding Space (GPTES). To obtain the fMRI vectors, the Natural Scenes Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is utilized. In contrast, the corresponding captions are obtained from the MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. These captions undergo conversion into GPT Embeddings via the Embedding API provided by OpenAI.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.3" class="ltx_p">In contrast to traditional word embeddings, the GPTES incorporates the contextual meaning of words. It is trained using a contrastive objective on paired data. Using cosine similarity, Transformer Encoder <math id="S4.SS1.p2.1.m1.1" class="ltx_math_unparsed" alttext="G(.)" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1b"><mi id="S4.SS1.p2.1.m1.1.1">G</mi><mrow id="S4.SS1.p2.1.m1.1.2"><mo stretchy="false" id="S4.SS1.p2.1.m1.1.2.1">(</mo><mo lspace="0em" rspace="0.167em" id="S4.SS1.p2.1.m1.1.2.2">.</mo><mo stretchy="false" id="S4.SS1.p2.1.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">G(.)</annotation></semantics></math> calculates the appropriate distance between a given training pair (<math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><msub id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">x</mi><mi id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">𝑥</ci><ci id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">x_{i}</annotation></semantics></math>, <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">y</mi><mi id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝑦</ci><ci id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">y_{i}</annotation></semantics></math>). This similarity measure ensures the preservation of contextual meaning within the GPTES.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="v_{x}=G(x_{i})" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><msub id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">v</mi><mi id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml">x</mi></msub><mo id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"></eq><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝑣</ci><ci id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">𝑥</ci></apply><apply id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><times id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.2"></times><ci id="S4.E1.m1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.3">𝐺</ci><apply id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.E1.m1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">v_{x}=G(x_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="v_{y}=G(y_{i})" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><msub id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml">v</mi><mi id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">y</mi></msub><mo id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.3" xref="S4.E2.m1.1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S4.E2.m1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"></eq><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">𝑣</ci><ci id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">𝑦</ci></apply><apply id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><times id="S4.E2.m1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.2"></times><ci id="S4.E2.m1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.3">𝐺</ci><apply id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2">𝑦</ci><ci id="S4.E2.m1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">v_{y}=G(y_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.9" class="ltx_p">The Transformer encoder <math id="S4.SS1.p4.1.m1.1" class="ltx_math_unparsed" alttext="G(.)" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1b"><mi id="S4.SS1.p4.1.m1.1.1">G</mi><mrow id="S4.SS1.p4.1.m1.1.2"><mo stretchy="false" id="S4.SS1.p4.1.m1.1.2.1">(</mo><mo lspace="0em" rspace="0.167em" id="S4.SS1.p4.1.m1.1.2.2">.</mo><mo stretchy="false" id="S4.SS1.p4.1.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">G(.)</annotation></semantics></math> maps the given inputs, denoted as <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mi id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">x</annotation></semantics></math> and <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mi id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><ci id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">y</annotation></semantics></math>, to their corresponding embeddings, namely <math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext="v_{x}" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><msub id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml"><mi id="S4.SS1.p4.4.m4.1.1.2" xref="S4.SS1.p4.4.m4.1.1.2.cmml">v</mi><mi id="S4.SS1.p4.4.m4.1.1.3" xref="S4.SS1.p4.4.m4.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><apply id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.4.m4.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p4.4.m4.1.1.2.cmml" xref="S4.SS1.p4.4.m4.1.1.2">𝑣</ci><ci id="S4.SS1.p4.4.m4.1.1.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">v_{x}</annotation></semantics></math> and <math id="S4.SS1.p4.5.m5.1" class="ltx_Math" alttext="v_{y}" display="inline"><semantics id="S4.SS1.p4.5.m5.1a"><msub id="S4.SS1.p4.5.m5.1.1" xref="S4.SS1.p4.5.m5.1.1.cmml"><mi id="S4.SS1.p4.5.m5.1.1.2" xref="S4.SS1.p4.5.m5.1.1.2.cmml">v</mi><mi id="S4.SS1.p4.5.m5.1.1.3" xref="S4.SS1.p4.5.m5.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.1b"><apply id="S4.SS1.p4.5.m5.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.5.m5.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p4.5.m5.1.1.2.cmml" xref="S4.SS1.p4.5.m5.1.1.2">𝑣</ci><ci id="S4.SS1.p4.5.m5.1.1.3.cmml" xref="S4.SS1.p4.5.m5.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.1c">v_{y}</annotation></semantics></math>, respectively. The similarity between two inputs is quantitatively assessed by measuring the cosine similarity between their respective embeddings, <math id="S4.SS1.p4.6.m6.1" class="ltx_Math" alttext="v_{x}" display="inline"><semantics id="S4.SS1.p4.6.m6.1a"><msub id="S4.SS1.p4.6.m6.1.1" xref="S4.SS1.p4.6.m6.1.1.cmml"><mi id="S4.SS1.p4.6.m6.1.1.2" xref="S4.SS1.p4.6.m6.1.1.2.cmml">v</mi><mi id="S4.SS1.p4.6.m6.1.1.3" xref="S4.SS1.p4.6.m6.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.1b"><apply id="S4.SS1.p4.6.m6.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.6.m6.1.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p4.6.m6.1.1.2.cmml" xref="S4.SS1.p4.6.m6.1.1.2">𝑣</ci><ci id="S4.SS1.p4.6.m6.1.1.3.cmml" xref="S4.SS1.p4.6.m6.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.1c">v_{x}</annotation></semantics></math> and <math id="S4.SS1.p4.7.m7.1" class="ltx_Math" alttext="v_{y}" display="inline"><semantics id="S4.SS1.p4.7.m7.1a"><msub id="S4.SS1.p4.7.m7.1.1" xref="S4.SS1.p4.7.m7.1.1.cmml"><mi id="S4.SS1.p4.7.m7.1.1.2" xref="S4.SS1.p4.7.m7.1.1.2.cmml">v</mi><mi id="S4.SS1.p4.7.m7.1.1.3" xref="S4.SS1.p4.7.m7.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.7.m7.1b"><apply id="S4.SS1.p4.7.m7.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.7.m7.1.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.p4.7.m7.1.1.2.cmml" xref="S4.SS1.p4.7.m7.1.1.2">𝑣</ci><ci id="S4.SS1.p4.7.m7.1.1.3.cmml" xref="S4.SS1.p4.7.m7.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.7.m7.1c">v_{y}</annotation></semantics></math>. The cosine similarity metric provides a measure of the directional similarity between the two vectors in the embedding space. It determines the cosine of the angle between the vectors, which ranges from <math id="S4.SS1.p4.8.m8.1" class="ltx_Math" alttext="-1" display="inline"><semantics id="S4.SS1.p4.8.m8.1a"><mrow id="S4.SS1.p4.8.m8.1.1" xref="S4.SS1.p4.8.m8.1.1.cmml"><mo id="S4.SS1.p4.8.m8.1.1a" xref="S4.SS1.p4.8.m8.1.1.cmml">−</mo><mn id="S4.SS1.p4.8.m8.1.1.2" xref="S4.SS1.p4.8.m8.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.8.m8.1b"><apply id="S4.SS1.p4.8.m8.1.1.cmml" xref="S4.SS1.p4.8.m8.1.1"><minus id="S4.SS1.p4.8.m8.1.1.1.cmml" xref="S4.SS1.p4.8.m8.1.1"></minus><cn type="integer" id="S4.SS1.p4.8.m8.1.1.2.cmml" xref="S4.SS1.p4.8.m8.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.8.m8.1c">-1</annotation></semantics></math> (indicating complete dissimilarity) to <math id="S4.SS1.p4.9.m9.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS1.p4.9.m9.1a"><mn id="S4.SS1.p4.9.m9.1.1" xref="S4.SS1.p4.9.m9.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.9.m9.1b"><cn type="integer" id="S4.SS1.p4.9.m9.1.1.cmml" xref="S4.SS1.p4.9.m9.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.9.m9.1c">1</annotation></semantics></math> (representing perfect similarity).</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.6" class="ltx_Math" alttext="sim(x,y)=\frac{v_{x}.v_{y}}{||v_{x}||.||v_{y}||}" display="block"><semantics id="S4.E3.m1.6a"><mrow id="S4.E3.m1.6.7" xref="S4.E3.m1.6.7.cmml"><mrow id="S4.E3.m1.6.7.2" xref="S4.E3.m1.6.7.2.cmml"><mi id="S4.E3.m1.6.7.2.2" xref="S4.E3.m1.6.7.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.6.7.2.1" xref="S4.E3.m1.6.7.2.1.cmml">​</mo><mi id="S4.E3.m1.6.7.2.3" xref="S4.E3.m1.6.7.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.6.7.2.1a" xref="S4.E3.m1.6.7.2.1.cmml">​</mo><mi id="S4.E3.m1.6.7.2.4" xref="S4.E3.m1.6.7.2.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.6.7.2.1b" xref="S4.E3.m1.6.7.2.1.cmml">​</mo><mrow id="S4.E3.m1.6.7.2.5.2" xref="S4.E3.m1.6.7.2.5.1.cmml"><mo stretchy="false" id="S4.E3.m1.6.7.2.5.2.1" xref="S4.E3.m1.6.7.2.5.1.cmml">(</mo><mi id="S4.E3.m1.5.5" xref="S4.E3.m1.5.5.cmml">x</mi><mo id="S4.E3.m1.6.7.2.5.2.2" xref="S4.E3.m1.6.7.2.5.1.cmml">,</mo><mi id="S4.E3.m1.6.6" xref="S4.E3.m1.6.6.cmml">y</mi><mo stretchy="false" id="S4.E3.m1.6.7.2.5.2.3" xref="S4.E3.m1.6.7.2.5.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.6.7.1" xref="S4.E3.m1.6.7.1.cmml">=</mo><mfrac id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml"><mrow id="S4.E3.m1.2.2.2.2" xref="S4.E3.m1.2.2.2.3.cmml"><msub id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.2.cmml">v</mi><mi id="S4.E3.m1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.3.cmml">x</mi></msub><mo lspace="0em" rspace="0.167em" id="S4.E3.m1.2.2.2.2.3" xref="S4.E3.m1.2.2.2.3a.cmml">.</mo><msub id="S4.E3.m1.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.cmml">v</mi><mi id="S4.E3.m1.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.3.cmml">y</mi></msub></mrow><mrow id="S4.E3.m1.4.4.4.2" xref="S4.E3.m1.4.4.4.3.cmml"><mrow id="S4.E3.m1.3.3.3.1.1.1" xref="S4.E3.m1.3.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.E3.m1.3.3.3.1.1.1.2" xref="S4.E3.m1.3.3.3.1.1.2.1.cmml">‖</mo><msub id="S4.E3.m1.3.3.3.1.1.1.1" xref="S4.E3.m1.3.3.3.1.1.1.1.cmml"><mi id="S4.E3.m1.3.3.3.1.1.1.1.2" xref="S4.E3.m1.3.3.3.1.1.1.1.2.cmml">v</mi><mi id="S4.E3.m1.3.3.3.1.1.1.1.3" xref="S4.E3.m1.3.3.3.1.1.1.1.3.cmml">x</mi></msub><mo stretchy="false" id="S4.E3.m1.3.3.3.1.1.1.3" xref="S4.E3.m1.3.3.3.1.1.2.1.cmml">‖</mo></mrow><mo lspace="0em" rspace="0.167em" id="S4.E3.m1.4.4.4.2.3" xref="S4.E3.m1.4.4.4.3a.cmml">.</mo><mrow id="S4.E3.m1.4.4.4.2.2.1" xref="S4.E3.m1.4.4.4.2.2.2.cmml"><mo stretchy="false" id="S4.E3.m1.4.4.4.2.2.1.2" xref="S4.E3.m1.4.4.4.2.2.2.1.cmml">‖</mo><msub id="S4.E3.m1.4.4.4.2.2.1.1" xref="S4.E3.m1.4.4.4.2.2.1.1.cmml"><mi id="S4.E3.m1.4.4.4.2.2.1.1.2" xref="S4.E3.m1.4.4.4.2.2.1.1.2.cmml">v</mi><mi id="S4.E3.m1.4.4.4.2.2.1.1.3" xref="S4.E3.m1.4.4.4.2.2.1.1.3.cmml">y</mi></msub><mo stretchy="false" id="S4.E3.m1.4.4.4.2.2.1.3" xref="S4.E3.m1.4.4.4.2.2.2.1.cmml">‖</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.6b"><apply id="S4.E3.m1.6.7.cmml" xref="S4.E3.m1.6.7"><eq id="S4.E3.m1.6.7.1.cmml" xref="S4.E3.m1.6.7.1"></eq><apply id="S4.E3.m1.6.7.2.cmml" xref="S4.E3.m1.6.7.2"><times id="S4.E3.m1.6.7.2.1.cmml" xref="S4.E3.m1.6.7.2.1"></times><ci id="S4.E3.m1.6.7.2.2.cmml" xref="S4.E3.m1.6.7.2.2">𝑠</ci><ci id="S4.E3.m1.6.7.2.3.cmml" xref="S4.E3.m1.6.7.2.3">𝑖</ci><ci id="S4.E3.m1.6.7.2.4.cmml" xref="S4.E3.m1.6.7.2.4">𝑚</ci><interval closure="open" id="S4.E3.m1.6.7.2.5.1.cmml" xref="S4.E3.m1.6.7.2.5.2"><ci id="S4.E3.m1.5.5.cmml" xref="S4.E3.m1.5.5">𝑥</ci><ci id="S4.E3.m1.6.6.cmml" xref="S4.E3.m1.6.6">𝑦</ci></interval></apply><apply id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4"><divide id="S4.E3.m1.4.4.5.cmml" xref="S4.E3.m1.4.4"></divide><apply id="S4.E3.m1.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.3a.cmml" xref="S4.E3.m1.2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.2">𝑣</ci><ci id="S4.E3.m1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.3">𝑥</ci></apply><apply id="S4.E3.m1.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2">𝑣</ci><ci id="S4.E3.m1.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.3">𝑦</ci></apply></apply><apply id="S4.E3.m1.4.4.4.3.cmml" xref="S4.E3.m1.4.4.4.2"><csymbol cd="ambiguous" id="S4.E3.m1.4.4.4.3a.cmml" xref="S4.E3.m1.4.4.4.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.3.3.3.1.1.2.cmml" xref="S4.E3.m1.3.3.3.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.3.3.3.1.1.2.1.cmml" xref="S4.E3.m1.3.3.3.1.1.1.2">norm</csymbol><apply id="S4.E3.m1.3.3.3.1.1.1.1.cmml" xref="S4.E3.m1.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.3.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.3.3.3.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.3.1.1.1.1.2">𝑣</ci><ci id="S4.E3.m1.3.3.3.1.1.1.1.3.cmml" xref="S4.E3.m1.3.3.3.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S4.E3.m1.4.4.4.2.2.2.cmml" xref="S4.E3.m1.4.4.4.2.2.1"><csymbol cd="latexml" id="S4.E3.m1.4.4.4.2.2.2.1.cmml" xref="S4.E3.m1.4.4.4.2.2.1.2">norm</csymbol><apply id="S4.E3.m1.4.4.4.2.2.1.1.cmml" xref="S4.E3.m1.4.4.4.2.2.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.4.4.4.2.2.1.1.1.cmml" xref="S4.E3.m1.4.4.4.2.2.1.1">subscript</csymbol><ci id="S4.E3.m1.4.4.4.2.2.1.1.2.cmml" xref="S4.E3.m1.4.4.4.2.2.1.1.2">𝑣</ci><ci id="S4.E3.m1.4.4.4.2.2.1.1.3.cmml" xref="S4.E3.m1.4.4.4.2.2.1.1.3">𝑦</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.6c">sim(x,y)=\frac{v_{x}.v_{y}}{||v_{x}||.||v_{y}||}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">The notion of similarity in terms of word meanings and contextual cues is a universally observed phenomenon. It applies across various domains, encompassing different languages and image-based contextual similarities. Based on this assumption, we propose the utilization of GPTES as a potential candidate for the Brain Representation Space through the concept of Domain Adaptability. It is important to note that the primary objective of the DreamCatcher framework is to generate meaningful captions from fMRI data. The framework does not focus on developing a biologically plausible representation space or a biologically interpretable model.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.10082/assets/Images/res1.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="514" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.10082/assets/Images/res2.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="514" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.10082/assets/Images/res3.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="514" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.10082/assets/Images/res4.png" id="S4.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="514" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustrative Example of Original and Predicted Caption for each Visual Stimuli.<span id="S4.F6.4.2.1" class="ltx_text ltx_font_medium"> The similarity in text can be observed directly by comparing the captions, which clearly proves the feasibility of fMRI Captioning.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>RevEmbedding Decoder</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The second component of the DreamCatcher Framework is the RevEmbedding Decoder, which is responsible for converting GPT Embeddings into captions. However, due to the absence of a Reverse Embedding API in OpenAI, approximations such as Nearest Neighbour are utilized, significantly compromising the accuracy of the generated captions. Hence, the RevEmbedding Decoder model assumes paramount importance in generating captions from the embeddings predicted by the Representation Space Encoder.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.8" class="ltx_p">In the initial stage, the embeddings-caption pairs generated in the preceding step undergo preprocessing and are stored to form a custom dataset. Subsequently, a Vocabulary module is defined to facilitate the creation and mapping of words to indices. This class also encompasses a function that constructs the vocabulary based on the captions present in the dataset. The function tokenizes the captions, calculates token frequencies, and filters out infrequent words. Additionally, special tokens such as ‘<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><lt id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">&lt;</annotation></semantics></math>pad<math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mo id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><gt id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">&gt;</annotation></semantics></math>,’ ‘<math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mo id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><lt id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">&lt;</annotation></semantics></math>start<math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mo id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><gt id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">&gt;</annotation></semantics></math>,’ ‘<math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mo id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><lt id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">&lt;</annotation></semantics></math>end<math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mo id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><gt id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">&gt;</annotation></semantics></math>,’ and ‘<math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mo id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><lt id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">&lt;</annotation></semantics></math>unk<math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><mo id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><gt id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">&gt;</annotation></semantics></math>’ are incorporated into the vocabulary.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Finally, a sequential one-to-many Long Short Term Memory(LSTM) module is trained on the embedding-vocabulary pair to generate the target captions. . It receives GPT Embeddings and caption sequences as input and trains as a one-to-many model. The resulting output corresponds to the predicted captions based on the fMRI input to the Representation Space Encoder.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2306.10082/assets/x6.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">t-SNE plot for GPT Embedding Space(Left) and input fMRI Space(Right).<span id="S4.F7.4.2.1" class="ltx_text ltx_font_medium"> It can be visually observed that GPT Embedding Space provides a category-wise segregation and therefore is a better latent representation in comparison to the input fMRI Space. The segregation also demonstrates that the Embedding Space adequately captures the relationship among the fMRI class labels. </span></span></figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.6.7.1" class="ltx_tr">
<th id="S4.T1.6.7.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T1.6.7.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t"></th>
<th id="S4.T1.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">Subject 1</th>
<th id="S4.T1.6.7.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S4.T1.6.7.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">Subject 2</th>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<th id="S4.T1.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Encoder-Decoder</th>
<th id="S4.T1.6.6.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">GPT Embedding</th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Sentence<math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Meteor<math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Perplexity<math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.6.6.9" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Sentence<math id="S4.T1.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.4.4.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.m1.1.1" xref="S4.T1.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Meteor<math id="S4.T1.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.m1.1a"><mo stretchy="false" id="S4.T1.5.5.5.m1.1.1" xref="S4.T1.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.6.6.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Perplexity<math id="S4.T1.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.6.6.6.m1.1a"><mo stretchy="false" id="S4.T1.6.6.6.m1.1.1" xref="S4.T1.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.6.8.1" class="ltx_tr">
<th id="S4.T1.6.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">-</th>
<th id="S4.T1.6.8.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">-</th>
<td id="S4.T1.6.8.1.3" class="ltx_td ltx_align_center ltx_border_t">0.253</td>
<td id="S4.T1.6.8.1.4" class="ltx_td ltx_align_center ltx_border_t">0.179</td>
<td id="S4.T1.6.8.1.5" class="ltx_td ltx_align_center ltx_border_t">3.486</td>
<td id="S4.T1.6.8.1.6" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.6.8.1.7" class="ltx_td ltx_align_center ltx_border_t">0.241</td>
<td id="S4.T1.6.8.1.8" class="ltx_td ltx_align_center ltx_border_t">0.157</td>
<td id="S4.T1.6.8.1.9" class="ltx_td ltx_align_center ltx_border_t">3.845</td>
</tr>
<tr id="S4.T1.6.9.2" class="ltx_tr">
<th id="S4.T1.6.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">✓</th>
<th id="S4.T1.6.9.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">-</th>
<td id="S4.T1.6.9.2.3" class="ltx_td ltx_align_center">0.276</td>
<td id="S4.T1.6.9.2.4" class="ltx_td ltx_align_center">0.183</td>
<td id="S4.T1.6.9.2.5" class="ltx_td ltx_align_center">1.803</td>
<td id="S4.T1.6.9.2.6" class="ltx_td"></td>
<td id="S4.T1.6.9.2.7" class="ltx_td ltx_align_center">0.247</td>
<td id="S4.T1.6.9.2.8" class="ltx_td ltx_align_center">0.167</td>
<td id="S4.T1.6.9.2.9" class="ltx_td ltx_align_center">1.952</td>
</tr>
<tr id="S4.T1.6.10.3" class="ltx_tr">
<th id="S4.T1.6.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T1.6.10.3.1.1" class="ltx_text ltx_font_bold">✓</span></th>
<th id="S4.T1.6.10.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T1.6.10.3.2.1" class="ltx_text ltx_font_bold">✓</span></th>
<td id="S4.T1.6.10.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.6.10.3.3.1" class="ltx_text ltx_font_bold">0.451</span></td>
<td id="S4.T1.6.10.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.6.10.3.4.1" class="ltx_text ltx_font_bold">0.323</span></td>
<td id="S4.T1.6.10.3.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.6.10.3.5.1" class="ltx_text ltx_font_bold">1.024</span></td>
<td id="S4.T1.6.10.3.6" class="ltx_td ltx_border_b"></td>
<td id="S4.T1.6.10.3.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.6.10.3.7.1" class="ltx_text ltx_font_bold">0.422</span></td>
<td id="S4.T1.6.10.3.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.6.10.3.8.1" class="ltx_text ltx_font_bold">0.308</span></td>
<td id="S4.T1.6.10.3.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.6.10.3.9.1" class="ltx_text ltx_font_bold">1.037</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.13.3.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.10.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Detailed Results of Component Analysis of each Component of DreamCatcher Framework.<span id="S4.T1.10.2.2" class="ltx_text ltx_font_medium"> The <math id="S4.T1.9.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.9.1.1.m1.1b"><mo stretchy="false" id="S4.T1.9.1.1.m1.1.1" xref="S4.T1.9.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.1.1.m1.1c"><ci id="S4.T1.9.1.1.m1.1.1.cmml" xref="S4.T1.9.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.1.1.m1.1d">\uparrow</annotation></semantics></math> and <math id="S4.T1.10.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.10.2.2.m2.1b"><mo stretchy="false" id="S4.T1.10.2.2.m2.1.1" xref="S4.T1.10.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.2.2.m2.1c"><ci id="S4.T1.10.2.2.m2.1.1.cmml" xref="S4.T1.10.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.2.2.m2.1d">\downarrow</annotation></semantics></math> represents the desired value for each metric.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Result and Analysis</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dataset</h3>

<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Natural Scene Dataset:</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">A comprehensive account of the Natural Scenes Dataset (NSD), specifications, and data acquisition procedures can be found in a publication by Allen et al. in Nature Neuroscience (2021) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The NSD dataset encompasses functional Magnetic Resonance Imaging (fMRI) measurements collected from 8 participants who were presented with a substantial number of distinct color natural scenes, ranging from 9,000 to 10,000 images (22,000 to 30,000 trials) across 30 to 40 scan sessions. The fMRI scanning was conducted at 7T using whole-brain gradient-echo Echo Planar Imaging (EPI) at a resolution of 1.8 mm and a repetition time of 1.6 seconds.</p>
</div>
<div id="S5.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p2.1" class="ltx_p">The images utilized in the NSD dataset were sourced from the Microsoft Common Objects in Context (COCO) database, square-cropped, and displayed at a size of 8.4° x 8.4°. Out of the total images, a specific set of 1,000 images was shared across all participants, while the remaining images were mutually exclusive for each participant. The images were presented for a duration of 3 seconds, with 1-second gaps between successive images. During the scanning sessions, the participants fixated centrally and engaged in a continuous long-term recognition task related to the presented images.</p>
</div>
<div id="S5.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p3.1" class="ltx_p">Pre-processing of the fMRI data involved performing temporal interpolation to correct slice time differences and spatial interpolation to account for head motion artifacts. Subsequently, a general linear model was employed to estimate single-trial beta weights. The NSD dataset also encompasses cortical surface reconstructions generated using FreeSurfer, with both volume- and surface-based versions of the beta weights being created for further analysis and interpretation.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MS COCO Dataset:</h4>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">The MS COCO (Microsoft Common Objects in Context) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> dataset has emerged as a significant benchmark for advancing the field of computer vision. It provides a diverse and comprehensive collection of annotated images, enabling the development and evaluation of a wide range of vision tasks, including object detection, segmentation, and image captioning. MS COCO Captions provide detailed textual descriptions for the images in the MS COCO dataset. With over 330,000 images, each accompanied by multiple human-generated captions, this annotation aspect of MS COCO has revolutionized image captioning research. The captions capture the salient objects, their attributes, and contextual information concisely and descriptively. They serve as a valuable resource for training and evaluating image captioning models, pushing the boundaries of image understanding and natural language processing. By bridging the gap between visual and textual domains, MS COCO captions have enabled advancements in generating human-like image descriptions, opening up new avenues for multimodal research.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Feasiblity test of fMRI Captioning</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The potential of generating textual descriptions directly from fMRI data offers a promising avenue for understanding brain activity and decoding mental representations. A feasibility test was conducted using a dataset of fMRI recordings obtained from 8 participants engaged in visual stimulus tasks(Natural Scenes Dataset). The fMRI data were preprocessed and subjected to feature extraction techniques to capture relevant brain activity patterns. Preliminary results from the feasibility test showed promising performance of the fMRI captioning model. The generated captions exhibited reasonable coherence and semantic relevance, capturing essential aspects of the visual stimuli. The graphical illustration can be found in <a href="#S4.F6" title="In 4.1 Representation Space Encoder ‣ 4 Proposed Framework ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Efficacy of DreamCatcher Framework</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Since fMRI captioning is still a nascent field, directly comparing our framework with state-of-the-art models was not feasible. Nevertheless, we conducted a comprehensive evaluation to assess the effectiveness of our framework. The evaluation of our proposed framework involved testing our model on two subjects and employing several metrics, including METEOR, Sentence, and Perplexity. The results of our evaluation demonstrated promising performance and the potential of our framework in generating accurate and coherent captions from fMRI data. The METEOR metric, which measures the quality of generated captions by comparing them to reference captions, indicated favorable scores across all subjects. The Sentence metric, which evaluates the syntactic and semantic similarity between generated and reference captions, also showed encouraging results. The detailed result for our model can be obtained in <a href="#S4.T1" title="In 4.2 RevEmbedding Decoder ‣ 4 Proposed Framework ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Verification of GPT Embedding Space Applicability</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">To evaluate the ability of our framework to capture adequate representations, we conducted a visualization of the generated representation space using PCA and t-SNE. This analysis aimed to provide insights into the distribution and clustering of the representations, indicating the framework’s capacity to capture and differentiate between various features and attributes effectively.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">The visualization of the generated representation space revealed encouraging results. The representations exhibited clear separation and distinct clusters, suggesting that our framework successfully captured the relevant information from the input fMRI data. The graphical illustration can be found in <a href="#S4.F7" title="In 4.2 RevEmbedding Decoder ‣ 4 Proposed Framework ‣ DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The human brain’s exceptional proficiency in visual processing, including image recognition and scene summarization, have been the subject of extensive research. However, despite significant efforts, a comprehensive understanding of the fundamental mechanisms underlying human visual processing still needs to be discovered. Functional magnetic resonance imaging (fMRI) has emerged as a valuable tool for investigating the neural activity of the human brain, leading to advancements in brain decoding techniques from primary classification approaches to more sophisticated fMRI-to-Image reconstruction methods.
The contributions of this research include the introduction of fMRI captioning as an alternative approach to fMRI-to-Image Reconstruction, the proposal and feasibility testing of the DreamCatcher framework, and the validation of the GPT Embedding space as a suitable brain representation space for fMRI captioning tasks.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In summary, the advancements in fMRI-based brain decoding techniques, particularly in fMRI captioning, provide valuable insights into the neural mechanisms underlying human visual processing. The DreamCatcher framework represents a significant step forward in capturing the rich cognitive processes involved in visual perception, and its versatility makes it adaptable to other modalities. Continued research in this area holds great promise for expanding our understanding of the human brain and improving various applications, ranging from cognitive neuroscience to educational practices and human-computer interaction.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Emily Allen, Ghislain St-Yves, Yihan Wu, Jesse Breedlove, Jacob Prince, Logan
Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, J.
Hutchinson, Thomas Naselaris, and Kendrick Kay.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">A massive 7t fmri dataset to bridge cognitive neuroscience and
artificial intelligence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Neuroscience</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 25, 01 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal
Irani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">From voxels to pixels and back: Self-supervision in natural-image
reconstruction from fmri, 07 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Enriching word vectors with subword information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">,
5, 07 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Mercy Bore, Xiqin Liu, Xianyang Gan, Lan Wang, Ting Xu, Stefania Ferraro,
Liyuan Li, Bo Zhou, Jie Zhang, Deniz Vatansever, Benjamin Klugah-Brown, and
Benjamin Becker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Distinct neurofunctional alterations during motivational and hedonic
processing of natural and monetary rewards in depression - a neuroimaging
meta-analysis, 12 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Yue, and Juan Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Seeing beyond the brain: Conditional diffusion model with sparse
masked modeling for vision decoding, 11 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Huikai Chua, Andrew Caines, and Helen Yannakoudakis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">A unified framework for cross-domain and cross-task learning of
mental health conditions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Second Workshop on NLP for Positive Impact
(NLP4PI)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 1–14, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022.
Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
David Cox and R. Savoy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Functional magnetic resonance imaging (fmri) ”brain reading”:
Detecting and classifying distributed patterns of fmri activity in human
visual cortex.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeuroImage</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 19:261–70, 07 2003.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Thirza Dado, Yağmur Güçlütürk, Luca Ambrogioni, Gabriëlle Ras, Sander
Bosch, Marcel Gerven, and Umut Güçlü.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Hyperrealistic neural decoding for reconstructing faces from fmri
activations via the gan latent space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Scientific Reports</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 12, 01 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Zijin Gu, Keith Jamison, Amy Kuceyeski, and Mert Sabuncu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Decoding natural image stimuli from fmri data with a surface-based
convolutional network, 12 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
James Haxby, Maria Gobbini, Maura Furey, Alumit Ishai, Jennifer Schouten, and
Pietro Pietrini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Distributed and overlapping representations of faces and objects in
ventral temporal cortex.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Science (New York, N.Y.)</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 293:2425–30, 10 2001.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
James Haxby, Jyothi Swaroop Guntupalli, Andrew Connolly, Yaroslav Halchenko,
Bryan Conroy, Maria Gobbini, Michael Hanke, and Peter Ramadge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">A common, high-dimensional model of the representational space in
human ventral temporal cortex.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neuron</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 72:404–16, 10 2011.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
John-Dylan Haynes and Geraint Rees.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Decoding mental states from brain activity in human.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature reviews. Neuroscience</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 7:523–34, 08 2006.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Yukiyasu Kamitani and Frank Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Decoding the visual and subjective contents of the human brain.
nature neurosci. 8, 679-685.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature neuroscience</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 8:679–85, 06 2005.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Kohitij Kar, Simon Kornblith, and Evelina Fedorenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Interpretability of artificial neural network models in artificial
intelligence versus neuroscience.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Machine Intelligence</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 4, 12 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Kendrick Kay, Thomas Naselaris, Ryan Prenger, and Jack Gallant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Identifying natural images from human brain activity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 452:352–5, 04 2008.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Nikolaus Kriegeskorte and Pamela Douglas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Cognitive computational neuroscience, 07 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Sikun Lin, Thomas Sprague, and Ambuj Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Mind reader: Reconstructing complex images from brain activities, 09
2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">volume 8693, 04 2014.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Tomas Mikolov, Kai Chen, G.s Corrado, and Jeffrey Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Efficient estimation of word representations in vector space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of Workshop at ICLR</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2013, 01 2013.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yoichi Miyawaki, Hajime Uchida, Okito Yamashita, Masa-aki Sato, Yusuke Morito,
Hiroki Tanabe, Norihiro Sadato, and Yukiyasu Kamitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Visual image reconstruction from human brain activity using a
combination of multiscale local image decoders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neuron</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 60:915–29, 01 2009.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Milad Mozafari, Leila Reddy, and Rufin VanRullen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Reconstructing natural scenes from fmri patterns using bigbigan, 01
2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Thomas Naselaris, Kendrick Kay, Shinji Nishimoto, and Jack Gallant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Encoding and decoding in fmri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeuroImage</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 56:400–10, 05 2011.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Thomas Naselaris, Ryan Prenger, Kendrick Kay, Michael Oliver, and Jack Gallant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Bayesian reconstruction of natural images from human brain activity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neuron</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 63:902–15, 09 2009.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Han, Jerry Tworek,
Qiming Yuan, Nikolas Tezak, Jong Kim, Chris Hallacy, Johannes Heidecke,
Pranav Shyam, Boris Power, Tyna Nekoul, Girish Sastry, Gretchen Krueger,
David Schnurr, Felipe Such, Kenny Hsu, and Lilian Weng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Text and code embeddings by contrastive pre-training, 01 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Shinji Nishimoto, An Vu, Thomas Naselaris, Yuval Benjamini, B. Yu, and Jack
Gallant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Reconstructing visual experiences from brain activity evoked by
natural movies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Current biology : CB</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 21:1641–6, 09 2011.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Norman, Sean Polyn, Greg Detre, and James Haxby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Beyond mind-reading: Multi-voxel pattern analysis of fmri data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Trends in cognitive sciences</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 10:424–30, 10 2006.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, and Rufin
VanRullen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Reconstruction of perceived images from fmri patterns and semantic
brain exploration using instance-conditioned gans, 02 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Sebastian Palacio, Joachim Folz, Jörn Hees, Federico Raue, Damian Borth, and
Andreas Dengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">What do deep networks like to see?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">03 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and Christopher Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">volume 14, pages 1532–1543, 01 2014.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Roshini Randeniya, Jason B. Mattingley, and Marta I. Garrido.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Increased context adjustment is associated with auditory
sensitivities but not with autistic traits.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Autism Research</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 15(8):1457–1468, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, and Xinbo Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Reconstructing seen image from brain activity by visually-guided
cognitive representation and adversarial learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeuroImage</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 228:117602, 03 2021.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Guohua Shen, Tomoyasu Horikawa, Kei Majima, and Yukiyasu Kamitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Deep image reconstruction from human brain activity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">PLOS Computational Biology</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 15:e1006633, 01 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Yu Takagi and Shinji Nishimoto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">High-resolution image reconstruction with latent diffusion models
from human brain activity, 11 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Rufin VanRullen and Leila Reddy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Reconstructing faces from fmri patterns using deep generative neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications Biology</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2, 05 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
Gomez, Lukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">06 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Sudheendra Vijayanarasimhan, Jon Shlens, Rajat Monga, and Jay Yagnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Deep networks with large output spaces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">12 2014.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Barret Zoph, Ashish Vaswani, Jonathan May, and Kevin Knight.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Simple, fast noise-contrastive estimation for large rnn vocabularies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">pages 1217–1222, 01 2016.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.10081" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.10082" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.10082">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.10082" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.10083" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 23:43:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
