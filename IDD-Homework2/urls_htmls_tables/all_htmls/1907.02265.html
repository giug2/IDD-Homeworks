<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1907.02265] Supervised Symbolic Music Style Translation Using Synthetic Data</title><meta property="og:description" content="Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style.
More recently, several attempts have been made t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Supervised Symbolic Music Style Translation Using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Supervised Symbolic Music Style Translation Using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1907.02265">

<!--Generated on Sat Mar  2 15:57:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Supervised Symbolic Music Style Translation Using Synthetic Data</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style.
More recently, several attempts have been made to extend such approaches to music (both symbolic and audio) in order to enable transforming musical style in a similar manner.
In this study, we focus on <em id="id1.id1.1" class="ltx_emph ltx_font_italic">symbolic music</em> with the goal of altering the ‘style’ of a piece while keeping its original ‘content’.
As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of ‘aligned’ data (i.e. the same musical piece played in multiple styles), we develop the first fully <em id="id1.id1.2" class="ltx_emph ltx_font_italic">supervised</em> algorithm for this task. At the core of our approach lies a <em id="id1.id1.3" class="ltx_emph ltx_font_italic">synthetic data</em> generation scheme which allows us to produce virtually unlimited amounts of aligned data, and hence avoid the above issue.
In view of this data generation scheme, we propose an encoder-decoder model for translating symbolic music <em id="id1.id1.4" class="ltx_emph ltx_font_italic">accompaniments</em> between a number of different styles. Our experiments show that our models, although trained entirely on synthetic data, are capable of producing musically meaningful accompaniments even for real (non-synthetic) MIDI recordings.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Artistic style transfer has become a well-established topic in the computer vision literature and is becoming of increasing interest in other areas of computer science, especially music and natural language processing.
More generally, we are dealing with a family of <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">style transformation</em> tasks, where the goal is to alter the <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">style</em> of a piece of data (e.g., an image, a musical piece, a document) while preserving – to some extent – its <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">content</em>.
In the music domain, a solution to these problems would have exciting industrial applications, not only as a way to generate new music automatically (as an alternative to fully automatic music composition, which still seems to be a distant goal), but also as a tool for music creators, allowing them to easily incorporate new styles and ideas into their work.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In computer vision, the most popular task in this direction is <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">style transfer</em>, where the algorithm has two inputs: the ‘content’ image to transform and a ‘style’ image, bearing the style that we wish to impose on (or <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">transfer</em> to) the content image.
On the other hand, work done on music so far has mostly focused on a different task, which we refer to as <em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">style translation</em>.
Contrary to style transfer, only the ‘content’ input is given, and the goal is to render it in a target style which is known in advance and usually <em id="S1.p2.1.4" class="ltx_emph ltx_font_italic">learned</em> from a large set of examples.
Note that although this second task is often also referred to as ‘style transfer’ in the context of music and text generation, we claim that this conflicts with how the term is traditionally understood <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>,
and that the term ‘translation’ is more appropriate and in line with other prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The focus of our work is on the latter task, and more specifically, on <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">accompaniment style translation</em> for <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">symbolic music</em>.
In particular, given a piece of music in a symbolic representation, our goal is to generate a new accompaniment for it in a different arrangement style while preserving the original harmonic structure. Even though our approach is generic, to narrow down our scope, we focus on generating bass and piano tracks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">A major difficulty of the music style translation task is that there are no publicly available ‘aligned’ or ‘parallel’ datasets (containing examples of the same music played in different styles). As a result, recent works closely related to ours
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
have adopted unsupervised learning frameworks – variational autoencoders (VAE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and CycleGANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> –
and applied them to genre-labeled datasets.
However, these extensions to symbolic music have not yet permitted to obtain results as compelling as those on images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and music audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this study, we adopt a different strategy to overcome the lack of aligned data, which is to <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">synthesize</em> it.
Synthetic training data has proven useful for music information retrieval tasks such as chord recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and fundamental frequency estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and is also popular for tasks like semantic segmentation
in computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
In our case, synthetic data opens up the possibility for supervised learning techniques known from the machine translation field.
Moreover, it allows us to work with fine-grained style labels, as opposed to genre labels, which may be too vague or ambiguous
for such purposes.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our main contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a supervised, end-to-end neural model for symbolic music style translation, along with a training data generation scheme.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our model is able to translate into a large number of different styles by conditioning a single decoder on the target style. To our knowledge, this is the first time this technique has been applied to music translation with some success.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">To evaluate the performance of our model, we propose an objective metric of music style similarity.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We show that an approach to music style translation based entirely on synthetic data is viable and generalizes well to more ‘natural’ inputs, even in unrelated styles.</p>
</div>
</li>
</ul>
<p id="S1.p6.2" class="ltx_p">We believe that our approach will foster new directions in this line of research; some of these will be briefly discussed in <a href="#S7" title="7 Conclusion ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref">the conclusion</a>. The source code of our system, built using TensorFlow, is available online.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://git.io/musicstyle" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://git.io/musicstyle</a></span></span></span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The work performed so far in the area of music style transformation is relatively small in volume but fairly diverse, since, as noted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the transformations can work with different music representations as well as on different conceptual levels.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">To our knowledge, the only work on music style transfer – in the original sense, as discussed in the <a href="#S1" title="1 Introduction ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref">introduction</a> – has been done on audio.
Some approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> combine signal decomposition techniques with <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">musaicing</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (a form of concatenative synthesis).
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, the authors attempt to transfer ‘sound textures’ from a recording by means of techniques adapted from image style transfer, but without specific focus on the musical aspects.
In both cases, the transformation is largely limited to timbre.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The problem of unsupervised music audio translation is tackled in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, where
the authors train a neural network to translate between a number of domains.
For symbolic music, style translation is studied in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, adapting unsupervised learning techniques from computer vision.
A different approach is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, consisting in training a model on the target style only and then using pseudo-Gibbs sampling to transform a given piece of music.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Finally, we should mention more ‘constrained’ problems from the symbolic music domain which can also be framed as style translation tasks, e.g. (re-)harmonization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and expressive performance generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic data generation</h2>

<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1907.02265/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="457" height="69" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1907.02265/assets/x2.png" id="S3.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="457" height="70" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text ltx_font_bold">Figure 1</span>: </span>Six bars of an accompaniment (piano and bass) for a 12-bar blues,
generated using BIAB in a ‘jazz swing’ style (top) and a ‘samba’ style (bottom).
The timing is only approximate.
The input chord sequence is displayed at the top.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Since we are in a supervised setting, our approach requires a large amount of <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">paired</em> examples where each pair consists of one musical fragment arranged in two different styles.
Given that no such dataset is currently available, we created a synthetic one, generated using RealBand from the Band-in-a-Box (BIAB) software package <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.2" class="ltx_p">First, we downloaded chord charts of around 3.5K songs in the BIAB format from a popular online archive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
We used BIAB to generate arrangements of these songs in different styles and filtered the resulting MIDI files to keep only those in  
<span id="S3.p2.1.m1.2.2.2.2" class="ltx_tabular ltx_markedasmath ltx_align_bottom">
<span id="S3.p2.1.m1.1.1.1.1.1" class="ltx_tr">
<span id="S3.p2.1.m1.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="S3.p2.1.m1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{4}" display="inline"><semantics id="S3.p2.1.m1.1.1.1.1.1.1.m1.1a"><mn mathsize="70%" id="S3.p2.1.m1.1.1.1.1.1.1.m1.1.1" xref="S3.p2.1.m1.1.1.1.1.1.1.m1.1.1.cmml">𝟒</mn><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.p2.1.m1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1.1.1.1.1.1.m1.1c">\scriptstyle\mathbf{4}</annotation></semantics></math></span></span>
<span id="S3.p2.1.m1.2.2.2.2.2" class="ltx_tr">
<span id="S3.p2.1.m1.2.2.2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="S3.p2.1.m1.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{4}" display="inline"><semantics id="S3.p2.1.m1.2.2.2.2.2.1.m1.1a"><mn mathsize="70%" id="S3.p2.1.m1.2.2.2.2.2.1.m1.1.1" xref="S3.p2.1.m1.2.2.2.2.2.1.m1.1.1.cmml">𝟒</mn><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2.2.2.2.2.1.m1.1b"><cn type="integer" id="S3.p2.1.m1.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.p2.1.m1.2.2.2.2.2.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2.2.2.2.2.1.m1.1c">\scriptstyle\mathbf{4}</annotation></semantics></math></span></span>
</span>  or  
<span id="S3.p2.2.m2.2.2.2.2" class="ltx_tabular ltx_markedasmath ltx_align_bottom">
<span id="S3.p2.2.m2.1.1.1.1.1" class="ltx_tr">
<span id="S3.p2.2.m2.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="S3.p2.2.m2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{12}" display="inline"><semantics id="S3.p2.2.m2.1.1.1.1.1.1.m1.1a"><mn mathsize="70%" id="S3.p2.2.m2.1.1.1.1.1.1.m1.1.1" xref="S3.p2.2.m2.1.1.1.1.1.1.m1.1.1.cmml">𝟏𝟐</mn><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.p2.2.m2.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1.1.1.1.1.1.m1.1c">\scriptstyle\mathbf{12}</annotation></semantics></math></span></span>
<span id="S3.p2.2.m2.2.2.2.2.2" class="ltx_tr">
<span id="S3.p2.2.m2.2.2.2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="S3.p2.2.m2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{8}" display="inline"><semantics id="S3.p2.2.m2.2.2.2.2.2.1.m1.1a"><mn mathsize="70%" id="S3.p2.2.m2.2.2.2.2.2.1.m1.1.1" xref="S3.p2.2.m2.2.2.2.2.2.1.m1.1.1.cmml">𝟖</mn><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.2.2.2.2.2.1.m1.1b"><cn type="integer" id="S3.p2.2.m2.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.p2.2.m2.2.2.2.2.2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.2.2.2.2.2.1.m1.1c">\scriptstyle\mathbf{8}</annotation></semantics></math></span></span>
</span>  time.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The time signature depends on the style as well as on the song itself. A song originally in  
<span id="footnote2.m1.2.2.2.2" class="ltx_tabular ltx_markedasmath ltx_align_bottom">
<span id="footnote2.m1.1.1.1.1.1" class="ltx_tr">
<span id="footnote2.m1.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="footnote2.m1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{4}" display="inline"><semantics id="footnote2.m1.1.1.1.1.1.1.m1.1b"><mn mathsize="70%" id="footnote2.m1.1.1.1.1.1.1.m1.1.1" xref="footnote2.m1.1.1.1.1.1.1.m1.1.1.cmml">𝟒</mn><annotation-xml encoding="MathML-Content" id="footnote2.m1.1.1.1.1.1.1.m1.1c"><cn type="integer" id="footnote2.m1.1.1.1.1.1.1.m1.1.1.cmml" xref="footnote2.m1.1.1.1.1.1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1.1.1.1.1.1.m1.1d">\scriptstyle\mathbf{4}</annotation></semantics></math></span></span>
<span id="footnote2.m1.2.2.2.2.2" class="ltx_tr">
<span id="footnote2.m1.2.2.2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="footnote2.m1.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{4}" display="inline"><semantics id="footnote2.m1.2.2.2.2.2.1.m1.1b"><mn mathsize="70%" id="footnote2.m1.2.2.2.2.2.1.m1.1.1" xref="footnote2.m1.2.2.2.2.2.1.m1.1.1.cmml">𝟒</mn><annotation-xml encoding="MathML-Content" id="footnote2.m1.2.2.2.2.2.1.m1.1c"><cn type="integer" id="footnote2.m1.2.2.2.2.2.1.m1.1.1.cmml" xref="footnote2.m1.2.2.2.2.2.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.2.2.2.2.2.1.m1.1d">\scriptstyle\mathbf{4}</annotation></semantics></math></span></span>
</span>  may have a  
<span id="footnote2.m2.2.2.2.2" class="ltx_tabular ltx_markedasmath ltx_align_bottom">
<span id="footnote2.m2.1.1.1.1.1" class="ltx_tr">
<span id="footnote2.m2.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="footnote2.m2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{12}" display="inline"><semantics id="footnote2.m2.1.1.1.1.1.1.m1.1b"><mn mathsize="70%" id="footnote2.m2.1.1.1.1.1.1.m1.1.1" xref="footnote2.m2.1.1.1.1.1.1.m1.1.1.cmml">𝟏𝟐</mn><annotation-xml encoding="MathML-Content" id="footnote2.m2.1.1.1.1.1.1.m1.1c"><cn type="integer" id="footnote2.m2.1.1.1.1.1.1.m1.1.1.cmml" xref="footnote2.m2.1.1.1.1.1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m2.1.1.1.1.1.1.m1.1d">\scriptstyle\mathbf{12}</annotation></semantics></math></span></span>
<span id="footnote2.m2.2.2.2.2.2" class="ltx_tr">
<span id="footnote2.m2.2.2.2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><math id="footnote2.m2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\scriptstyle\mathbf{8}" display="inline"><semantics id="footnote2.m2.2.2.2.2.2.1.m1.1b"><mn mathsize="70%" id="footnote2.m2.2.2.2.2.2.1.m1.1.1" xref="footnote2.m2.2.2.2.2.2.1.m1.1.1.cmml">𝟖</mn><annotation-xml encoding="MathML-Content" id="footnote2.m2.2.2.2.2.2.1.m1.1c"><cn type="integer" id="footnote2.m2.2.2.2.2.2.1.m1.1.1.cmml" xref="footnote2.m2.2.2.2.2.2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m2.2.2.2.2.2.1.m1.1d">\scriptstyle\mathbf{8}</annotation></semantics></math></span></span>
</span>  arrangement and vice versa.</span></span></span>
We then chopped those files into segments of 8 bars, splitting notes that overlap segment boundaries.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We selected a total of 70 styles from the ‘0 MIDI’ and ‘1 MIDI’ style packs included in Band-in-a-Box 2018, representing a wide variety of popular music genres. Each style contains up to 5 accompaniment tracks (drums, bass, piano, guitar, strings).<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>These 5 labels are not always accurate; for example, some styles have two guitar tracks, one of which is labeled as piano.</span></span></span>
We generated each song in 3 randomly picked styles, providing <math id="S3.p3.1.m1.2" class="ltx_Math" alttext="2\times\binom{3}{2}=6" display="inline"><semantics id="S3.p3.1.m1.2a"><mrow id="S3.p3.1.m1.2.3" xref="S3.p3.1.m1.2.3.cmml"><mrow id="S3.p3.1.m1.2.3.2" xref="S3.p3.1.m1.2.3.2.cmml"><mn id="S3.p3.1.m1.2.3.2.2" xref="S3.p3.1.m1.2.3.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p3.1.m1.2.3.2.1" xref="S3.p3.1.m1.2.3.2.1.cmml">×</mo><mrow id="S3.p3.1.m1.2.2.4" xref="S3.p3.1.m1.2.2.3.cmml"><mo id="S3.p3.1.m1.2.2.4.1" xref="S3.p3.1.m1.2.2.3.1.cmml">(</mo><mfrac linethickness="0pt" id="S3.p3.1.m1.2.2.2.2" xref="S3.p3.1.m1.2.2.3.cmml"><mn id="S3.p3.1.m1.1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.1.cmml">3</mn><mn id="S3.p3.1.m1.2.2.2.2.2.1" xref="S3.p3.1.m1.2.2.2.2.2.1.cmml">2</mn></mfrac><mo id="S3.p3.1.m1.2.2.4.2" xref="S3.p3.1.m1.2.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.p3.1.m1.2.3.1" xref="S3.p3.1.m1.2.3.1.cmml">=</mo><mn id="S3.p3.1.m1.2.3.3" xref="S3.p3.1.m1.2.3.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.2b"><apply id="S3.p3.1.m1.2.3.cmml" xref="S3.p3.1.m1.2.3"><eq id="S3.p3.1.m1.2.3.1.cmml" xref="S3.p3.1.m1.2.3.1"></eq><apply id="S3.p3.1.m1.2.3.2.cmml" xref="S3.p3.1.m1.2.3.2"><times id="S3.p3.1.m1.2.3.2.1.cmml" xref="S3.p3.1.m1.2.3.2.1"></times><cn type="integer" id="S3.p3.1.m1.2.3.2.2.cmml" xref="S3.p3.1.m1.2.3.2.2">2</cn><apply id="S3.p3.1.m1.2.2.3.cmml" xref="S3.p3.1.m1.2.2.4"><csymbol cd="latexml" id="S3.p3.1.m1.2.2.3.1.cmml" xref="S3.p3.1.m1.2.2.4.1">binomial</csymbol><cn type="integer" id="S3.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1">3</cn><cn type="integer" id="S3.p3.1.m1.2.2.2.2.2.1.cmml" xref="S3.p3.1.m1.2.2.2.2.2.1">2</cn></apply></apply><cn type="integer" id="S3.p3.1.m1.2.3.3.cmml" xref="S3.p3.1.m1.2.3.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.2c">2\times\binom{3}{2}=6</annotation></semantics></math> training pairs per segment, or around 658K training examples in total.
An example of a possible training pair is shown in <a href="#S3.F1" title="In 3 Synthetic data generation ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">In all experiments, we used 2,809 songs for training, 46 songs as a validation set and 46 songs for evaluation, each in 3 examples in different styles. The song names, along with the styles used for each song, are included in the supplementary material <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed model</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose an architecture based on RNN encoder-decoder sequence-to-sequence models with attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, commonly employed in machine translation and other areas of natural language processing.
This choice is motivated by the successes of RNNs on symbolic music generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and by the ability of the attention mechanism to condition the generation on arbitrary input data without a prior alignment.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.3" class="ltx_p">Our model is designed so that it is capable of translating music between a potentially large number of different styles.
This is achieved by conditioning the decoder on the target style.
An obvious advantage of this design is efficiency: to translate between <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">n</annotation></semantics></math> styles, we only need to train a single model, compared to <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">n</annotation></semantics></math> models (one for each target style; possibly with a shared encoder as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>) or even <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\Theta(n^{2})" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">Θ</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">​</mo><mrow id="S4.p2.3.m3.1.1.1.1" xref="S4.p2.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p2.3.m3.1.1.1.1.2" xref="S4.p2.3.m3.1.1.1.1.1.cmml">(</mo><msup id="S4.p2.3.m3.1.1.1.1.1" xref="S4.p2.3.m3.1.1.1.1.1.cmml"><mi id="S4.p2.3.m3.1.1.1.1.1.2" xref="S4.p2.3.m3.1.1.1.1.1.2.cmml">n</mi><mn id="S4.p2.3.m3.1.1.1.1.1.3" xref="S4.p2.3.m3.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S4.p2.3.m3.1.1.1.1.3" xref="S4.p2.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><times id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2"></times><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">Θ</ci><apply id="S4.p2.3.m3.1.1.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1.1">superscript</csymbol><ci id="S4.p2.3.m3.1.1.1.1.1.2.cmml" xref="S4.p2.3.m3.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S4.p2.3.m3.1.1.1.1.1.3.cmml" xref="S4.p2.3.m3.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\Theta(n^{2})</annotation></semantics></math> models (one for each pair of styles, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>).
Other implications of this choice are investigated in <a href="#S6.SS2" title="6.2 Comparison with a single-pair model ‣ 6 Experimental results ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">On the other hand, to simplify the task and facilitate evaluation, we train a dedicated model for each target instrument track.
Our output representation and decoder architecture are chosen accordingly and would not necessarily be suitable for generating several independent tracks.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Input and output representation.  </span>
A common choice of representation of symbolic non-monophonic music for neural processing is a piano roll. We use a binary-valued piano roll with 128 pitches and 4 columns per beat (quarter note) to encode our input.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">For representing the output (and also as an alternative input representation), we opted for a MIDI-like encoding, which – unlike a piano roll – is straightforward to model using an RNN decoder.
Specifically, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, we encode the music as a sequence of 3 types of events, each with one integer argument:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">NoteOn(<span id="S4.I1.i1.p1.1.1.1" class="ltx_text ltx_font_italic">pitch</span>)</span>: start a new note at the given pitch;</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">NoteOff(<span id="S4.I1.i2.p1.1.1.1" class="ltx_text ltx_font_italic">pitch</span>)</span>: end the note at the given pitch;</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">TimeShift(<span id="S4.I1.i3.p1.1.1.1" class="ltx_text ltx_font_italic">delta</span>)</span>: move forward in time by the specified amount, measured in 12ths of a beat.</p>
</div>
</li>
</ul>
<p id="S4.p5.2" class="ltx_p"><span id="S4.p5.2.1" class="ltx_text ltx_font_typewriter">NoteOn</span> and <span id="S4.p5.2.2" class="ltx_text ltx_font_typewriter">NoteOff</span> take values in the range 0–127, whereas <span id="S4.p5.2.3" class="ltx_text ltx_font_typewriter">TimeShift</span> is within 1–24.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>When encoding the piano track, we compress the sequences by also including a <span id="footnote4.1" class="ltx_text ltx_font_typewriter">NoteOff(All)</span> event which ends all currently active notes.</span></span></span>
In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, our representation is tempo-invariant and we do not model dynamics.
<a href="#S4.F2" title="In 4 Proposed model ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> illustrates both representations.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div id="S4.F2.2" class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" style="width:368.6pt;">
<div id="S4.F2.1.1" class="ltx_block ltx_minipage ltx_align_middle" style="width:238.5pt;">
<img src="/html/1907.02265/assets/x3.png" id="S4.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="332" height="231" alt="Refer to caption">
</div>
<div id="S4.F2.2.2" class="ltx_block ltx_minipage ltx_align_middle" style="width:182.1pt;">
<img src="/html/1907.02265/assets/x4.png" id="S4.F2.2.2.g1" class="ltx_graphics ltx_img_square" width="461" height="392" alt="Refer to caption">
</div>
<p id="S4.F2.2.3" class="ltx_p ltx_minipage ltx_align_middle" style="width:433.6pt;"><span id="S4.F2.2.3.1" class="ltx_text ltx_font_typewriter">NoteOn(50)
TimeShift(9)
NoteOn(60)
NoteOn(65)
NoteOn(69)
NoteOn(76)
TimeShift(12)
NoteOff(60)
NoteOff(65)
NoteOff(69)
NoteOff(76)
TimeShift(3)
NoteOff(50)
NoteOn(43)
NoteOn(59)
NoteOn(65)
NoteOn(69)
NoteOn(76)
TimeShift(24)
NoteOff(All)
</span></p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.4.1.1" class="ltx_text ltx_font_bold">Figure 2</span>: </span>A bar of music, represented as a piano roll (top right) and as a sequence of 20 event tokens (bottom).</figcaption>
</figure>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Model architecture and training.  </span>

The proposed model consists of an encoder and a decoder; the former serves to compute a dense representation of the input, while the latter generates the output event sequence, conditioned on the encoded input and the target style.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">The architecture of the encoder depends on the type of input representation:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">If the input is a piano roll, we use a two-layer convolutional network (CNN),
followed by a bidirectional RNN with a gated recurrent unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
The CNN serves to compress the input, resulting in a sequence of 1280-dimensional vectors with 2 vectors per bar.
The bidirectional GRU then adds the ability to incorporate information from a wider context.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">If the input is a sequence of tokens, we use an embedding layer, also followed by a bidirectional GRU.</p>
</div>
</li>
</ul>
<p id="S4.p7.2" class="ltx_p">We refer to the two variants of the model as ‘roll2seq’ and ‘seq2seq’, respectively.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.2" class="ltx_p">The decoder is also implemented using a GRU, conditioned on the target style and equipped with a feed-forward attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> acting on the encoder outputs.
More precisely, as illustrated in <a href="#S4.F3" title="In 4 Proposed model ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, the <math id="S4.p8.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p8.1.m1.1a"><mi id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.1b"><ci id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.1c">i</annotation></semantics></math>-th decoder state <math id="S4.p8.2.m2.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S4.p8.2.m2.1a"><msub id="S4.p8.2.m2.1.1" xref="S4.p8.2.m2.1.1.cmml"><mi id="S4.p8.2.m2.1.1.2" xref="S4.p8.2.m2.1.1.2.cmml">s</mi><mi id="S4.p8.2.m2.1.1.3" xref="S4.p8.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p8.2.m2.1b"><apply id="S4.p8.2.m2.1.1.cmml" xref="S4.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p8.2.m2.1.1.1.cmml" xref="S4.p8.2.m2.1.1">subscript</csymbol><ci id="S4.p8.2.m2.1.1.2.cmml" xref="S4.p8.2.m2.1.1.2">𝑠</ci><ci id="S4.p8.2.m2.1.1.3.cmml" xref="S4.p8.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.2.m2.1c">s_{i}</annotation></semantics></math> is computed as</p>
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1.1" class="ltx_Math" alttext="s_{i}=\text{GRU}([c_{i},W^{\mathrm{s}}z,W^{\mathrm{e}}y_{i-1}],s_{i-1})," display="block"><semantics id="S4.Ex1.m1.1a"><mrow id="S4.Ex1.m1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.cmml"><mrow id="S4.Ex1.m1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.cmml"><msub id="S4.Ex1.m1.1.1.1.1.4" xref="S4.Ex1.m1.1.1.1.1.4.cmml"><mi id="S4.Ex1.m1.1.1.1.1.4.2" xref="S4.Ex1.m1.1.1.1.1.4.2.cmml">s</mi><mi id="S4.Ex1.m1.1.1.1.1.4.3" xref="S4.Ex1.m1.1.1.1.1.4.3.cmml">i</mi></msub><mo id="S4.Ex1.m1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.Ex1.m1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.2.cmml"><mtext id="S4.Ex1.m1.1.1.1.1.2.4" xref="S4.Ex1.m1.1.1.1.1.2.4a.cmml">GRU</mtext><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.2.3" xref="S4.Ex1.m1.1.1.1.1.2.3.cmml">​</mo><mrow id="S4.Ex1.m1.1.1.1.1.2.2.2" xref="S4.Ex1.m1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.2.2.2.3" xref="S4.Ex1.m1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml"><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.4" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml">[</mo><msub id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.5" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml">,</mo><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml"><msup id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">W</mi><mi mathvariant="normal" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">s</mi></msup><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">z</mi></mrow><mo id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.6" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml">,</mo><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><msup id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml">W</mi><mi mathvariant="normal" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml">e</mi></msup><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">y</mi><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.7" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml">]</mo></mrow><mo id="S4.Ex1.m1.1.1.1.1.2.2.2.4" xref="S4.Ex1.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S4.Ex1.m1.1.1.1.1.2.2.2.2" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S4.Ex1.m1.1.1.1.1.2.2.2.2.2" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.2.cmml">s</mi><mrow id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.2" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.2.cmml">i</mi><mo id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.1" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.1.cmml">−</mo><mn id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.3" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.2.2.2.5" xref="S4.Ex1.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S4.Ex1.m1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.1b"><apply id="S4.Ex1.m1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1"><eq id="S4.Ex1.m1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.1.3"></eq><apply id="S4.Ex1.m1.1.1.1.1.4.cmml" xref="S4.Ex1.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.4.1.cmml" xref="S4.Ex1.m1.1.1.1.1.4">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.4.2.cmml" xref="S4.Ex1.m1.1.1.1.1.4.2">𝑠</ci><ci id="S4.Ex1.m1.1.1.1.1.4.3.cmml" xref="S4.Ex1.m1.1.1.1.1.4.3">𝑖</ci></apply><apply id="S4.Ex1.m1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.2"><times id="S4.Ex1.m1.1.1.1.1.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.2.3"></times><ci id="S4.Ex1.m1.1.1.1.1.2.4a.cmml" xref="S4.Ex1.m1.1.1.1.1.2.4"><mtext id="S4.Ex1.m1.1.1.1.1.2.4.cmml" xref="S4.Ex1.m1.1.1.1.1.2.4">GRU</mtext></ci><interval closure="open" id="S4.Ex1.m1.1.1.1.1.2.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2"><list id="S4.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3"><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2">𝑐</ci><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2"><times id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.1"></times><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2">𝑊</ci><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3">s</ci></apply><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3">𝑧</ci></apply><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2">superscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.2">𝑊</ci><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.3">e</ci></apply><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2">𝑦</ci><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3"><minus id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn type="integer" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply></apply></list><apply id="S4.Ex1.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.2">𝑠</ci><apply id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3"><minus id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.1"></minus><ci id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.2">𝑖</ci><cn type="integer" id="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.1c">s_{i}=\text{GRU}([c_{i},W^{\mathrm{s}}z,W^{\mathrm{e}}y_{i-1}],s_{i-1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.p8.7" class="ltx_p">where <math id="S4.p8.3.m1.1" class="ltx_Math" alttext="[\cdot]" display="inline"><semantics id="S4.p8.3.m1.1a"><mrow id="S4.p8.3.m1.1.2.2" xref="S4.p8.3.m1.1.2.1.cmml"><mo stretchy="false" id="S4.p8.3.m1.1.2.2.1" xref="S4.p8.3.m1.1.2.1.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S4.p8.3.m1.1.1" xref="S4.p8.3.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.p8.3.m1.1.2.2.2" xref="S4.p8.3.m1.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.3.m1.1b"><apply id="S4.p8.3.m1.1.2.1.cmml" xref="S4.p8.3.m1.1.2.2"><csymbol cd="latexml" id="S4.p8.3.m1.1.2.1.1.cmml" xref="S4.p8.3.m1.1.2.2.1">delimited-[]</csymbol><ci id="S4.p8.3.m1.1.1.cmml" xref="S4.p8.3.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.3.m1.1c">[\cdot]</annotation></semantics></math> denotes concatenation, <math id="S4.p8.4.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.p8.4.m2.1a"><mi id="S4.p8.4.m2.1.1" xref="S4.p8.4.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.p8.4.m2.1b"><ci id="S4.p8.4.m2.1.1.cmml" xref="S4.p8.4.m2.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.4.m2.1c">z</annotation></semantics></math> and <math id="S4.p8.5.m3.1" class="ltx_Math" alttext="y_{i-1}" display="inline"><semantics id="S4.p8.5.m3.1a"><msub id="S4.p8.5.m3.1.1" xref="S4.p8.5.m3.1.1.cmml"><mi id="S4.p8.5.m3.1.1.2" xref="S4.p8.5.m3.1.1.2.cmml">y</mi><mrow id="S4.p8.5.m3.1.1.3" xref="S4.p8.5.m3.1.1.3.cmml"><mi id="S4.p8.5.m3.1.1.3.2" xref="S4.p8.5.m3.1.1.3.2.cmml">i</mi><mo id="S4.p8.5.m3.1.1.3.1" xref="S4.p8.5.m3.1.1.3.1.cmml">−</mo><mn id="S4.p8.5.m3.1.1.3.3" xref="S4.p8.5.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p8.5.m3.1b"><apply id="S4.p8.5.m3.1.1.cmml" xref="S4.p8.5.m3.1.1"><csymbol cd="ambiguous" id="S4.p8.5.m3.1.1.1.cmml" xref="S4.p8.5.m3.1.1">subscript</csymbol><ci id="S4.p8.5.m3.1.1.2.cmml" xref="S4.p8.5.m3.1.1.2">𝑦</ci><apply id="S4.p8.5.m3.1.1.3.cmml" xref="S4.p8.5.m3.1.1.3"><minus id="S4.p8.5.m3.1.1.3.1.cmml" xref="S4.p8.5.m3.1.1.3.1"></minus><ci id="S4.p8.5.m3.1.1.3.2.cmml" xref="S4.p8.5.m3.1.1.3.2">𝑖</ci><cn type="integer" id="S4.p8.5.m3.1.1.3.3.cmml" xref="S4.p8.5.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.5.m3.1c">y_{i-1}</annotation></semantics></math> respectively denote the one-hot encoded representations of the target style and the previous output event,
<math id="S4.p8.6.m4.2" class="ltx_Math" alttext="W^{\mathrm{s}},W^{\mathrm{e}}" display="inline"><semantics id="S4.p8.6.m4.2a"><mrow id="S4.p8.6.m4.2.2.2" xref="S4.p8.6.m4.2.2.3.cmml"><msup id="S4.p8.6.m4.1.1.1.1" xref="S4.p8.6.m4.1.1.1.1.cmml"><mi id="S4.p8.6.m4.1.1.1.1.2" xref="S4.p8.6.m4.1.1.1.1.2.cmml">W</mi><mi mathvariant="normal" id="S4.p8.6.m4.1.1.1.1.3" xref="S4.p8.6.m4.1.1.1.1.3.cmml">s</mi></msup><mo id="S4.p8.6.m4.2.2.2.3" xref="S4.p8.6.m4.2.2.3.cmml">,</mo><msup id="S4.p8.6.m4.2.2.2.2" xref="S4.p8.6.m4.2.2.2.2.cmml"><mi id="S4.p8.6.m4.2.2.2.2.2" xref="S4.p8.6.m4.2.2.2.2.2.cmml">W</mi><mi mathvariant="normal" id="S4.p8.6.m4.2.2.2.2.3" xref="S4.p8.6.m4.2.2.2.2.3.cmml">e</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.6.m4.2b"><list id="S4.p8.6.m4.2.2.3.cmml" xref="S4.p8.6.m4.2.2.2"><apply id="S4.p8.6.m4.1.1.1.1.cmml" xref="S4.p8.6.m4.1.1.1.1"><csymbol cd="ambiguous" id="S4.p8.6.m4.1.1.1.1.1.cmml" xref="S4.p8.6.m4.1.1.1.1">superscript</csymbol><ci id="S4.p8.6.m4.1.1.1.1.2.cmml" xref="S4.p8.6.m4.1.1.1.1.2">𝑊</ci><ci id="S4.p8.6.m4.1.1.1.1.3.cmml" xref="S4.p8.6.m4.1.1.1.1.3">s</ci></apply><apply id="S4.p8.6.m4.2.2.2.2.cmml" xref="S4.p8.6.m4.2.2.2.2"><csymbol cd="ambiguous" id="S4.p8.6.m4.2.2.2.2.1.cmml" xref="S4.p8.6.m4.2.2.2.2">superscript</csymbol><ci id="S4.p8.6.m4.2.2.2.2.2.cmml" xref="S4.p8.6.m4.2.2.2.2.2">𝑊</ci><ci id="S4.p8.6.m4.2.2.2.2.3.cmml" xref="S4.p8.6.m4.2.2.2.2.3">e</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.6.m4.2c">W^{\mathrm{s}},W^{\mathrm{e}}</annotation></semantics></math> are the corresponding embedding matrices,
and <math id="S4.p8.7.m5.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.p8.7.m5.1a"><msub id="S4.p8.7.m5.1.1" xref="S4.p8.7.m5.1.1.cmml"><mi id="S4.p8.7.m5.1.1.2" xref="S4.p8.7.m5.1.1.2.cmml">c</mi><mi id="S4.p8.7.m5.1.1.3" xref="S4.p8.7.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p8.7.m5.1b"><apply id="S4.p8.7.m5.1.1.cmml" xref="S4.p8.7.m5.1.1"><csymbol cd="ambiguous" id="S4.p8.7.m5.1.1.1.cmml" xref="S4.p8.7.m5.1.1">subscript</csymbol><ci id="S4.p8.7.m5.1.1.2.cmml" xref="S4.p8.7.m5.1.1.2">𝑐</ci><ci id="S4.p8.7.m5.1.1.3.cmml" xref="S4.p8.7.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.7.m5.1c">c_{i}</annotation></semantics></math> is the context vector.
The latter is a weighted average of the encoder outputs, computed by the attention mechanism.
The purpose of attention is to provide an <em id="S4.p8.7.1" class="ltx_emph ltx_font_italic">alignment</em> between the encoder and decoder states.
The need for this alignment arises from the fact that the positions in the output sequence are not linear in time (due to the chosen encoding), and the decoder therefore needs to be able to move its focus flexibly over the input.
For a complete description of attention, see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1907.02265/assets/x5.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="132" height="89" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.14.1.1" class="ltx_text ltx_font_bold">Figure 3</span>: </span>The attention-based decoder.
During the <math id="S4.F3.7.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.F3.7.m1.1b"><mi id="S4.F3.7.m1.1.1" xref="S4.F3.7.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.F3.7.m1.1c"><ci id="S4.F3.7.m1.1.1.cmml" xref="S4.F3.7.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.7.m1.1d">i</annotation></semantics></math>-th decoding step (here <math id="S4.F3.8.m2.1" class="ltx_Math" alttext="i=3" display="inline"><semantics id="S4.F3.8.m2.1b"><mrow id="S4.F3.8.m2.1.1" xref="S4.F3.8.m2.1.1.cmml"><mi id="S4.F3.8.m2.1.1.2" xref="S4.F3.8.m2.1.1.2.cmml">i</mi><mo id="S4.F3.8.m2.1.1.1" xref="S4.F3.8.m2.1.1.1.cmml">=</mo><mn id="S4.F3.8.m2.1.1.3" xref="S4.F3.8.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.8.m2.1c"><apply id="S4.F3.8.m2.1.1.cmml" xref="S4.F3.8.m2.1.1"><eq id="S4.F3.8.m2.1.1.1.cmml" xref="S4.F3.8.m2.1.1.1"></eq><ci id="S4.F3.8.m2.1.1.2.cmml" xref="S4.F3.8.m2.1.1.2">𝑖</ci><cn type="integer" id="S4.F3.8.m2.1.1.3.cmml" xref="S4.F3.8.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.8.m2.1d">i=3</annotation></semantics></math>), a set of coefficients <math id="S4.F3.9.m3.1" class="ltx_Math" alttext="\alpha_{ij}" display="inline"><semantics id="S4.F3.9.m3.1b"><msub id="S4.F3.9.m3.1.1" xref="S4.F3.9.m3.1.1.cmml"><mi id="S4.F3.9.m3.1.1.2" xref="S4.F3.9.m3.1.1.2.cmml">α</mi><mrow id="S4.F3.9.m3.1.1.3" xref="S4.F3.9.m3.1.1.3.cmml"><mi id="S4.F3.9.m3.1.1.3.2" xref="S4.F3.9.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.F3.9.m3.1.1.3.1" xref="S4.F3.9.m3.1.1.3.1.cmml">​</mo><mi id="S4.F3.9.m3.1.1.3.3" xref="S4.F3.9.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F3.9.m3.1c"><apply id="S4.F3.9.m3.1.1.cmml" xref="S4.F3.9.m3.1.1"><csymbol cd="ambiguous" id="S4.F3.9.m3.1.1.1.cmml" xref="S4.F3.9.m3.1.1">subscript</csymbol><ci id="S4.F3.9.m3.1.1.2.cmml" xref="S4.F3.9.m3.1.1.2">𝛼</ci><apply id="S4.F3.9.m3.1.1.3.cmml" xref="S4.F3.9.m3.1.1.3"><times id="S4.F3.9.m3.1.1.3.1.cmml" xref="S4.F3.9.m3.1.1.3.1"></times><ci id="S4.F3.9.m3.1.1.3.2.cmml" xref="S4.F3.9.m3.1.1.3.2">𝑖</ci><ci id="S4.F3.9.m3.1.1.3.3.cmml" xref="S4.F3.9.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.9.m3.1d">\alpha_{ij}</annotation></semantics></math> is computed and used to weight the encoder states <math id="S4.F3.10.m4.2" class="ltx_Math" alttext="h_{j}=[h^{\mbox{fw}}_{j},h^{\mbox{bw}}_{j}]" display="inline"><semantics id="S4.F3.10.m4.2b"><mrow id="S4.F3.10.m4.2.2" xref="S4.F3.10.m4.2.2.cmml"><msub id="S4.F3.10.m4.2.2.4" xref="S4.F3.10.m4.2.2.4.cmml"><mi id="S4.F3.10.m4.2.2.4.2" xref="S4.F3.10.m4.2.2.4.2.cmml">h</mi><mi id="S4.F3.10.m4.2.2.4.3" xref="S4.F3.10.m4.2.2.4.3.cmml">j</mi></msub><mo id="S4.F3.10.m4.2.2.3" xref="S4.F3.10.m4.2.2.3.cmml">=</mo><mrow id="S4.F3.10.m4.2.2.2.2" xref="S4.F3.10.m4.2.2.2.3.cmml"><mo stretchy="false" id="S4.F3.10.m4.2.2.2.2.3" xref="S4.F3.10.m4.2.2.2.3.cmml">[</mo><msubsup id="S4.F3.10.m4.1.1.1.1.1" xref="S4.F3.10.m4.1.1.1.1.1.cmml"><mi id="S4.F3.10.m4.1.1.1.1.1.2.2" xref="S4.F3.10.m4.1.1.1.1.1.2.2.cmml">h</mi><mi id="S4.F3.10.m4.1.1.1.1.1.3" xref="S4.F3.10.m4.1.1.1.1.1.3.cmml">j</mi><mtext id="S4.F3.10.m4.1.1.1.1.1.2.3" xref="S4.F3.10.m4.1.1.1.1.1.2.3a.cmml">fw</mtext></msubsup><mo id="S4.F3.10.m4.2.2.2.2.4" xref="S4.F3.10.m4.2.2.2.3.cmml">,</mo><msubsup id="S4.F3.10.m4.2.2.2.2.2" xref="S4.F3.10.m4.2.2.2.2.2.cmml"><mi id="S4.F3.10.m4.2.2.2.2.2.2.2" xref="S4.F3.10.m4.2.2.2.2.2.2.2.cmml">h</mi><mi id="S4.F3.10.m4.2.2.2.2.2.3" xref="S4.F3.10.m4.2.2.2.2.2.3.cmml">j</mi><mtext id="S4.F3.10.m4.2.2.2.2.2.2.3" xref="S4.F3.10.m4.2.2.2.2.2.2.3a.cmml">bw</mtext></msubsup><mo stretchy="false" id="S4.F3.10.m4.2.2.2.2.5" xref="S4.F3.10.m4.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.10.m4.2c"><apply id="S4.F3.10.m4.2.2.cmml" xref="S4.F3.10.m4.2.2"><eq id="S4.F3.10.m4.2.2.3.cmml" xref="S4.F3.10.m4.2.2.3"></eq><apply id="S4.F3.10.m4.2.2.4.cmml" xref="S4.F3.10.m4.2.2.4"><csymbol cd="ambiguous" id="S4.F3.10.m4.2.2.4.1.cmml" xref="S4.F3.10.m4.2.2.4">subscript</csymbol><ci id="S4.F3.10.m4.2.2.4.2.cmml" xref="S4.F3.10.m4.2.2.4.2">ℎ</ci><ci id="S4.F3.10.m4.2.2.4.3.cmml" xref="S4.F3.10.m4.2.2.4.3">𝑗</ci></apply><interval closure="closed" id="S4.F3.10.m4.2.2.2.3.cmml" xref="S4.F3.10.m4.2.2.2.2"><apply id="S4.F3.10.m4.1.1.1.1.1.cmml" xref="S4.F3.10.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.F3.10.m4.1.1.1.1.1.1.cmml" xref="S4.F3.10.m4.1.1.1.1.1">subscript</csymbol><apply id="S4.F3.10.m4.1.1.1.1.1.2.cmml" xref="S4.F3.10.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.F3.10.m4.1.1.1.1.1.2.1.cmml" xref="S4.F3.10.m4.1.1.1.1.1">superscript</csymbol><ci id="S4.F3.10.m4.1.1.1.1.1.2.2.cmml" xref="S4.F3.10.m4.1.1.1.1.1.2.2">ℎ</ci><ci id="S4.F3.10.m4.1.1.1.1.1.2.3a.cmml" xref="S4.F3.10.m4.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.F3.10.m4.1.1.1.1.1.2.3.cmml" xref="S4.F3.10.m4.1.1.1.1.1.2.3">fw</mtext></ci></apply><ci id="S4.F3.10.m4.1.1.1.1.1.3.cmml" xref="S4.F3.10.m4.1.1.1.1.1.3">𝑗</ci></apply><apply id="S4.F3.10.m4.2.2.2.2.2.cmml" xref="S4.F3.10.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.F3.10.m4.2.2.2.2.2.1.cmml" xref="S4.F3.10.m4.2.2.2.2.2">subscript</csymbol><apply id="S4.F3.10.m4.2.2.2.2.2.2.cmml" xref="S4.F3.10.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.F3.10.m4.2.2.2.2.2.2.1.cmml" xref="S4.F3.10.m4.2.2.2.2.2">superscript</csymbol><ci id="S4.F3.10.m4.2.2.2.2.2.2.2.cmml" xref="S4.F3.10.m4.2.2.2.2.2.2.2">ℎ</ci><ci id="S4.F3.10.m4.2.2.2.2.2.2.3a.cmml" xref="S4.F3.10.m4.2.2.2.2.2.2.3"><mtext mathsize="70%" id="S4.F3.10.m4.2.2.2.2.2.2.3.cmml" xref="S4.F3.10.m4.2.2.2.2.2.2.3">bw</mtext></ci></apply><ci id="S4.F3.10.m4.2.2.2.2.2.3.cmml" xref="S4.F3.10.m4.2.2.2.2.2.3">𝑗</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.10.m4.2d">h_{j}=[h^{\mbox{fw}}_{j},h^{\mbox{bw}}_{j}]</annotation></semantics></math> to obtain the context vector <math id="S4.F3.11.m5.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.F3.11.m5.1b"><msub id="S4.F3.11.m5.1.1" xref="S4.F3.11.m5.1.1.cmml"><mi id="S4.F3.11.m5.1.1.2" xref="S4.F3.11.m5.1.1.2.cmml">c</mi><mi id="S4.F3.11.m5.1.1.3" xref="S4.F3.11.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.11.m5.1c"><apply id="S4.F3.11.m5.1.1.cmml" xref="S4.F3.11.m5.1.1"><csymbol cd="ambiguous" id="S4.F3.11.m5.1.1.1.cmml" xref="S4.F3.11.m5.1.1">subscript</csymbol><ci id="S4.F3.11.m5.1.1.2.cmml" xref="S4.F3.11.m5.1.1.2">𝑐</ci><ci id="S4.F3.11.m5.1.1.3.cmml" xref="S4.F3.11.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.11.m5.1d">c_{i}</annotation></semantics></math>, which in turn is used as input for the decoder cell to compute the next state, <math id="S4.F3.12.m6.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S4.F3.12.m6.1b"><msub id="S4.F3.12.m6.1.1" xref="S4.F3.12.m6.1.1.cmml"><mi id="S4.F3.12.m6.1.1.2" xref="S4.F3.12.m6.1.1.2.cmml">s</mi><mi id="S4.F3.12.m6.1.1.3" xref="S4.F3.12.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.12.m6.1c"><apply id="S4.F3.12.m6.1.1.cmml" xref="S4.F3.12.m6.1.1"><csymbol cd="ambiguous" id="S4.F3.12.m6.1.1.1.cmml" xref="S4.F3.12.m6.1.1">subscript</csymbol><ci id="S4.F3.12.m6.1.1.2.cmml" xref="S4.F3.12.m6.1.1.2">𝑠</ci><ci id="S4.F3.12.m6.1.1.3.cmml" xref="S4.F3.12.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.12.m6.1d">s_{i}</annotation></semantics></math>.</figcaption>
</figure>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.6" class="ltx_p">The training pipeline is portrayed in <a href="#S5.F4" title="In 5 Evaluation metrics ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Each training example consists of a song segment <math id="S4.p9.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.p9.1.m1.1a"><mi id="S4.p9.1.m1.1.1" xref="S4.p9.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.p9.1.m1.1b"><ci id="S4.p9.1.m1.1.1.cmml" xref="S4.p9.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.1.m1.1c">x</annotation></semantics></math> in one style (the source style) along with the corresponding segment <math id="S4.p9.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.p9.2.m2.1a"><mi id="S4.p9.2.m2.1.1" xref="S4.p9.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.p9.2.m2.1b"><ci id="S4.p9.2.m2.1.1.cmml" xref="S4.p9.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.2.m2.1c">y</annotation></semantics></math> in a different style (<math id="S4.p9.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.p9.3.m3.1a"><mi id="S4.p9.3.m3.1.1" xref="S4.p9.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.p9.3.m3.1b"><ci id="S4.p9.3.m3.1.1.cmml" xref="S4.p9.3.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.3.m3.1c">z</annotation></semantics></math>, the target style). We train the model by minimizing the loss on <math id="S4.p9.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.p9.4.m4.1a"><mi id="S4.p9.4.m4.1.1" xref="S4.p9.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.p9.4.m4.1b"><ci id="S4.p9.4.m4.1.1.cmml" xref="S4.p9.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.4.m4.1c">y</annotation></semantics></math> while passing <math id="S4.p9.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.p9.5.m5.1a"><mi id="S4.p9.5.m5.1.1" xref="S4.p9.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.p9.5.m5.1b"><ci id="S4.p9.5.m5.1.1.cmml" xref="S4.p9.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.5.m5.1c">x</annotation></semantics></math> to the encoder and conditioning the decoder on <math id="S4.p9.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.p9.6.m6.1a"><mi id="S4.p9.6.m6.1.1" xref="S4.p9.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.p9.6.m6.1b"><ci id="S4.p9.6.m6.1.1.cmml" xref="S4.p9.6.m6.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.6.m6.1c">z</annotation></semantics></math>.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">The models are trained using Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> with learning rate decay and with early stopping on the development set.
Our configuration files with complete hyperparameter settings are included with the source code.</p>
</div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p">Once the model is trained, we perform style translation using greedy decoding, i.e. by taking the most likely output token at every step (and using that as input in the next step). We also explored random sampling with different softmax temperatures, but found that this leads to a higher number of errors (i.e. invalid sequences or incorrect timing) and does not significantly improve the quality of the outputs.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation metrics</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">When evaluating a style transformation, we need to consider two complementary criteria: how well the transformed music fits the desired style (<em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">style fit</em>) and how much content it retains from the original (<em id="S5.p1.1.2" class="ltx_emph ltx_font_italic">content preservation</em>).
Note that it is trivial (but useless) to achieve perfect results on either of these two criteria <em id="S5.p1.1.3" class="ltx_emph ltx_font_italic">alone</em>, so it is essential to evaluate both of them.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In this section, we describe ‘objective’, automatically computed metrics for both criteria.
Even though we believe these metrics are sound and well-motivated, we acknowledge the limitations of automatic metrics in general and encourage the reader to listen to the provided example outputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to get a real sense of their quality.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div id="S5.F4.2" class="ltx_block">
<svg id="S5.F4.pic1" class="ltx_picture ltx_centering" height="194.72" overflow="visible" version="1.1" width="271.6"><g transform="translate(0,194.72) matrix(1 0 0 -1 0 0) translate(65.65,0) translate(0,86.74)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g stroke-dasharray="0.4pt,2.0pt" stroke-dashoffset="0.0pt" fill="#404040" stroke="#404040" color="#404040"><path d="M 77.91 -79.92 L 77.91 101.97" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.04 93)" fill="#000000" stroke="#000000"><foreignObject width="35.17" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.5.5.5.1.1" class="ltx_text">(a) <span id="S5.F4.pic1.5.5.5.1.1.1" class="ltx_text ltx_font_bold">Data generation</span></span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 117.55 93)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.6.6.6.1.1" class="ltx_text">(b) <span id="S5.F4.pic1.6.6.6.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></foreignObject></g><g stroke="#000000" stroke-width="0.8pt"><path d="M 44.05 13.84 L -44.05 13.84 C -47.1 13.84 -49.58 11.36 -49.58 8.3 L -49.58 -8.3 C -49.58 -11.36 -47.1 -13.84 -44.05 -13.84 L 44.05 -13.84 C 47.1 -13.84 49.58 -11.36 49.58 -8.3 L 49.58 8.3 C 49.58 11.36 47.1 13.84 44.05 13.84 Z M -49.58 -13.84" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -44.97 -4.84)"><foreignObject width="89.94" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S5.F4.pic1.7.7.7.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S5.F4.pic1.7.7.7.1.1.1" class="ltx_p"></span>
<span id="S5.F4.pic1.7.7.7.1.1.2" class="ltx_p">BIAB</span>
</span></foreignObject></g><g stroke="#000000"><path d="M -41.55 39.47 h 83.1 v 20.76 h -83.1 Z" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -34.63 45.04)" fill="#000000" stroke="#000000"><foreignObject width="69.65" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.8.8.8.1.1" class="ltx_text">chord chart</span></foreignObject></g><path d="M 0 39.19 L 0 15.77" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 15.77)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g stroke="#000000" stroke-width="0.8pt"><path d="M 199.86 44.14 L 111.76 44.14 C 108.71 44.14 106.23 41.66 106.23 38.6 L 106.23 22 C 106.23 18.94 108.71 16.47 111.76 16.47 L 199.86 16.47 C 202.92 16.47 205.39 18.94 205.39 22 L 205.39 38.6 C 205.39 41.66 202.92 44.14 199.86 44.14 Z M 106.23 16.47" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 110.84 25.46)"><foreignObject width="89.94" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S5.F4.pic1.9.9.9.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S5.F4.pic1.9.9.9.1.1.1" class="ltx_p"></span>
<span id="S5.F4.pic1.9.9.9.1.1.2" class="ltx_p">encoder</span>
</span></foreignObject></g><g stroke="#000000" stroke-width="0.8pt"><path d="M 199.86 -16.47 L 111.76 -16.47 C 108.71 -16.47 106.23 -18.94 106.23 -22 L 106.23 -38.6 C 106.23 -41.66 108.71 -44.14 111.76 -44.14 L 199.86 -44.14 C 202.92 -44.14 205.39 -41.66 205.39 -38.6 L 205.39 -22 C 205.39 -18.94 202.92 -16.47 199.86 -16.47 Z M 106.23 -44.14" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 110.84 -35.15)"><foreignObject width="89.94" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S5.F4.pic1.10.10.10.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S5.F4.pic1.10.10.10.1.1.1" class="ltx_p"></span>
<span id="S5.F4.pic1.10.10.10.1.1.2" class="ltx_p">decoder</span>
</span></foreignObject></g><path d="M 155.81 -14.53 L 155.81 15.91" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 155.81 -14.53)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g stroke="#000000"><path d="M 123.74 65.64 h 64.14 v 20.76 h -64.14 Z" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 130.66 73.04)" fill="#000000" stroke="#000000"><foreignObject width="50.3" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text">source <math id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">x</annotation></semantics></math></span></foreignObject></g><path d="M 155.81 65.36 L 155.81 46.08" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 155.81 46.08)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g stroke="#000000"><path d="M 124.86 -86.39 h 61.9 v 20.76 h -61.9 Z" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 131.78 -78.93)" fill="#000000" stroke="#000000"><foreignObject width="48.06" height="11.2" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text">target <math id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">y</annotation></semantics></math></span></foreignObject></g><g stroke-dasharray="3.0pt,2.0pt" stroke-dashoffset="0.0pt"><path d="M 155.81 -63.98 L 155.81 -44.69" style="fill:none"></path></g><g transform="matrix(0.0 -1.0 1.0 0.0 155.81 -63.98)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 50.14 3.94 L 122.48 75.05" style="fill:none"></path><g transform="matrix(0.71317 0.701 -0.701 0.71317 122.48 75.05)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g transform="matrix(0.71317 0.701 -0.701 0.71317 56.47 20.8)" fill="#000000" stroke="#000000"><foreignObject width="70.15" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.11.11.11.1.1" class="ltx_text">source style</span></foreignObject></g><path d="M 50.14 -3.94 L 123.59 -75.05" style="fill:none"></path><g transform="matrix(0.71844 -0.69559 0.69559 0.71844 123.59 -75.05)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g transform="matrix(0.71844 -0.69559 0.69559 0.71844 48.47 -22.5)" fill="#000000" stroke="#000000"><foreignObject width="80.19" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1" class="ltx_text">target style <math id="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.m1.1c">z</annotation></semantics></math></span></foreignObject></g><g stroke="#000000"><path d="M 176.72 -10.38 h 20.88 v 20.76 h -20.88 Z" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 183.64 -2.98)" fill="#000000" stroke="#000000"><foreignObject width="7.04" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.m1.1c">z</annotation></semantics></math></foreignObject></g><path d="M 176.45 0 L 163.69 0 L 163.69 -14.53" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 163.69 -14.53)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g></g></svg><span id="S5.F4.2.1" class="ltx_ERROR ltx_centering undefined">\phantomsubcaption</span><span id="S5.F4.2.2" class="ltx_ERROR ltx_centering undefined">\phantomsubcaption</span>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.3.1.1" class="ltx_text ltx_font_bold">Figure 4</span>: </span>A scheme of the training pipeline.

<span id="S5.I2" class="ltx_inline-enumerate">
<span id="S5.I2.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(a)</span> <span id="S5.I2.i1.1" class="ltx_text">We use BIAB to generate each song in different arrangement styles (see <a href="#S3" title="3 Synthetic data generation ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>).
</span></span>
<span id="S5.I2.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(b)</span> <span id="S5.I2.i2.3" class="ltx_text">The model is trained to predict the target-style segment <math id="S5.I2.i2.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S5.I2.i2.1.m1.1b"><mi id="S5.I2.i2.1.m1.1.1" xref="S5.I2.i2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i2.1.m1.1c"><ci id="S5.I2.i2.1.m1.1.1.cmml" xref="S5.I2.i2.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.1.m1.1d">y</annotation></semantics></math> given a source segment <math id="S5.I2.i2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.I2.i2.2.m2.1b"><mi id="S5.I2.i2.2.m2.1.1" xref="S5.I2.i2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i2.2.m2.1c"><ci id="S5.I2.i2.2.m2.1.1.cmml" xref="S5.I2.i2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.2.m2.1d">x</annotation></semantics></math> and the target style <math id="S5.I2.i2.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.I2.i2.3.m3.1b"><mi id="S5.I2.i2.3.m3.1.1" xref="S5.I2.i2.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i2.3.m3.1c"><ci id="S5.I2.i2.3.m3.1.1.cmml" xref="S5.I2.i2.3.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.3.m3.1d">z</annotation></semantics></math> (see <a href="#S4.F2" title="In 4 Proposed model ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>).
</span></span>
</span></figcaption>
</figure>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Content preservation.  </span>
We use a content preservation metric similar to the one proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, computed by correlating the chroma representation of the generated segment with that of the corresponding segment in the source style.
This is motivated by the fact that we expect the output to follow the same sequence of chords as the input.
More precisely, we compute chroma features for each segment at a rate of 12 frames per beat and smooth each of them using an averaging filter with a window size of 2 beats (24 frames) and a stride of 1 beat (12 frames).
Finally, we calculate the average frame-wise cosine similarity between the two sets of chroma features.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Style fit.  </span>
In some of the recent music style transformation works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, the quality of a transformation is measured by means of a binary style classifier trained on a pair of styles.
However, the merit of such evaluation is limited, since a high classifier score merely demonstrates that the output has some of the distinguishing features of the target style, and not necessarily that it actually fits the style.
For this reason, we aim for a more interpretable metric of style fit.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">As observed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, musical style is well captured in pairwise statistics between neighboring events. Drawing inspiration from the features proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, we devise
a key- and time-invariant style representation which we call the <em id="S5.p5.1.1" class="ltx_emph ltx_font_italic">style profile</em>.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.5" class="ltx_p">To compute the style profile, we consider all pairs of note onsets less than 4 beats apart and at most 20 semitones apart, and record the time difference and interval for each pair. In other words, we define the following multiset of ordered pairs:</p>
<table id="S5.Ex2" class="ltx_equationgroup ltx_eqn_table">

<tbody id="S5.Ex2X"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.Ex2X.2.1.1.m1.2" class="ltx_math_unparsed" alttext="\displaystyle\mathcal{S}=\{(t_{b}-t_{a},\&gt;p_{b}-p_{a})\,|\,a,b\in\text{notes},\ a\neq b," display="inline"><semantics id="S5.Ex2X.2.1.1.m1.2a"><mrow id="S5.Ex2X.2.1.1.m1.2b"><mi class="ltx_font_mathcaligraphic" id="S5.Ex2X.2.1.1.m1.2.3">𝒮</mi><mo id="S5.Ex2X.2.1.1.m1.2.4">=</mo><mrow id="S5.Ex2X.2.1.1.m1.2.5"><mo stretchy="false" id="S5.Ex2X.2.1.1.m1.2.5.1">{</mo><mrow id="S5.Ex2X.2.1.1.m1.2.5.2"><mo stretchy="false" id="S5.Ex2X.2.1.1.m1.2.5.2.1">(</mo><msub id="S5.Ex2X.2.1.1.m1.2.5.2.2"><mi id="S5.Ex2X.2.1.1.m1.2.5.2.2.2">t</mi><mi id="S5.Ex2X.2.1.1.m1.2.5.2.2.3">b</mi></msub><mo id="S5.Ex2X.2.1.1.m1.2.5.2.3">−</mo><msub id="S5.Ex2X.2.1.1.m1.2.5.2.4"><mi id="S5.Ex2X.2.1.1.m1.2.5.2.4.2">t</mi><mi id="S5.Ex2X.2.1.1.m1.2.5.2.4.3">a</mi></msub><mo rspace="0.387em" id="S5.Ex2X.2.1.1.m1.2.5.2.5">,</mo><msub id="S5.Ex2X.2.1.1.m1.2.5.2.6"><mi id="S5.Ex2X.2.1.1.m1.2.5.2.6.2">p</mi><mi id="S5.Ex2X.2.1.1.m1.2.5.2.6.3">b</mi></msub><mo id="S5.Ex2X.2.1.1.m1.2.5.2.7">−</mo><msub id="S5.Ex2X.2.1.1.m1.2.5.2.8"><mi id="S5.Ex2X.2.1.1.m1.2.5.2.8.2">p</mi><mi id="S5.Ex2X.2.1.1.m1.2.5.2.8.3">a</mi></msub><mo rspace="0.170em" stretchy="false" id="S5.Ex2X.2.1.1.m1.2.5.2.9">)</mo></mrow><mo fence="false" rspace="0.337em" stretchy="false" id="S5.Ex2X.2.1.1.m1.2.5.3">|</mo><mi id="S5.Ex2X.2.1.1.m1.1.1">a</mi><mo id="S5.Ex2X.2.1.1.m1.2.5.4">,</mo><mi id="S5.Ex2X.2.1.1.m1.2.2">b</mi><mo id="S5.Ex2X.2.1.1.m1.2.5.5">∈</mo><mtext id="S5.Ex2X.2.1.1.m1.2.5.6">notes</mtext><mo rspace="0.667em" id="S5.Ex2X.2.1.1.m1.2.5.7">,</mo><mi id="S5.Ex2X.2.1.1.m1.2.5.8">a</mi><mo id="S5.Ex2X.2.1.1.m1.2.5.9">≠</mo><mi id="S5.Ex2X.2.1.1.m1.2.5.10">b</mi><mo id="S5.Ex2X.2.1.1.m1.2.5.11">,</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.Ex2X.2.1.1.m1.2c">\displaystyle\mathcal{S}=\{(t_{b}-t_{a},\&gt;p_{b}-p_{a})\,|\,a,b\in\text{notes},\ a\neq b,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S5.Ex2Xa"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.Ex2Xa.2.1.1.m1.1" class="ltx_math_unparsed" alttext="\displaystyle 0\leq t_{b}-t_{a}&lt;4,\&gt;\lvert p_{b}-p_{a}\rvert\leq 20\}," display="inline"><semantics id="S5.Ex2Xa.2.1.1.m1.1a"><mrow id="S5.Ex2Xa.2.1.1.m1.1b"><mn id="S5.Ex2Xa.2.1.1.m1.1.2">0</mn><mo id="S5.Ex2Xa.2.1.1.m1.1.3">≤</mo><msub id="S5.Ex2Xa.2.1.1.m1.1.4"><mi id="S5.Ex2Xa.2.1.1.m1.1.4.2">t</mi><mi id="S5.Ex2Xa.2.1.1.m1.1.4.3">b</mi></msub><mo id="S5.Ex2Xa.2.1.1.m1.1.5">−</mo><msub id="S5.Ex2Xa.2.1.1.m1.1.6"><mi id="S5.Ex2Xa.2.1.1.m1.1.6.2">t</mi><mi id="S5.Ex2Xa.2.1.1.m1.1.6.3">a</mi></msub><mo id="S5.Ex2Xa.2.1.1.m1.1.7">&lt;</mo><mn id="S5.Ex2Xa.2.1.1.m1.1.1">4</mn><mo rspace="0.387em" id="S5.Ex2Xa.2.1.1.m1.1.8">,</mo><mrow id="S5.Ex2Xa.2.1.1.m1.1.9"><mo stretchy="false" id="S5.Ex2Xa.2.1.1.m1.1.9.1">|</mo><msub id="S5.Ex2Xa.2.1.1.m1.1.9.2"><mi id="S5.Ex2Xa.2.1.1.m1.1.9.2.2">p</mi><mi id="S5.Ex2Xa.2.1.1.m1.1.9.2.3">b</mi></msub><mo id="S5.Ex2Xa.2.1.1.m1.1.9.3">−</mo><msub id="S5.Ex2Xa.2.1.1.m1.1.9.4"><mi id="S5.Ex2Xa.2.1.1.m1.1.9.4.2">p</mi><mi id="S5.Ex2Xa.2.1.1.m1.1.9.4.3">a</mi></msub><mo stretchy="false" id="S5.Ex2Xa.2.1.1.m1.1.9.5">|</mo></mrow><mo id="S5.Ex2Xa.2.1.1.m1.1.10">≤</mo><mn id="S5.Ex2Xa.2.1.1.m1.1.11">20</mn><mo stretchy="false" id="S5.Ex2Xa.2.1.1.m1.1.12">}</mo><mo id="S5.Ex2Xa.2.1.1.m1.1.13">,</mo></mrow><annotation encoding="application/x-tex" id="S5.Ex2Xa.2.1.1.m1.1c">\displaystyle 0\leq t_{b}-t_{a}&lt;4,\&gt;\lvert p_{b}-p_{a}\rvert\leq 20\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.p6.4" class="ltx_p">where <math id="S5.p6.1.m1.1" class="ltx_Math" alttext="t_{x}" display="inline"><semantics id="S5.p6.1.m1.1a"><msub id="S5.p6.1.m1.1.1" xref="S5.p6.1.m1.1.1.cmml"><mi id="S5.p6.1.m1.1.1.2" xref="S5.p6.1.m1.1.1.2.cmml">t</mi><mi id="S5.p6.1.m1.1.1.3" xref="S5.p6.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p6.1.m1.1b"><apply id="S5.p6.1.m1.1.1.cmml" xref="S5.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p6.1.m1.1.1.1.cmml" xref="S5.p6.1.m1.1.1">subscript</csymbol><ci id="S5.p6.1.m1.1.1.2.cmml" xref="S5.p6.1.m1.1.1.2">𝑡</ci><ci id="S5.p6.1.m1.1.1.3.cmml" xref="S5.p6.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.1.m1.1c">t_{x}</annotation></semantics></math> is the onset time of the note <math id="S5.p6.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.p6.2.m2.1a"><mi id="S5.p6.2.m2.1.1" xref="S5.p6.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.p6.2.m2.1b"><ci id="S5.p6.2.m2.1.1.cmml" xref="S5.p6.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.2.m2.1c">x</annotation></semantics></math> (measured in fractional beats) and <math id="S5.p6.3.m3.1" class="ltx_Math" alttext="p_{x}" display="inline"><semantics id="S5.p6.3.m3.1a"><msub id="S5.p6.3.m3.1.1" xref="S5.p6.3.m3.1.1.cmml"><mi id="S5.p6.3.m3.1.1.2" xref="S5.p6.3.m3.1.1.2.cmml">p</mi><mi id="S5.p6.3.m3.1.1.3" xref="S5.p6.3.m3.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p6.3.m3.1b"><apply id="S5.p6.3.m3.1.1.cmml" xref="S5.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p6.3.m3.1.1.1.cmml" xref="S5.p6.3.m3.1.1">subscript</csymbol><ci id="S5.p6.3.m3.1.1.2.cmml" xref="S5.p6.3.m3.1.1.2">𝑝</ci><ci id="S5.p6.3.m3.1.1.3.cmml" xref="S5.p6.3.m3.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.3.m3.1c">p_{x}</annotation></semantics></math> is its MIDI note number.
We then obtain the style profile as a normalized 2D histogram of <math id="S5.p6.4.m4.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S5.p6.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p6.4.m4.1.1" xref="S5.p6.4.m4.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S5.p6.4.m4.1b"><ci id="S5.p6.4.m4.1.1.cmml" xref="S5.p6.4.m4.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.4.m4.1c">\mathcal{S}</annotation></semantics></math> with 6 bins per beat and one bin per semitone, and flatten it to get a 984-dimensional vector.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">Finally, to quantify the style fit of a particular set of outputs, we compute their style profile and measure its cosine similarity to a reference profile.
Note that an 8-bar segment may not be sufficient to obtain a reliable style profile; instead, we always aggregate the statistics over a number of segments.
In particular, we put forward two variants of the style fit metric, obtained as follows:</p>
<ol id="S5.I3" class="ltx_enumerate">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p">Compute a style profile aggregated over all outputs of a model in a given target style and measure its cosine similarity to the reference.</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p">Compute a style profile for each translated song separately and measure its cosine similarity to the reference.
We report the mean and standard deviation over all songs.</p>
</div>
</li>
</ol>
<p id="S5.p7.2" class="ltx_p">We refer to (a) and (b) as ‘macro-style’ and ‘song-style’, respectively.
In both cases, the reference style profile is extracted from the training set, separately for each track.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p">While we do not claim that this metric is able to distinguish between broad style categories (such as genres), yet it can definitely capture the differences and similarities between specific ‘grooves’, which makes it well-suited for our purpose.
This is illustrated in <a href="#S5.F5" title="In 5 Evaluation metrics ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, showing the pairwise similarities between the profiles of the bass tracks of different BIAB styles, with clearly visible clusters of jazz, rock or country styles.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/1907.02265/assets/x6.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="427" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text ltx_font_bold">Figure 5</span>: </span>Pairwise cosine similarities of selected style profiles computed on training bass tracks. The styles are ordered based on a hierarchical clustering of the profiles.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1907.02265/assets/x7.png" id="S5.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="321" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span><span id="S5.F6.sf1.2.1" class="ltx_text ltx_font_smallcaps">bass</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1907.02265/assets/x8.png" id="S5.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="321" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span><span id="S5.F6.sf2.2.1" class="ltx_text ltx_font_smallcaps">piano</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text ltx_font_bold">Figure 6</span>: </span>Evaluation results on content preservation and style fit. ‘Source’ is the original track (bass or piano), ‘reference’ is a track generated by BIAB in the target style and ‘random’ is a random permutation of the references. For ‘song-style’, we plot the mean and the standard deviation over all songs and target styles.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experimental results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.4" class="ltx_p">In our experiments, we focus on generating the bass and piano tracks, and we train a dedicated model for each of them.
For each track, we consider two scenarios: generating the track given only the corresponding source track (<span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">bass<math id="S6.p1.1.1.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.p1.1.1.m1.1a"><mo stretchy="false" id="S6.p1.1.1.m1.1.1" xref="S6.p1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.1.m1.1b"><ci id="S6.p1.1.1.m1.1.1.cmml" xref="S6.p1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.1.m1.1c">\to</annotation></semantics></math>bass</span>, <span id="S6.p1.2.2" class="ltx_text ltx_font_smallcaps">piano<math id="S6.p1.2.2.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.p1.2.2.m1.1a"><mo stretchy="false" id="S6.p1.2.2.m1.1.1" xref="S6.p1.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p1.2.2.m1.1b"><ci id="S6.p1.2.2.m1.1.1.cmml" xref="S6.p1.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.2.m1.1c">\to</annotation></semantics></math>piano</span>), and using all non-drum accompaniment tracks from the input (<span id="S6.p1.3.3" class="ltx_text ltx_font_smallcaps">all<math id="S6.p1.3.3.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.p1.3.3.m1.1a"><mo stretchy="false" id="S6.p1.3.3.m1.1.1" xref="S6.p1.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p1.3.3.m1.1b"><ci id="S6.p1.3.3.m1.1.1.cmml" xref="S6.p1.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.3.m1.1c">\to</annotation></semantics></math>bass</span>, <span id="S6.p1.4.4" class="ltx_text ltx_font_smallcaps">all<math id="S6.p1.4.4.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.p1.4.4.m1.1a"><mo stretchy="false" id="S6.p1.4.4.m1.1.1" xref="S6.p1.4.4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p1.4.4.m1.1b"><ci id="S6.p1.4.4.m1.1.1.cmml" xref="S6.p1.4.4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.4.m1.1c">\to</annotation></semantics></math>piano</span>).</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">For <span id="S6.p2.1.1" class="ltx_text ltx_font_smallcaps">bass<math id="S6.p2.1.1.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.p2.1.1.m1.1a"><mo stretchy="false" id="S6.p2.1.1.m1.1.1" xref="S6.p2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p2.1.1.m1.1b"><ci id="S6.p2.1.1.m1.1.1.cmml" xref="S6.p2.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.1.m1.1c">\to</annotation></semantics></math>bass</span>, we compare the seq2seq and roll2seq architectures defined in <a href="#S4.F2" title="In 4 Proposed model ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
For all other pairs, where the input is non-monophonic, we only employ roll2seq, since the sequential representation grows disproportionately in length in these cases and the computational cost of the attention mechanism becomes too heavy.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">We evaluate our models on our synthetic test set generated by BIAB and on the
Bodhidharma MIDI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
The latter is a diverse collection of 950 MIDI recordings annotated with genre labels. We filtered and pre-processed the dataset in the same way as the synthetic test set and we extracted the bass and piano tracks.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>To form the bass track, we retrieve all notes assigned to any Bass instrument. For the piano track, we use the Piano and Organ classes.</span></span></span></p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">We also made extensive attempts to train the recent models of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> on our data using the source code published by the authors, but unfortunately without success. This has prevented us from comparing these models with our proposal. Nonetheless, the provided outputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> can serve as a basis for perceptual comparison.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Evaluation</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">For a comprehensive evaluation of each model, we translated all inputs to all 70 styles and calculated the content preservation and style fit metrics. The results (averaged) are presented in <a href="#S5.F6" title="In 5 Evaluation metrics ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">We provide two baselines for each track (bass and piano): ‘source’, which is simply the same track before the translation, and ‘reference’, which is a track generated by BIAB based on the chord chart (only available for the synthetic test set).
As expected, the style fit is low for the source track (measured with respect to the target style) and close to 1 for the reference track.
Our models’ outputs generally do not fit the target style as perfectly as the reference does, but still score high compared to the source.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">As for content preservation, we can notice that the reference value is quite low (0.78 for <span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_smallcaps">bass</span> and 0.79 for <span id="S6.SS1.p3.1.2" class="ltx_text ltx_font_smallcaps">piano</span>).
This should not be too surprising, since we are comparing accompaniments in two different styles, which might have different pitch-class distributions; moreover, there is some random harmonic variation within each style (see e.g. bars 5–6 in <a href="#S3.F1" title="In 3 Synthetic data generation ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
The results achieved by our models on the synthetic test set are very close to the reference.
To illustrate the value range of the metric, we provide the results obtained by a ‘randomized’ baseline (shown as ‘random’ in <a href="#S5.F6" title="In 5 Evaluation metrics ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>), where we randomly permuted the reference segments for each style (obtaining a reference with the correct style, but the wrong content).
The resulting value is very low (0.16 for <span id="S6.SS1.p3.1.3" class="ltx_text ltx_font_smallcaps">bass</span> and 0.31 for <span id="S6.SS1.p3.1.4" class="ltx_text ltx_font_smallcaps">piano</span>) compared both to the true reference and to our models, indicating that the metric is useful and the models are performing well.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">On Bodhidharma, content preservation is generally weaker than on the synthetic test set.
One interpretation can be that the encoder simply fails to extract the content information accurately, since it was trained on a different domain.
However, we also find that the models often make timing errors on Bodhidharma inputs, leading to misalignment between the input and the output, which may also cause the content preservation metric to drop.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">On the other hand, the style fit on Bodhidharma is close to the results on the synthetic test set (and not consistently lower or higher), and the difference to ‘source’ (i.e. the corresponding input track) is more marked, perhaps reflecting a higher style variability in the Bodhidharma data.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para">
<p id="S6.SS1.p6.1" class="ltx_p">Upon listening, we clearly observe that the outputs are musical and seem to both fit the target style and follow the harmonic structure of the inputs.
Besides, even though the piano and the bass tracks are generated independently, they sound surprisingly coherent. However, as mentioned above, we also observe occasional timing errors (especially in heavily syncopated grooves), which become more prominent when the bass and piano tracks are combined. A potential remedy for this issue would be to modify the encoding to make it more robust, e.g. by representing the timing in a beat-aware manner.</p>
</div>
<div id="S6.SS1.p7" class="ltx_para">
<p id="S6.SS1.p7.1" class="ltx_p">We also note that the single-track models output harmonically incorrect notes more often than the <span id="S6.SS1.p7.1.1" class="ltx_text ltx_font_smallcaps">all</span> models; this is expected, since their <em id="S6.SS1.p7.1.2" class="ltx_emph ltx_font_italic">input</em> is less harmonically rich.
This effect is clearly audible (especially in <span id="S6.SS1.p7.1.3" class="ltx_text ltx_font_smallcaps">bass</span>, where important scale degrees are often missing in the input), but cannot be captured by the content preservation metric, which is computed against the same input.</p>
</div>
<figure id="S6.F7" class="ltx_figure"><img src="/html/1907.02265/assets/x9.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.8.1.1" class="ltx_text ltx_font_bold">Figure 7</span>: </span>Comparison of a single-style-pair model (1<math id="S6.F7.4.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.F7.4.m1.1b"><mo stretchy="false" id="S6.F7.4.m1.1.1" xref="S6.F7.4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.F7.4.m1.1c"><ci id="S6.F7.4.m1.1.1.cmml" xref="S6.F7.4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F7.4.m1.1d">\to</annotation></semantics></math>1) and a full model (70<math id="S6.F7.5.m2.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.F7.5.m2.1b"><mo stretchy="false" id="S6.F7.5.m2.1.1" xref="S6.F7.5.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.F7.5.m2.1c"><ci id="S6.F7.5.m2.1.1.cmml" xref="S6.F7.5.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F7.5.m2.1d">\to</annotation></semantics></math>70) on the ZZJAZZSW<math id="S6.F7.6.m3.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.F7.6.m3.1b"><mo stretchy="false" id="S6.F7.6.m3.1.1" xref="S6.F7.6.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.F7.6.m3.1c"><ci id="S6.F7.6.m3.1.1.cmml" xref="S6.F7.6.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F7.6.m3.1d">\to</annotation></semantics></math>TWIST style pair.</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Comparison with a single-pair model</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">All models presented so far were trained on music in 70 different styles, as opposed to a single style pair.
To investigate the effect of this choice, we picked a pair of fairly dissimilar styles – ZZJAZZSW (‘Jazz Swing Variation’) and TWIST (‘Twist Style’, categorized as ‘Lite Pop’) – and generated a new training, validation and test set with each song rendered in these two styles only.
To increase the amount of data, we performed this twice for each song (with different results), obtaining <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="2\times 2=4" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mrow id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml"><mrow id="S6.SS2.p1.1.m1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.2.cmml"><mn id="S6.SS2.p1.1.m1.1.1.2.2" xref="S6.SS2.p1.1.m1.1.1.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS2.p1.1.m1.1.1.2.1" xref="S6.SS2.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="S6.SS2.p1.1.m1.1.1.2.3" xref="S6.SS2.p1.1.m1.1.1.2.3.cmml">2</mn></mrow><mo id="S6.SS2.p1.1.m1.1.1.1" xref="S6.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S6.SS2.p1.1.m1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><apply id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"><eq id="S6.SS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1"></eq><apply id="S6.SS2.p1.1.m1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.2"><times id="S6.SS2.p1.1.m1.1.1.2.1.cmml" xref="S6.SS2.p1.1.m1.1.1.2.1"></times><cn type="integer" id="S6.SS2.p1.1.m1.1.1.2.2.cmml" xref="S6.SS2.p1.1.m1.1.1.2.2">2</cn><cn type="integer" id="S6.SS2.p1.1.m1.1.1.2.3.cmml" xref="S6.SS2.p1.1.m1.1.1.2.3">2</cn></apply><cn type="integer" id="S6.SS2.p1.1.m1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">2\times 2=4</annotation></semantics></math> training pairs per segment.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.3" class="ltx_p">We used this new dataset to train single-style-pair versions of all models (in the ZZJAZZSW<math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mo stretchy="false" id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\to</annotation></semantics></math>TWIST direction only), preserving the original architectures except for the conditioning on the target style.
We compare these ‘1<math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><mo stretchy="false" id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><ci id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">\to</annotation></semantics></math>1’ models to the full versions (70<math id="S6.SS2.p2.3.m3.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.SS2.p2.3.m3.1a"><mo stretchy="false" id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><ci id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">\to</annotation></semantics></math>70) on two sets of inputs:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">the synthetic test set in the ZZJAZZSW style;</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">the ‘Swing’ section of Bodhidharma (23 songs).</p>
</div>
</li>
</ul>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.3" class="ltx_p">In <a href="#S6.F7" title="In 6.1 Evaluation ‣ 6 Experimental results ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we show the results for the two variants of the <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_smallcaps">all<math id="S6.SS2.p3.1.1.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.SS2.p3.1.1.m1.1a"><mo stretchy="false" id="S6.SS2.p3.1.1.m1.1.1" xref="S6.SS2.p3.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.1.m1.1b"><ci id="S6.SS2.p3.1.1.m1.1.1.cmml" xref="S6.SS2.p3.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.1.m1.1c">\to</annotation></semantics></math>bass</span> model.
While the performance on the synthetic data seems to be the same, the scores of the 1<math id="S6.SS2.p3.2.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.SS2.p3.2.m1.1a"><mo stretchy="false" id="S6.SS2.p3.2.m1.1.1" xref="S6.SS2.p3.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.2.m1.1b"><ci id="S6.SS2.p3.2.m1.1.1.cmml" xref="S6.SS2.p3.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.2.m1.1c">\to</annotation></semantics></math>1 model drop considerably on the Bodhidharma data, suggesting that the model is overfitted to the ‘synthetic’ swing style.
On the other hand, the performance of the 70<math id="S6.SS2.p3.3.m2.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.SS2.p3.3.m2.1a"><mo stretchy="false" id="S6.SS2.p3.3.m2.1.1" xref="S6.SS2.p3.3.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.3.m2.1b"><ci id="S6.SS2.p3.3.m2.1.1.cmml" xref="S6.SS2.p3.3.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.3.m2.1c">\to</annotation></semantics></math>70 model stays high, showing that training on many different styles helped the model generalize to real swing.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Style embedding analysis</h3>

<figure id="S6.F8" class="ltx_figure"><img src="/html/1907.02265/assets/x10.png" id="S6.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.4.2.1" class="ltx_text ltx_font_bold">Figure 8</span>: </span>Style embeddings learned by the <span id="S6.F8.2.1" class="ltx_text ltx_font_smallcaps">all<math id="S6.F8.2.1.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S6.F8.2.1.m1.1b"><mo stretchy="false" id="S6.F8.2.1.m1.1.1" xref="S6.F8.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.F8.2.1.m1.1c"><ci id="S6.F8.2.1.m1.1.1.cmml" xref="S6.F8.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F8.2.1.m1.1d">\to</annotation></semantics></math>piano</span> model, labeled with ‘feel’ annotations provided by BIAB. Dimensionality reduction was performed using linear discriminant analysis (LDA) with the feel labels as targets.</figcaption>
</figure>
<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Neural representation spaces are often found to exhibit a meaningful geometry, and our learned style embedding space is no exception.
As an example, <a href="#S6.F8" title="In 6.3 Style embedding analysis ‣ 6 Experimental results ‣ Supervised Symbolic Music Style Translation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows a projection of the embeddings labeled by the ‘feel’ of each style, with ‘even’ and ‘swing’ feel styles being clearly separated.
We include more plots in the supplementary material and also make available an interactive visualization.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://bit.ly/2G5Jgnq" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/2G5Jgnq</a></span></span></span></p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this study, we focused on symbolic music accompaniment style translation.
As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of aligned datasets, we developed the first fully supervised algorithm for this task, leveraging the power of synthetic training data.
Our experiments show that our models are capable of producing musically meaningful accompaniments even for real MIDI recordings.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We believe that these results point to interesting research directions.
First, synthetic data seem to be an excellent resource for music style translation, and could be used as a starting point even for unsupervised learning, allowing to validate a given approach before moving on to more challenging, unaligned datasets.
Second, our supervised approach could be used to address more general music transformation tasks, and we are already working on an extension in this direction.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgement</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">This research is supported by the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No. 765068 (MIP-Frontiers) and by the French National Research Agency (ANR) as a part of the FBIMATRIX (ANR-16-CE23-0014) project.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1409.0473, 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Band-in-a-Box.

</span>
<span class="ltx_bibblock">PG Music Inc.

</span>
<span class="ltx_bibblock">
<br class="ltx_break"><a target="_blank" href="https://www.pgmusic.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.pgmusic.com/</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Band-in-a-Box (BIAB) file archive.

</span>
<span class="ltx_bibblock">
<br class="ltx_break"><a target="_blank" href="https://groups.yahoo.com/group/Band-in-a-Box-Files/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://groups.yahoo.com/group/Band-in-a-Box-Files/</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Gino Brunner, Andres Konrad, Yuyi Wang, and Roger Wattenhofer.

</span>
<span class="ltx_bibblock">MIDI-VAE: Modeling dynamics and instrumentation of music with
applications to style transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ISMIR</span>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Gino Brunner, Yuyi Wang, Roger Wattenhofer, and Sumu Zhao.

</span>
<span class="ltx_bibblock">Symbolic music genre transfer with CycleGAN.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1809.07575, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Learning phrase representations using RNN encoder-decoder for
statistical machine translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2014.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ondřej Cífka.

</span>
<span class="ltx_bibblock">Supplementary material: Supervised symbolic music style translation
using synthetic data.

</span>
<span class="ltx_bibblock">Zenodo, 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.3250606" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.3250606</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Shuqi Dai, Zheng Zhang, and Gus Xia.

</span>
<span class="ltx_bibblock">Music style transfer: A position paper.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1803.06841, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jonathan Driedger, Thomas Prätzlich, and Meinard Müller.

</span>
<span class="ltx_bibblock">Let it Bee – towards NMF-inspired audio mosaicing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">ISMIR</span>, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Douglas Eck and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Finding temporal structure in music: blues improvisation with LSTM
recurrent networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">NNSP</span>, 2002.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Alexei A. Efros and William T. Freeman.

</span>
<span class="ltx_bibblock">Image quilting for texture synthesis and transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">SIGGRAPH</span>, 2001.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Sebastian Flossmann and Gerhard Widmer.

</span>
<span class="ltx_bibblock">Toward a multilevel model of expressive piano performance.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">ISPS</span>, 2011.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.

</span>
<span class="ltx_bibblock">Image style transfer using convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 2414–2423, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Eric Grinstein, Ngoc Q. K. Duong, Alexey Ozerov, and Patrick Pérez.

</span>
<span class="ltx_bibblock">Audio style transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 586–590, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Gaëtan Hadjeres and François Pachet.

</span>
<span class="ltx_bibblock">DeepBach: a steerable model for Bach chorales generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Gaëtan Hadjeres, Jason Sakellariou, and François Pachet.

</span>
<span class="ltx_bibblock">Style imitation and chord invention in polyphonic music with
exponential families.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1609.05152, 2016.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros.

</span>
<span class="ltx_bibblock">Image-to-image translation with conditional adversarial networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 5967–5976, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1412.6980, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Max Welling.

</span>
<span class="ltx_bibblock">Auto-encoding variational Bayes.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1312.6114, 2014.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato.

</span>
<span class="ltx_bibblock">Unsupervised machine translation using monolingual corpora only.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1711.00043, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Kyogu Lee and Malcolm Slaney.

</span>
<span class="ltx_bibblock">Acoustic chord transcription and key extraction from audio using
key-dependent hmms trained on synthesized audio.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</span>,
16:291–301, 2008.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Ming-Yu Liu, Thomas Breuel, and Jan Kautz.

</span>
<span class="ltx_bibblock">Unsupervised image-to-image translation networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Wei-Tsung Lu and Li Su.

</span>
<span class="ltx_bibblock">Transferring the style of homophonic music using recurrent neural
networks and autoregressive models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">ISMIR</span>, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Iman Malik and Carl Henrik Ek.

</span>
<span class="ltx_bibblock">Neural translation of musical style.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1708.03535, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Matthias Mauch and Simon Dixon.

</span>
<span class="ltx_bibblock">PYIN: A fundamental frequency estimator using probabilistic
threshold distributions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 659–663, 2014.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Cory McKay.

</span>
<span class="ltx_bibblock">Automatic genre classification of MIDI recordings.

</span>
<span class="ltx_bibblock">M.A. Thesis, McGill University, 2004.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Cory McKay and Ichiro Fujinaga.

</span>
<span class="ltx_bibblock">The Bodhidharma system and the results of the MIREX 2005 symbolic
genre classification contest.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">ISMIR</span>, 2005.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Noam Mor, Lior Wolf, Adam Polyak, and Yaniv Taigman.

</span>
<span class="ltx_bibblock">A universal music translation network.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1805.07848, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
François Pachet and Pierre Roy.

</span>
<span class="ltx_bibblock">Non-conformant harmonization: the Real Book in the style of Take
6.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">ICCC</span>, 2014.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Germán Ros, Laura Sellart, Joanna Materzynska, David Vázquez, and
Antonio M. López.

</span>
<span class="ltx_bibblock">The SYNTHIA dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 3234–3243, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Jason Sakellariou, Francesca Tria, Vittorio Loreto, and François Pachet.

</span>
<span class="ltx_bibblock">Maximum entropy models capture melodic styles.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Scientific Reports</span>, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Justin Salamon, Rachel M. Bittner, Jordi Bonada, Juan J. Bosch, Emilia
Gómez, and Juan Pablo Bello.

</span>
<span class="ltx_bibblock">An analysis/synthesis framework for automatic F0 annotation of
multitrack datasets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">ISMIR</span>, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ian Simon and Sageev Oore.

</span>
<span class="ltx_bibblock">Performance RNN: Generating music with expressive timing and
dynamics.

</span>
<span class="ltx_bibblock">Magenta Blog, 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://magenta.tensorflow.org/performance-rnn" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://magenta.tensorflow.org/performance-rnn</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Bob L. Sturm, João Felipe Santos, Oded Ben-Tal, and Iryna Korshunova.

</span>
<span class="ltx_bibblock">Music transcription modelling and composition using deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1604.08723, 2016.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Christopher J. Tralie.

</span>
<span class="ltx_bibblock">Cover song synthesis by analogy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">ISMIR</span>, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black,
Ivan Laptev, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Learning from synthetic humans.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 4627–4635, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Gerhard Widmer, Sebastian Flossmann, and Maarten Grachten.

</span>
<span class="ltx_bibblock">YQX plays Chopin.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">AI Magazine</span>, 30:35–48, 2009.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Xuexiang Xie, Feng Tian, and Seah Hock Soon.

</span>
<span class="ltx_bibblock">Feature guided texture synthesis (FGTS) for artistic style
transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">DIMEA</span>, 2007.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and Yann LeCun.

</span>
<span class="ltx_bibblock">Adversarially regularized autoencoders.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros.

</span>
<span class="ltx_bibblock">Unpaired image-to-image translation using cycle-consistent
adversarial networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">ICCV</span>, pages 2242–2251, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Aymeric Zils and François Pachet.

</span>
<span class="ltx_bibblock">Musical mosaicing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">COST G-6 Conference on Digital Audio Effects (DAFX-01)</span>,
2001.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1907.02264" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1907.02265" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1907.02265">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1907.02265" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1907.02266" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 15:57:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
