<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.08926] ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation</title><meta property="og:description" content="Transparent object depth perception poses a challenge in everyday life and logistics, primarily due to the inability of standard 3D sensors to accurately capture depth on transparent or reflective surfaces. This limita…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.08926">

<!--Generated on Sun Oct  6 00:14:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">ClearDepth: Enhanced Stereo Perception of Transparent Objects 
<br class="ltx_break">for Robotic Manipulation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaixin Bai<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Huajian Zeng<sup id="id9.9.id2" class="ltx_sup"><span id="id9.9.id2.1" class="ltx_text ltx_font_italic">2,3</span></sup>, Lei Zhang<sup id="id10.10.id3" class="ltx_sup"><span id="id10.10.id3.1" class="ltx_text ltx_font_italic">1,2†</span></sup>, Yiwen Liu<sup id="id11.11.id4" class="ltx_sup"><span id="id11.11.id4.1" class="ltx_text ltx_font_italic">2,3</span></sup>, Hongli Xu<sup id="id12.12.id5" class="ltx_sup"><span id="id12.12.id5.1" class="ltx_text ltx_font_italic">3</span></sup>, Zhaopeng Chen<sup id="id13.13.id6" class="ltx_sup"><span id="id13.13.id6.1" class="ltx_text ltx_font_italic">2</span></sup>, Jianwei Zhang<sup id="id14.14.id7" class="ltx_sup"><span id="id14.14.id7.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes">†Corresponding author. lei.zhang-1@studium.uni-hamburg.de<sup id="id15.15.id1" class="ltx_sup">1</sup>MIN-Fakultät Fachbereich Informatik TAMS, University of Hamburg <span id="id16.16.id2" class="ltx_text ltx_font_typewriter">{name.surname}@studium.uni-hamburg.de</span><sup id="id17.17.id1" class="ltx_sup">2</sup>Agile Robots SE <span id="id18.18.id2" class="ltx_text ltx_font_typewriter">{name.surname}@agile-robots.com</span><sup id="id19.19.id1" class="ltx_sup">3</sup>Technical University of Munich <span id="id20.20.id2" class="ltx_text ltx_font_typewriter">{name.surname}@tum.de</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id21.id1" class="ltx_p">Transparent object depth perception poses a challenge in everyday life and logistics, primarily due to the inability of standard 3D sensors to accurately capture depth on transparent or reflective surfaces. This limitation significantly affects depth map and point cloud-reliant applications, especially in robotic manipulation. We developed a vision transformer-based algorithm for stereo depth recovery of transparent objects. This approach is complemented by an innovative feature post-fusion module, which enhances the accuracy of depth recovery by structural features in images. To address the high costs associated with dataset collection for stereo camera-based perception of transparent objects, our method incorporates a parameter-aligned, domain-adaptive, and physically realistic Sim2Real simulation for efficient data generation, accelerated by AI algorithm. Our experimental results demonstrate the model’s exceptional Sim2Real generalizability in real-world scenarios, enabling precise depth mapping of transparent objects to assist in robotic manipulation.
Project details are available at <a target="_blank" href="https://sites.google.com/view/cleardepth/" title="" class="ltx_ref ltx_href">https://sites.google.com/view/cleardepth/</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Transparent objects, such as glass bottles or cups, are commonly found in domestic service robotics and logistics goods sorting scenarios. Their transparent nature, particularly the complexity of refraction and reflection, poses challenges for imaging and recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Accurate handling of these objects is crucial for robots that rely on precise three-dimensional information. Whether it’s accurately grasping glass items in home services or efficiently sorting transparent goods at logistics centers, advanced algorithms and technologies are required for machine vision systems. This includes the development of algorithms capable of handling materials with high reflectivity and refraction, or new sensor technologies to better capture the characteristics of transparent objects. These advancements will enhance the ability of robots to handle transparent objects in various environments, push the application of robotic vision technology into broader fields, and enable robots to perform complex tasks more autonomously.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.08926/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">
In this work, we explore how to reconstruct depth maps of transparent objects using Sim2Real technology combined with stereo vision, thereby enabling precise manipulation of transparent objects by robots. In the Section <a href="#S3" title="III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we focus on introducing a simulation data generation method enhanced by AI, aimed at achieving an optimal balance between the effectiveness and speed of data generation. The Section <a href="#S4" title="IV Experiments ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> conducts both qualitative and quantitative analyses comparing the performance of our method with the baseline, and demonstrates the efficacy of our approach through robot grasping experiments.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In addressing the depth perception challenges posed by transparent objects, researchers have developed innovative approaches. Initial efforts focused on using deep learning techniques to reconstruct depth information from incomplete depth images captured by RealSense cameras, optimizing for greater accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Subsequent studies explored stereo vision systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and multi-view approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, leveraging neural networks to enhance perception capabilities.
Despite these advancements, real-world applications still face challenges, such as inconsistent depth inputs and the complexities of multi-view imaging. A major obstacle in 3D reconstruction of transparent objects is the scarcity of reliable feature points. Innovations addressing this include extracting structural details, such as boundary identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and improving hardware capabilities for more accurate depth imagery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Deep learning has played a pivotal role in learning and interpreting the complex geometric features of these objects, thereby enhancing the precision and reliability of 3D models.
For deep learning tasks involving transparent objects, extensive and accurate datasets are crucial. Various techniques have been used for dataset collection, including using markers for object poses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, substituting transparent objects with opaque ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and manual manipulation in 3D software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. However, these methods are labor-intensive and often result in noisy depth maps. To overcome these challenges, simulation engines are increasingly used to generate synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, though balancing the realism of ray-tracing with the efficiency of rasterization remains a significant challenge.
Our approach includes developing an AI-accelerated tool for generating realistic simulation datasets for transparent object depth recovery. This tool enables direct application of models on actual sensors without post-training, producing instance segmentation, poses, and depth maps. We also introduce a stereo depth recovery network specifically designed for transparent objects. Utilizing a cascaded vision transformer (ViT) backbone, it efficiently extracts contextual structural information. This network outperforms previous CNN and ViT-based models, particularly for high-resolution images and robotic manipulation. Our contributions address the limitations of existing depth estimation methods and datasets for transparent objects. We present a novel AI-enhanced stereo-based simulation dataset generation method, creating a detailed, noise-free dataset. Additionally, our end-to-end stereo depth recovery network operates independently of mask priors and includes a post-processing structure that enhances structural details. This stereo imaging algorithm can be directly integrated into existing robotic grasping pipelines. The whole pipeline is as in Fig. <a href="#S1.F1" title="Figure 1 ‣ I INTRODUCTION ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Perception of Transparent Objects</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Robotics faces significant challenges in perceiving transparent objects due to their low contrast and complex light behavior, which affects sensor accuracy in determining their position and shape. Traditional sensors like RGB and RGB-D cameras often struggle with transparent objects because they focus on intensity data, missing the nuances of optical properties.
To address this, research has increasingly turned to polarized cameras, which reduce reflections, increase contrast, and provide richer light information, enhancing the detection of transparent objects’ shape and position <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, their high cost limits widespread use in robotics.
Some studies, like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, enhance transparent object tracking by combining CNN and transformer-based encoders, while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> employs Sim2Real strategies with simulated datasets. Research such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> addresses transparent object matting by predicting alpha channel values, aiding image editing.
For robotic manipulation and pose estimation, recent works have expanded perception models to include multi-task extensions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Depth recovery and reconstruction remain particularly challenging due to light interactions, with techniques like Nerf and volumetric rendering used for surface reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and stereo and multiview approaches for depth and geometry regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
These efforts utilize various sensors, including RGB-D, stereo-vision, and multi-view systems, to improve transparent object perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The field, while complex, is advancing with deep learning and sensor technology, aiming to enhance accuracy and reliability.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Deep Learning-based Stereo Depth Recovery</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Deep learning-based stereo matching methods have recently outperformed traditional approaches, with 2D convolutional models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> offering simplicity and efficiency. These models achieve high accuracy even on limited computational resources, making them suitable for engineering applications, though they still require improvements in accuracy and robustness due to 3D cost space constraints.
3D convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> provide better interpretability and higher disparity map accuracy but require optimization due to their computational demands. RAFT-Stereo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, an extension of RAFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, applies optical flow techniques to stereo matching, improving generalization and robustness with its lightweight GRU module, but struggles with global context extraction due to its CNN architecture.
STTR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, inspired by SuperGlue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, uses transformers with positional embedding and attention mechanisms for binocular dense matching, producing disparity and depth maps. However, these methods are computationally intensive and slow in inference, limiting their suitability for high-resolution images and downstream robotic tasks.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Transparent Object Datasets</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In the open-source community, various datasets for transparent objects are available, such as real-world datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. In Depth Anything v1, the authors used both open-source real and synthetic datasets for training, but in v2, they switched entirely to synthetic data. Introducing even 10% real data can significantly degrade performance due to sensor noise in the labels, which is particularly detrimental in depth recovery tasks. The challenge of accurately annotating complex, occluded real-world scenes led us to prioritize synthetic data. To mitigate potential sim2real gaps, we used ray-tracing renderers to simulate realistic light refraction. Existing synthetic datasets, such as  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, typically feature transparent objects on desktops. However, these datasets lack the complexity necessary for generalization to real-world scenarios like kitchens, bedrooms, and offices, where service and humanoid robots operate.
Moreover, these datasets often require extensive pre- or post-processing, such as segmentation or background reconstruction, which is impractical for end-to-end algorithms crucial to embodied intelligence applications. Synthetic datasets with HDRI backgrounds, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, also face challenges. HDRI backgrounds lack depth value labels, leading to confusion and poor generalization, especially in zero-shot scenarios for service robots. Simulation data for Realsense cameras, like  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, involve complex rendering processes that reduce efficiency. The process includes rendering the RGB image, simulating IR projection with spotlights, and generating noisy depth maps, which is both complex and time-consuming.
In designing our dataset and generation process, we address the limitations of existing datasets while anticipating the needs of future embodied intelligence algorithms. Our dataset includes indoor scenes typical for service robots, enriched with background depth values for transparent objects. It features both container-type and cosmetic transparent objects, carefully designed to overcome current deficiencies and ensure future scalability.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Network Overview</span>
</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.08926/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">
Our stereo depth recovery network for transparent objects. Feature extraction is performed on left and right images, with additional processing by a shared weighted context encoder for the left image. A correlation pyramid, created by merging the feature maps, is refined through a GRU loop, enhancing structural information for depth recovery. The output is a refined disparity map.</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Transparent objects refract background textures, making texture features less relevant and structural features more critical for imaging and perception tasks. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, it was found that CNNs excel at recognizing textures, while ViTs are better at recognizing shapes.
Traditional ViTs downsample inputs and outputs to a smaller size, using learnable upsampling. While effective in many vision tasks, ViTs are challenging to train, computationally intensive, and less effective at fine-grained feature extraction. To address this, models like SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and DinoV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> use cascaded structures to enhance ViTs for fine-grained tasks like depth estimation and semantic segmentation, providing multi-scale features and easing training.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For transparent objects, structural features are crucial. Considering the fine-grained needs of stereo tasks and the demand for lightweight, fast models in robotics, we utilize MixVisionTransformer B5 for feature extraction. Traditional stereo deep learning networks rely on feature dot products for similarity, which is inadequate for transparent objects due to background texture refraction. Incorporating structural feature priors in the GRU loop is essential. While cross-attention or spatial-attention structures could be used, they are parameter-heavy and slow in inference. Therefore, we designed a lightweight structural feature post-fusion module to introduce these features into the GRU loop, enhancing performance by leveraging the low-level structural feature. Our network structure is as in Fig. <a href="#S3.F2" title="Figure 2 ‣ III-A Network Overview ‣ III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Cascaded Vision Transformer Backbone.</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.12" class="ltx_p">Our backbone architecture, conforming to the B5 settings of SegFormer, initiates with overlap patch embedding for initial image tokenization, crucial for preserving local features. This process sequentially passes the tokens through four transformer blocks (<math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="N=4" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">N</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><eq id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1"></eq><ci id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">N=4</annotation></semantics></math>), effectively generating feature maps at scaled dimensions of <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\frac{1}{4}" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mfrac id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">1</mn><mn id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml">4</mn></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><divide id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"></divide><cn type="integer" id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">\frac{1}{4}</annotation></semantics></math>, <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\frac{1}{8}" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mfrac id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml"><mn id="S3.SS1.SSS1.p1.3.m3.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml">1</mn><mn id="S3.SS1.SSS1.p1.3.m3.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml">8</mn></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><apply id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1"><divide id="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1"></divide><cn type="integer" id="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">\frac{1}{8}</annotation></semantics></math>, <math id="S3.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="\frac{1}{16}" display="inline"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><mfrac id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml"><mn id="S3.SS1.SSS1.p1.4.m4.1.1.2" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml">1</mn><mn id="S3.SS1.SSS1.p1.4.m4.1.1.3" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml">16</mn></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><apply id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1"><divide id="S3.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1"></divide><cn type="integer" id="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">\frac{1}{16}</annotation></semantics></math>, and <math id="S3.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\frac{1}{32}" display="inline"><semantics id="S3.SS1.SSS1.p1.5.m5.1a"><mfrac id="S3.SS1.SSS1.p1.5.m5.1.1" xref="S3.SS1.SSS1.p1.5.m5.1.1.cmml"><mn id="S3.SS1.SSS1.p1.5.m5.1.1.2" xref="S3.SS1.SSS1.p1.5.m5.1.1.2.cmml">1</mn><mn id="S3.SS1.SSS1.p1.5.m5.1.1.3" xref="S3.SS1.SSS1.p1.5.m5.1.1.3.cmml">32</mn></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.5.m5.1b"><apply id="S3.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1"><divide id="S3.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1"></divide><cn type="integer" id="S3.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m5.1c">\frac{1}{32}</annotation></semantics></math>. To optimize computational efficiency, the model incorporates efficient self-attention, which significantly reduces the computational burden from <math id="S3.SS1.SSS1.p1.6.m6.1" class="ltx_Math" alttext="O(N^{2})" display="inline"><semantics id="S3.SS1.SSS1.p1.6.m6.1a"><mrow id="S3.SS1.SSS1.p1.6.m6.1.1" xref="S3.SS1.SSS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS1.p1.6.m6.1.1.3" xref="S3.SS1.SSS1.p1.6.m6.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.6.m6.1.1.2" xref="S3.SS1.SSS1.p1.6.m6.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS1.p1.6.m6.1.1.1.1" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.2" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.cmml">(</mo><msup id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.2" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.2.cmml">N</mi><mn id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.3" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.3" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.6.m6.1b"><apply id="S3.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1"><times id="S3.SS1.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.2"></times><ci id="S3.SS1.SSS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.3">𝑂</ci><apply id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.2">𝑁</ci><cn type="integer" id="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.6.m6.1c">O(N^{2})</annotation></semantics></math> to <math id="S3.SS1.SSS1.p1.7.m7.1" class="ltx_Math" alttext="O(\frac{N^{2}}{R})" display="inline"><semantics id="S3.SS1.SSS1.p1.7.m7.1a"><mrow id="S3.SS1.SSS1.p1.7.m7.1.2" xref="S3.SS1.SSS1.p1.7.m7.1.2.cmml"><mi id="S3.SS1.SSS1.p1.7.m7.1.2.2" xref="S3.SS1.SSS1.p1.7.m7.1.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.7.m7.1.2.1" xref="S3.SS1.SSS1.p1.7.m7.1.2.1.cmml">​</mo><mrow id="S3.SS1.SSS1.p1.7.m7.1.2.3.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.7.m7.1.2.3.2.1" xref="S3.SS1.SSS1.p1.7.m7.1.1.cmml">(</mo><mfrac id="S3.SS1.SSS1.p1.7.m7.1.1" xref="S3.SS1.SSS1.p1.7.m7.1.1.cmml"><msup id="S3.SS1.SSS1.p1.7.m7.1.1.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS1.SSS1.p1.7.m7.1.1.2.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.2.cmml">N</mi><mn id="S3.SS1.SSS1.p1.7.m7.1.1.2.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.3.cmml">2</mn></msup><mi id="S3.SS1.SSS1.p1.7.m7.1.1.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.cmml">R</mi></mfrac><mo stretchy="false" id="S3.SS1.SSS1.p1.7.m7.1.2.3.2.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.7.m7.1b"><apply id="S3.SS1.SSS1.p1.7.m7.1.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.2"><times id="S3.SS1.SSS1.p1.7.m7.1.2.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.2.1"></times><ci id="S3.SS1.SSS1.p1.7.m7.1.2.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.2.2">𝑂</ci><apply id="S3.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.2.3.2"><divide id="S3.SS1.SSS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.2.3.2"></divide><apply id="S3.SS1.SSS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2">superscript</csymbol><ci id="S3.SS1.SSS1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.2">𝑁</ci><cn type="integer" id="S3.SS1.SSS1.p1.7.m7.1.1.2.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.3">2</cn></apply><ci id="S3.SS1.SSS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.7.m7.1c">O(\frac{N^{2}}{R})</annotation></semantics></math> by implementing a reduction ratio <math id="S3.SS1.SSS1.p1.8.m8.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS1.p1.8.m8.1a"><mi id="S3.SS1.SSS1.p1.8.m8.1.1" xref="S3.SS1.SSS1.p1.8.m8.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.8.m8.1b"><ci id="S3.SS1.SSS1.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS1.p1.8.m8.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.8.m8.1c">R</annotation></semantics></math>. This reduction is achieved by first reshaping the input sequence from <math id="S3.SS1.SSS1.p1.9.m9.1" class="ltx_Math" alttext="N\cdot C" display="inline"><semantics id="S3.SS1.SSS1.p1.9.m9.1a"><mrow id="S3.SS1.SSS1.p1.9.m9.1.1" xref="S3.SS1.SSS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.SSS1.p1.9.m9.1.1.2" xref="S3.SS1.SSS1.p1.9.m9.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.9.m9.1.1.1" xref="S3.SS1.SSS1.p1.9.m9.1.1.1.cmml">⋅</mo><mi id="S3.SS1.SSS1.p1.9.m9.1.1.3" xref="S3.SS1.SSS1.p1.9.m9.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.9.m9.1b"><apply id="S3.SS1.SSS1.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS1.p1.9.m9.1.1"><ci id="S3.SS1.SSS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.SSS1.p1.9.m9.1.1.1">⋅</ci><ci id="S3.SS1.SSS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.SSS1.p1.9.m9.1.1.2">𝑁</ci><ci id="S3.SS1.SSS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.SSS1.p1.9.m9.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.9.m9.1c">N\cdot C</annotation></semantics></math> to <math id="S3.SS1.SSS1.p1.10.m10.1" class="ltx_Math" alttext="\frac{N}{R}\times(C\cdot R)" display="inline"><semantics id="S3.SS1.SSS1.p1.10.m10.1a"><mrow id="S3.SS1.SSS1.p1.10.m10.1.1" xref="S3.SS1.SSS1.p1.10.m10.1.1.cmml"><mfrac id="S3.SS1.SSS1.p1.10.m10.1.1.3" xref="S3.SS1.SSS1.p1.10.m10.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.10.m10.1.1.3.2" xref="S3.SS1.SSS1.p1.10.m10.1.1.3.2.cmml">N</mi><mi id="S3.SS1.SSS1.p1.10.m10.1.1.3.3" xref="S3.SS1.SSS1.p1.10.m10.1.1.3.3.cmml">R</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.10.m10.1.1.2" xref="S3.SS1.SSS1.p1.10.m10.1.1.2.cmml">×</mo><mrow id="S3.SS1.SSS1.p1.10.m10.1.1.1.1" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.2" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.2" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.1" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.3" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.3.cmml">R</mi></mrow><mo stretchy="false" id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.3" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.10.m10.1b"><apply id="S3.SS1.SSS1.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1"><times id="S3.SS1.SSS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.2"></times><apply id="S3.SS1.SSS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.3"><divide id="S3.SS1.SSS1.p1.10.m10.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.3"></divide><ci id="S3.SS1.SSS1.p1.10.m10.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.3.2">𝑁</ci><ci id="S3.SS1.SSS1.p1.10.m10.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.3.3">𝑅</ci></apply><apply id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1"><ci id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.1">⋅</ci><ci id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.2">𝐶</ci><ci id="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1.1.1.1.3">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.10.m10.1c">\frac{N}{R}\times(C\cdot R)</annotation></semantics></math> by 2d convolutional layer with the stride <math id="S3.SS1.SSS1.p1.11.m11.4" class="ltx_Math" alttext="8,4,2,1" display="inline"><semantics id="S3.SS1.SSS1.p1.11.m11.4a"><mrow id="S3.SS1.SSS1.p1.11.m11.4.5.2" xref="S3.SS1.SSS1.p1.11.m11.4.5.1.cmml"><mn id="S3.SS1.SSS1.p1.11.m11.1.1" xref="S3.SS1.SSS1.p1.11.m11.1.1.cmml">8</mn><mo id="S3.SS1.SSS1.p1.11.m11.4.5.2.1" xref="S3.SS1.SSS1.p1.11.m11.4.5.1.cmml">,</mo><mn id="S3.SS1.SSS1.p1.11.m11.2.2" xref="S3.SS1.SSS1.p1.11.m11.2.2.cmml">4</mn><mo id="S3.SS1.SSS1.p1.11.m11.4.5.2.2" xref="S3.SS1.SSS1.p1.11.m11.4.5.1.cmml">,</mo><mn id="S3.SS1.SSS1.p1.11.m11.3.3" xref="S3.SS1.SSS1.p1.11.m11.3.3.cmml">2</mn><mo id="S3.SS1.SSS1.p1.11.m11.4.5.2.3" xref="S3.SS1.SSS1.p1.11.m11.4.5.1.cmml">,</mo><mn id="S3.SS1.SSS1.p1.11.m11.4.4" xref="S3.SS1.SSS1.p1.11.m11.4.4.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.11.m11.4b"><list id="S3.SS1.SSS1.p1.11.m11.4.5.1.cmml" xref="S3.SS1.SSS1.p1.11.m11.4.5.2"><cn type="integer" id="S3.SS1.SSS1.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS1.p1.11.m11.1.1">8</cn><cn type="integer" id="S3.SS1.SSS1.p1.11.m11.2.2.cmml" xref="S3.SS1.SSS1.p1.11.m11.2.2">4</cn><cn type="integer" id="S3.SS1.SSS1.p1.11.m11.3.3.cmml" xref="S3.SS1.SSS1.p1.11.m11.3.3">2</cn><cn type="integer" id="S3.SS1.SSS1.p1.11.m11.4.4.cmml" xref="S3.SS1.SSS1.p1.11.m11.4.4">1</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.11.m11.4c">8,4,2,1</annotation></semantics></math> for different ViT blocks, as detailed in equation <a href="#S3.E1" title="In III-A1 Cascaded Vision Transformer Backbone. ‣ III-A Network Overview ‣ III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, and then adjusting the sequence dimensions back to <math id="S3.SS1.SSS1.p1.12.m12.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.SSS1.p1.12.m12.1a"><mi id="S3.SS1.SSS1.p1.12.m12.1.1" xref="S3.SS1.SSS1.p1.12.m12.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.12.m12.1b"><ci id="S3.SS1.SSS1.p1.12.m12.1.1.cmml" xref="S3.SS1.SSS1.p1.12.m12.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.12.m12.1c">C</annotation></semantics></math> through linear layers, as described in equation <a href="#S3.E2" title="In III-A1 Cascaded Vision Transformer Backbone. ‣ III-A Network Overview ‣ III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\displaystyle\hat{K}=Reshape(\frac{N}{R},C\cdot R)(K)" display="inline"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mover accent="true" id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml">K</mi><mo id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">^</mo></mover><mo id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.4" xref="S3.E1.m1.3.3.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2a" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.5" xref="S3.E1.m1.3.3.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2b" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.6" xref="S3.E1.m1.3.3.1.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2c" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.7" xref="S3.E1.m1.3.3.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2d" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.8" xref="S3.E1.m1.3.3.1.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2e" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.9" xref="S3.E1.m1.3.3.1.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2f" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">(</mo><mstyle displaystyle="true" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mfrac id="S3.E1.m1.1.1a" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">N</mi><mi id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">R</mi></mfrac></mstyle><mo id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.cmml">,</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">⋅</mo><mi id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">R</mi></mrow><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.4" xref="S3.E1.m1.3.3.1.1.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2g" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.10.2" xref="S3.E1.m1.3.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.10.2.1" xref="S3.E1.m1.3.3.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">K</mi><mo stretchy="false" id="S3.E1.m1.3.3.1.10.2.2" xref="S3.E1.m1.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"></eq><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><ci id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1">^</ci><ci id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2">𝐾</ci></apply><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><times id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></times><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">𝑅</ci><ci id="S3.E1.m1.3.3.1.4.cmml" xref="S3.E1.m1.3.3.1.4">𝑒</ci><ci id="S3.E1.m1.3.3.1.5.cmml" xref="S3.E1.m1.3.3.1.5">𝑠</ci><ci id="S3.E1.m1.3.3.1.6.cmml" xref="S3.E1.m1.3.3.1.6">ℎ</ci><ci id="S3.E1.m1.3.3.1.7.cmml" xref="S3.E1.m1.3.3.1.7">𝑎</ci><ci id="S3.E1.m1.3.3.1.8.cmml" xref="S3.E1.m1.3.3.1.8">𝑝</ci><ci id="S3.E1.m1.3.3.1.9.cmml" xref="S3.E1.m1.3.3.1.9">𝑒</ci><interval closure="open" id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1"></divide><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝑁</ci><ci id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">𝑅</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><ci id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">⋅</ci><ci id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">𝐶</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">𝑅</ci></apply></interval><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\displaystyle\hat{K}=Reshape(\frac{N}{R},C\cdot R)(K)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\displaystyle K=Linear(C\cdot R,C)(\hat{K})" display="inline"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><mi id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml">K</mi><mo id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mi id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.3.3.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mi id="S3.E2.m1.3.3.1.4" xref="S3.E2.m1.3.3.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2a" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mi id="S3.E2.m1.3.3.1.5" xref="S3.E2.m1.3.3.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2b" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mi id="S3.E2.m1.3.3.1.6" xref="S3.E2.m1.3.3.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2c" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mi id="S3.E2.m1.3.3.1.7" xref="S3.E2.m1.3.3.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2d" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mi id="S3.E2.m1.3.3.1.8" xref="S3.E2.m1.3.3.1.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2e" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">⋅</mo><mi id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.3.cmml">R</mi></mrow><mo id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.2.cmml">,</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">C</mi><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.4" xref="S3.E2.m1.3.3.1.1.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2f" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.9.2" xref="S3.E2.m1.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.9.2.1" xref="S3.E2.m1.2.2.cmml">(</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mi id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">K</mi><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E2.m1.3.3.1.9.2.2" xref="S3.E2.m1.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"></eq><ci id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3">𝐾</ci><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><times id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1.2"></times><ci id="S3.E2.m1.3.3.1.3.cmml" xref="S3.E2.m1.3.3.1.3">𝐿</ci><ci id="S3.E2.m1.3.3.1.4.cmml" xref="S3.E2.m1.3.3.1.4">𝑖</ci><ci id="S3.E2.m1.3.3.1.5.cmml" xref="S3.E2.m1.3.3.1.5">𝑛</ci><ci id="S3.E2.m1.3.3.1.6.cmml" xref="S3.E2.m1.3.3.1.6">𝑒</ci><ci id="S3.E2.m1.3.3.1.7.cmml" xref="S3.E2.m1.3.3.1.7">𝑎</ci><ci id="S3.E2.m1.3.3.1.8.cmml" xref="S3.E2.m1.3.3.1.8">𝑟</ci><interval closure="open" id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1"><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><ci id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1">⋅</ci><ci id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2">𝐶</ci><ci id="S3.E2.m1.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.3">𝑅</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐶</ci></interval><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.3.3.1.9.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">^</ci><ci id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\displaystyle K=Linear(C\cdot R,C)(\hat{K})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS1.p1.13" class="ltx_p">Additionally, the Mix-FFN module in the architecture addresses the challenge of performance degradation due to the interpolation of positional embeddings in the original ViT structure, especially when dealing with varying input image sizes, here by substituting positional embeddings with learnable depth-wise convolutions. The equation is as <a href="#S3.E3" title="In III-A1 Cascaded Vision Transformer Backbone. ‣ III-A Network Overview ‣ III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{x}_{\text{out}}=\text{MLP}(\text{GELU}(\text{Conv}_{3\times 3}(\text{MLP}(\mathbf{x}_{\text{in}}))))+\mathbf{x}_{\text{in}}" display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">𝐱</mi><mtext id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3a.cmml">out</mtext></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3a.cmml">GELU</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml">Conv</mtext><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">×</mo><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">3</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐱</mi><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml">in</mtext></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">+</mo><msub id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml">𝐱</mi><mtext id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3a.cmml">in</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝐱</ci><ci id="S3.E3.m1.1.1.3.3a.cmml" xref="S3.E3.m1.1.1.3.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">out</mtext></ci></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><plus id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></plus><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">MLP</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3">GELU</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2">Conv</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.1"></times><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.2">3</cn><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3">3</cn></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">MLP</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐱</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">in</mtext></ci></apply></apply></apply></apply></apply><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2">𝐱</ci><ci id="S3.E3.m1.1.1.1.3.3a.cmml" xref="S3.E3.m1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">in</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\mathbf{x}_{\text{out}}=\text{MLP}(\text{GELU}(\text{Conv}_{3\times 3}(\text{MLP}(\mathbf{x}_{\text{in}}))))+\mathbf{x}_{\text{in}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.08926/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">
We utilize hardware-accelerated ray tracing, OptiX AI denoiser, and AI-driven super-resolution. This process accelerates the dataset generation process.
</span></figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.2" class="ltx_p">Then, we concatenate multi-scale feature maps from different ViT blocks by upsampling them to a unified scale of <math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\frac{1}{4}" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mfrac id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml"><mn id="S3.SS1.SSS1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml">1</mn><mn id="S3.SS1.SSS1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml">4</mn></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"><divide id="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"></divide><cn type="integer" id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">\frac{1}{4}</annotation></semantics></math>. This combined feature map undergoes further refinement through a precise <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="1\cdot 1" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mrow id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml"><mn id="S3.SS1.SSS1.p2.2.m2.1.1.2" xref="S3.SS1.SSS1.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p2.2.m2.1.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.cmml">⋅</mo><mn id="S3.SS1.SSS1.p2.2.m2.1.1.3" xref="S3.SS1.SSS1.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><apply id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.1">⋅</ci><cn type="integer" id="S3.SS1.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">1\cdot 1</annotation></semantics></math> convolution, facilitating optimal dimension adjustment.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Structural Feature Post-Fusion</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In this work, we propose a modified GRU-based architecture to optimize disparity maps in a coarse-to-fine manner. Our Post-Fusion modification is particularly introduced to enhance the processing of transparent objects.
We made this modification because, during our depth estimation experiments on transparent objects, we observed that unlike regular objects, depth estimation for transparent objects particularly requires precise structural information.
Additionally, for transparent objects, due to the refractive nature of their background texture features, the feature similarity information obtained through dot product is not suitable for reconstructing transparent objects.
Therefore, it makes sense to introduce the structural information of the image itself during the GRU iterations. This approach ensures that the structural information extracted at different resolutions remains consistent throughout the iterative process.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">The core update equations in our model are defined as follows:</p>
<table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle x_{k}=" display="inline"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml">x</mi><mi id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mi id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2">𝑥</ci><ci id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3">𝑘</ci></apply><csymbol cd="latexml" id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle x_{k}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.5" class="ltx_Math" alttext="\displaystyle\;[\mathbf{C}_{k},\mathbf{d}_{k},\mathbf{c}_{k},\mathbf{c}_{r},\mathbf{c}_{h}]" display="inline"><semantics id="S3.E4.m2.5a"><mrow id="S3.E4.m2.5.5.5" xref="S3.E4.m2.5.5.6.cmml"><mo stretchy="false" id="S3.E4.m2.5.5.5.6" xref="S3.E4.m2.5.5.6.cmml">[</mo><msub id="S3.E4.m2.1.1.1.1" xref="S3.E4.m2.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.2.cmml">𝐂</mi><mi id="S3.E4.m2.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E4.m2.5.5.5.7" xref="S3.E4.m2.5.5.6.cmml">,</mo><msub id="S3.E4.m2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.cmml"><mi id="S3.E4.m2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.2.cmml">𝐝</mi><mi id="S3.E4.m2.2.2.2.2.3" xref="S3.E4.m2.2.2.2.2.3.cmml">k</mi></msub><mo id="S3.E4.m2.5.5.5.8" xref="S3.E4.m2.5.5.6.cmml">,</mo><msub id="S3.E4.m2.3.3.3.3" xref="S3.E4.m2.3.3.3.3.cmml"><mi id="S3.E4.m2.3.3.3.3.2" xref="S3.E4.m2.3.3.3.3.2.cmml">𝐜</mi><mi id="S3.E4.m2.3.3.3.3.3" xref="S3.E4.m2.3.3.3.3.3.cmml">k</mi></msub><mo id="S3.E4.m2.5.5.5.9" xref="S3.E4.m2.5.5.6.cmml">,</mo><msub id="S3.E4.m2.4.4.4.4" xref="S3.E4.m2.4.4.4.4.cmml"><mi id="S3.E4.m2.4.4.4.4.2" xref="S3.E4.m2.4.4.4.4.2.cmml">𝐜</mi><mi id="S3.E4.m2.4.4.4.4.3" xref="S3.E4.m2.4.4.4.4.3.cmml">r</mi></msub><mo id="S3.E4.m2.5.5.5.10" xref="S3.E4.m2.5.5.6.cmml">,</mo><msub id="S3.E4.m2.5.5.5.5" xref="S3.E4.m2.5.5.5.5.cmml"><mi id="S3.E4.m2.5.5.5.5.2" xref="S3.E4.m2.5.5.5.5.2.cmml">𝐜</mi><mi id="S3.E4.m2.5.5.5.5.3" xref="S3.E4.m2.5.5.5.5.3.cmml">h</mi></msub><mo stretchy="false" id="S3.E4.m2.5.5.5.11" xref="S3.E4.m2.5.5.6.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.5b"><list id="S3.E4.m2.5.5.6.cmml" xref="S3.E4.m2.5.5.5"><apply id="S3.E4.m2.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.2">𝐂</ci><ci id="S3.E4.m2.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.3">𝑘</ci></apply><apply id="S3.E4.m2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.2.2.1.cmml" xref="S3.E4.m2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m2.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2.2">𝐝</ci><ci id="S3.E4.m2.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.2.3">𝑘</ci></apply><apply id="S3.E4.m2.3.3.3.3.cmml" xref="S3.E4.m2.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m2.3.3.3.3.1.cmml" xref="S3.E4.m2.3.3.3.3">subscript</csymbol><ci id="S3.E4.m2.3.3.3.3.2.cmml" xref="S3.E4.m2.3.3.3.3.2">𝐜</ci><ci id="S3.E4.m2.3.3.3.3.3.cmml" xref="S3.E4.m2.3.3.3.3.3">𝑘</ci></apply><apply id="S3.E4.m2.4.4.4.4.cmml" xref="S3.E4.m2.4.4.4.4"><csymbol cd="ambiguous" id="S3.E4.m2.4.4.4.4.1.cmml" xref="S3.E4.m2.4.4.4.4">subscript</csymbol><ci id="S3.E4.m2.4.4.4.4.2.cmml" xref="S3.E4.m2.4.4.4.4.2">𝐜</ci><ci id="S3.E4.m2.4.4.4.4.3.cmml" xref="S3.E4.m2.4.4.4.4.3">𝑟</ci></apply><apply id="S3.E4.m2.5.5.5.5.cmml" xref="S3.E4.m2.5.5.5.5"><csymbol cd="ambiguous" id="S3.E4.m2.5.5.5.5.1.cmml" xref="S3.E4.m2.5.5.5.5">subscript</csymbol><ci id="S3.E4.m2.5.5.5.5.2.cmml" xref="S3.E4.m2.5.5.5.5.2">𝐜</ci><ci id="S3.E4.m2.5.5.5.5.3.cmml" xref="S3.E4.m2.5.5.5.5.3">ℎ</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.5c">\displaystyle\;[\mathbf{C}_{k},\mathbf{d}_{k},\mathbf{c}_{k},\mathbf{c}_{r},\mathbf{c}_{h}]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle z_{k}=" display="inline"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.2.2" xref="S3.E5.m1.1.1.2.2.cmml">z</mi><mi id="S3.E5.m1.1.1.2.3" xref="S3.E5.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">=</mo><mi id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"></eq><apply id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.2.2">𝑧</ci><ci id="S3.E5.m1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.2.3">𝑘</ci></apply><csymbol cd="latexml" id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle z_{k}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.1" class="ltx_Math" alttext="\displaystyle\;\sigma(\text{Conv}([h_{k-1},x_{k}],W_{z})+c_{k})," display="inline"><semantics id="S3.E5.m2.1a"><mrow id="S3.E5.m2.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><mrow id="S3.E5.m2.1.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m2.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m2.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m2.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.2.cmml"><mtext id="S3.E5.m2.1.1.1.1.1.1.1.2.4" xref="S3.E5.m2.1.1.1.1.1.1.1.2.4a.cmml">Conv</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.1.1.1.2.3" xref="S3.E5.m2.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">[</mo><msub id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">h</mi><mrow id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">k</mi><mo id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">x</mi><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.5" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.2.cmml">W</mi><mi id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.3.cmml">z</mi></msub><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E5.m2.1.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.3.cmml">+</mo><msub id="S3.E5.m2.1.1.1.1.1.1.1.4" xref="S3.E5.m2.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.4.2" xref="S3.E5.m2.1.1.1.1.1.1.1.4.2.cmml">c</mi><mi id="S3.E5.m2.1.1.1.1.1.1.1.4.3" xref="S3.E5.m2.1.1.1.1.1.1.1.4.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.E5.m2.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m2.1.1.1.2" xref="S3.E5.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.1b"><apply id="S3.E5.m2.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1"><times id="S3.E5.m2.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.2"></times><ci id="S3.E5.m2.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.3">𝜎</ci><apply id="S3.E5.m2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1"><plus id="S3.E5.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.3"></plus><apply id="S3.E5.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2"><times id="S3.E5.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.3"></times><ci id="S3.E5.m2.1.1.1.1.1.1.1.2.4a.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.4"><mtext id="S3.E5.m2.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.4">Conv</mtext></ci><interval closure="open" id="S3.E5.m2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2"><interval closure="closed" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2"><apply id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ℎ</ci><apply id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑘</ci><cn type="integer" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑥</ci><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑘</ci></apply></interval><apply id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.2">𝑊</ci><ci id="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2.2.2.2.3">𝑧</ci></apply></interval></apply><apply id="S3.E5.m2.1.1.1.1.1.1.1.4.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.4.2">𝑐</ci><ci id="S3.E5.m2.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.4.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.1c">\displaystyle\;\sigma(\text{Conv}([h_{k-1},x_{k}],W_{z})+c_{k}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\displaystyle r_{k}=" display="inline"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><msub id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.2.2" xref="S3.E6.m1.1.1.2.2.cmml">r</mi><mi id="S3.E6.m1.1.1.2.3" xref="S3.E6.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml">=</mo><mi id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"></eq><apply id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.2.2">𝑟</ci><ci id="S3.E6.m1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.2.3">𝑘</ci></apply><csymbol cd="latexml" id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle r_{k}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E6.m2.1" class="ltx_Math" alttext="\displaystyle\;\sigma(\text{Conv}([h_{k-1},x_{k}],W_{r})+c_{r})," display="inline"><semantics id="S3.E6.m2.1a"><mrow id="S3.E6.m2.1.1.1" xref="S3.E6.m2.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1" xref="S3.E6.m2.1.1.1.1.cmml"><mi id="S3.E6.m2.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m2.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m2.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.2.cmml"><mtext id="S3.E6.m2.1.1.1.1.1.1.1.2.4" xref="S3.E6.m2.1.1.1.1.1.1.1.2.4a.cmml">Conv</mtext><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.1.1.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">[</mo><msub id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">h</mi><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">k</mi><mo id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">x</mi><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.5" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.2.cmml">W</mi><mi id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.3.cmml">r</mi></msub><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E6.m2.1.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.1.3.cmml">+</mo><msub id="S3.E6.m2.1.1.1.1.1.1.1.4" xref="S3.E6.m2.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.4.2" xref="S3.E6.m2.1.1.1.1.1.1.1.4.2.cmml">c</mi><mi id="S3.E6.m2.1.1.1.1.1.1.1.4.3" xref="S3.E6.m2.1.1.1.1.1.1.1.4.3.cmml">r</mi></msub></mrow><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m2.1.1.1.2" xref="S3.E6.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.1b"><apply id="S3.E6.m2.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1"><times id="S3.E6.m2.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.2"></times><ci id="S3.E6.m2.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.3">𝜎</ci><apply id="S3.E6.m2.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1"><plus id="S3.E6.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.3"></plus><apply id="S3.E6.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2"><times id="S3.E6.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.3"></times><ci id="S3.E6.m2.1.1.1.1.1.1.1.2.4a.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.4"><mtext id="S3.E6.m2.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.4">Conv</mtext></ci><interval closure="open" id="S3.E6.m2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2"><interval closure="closed" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2"><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ℎ</ci><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑘</ci><cn type="integer" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑥</ci><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑘</ci></apply></interval><apply id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.2">𝑊</ci><ci id="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.2.2.2.2.3">𝑟</ci></apply></interval></apply><apply id="S3.E6.m2.1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.4.2">𝑐</ci><ci id="S3.E6.m2.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.4.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.1c">\displaystyle\;\sigma(\text{Conv}([h_{k-1},x_{k}],W_{r})+c_{r}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\displaystyle\tilde{h}_{k}=" display="inline"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><msub id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml"><mover accent="true" id="S3.E7.m1.1.1.2.2" xref="S3.E7.m1.1.1.2.2.cmml"><mi id="S3.E7.m1.1.1.2.2.2" xref="S3.E7.m1.1.1.2.2.2.cmml">h</mi><mo id="S3.E7.m1.1.1.2.2.1" xref="S3.E7.m1.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.E7.m1.1.1.2.3" xref="S3.E7.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml">=</mo><mi id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"></eq><apply id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.2">subscript</csymbol><apply id="S3.E7.m1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.2.2"><ci id="S3.E7.m1.1.1.2.2.1.cmml" xref="S3.E7.m1.1.1.2.2.1">~</ci><ci id="S3.E7.m1.1.1.2.2.2.cmml" xref="S3.E7.m1.1.1.2.2.2">ℎ</ci></apply><ci id="S3.E7.m1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.2.3">𝑘</ci></apply><csymbol cd="latexml" id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\displaystyle\tilde{h}_{k}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E7.m2.2" class="ltx_Math" alttext="\displaystyle\,\tanh(\text{Conv}([r_{k}\odot h_{k-1},x_{k}],W_{h})+c_{h})," display="inline"><semantics id="S3.E7.m2.2a"><mrow id="S3.E7.m2.2.2.1"><mrow id="S3.E7.m2.2.2.1.1.1" xref="S3.E7.m2.2.2.1.1.2.cmml"><mi id="S3.E7.m2.1.1" xref="S3.E7.m2.1.1.cmml">tanh</mi><mo id="S3.E7.m2.2.2.1.1.1a" xref="S3.E7.m2.2.2.1.1.2.cmml">⁡</mo><mrow id="S3.E7.m2.2.2.1.1.1.1" xref="S3.E7.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.2" xref="S3.E7.m2.2.2.1.1.2.cmml">(</mo><mrow id="S3.E7.m2.2.2.1.1.1.1.1" xref="S3.E7.m2.2.2.1.1.1.1.1.cmml"><mrow id="S3.E7.m2.2.2.1.1.1.1.1.2" xref="S3.E7.m2.2.2.1.1.1.1.1.2.cmml"><mtext id="S3.E7.m2.2.2.1.1.1.1.1.2.4" xref="S3.E7.m2.2.2.1.1.1.1.1.2.4a.cmml">Conv</mtext><mo lspace="0em" rspace="0em" id="S3.E7.m2.2.2.1.1.1.1.1.2.3" xref="S3.E7.m2.2.2.1.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.3" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml">[</mo><mrow id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">r</mi><mi id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⊙</mo><msub id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">h</mi><mrow id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">k</mi><mo id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml">x</mi><mi id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.5" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.4" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.2" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.2.cmml">W</mi><mi id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.3" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.3.cmml">h</mi></msub><mo stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.5" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E7.m2.2.2.1.1.1.1.1.3" xref="S3.E7.m2.2.2.1.1.1.1.1.3.cmml">+</mo><msub id="S3.E7.m2.2.2.1.1.1.1.1.4" xref="S3.E7.m2.2.2.1.1.1.1.1.4.cmml"><mi id="S3.E7.m2.2.2.1.1.1.1.1.4.2" xref="S3.E7.m2.2.2.1.1.1.1.1.4.2.cmml">c</mi><mi id="S3.E7.m2.2.2.1.1.1.1.1.4.3" xref="S3.E7.m2.2.2.1.1.1.1.1.4.3.cmml">h</mi></msub></mrow><mo stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.3" xref="S3.E7.m2.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E7.m2.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m2.2b"><apply id="S3.E7.m2.2.2.1.1.2.cmml" xref="S3.E7.m2.2.2.1.1.1"><tanh id="S3.E7.m2.1.1.cmml" xref="S3.E7.m2.1.1"></tanh><apply id="S3.E7.m2.2.2.1.1.1.1.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1"><plus id="S3.E7.m2.2.2.1.1.1.1.1.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.3"></plus><apply id="S3.E7.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2"><times id="S3.E7.m2.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.3"></times><ci id="S3.E7.m2.2.2.1.1.1.1.1.2.4a.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.4"><mtext id="S3.E7.m2.2.2.1.1.1.1.1.2.4.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.4">Conv</mtext></ci><interval closure="open" id="S3.E7.m2.2.2.1.1.1.1.1.2.2.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2"><interval closure="closed" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2"><apply id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.1">direct-product</csymbol><apply id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑟</ci><ci id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2">ℎ</ci><apply id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3"><minus id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑘</ci><cn type="integer" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply><apply id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.2">𝑥</ci><ci id="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1.1.1.1.2.2.3">𝑘</ci></apply></interval><apply id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.2">𝑊</ci><ci id="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.2.2.3">ℎ</ci></apply></interval></apply><apply id="S3.E7.m2.2.2.1.1.1.1.1.4.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E7.m2.2.2.1.1.1.1.1.4.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E7.m2.2.2.1.1.1.1.1.4.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.4.2">𝑐</ci><ci id="S3.E7.m2.2.2.1.1.1.1.1.4.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.4.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m2.2c">\displaystyle\,\tanh(\text{Conv}([r_{k}\odot h_{k-1},x_{k}],W_{h})+c_{h}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E8.m1.1" class="ltx_Math" alttext="\displaystyle h_{k}=" display="inline"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><msub id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml"><mi id="S3.E8.m1.1.1.2.2" xref="S3.E8.m1.1.1.2.2.cmml">h</mi><mi id="S3.E8.m1.1.1.2.3" xref="S3.E8.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml">=</mo><mi id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><eq id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"></eq><apply id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.2">subscript</csymbol><ci id="S3.E8.m1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.2.2">ℎ</ci><ci id="S3.E8.m1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.2.3">𝑘</ci></apply><csymbol cd="latexml" id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\displaystyle h_{k}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E8.m2.1" class="ltx_Math" alttext="\displaystyle\;(1-z_{k})\odot h_{k-1}+z_{k}\odot\tilde{h}_{k}," display="inline"><semantics id="S3.E8.m2.1a"><mrow id="S3.E8.m2.1.1.1" xref="S3.E8.m2.1.1.1.1.cmml"><mrow id="S3.E8.m2.1.1.1.1" xref="S3.E8.m2.1.1.1.1.cmml"><mrow id="S3.E8.m2.1.1.1.1.1" xref="S3.E8.m2.1.1.1.1.1.cmml"><mrow id="S3.E8.m2.1.1.1.1.1.1.1" xref="S3.E8.m2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m2.1.1.1.1.1.1.1.2" xref="S3.E8.m2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E8.m2.1.1.1.1.1.1.1.1" xref="S3.E8.m2.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E8.m2.1.1.1.1.1.1.1.1.2" xref="S3.E8.m2.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E8.m2.1.1.1.1.1.1.1.1.1" xref="S3.E8.m2.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E8.m2.1.1.1.1.1.1.1.1.3" xref="S3.E8.m2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E8.m2.1.1.1.1.1.1.1.1.3.2" xref="S3.E8.m2.1.1.1.1.1.1.1.1.3.2.cmml">z</mi><mi id="S3.E8.m2.1.1.1.1.1.1.1.1.3.3" xref="S3.E8.m2.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.E8.m2.1.1.1.1.1.1.1.3" xref="S3.E8.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E8.m2.1.1.1.1.1.2" xref="S3.E8.m2.1.1.1.1.1.2.cmml">⊙</mo><msub id="S3.E8.m2.1.1.1.1.1.3" xref="S3.E8.m2.1.1.1.1.1.3.cmml"><mi id="S3.E8.m2.1.1.1.1.1.3.2" xref="S3.E8.m2.1.1.1.1.1.3.2.cmml">h</mi><mrow id="S3.E8.m2.1.1.1.1.1.3.3" xref="S3.E8.m2.1.1.1.1.1.3.3.cmml"><mi id="S3.E8.m2.1.1.1.1.1.3.3.2" xref="S3.E8.m2.1.1.1.1.1.3.3.2.cmml">k</mi><mo id="S3.E8.m2.1.1.1.1.1.3.3.1" xref="S3.E8.m2.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.E8.m2.1.1.1.1.1.3.3.3" xref="S3.E8.m2.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S3.E8.m2.1.1.1.1.2" xref="S3.E8.m2.1.1.1.1.2.cmml">+</mo><mrow id="S3.E8.m2.1.1.1.1.3" xref="S3.E8.m2.1.1.1.1.3.cmml"><msub id="S3.E8.m2.1.1.1.1.3.2" xref="S3.E8.m2.1.1.1.1.3.2.cmml"><mi id="S3.E8.m2.1.1.1.1.3.2.2" xref="S3.E8.m2.1.1.1.1.3.2.2.cmml">z</mi><mi id="S3.E8.m2.1.1.1.1.3.2.3" xref="S3.E8.m2.1.1.1.1.3.2.3.cmml">k</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E8.m2.1.1.1.1.3.1" xref="S3.E8.m2.1.1.1.1.3.1.cmml">⊙</mo><msub id="S3.E8.m2.1.1.1.1.3.3" xref="S3.E8.m2.1.1.1.1.3.3.cmml"><mover accent="true" id="S3.E8.m2.1.1.1.1.3.3.2" xref="S3.E8.m2.1.1.1.1.3.3.2.cmml"><mi id="S3.E8.m2.1.1.1.1.3.3.2.2" xref="S3.E8.m2.1.1.1.1.3.3.2.2.cmml">h</mi><mo id="S3.E8.m2.1.1.1.1.3.3.2.1" xref="S3.E8.m2.1.1.1.1.3.3.2.1.cmml">~</mo></mover><mi id="S3.E8.m2.1.1.1.1.3.3.3" xref="S3.E8.m2.1.1.1.1.3.3.3.cmml">k</mi></msub></mrow></mrow><mo id="S3.E8.m2.1.1.1.2" xref="S3.E8.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m2.1b"><apply id="S3.E8.m2.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1"><plus id="S3.E8.m2.1.1.1.1.2.cmml" xref="S3.E8.m2.1.1.1.1.2"></plus><apply id="S3.E8.m2.1.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E8.m2.1.1.1.1.1.2.cmml" xref="S3.E8.m2.1.1.1.1.1.2">direct-product</csymbol><apply id="S3.E8.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1.1.1.1.1"><minus id="S3.E8.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E8.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m2.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E8.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E8.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m2.1.1.1.1.1.1.1.1.3.2">𝑧</ci><ci id="S3.E8.m2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E8.m2.1.1.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply><apply id="S3.E8.m2.1.1.1.1.1.3.cmml" xref="S3.E8.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m2.1.1.1.1.1.3.1.cmml" xref="S3.E8.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E8.m2.1.1.1.1.1.3.2.cmml" xref="S3.E8.m2.1.1.1.1.1.3.2">ℎ</ci><apply id="S3.E8.m2.1.1.1.1.1.3.3.cmml" xref="S3.E8.m2.1.1.1.1.1.3.3"><minus id="S3.E8.m2.1.1.1.1.1.3.3.1.cmml" xref="S3.E8.m2.1.1.1.1.1.3.3.1"></minus><ci id="S3.E8.m2.1.1.1.1.1.3.3.2.cmml" xref="S3.E8.m2.1.1.1.1.1.3.3.2">𝑘</ci><cn type="integer" id="S3.E8.m2.1.1.1.1.1.3.3.3.cmml" xref="S3.E8.m2.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply><apply id="S3.E8.m2.1.1.1.1.3.cmml" xref="S3.E8.m2.1.1.1.1.3"><csymbol cd="latexml" id="S3.E8.m2.1.1.1.1.3.1.cmml" xref="S3.E8.m2.1.1.1.1.3.1">direct-product</csymbol><apply id="S3.E8.m2.1.1.1.1.3.2.cmml" xref="S3.E8.m2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E8.m2.1.1.1.1.3.2.1.cmml" xref="S3.E8.m2.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E8.m2.1.1.1.1.3.2.2.cmml" xref="S3.E8.m2.1.1.1.1.3.2.2">𝑧</ci><ci id="S3.E8.m2.1.1.1.1.3.2.3.cmml" xref="S3.E8.m2.1.1.1.1.3.2.3">𝑘</ci></apply><apply id="S3.E8.m2.1.1.1.1.3.3.cmml" xref="S3.E8.m2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E8.m2.1.1.1.1.3.3.1.cmml" xref="S3.E8.m2.1.1.1.1.3.3">subscript</csymbol><apply id="S3.E8.m2.1.1.1.1.3.3.2.cmml" xref="S3.E8.m2.1.1.1.1.3.3.2"><ci id="S3.E8.m2.1.1.1.1.3.3.2.1.cmml" xref="S3.E8.m2.1.1.1.1.3.3.2.1">~</ci><ci id="S3.E8.m2.1.1.1.1.3.3.2.2.cmml" xref="S3.E8.m2.1.1.1.1.3.3.2.2">ℎ</ci></apply><ci id="S3.E8.m2.1.1.1.1.3.3.3.cmml" xref="S3.E8.m2.1.1.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m2.1c">\displaystyle\;(1-z_{k})\odot h_{k-1}+z_{k}\odot\tilde{h}_{k},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.9" class="ltx_p">Here, <math id="S3.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="x_{k}" display="inline"><semantics id="S3.SS1.SSS2.p3.1.m1.1a"><msub id="S3.SS1.SSS2.p3.1.m1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.p3.1.m1.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.1b"><apply id="S3.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.1c">x_{k}</annotation></semantics></math> is a concatenation of several feature maps, including the correlation <math id="S3.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{C}_{k}" display="inline"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><msub id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p3.2.m2.1.1.2" xref="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml">𝐂</mi><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><apply id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.2">𝐂</ci><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">\mathbf{C}_{k}</annotation></semantics></math>, the current disparity <math id="S3.SS1.SSS2.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{d}_{k}" display="inline"><semantics id="S3.SS1.SSS2.p3.3.m3.1a"><msub id="S3.SS1.SSS2.p3.3.m3.1.1" xref="S3.SS1.SSS2.p3.3.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p3.3.m3.1.1.2" xref="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml">𝐝</mi><mi id="S3.SS1.SSS2.p3.3.m3.1.1.3" xref="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.3.m3.1b"><apply id="S3.SS1.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.2">𝐝</ci><ci id="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.3.m3.1c">\mathbf{d}_{k}</annotation></semantics></math>, and structural context features <math id="S3.SS1.SSS2.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{c}_{k}" display="inline"><semantics id="S3.SS1.SSS2.p3.4.m4.1a"><msub id="S3.SS1.SSS2.p3.4.m4.1.1" xref="S3.SS1.SSS2.p3.4.m4.1.1.cmml"><mi id="S3.SS1.SSS2.p3.4.m4.1.1.2" xref="S3.SS1.SSS2.p3.4.m4.1.1.2.cmml">𝐜</mi><mi id="S3.SS1.SSS2.p3.4.m4.1.1.3" xref="S3.SS1.SSS2.p3.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.4.m4.1b"><apply id="S3.SS1.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.4.m4.1.1.1.cmml" xref="S3.SS1.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.p3.4.m4.1.1.2">𝐜</ci><ci id="S3.SS1.SSS2.p3.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.p3.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.4.m4.1c">\mathbf{c}_{k}</annotation></semantics></math>, <math id="S3.SS1.SSS2.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{c}_{r}" display="inline"><semantics id="S3.SS1.SSS2.p3.5.m5.1a"><msub id="S3.SS1.SSS2.p3.5.m5.1.1" xref="S3.SS1.SSS2.p3.5.m5.1.1.cmml"><mi id="S3.SS1.SSS2.p3.5.m5.1.1.2" xref="S3.SS1.SSS2.p3.5.m5.1.1.2.cmml">𝐜</mi><mi id="S3.SS1.SSS2.p3.5.m5.1.1.3" xref="S3.SS1.SSS2.p3.5.m5.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.5.m5.1b"><apply id="S3.SS1.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.5.m5.1.1.1.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1.2">𝐜</ci><ci id="S3.SS1.SSS2.p3.5.m5.1.1.3.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.5.m5.1c">\mathbf{c}_{r}</annotation></semantics></math>, and <math id="S3.SS1.SSS2.p3.6.m6.1" class="ltx_Math" alttext="\mathbf{c}_{h}" display="inline"><semantics id="S3.SS1.SSS2.p3.6.m6.1a"><msub id="S3.SS1.SSS2.p3.6.m6.1.1" xref="S3.SS1.SSS2.p3.6.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p3.6.m6.1.1.2" xref="S3.SS1.SSS2.p3.6.m6.1.1.2.cmml">𝐜</mi><mi id="S3.SS1.SSS2.p3.6.m6.1.1.3" xref="S3.SS1.SSS2.p3.6.m6.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.6.m6.1b"><apply id="S3.SS1.SSS2.p3.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p3.6.m6.1.1.2">𝐜</ci><ci id="S3.SS1.SSS2.p3.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p3.6.m6.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.6.m6.1c">\mathbf{c}_{h}</annotation></semantics></math>. Specifically, <math id="S3.SS1.SSS2.p3.7.m7.1" class="ltx_Math" alttext="\mathbf{c}_{k}" display="inline"><semantics id="S3.SS1.SSS2.p3.7.m7.1a"><msub id="S3.SS1.SSS2.p3.7.m7.1.1" xref="S3.SS1.SSS2.p3.7.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p3.7.m7.1.1.2" xref="S3.SS1.SSS2.p3.7.m7.1.1.2.cmml">𝐜</mi><mi id="S3.SS1.SSS2.p3.7.m7.1.1.3" xref="S3.SS1.SSS2.p3.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.7.m7.1b"><apply id="S3.SS1.SSS2.p3.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.7.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p3.7.m7.1.1.2">𝐜</ci><ci id="S3.SS1.SSS2.p3.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p3.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.7.m7.1c">\mathbf{c}_{k}</annotation></semantics></math>, <math id="S3.SS1.SSS2.p3.8.m8.1" class="ltx_Math" alttext="\mathbf{c}_{r}" display="inline"><semantics id="S3.SS1.SSS2.p3.8.m8.1a"><msub id="S3.SS1.SSS2.p3.8.m8.1.1" xref="S3.SS1.SSS2.p3.8.m8.1.1.cmml"><mi id="S3.SS1.SSS2.p3.8.m8.1.1.2" xref="S3.SS1.SSS2.p3.8.m8.1.1.2.cmml">𝐜</mi><mi id="S3.SS1.SSS2.p3.8.m8.1.1.3" xref="S3.SS1.SSS2.p3.8.m8.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.8.m8.1b"><apply id="S3.SS1.SSS2.p3.8.m8.1.1.cmml" xref="S3.SS1.SSS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.8.m8.1.1.1.cmml" xref="S3.SS1.SSS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.8.m8.1.1.2.cmml" xref="S3.SS1.SSS2.p3.8.m8.1.1.2">𝐜</ci><ci id="S3.SS1.SSS2.p3.8.m8.1.1.3.cmml" xref="S3.SS1.SSS2.p3.8.m8.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.8.m8.1c">\mathbf{c}_{r}</annotation></semantics></math>, and <math id="S3.SS1.SSS2.p3.9.m9.1" class="ltx_Math" alttext="\mathbf{c}_{h}" display="inline"><semantics id="S3.SS1.SSS2.p3.9.m9.1a"><msub id="S3.SS1.SSS2.p3.9.m9.1.1" xref="S3.SS1.SSS2.p3.9.m9.1.1.cmml"><mi id="S3.SS1.SSS2.p3.9.m9.1.1.2" xref="S3.SS1.SSS2.p3.9.m9.1.1.2.cmml">𝐜</mi><mi id="S3.SS1.SSS2.p3.9.m9.1.1.3" xref="S3.SS1.SSS2.p3.9.m9.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.9.m9.1b"><apply id="S3.SS1.SSS2.p3.9.m9.1.1.cmml" xref="S3.SS1.SSS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.9.m9.1.1.1.cmml" xref="S3.SS1.SSS2.p3.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.9.m9.1.1.2.cmml" xref="S3.SS1.SSS2.p3.9.m9.1.1.2">𝐜</ci><ci id="S3.SS1.SSS2.p3.9.m9.1.1.3.cmml" xref="S3.SS1.SSS2.p3.9.m9.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.9.m9.1c">\mathbf{c}_{h}</annotation></semantics></math> represent structural features derived from the left image. These features are incorporated as residuals into the GRU loop, allowing for enhanced participation of structural information during the disparity map refinement process.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.3" class="ltx_p">Then, Our approach decode GRUs at each resolutions to obtain multi-scale disparity updates for coarse to fine gradual optimization:</p>
<table id="S5.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E9.m1.2" class="ltx_Math" alttext="\displaystyle\triangle\mathbf{d}_{k,\frac{1}{32}}" display="inline"><semantics id="S3.E9.m1.2a"><mrow id="S3.E9.m1.2.3" xref="S3.E9.m1.2.3.cmml"><mi mathvariant="normal" id="S3.E9.m1.2.3.2" xref="S3.E9.m1.2.3.2.cmml">△</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.3.1" xref="S3.E9.m1.2.3.1.cmml">​</mo><msub id="S3.E9.m1.2.3.3" xref="S3.E9.m1.2.3.3.cmml"><mi id="S3.E9.m1.2.3.3.2" xref="S3.E9.m1.2.3.3.2.cmml">𝐝</mi><mrow id="S3.E9.m1.2.2.2.4" xref="S3.E9.m1.2.2.2.3.cmml"><mi id="S3.E9.m1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.cmml">k</mi><mo id="S3.E9.m1.2.2.2.4.1" xref="S3.E9.m1.2.2.2.3.cmml">,</mo><mfrac id="S3.E9.m1.2.2.2.2" xref="S3.E9.m1.2.2.2.2.cmml"><mn id="S3.E9.m1.2.2.2.2.2" xref="S3.E9.m1.2.2.2.2.2.cmml">1</mn><mn id="S3.E9.m1.2.2.2.2.3" xref="S3.E9.m1.2.2.2.2.3.cmml">32</mn></mfrac></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.2b"><apply id="S3.E9.m1.2.3.cmml" xref="S3.E9.m1.2.3"><times id="S3.E9.m1.2.3.1.cmml" xref="S3.E9.m1.2.3.1"></times><ci id="S3.E9.m1.2.3.2.cmml" xref="S3.E9.m1.2.3.2">△</ci><apply id="S3.E9.m1.2.3.3.cmml" xref="S3.E9.m1.2.3.3"><csymbol cd="ambiguous" id="S3.E9.m1.2.3.3.1.cmml" xref="S3.E9.m1.2.3.3">subscript</csymbol><ci id="S3.E9.m1.2.3.3.2.cmml" xref="S3.E9.m1.2.3.3.2">𝐝</ci><list id="S3.E9.m1.2.2.2.3.cmml" xref="S3.E9.m1.2.2.2.4"><ci id="S3.E9.m1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1">𝑘</ci><apply id="S3.E9.m1.2.2.2.2.cmml" xref="S3.E9.m1.2.2.2.2"><divide id="S3.E9.m1.2.2.2.2.1.cmml" xref="S3.E9.m1.2.2.2.2"></divide><cn type="integer" id="S3.E9.m1.2.2.2.2.2.cmml" xref="S3.E9.m1.2.2.2.2.2">1</cn><cn type="integer" id="S3.E9.m1.2.2.2.2.3.cmml" xref="S3.E9.m1.2.2.2.2.3">32</cn></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.2c">\displaystyle\triangle\mathbf{d}_{k,\frac{1}{32}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E9.m2.3" class="ltx_Math" alttext="\displaystyle=\text{Decoder}(h_{k,\frac{1}{32}})," display="inline"><semantics id="S3.E9.m2.3a"><mrow id="S3.E9.m2.3.3.1" xref="S3.E9.m2.3.3.1.1.cmml"><mrow id="S3.E9.m2.3.3.1.1" xref="S3.E9.m2.3.3.1.1.cmml"><mi id="S3.E9.m2.3.3.1.1.3" xref="S3.E9.m2.3.3.1.1.3.cmml"></mi><mo id="S3.E9.m2.3.3.1.1.2" xref="S3.E9.m2.3.3.1.1.2.cmml">=</mo><mrow id="S3.E9.m2.3.3.1.1.1" xref="S3.E9.m2.3.3.1.1.1.cmml"><mtext id="S3.E9.m2.3.3.1.1.1.3" xref="S3.E9.m2.3.3.1.1.1.3a.cmml">Decoder</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m2.3.3.1.1.1.2" xref="S3.E9.m2.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m2.3.3.1.1.1.1.1" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E9.m2.3.3.1.1.1.1.1.2" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E9.m2.3.3.1.1.1.1.1.1" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E9.m2.3.3.1.1.1.1.1.1.2" xref="S3.E9.m2.3.3.1.1.1.1.1.1.2.cmml">h</mi><mrow id="S3.E9.m2.2.2.2.4" xref="S3.E9.m2.2.2.2.3.cmml"><mi id="S3.E9.m2.1.1.1.1" xref="S3.E9.m2.1.1.1.1.cmml">k</mi><mo id="S3.E9.m2.2.2.2.4.1" xref="S3.E9.m2.2.2.2.3.cmml">,</mo><mfrac id="S3.E9.m2.2.2.2.2" xref="S3.E9.m2.2.2.2.2.cmml"><mn id="S3.E9.m2.2.2.2.2.2" xref="S3.E9.m2.2.2.2.2.2.cmml">1</mn><mn id="S3.E9.m2.2.2.2.2.3" xref="S3.E9.m2.2.2.2.2.3.cmml">32</mn></mfrac></mrow></msub><mo stretchy="false" id="S3.E9.m2.3.3.1.1.1.1.1.3" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E9.m2.3.3.1.2" xref="S3.E9.m2.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m2.3b"><apply id="S3.E9.m2.3.3.1.1.cmml" xref="S3.E9.m2.3.3.1"><eq id="S3.E9.m2.3.3.1.1.2.cmml" xref="S3.E9.m2.3.3.1.1.2"></eq><csymbol cd="latexml" id="S3.E9.m2.3.3.1.1.3.cmml" xref="S3.E9.m2.3.3.1.1.3">absent</csymbol><apply id="S3.E9.m2.3.3.1.1.1.cmml" xref="S3.E9.m2.3.3.1.1.1"><times id="S3.E9.m2.3.3.1.1.1.2.cmml" xref="S3.E9.m2.3.3.1.1.1.2"></times><ci id="S3.E9.m2.3.3.1.1.1.3a.cmml" xref="S3.E9.m2.3.3.1.1.1.3"><mtext id="S3.E9.m2.3.3.1.1.1.3.cmml" xref="S3.E9.m2.3.3.1.1.1.3">Decoder</mtext></ci><apply id="S3.E9.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E9.m2.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1.1.2">ℎ</ci><list id="S3.E9.m2.2.2.2.3.cmml" xref="S3.E9.m2.2.2.2.4"><ci id="S3.E9.m2.1.1.1.1.cmml" xref="S3.E9.m2.1.1.1.1">𝑘</ci><apply id="S3.E9.m2.2.2.2.2.cmml" xref="S3.E9.m2.2.2.2.2"><divide id="S3.E9.m2.2.2.2.2.1.cmml" xref="S3.E9.m2.2.2.2.2"></divide><cn type="integer" id="S3.E9.m2.2.2.2.2.2.cmml" xref="S3.E9.m2.2.2.2.2.2">1</cn><cn type="integer" id="S3.E9.m2.2.2.2.2.3.cmml" xref="S3.E9.m2.2.2.2.2.3">32</cn></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m2.3c">\displaystyle=\text{Decoder}(h_{k,\frac{1}{32}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E10.m1.2" class="ltx_Math" alttext="\displaystyle\triangle\mathbf{d}_{k,\frac{1}{16}}" display="inline"><semantics id="S3.E10.m1.2a"><mrow id="S3.E10.m1.2.3" xref="S3.E10.m1.2.3.cmml"><mi mathvariant="normal" id="S3.E10.m1.2.3.2" xref="S3.E10.m1.2.3.2.cmml">△</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.2.3.1" xref="S3.E10.m1.2.3.1.cmml">​</mo><msub id="S3.E10.m1.2.3.3" xref="S3.E10.m1.2.3.3.cmml"><mi id="S3.E10.m1.2.3.3.2" xref="S3.E10.m1.2.3.3.2.cmml">𝐝</mi><mrow id="S3.E10.m1.2.2.2.4" xref="S3.E10.m1.2.2.2.3.cmml"><mi id="S3.E10.m1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.cmml">k</mi><mo id="S3.E10.m1.2.2.2.4.1" xref="S3.E10.m1.2.2.2.3.cmml">,</mo><mfrac id="S3.E10.m1.2.2.2.2" xref="S3.E10.m1.2.2.2.2.cmml"><mn id="S3.E10.m1.2.2.2.2.2" xref="S3.E10.m1.2.2.2.2.2.cmml">1</mn><mn id="S3.E10.m1.2.2.2.2.3" xref="S3.E10.m1.2.2.2.2.3.cmml">16</mn></mfrac></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.2b"><apply id="S3.E10.m1.2.3.cmml" xref="S3.E10.m1.2.3"><times id="S3.E10.m1.2.3.1.cmml" xref="S3.E10.m1.2.3.1"></times><ci id="S3.E10.m1.2.3.2.cmml" xref="S3.E10.m1.2.3.2">△</ci><apply id="S3.E10.m1.2.3.3.cmml" xref="S3.E10.m1.2.3.3"><csymbol cd="ambiguous" id="S3.E10.m1.2.3.3.1.cmml" xref="S3.E10.m1.2.3.3">subscript</csymbol><ci id="S3.E10.m1.2.3.3.2.cmml" xref="S3.E10.m1.2.3.3.2">𝐝</ci><list id="S3.E10.m1.2.2.2.3.cmml" xref="S3.E10.m1.2.2.2.4"><ci id="S3.E10.m1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1">𝑘</ci><apply id="S3.E10.m1.2.2.2.2.cmml" xref="S3.E10.m1.2.2.2.2"><divide id="S3.E10.m1.2.2.2.2.1.cmml" xref="S3.E10.m1.2.2.2.2"></divide><cn type="integer" id="S3.E10.m1.2.2.2.2.2.cmml" xref="S3.E10.m1.2.2.2.2.2">1</cn><cn type="integer" id="S3.E10.m1.2.2.2.2.3.cmml" xref="S3.E10.m1.2.2.2.2.3">16</cn></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.2c">\displaystyle\triangle\mathbf{d}_{k,\frac{1}{16}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E10.m2.5" class="ltx_Math" alttext="\displaystyle=\text{Decoder}(h_{k,\frac{1}{16}}+\text{Interp}(\triangle\mathbf{d}_{k,\frac{1}{32}}))," display="inline"><semantics id="S3.E10.m2.5a"><mrow id="S3.E10.m2.5.5.1" xref="S3.E10.m2.5.5.1.1.cmml"><mrow id="S3.E10.m2.5.5.1.1" xref="S3.E10.m2.5.5.1.1.cmml"><mi id="S3.E10.m2.5.5.1.1.3" xref="S3.E10.m2.5.5.1.1.3.cmml"></mi><mo id="S3.E10.m2.5.5.1.1.2" xref="S3.E10.m2.5.5.1.1.2.cmml">=</mo><mrow id="S3.E10.m2.5.5.1.1.1" xref="S3.E10.m2.5.5.1.1.1.cmml"><mtext id="S3.E10.m2.5.5.1.1.1.3" xref="S3.E10.m2.5.5.1.1.1.3a.cmml">Decoder</mtext><mo lspace="0em" rspace="0em" id="S3.E10.m2.5.5.1.1.1.2" xref="S3.E10.m2.5.5.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m2.5.5.1.1.1.1.1" xref="S3.E10.m2.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E10.m2.5.5.1.1.1.1.1.2" xref="S3.E10.m2.5.5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E10.m2.5.5.1.1.1.1.1.1" xref="S3.E10.m2.5.5.1.1.1.1.1.1.cmml"><msub id="S3.E10.m2.5.5.1.1.1.1.1.1.3" xref="S3.E10.m2.5.5.1.1.1.1.1.1.3.cmml"><mi id="S3.E10.m2.5.5.1.1.1.1.1.1.3.2" xref="S3.E10.m2.5.5.1.1.1.1.1.1.3.2.cmml">h</mi><mrow id="S3.E10.m2.2.2.2.4" xref="S3.E10.m2.2.2.2.3.cmml"><mi id="S3.E10.m2.1.1.1.1" xref="S3.E10.m2.1.1.1.1.cmml">k</mi><mo id="S3.E10.m2.2.2.2.4.1" xref="S3.E10.m2.2.2.2.3.cmml">,</mo><mfrac id="S3.E10.m2.2.2.2.2" xref="S3.E10.m2.2.2.2.2.cmml"><mn id="S3.E10.m2.2.2.2.2.2" xref="S3.E10.m2.2.2.2.2.2.cmml">1</mn><mn id="S3.E10.m2.2.2.2.2.3" xref="S3.E10.m2.2.2.2.2.3.cmml">16</mn></mfrac></mrow></msub><mo id="S3.E10.m2.5.5.1.1.1.1.1.1.2" xref="S3.E10.m2.5.5.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E10.m2.5.5.1.1.1.1.1.1.1" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.cmml"><mtext id="S3.E10.m2.5.5.1.1.1.1.1.1.1.3" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.3a.cmml">Interp</mtext><mo lspace="0em" rspace="0em" id="S3.E10.m2.5.5.1.1.1.1.1.1.1.2" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.2" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml">△</mi><mo lspace="0em" rspace="0em" id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2.cmml">𝐝</mi><mrow id="S3.E10.m2.4.4.2.4" xref="S3.E10.m2.4.4.2.3.cmml"><mi id="S3.E10.m2.3.3.1.1" xref="S3.E10.m2.3.3.1.1.cmml">k</mi><mo id="S3.E10.m2.4.4.2.4.1" xref="S3.E10.m2.4.4.2.3.cmml">,</mo><mfrac id="S3.E10.m2.4.4.2.2" xref="S3.E10.m2.4.4.2.2.cmml"><mn id="S3.E10.m2.4.4.2.2.2" xref="S3.E10.m2.4.4.2.2.2.cmml">1</mn><mn id="S3.E10.m2.4.4.2.2.3" xref="S3.E10.m2.4.4.2.2.3.cmml">32</mn></mfrac></mrow></msub></mrow><mo stretchy="false" id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.3" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E10.m2.5.5.1.1.1.1.1.3" xref="S3.E10.m2.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E10.m2.5.5.1.2" xref="S3.E10.m2.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m2.5b"><apply id="S3.E10.m2.5.5.1.1.cmml" xref="S3.E10.m2.5.5.1"><eq id="S3.E10.m2.5.5.1.1.2.cmml" xref="S3.E10.m2.5.5.1.1.2"></eq><csymbol cd="latexml" id="S3.E10.m2.5.5.1.1.3.cmml" xref="S3.E10.m2.5.5.1.1.3">absent</csymbol><apply id="S3.E10.m2.5.5.1.1.1.cmml" xref="S3.E10.m2.5.5.1.1.1"><times id="S3.E10.m2.5.5.1.1.1.2.cmml" xref="S3.E10.m2.5.5.1.1.1.2"></times><ci id="S3.E10.m2.5.5.1.1.1.3a.cmml" xref="S3.E10.m2.5.5.1.1.1.3"><mtext id="S3.E10.m2.5.5.1.1.1.3.cmml" xref="S3.E10.m2.5.5.1.1.1.3">Decoder</mtext></ci><apply id="S3.E10.m2.5.5.1.1.1.1.1.1.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1"><plus id="S3.E10.m2.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.2"></plus><apply id="S3.E10.m2.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m2.5.5.1.1.1.1.1.1.3.1.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E10.m2.5.5.1.1.1.1.1.1.3.2.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.3.2">ℎ</ci><list id="S3.E10.m2.2.2.2.3.cmml" xref="S3.E10.m2.2.2.2.4"><ci id="S3.E10.m2.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1">𝑘</ci><apply id="S3.E10.m2.2.2.2.2.cmml" xref="S3.E10.m2.2.2.2.2"><divide id="S3.E10.m2.2.2.2.2.1.cmml" xref="S3.E10.m2.2.2.2.2"></divide><cn type="integer" id="S3.E10.m2.2.2.2.2.2.cmml" xref="S3.E10.m2.2.2.2.2.2">1</cn><cn type="integer" id="S3.E10.m2.2.2.2.2.3.cmml" xref="S3.E10.m2.2.2.2.2.3">16</cn></apply></list></apply><apply id="S3.E10.m2.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1"><times id="S3.E10.m2.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.2"></times><ci id="S3.E10.m2.5.5.1.1.1.1.1.1.1.3a.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.3"><mtext id="S3.E10.m2.5.5.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.3">Interp</mtext></ci><apply id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1"><times id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.2">△</ci><apply id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E10.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2">𝐝</ci><list id="S3.E10.m2.4.4.2.3.cmml" xref="S3.E10.m2.4.4.2.4"><ci id="S3.E10.m2.3.3.1.1.cmml" xref="S3.E10.m2.3.3.1.1">𝑘</ci><apply id="S3.E10.m2.4.4.2.2.cmml" xref="S3.E10.m2.4.4.2.2"><divide id="S3.E10.m2.4.4.2.2.1.cmml" xref="S3.E10.m2.4.4.2.2"></divide><cn type="integer" id="S3.E10.m2.4.4.2.2.2.cmml" xref="S3.E10.m2.4.4.2.2.2">1</cn><cn type="integer" id="S3.E10.m2.4.4.2.2.3.cmml" xref="S3.E10.m2.4.4.2.2.3">32</cn></apply></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m2.5c">\displaystyle=\text{Decoder}(h_{k,\frac{1}{16}}+\text{Interp}(\triangle\mathbf{d}_{k,\frac{1}{32}})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
<tbody id="S3.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E11.m1.2" class="ltx_Math" alttext="\displaystyle\triangle\mathbf{d}_{k,\frac{1}{8}}" display="inline"><semantics id="S3.E11.m1.2a"><mrow id="S3.E11.m1.2.3" xref="S3.E11.m1.2.3.cmml"><mi mathvariant="normal" id="S3.E11.m1.2.3.2" xref="S3.E11.m1.2.3.2.cmml">△</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.2.3.1" xref="S3.E11.m1.2.3.1.cmml">​</mo><msub id="S3.E11.m1.2.3.3" xref="S3.E11.m1.2.3.3.cmml"><mi id="S3.E11.m1.2.3.3.2" xref="S3.E11.m1.2.3.3.2.cmml">𝐝</mi><mrow id="S3.E11.m1.2.2.2.4" xref="S3.E11.m1.2.2.2.3.cmml"><mi id="S3.E11.m1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.cmml">k</mi><mo id="S3.E11.m1.2.2.2.4.1" xref="S3.E11.m1.2.2.2.3.cmml">,</mo><mfrac id="S3.E11.m1.2.2.2.2" xref="S3.E11.m1.2.2.2.2.cmml"><mn id="S3.E11.m1.2.2.2.2.2" xref="S3.E11.m1.2.2.2.2.2.cmml">1</mn><mn id="S3.E11.m1.2.2.2.2.3" xref="S3.E11.m1.2.2.2.2.3.cmml">8</mn></mfrac></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E11.m1.2b"><apply id="S3.E11.m1.2.3.cmml" xref="S3.E11.m1.2.3"><times id="S3.E11.m1.2.3.1.cmml" xref="S3.E11.m1.2.3.1"></times><ci id="S3.E11.m1.2.3.2.cmml" xref="S3.E11.m1.2.3.2">△</ci><apply id="S3.E11.m1.2.3.3.cmml" xref="S3.E11.m1.2.3.3"><csymbol cd="ambiguous" id="S3.E11.m1.2.3.3.1.cmml" xref="S3.E11.m1.2.3.3">subscript</csymbol><ci id="S3.E11.m1.2.3.3.2.cmml" xref="S3.E11.m1.2.3.3.2">𝐝</ci><list id="S3.E11.m1.2.2.2.3.cmml" xref="S3.E11.m1.2.2.2.4"><ci id="S3.E11.m1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1">𝑘</ci><apply id="S3.E11.m1.2.2.2.2.cmml" xref="S3.E11.m1.2.2.2.2"><divide id="S3.E11.m1.2.2.2.2.1.cmml" xref="S3.E11.m1.2.2.2.2"></divide><cn type="integer" id="S3.E11.m1.2.2.2.2.2.cmml" xref="S3.E11.m1.2.2.2.2.2">1</cn><cn type="integer" id="S3.E11.m1.2.2.2.2.3.cmml" xref="S3.E11.m1.2.2.2.2.3">8</cn></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E11.m1.2c">\displaystyle\triangle\mathbf{d}_{k,\frac{1}{8}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E11.m2.5" class="ltx_Math" alttext="\displaystyle=\text{Decoder}(h_{k,\frac{1}{8}}+\text{Interp}(\triangle\mathbf{d}_{k,\frac{1}{16}}))," display="inline"><semantics id="S3.E11.m2.5a"><mrow id="S3.E11.m2.5.5.1" xref="S3.E11.m2.5.5.1.1.cmml"><mrow id="S3.E11.m2.5.5.1.1" xref="S3.E11.m2.5.5.1.1.cmml"><mi id="S3.E11.m2.5.5.1.1.3" xref="S3.E11.m2.5.5.1.1.3.cmml"></mi><mo id="S3.E11.m2.5.5.1.1.2" xref="S3.E11.m2.5.5.1.1.2.cmml">=</mo><mrow id="S3.E11.m2.5.5.1.1.1" xref="S3.E11.m2.5.5.1.1.1.cmml"><mtext id="S3.E11.m2.5.5.1.1.1.3" xref="S3.E11.m2.5.5.1.1.1.3a.cmml">Decoder</mtext><mo lspace="0em" rspace="0em" id="S3.E11.m2.5.5.1.1.1.2" xref="S3.E11.m2.5.5.1.1.1.2.cmml">​</mo><mrow id="S3.E11.m2.5.5.1.1.1.1.1" xref="S3.E11.m2.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E11.m2.5.5.1.1.1.1.1.2" xref="S3.E11.m2.5.5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E11.m2.5.5.1.1.1.1.1.1" xref="S3.E11.m2.5.5.1.1.1.1.1.1.cmml"><msub id="S3.E11.m2.5.5.1.1.1.1.1.1.3" xref="S3.E11.m2.5.5.1.1.1.1.1.1.3.cmml"><mi id="S3.E11.m2.5.5.1.1.1.1.1.1.3.2" xref="S3.E11.m2.5.5.1.1.1.1.1.1.3.2.cmml">h</mi><mrow id="S3.E11.m2.2.2.2.4" xref="S3.E11.m2.2.2.2.3.cmml"><mi id="S3.E11.m2.1.1.1.1" xref="S3.E11.m2.1.1.1.1.cmml">k</mi><mo id="S3.E11.m2.2.2.2.4.1" xref="S3.E11.m2.2.2.2.3.cmml">,</mo><mfrac id="S3.E11.m2.2.2.2.2" xref="S3.E11.m2.2.2.2.2.cmml"><mn id="S3.E11.m2.2.2.2.2.2" xref="S3.E11.m2.2.2.2.2.2.cmml">1</mn><mn id="S3.E11.m2.2.2.2.2.3" xref="S3.E11.m2.2.2.2.2.3.cmml">8</mn></mfrac></mrow></msub><mo id="S3.E11.m2.5.5.1.1.1.1.1.1.2" xref="S3.E11.m2.5.5.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E11.m2.5.5.1.1.1.1.1.1.1" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.cmml"><mtext id="S3.E11.m2.5.5.1.1.1.1.1.1.1.3" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.3a.cmml">Interp</mtext><mo lspace="0em" rspace="0em" id="S3.E11.m2.5.5.1.1.1.1.1.1.1.2" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.2" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml">△</mi><mo lspace="0em" rspace="0em" id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2.cmml">𝐝</mi><mrow id="S3.E11.m2.4.4.2.4" xref="S3.E11.m2.4.4.2.3.cmml"><mi id="S3.E11.m2.3.3.1.1" xref="S3.E11.m2.3.3.1.1.cmml">k</mi><mo id="S3.E11.m2.4.4.2.4.1" xref="S3.E11.m2.4.4.2.3.cmml">,</mo><mfrac id="S3.E11.m2.4.4.2.2" xref="S3.E11.m2.4.4.2.2.cmml"><mn id="S3.E11.m2.4.4.2.2.2" xref="S3.E11.m2.4.4.2.2.2.cmml">1</mn><mn id="S3.E11.m2.4.4.2.2.3" xref="S3.E11.m2.4.4.2.2.3.cmml">16</mn></mfrac></mrow></msub></mrow><mo stretchy="false" id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.3" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E11.m2.5.5.1.1.1.1.1.3" xref="S3.E11.m2.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E11.m2.5.5.1.2" xref="S3.E11.m2.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E11.m2.5b"><apply id="S3.E11.m2.5.5.1.1.cmml" xref="S3.E11.m2.5.5.1"><eq id="S3.E11.m2.5.5.1.1.2.cmml" xref="S3.E11.m2.5.5.1.1.2"></eq><csymbol cd="latexml" id="S3.E11.m2.5.5.1.1.3.cmml" xref="S3.E11.m2.5.5.1.1.3">absent</csymbol><apply id="S3.E11.m2.5.5.1.1.1.cmml" xref="S3.E11.m2.5.5.1.1.1"><times id="S3.E11.m2.5.5.1.1.1.2.cmml" xref="S3.E11.m2.5.5.1.1.1.2"></times><ci id="S3.E11.m2.5.5.1.1.1.3a.cmml" xref="S3.E11.m2.5.5.1.1.1.3"><mtext id="S3.E11.m2.5.5.1.1.1.3.cmml" xref="S3.E11.m2.5.5.1.1.1.3">Decoder</mtext></ci><apply id="S3.E11.m2.5.5.1.1.1.1.1.1.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1"><plus id="S3.E11.m2.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.2"></plus><apply id="S3.E11.m2.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E11.m2.5.5.1.1.1.1.1.1.3.1.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E11.m2.5.5.1.1.1.1.1.1.3.2.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.3.2">ℎ</ci><list id="S3.E11.m2.2.2.2.3.cmml" xref="S3.E11.m2.2.2.2.4"><ci id="S3.E11.m2.1.1.1.1.cmml" xref="S3.E11.m2.1.1.1.1">𝑘</ci><apply id="S3.E11.m2.2.2.2.2.cmml" xref="S3.E11.m2.2.2.2.2"><divide id="S3.E11.m2.2.2.2.2.1.cmml" xref="S3.E11.m2.2.2.2.2"></divide><cn type="integer" id="S3.E11.m2.2.2.2.2.2.cmml" xref="S3.E11.m2.2.2.2.2.2">1</cn><cn type="integer" id="S3.E11.m2.2.2.2.2.3.cmml" xref="S3.E11.m2.2.2.2.2.3">8</cn></apply></list></apply><apply id="S3.E11.m2.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1"><times id="S3.E11.m2.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.2"></times><ci id="S3.E11.m2.5.5.1.1.1.1.1.1.1.3a.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.3"><mtext id="S3.E11.m2.5.5.1.1.1.1.1.1.1.3.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.3">Interp</mtext></ci><apply id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1"><times id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.2">△</ci><apply id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E11.m2.5.5.1.1.1.1.1.1.1.1.1.1.3.2">𝐝</ci><list id="S3.E11.m2.4.4.2.3.cmml" xref="S3.E11.m2.4.4.2.4"><ci id="S3.E11.m2.3.3.1.1.cmml" xref="S3.E11.m2.3.3.1.1">𝑘</ci><apply id="S3.E11.m2.4.4.2.2.cmml" xref="S3.E11.m2.4.4.2.2"><divide id="S3.E11.m2.4.4.2.2.1.cmml" xref="S3.E11.m2.4.4.2.2"></divide><cn type="integer" id="S3.E11.m2.4.4.2.2.2.cmml" xref="S3.E11.m2.4.4.2.2.2">1</cn><cn type="integer" id="S3.E11.m2.4.4.2.2.3.cmml" xref="S3.E11.m2.4.4.2.2.3">16</cn></apply></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E11.m2.5c">\displaystyle=\text{Decoder}(h_{k,\frac{1}{8}}+\text{Interp}(\triangle\mathbf{d}_{k,\frac{1}{16}})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS2.p4.2" class="ltx_p">where <span id="S3.SS1.SSS2.p4.2.1" class="ltx_text ltx_markedasmath">Decoder</span> consist of two convolutional layers and <span id="S3.SS1.SSS2.p4.2.2" class="ltx_text ltx_markedasmath">Interp</span> is bilinear interpolation scaled up by a factor of two.
Finally, the updated disparity is calculated as:</p>
<table id="S3.E12" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E12X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E12X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{d}_{k+1}=" display="inline"><semantics id="S3.E12X.2.1.1.m1.1a"><mrow id="S3.E12X.2.1.1.m1.1.1" xref="S3.E12X.2.1.1.m1.1.1.cmml"><msub id="S3.E12X.2.1.1.m1.1.1.2" xref="S3.E12X.2.1.1.m1.1.1.2.cmml"><mi id="S3.E12X.2.1.1.m1.1.1.2.2" xref="S3.E12X.2.1.1.m1.1.1.2.2.cmml">𝐝</mi><mrow id="S3.E12X.2.1.1.m1.1.1.2.3" xref="S3.E12X.2.1.1.m1.1.1.2.3.cmml"><mi id="S3.E12X.2.1.1.m1.1.1.2.3.2" xref="S3.E12X.2.1.1.m1.1.1.2.3.2.cmml">k</mi><mo id="S3.E12X.2.1.1.m1.1.1.2.3.1" xref="S3.E12X.2.1.1.m1.1.1.2.3.1.cmml">+</mo><mn id="S3.E12X.2.1.1.m1.1.1.2.3.3" xref="S3.E12X.2.1.1.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.E12X.2.1.1.m1.1.1.1" xref="S3.E12X.2.1.1.m1.1.1.1.cmml">=</mo><mi id="S3.E12X.2.1.1.m1.1.1.3" xref="S3.E12X.2.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E12X.2.1.1.m1.1b"><apply id="S3.E12X.2.1.1.m1.1.1.cmml" xref="S3.E12X.2.1.1.m1.1.1"><eq id="S3.E12X.2.1.1.m1.1.1.1.cmml" xref="S3.E12X.2.1.1.m1.1.1.1"></eq><apply id="S3.E12X.2.1.1.m1.1.1.2.cmml" xref="S3.E12X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E12X.2.1.1.m1.1.1.2.1.cmml" xref="S3.E12X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S3.E12X.2.1.1.m1.1.1.2.2.cmml" xref="S3.E12X.2.1.1.m1.1.1.2.2">𝐝</ci><apply id="S3.E12X.2.1.1.m1.1.1.2.3.cmml" xref="S3.E12X.2.1.1.m1.1.1.2.3"><plus id="S3.E12X.2.1.1.m1.1.1.2.3.1.cmml" xref="S3.E12X.2.1.1.m1.1.1.2.3.1"></plus><ci id="S3.E12X.2.1.1.m1.1.1.2.3.2.cmml" xref="S3.E12X.2.1.1.m1.1.1.2.3.2">𝑘</ci><cn type="integer" id="S3.E12X.2.1.1.m1.1.1.2.3.3.cmml" xref="S3.E12X.2.1.1.m1.1.1.2.3.3">1</cn></apply></apply><csymbol cd="latexml" id="S3.E12X.2.1.1.m1.1.1.3.cmml" xref="S3.E12X.2.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E12X.2.1.1.m1.1c">\displaystyle\mathbf{d}_{k+1}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E12X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle\;\mathbf{d}_{k}+\triangle\mathbf{d}_{k}" display="inline"><semantics id="S3.E12X.3.2.2.m1.1a"><mrow id="S3.E12X.3.2.2.m1.1.1" xref="S3.E12X.3.2.2.m1.1.1.cmml"><msub id="S3.E12X.3.2.2.m1.1.1.2" xref="S3.E12X.3.2.2.m1.1.1.2.cmml"><mi id="S3.E12X.3.2.2.m1.1.1.2.2" xref="S3.E12X.3.2.2.m1.1.1.2.2.cmml">𝐝</mi><mi id="S3.E12X.3.2.2.m1.1.1.2.3" xref="S3.E12X.3.2.2.m1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E12X.3.2.2.m1.1.1.1" xref="S3.E12X.3.2.2.m1.1.1.1.cmml">+</mo><mrow id="S3.E12X.3.2.2.m1.1.1.3" xref="S3.E12X.3.2.2.m1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E12X.3.2.2.m1.1.1.3.2" xref="S3.E12X.3.2.2.m1.1.1.3.2.cmml">△</mi><mo lspace="0em" rspace="0em" id="S3.E12X.3.2.2.m1.1.1.3.1" xref="S3.E12X.3.2.2.m1.1.1.3.1.cmml">​</mo><msub id="S3.E12X.3.2.2.m1.1.1.3.3" xref="S3.E12X.3.2.2.m1.1.1.3.3.cmml"><mi id="S3.E12X.3.2.2.m1.1.1.3.3.2" xref="S3.E12X.3.2.2.m1.1.1.3.3.2.cmml">𝐝</mi><mi id="S3.E12X.3.2.2.m1.1.1.3.3.3" xref="S3.E12X.3.2.2.m1.1.1.3.3.3.cmml">k</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E12X.3.2.2.m1.1b"><apply id="S3.E12X.3.2.2.m1.1.1.cmml" xref="S3.E12X.3.2.2.m1.1.1"><plus id="S3.E12X.3.2.2.m1.1.1.1.cmml" xref="S3.E12X.3.2.2.m1.1.1.1"></plus><apply id="S3.E12X.3.2.2.m1.1.1.2.cmml" xref="S3.E12X.3.2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E12X.3.2.2.m1.1.1.2.1.cmml" xref="S3.E12X.3.2.2.m1.1.1.2">subscript</csymbol><ci id="S3.E12X.3.2.2.m1.1.1.2.2.cmml" xref="S3.E12X.3.2.2.m1.1.1.2.2">𝐝</ci><ci id="S3.E12X.3.2.2.m1.1.1.2.3.cmml" xref="S3.E12X.3.2.2.m1.1.1.2.3">𝑘</ci></apply><apply id="S3.E12X.3.2.2.m1.1.1.3.cmml" xref="S3.E12X.3.2.2.m1.1.1.3"><times id="S3.E12X.3.2.2.m1.1.1.3.1.cmml" xref="S3.E12X.3.2.2.m1.1.1.3.1"></times><ci id="S3.E12X.3.2.2.m1.1.1.3.2.cmml" xref="S3.E12X.3.2.2.m1.1.1.3.2">△</ci><apply id="S3.E12X.3.2.2.m1.1.1.3.3.cmml" xref="S3.E12X.3.2.2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E12X.3.2.2.m1.1.1.3.3.1.cmml" xref="S3.E12X.3.2.2.m1.1.1.3.3">subscript</csymbol><ci id="S3.E12X.3.2.2.m1.1.1.3.3.2.cmml" xref="S3.E12X.3.2.2.m1.1.1.3.3.2">𝐝</ci><ci id="S3.E12X.3.2.2.m1.1.1.3.3.3.cmml" xref="S3.E12X.3.2.2.m1.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E12X.3.2.2.m1.1c">\displaystyle\;\mathbf{d}_{k}+\triangle\mathbf{d}_{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(12)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p">In summary, to address the challenges of transparent objects, we selected an appropriate image feature extractor. Additionally, considering the unique difficulties of transparent objects and the need for lightweight models in robotics, we designed a structural feature post-fusion architecture. Every detail of our network structure is tailored to the characteristics of transparent object scenarios.</p>
</div>
<div id="S3.SS1.SSS2.p6" class="ltx_para">
<p id="S3.SS1.SSS2.p6.1" class="ltx_p">In the comparative experiments section, the visual results demonstrate that our model significantly enhances the stereo imaging of transparent objects.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Synthetic Dataset Generation</span>
</h3>

<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.08926/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">
The left image displays rendered models, with the first two rows featuring transparent containers such as vases, bowls, and cups, while the last two rows showcase cosmetic products made of transparent glass and plastic. The right image shows the real objects used for Sim2Real testing.
</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2409.08926/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="442" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">
Sample images from our SynClearDepth dataset are presented, showcasing randomly placed transparent objects in various indoor scenes (bathroom, dining room, kitchen, living room) under different lighting conditions. The objects include common transparent items like cosmetic packaging, glass containers, etc.
</span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">To enhance dataset generation efficiency, we utilized OptiX’s AI denoiser and deep learning super-resolution techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, notably reducing the time from an average of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="12.77" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">12.77</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="float" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">12.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">12.77</annotation></semantics></math> seconds to <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="4.40" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">4.40</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="float" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">4.40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">4.40</annotation></semantics></math> seconds per set (including stereo RGB, depth, masks, and object-camera poses), significantly speeding up the process by cutting nearly two-thirds data generation time while maintaining high-quality outputs. Our dataset generation process is illustrated in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A1 Cascaded Vision Transformer Backbone. ‣ III-A Network Overview ‣ III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Our proposed dataset SynClearDepth comprises 16 selected objects, including 10 common transparent containers in daily life and 6 products with glass material, as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Synthetic Dataset Generation ‣ III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. To ensure that the background of the generated dataset also contains depth value and avoid ambiguities in depth recovery training due to lack of dataset labeling of background part, we collected indoor scenes in combination with the object models, encompassing 6 bathrooms, 3 dining rooms, 5 kitchens, and 6 living rooms. Through this combination, we generated 14,091 image sets, each containing left and right RGB images with size 1280*720, ground truth depth, instance segmentation maps, as well as object and camera poses, as illustrated in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-B Synthetic Dataset Generation ‣ III Method ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In the dataset generation process, we applied domain randomization to object types, quantities, positions and poses, lighting conditions, and camera shooting angles.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">metrics</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">AvgErr (Average Error):</span> Represents the average disparity error across all pixels, indicating the general accuracy of the disparity map.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">RMS (Root Mean Square Error):</span> Measures the square root of the average squared disparity error, reflecting the overall deviation from the ground truth.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Bad 0.5 (%), Bad 1.0 (%), Bad 2.0 (%), Bad 4.0 (%):</span> These metrics indicate the percentage of pixels where the disparity error exceeds 0.5, 1.0, 2.0, and 4.0 pixels, respectively, highlighting the proportion of significant errors in the disparity map.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Together, these metrics provide a comprehensive assessment of stereo matching performance, balancing both overall accuracy and the frequency of large errors.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Technical Specifications</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our network is firstly pre-trained on CREStereo dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and Scene Flow dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and then fine-tuned on our proposed SynClearDepth dataset for transparent object stereo imaging. Our model is trained on 1 block of NVIDIA RTX A6000 with batch size 8 and the whole training lasts for 300,000 steps. We use AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> as optimizer, the learning rate is set to 0.0002, updated with a warm-up mechanism and used one-cycle learning rate scheduler. The final learning rate when training finished is 0.0001. The input size of the model is resized to <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="360\times 720" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">360</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">720</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">360</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">720</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">360\times 720</annotation></semantics></math>. Fine-tune for transparent objects takes the same training parameters as pretraining on the opaque dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Qualitative and Quantitative Studies for Stereo Depth Estimation</span>
</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.5.1.1" class="ltx_text">IV-B</span>1 </span>Quantitative Analysis on Middlebury Dataset</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">The Middlebury 2014 dataset comprises 23 pairs of images designate for training and validation purposes. We refine our model over these 23 pairs, conducting fine-tuning across 4,000 iterations with an image resolution of <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="384\times 1024" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><times id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">384</cn><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">384\times 1024</annotation></semantics></math>. Benchmark against standard baseline approaches RAFT-Stereo and CREStereo using various stereo evaluation metrics further underscores the efficacy of our approach, as outlined in Tab. <a href="#S4.T1" title="TABLE I ‣ IV-B1 Quantitative Analysis on Middlebury Dataset ‣ IV-B Qualitative and Quantitative Studies for Stereo Depth Estimation ‣ IV Experiments ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. More comparison results with other methods can be found at <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:70.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.0pt,1.4pt) scale(0.960160417093092,0.960160417093092) ;">
<table id="S4.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Methods</th>
<th id="S4.T1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">AvgErr</th>
<th id="S4.T1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">RMS</th>
<th id="S4.T1.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 0.5 (%)</th>
<th id="S4.T1.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 1.0 (%)</th>
<th id="S4.T1.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 2.0 (%)</th>
<th id="S4.T1.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 4.0 (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.1.2.1" class="ltx_tr">
<th id="S4.T1.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">RAFT-Stereo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T1.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1.27</td>
<td id="S4.T1.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">8.41</td>
<td id="S4.T1.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">27.7</td>
<td id="S4.T1.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">9.37</td>
<td id="S4.T1.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">4.14</td>
<td id="S4.T1.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2.75</td>
</tr>
<tr id="S4.T1.2.1.3.2" class="ltx_tr">
<th id="S4.T1.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">CREStereo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</th>
<td id="S4.T1.2.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.3.2.2.1" class="ltx_text ltx_font_bold">1.15</span></td>
<td id="S4.T1.2.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.3.2.3.1" class="ltx_text ltx_font_bold">7.70</span></td>
<td id="S4.T1.2.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">28.0</td>
<td id="S4.T1.2.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">8.25</td>
<td id="S4.T1.2.1.3.2.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.71</td>
<td id="S4.T1.2.1.3.2.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.04</td>
</tr>
<tr id="S4.T1.2.1.4.3" class="ltx_tr" style="background-color:#BFBFBF;">
<th id="S4.T1.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.4.3.1.1" class="ltx_text" style="background-color:#BFBFBF;">clearDepth</span></th>
<td id="S4.T1.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.4.3.2.1" class="ltx_text" style="background-color:#BFBFBF;">1.33</span></td>
<td id="S4.T1.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.4.3.3.1" class="ltx_text" style="background-color:#BFBFBF;">8.68</span></td>
<td id="S4.T1.2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.4.3.4.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFBF;">25.30</span></td>
<td id="S4.T1.2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.4.3.5.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFBF;">7.39</span></td>
<td id="S4.T1.2.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.4.3.6.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFBF;">3.48</span></td>
<td id="S4.T1.2.1.4.3.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T1.2.1.4.3.7.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFBF;">2.00</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Quantitative results on Middleburry Stereo Evaluation Benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. All metrics have been calculated using undisclosed weighting factors. The outcomes unequivocally demonstrate that our technique significantly outperforms the baseline method.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.5.1.1" class="ltx_text">IV-B</span>2 </span>Quantitative Analysis on KITTI Dataset</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We fine-tune our pre-trained model using the KITTI 2015 training set across 5,000 steps, employing image crops sized at <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="320\times 1000" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS2.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><times id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">320</cn><cn type="integer" id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">320\times 1000</annotation></semantics></math>. The learning rate is established at 0.00001, with the batch size held at 3. In terms of GRU updates, we perform 22 iterations during training, adjusting to 32 iterations for testing.
Given that the labels and metrics of the KITTI dataset cannot fully reflect imaging quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, we conducted a qualitative analysis on the KITTI dataset.
Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-B2 Quantitative Analysis on KITTI Dataset ‣ IV-B Qualitative and Quantitative Studies for Stereo Depth Estimation ‣ IV Experiments ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrates a competitive comparison focused on detail recovery, our method shows exceptional proficiency in reconstructing depth details of foreground objects, significantly outstripping alternative approaches by a substantial margin.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.08926/assets/left_image_test_3.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="181" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">((a))</span> </span><span id="S4.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">Left image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.08926/assets/raft-stereo_test_3.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="181" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">((b))</span> </span><span id="S4.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">RAFT-Stereo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.08926/assets/stereo_base_test_3.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="181" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">((c))</span> </span><span id="S4.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">StereoBase<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.08926/assets/cleardepth_test_3_with_bbox.png" id="S4.F6.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="181" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">((d))</span> </span><span id="S4.F6.sf4.3.2" class="ltx_text" style="font-size:90%;">Ours</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Visual comparisons on KITTI 2015 with SOTA StereoBase and baseline RAFT-Stereo. Our method is more robust to overall scene details.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS3.5.1.1" class="ltx_text">IV-B</span>3 </span>Evaluation on Our Transparent Object Dataset</h4>

<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:90.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.6pt,9.0pt) scale(0.832526691256787,0.832526691256787) ;">
<table id="S4.T2.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Methods</th>
<th id="S4.T2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AvgErr</th>
<th id="S4.T2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">RMS</th>
<th id="S4.T2.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">bad 0.5 (%)</th>
<th id="S4.T2.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">bad 1.0 (%)</th>
<th id="S4.T2.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">bad 2.0 (%)</th>
<th id="S4.T2.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">bad 4.0 (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.2.1" class="ltx_tr">
<th id="S4.T2.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">IGEV-Stereo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="S4.T2.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2.0766</td>
<td id="S4.T2.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">5.5301</td>
<td id="S4.T2.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">58.9743</td>
<td id="S4.T2.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">36.127</td>
<td id="S4.T2.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">19.9668</td>
<td id="S4.T2.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">10.7771</td>
</tr>
<tr id="S4.T2.2.1.3.2" class="ltx_tr">
<th id="S4.T2.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DLNR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T2.2.1.3.2.2" class="ltx_td ltx_align_center">3.097</td>
<td id="S4.T2.2.1.3.2.3" class="ltx_td ltx_align_center">8.4269</td>
<td id="S4.T2.2.1.3.2.4" class="ltx_td ltx_align_center">28.1088</td>
<td id="S4.T2.2.1.3.2.5" class="ltx_td ltx_align_center">21.8442</td>
<td id="S4.T2.2.1.3.2.6" class="ltx_td ltx_align_center">16.481</td>
<td id="S4.T2.2.1.3.2.7" class="ltx_td ltx_align_center">11.9046</td>
</tr>
<tr id="S4.T2.2.1.4.3" class="ltx_tr">
<th id="S4.T2.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Selective-IGEV<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S4.T2.2.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.4.3.2.1" class="ltx_text ltx_font_bold">1.273</span></td>
<td id="S4.T2.2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.4.3.3.1" class="ltx_text ltx_font_bold">4.3365</span></td>
<td id="S4.T2.2.1.4.3.4" class="ltx_td ltx_align_center">34.8229</td>
<td id="S4.T2.2.1.4.3.5" class="ltx_td ltx_align_center">17.6288</td>
<td id="S4.T2.2.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.4.3.6.1" class="ltx_text ltx_font_bold">9.561</span></td>
<td id="S4.T2.2.1.4.3.7" class="ltx_td ltx_align_center">5.8707</td>
</tr>
<tr id="S4.T2.2.1.5.4" class="ltx_tr">
<th id="S4.T2.2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RAFT-Stereo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T2.2.1.5.4.2" class="ltx_td ltx_align_center">2.245</td>
<td id="S4.T2.2.1.5.4.3" class="ltx_td ltx_align_center">8.8016</td>
<td id="S4.T2.2.1.5.4.4" class="ltx_td ltx_align_center">29.7356</td>
<td id="S4.T2.2.1.5.4.5" class="ltx_td ltx_align_center">17.4521</td>
<td id="S4.T2.2.1.5.4.6" class="ltx_td ltx_align_center">10.835</td>
<td id="S4.T2.2.1.5.4.7" class="ltx_td ltx_align_center">6.2107</td>
</tr>
<tr id="S4.T2.2.1.6.5" class="ltx_tr" style="background-color:#BFBFBF;">
<th id="S4.T2.2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.2.1.6.5.1.1" class="ltx_text" style="background-color:#BFBFBF;">clearDepth</span></th>
<td id="S4.T2.2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.1.6.5.2.1" class="ltx_text" style="background-color:#BFBFBF;">2.138</span></td>
<td id="S4.T2.2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.1.6.5.3.1" class="ltx_text" style="background-color:#BFBFBF;">8.7282</span></td>
<td id="S4.T2.2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.1.6.5.4.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFBF;">24.7329</span></td>
<td id="S4.T2.2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.1.6.5.5.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFBF;">16.3178</span></td>
<td id="S4.T2.2.1.6.5.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.1.6.5.6.1" class="ltx_text" style="background-color:#BFBFBF;">9.8459</span></td>
<td id="S4.T2.2.1.6.5.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.1.6.5.7.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFBF;">5.76</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Quantitative results on transparent object dataset compared with stereo SOTA methods.</span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2409.08926/assets/x6.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">
The visualization results of our transparent object stereo depth reconstruction method compare with other SOTA stereo depth estimation methods by fine-tuning on SynClearDepth dataset.</span></figcaption>
</figure>
<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">To validate the efficacy of our model and our proposed dataset for the task of depth recovery of transparent objects using stereo vision, we fine-tuned our pre-trained model on the SynClearDepth dataset. This fine-tuning process involved utilizing the same training parameters established during the pre-training phase. Additionally, we finetuned the baseline model, RAFT-Stereo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and other SOTA models on Middlebury benchmark, on the SynClearDepth dataset and conducted a performance comparison. This comparative analysis aimed to underscore the advancements and improvements our model offers in the specific context of stereo-based depth perception for transparent objects. Tab. <a href="#S4.T2" title="TABLE II ‣ IV-B3 Evaluation on Our Transparent Object Dataset ‣ IV-B Qualitative and Quantitative Studies for Stereo Depth Estimation ‣ IV Experiments ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> displays the quantitative validation results. The visualization results of the stereo imaging for transparent objects are illustrated in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-B3 Evaluation on Our Transparent Object Dataset ‣ IV-B Qualitative and Quantitative Studies for Stereo Depth Estimation ‣ IV Experiments ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. From the visualization results, it is evident that the imaging performance of our network, specifically designed for transparent objects, significantly surpasses that of other stereo imaging methods in the transparent object regions.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS4.5.1.1" class="ltx_text">IV-B</span>4 </span>Ablation Study of Feature Post-Fusion Module</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">To ascertain the impact of our feature post-fusion module, we embarked on a series of ablation studies. These studies compared networks equipped with and without the feature post-fusion module, specifically focusing on our transparent object dataset, SynClearDepth. The results in Tab. <a href="#S4.T3" title="TABLE III ‣ IV-C Robotic Grasping Experiments ‣ IV Experiments ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> demonstrate that the inclusion of the feature post-fusion module significantly improves the network’s overall performance in processing transparent objects. This enhancement is particularly evident in challenging scenarios involving complex transparency and light refraction, underscoring the module’s effectiveness in capturing and integrating crucial features for more accurate depth estimation and object recognition in such contexts. The ablation study utilizes 100,000 steps in training.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Robotic Grasping Experiments</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We employ the ZED 2 stereo camera with our proposed network, integrated with a robotic arm and a five-fingered dexterous hand, to perform grasping tasks.
We tested the success rate of grasping transparent objects in service robot scenarios, following the experimental setup of FFHClutteredGrasping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Using our stereo method, we generated point clouds of scenes with transparent objects and performed pose estimation. We then planned the grasping posture for the five-fingered hand. We conducted 150 experiments for each scenario, varying the number of transparent objects from 1 to 5, and the results are shown in Table <a href="#S4.T4" title="TABLE IV ‣ IV-C Robotic Grasping Experiments ‣ IV Experiments ‣ ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. We achieved an average success rate of 86.2%.
Our approach integrates seamlessly into existing robotic grasping frameworks without needing additional processing of intermediate results. Our method does not require multi-view images capture, thus maintaining the robot’s operational efficiency. Additionally, our data generation process is fast and efficient, enabling rapid expansion of datasets for further tasks. Our findings demonstrate that our transparent object depth recovery network significantly improves the robot’s ability to grasp transparent objects.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:61.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(27.5pt,-3.9pt) scale(1.14512085782156,1.14512085782156) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Methods</th>
<th id="S4.T3.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">AvgErr</th>
<th id="S4.T3.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">RMS</th>
<th id="S4.T3.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 0.5 (%)</th>
<th id="S4.T3.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 1.0 (%)</th>
<th id="S4.T3.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 2.0 (%)</th>
<th id="S4.T3.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">bad 4.0 (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.2.1" class="ltx_tr">
<th id="S4.T3.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">w/o Fusion</th>
<td id="S4.T3.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">6.90</td>
<td id="S4.T3.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">15.48</td>
<td id="S4.T3.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">43.34</td>
<td id="S4.T3.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">29.63</td>
<td id="S4.T3.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">21.52</td>
<td id="S4.T3.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">16.62</td>
</tr>
<tr id="S4.T3.2.1.3.2" class="ltx_tr">
<th id="S4.T3.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">Feature Fusion</th>
<td id="S4.T3.2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T3.2.1.3.2.2.1" class="ltx_text ltx_font_bold">2.64</span></td>
<td id="S4.T3.2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T3.2.1.3.2.3.1" class="ltx_text ltx_font_bold">8.59</span></td>
<td id="S4.T3.2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T3.2.1.3.2.4.1" class="ltx_text ltx_font_bold">27.23</span></td>
<td id="S4.T3.2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T3.2.1.3.2.5.1" class="ltx_text ltx_font_bold">16.87</span></td>
<td id="S4.T3.2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T3.2.1.3.2.6.1" class="ltx_text ltx_font_bold">11.28</span></td>
<td id="S4.T3.2.1.3.2.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T3.2.1.3.2.7.1" class="ltx_text ltx_font_bold">7.72</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Ablation study for the feature post-fusion module in clearDepth with 100,000 steps on SynClearDepth dataset.</span></figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:45.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(27.1pt,-4.7pt) scale(1.26281090806778,1.26281090806778) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Object Number</th>
<th id="S4.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1</th>
<th id="S4.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2</th>
<th id="S4.T4.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3</th>
<th id="S4.T4.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">4</th>
<th id="S4.T4.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.2.1" class="ltx_tr">
<td id="S4.T4.2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">success rate</td>
<td id="S4.T4.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">98%</td>
<td id="S4.T4.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">92%</td>
<td id="S4.T4.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">86%</td>
<td id="S4.T4.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">78%</td>
<td id="S4.T4.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">76%</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">Grasping success rate with varying number of transparent objects.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose a new stereo transparent object dataset SynClearDepth, containing RGB, depth, mask, object poses, and camera poses. Compared to other real datasets, our simulated data does not introduce the inherent noise of real 3D sensors. Unlike other simulated datasets, we have collected a large number of indoor scene models to fill the dataset background, avoiding ambiguities for depth recovery networks. We also propose a stereo depth recovery network for transparent objects, which enhances the underlying structural information, allowing our network to recover the depth of transparent objects implicitly without the need for a transparent object mask prior.
We conducted comprehensive comparative experiments on our model using both public datasets and our proprietary dataset for transparent objects. Additionally, we performed ablation studies to evaluate the design of our model. These experiments demonstrated the exceptional performance of our model, highlighting its robustness and accuracy in various scenarios for transparent object perception, aiding in the manipulation tasks of transparent objects by robots.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Jiang, G. Cao, J. Deng, T.-T. Do, and S. Luo, “Robotic perception of transparent objects: A review,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Artificial Intelligence</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Li, Z. Chen, H. Liu, and C. Wang, “Fdct: Fast depth completion for transparent objects,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K. Chen, S. Wang, B. Xia, D. Li, Z. Kan, and B. Li, “Tode-trans: Transparent object depth estimation with transformer,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2023, pp. 4880–4886.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. Chen, S. James, C. Sui, Y.-H. Liu, P. Abbeel, and Q. Dou, “Stereopose: Category-level 6d transparent object pose estimation from stereo images via back-view nocs,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2023, pp. 2855–2861.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. R. Wang, Y. Zhao, H. Xu, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and A. Garg, “Mvtrans: Multi-view perception of transparent objects,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2023, pp. 3771–3778.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Cao, Z. Zhang, E. Xie, Q. Hou, K. Zhao, X. Luo, and J. Tuo, “Fakemix augmentation improves transparent object detection,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.13279</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Shao, C. Xia, D. Duan, and X. Wang, “Polarimetric inverse rendering for transparent shapes reconstruction,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.11836</em>, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Fang, H.-S. Fang, S. Xu, and C. Lu, “Transcg: A large-scale real-world dataset for transparent object depth completion and a grasping baseline,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 7, no. 3, pp. 7383–7390, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Xu, Y. R. Wang, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and A. Garg, “Seeing glass: joint point cloud and depth completion for transparent objects,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.00087</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Sajjan, M. Moore, M. Pan, G. Nagaraja, J. Lee, A. Zeng, and S. Song, “Clear grasp: 3d shape estimation of transparent objects for manipulation,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2020, pp. 3634–3642.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. Chen, H. Zhang, Z. Yu, A. Opipari, and O. Chadwicke Jenkins, “Clearpose: Large-scale transparent object dataset and benchmark,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2022, pp. 381–396.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Kalra, V. Taamazyan, S. K. Rao, K. Venkataraman, R. Raskar, and A. Kadambi, “Deep polarization cues for transparent object segmentation,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 8602–8611.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. Garigapati, E. Blasch, J. Wei, and H. Ling, “Transparent object tracking with enhanced fusion module,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2023, pp. 7696–7703.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Lukezic, Z. Trojer, J. Matas, and M. Kristan, “Trans2k: Unlocking the power of deep models for transparent object tracking,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.03436</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Cai, F. Xue, L. Xu, and L. Guo, “Transmatting: Tri-token equipped transformer model for image matting,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.06476</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Q. Dai, J. Zhang, Q. Li, T. Wu, H. Dong, Z. Liu, P. Tan, and H. Wang, “Domain randomization-enhanced depth simulation and restoration for perceiving and grasping specular and transparent objects,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2022, pp. 374–391.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Z. Li, X. Long, Y. Wang, T. Cao, W. Wang, F. Luo, and C. Xiao, “Neto: Neural reconstruction of transparent objects with self-occlusion aware refraction-tracing,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11219</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Q. Dai, Y. Zhu, Y. Geng, C. Ruan, J. Zhang, and H. Wang, “Graspnerf: Multiview-based 6-dof grasp detection for transparent and specular objects using generalizable nerf,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2023, pp. 1757–1763.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Z. Li, Y.-Y. Yeh, and M. Chandraker, “Through the looking glass: neural 3d reconstruction of transparent shapes,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 1262–1271.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Zhang, A. Opipari, X. Chen, J. Zhu, Z. Yu, and O. C. Jenkins, “Transnet: Transparent object manipulation through category-level pose estimation,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.12400</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox, “A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 4040–4048.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Xu and J. Zhang, “Aanet: Adaptive aggregation network for efficient stereo matching,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 1959–1968.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “Gcnet: Non-local networks meet squeeze-excitation networks and beyond,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision workshops</em>, 2019, pp. 0–0.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 5410–5418.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
L. Lipson, Z. Teed, and J. Deng, “Raft-stereo: Multilevel recurrent field transforms for stereo matching,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2021 International Conference on 3D Vision (3DV)</em>.   IEEE, 2021, pp. 218–227.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Teed and J. Deng, “Raft: Recurrent all-pairs field transforms for optical flow,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16</em>.   Springer, 2020, pp. 402–419.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Z. Li, X. Liu, N. Drenkow, A. Ding, F. X. Creighton, R. H. Taylor, and M. Unberath, “Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 6197–6206.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superglue: Learning feature matching with graph neural networks,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 4938–4947.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Kim, M.-H. Jeon, S. Jung, W. Yang, M. Jung, J. Shin, and A. Kim, “Transpose: Large-scale multispectral dataset for transparent object,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">The International Journal of Robotics Research</em>, p. 02783649231213117, 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Jiang, G. Cao, T.-T. Do, and S. Luo, “A4t: Hierarchical affordance detection for transparent objects depth reconstruction and manipulation,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 7, no. 4, pp. 9826–9833, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
L. Zhu, A. Mousavian, Y. Xiang, H. Mazhar, J. van Eenbergen, S. Debnath, and D. Fox, “Rgb-d local implicit function for depth completion of transparent objects,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 4649–4658.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Ichnowski, Y. Avigal, J. Kerr, and K. Goldberg, “Dex-nerf: Using a neural radiance field to grasp transparent objects,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14217</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Shi, Y. Jin, D. Li, H. Niu, Z. Jin, H. Wang <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Asgrasp: Generalizable transparent object reconstruction and grasping from rgb-d active stereo camera,” <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.05648</em>, 2024.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Tuli, I. Dasgupta, E. Grant, and T. L. Griffiths, “Are convolutional neural networks or transformers more like human vision?” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.07197</em>, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 12 077–12 090, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Dinov2: Learning robust visual features without supervision,” <em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07193</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
N. Ahn, B. Kang, and K.-A. Sohn, “Fast, accurate, and lightweight super-resolution with cascading residual network,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 252–268.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan, and S. Liu, “Practical stereo matching via cascaded recurrent network with adaptive correlation,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 16 263–16 272.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
N. Mayer, E. Ilg, P. Häusser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox, “A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016, arXiv:1512.02134. [Online]. Available: <a target="_blank" href="http://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
“Middlebury stereo vision page,” <a target="_blank" href="https://vision.middlebury.edu/stereo/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vision.middlebury.edu/stereo/</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, “Depth anything v2,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv:2406.09414</em>, 2024.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
X. Guo, J. Lu, C. Zhang, Y. Wang, Y. Duan, T. Yang, Z. Zhu, and L. Chen, “Openstereo: A comprehensive benchmark for stereo matching and strong baseline,” 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
G. Xu, X. Wang, X. Ding, and X. Yang, “Iterative geometry encoding volume for stereo matching,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 21 919–21 928.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
H. Zhao, H. Zhou, Y. Zhang, J. Chen, Y. Yang, and Y. Zhao, “High-frequency stereo matching network,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 1327–1336.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
X. Wang, G. Xu, H. Jia, and X. Yang, “Selective-stereo: Adaptive frequency information selection for stereo matching,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 19 701–19 710.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
L. Zhang, K. Bai, G. Huang, Z. Chen, and J. Zhang, “Multi-fingered robotic hand grasping in cluttered environments through hand-object contact semantic mapping,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.08844</em>, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.08925" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.08926" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.08926">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.08926" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.08927" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:14:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
