<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2201.03965] On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering</title><meta property="og:description" content="In recent years, multi-modal transformers have shown significant progress in Vision-Language tasks, such as Visual Question Answering (VQA), outperforming previous architectures by a considerable margin. This improveme…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2201.03965">

<!--Generated on Wed Mar  6 10:31:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ankur Sikarwar 
<br class="ltx_break">Department of ECE 
<br class="ltx_break">Birla Institute of Technology, Mesra 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">ankursikarwardc@gmail.com</span> 
<br class="ltx_break">Gabriel Kreiman 
<br class="ltx_break">Center for Brains, Minds and Machines 
<br class="ltx_break">Harvard Medical School 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">gabriel.kreiman@tch.harvard.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">In recent years, multi-modal transformers have shown significant progress in Vision-Language tasks, such as Visual Question Answering (VQA), outperforming previous architectures by a considerable margin. This improvement in VQA is often attributed to the rich interactions between vision and language streams. In this work, we investigate the efficacy of co-attention transformer layers in helping the network focus on relevant regions while answering the question. We generate visual attention maps using the question-conditioned image attention scores in these co-attention layers. We evaluate the effect of the following critical components on visual attention of a state-of-the-art VQA model: (i) number of object region proposals, (ii) question part of speech (POS) tags, (iii) question semantics, (iv) number of co-attention layers, and (v) answer accuracy. We compare the neural network attention maps against human attention maps both qualitatively and quantitatively. Our findings indicate that co-attention transformer modules are crucial in attending to relevant regions of the image given a question. Importantly, we observe that the semantic meaning of the question is <em id="id3.id1.1" class="ltx_emph ltx_font_italic">not</em> what drives visual attention, but specific keywords in the question do. Our work sheds light on the function and interpretation of co-attention transformer layers, highlights gaps in current networks, and can guide the development of future VQA models and networks that simultaneously process visual and language streams.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The ability of humans to efficiently ground information across different modalities, such as vision and language, plays a central role in cognitive function. The interactions between vision and language are highlighted in visual question answering (VQA) tasks, where attentional allocation is naturally routed by combination of sensory and semantic cues. For instance, given an image of people playing football and the question ’What color shirt is the person behind the referee wearing?’, subjects rapidly identify the referee, saccade to the player behind the referee, and process the relevant regions of the image to find the answer. A four-year old can easily answer such questions and seamlessly direct visual attention to the relevant regions based on the question.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In contrast, such multi-modal tasks are quite challenging for current AI systems because the solution encompasses several increasingly complex subtasks. First of all, the system has to interpret the key elements in the question for attention allocation, in this case, referees, players, and shirt. Distinguishing the referee from the players is complicated in itself, as it requires further background knowledge about sports. Next, the system has to make sense of prepositions like ’behind’ to capture spatial relationships between objects or agents, in this case, to attend to one specific player. Finally, the system needs to visually attend to the task-relevant regions, distill the type of information required (shirt color), and produce the answer.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, there has been an exciting trend of extending the successful transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite> to solve multi-modal tasks combining modalities including text, audio, images, and videos <cite class="ltx_cite ltx_citemacro_citep">(Chuang et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Gabeur et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Sun et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>. This trend has led to significant improvements in state-of-the-art models for Vision-Language tasks like visual grounding, referring expressions, and visual question answering. These families of models are based on either single-stream or two-stream architectures. The former shares the parameters across both modalities, while the latter has separate processing stacks for vision and language. In <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, Co-Attention Transformer Layers (<span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>) are used to facilitate interactions between the visual and language streams of the network.
The task-relevant representations from the language stream modulate processing in the visual stream in the form of attention.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we assess the capabilities of co-attention transformer layers in guiding visual attention to task-relevant regions. We focus specifically on the Visual Question Answering task and conduct experiments to gain insight into the attention mechanisms of these layers and compare these mechanisms to human attention. Given an image/question pair, we generate attention maps for different co-attention layers based on the question-conditioned image attention scores and evaluate these maps against human attention maps, quantitatively and qualitatively, via rank-correlation and visualizations. We ask the following questions:
1) Does the use of object-based region proposals act as a bottleneck?
2) Is the model more likely to correctly answer a question when its attention map is better correlated to humans?
3) What is the role of question semantics in driving the model’s visual attention?
4) What is the importance of different parts of speech in guiding the model to attend to task-relevant regions?
Our experiments demonstrate that object-based region proposals often restrict the model from focusing on task-relevant regions. We show that rank-correlation between human and machine attention is considerably higher in current state-of-the-art transformer-based architectures compared to previous CNN/LSTM networks. Lastly, we find that question semantics have little influence on the model’s visual attention, and only specific keywords in the question are responsible for driving attention.
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2201.03965/assets/Images/co_attention_layer.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1988" height="1399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Co-attention transformer layer <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The Visual Question Answering (VQA) v1 dataset containing images from the MSCOCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib16" title="" class="ltx_ref">2014</a>)</cite> with over 760K questions and 10M answers was introduced in <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>, and a more balanced VQA v2 dataset was introduced in <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>. The initial model for VQA <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite> employed deep convolutional neural networks and recurrent neural networks to compute image and question representations separately. These were then fused using point-wise multiplication and fed to a Multi-Layer Perceptron (MLP) to predict the answer. Later, <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite> proposed Stacked Attention Networks (SAN), in which the question representation from an LSTM was used for predicting an attention distribution over different parts of the image. Based on this attention and the question representation, another level of attention was performed over the image. The Hierarchical Co-Attention Model <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> introduced co-attention, where the model attends to parts of the image along with parts of the question. Given a question about an image, this model hierarchically uses word-level, phrase-level, and question-level co-attention.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The VQA-HAT dataset consisting of human attention maps for question/image pairs from the VQA v1 dataset was introduced in <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite>. These maps were collected by asking humans to deblur different image regions by clicking on those regions to answer the question. Attention-based VQA models <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib25" title="" class="ltx_ref">2016</a>; Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> based on convolutional neural networks and LSTM modules, but not transformer-based models, were compared against human attention maps <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite>. The authors concluded that these models did <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">not</em> attend to the same regions as humans while answering the question. However, increased performance was weakly associated with a better correlation between human and model attention maps.
Later, <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> used guided backpropagation and occlusion techniques to generate image importance maps for a VQA model and then compared those with human attention maps.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Various transformer-based VQA models <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Su et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2019b</a>, <a href="#bib.bib13" title="" class="ltx_ref">a</a>; Zhou et al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>; Chefer et al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> have been introduced in the last few years. Among them, <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> are two-stream transformer architectures that use cross-attention layers and co-attention layers, respectively, to allow information exchange across modalities. There are several studies on the interpretability of VQA models <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>; Agrawal et al., <a href="#bib.bib1" title="" class="ltx_ref">2016</a>; Kafle and Kanan, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>; Jabri et al., <a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>, and yet very few have focused on the co-attention transformer layers used in recent VQA models. In this work, we use ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> for our study as it employs these co-attention layers.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We study the co-attention module between language and vision and the interactions within this module. To study co-attention in two-stream vision-language transformer architectures, we evaluated visual attention in the model by comparing it against human attention maps. ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> is an extension of the BERT architecture <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> to process visual inputs. Given a question and an image, the model processes them separately in the language and visual streams, respectively. Both visual and language streams contain a stack of transformer and co-attention transformer layers. The embeddings for the word tokens and other special tokens are fed to the language stream after adding positional embeddings. The image is processed through the Faster RCNN network <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> to generate features for different region proposals. The feature representations of region proposals with the highest objectness score are fed to the visual stream. The model then processes these inputs through the two streams while fusing information within them using subsequent <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">co-attention layers</em> (<span id="S3.p1.1.2" class="ltx_text ltx_font_bold">Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Setup</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">The ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> network variant in our study uses the BERT<sub id="S3.SS1.p1.4.1" class="ltx_sub">BASE</sub> model <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> for the language part, composed of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">12</annotation></semantics></math> transformer blocks. The latter <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">6</annotation></semantics></math> blocks have co-attention transformer modules stacked between them. The visual stream comprises <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">6</annotation></semantics></math> transformer and co-attention transformer modules. The co-attention transformer layer uses <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><cn type="integer" id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">8</annotation></semantics></math> parallel attention heads. All experiments were performed on a single NVIDIA 1080 Ti GPU. The source code will be publicly available upon publication.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Attention Map Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">Given an image and a question, the inputs to the visual stream are the region features <math id="S3.SS2.p1.1.m1.4" class="ltx_Math" alttext="v_{0},v_{1},\ldots,v_{T}" display="inline"><semantics id="S3.SS2.p1.1.m1.4a"><mrow id="S3.SS2.p1.1.m1.4.4.3" xref="S3.SS2.p1.1.m1.4.4.4.cmml"><msub id="S3.SS2.p1.1.m1.2.2.1.1" xref="S3.SS2.p1.1.m1.2.2.1.1.cmml"><mi id="S3.SS2.p1.1.m1.2.2.1.1.2" xref="S3.SS2.p1.1.m1.2.2.1.1.2.cmml">v</mi><mn id="S3.SS2.p1.1.m1.2.2.1.1.3" xref="S3.SS2.p1.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.p1.1.m1.4.4.3.4" xref="S3.SS2.p1.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS2.p1.1.m1.3.3.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.cmml"><mi id="S3.SS2.p1.1.m1.3.3.2.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.2.cmml">v</mi><mn id="S3.SS2.p1.1.m1.3.3.2.2.3" xref="S3.SS2.p1.1.m1.3.3.2.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.1.m1.4.4.3.5" xref="S3.SS2.p1.1.m1.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">…</mi><mo id="S3.SS2.p1.1.m1.4.4.3.6" xref="S3.SS2.p1.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS2.p1.1.m1.4.4.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.cmml"><mi id="S3.SS2.p1.1.m1.4.4.3.3.2" xref="S3.SS2.p1.1.m1.4.4.3.3.2.cmml">v</mi><mi id="S3.SS2.p1.1.m1.4.4.3.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.3.cmml">T</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.4b"><list id="S3.SS2.p1.1.m1.4.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4.3"><apply id="S3.SS2.p1.1.m1.2.2.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.2">𝑣</ci><cn type="integer" id="S3.SS2.p1.1.m1.2.2.1.1.3.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.3">0</cn></apply><apply id="S3.SS2.p1.1.m1.3.3.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.3.3.2.2.1.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2">𝑣</ci><cn type="integer" id="S3.SS2.p1.1.m1.3.3.2.2.3.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.3">1</cn></apply><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">…</ci><apply id="S3.SS2.p1.1.m1.4.4.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.4.4.3.3.1.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.4.4.3.3.2.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.2">𝑣</ci><ci id="S3.SS2.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3">𝑇</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.4c">v_{0},v_{1},\ldots,v_{T}</annotation></semantics></math> and the input to the language stream are <math id="S3.SS2.p1.2.m2.4" class="ltx_Math" alttext="w_{0},w_{1},\ldots,w_{N}" display="inline"><semantics id="S3.SS2.p1.2.m2.4a"><mrow id="S3.SS2.p1.2.m2.4.4.3" xref="S3.SS2.p1.2.m2.4.4.4.cmml"><msub id="S3.SS2.p1.2.m2.2.2.1.1" xref="S3.SS2.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.2.2.1.1.2" xref="S3.SS2.p1.2.m2.2.2.1.1.2.cmml">w</mi><mn id="S3.SS2.p1.2.m2.2.2.1.1.3" xref="S3.SS2.p1.2.m2.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.p1.2.m2.4.4.3.4" xref="S3.SS2.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.3.3.2.2" xref="S3.SS2.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS2.p1.2.m2.3.3.2.2.2" xref="S3.SS2.p1.2.m2.3.3.2.2.2.cmml">w</mi><mn id="S3.SS2.p1.2.m2.3.3.2.2.3" xref="S3.SS2.p1.2.m2.3.3.2.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.2.m2.4.4.3.5" xref="S3.SS2.p1.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS2.p1.2.m2.4.4.3.6" xref="S3.SS2.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.4.4.3.3" xref="S3.SS2.p1.2.m2.4.4.3.3.cmml"><mi id="S3.SS2.p1.2.m2.4.4.3.3.2" xref="S3.SS2.p1.2.m2.4.4.3.3.2.cmml">w</mi><mi id="S3.SS2.p1.2.m2.4.4.3.3.3" xref="S3.SS2.p1.2.m2.4.4.3.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.4b"><list id="S3.SS2.p1.2.m2.4.4.4.cmml" xref="S3.SS2.p1.2.m2.4.4.3"><apply id="S3.SS2.p1.2.m2.2.2.1.1.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1.2">𝑤</ci><cn type="integer" id="S3.SS2.p1.2.m2.2.2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1.3">0</cn></apply><apply id="S3.SS2.p1.2.m2.3.3.2.2.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2.2">𝑤</ci><cn type="integer" id="S3.SS2.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2.3">1</cn></apply><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">…</ci><apply id="S3.SS2.p1.2.m2.4.4.3.3.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.4.4.3.3.1.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.4.4.3.3.2.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3.2">𝑤</ci><ci id="S3.SS2.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3.3">𝑁</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.4c">w_{0},w_{1},\ldots,w_{N}</annotation></semantics></math>. We generate an attention map for each co-attention transformer layer in the model as shown in <span id="S3.SS2.p1.4.1" class="ltx_text ltx_font_bold">Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.3 Comparison Metric ‣ 3 Methods ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span>. Inside the multi-head attention block in each co-attention transformer layer, the key and value matrices from one stream are projected onto another stream and vice versa. Consequently, inside the language stream, the multiplication of the Query matrix (<math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="Q_{L}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">Q</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑄</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">Q_{L}</annotation></semantics></math>) from the language stream and the Key matrix (<math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="K_{V}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">K</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝐾</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">K_{V}</annotation></semantics></math>) from the visual stream produces attention scores over the different image regions based on the question. These attention scores are then passed through a softmax operation to generate respective attention probabilities</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.2" class="ltx_Math" alttext="a^{i}_{h}=softmax(\frac{Q_{L}K^{T}_{V}}{\sqrt{d_{k}}})," display="block"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.2.1" xref="S3.Ex1.m1.2.2.1.1.cmml"><mrow id="S3.Ex1.m1.2.2.1.1" xref="S3.Ex1.m1.2.2.1.1.cmml"><msubsup id="S3.Ex1.m1.2.2.1.1.2" xref="S3.Ex1.m1.2.2.1.1.2.cmml"><mi id="S3.Ex1.m1.2.2.1.1.2.2.2" xref="S3.Ex1.m1.2.2.1.1.2.2.2.cmml">a</mi><mi id="S3.Ex1.m1.2.2.1.1.2.3" xref="S3.Ex1.m1.2.2.1.1.2.3.cmml">h</mi><mi id="S3.Ex1.m1.2.2.1.1.2.2.3" xref="S3.Ex1.m1.2.2.1.1.2.2.3.cmml">i</mi></msubsup><mo id="S3.Ex1.m1.2.2.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.2.2.1.1.3" xref="S3.Ex1.m1.2.2.1.1.3.cmml"><mi id="S3.Ex1.m1.2.2.1.1.3.2" xref="S3.Ex1.m1.2.2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.1.1.3.1" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.1.1.3.3" xref="S3.Ex1.m1.2.2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.1.1.3.1a" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.1.1.3.4" xref="S3.Ex1.m1.2.2.1.1.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.1.1.3.1b" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.1.1.3.5" xref="S3.Ex1.m1.2.2.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.1.1.3.1c" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.1.1.3.6" xref="S3.Ex1.m1.2.2.1.1.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.1.1.3.1d" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.1.1.3.7" xref="S3.Ex1.m1.2.2.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.1.1.3.1e" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.1.1.3.8" xref="S3.Ex1.m1.2.2.1.1.3.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.1.1.3.1f" xref="S3.Ex1.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S3.Ex1.m1.2.2.1.1.3.9.2" xref="S3.Ex1.m1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.2.2.1.1.3.9.2.1" xref="S3.Ex1.m1.1.1.cmml">(</mo><mfrac id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml"><msub id="S3.Ex1.m1.1.1.2.2" xref="S3.Ex1.m1.1.1.2.2.cmml"><mi id="S3.Ex1.m1.1.1.2.2.2" xref="S3.Ex1.m1.1.1.2.2.2.cmml">Q</mi><mi id="S3.Ex1.m1.1.1.2.2.3" xref="S3.Ex1.m1.1.1.2.2.3.cmml">L</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.2.1" xref="S3.Ex1.m1.1.1.2.1.cmml">​</mo><msubsup id="S3.Ex1.m1.1.1.2.3" xref="S3.Ex1.m1.1.1.2.3.cmml"><mi id="S3.Ex1.m1.1.1.2.3.2.2" xref="S3.Ex1.m1.1.1.2.3.2.2.cmml">K</mi><mi id="S3.Ex1.m1.1.1.2.3.3" xref="S3.Ex1.m1.1.1.2.3.3.cmml">V</mi><mi id="S3.Ex1.m1.1.1.2.3.2.3" xref="S3.Ex1.m1.1.1.2.3.2.3.cmml">T</mi></msubsup></mrow><msqrt id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><msub id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml"><mi id="S3.Ex1.m1.1.1.3.2.2" xref="S3.Ex1.m1.1.1.3.2.2.cmml">d</mi><mi id="S3.Ex1.m1.1.1.3.2.3" xref="S3.Ex1.m1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo stretchy="false" id="S3.Ex1.m1.2.2.1.1.3.9.2.2" xref="S3.Ex1.m1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1.m1.2.2.1.2" xref="S3.Ex1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.1"><eq id="S3.Ex1.m1.2.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1"></eq><apply id="S3.Ex1.m1.2.2.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.2.1.cmml" xref="S3.Ex1.m1.2.2.1.1.2">subscript</csymbol><apply id="S3.Ex1.m1.2.2.1.1.2.2.cmml" xref="S3.Ex1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.2.2.1.cmml" xref="S3.Ex1.m1.2.2.1.1.2">superscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.1.1.2.2.2">𝑎</ci><ci id="S3.Ex1.m1.2.2.1.1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.1.1.2.2.3">𝑖</ci></apply><ci id="S3.Ex1.m1.2.2.1.1.2.3.cmml" xref="S3.Ex1.m1.2.2.1.1.2.3">ℎ</ci></apply><apply id="S3.Ex1.m1.2.2.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.3"><times id="S3.Ex1.m1.2.2.1.1.3.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.1"></times><ci id="S3.Ex1.m1.2.2.1.1.3.2.cmml" xref="S3.Ex1.m1.2.2.1.1.3.2">𝑠</ci><ci id="S3.Ex1.m1.2.2.1.1.3.3.cmml" xref="S3.Ex1.m1.2.2.1.1.3.3">𝑜</ci><ci id="S3.Ex1.m1.2.2.1.1.3.4.cmml" xref="S3.Ex1.m1.2.2.1.1.3.4">𝑓</ci><ci id="S3.Ex1.m1.2.2.1.1.3.5.cmml" xref="S3.Ex1.m1.2.2.1.1.3.5">𝑡</ci><ci id="S3.Ex1.m1.2.2.1.1.3.6.cmml" xref="S3.Ex1.m1.2.2.1.1.3.6">𝑚</ci><ci id="S3.Ex1.m1.2.2.1.1.3.7.cmml" xref="S3.Ex1.m1.2.2.1.1.3.7">𝑎</ci><ci id="S3.Ex1.m1.2.2.1.1.3.8.cmml" xref="S3.Ex1.m1.2.2.1.1.3.8">𝑥</ci><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.9.2"><divide id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.3.9.2"></divide><apply id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2"><times id="S3.Ex1.m1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.2.1"></times><apply id="S3.Ex1.m1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.1.1.2.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.1.1.2.2.2">𝑄</ci><ci id="S3.Ex1.m1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.1.1.2.2.3">𝐿</ci></apply><apply id="S3.Ex1.m1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.3.1.cmml" xref="S3.Ex1.m1.1.1.2.3">subscript</csymbol><apply id="S3.Ex1.m1.1.1.2.3.2.cmml" xref="S3.Ex1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.2.3.2.1.cmml" xref="S3.Ex1.m1.1.1.2.3">superscript</csymbol><ci id="S3.Ex1.m1.1.1.2.3.2.2.cmml" xref="S3.Ex1.m1.1.1.2.3.2.2">𝐾</ci><ci id="S3.Ex1.m1.1.1.2.3.2.3.cmml" xref="S3.Ex1.m1.1.1.2.3.2.3">𝑇</ci></apply><ci id="S3.Ex1.m1.1.1.2.3.3.cmml" xref="S3.Ex1.m1.1.1.2.3.3">𝑉</ci></apply></apply><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><root id="S3.Ex1.m1.1.1.3a.cmml" xref="S3.Ex1.m1.1.1.3"></root><apply id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.2.1.cmml" xref="S3.Ex1.m1.1.1.3.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.2.2.cmml" xref="S3.Ex1.m1.1.1.3.2.2">𝑑</ci><ci id="S3.Ex1.m1.1.1.3.2.3.cmml" xref="S3.Ex1.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">a^{i}_{h}=softmax(\frac{Q_{L}K^{T}_{V}}{\sqrt{d_{k}}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.10" class="ltx_p">where <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">i</annotation></semantics></math> is the co-attention layer number, <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">h</annotation></semantics></math> is the attention head number, and <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\sqrt{d_{k}}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msqrt id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><msub id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2.2" xref="S3.SS2.p3.3.m3.1.1.2.2.cmml">d</mi><mi id="S3.SS2.p3.3.m3.1.1.2.3" xref="S3.SS2.p3.3.m3.1.1.2.3.cmml">k</mi></msub></msqrt><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><root id="S3.SS2.p3.3.m3.1.1a.cmml" xref="S3.SS2.p3.3.m3.1.1"></root><apply id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.2.1.cmml" xref="S3.SS2.p3.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2.2">𝑑</ci><ci id="S3.SS2.p3.3.m3.1.1.2.3.cmml" xref="S3.SS2.p3.3.m3.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\sqrt{d_{k}}</annotation></semantics></math> is a scaling factor <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>. These probabilities over the <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mn id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><cn type="integer" id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">8</annotation></semantics></math> attention heads capture the modulations from each text token to different image regions. To generate question-level attention maps, we first average these attention probabilities (before dropout) over all the attention heads and then across the words present in the question. This gives us attention data <math id="S3.SS2.p3.5.m5.3" class="ltx_Math" alttext="\bm{A^{1}},\ldots,\bm{A^{6}}" display="inline"><semantics id="S3.SS2.p3.5.m5.3a"><mrow id="S3.SS2.p3.5.m5.3.3.2" xref="S3.SS2.p3.5.m5.3.3.3.cmml"><msup id="S3.SS2.p3.5.m5.2.2.1.1" xref="S3.SS2.p3.5.m5.2.2.1.1.cmml"><mi id="S3.SS2.p3.5.m5.2.2.1.1.2" xref="S3.SS2.p3.5.m5.2.2.1.1.2.cmml">𝑨</mi><mn id="S3.SS2.p3.5.m5.2.2.1.1.3" xref="S3.SS2.p3.5.m5.2.2.1.1.3.cmml">𝟏</mn></msup><mo id="S3.SS2.p3.5.m5.3.3.2.3" xref="S3.SS2.p3.5.m5.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">…</mi><mo id="S3.SS2.p3.5.m5.3.3.2.4" xref="S3.SS2.p3.5.m5.3.3.3.cmml">,</mo><msup id="S3.SS2.p3.5.m5.3.3.2.2" xref="S3.SS2.p3.5.m5.3.3.2.2.cmml"><mi id="S3.SS2.p3.5.m5.3.3.2.2.2" xref="S3.SS2.p3.5.m5.3.3.2.2.2.cmml">𝑨</mi><mn id="S3.SS2.p3.5.m5.3.3.2.2.3" xref="S3.SS2.p3.5.m5.3.3.2.2.3.cmml">𝟔</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.3b"><list id="S3.SS2.p3.5.m5.3.3.3.cmml" xref="S3.SS2.p3.5.m5.3.3.2"><apply id="S3.SS2.p3.5.m5.2.2.1.1.cmml" xref="S3.SS2.p3.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.2.2.1.1.1.cmml" xref="S3.SS2.p3.5.m5.2.2.1.1">superscript</csymbol><ci id="S3.SS2.p3.5.m5.2.2.1.1.2.cmml" xref="S3.SS2.p3.5.m5.2.2.1.1.2">𝑨</ci><cn type="integer" id="S3.SS2.p3.5.m5.2.2.1.1.3.cmml" xref="S3.SS2.p3.5.m5.2.2.1.1.3">1</cn></apply><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">…</ci><apply id="S3.SS2.p3.5.m5.3.3.2.2.cmml" xref="S3.SS2.p3.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.3.3.2.2.1.cmml" xref="S3.SS2.p3.5.m5.3.3.2.2">superscript</csymbol><ci id="S3.SS2.p3.5.m5.3.3.2.2.2.cmml" xref="S3.SS2.p3.5.m5.3.3.2.2.2">𝑨</ci><cn type="integer" id="S3.SS2.p3.5.m5.3.3.2.2.3.cmml" xref="S3.SS2.p3.5.m5.3.3.2.2.3">6</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.3c">\bm{A^{1}},\ldots,\bm{A^{6}}</annotation></semantics></math> for the <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mn id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><cn type="integer" id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">6</annotation></semantics></math> co-attention layers, where <math id="S3.SS2.p3.7.m7.3" class="ltx_Math" alttext="\bm{A^{i}}=\{A^{i}_{v1},\ldots,A^{i}_{vT}\}" display="inline"><semantics id="S3.SS2.p3.7.m7.3a"><mrow id="S3.SS2.p3.7.m7.3.3" xref="S3.SS2.p3.7.m7.3.3.cmml"><msup id="S3.SS2.p3.7.m7.3.3.4" xref="S3.SS2.p3.7.m7.3.3.4.cmml"><mi id="S3.SS2.p3.7.m7.3.3.4.2" xref="S3.SS2.p3.7.m7.3.3.4.2.cmml">𝑨</mi><mi id="S3.SS2.p3.7.m7.3.3.4.3" xref="S3.SS2.p3.7.m7.3.3.4.3.cmml">𝒊</mi></msup><mo id="S3.SS2.p3.7.m7.3.3.3" xref="S3.SS2.p3.7.m7.3.3.3.cmml">=</mo><mrow id="S3.SS2.p3.7.m7.3.3.2.2" xref="S3.SS2.p3.7.m7.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS2.p3.7.m7.3.3.2.2.3" xref="S3.SS2.p3.7.m7.3.3.2.3.cmml">{</mo><msubsup id="S3.SS2.p3.7.m7.2.2.1.1.1" xref="S3.SS2.p3.7.m7.2.2.1.1.1.cmml"><mi id="S3.SS2.p3.7.m7.2.2.1.1.1.2.2" xref="S3.SS2.p3.7.m7.2.2.1.1.1.2.2.cmml">A</mi><mrow id="S3.SS2.p3.7.m7.2.2.1.1.1.3" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p3.7.m7.2.2.1.1.1.3.2" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.2.2.1.1.1.3.1" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3.1.cmml">​</mo><mn id="S3.SS2.p3.7.m7.2.2.1.1.1.3.3" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p3.7.m7.2.2.1.1.1.2.3" xref="S3.SS2.p3.7.m7.2.2.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS2.p3.7.m7.3.3.2.2.4" xref="S3.SS2.p3.7.m7.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">…</mi><mo id="S3.SS2.p3.7.m7.3.3.2.2.5" xref="S3.SS2.p3.7.m7.3.3.2.3.cmml">,</mo><msubsup id="S3.SS2.p3.7.m7.3.3.2.2.2" xref="S3.SS2.p3.7.m7.3.3.2.2.2.cmml"><mi id="S3.SS2.p3.7.m7.3.3.2.2.2.2.2" xref="S3.SS2.p3.7.m7.3.3.2.2.2.2.2.cmml">A</mi><mrow id="S3.SS2.p3.7.m7.3.3.2.2.2.3" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3.cmml"><mi id="S3.SS2.p3.7.m7.3.3.2.2.2.3.2" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.3.3.2.2.2.3.1" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p3.7.m7.3.3.2.2.2.3.3" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3.3.cmml">T</mi></mrow><mi id="S3.SS2.p3.7.m7.3.3.2.2.2.2.3" xref="S3.SS2.p3.7.m7.3.3.2.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.SS2.p3.7.m7.3.3.2.2.6" xref="S3.SS2.p3.7.m7.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.3b"><apply id="S3.SS2.p3.7.m7.3.3.cmml" xref="S3.SS2.p3.7.m7.3.3"><eq id="S3.SS2.p3.7.m7.3.3.3.cmml" xref="S3.SS2.p3.7.m7.3.3.3"></eq><apply id="S3.SS2.p3.7.m7.3.3.4.cmml" xref="S3.SS2.p3.7.m7.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.3.3.4.1.cmml" xref="S3.SS2.p3.7.m7.3.3.4">superscript</csymbol><ci id="S3.SS2.p3.7.m7.3.3.4.2.cmml" xref="S3.SS2.p3.7.m7.3.3.4.2">𝑨</ci><ci id="S3.SS2.p3.7.m7.3.3.4.3.cmml" xref="S3.SS2.p3.7.m7.3.3.4.3">𝒊</ci></apply><set id="S3.SS2.p3.7.m7.3.3.2.3.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2"><apply id="S3.SS2.p3.7.m7.2.2.1.1.1.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1">subscript</csymbol><apply id="S3.SS2.p3.7.m7.2.2.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.1.1.1.2.1.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1">superscript</csymbol><ci id="S3.SS2.p3.7.m7.2.2.1.1.1.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1.2.2">𝐴</ci><ci id="S3.SS2.p3.7.m7.2.2.1.1.1.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.7.m7.2.2.1.1.1.3.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3"><times id="S3.SS2.p3.7.m7.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3.1"></times><ci id="S3.SS2.p3.7.m7.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3.2">𝑣</ci><cn type="integer" id="S3.SS2.p3.7.m7.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p3.7.m7.2.2.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">…</ci><apply id="S3.SS2.p3.7.m7.3.3.2.2.2.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.3.3.2.2.2.1.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2">subscript</csymbol><apply id="S3.SS2.p3.7.m7.3.3.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.3.3.2.2.2.2.1.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2">superscript</csymbol><ci id="S3.SS2.p3.7.m7.3.3.2.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2.2.2">𝐴</ci><ci id="S3.SS2.p3.7.m7.3.3.2.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.7.m7.3.3.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3"><times id="S3.SS2.p3.7.m7.3.3.2.2.2.3.1.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3.1"></times><ci id="S3.SS2.p3.7.m7.3.3.2.2.2.3.2.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3.2">𝑣</ci><ci id="S3.SS2.p3.7.m7.3.3.2.2.2.3.3.cmml" xref="S3.SS2.p3.7.m7.3.3.2.2.2.3.3">𝑇</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.3c">\bm{A^{i}}=\{A^{i}_{v1},\ldots,A^{i}_{vT}\}</annotation></semantics></math>.
Based on the attention probability of different region proposal, i.e., <math id="S3.SS2.p3.8.m8.3" class="ltx_Math" alttext="A^{i}_{v1},\ldots,A^{i}_{vT}" display="inline"><semantics id="S3.SS2.p3.8.m8.3a"><mrow id="S3.SS2.p3.8.m8.3.3.2" xref="S3.SS2.p3.8.m8.3.3.3.cmml"><msubsup id="S3.SS2.p3.8.m8.2.2.1.1" xref="S3.SS2.p3.8.m8.2.2.1.1.cmml"><mi id="S3.SS2.p3.8.m8.2.2.1.1.2.2" xref="S3.SS2.p3.8.m8.2.2.1.1.2.2.cmml">A</mi><mrow id="S3.SS2.p3.8.m8.2.2.1.1.3" xref="S3.SS2.p3.8.m8.2.2.1.1.3.cmml"><mi id="S3.SS2.p3.8.m8.2.2.1.1.3.2" xref="S3.SS2.p3.8.m8.2.2.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m8.2.2.1.1.3.1" xref="S3.SS2.p3.8.m8.2.2.1.1.3.1.cmml">​</mo><mn id="S3.SS2.p3.8.m8.2.2.1.1.3.3" xref="S3.SS2.p3.8.m8.2.2.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p3.8.m8.2.2.1.1.2.3" xref="S3.SS2.p3.8.m8.2.2.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS2.p3.8.m8.3.3.2.3" xref="S3.SS2.p3.8.m8.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">…</mi><mo id="S3.SS2.p3.8.m8.3.3.2.4" xref="S3.SS2.p3.8.m8.3.3.3.cmml">,</mo><msubsup id="S3.SS2.p3.8.m8.3.3.2.2" xref="S3.SS2.p3.8.m8.3.3.2.2.cmml"><mi id="S3.SS2.p3.8.m8.3.3.2.2.2.2" xref="S3.SS2.p3.8.m8.3.3.2.2.2.2.cmml">A</mi><mrow id="S3.SS2.p3.8.m8.3.3.2.2.3" xref="S3.SS2.p3.8.m8.3.3.2.2.3.cmml"><mi id="S3.SS2.p3.8.m8.3.3.2.2.3.2" xref="S3.SS2.p3.8.m8.3.3.2.2.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m8.3.3.2.2.3.1" xref="S3.SS2.p3.8.m8.3.3.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p3.8.m8.3.3.2.2.3.3" xref="S3.SS2.p3.8.m8.3.3.2.2.3.3.cmml">T</mi></mrow><mi id="S3.SS2.p3.8.m8.3.3.2.2.2.3" xref="S3.SS2.p3.8.m8.3.3.2.2.2.3.cmml">i</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.3b"><list id="S3.SS2.p3.8.m8.3.3.3.cmml" xref="S3.SS2.p3.8.m8.3.3.2"><apply id="S3.SS2.p3.8.m8.2.2.1.1.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.2.2.1.1.1.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1">subscript</csymbol><apply id="S3.SS2.p3.8.m8.2.2.1.1.2.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.2.2.1.1.2.1.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1">superscript</csymbol><ci id="S3.SS2.p3.8.m8.2.2.1.1.2.2.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1.2.2">𝐴</ci><ci id="S3.SS2.p3.8.m8.2.2.1.1.2.3.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.8.m8.2.2.1.1.3.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1.3"><times id="S3.SS2.p3.8.m8.2.2.1.1.3.1.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1.3.1"></times><ci id="S3.SS2.p3.8.m8.2.2.1.1.3.2.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1.3.2">𝑣</ci><cn type="integer" id="S3.SS2.p3.8.m8.2.2.1.1.3.3.cmml" xref="S3.SS2.p3.8.m8.2.2.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">…</ci><apply id="S3.SS2.p3.8.m8.3.3.2.2.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.3.3.2.2.1.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2">subscript</csymbol><apply id="S3.SS2.p3.8.m8.3.3.2.2.2.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.3.3.2.2.2.1.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2">superscript</csymbol><ci id="S3.SS2.p3.8.m8.3.3.2.2.2.2.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2.2.2">𝐴</ci><ci id="S3.SS2.p3.8.m8.3.3.2.2.2.3.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.8.m8.3.3.2.2.3.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2.3"><times id="S3.SS2.p3.8.m8.3.3.2.2.3.1.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2.3.1"></times><ci id="S3.SS2.p3.8.m8.3.3.2.2.3.2.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2.3.2">𝑣</ci><ci id="S3.SS2.p3.8.m8.3.3.2.2.3.3.cmml" xref="S3.SS2.p3.8.m8.3.3.2.2.3.3">𝑇</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.3c">A^{i}_{v1},\ldots,A^{i}_{vT}</annotation></semantics></math>, we weigh the corresponding pixel intensities in an image matrix and then normalize this image matrix to get the final attention map over the image, conditioned on the question. We do this for all <math id="S3.SS2.p3.9.m9.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS2.p3.9.m9.1a"><mn id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><cn type="integer" id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">6</annotation></semantics></math> co-attention layers to get attention maps <math id="S3.SS2.p3.10.m10.3" class="ltx_Math" alttext="M^{1},\ldots,M^{6}" display="inline"><semantics id="S3.SS2.p3.10.m10.3a"><mrow id="S3.SS2.p3.10.m10.3.3.2" xref="S3.SS2.p3.10.m10.3.3.3.cmml"><msup id="S3.SS2.p3.10.m10.2.2.1.1" xref="S3.SS2.p3.10.m10.2.2.1.1.cmml"><mi id="S3.SS2.p3.10.m10.2.2.1.1.2" xref="S3.SS2.p3.10.m10.2.2.1.1.2.cmml">M</mi><mn id="S3.SS2.p3.10.m10.2.2.1.1.3" xref="S3.SS2.p3.10.m10.2.2.1.1.3.cmml">1</mn></msup><mo id="S3.SS2.p3.10.m10.3.3.2.3" xref="S3.SS2.p3.10.m10.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml">…</mi><mo id="S3.SS2.p3.10.m10.3.3.2.4" xref="S3.SS2.p3.10.m10.3.3.3.cmml">,</mo><msup id="S3.SS2.p3.10.m10.3.3.2.2" xref="S3.SS2.p3.10.m10.3.3.2.2.cmml"><mi id="S3.SS2.p3.10.m10.3.3.2.2.2" xref="S3.SS2.p3.10.m10.3.3.2.2.2.cmml">M</mi><mn id="S3.SS2.p3.10.m10.3.3.2.2.3" xref="S3.SS2.p3.10.m10.3.3.2.2.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.3b"><list id="S3.SS2.p3.10.m10.3.3.3.cmml" xref="S3.SS2.p3.10.m10.3.3.2"><apply id="S3.SS2.p3.10.m10.2.2.1.1.cmml" xref="S3.SS2.p3.10.m10.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.2.2.1.1.1.cmml" xref="S3.SS2.p3.10.m10.2.2.1.1">superscript</csymbol><ci id="S3.SS2.p3.10.m10.2.2.1.1.2.cmml" xref="S3.SS2.p3.10.m10.2.2.1.1.2">𝑀</ci><cn type="integer" id="S3.SS2.p3.10.m10.2.2.1.1.3.cmml" xref="S3.SS2.p3.10.m10.2.2.1.1.3">1</cn></apply><ci id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1">…</ci><apply id="S3.SS2.p3.10.m10.3.3.2.2.cmml" xref="S3.SS2.p3.10.m10.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.3.3.2.2.1.cmml" xref="S3.SS2.p3.10.m10.3.3.2.2">superscript</csymbol><ci id="S3.SS2.p3.10.m10.3.3.2.2.2.cmml" xref="S3.SS2.p3.10.m10.3.3.2.2.2">𝑀</ci><cn type="integer" id="S3.SS2.p3.10.m10.3.3.2.2.3.cmml" xref="S3.SS2.p3.10.m10.3.3.2.2.3">6</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.3c">M^{1},\ldots,M^{6}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Comparison Metric</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">We use rank-correlation (denoted by <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\rho</annotation></semantics></math> in the visualization figures) to compare ViLBERT’s attention with human attention <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite>. Both attention maps are scaled to <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn type="integer" id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">14</annotation></semantics></math> x <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mn id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><cn type="integer" id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">14</annotation></semantics></math> and then flattened to get a <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="196" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mn id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">196</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><cn type="integer" id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">196</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">196</annotation></semantics></math> dimensional vector. These two vectors are then ranked based on their spatial attention and then we compute the correlation between the two rank vectors. All reported rank-correlation values except Question POS tag experiments (Sec. <a href="#S4.SS3" title="4.3 Nouns drive attention ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>), show averages over <math id="S3.SS3.p1.5.m5.2" class="ltx_Math" alttext="1,374" display="inline"><semantics id="S3.SS3.p1.5.m5.2a"><mrow id="S3.SS3.p1.5.m5.2.3.2" xref="S3.SS3.p1.5.m5.2.3.1.cmml"><mn id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">1</mn><mo id="S3.SS3.p1.5.m5.2.3.2.1" xref="S3.SS3.p1.5.m5.2.3.1.cmml">,</mo><mn id="S3.SS3.p1.5.m5.2.2" xref="S3.SS3.p1.5.m5.2.2.cmml">374</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.2b"><list id="S3.SS3.p1.5.m5.2.3.1.cmml" xref="S3.SS3.p1.5.m5.2.3.2"><cn type="integer" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">1</cn><cn type="integer" id="S3.SS3.p1.5.m5.2.2.cmml" xref="S3.SS3.p1.5.m5.2.2">374</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.2c">1,374</annotation></semantics></math> question/image pairs from the VQA-HAT <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> validation set.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2201.03965/assets/Images/illustration.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="546" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of our attention map generation process.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Similarity to human attention shows a small dependence on the number of region proposals</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We investigated the influence of the number of region proposals on the model’s ability to examine task-relevant regions.
Since humans rely on context to solve a problem, we hypothesize that more region proposals bring in more task-relevant context from the image, thus increasing the rank-correlation of the model’s attention to that of humans and, in turn, increasing the answering accuracy. We show the rank-correlation of ViLBERT’s <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> attention maps with human attention maps across successive co-attention layers in <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.1 Similarity to human attention shows a small dependence on the number of region proposals ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span> for varying numbers of region proposals. To put results in perspective, we compare the results against an upper bound given by the rank-correlation for inter-human comparisons and a lower bound given by random attention allocation.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Increasing the number of region proposals led layers 3-6 of the model to attend to regions more similar to those attended by humans. The increased context due to more region proposals also improved the model’s VQA accuracy (<span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Similarity to human attention shows a small dependence on the number of region proposals ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span> and examples in <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Similarity to human attention shows a small dependence on the number of region proposals ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span>). The region proposals are generated using Faster RCNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>, an object detection architecture. Therefore, even in the first co-attention layer, which has little interaction with the language stream, the rank-correlation of the model’s visual attention with human attention is well above chance. The correlation in the lower layers is likely due to the observation that the majority of the questions in the VQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite> focus either on object categories or object attributes that are salient in terms of basic visual features.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2201.03965/assets/Images/rank_corr_region_proposals_plot.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="353" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The similarity between ViLBERT and human attention benefits from more region proposals.<span id="S4.F3.4.2.1" class="ltx_text ltx_font_medium">
The rank-correlation of ViLBERT’s <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> attention with human attention increases monotonically up to layer 4 (see section <a href="#S4.SS1" title="4.1 Similarity to human attention shows a small dependence on the number of region proposals ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> for details). Error bars showing standard error of means are smaller than the symbol size in this plot.</span></span></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/qualitative_viz_roi_677652_3155652_1.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="3186" height="947" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/qualitative_viz_roi_677652_3155652_2.png" id="S4.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="3186" height="798" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/colorbar.png" id="S4.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="720" height="173" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization for cases where increasing number of region proposals increases accuracy as well as rank-correlation with human attention.<span id="S4.F4.5.2.1" class="ltx_text ltx_font_medium">
The question and answers are shown above and below the images.
Column 1: input image, Column 2: human attention map. Columns 3, 4, 5: ViLBERT’s <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> attention map for 36, 72, and 108 region proposals. The bottom colormap describes the intensity of the attention maps. Additional visualizations are provided in </span>Appendix A.1<span id="S4.F4.5.2.2" class="ltx_text ltx_font_medium">.</span></span></figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Given a fixed number of region proposals, the rank-correlation increases monotonically until layer 4 and then stays approximately constant. This initial increase validates the crucial role of co-attention layers in guiding visual attention in the model. Additionally, increasing the number of region proposals captures objects’ features using multiple aspect ratios and scales, often helping the model to better attend to the object in question, as depicted in the example in <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Similarity to human attention shows a small dependence on the number of region proposals ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span> (row 2).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">VQA accuracy of ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> with different number of region proposals. Accuracies are computed over all the question/image pairs in the VQA-HAT <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> validation set.
<br class="ltx_break"></span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.2.1" class="ltx_tr">
<td id="S4.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> (36 Region Proposals)</td>
<td id="S4.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">76.57</td>
</tr>
<tr id="S4.T1.4.3.2" class="ltx_tr">
<td id="S4.T1.4.3.2.1" class="ltx_td ltx_align_center">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> (72 Region Proposals)</td>
<td id="S4.T1.4.3.2.2" class="ltx_td ltx_align_center">79.39</td>
</tr>
<tr id="S4.T1.4.4.3" class="ltx_tr">
<td id="S4.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> (108 Region Proposals)</td>
<td id="S4.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">80.83</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Words matter more than grammar or semantics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Next, we evaluated the influence of question semantics in driving the visual attention mechanism. Given a question/image pair, we randomly shuffled the order of words in the question and then forward propagated the question and the image through the ViLBERT model <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. For instance, a question like ’What color is the floor?’ could become ’Is color floor what the?’. The new question makes no semantic or grammatical sense. The shuffling procedure was done only at test time, while the model was trained with the words in the original order.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">We expected that the rank-correlation of the model’s attention with human attention for these modified questions should drop along with the VQA accuracy. However, the results did not match our expectations (<span id="S4.SS2.p2.2.1" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a></span>, and visualization examples in <span id="S4.SS2.p2.2.2" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a></span>). There was only a minimal drop in the degree of similarity of the attention maps upon shuffling the word order. For example, in <span id="S4.SS2.p2.2.3" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a></span> row 1, “What color is the floor?” led to the correct answer (brown) and <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\rho=0.548" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">ρ</mi><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">0.548</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝜌</ci><cn type="float" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">0.548</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\rho=0.548</annotation></semantics></math> and the shuffled version “Is color floor what the?” also led to the correct answer and <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\rho=0.556" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">ρ</mi><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">0.556</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><eq id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></eq><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝜌</ci><cn type="float" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">0.556</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\rho=0.556</annotation></semantics></math>. These results suggest that the question grammar and semantics play little to no role in modulating visual attention. Instead, the presence of specific keywords in the question is responsible for driving attention.
Most of the visual grounding here is based on object-centric concepts rather than the overall semantics of the question.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2201.03965/assets/Images/rank_corr_ques_sem_viz.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="353" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.8.3.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text" style="font-size:90%;">
<span id="S4.F5.4.2.1" class="ltx_text ltx_font_bold">The semantics of the question plays little role in driving the model’s attention map.</span>
Similarity between model and human attention maps (<math id="S4.F5.3.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.F5.3.1.m1.1b"><mi id="S4.F5.3.1.m1.1.1" xref="S4.F5.3.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.F5.3.1.m1.1c"><ci id="S4.F5.3.1.m1.1.1.cmml" xref="S4.F5.3.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.3.1.m1.1d">\rho</annotation></semantics></math>, using 36 region proposals) for each of the 6 co-attention layers for the default (normal) model (blue), for the shuffled words condition (orange), and a condition where the image is paired with a random question (green). The format is similar to <span id="S4.F5.4.2.2" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.1 Similarity to human attention shows a small dependence on the number of region proposals ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span>, showing the between-human upper bound and the random levels.
There is minimal change in <math id="S4.F5.4.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.F5.4.2.m2.1b"><mi id="S4.F5.4.2.m2.1.1" xref="S4.F5.4.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.F5.4.2.m2.1c"><ci id="S4.F5.4.2.m2.1.1.cmml" xref="S4.F5.4.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.4.2.m2.1d">\rho</annotation></semantics></math> after shuffling the words, indicating that semantics has little influence on ViLBERT’s <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> attention. </span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The model’s VQA accuracy dropped considerably after shuffling the words (<span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span>). Thus, while attention seems to be largely independent of grammar and semantics, the ability to answer the questions correctly does require some notion of grammar and/or semantic information.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/ques_sem_viz.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="549" height="589" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/colorbar.png" id="S4.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="720" height="173" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.7.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization for different question/image pairs and their corresponding attention maps across multiple controls<span id="S4.F6.8.2.1" class="ltx_text ltx_font_medium">. Column 1 shows the input image, column 2 contains the human attention maps and Column 3, 4, and 5 show ViLBERT’s <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> attention map for </span>Normal<span id="S4.F6.8.2.2" class="ltx_text ltx_font_medium">, </span>Shuffled_Words<span id="S4.F6.8.2.3" class="ltx_text ltx_font_medium">, and </span>Unrelated Question/Image Pair<span id="S4.F6.8.2.4" class="ltx_text ltx_font_medium"> conditions, respectively. The answers in bold are ground-truth and the predicted answers are not in bold (see </span>Appendix A.2<span id="S4.F6.8.2.5" class="ltx_text ltx_font_medium"> for extended analyses).
</span></span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">VQA accuracy of ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> in different controls. Note that the reported accuracy is over question/image pairs in VQA-HAT <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> validation set. Refer section <a href="#S4.SS2" title="4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> for more details.
<br class="ltx_break"></span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<th id="S4.T2.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.2.1" class="ltx_tr">
<th id="S4.T2.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> (Normal)</th>
<td id="S4.T2.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">76.57</td>
</tr>
<tr id="S4.T2.4.3.2" class="ltx_tr">
<th id="S4.T2.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> (Shuffled Words)</th>
<td id="S4.T2.4.3.2.2" class="ltx_td ltx_align_center">60.2</td>
</tr>
<tr id="S4.T2.4.4.3" class="ltx_tr">
<th id="S4.T2.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> (Unrelated Question/Image Pair)</th>
<td id="S4.T2.4.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">10.8</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Given that attention was not dependent on the semantic content, we wondered whether it is possible that the model was focusing exclusively on visual information and simply ignoring the language part to drive attention allocation. To assess this possibility, we paired images with another randomly chosen question and compared the human attention maps with a given image/question pair and the model attention maps with the same image but a random question (<span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a></span>).
The rank-correlation in the case of Unrelated Question/Image Pair was largely driven by the visual input, any contribution from language in this case would be spurious.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.2" class="ltx_p">Following the example in <span id="S4.SS2.p5.2.1" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a></span>, row 1, the same image but using the question “Is this singles or doubles?” (instead of “What color is the floor?”), led to the erroneous answer “singles” and <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="\rho=0.02" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mrow id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml"><mi id="S4.SS2.p5.1.m1.1.1.2" xref="S4.SS2.p5.1.m1.1.1.2.cmml">ρ</mi><mo id="S4.SS2.p5.1.m1.1.1.1" xref="S4.SS2.p5.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p5.1.m1.1.1.3" xref="S4.SS2.p5.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><apply id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1"><eq id="S4.SS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1.1"></eq><ci id="S4.SS2.p5.1.m1.1.1.2.cmml" xref="S4.SS2.p5.1.m1.1.1.2">𝜌</ci><cn type="float" id="S4.SS2.p5.1.m1.1.1.3.cmml" xref="S4.SS2.p5.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">\rho=0.02</annotation></semantics></math> (cf. <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="\rho=0.548" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mrow id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml"><mi id="S4.SS2.p5.2.m2.1.1.2" xref="S4.SS2.p5.2.m2.1.1.2.cmml">ρ</mi><mo id="S4.SS2.p5.2.m2.1.1.1" xref="S4.SS2.p5.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p5.2.m2.1.1.3" xref="S4.SS2.p5.2.m2.1.1.3.cmml">0.548</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><apply id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1"><eq id="S4.SS2.p5.2.m2.1.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1.1"></eq><ci id="S4.SS2.p5.2.m2.1.1.2.cmml" xref="S4.SS2.p5.2.m2.1.1.2">𝜌</ci><cn type="float" id="S4.SS2.p5.2.m2.1.1.3.cmml" xref="S4.SS2.p5.2.m2.1.1.3">0.548</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">\rho=0.548</annotation></semantics></math> for the correct question/image pair). The similarity with human attention was largely independent of the layer number but remained well above chance levels in the case of Unrelated Question/Image Pair (<span id="S4.SS2.p5.2.2" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a></span>). Visual attention alone is sufficient to drive the rank-correlation with humans. Interestingly, even the unrelated question case shows higher similarity than previous benchmarks that combined visual and correct language information (<span id="S4.SS2.p5.2.3" class="ltx_text ltx_font_bold">Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Better performing VQA models show higher correlation with human attention maps ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span>). For layers 3-6, the similarity with human attention dropped considerably with respect to the correct question condition. Thus, attention is largely dictated by visual information, combined with focused co-attention driven by the presence of specific key words irrespective of their ordering.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Nouns drive attention</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We quantified the importance of different parts of speech (POS) in guiding the model’s attention to task-relevant image regions. Given a question and the corresponding image, we dropped words with a certain POS tag. For example, the question “what is the girl holding?” would become “what is the holding?” upon removing nouns.
Then, we forward propagated the image and the modified question through the network and generated the corresponding attention maps, and computed the rank-correlation
with the human attention maps. Similar to <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>, we group POS tags into the following categories: Noun, Pronoun, Verb, Adjective, Preposition, Determiner, and Wh-Words. The Wh-Words category includes WP, WDT, and WRB tags containing words like who, which, and where respectively. We show the results of this experiment in <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.3 Nouns drive attention ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a></span>, using 36 region proposals.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2201.03965/assets/Images/question_pos_plot.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="546" height="255" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.5.2.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.2.1" class="ltx_text" style="font-size:90%;">
<span id="S4.F7.2.1.1" class="ltx_text ltx_font_bold">Removing nouns, and to a lesser degree prepositions, led to a drop in similarity of attention maps</span>. Rank-correlation with human attention map (<math id="S4.F7.2.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.F7.2.1.m1.1b"><mi id="S4.F7.2.1.m1.1.1" xref="S4.F7.2.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.F7.2.1.m1.1c"><ci id="S4.F7.2.1.m1.1.1.cmml" xref="S4.F7.2.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.2.1.m1.1d">\rho</annotation></semantics></math>) for each of the 6 co-attention layers upon removing different parts of speech (blue). The reduction in rank-correlation was maximal in the case of nouns, followed by prepositions and pronouns. Other parts of speech had little effect on the rank-correlation. Rank-correlation values shown here were averaged over question/image pairs containing words from the corresponding category (see Section <a href="#S4.SS3" title="4.3 Nouns drive attention ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> for details). Error bars showing standard error of means are smaller than the symbol size in this plot.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Consistent with our findings in Section <a href="#S4.SS2" title="4.2 Words matter more than grammar or semantics ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> that words are more important than semantics, we noticed that nouns specifically played an important role in driving visual attention, followed by prepositions and pronouns. Given a question, nouns often help the model filter the relevant object categories from all the object region proposals. In addition, prepositions sometimes help guide attention based on spatial relationships between objects (see <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Appendix A.3</span> for visualizations and additional qualitative results).</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Better performing VQA models show higher correlation with human attention maps</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Better performing VQA models show higher correlation with human attention maps ‣ 4 Experiments ‣ On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span>, we show the VQA accuracy and rank-correlation of the model’s attention maps and human attention maps for the following networks: ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, Stacked Attention Network <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite> with 2 attention layers (SAN-2), Hierarchical Co-Attention Network <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> with Word-Level (HieCoAtt-W), Phrase-Level (HieCoAtt-P), and Question-Level (HieCoAtt-Q). ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> uses a multi-modal transformer architecture while SAN-2 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite> and HieCoAtt <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> are based on CNN and LSTM architectures. The rank-correlation for the CNN/LSTM based models is considerably lower than the transformer-based model indicating a superior co-attention mechanism and better fusion of vision and language information in multi-modal transformers. Finally, it’s interesting also to note that an increase in the VQA accuracy is accompanied by a better correlation with human attention.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Accuracy for different VQA models on the VQA test-std set as reported in <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib25" title="" class="ltx_ref">2016</a>; Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. Error bars in rank-correlation here show standard error of means.
<br class="ltx_break"></span></figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.4.1.1" class="ltx_tr">
<td id="S4.T3.4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Method</td>
<td id="S4.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Rank-Correlation</td>
<td id="S4.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">VQA Accuracy</td>
</tr>
<tr id="S4.T3.4.2.2" class="ltx_tr">
<td id="S4.T3.4.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Random</td>
<td id="S4.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">0.000 ± 0.001</td>
<td id="S4.T3.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.4.3.3" class="ltx_tr">
<td id="S4.T3.4.3.3.1" class="ltx_td ltx_align_center ltx_border_t">SAN-2 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S4.T3.4.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0.249 ± 0.004</td>
<td id="S4.T3.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t">58.9</td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t">HieCoAtt-W <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">0.246 ± 0.004</td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T3.4.5.5" class="ltx_tr">
<td id="S4.T3.4.5.5.1" class="ltx_td ltx_align_center">HieCoAtt-P <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S4.T3.4.5.5.2" class="ltx_td ltx_align_center">0.256 ± 0.004</td>
<td id="S4.T3.4.5.5.3" class="ltx_td ltx_align_center">62.1</td>
</tr>
<tr id="S4.T3.4.6.6" class="ltx_tr">
<td id="S4.T3.4.6.6.1" class="ltx_td ltx_align_center">HieCoAtt-Q <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S4.T3.4.6.6.2" class="ltx_td ltx_align_center">0.264 ± 0.004</td>
<td id="S4.T3.4.6.6.3" class="ltx_td"></td>
</tr>
<tr id="S4.T3.4.7.7" class="ltx_tr">
<td id="S4.T3.4.7.7.1" class="ltx_td ltx_align_center ltx_border_t">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S4.T3.4.7.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.7.7.2.1" class="ltx_text ltx_font_bold">0.434 ± 0.006</span></td>
<td id="S4.T3.4.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.7.7.3.1" class="ltx_text ltx_font_bold">70.92</span></td>
</tr>
<tr id="S4.T3.4.8.8" class="ltx_tr">
<td id="S4.T3.4.8.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Human</td>
<td id="S4.T3.4.8.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.618 ± 0.006</td>
<td id="S4.T3.4.8.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion &amp; Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We conducted a series of experiments to interpret and study co-attention transformer layers and their role in aiding rich cross-modal interactions. We probed the modulation from language to vision in these co-attention layers and compared them with human attention maps. Transformer models lead to a substantial improvement in the similarity of attention maps with humans.
In addition, the attention maps of VQA models with higher accuracy are better correlated with human attention maps
Interestingly, the overall question semantics play a minimal role in guiding visual attention. Attention is governed by the visual inputs and by the presence of key nouns in the question.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The interpretability of multi-modal transformers has received little attention, despite their notable success in terms of performance metrics. While we are enthusiastic about recent advancements in Vision-Language models, it is also critical and instructive to examine transformer layers carefully. We illustrate through visualizations the observation that the object-based region proposals often act as a bottleneck and prevent the network from looking at task-relevant regions. There remains a large gap in accuracy between state-of-the-art VQA models and human performance. At the same time, even though our results demonstrate that co-attention transformer layers yield a large boost to the congruency of attentional modulation in models and humans with respect to previous baselines, there is also a gap in the similarity of attention maps. We argue that this two gaps are related: building models that better capture human attention maps, perhaps by emphasizing the role of word combinations and semantics, can bring fundamental improvements in future VQA networks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. [2016]</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Analyzing the behavior of visual question answering models.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.07356</em>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chefer et al. [2021]</span>
<span class="ltx_bibblock">
Hila Chefer, Shir Gur, and Lior Wolf.

</span>
<span class="ltx_bibblock">Generic attention-model explainability for interpreting bi-modal and
encoder-decoder transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.15679</em>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 104–120.
Springer, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chuang et al. [2019]</span>
<span class="ltx_bibblock">
Yung-Sung Chuang, Chi-Liang Liu, Hung-yi Lee, and Lin-shan Lee.

</span>
<span class="ltx_bibblock">Speechbert: An audio-and-text jointly learned language model for
end-to-end spoken question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.11559</em>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. [2016]</span>
<span class="ltx_bibblock">
Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Human Attention in Visual Question Answering: Do Humans and Deep
Networks Look at the Same Regions?

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing (EMNLP)</em>, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2018]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gabeur et al. [2020]</span>
<span class="ltx_bibblock">
Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Multi-modal transformer for video retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, volume 5.
Springer, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2016]</span>
<span class="ltx_bibblock">
Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Towards transparent ai systems: Interpreting visual question
answering models.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1608.08974</em>, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2017]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabri et al. [2016]</span>
<span class="ltx_bibblock">
Allan Jabri, Armand Joulin, and Laurens Van Der Maaten.

</span>
<span class="ltx_bibblock">Revisiting visual question answering baselines.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 727–739.
Springer, 2016.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan [2017]</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan.

</span>
<span class="ltx_bibblock">An analysis of visual question answering algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pages 1965–1973, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019a]</span>
<span class="ltx_bibblock">
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou.

</span>
<span class="ltx_bibblock">Unicoder-vl: A universal encoder for vision and language by
cross-modal pre-training, 2019a.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019b]</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.03557</em>, 2019b.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020]</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, et al.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for vision-language
tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 121–137.
Springer, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740–755.
Springer, 2014.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2016]</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.00061</em>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2019]</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
13–23, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2020]</span>
<span class="ltx_bibblock">
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.

</span>
<span class="ltx_bibblock">12-in-1: Multi-task vision and language representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">The IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. [2016]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 39(6):1137–1149, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2019]</span>
<span class="ltx_bibblock">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Vl-bert: Pre-training of generic visual-linguistic representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.08530</em>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2019]</span>
<span class="ltx_bibblock">
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Videobert: A joint model for video and language representation
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 7464–7473, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal [2019]</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07490</em>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.03762</em>, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2016]</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 21–29, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2019]</span>
<span class="ltx_bibblock">
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng
Gao.

</span>
<span class="ltx_bibblock">Unified vision-language pre-training for image captioning and vqa,
2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Additional qualitative results</h3>

<figure id="A1.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/failure_cases.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="432" height="687" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/colorbar.png" id="A1.F8.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="720" height="173" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A1.F8.3.2" class="ltx_text" style="font-size:90%;">Row 1: high rank-correlation with 100% accuracy, Row 2: high rank-correlation with 0% accuracy, Row 3: low rank-correlation with 100% accuracy, Row 4: low rank-correlation with 0% accuracy. Column 1 shows the input image, column 2 contains the human attention maps, and column 3 shows ViLBERT’s attention map. The answers in bold are ground-truth and the predicted answers are not in bold.
</span></figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Object region proposals act as a bottleneck</h3>

<figure id="A1.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/Ques_ROI_appendix.png" id="A1.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="549" height="707" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/colorbar.png" id="A1.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="720" height="173" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A1.F9.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization for cases where number of regions proposals act as a bottleneck and restrict the network from attending to task-relevant regions.<span id="A1.F9.4.2.1" class="ltx_text ltx_font_medium">
Column 1 shows the input image, column 2 contains the human attention maps, and Column 3,4, and 5 show ViLBERT’s attention map for 36, 72, and 108 regions respectively. The answers in bold are ground-truth and the predicted answers are not in bold.
</span></span></figcaption>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Question semantics play little role in visual attention</h3>

<figure id="A1.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/qualitative_ques_sem_appendix_fin.png" id="A1.F10.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="549" height="707" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/colorbar.png" id="A1.F10.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="720" height="173" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F10.6.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A1.F10.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Additional visualizations for different question/image pairs and their corresponding attention
maps across multiple controls.<span id="A1.F10.7.2.1" class="ltx_text ltx_font_medium">
Column 1 shows the input image, column 2 contains the human attention maps, and Column 3,4, and 5 show ViLBERT’s attention map for </span>Normal<span id="A1.F10.7.2.2" class="ltx_text ltx_font_medium">, </span>Shuffled_Words<span id="A1.F10.7.2.3" class="ltx_text ltx_font_medium">, and </span>Unrelated Question/Image Pair<span id="A1.F10.7.2.4" class="ltx_text ltx_font_medium"> conditions, respectively. The answers in bold are ground-truth and the predicted answers are not in bold.
</span></span></figcaption>
</figure>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Importance of certain POS tags in guiding model’s attention</h3>

<figure id="A1.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/noun_appendix_up.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="549" height="189" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/preposition_appendix.png" id="A1.F11.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="549" height="181" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/pronoun_appendix.png" id="A1.F11.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="549" height="181" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/verb_appendix.png" id="A1.F11.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="549" height="181" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2201.03965/assets/Images/colorbar.png" id="A1.F11.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="720" height="173" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A1.F11.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization for different question/image pairs and their corresponding attention
maps after dropping words with certain POS tags.<span id="A1.F11.4.2.1" class="ltx_text ltx_font_medium"> Row 1: Nouns dropped, Row 2: Prepositions dropped, Row 3: Pronouns dropped, Row 4: Verbs dropped. The answers in bold are ground-truth and the predicted answers are not in bold.</span></span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2201.03964" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2201.03965" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2201.03965">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2201.03965" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2201.03966" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 10:31:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
