<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1604.01485] A Focused Dynamic Attention Model for Visual Question Answering</title><meta property="og:description" content="Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines.
Solving VQA problems requires techniques from both computer vision for understanding the visual conten…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Focused Dynamic Attention Model for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Focused Dynamic Attention Model for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1604.01485">

<!--Generated on Sat Mar 16 14:43:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Visual Question Answering,  Attention">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>National University of Singapore</span></span></span>
<h1 class="ltx_title ltx_title_document">A Focused Dynamic Attention Model for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ilija Ilievski
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Shuicheng Yan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Jiashi Feng
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ilija.ilievski@u.nus.edu, {eleyans,elefjia}@nus.edu.sg</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines.
Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers.
Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects.
Extracting features from auto-generated regions – as some region-based image recognition methods do – cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question.
In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions.
Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit.
Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers.
Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Visual Question Answering, Attention
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA) is an active research direction that lies in the intersection of computer vision, natural language processing, and machine learning.
Even though with a very short history, it already has received great research attention from multiple communities.
Generally, the VQA investigates a generalization of traditional QA problems where visual input (<em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, an image) is necessary to be considered.
More concretely, VQA is about how to provide a correct answer to a human posed question concerning contents of one presented image or video.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA is a quite challenging task and undoubtedly important for developing modern AI systems.
The VQA problem can be regarded as a Visual Turing Test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and besides contributing to the advancement of the involved research areas, it has other important applications, such as blind person assistance and image retrieval.
Coming up with solutions to this task requires natural language processing techniques for understanding the questions and generating the answers, as well as computer vision techniques for understanding contents of the concerned image.
With help of these two core techniques, the computer can perform reasoning about the perceived contents and posed questions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, VQA is advanced significantly by the development of machine learning methods (in particular the deep learning ones) that can learn proper representations of questions and images, align and fuse them in a joint question-image space and provide a direct mapping from this joint representation to a correct answer.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">For example, consider the following image-question pair: an image of an apple tree with a basket of apples next to it, and a question “How many apples are in the basket?”.
Answering this question requires VQA methods to first understand the semantics of the question, then locate the objects (apples) in the image, understand the relation between the image objects (which apples are in the basket), and finally count them and generate an answer with the correct number of apples.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The first feasible solution to VQA problems was provided by Malinowski and Fritz in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, where they used a semantic language parser and a Bayesian reasoning model, to understand the meaning of questions and to generate the proper answers.
Malinowski and Fritz also constructed the first VQA benchmark dataset, named as DAQUAR, which contains 1,449 images and 12,468 questions generated by humans or automatically by following a template and extracting facts from a database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Shortly after, Ren et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> released the TORONTO–QA dataset, which contains a large number of images (123,287) and questions (117,684), but the questions are automatically generated and thus can be answered without complex reasoning.
Nevertheless, the release of the TORONTO–QA dataset was important since it provided enough data for deep learning models to be trained and evaluated on the VQA problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
More recently, Antol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> published the currently largest VQA dataset.
It consists of three human posed questions and ten answers given by different human subjects, for each one of the 204,721 images found in the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Answering the 614,163 questions requires complex reasoning, common sense, and real-world knowledge, making the VQA dataset suitable for a true Visual Turing Test.
The VQA authors split the evaluation on their dataset on two tasks: an open-ended task, where the method should generate a natural language answer, and a multiple-choice task, where for each question the method should chose one of the 18 different answers.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The current top performing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> to extract image features and a Long Short-Term Memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> network to extract the representations for questions.
The CNN and LSTM representation vectors are then usually fused by concatenation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> or element-wise multiplication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
Other approaches additionally incorporate some kind of attention mechanism over the image features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Properly modeling the image contents is one of the critical factors for solving VQA problems well.
A common practice with existing VQA methods on modeling image contents is to extract global features for the overall image.
However, only using global feature is arguably insufficient to capture all the necessary visual information and provide full understanding of image contents such as multiple objects, spatial configuration of the objects and informative background.
This issue can be relieved to some extent by extracting features from object proposals – the image regions that possibly contain objects of interest.
However, using features from all image regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> may provide too much noise or overwhelming information irrelevant to the question and thus hurt the overall VQA performance.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In this work, we propose a question driven attention model that is able to automatically identify and focus on image regions relevant for the current question.
We name our proposed model Focused Dynamic Attention (FDA) for Visual Question Answering.
With the FDA model, computers can select and recognize the image regions in a well-aligned sequence with the key words containing in a given question.
Recall the above VQA example.
To answer the question of “How many apples are in the basket?”, FDA would first localize the regions corresponding to the key words “apples” and “basket” (with the help of a generic object detector) and extract description features from these regions of interest.
Then VQA compliments the features from selected image regions with a global image feature providing contextual information for the overall image, and reconstruct a visual representation by encoding them with a Long Short-Term Memory (LSTM) unit.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">We evaluate and compare the performance of our proposed FDA model on two types of VQA tasks, <em id="S1.p9.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, the open-ended task and the multiple-choice task, on the VQA dataset – the largest VQA benchmark dataset.
Extensive experiments demonstrate that FDA brings substantial performance improvement upon well-established baselines.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">The main contributions of this work can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a focused dynamic attention mechanism that learns to use the question word order to shift the focus from one image object, to another.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We describe a model that fuses local and global context visual features with textual features.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We perform an extensive evaluation, comparing to all existing methods, and achieve state-of-the-art accuracy on the open-ended, and on the multiple-choice VQA tasks.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">The rest of the paper is organized as follows.
In Section <a href="#S2" title="2 Related Work ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we review the current VQA models, and compare them to our model.
We formulate the problem and explain our motivation in Section <a href="#S3" title="3 Method Overview ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We describe our model in Section <a href="#S4" title="4 Focused Dynamic Attention for VQA ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and in Section <a href="#S5" title="5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we evaluate and compare it with the current state-of-the-art models.
We conclude our work in Section <a href="#S6" title="6 Conclusion ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">VQA has received great research attention recently and a couple of methods have been developed to solve this problem.
The most similar model to ours is the Stacked Attention Networks (SAN) proposed by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Both models use attention mechanism that combines the words and image regions.
However, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> use convolutional neural network to put attention over the image regions, based on the question word unigrams, bigrams, and trigrams.
Further, their attention mechanism is not using object bounding boxes, which makes the attention less focused.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Another model that uses attention mechanism in solving VQA problems is the ABC-CNN model described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
ABC-CNN uses the question embedding to configure convolutional kernels that will define an attention weighted map over the image features.
The advantage of our FDA model over ABC-CNN is two fold.
First, FDA employs an LSTM network to encode the image region features in a order that corresponds to the question word order.
Second, FDA does not put handcrafted weights on the image features (a practice showed to hurt the learning process in our experiments). Instead, FDA extracts CNN features directly from the cropped image regions of interest. In this sense, FDA is more efficient than ABC-CNN in visual contents modeling.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Yet another attention model for visual question answering is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
The work, is closely related to the work by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, in that it also applies a weighted map over the image and the question word features.
However, similar to our work, they use object proposals from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to select image regions instead of the whole image.
Different from that work, our proposed FDA model also employs the information embedded in the order of the question words and focuses on the corresponding object bounding boxes.
In contrast, the model proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> straightforwardly concatenate all the image region features with the question word features and feed them all at once to a two layer network.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Jiang et al. propose another model that combines the CNN image features and an LSTM network for encoding the multimodal representation, with the addition of a Compositional Memory units which fuse the image and word feature vectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Ma et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> take an interesting approach and use three convolutional neural networks to represent not only the image, but also the question, and their common representation in a multimodal space.
The multimodal representation is then fed to a SoftMax layer to produce the answer.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Another interesting approach worth mentioning is the work by Andreas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
They use a semantic grammar parser to parse the question and propose neural network layouts accordingly.
They train a model to learn to compose a network from one of the proposed network layouts using several types of neural modules, each specifically designed to address the different sub-tasks of the VQA problem (e.g. counting, locating an object, etc.).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method Overview</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we briefly describe the motivation and give formal problem formulation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Formulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">The visual question answering problem can be represented as predicting the best answer <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\hat{a}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mover accent="true" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">a</mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><ci id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">^</ci><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\hat{a}</annotation></semantics></math> given an image <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">I</annotation></semantics></math> and a question <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">q</annotation></semantics></math>.
Common practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is to use the 1,000 most common answers in the training set and thus simplify the VQA task to a classification problem.
The following equation represents the problem mathematically:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\boldsymbol{\hat{a}}=\operatorname*{arg\,max}_{\boldsymbol{a}\in\Omega}p(\boldsymbol{a}|\boldsymbol{I},\boldsymbol{q};\boldsymbol{\theta})" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mover accent="true" id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><mi id="S3.E1.m1.4.4.3.2" xref="S3.E1.m1.4.4.3.2.cmml">𝒂</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E1.m1.4.4.3.1" xref="S3.E1.m1.4.4.3.1.cmml">^</mo></mover><mo id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.cmml"><mrow id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.4.4.1.3.cmml"><munder id="S3.E1.m1.4.4.1.3.1" xref="S3.E1.m1.4.4.1.3.1.cmml"><mrow id="S3.E1.m1.4.4.1.3.1.2" xref="S3.E1.m1.4.4.1.3.1.2.cmml"><mi id="S3.E1.m1.4.4.1.3.1.2.2" xref="S3.E1.m1.4.4.1.3.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E1.m1.4.4.1.3.1.2.1" xref="S3.E1.m1.4.4.1.3.1.2.1.cmml">​</mo><mi id="S3.E1.m1.4.4.1.3.1.2.3" xref="S3.E1.m1.4.4.1.3.1.2.3.cmml">max</mi></mrow><mrow id="S3.E1.m1.4.4.1.3.1.3" xref="S3.E1.m1.4.4.1.3.1.3.cmml"><mi id="S3.E1.m1.4.4.1.3.1.3.2" xref="S3.E1.m1.4.4.1.3.1.3.2.cmml">𝒂</mi><mo id="S3.E1.m1.4.4.1.3.1.3.1" xref="S3.E1.m1.4.4.1.3.1.3.1.cmml">∈</mo><mi mathvariant="normal" id="S3.E1.m1.4.4.1.3.1.3.3" xref="S3.E1.m1.4.4.1.3.1.3.3.cmml">Ω</mi></mrow></munder><mo id="S3.E1.m1.4.4.1.3a" xref="S3.E1.m1.4.4.1.3.cmml">⁡</mo><mi id="S3.E1.m1.4.4.1.3.2" xref="S3.E1.m1.4.4.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">𝒂</mi><mo fence="false" id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝑰</mi><mo id="S3.E1.m1.4.4.1.1.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">𝒒</mi><mo id="S3.E1.m1.4.4.1.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.3.1.cmml">;</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">𝜽</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"></eq><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><ci id="S3.E1.m1.4.4.3.1.cmml" xref="S3.E1.m1.4.4.3.1">bold-^</ci><ci id="S3.E1.m1.4.4.3.2.cmml" xref="S3.E1.m1.4.4.3.2">𝒂</ci></apply><apply id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.4.1"><times id="S3.E1.m1.4.4.1.2.cmml" xref="S3.E1.m1.4.4.1.2"></times><apply id="S3.E1.m1.4.4.1.3.cmml" xref="S3.E1.m1.4.4.1.3"><apply id="S3.E1.m1.4.4.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.3.1.1.cmml" xref="S3.E1.m1.4.4.1.3.1">subscript</csymbol><apply id="S3.E1.m1.4.4.1.3.1.2.cmml" xref="S3.E1.m1.4.4.1.3.1.2"><times id="S3.E1.m1.4.4.1.3.1.2.1.cmml" xref="S3.E1.m1.4.4.1.3.1.2.1"></times><ci id="S3.E1.m1.4.4.1.3.1.2.2.cmml" xref="S3.E1.m1.4.4.1.3.1.2.2">arg</ci><ci id="S3.E1.m1.4.4.1.3.1.2.3.cmml" xref="S3.E1.m1.4.4.1.3.1.2.3">max</ci></apply><apply id="S3.E1.m1.4.4.1.3.1.3.cmml" xref="S3.E1.m1.4.4.1.3.1.3"><in id="S3.E1.m1.4.4.1.3.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3.1.3.1"></in><ci id="S3.E1.m1.4.4.1.3.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.1.3.2">𝒂</ci><ci id="S3.E1.m1.4.4.1.3.1.3.3.cmml" xref="S3.E1.m1.4.4.1.3.1.3.3">Ω</ci></apply></apply><ci id="S3.E1.m1.4.4.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.2">𝑝</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2">𝒂</ci><list id="S3.E1.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑰</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝒒</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝜽</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\boldsymbol{\hat{a}}=\operatorname*{arg\,max}_{\boldsymbol{a}\in\Omega}p(\boldsymbol{a}|\boldsymbol{I},\boldsymbol{q};\boldsymbol{\theta})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.2" class="ltx_p">where <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\Omega" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi mathvariant="normal" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Ω</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">Ω</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\Omega</annotation></semantics></math> is the set of all possible answers and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\theta</annotation></semantics></math> are the model weights.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Motivation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The baseline methods from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> show only modest increase in accuracy when including the image features (4.98% for open-ended questions, and 2.42% for multiple-choice question).
We believe that the image contains a lot more information and should increase the accuracy much more.
Thus, we focus on improving the image features and design a visual attention mechanism, which learns to focus on the question related image regions.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The proposed attention mechanism is loosely inspired on the human visual attention mechanism.
Humans shift the focus from one image region to another, before understanding how the regions relate to each other and grasping the meaning of the whole image.
Similarly, we feed our model image regions relevant for the question at hand, before showing the whole image.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1604.01485/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="222" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Focused dynamic attention model diagram.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Focused Dynamic Attention for VQA</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The FDA model is composed of question and image understanding components, attention mechanism, and a multimodal representation fusion network (Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Motivation ‣ 3 Method Overview ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
In this section we describe them individually.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Question Understanding</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Following a common practice, our FDA model uses an LSTM network to encode the question in a vector representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
The LSTM network learns to keep in its state the feature vectors of the important question words, and thus provides the question understanding component with a word attention mechanism.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Image Understanding</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Following prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we use a pre-trained convolutional neural network (CNN) to extract image feature vectors.
Specifically, we use the Deep Residual Networks model used in ILSVRC and COCO 2015 competitions, which won the 1<sup id="S4.SS2.p1.1.1" class="ltx_sup">st</sup> places in: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
We extract the weights of the layer immediately before the final SoftMax layer and regard them as visual features.
We extract such features for the whole image (global visual features) and for the specific image regions (local visual features).
However, contrary to the existing approaches, we employ an LSTM network to combine the local and global visual features into a joint representation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Focused Dynamic Attention Mechanism</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We introduce a focused dynamic attention mechanism that learns to focus on image regions related to the question words.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The attention mechanism works as follows.
For each image object<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>During training we use the ground truth object bounding boxes and labels. At test time we use the precomputed bounding boxes from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and classify them with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to obtain the object labels.</span></span></span> it uses word2vec word embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to measure the similarity between the question words and the object label.
Next, it selects objects with similarity score greater than 0.5 and extracts the feature vectors of the objects bounding boxes with a pre-trained ResNet model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Following the question word order, it feeds the LSTM network with the corresponding object feature vectors.
Finally, it feeds the LSTM network with the feature vector of the whole image and it uses the resulting LSTM state as a visual representation.
Thus, the attention mechanism enables the model to combine the local and global visual features into a single representation, necessary for answering complex visual questions.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Motivation ‣ 3 Method Overview ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the focused dynamic attention mechanism with an example.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Multimodal Representation Fusion</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We regard the final state of the two LSTM networks as a question and image representation.
We start fusing them into single representation by applying <em id="S4.SS4.p1.1.1" class="ltx_emph ltx_font_italic">Tanh</em> on the question representation and <em id="S4.SS4.p1.1.2" class="ltx_emph ltx_font_italic">ReLU</em><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Defined as <math id="footnote2.m1.4" class="ltx_Math" alttext="f(x)=\max(0,x)" display="inline"><semantics id="footnote2.m1.4b"><mrow id="footnote2.m1.4.5" xref="footnote2.m1.4.5.cmml"><mrow id="footnote2.m1.4.5.2" xref="footnote2.m1.4.5.2.cmml"><mi id="footnote2.m1.4.5.2.2" xref="footnote2.m1.4.5.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="footnote2.m1.4.5.2.1" xref="footnote2.m1.4.5.2.1.cmml">​</mo><mrow id="footnote2.m1.4.5.2.3.2" xref="footnote2.m1.4.5.2.cmml"><mo stretchy="false" id="footnote2.m1.4.5.2.3.2.1" xref="footnote2.m1.4.5.2.cmml">(</mo><mi id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">x</mi><mo stretchy="false" id="footnote2.m1.4.5.2.3.2.2" xref="footnote2.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="footnote2.m1.4.5.1" xref="footnote2.m1.4.5.1.cmml">=</mo><mrow id="footnote2.m1.4.5.3.2" xref="footnote2.m1.4.5.3.1.cmml"><mi id="footnote2.m1.2.2" xref="footnote2.m1.2.2.cmml">max</mi><mo id="footnote2.m1.4.5.3.2b" xref="footnote2.m1.4.5.3.1.cmml">⁡</mo><mrow id="footnote2.m1.4.5.3.2.1" xref="footnote2.m1.4.5.3.1.cmml"><mo stretchy="false" id="footnote2.m1.4.5.3.2.1.1" xref="footnote2.m1.4.5.3.1.cmml">(</mo><mn id="footnote2.m1.3.3" xref="footnote2.m1.3.3.cmml">0</mn><mo id="footnote2.m1.4.5.3.2.1.2" xref="footnote2.m1.4.5.3.1.cmml">,</mo><mi id="footnote2.m1.4.4" xref="footnote2.m1.4.4.cmml">x</mi><mo stretchy="false" id="footnote2.m1.4.5.3.2.1.3" xref="footnote2.m1.4.5.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote2.m1.4c"><apply id="footnote2.m1.4.5.cmml" xref="footnote2.m1.4.5"><eq id="footnote2.m1.4.5.1.cmml" xref="footnote2.m1.4.5.1"></eq><apply id="footnote2.m1.4.5.2.cmml" xref="footnote2.m1.4.5.2"><times id="footnote2.m1.4.5.2.1.cmml" xref="footnote2.m1.4.5.2.1"></times><ci id="footnote2.m1.4.5.2.2.cmml" xref="footnote2.m1.4.5.2.2">𝑓</ci><ci id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1">𝑥</ci></apply><apply id="footnote2.m1.4.5.3.1.cmml" xref="footnote2.m1.4.5.3.2"><max id="footnote2.m1.2.2.cmml" xref="footnote2.m1.2.2"></max><cn type="integer" id="footnote2.m1.3.3.cmml" xref="footnote2.m1.3.3">0</cn><ci id="footnote2.m1.4.4.cmml" xref="footnote2.m1.4.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.4d">f(x)=\max(0,x)</annotation></semantics></math>.</span></span></span> on the image representation <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Applying different activation functions gave slightly worse overall results</span></span></span>.
We proceed by doing an element-wise multiplication of the two vector representations and the resulting vector is fed to a fully-connected neural network.
Finally a SoftMax layer classify the multimodal representation into one of the possible<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and use the 1000 most common answers</span></span></span> answers.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section we detail the model implementation and compare our model against the current state-of-the-art methods.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/1604.01485/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="234" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Representative examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for two images from the VQA dataset. Examples provided by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">For all experiments we use the Visual Question Answering (VQA) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which is the largest and most complex image dataset for the visual question answering task.
The dataset contains three human posed questions and ten answers given by different human subjects, for each one of the 204,721 images found in the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows two representative examples found in the dataset.
The evaluation is done on following two test splits test-dev and test-std and on following two tasks:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">An open-ended task, where the method should generate a natural language answer;</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">A multiple-choice task, where for each question the method should chose one of the 18 different answers.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We evaluate the performance of all the methods in the experiments using the public evaluation server for fair evaluation.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Baseline Model</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We compare our model against the baseline models provided by the VQA dataset authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which currently achieve the best performance on the test-standard split for the multiple-choice task.
The model, first described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, is a standard implementation of an LSTM<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><plus id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">+</annotation></semantics></math>CNN VQA model.
It uses an LSTM to encode the question and CNN features to encode the image.
To answer a question it multiplies the last LSTM state with the image CNN features and feeds the result into a SoftMax layer for classification into one of the 1,000 most common answers.
The implementation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> uses a deeper two layer LSTM network for encoding the question, and normalized image CNN features, which showed crucial for achieving the state-of-the-art.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison between the baselines from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, the state-of-the-art models and our FDA model on VQA test-dev and test-standard data for the <span id="S5.T1.2.1" class="ltx_text ltx_font_bold">open-ended</span> task. Results from most recent methods including CM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, iBOWIMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, D-NMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, D-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> are provided and compared with. </figcaption>
<table id="S5.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.3.1.1" class="ltx_tr">
<th id="S5.T1.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S5.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="4">test-dev</th>
<td id="S5.T1.3.1.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">test-std</td>
</tr>
<tr id="S5.T1.3.2.2" class="ltx_tr">
<th id="S5.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="S5.T1.3.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">All</th>
<th id="S5.T1.3.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Y/N</th>
<td id="S5.T1.3.2.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Other</td>
<td id="S5.T1.3.2.2.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Num</td>
<td id="S5.T1.3.2.2.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">All</td>
</tr>
<tr id="S5.T1.3.3.3" class="ltx_tr">
<th id="S5.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">VQA</th>
<th id="S5.T1.3.3.3.2" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S5.T1.3.3.3.3" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<td id="S5.T1.3.3.3.4" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T1.3.3.3.5" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T1.3.3.3.6" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
</tr>
<tr id="S5.T1.3.4.4" class="ltx_tr">
<th id="S5.T1.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Question</th>
<th id="S5.T1.3.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">48.09</th>
<th id="S5.T1.3.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">75.66</th>
<td id="S5.T1.3.4.4.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">27.14</td>
<td id="S5.T1.3.4.4.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">36.70</td>
<td id="S5.T1.3.4.4.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T1.3.5.5" class="ltx_tr">
<th id="S5.T1.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Image</th>
<th id="S5.T1.3.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">28.13</th>
<th id="S5.T1.3.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">64.01</th>
<td id="S5.T1.3.5.5.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">3.77</td>
<td id="S5.T1.3.5.5.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.42</td>
<td id="S5.T1.3.5.5.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T1.3.6.6" class="ltx_tr">
<th id="S5.T1.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Q+I</th>
<th id="S5.T1.3.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">52.64</th>
<th id="S5.T1.3.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">75.55</th>
<td id="S5.T1.3.6.6.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">37.37</td>
<td id="S5.T1.3.6.6.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">33.67</td>
<td id="S5.T1.3.6.6.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T1.3.7.7" class="ltx_tr">
<th id="S5.T1.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">LSTM Q+I</th>
<th id="S5.T1.3.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">53.74</th>
<th id="S5.T1.3.7.7.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">78.94</th>
<td id="S5.T1.3.7.7.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">36.42</td>
<td id="S5.T1.3.7.7.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">35.24</td>
<td id="S5.T1.3.7.7.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">54.06</td>
</tr>
<tr id="S5.T1.3.8.8" class="ltx_tr">
<th id="S5.T1.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">CM</th>
<th id="S5.T1.3.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">52.62</th>
<th id="S5.T1.3.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">78.33</th>
<td id="S5.T1.3.8.8.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">35.93</td>
<td id="S5.T1.3.8.8.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">34.46</td>
<td id="S5.T1.3.8.8.6" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
</tr>
<tr id="S5.T1.3.9.9" class="ltx_tr">
<th id="S5.T1.3.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">ACK</th>
<th id="S5.T1.3.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">55.72</th>
<th id="S5.T1.3.9.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">79.23</th>
<td id="S5.T1.3.9.9.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">40.08</td>
<td id="S5.T1.3.9.9.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">36.13</td>
<td id="S5.T1.3.9.9.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">55.98</td>
</tr>
<tr id="S5.T1.3.10.10" class="ltx_tr">
<th id="S5.T1.3.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">iBOWIMG</th>
<th id="S5.T1.3.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">55.72</th>
<th id="S5.T1.3.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">76.55</th>
<td id="S5.T1.3.10.10.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">42.62</td>
<td id="S5.T1.3.10.10.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">35.03</td>
<td id="S5.T1.3.10.10.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">55.89</td>
</tr>
<tr id="S5.T1.3.11.11" class="ltx_tr">
<th id="S5.T1.3.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">DPPnet</th>
<th id="S5.T1.3.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">57.22</th>
<th id="S5.T1.3.11.11.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">80.71</th>
<td id="S5.T1.3.11.11.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">41.71</td>
<td id="S5.T1.3.11.11.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">37.24</td>
<td id="S5.T1.3.11.11.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">57.36</td>
</tr>
<tr id="S5.T1.3.12.12" class="ltx_tr">
<th id="S5.T1.3.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">D-NMN</th>
<th id="S5.T1.3.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">57.90</th>
<th id="S5.T1.3.12.12.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">80.50</th>
<td id="S5.T1.3.12.12.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">43.10</td>
<td id="S5.T1.3.12.12.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">37.40</td>
<td id="S5.T1.3.12.12.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">58.00</td>
</tr>
<tr id="S5.T1.3.13.13" class="ltx_tr">
<th id="S5.T1.3.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">D-LSTM</th>
<th id="S5.T1.3.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">-</th>
<th id="S5.T1.3.13.13.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">-</th>
<td id="S5.T1.3.13.13.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T1.3.13.13.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T1.3.13.13.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">58.16</td>
</tr>
<tr id="S5.T1.3.14.14" class="ltx_tr">
<th id="S5.T1.3.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">SAN</th>
<th id="S5.T1.3.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">58.70</th>
<th id="S5.T1.3.14.14.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">79.30</th>
<td id="S5.T1.3.14.14.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">46.10</td>
<td id="S5.T1.3.14.14.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">36.6</td>
<td id="S5.T1.3.14.14.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">58.90</td>
</tr>
<tr id="S5.T1.3.15.15" class="ltx_tr">
<th id="S5.T1.3.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">FDA</th>
<th id="S5.T1.3.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T1.3.15.15.2.1" class="ltx_text ltx_font_bold">59.24</span></th>
<th id="S5.T1.3.15.15.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">81.14</th>
<td id="S5.T1.3.15.15.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">45.77</td>
<td id="S5.T1.3.15.15.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">36.16</td>
<td id="S5.T1.3.15.15.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T1.3.15.15.6.1" class="ltx_text ltx_font_bold">59.54</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison between the baselines from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, the state-of-the-art models and our FDA model on VQA test-dev and test-standard data for the <span id="S5.T2.2.1" class="ltx_text ltx_font_bold">multiple-choice</span> task. Results from most recent methods including WR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, iBOWIMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and D-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> are also shown for comparison. </figcaption>
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.3.1.1" class="ltx_tr">
<td id="S5.T2.3.1.1.1" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="4">test-dev</td>
<td id="S5.T2.3.1.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">test-std</td>
</tr>
<tr id="S5.T2.3.2.2" class="ltx_tr">
<td id="S5.T2.3.2.2.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Method</td>
<td id="S5.T2.3.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">All</td>
<td id="S5.T2.3.2.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Y/N</td>
<td id="S5.T2.3.2.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Other</td>
<td id="S5.T2.3.2.2.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Num</td>
<td id="S5.T2.3.2.2.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">All</td>
</tr>
<tr id="S5.T2.3.3.3" class="ltx_tr">
<td id="S5.T2.3.3.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">VQA</td>
<td id="S5.T2.3.3.3.2" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T2.3.3.3.3" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T2.3.3.3.4" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T2.3.3.3.5" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T2.3.3.3.6" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
</tr>
<tr id="S5.T2.3.4.4" class="ltx_tr">
<td id="S5.T2.3.4.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Question</td>
<td id="S5.T2.3.4.4.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">53.68</td>
<td id="S5.T2.3.4.4.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">75.71</td>
<td id="S5.T2.3.4.4.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">38.64</td>
<td id="S5.T2.3.4.4.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">37.05</td>
<td id="S5.T2.3.4.4.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T2.3.5.5" class="ltx_tr">
<td id="S5.T2.3.5.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Image</td>
<td id="S5.T2.3.5.5.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">30.53</td>
<td id="S5.T2.3.5.5.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">69.87</td>
<td id="S5.T2.3.5.5.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">3.76</td>
<td id="S5.T2.3.5.5.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.45</td>
<td id="S5.T2.3.5.5.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T2.3.6.6" class="ltx_tr">
<td id="S5.T2.3.6.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Q+I</td>
<td id="S5.T2.3.6.6.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">58.97</td>
<td id="S5.T2.3.6.6.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">75.59</td>
<td id="S5.T2.3.6.6.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">50.33</td>
<td id="S5.T2.3.6.6.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">34.35</td>
<td id="S5.T2.3.6.6.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T2.3.7.7" class="ltx_tr">
<td id="S5.T2.3.7.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">LSTM Q+I</td>
<td id="S5.T2.3.7.7.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">57.17</td>
<td id="S5.T2.3.7.7.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">78.95</td>
<td id="S5.T2.3.7.7.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">43.41</td>
<td id="S5.T2.3.7.7.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">35.80</td>
<td id="S5.T2.3.7.7.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">57.57</td>
</tr>
<tr id="S5.T2.3.8.8" class="ltx_tr">
<td id="S5.T2.3.8.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">WR</td>
<td id="S5.T2.3.8.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">60.96</td>
<td id="S5.T2.3.8.8.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T2.3.8.8.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T2.3.8.8.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T2.3.8.8.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S5.T2.3.9.9" class="ltx_tr">
<td id="S5.T2.3.9.9.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">iBOWIMG</td>
<td id="S5.T2.3.9.9.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">61.68</td>
<td id="S5.T2.3.9.9.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">76.68</td>
<td id="S5.T2.3.9.9.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">54.44</td>
<td id="S5.T2.3.9.9.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">38.94</td>
<td id="S5.T2.3.9.9.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">61.97</td>
</tr>
<tr id="S5.T2.3.10.10" class="ltx_tr">
<td id="S5.T2.3.10.10.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DPPnet</td>
<td id="S5.T2.3.10.10.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">62.48</td>
<td id="S5.T2.3.10.10.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">80.79</td>
<td id="S5.T2.3.10.10.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">52.16</td>
<td id="S5.T2.3.10.10.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">38.94</td>
<td id="S5.T2.3.10.10.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">62.69</td>
</tr>
<tr id="S5.T2.3.11.11" class="ltx_tr">
<td id="S5.T2.3.11.11.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">D-LSTM</td>
<td id="S5.T2.3.11.11.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T2.3.11.11.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T2.3.11.11.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T2.3.11.11.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S5.T2.3.11.11.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">63.09</td>
</tr>
<tr id="S5.T2.3.12.12" class="ltx_tr">
<td id="S5.T2.3.12.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">FDA</td>
<td id="S5.T2.3.12.12.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T2.3.12.12.2.1" class="ltx_text ltx_font_bold">64.01</span></td>
<td id="S5.T2.3.12.12.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">81.50</td>
<td id="S5.T2.3.12.12.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">54.72</td>
<td id="S5.T2.3.12.12.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">39.00</td>
<td id="S5.T2.3.12.12.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T2.3.12.12.6.1" class="ltx_text ltx_font_bold">64.18</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Model Implementation and Training Details</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We transform the question words into a vector form by multiplying one-hot vector representation with a word embedding matrix.
The vocabulary size is 12,602 and the word embeddings are 300 dimensional.
We feed a pre-trained ResNet network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and use the 2,048 dimensional weight vector of the layer before the last fully-connected layer.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The word and image vectors are feed into two separate LSTM networks.
The LSTM networks are standard implementation of one layer LSTM network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, with a 512 dimensional state vector.
The final state of the question LSTM is passed through <em id="S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">Tanh</em>, while the final state of the image LSTM is passed through ReLU<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Defined as <math id="footnote5.m1.4" class="ltx_Math" alttext="f(x)=\max(0,x)" display="inline"><semantics id="footnote5.m1.4b"><mrow id="footnote5.m1.4.5" xref="footnote5.m1.4.5.cmml"><mrow id="footnote5.m1.4.5.2" xref="footnote5.m1.4.5.2.cmml"><mi id="footnote5.m1.4.5.2.2" xref="footnote5.m1.4.5.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="footnote5.m1.4.5.2.1" xref="footnote5.m1.4.5.2.1.cmml">​</mo><mrow id="footnote5.m1.4.5.2.3.2" xref="footnote5.m1.4.5.2.cmml"><mo stretchy="false" id="footnote5.m1.4.5.2.3.2.1" xref="footnote5.m1.4.5.2.cmml">(</mo><mi id="footnote5.m1.1.1" xref="footnote5.m1.1.1.cmml">x</mi><mo stretchy="false" id="footnote5.m1.4.5.2.3.2.2" xref="footnote5.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="footnote5.m1.4.5.1" xref="footnote5.m1.4.5.1.cmml">=</mo><mrow id="footnote5.m1.4.5.3.2" xref="footnote5.m1.4.5.3.1.cmml"><mi id="footnote5.m1.2.2" xref="footnote5.m1.2.2.cmml">max</mi><mo id="footnote5.m1.4.5.3.2b" xref="footnote5.m1.4.5.3.1.cmml">⁡</mo><mrow id="footnote5.m1.4.5.3.2.1" xref="footnote5.m1.4.5.3.1.cmml"><mo stretchy="false" id="footnote5.m1.4.5.3.2.1.1" xref="footnote5.m1.4.5.3.1.cmml">(</mo><mn id="footnote5.m1.3.3" xref="footnote5.m1.3.3.cmml">0</mn><mo id="footnote5.m1.4.5.3.2.1.2" xref="footnote5.m1.4.5.3.1.cmml">,</mo><mi id="footnote5.m1.4.4" xref="footnote5.m1.4.4.cmml">x</mi><mo stretchy="false" id="footnote5.m1.4.5.3.2.1.3" xref="footnote5.m1.4.5.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote5.m1.4c"><apply id="footnote5.m1.4.5.cmml" xref="footnote5.m1.4.5"><eq id="footnote5.m1.4.5.1.cmml" xref="footnote5.m1.4.5.1"></eq><apply id="footnote5.m1.4.5.2.cmml" xref="footnote5.m1.4.5.2"><times id="footnote5.m1.4.5.2.1.cmml" xref="footnote5.m1.4.5.2.1"></times><ci id="footnote5.m1.4.5.2.2.cmml" xref="footnote5.m1.4.5.2.2">𝑓</ci><ci id="footnote5.m1.1.1.cmml" xref="footnote5.m1.1.1">𝑥</ci></apply><apply id="footnote5.m1.4.5.3.1.cmml" xref="footnote5.m1.4.5.3.2"><max id="footnote5.m1.2.2.cmml" xref="footnote5.m1.2.2"></max><cn type="integer" id="footnote5.m1.3.3.cmml" xref="footnote5.m1.3.3">0</cn><ci id="footnote5.m1.4.4.cmml" xref="footnote5.m1.4.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m1.4d">f(x)=\max(0,x)</annotation></semantics></math>.</span></span></span>.
We do element-wise multiplication on the resulting vectors, to obtain a multimodal representation vector, which is then fed to a fully-connected neural network.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Model Evaluation and Comparison</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We compare our model with the baselines provided by the VQA authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
The results for the open-ended task are listed in Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Baseline Model ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and the results for the multiple-choice task are given in Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Baseline Model ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
In the tables, the “Question” and “Image” baselines are only using the question words and the image, respectively.
The “Q+I” is a baseline that combines the two, but do not use an LSTM network.
“LSTM Q+I” and “D-LSTM” are LSTM models, with one and two layers accordingly.
Comparing the performance of baselines we can observe the accuracy increase with the addition of information from each modality.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">From Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Baseline Model ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, one can observe that our proposed FDA model achieves the best performance on this benchmark dataset.
It outperforms the state-of-the-art (SAN) with a margin of around <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="0.6\%" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mn id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">0.6</mn><mo id="S5.SS4.p2.1.m1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS4.p2.1.m1.1.1.2.cmml" xref="S5.SS4.p2.1.m1.1.1.2">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">0.6\%</annotation></semantics></math>.
The SAN model also employs attention to focus on specific regions.
However, their attention model (without access to the automatically generated bounding boxes) is focusing on more spread regions which may include cluttered and noisy background.
In contrast, FDA only focuses on the selected regions and extracts more clean information for answering the questions.
This is the main reason that FDA can outperform SAN although these two methods are both based on attention models.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">The advantage of employing focused dynamic attention in FDA is more significant when solving the multiple-choice VQA problems.
From Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Baseline Model ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, one can observe that our proposed FDA model achieves the best ever performance on the VQA dataset.
In particular, it improves the performance of the state-of-the-art (D-LSTM) by a margin of <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="1.1\%" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mrow id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml"><mn id="S5.SS4.p3.1.m1.1.1.2" xref="S5.SS4.p3.1.m1.1.1.2.cmml">1.1</mn><mo id="S5.SS4.p3.1.m1.1.1.1" xref="S5.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><apply id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS4.p3.1.m1.1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS4.p3.1.m1.1.1.2.cmml" xref="S5.SS4.p3.1.m1.1.1.2">1.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">1.1\%</annotation></semantics></math> which is quite significant for this challenging task.
The D-LSTM method employs a deeper network to enhance the discriminative capacity of the visual features.
However, they do not identify the informative regions for answering the questions.
In contrast, FDA incorporates the automatic region localization by employing a question-driven attention model.
This is helpful for filtering out irrelevant noise, and establishing the correspondence between regions and candidate answers.
Thus FDA gives substantial performance improvement.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Qualitative Results</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">We qualitatively evaluate our model on a set of examples where complex reasoning and focusing on the relevant local visual features are needed for answering the question correctly.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ 5.5 Qualitative Results ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows particularly difficult examples (the predominant image color is <span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_bold">not</span> the correct answer) of “What color” type of questions.
But, by focusing on the question related image regions, the FDA model is still able to produce the correct answer.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">In Figure <a href="#S5.F4" title="Figure 4 ‣ 5.5 Qualitative Results ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we show examples where the model focuses on different regions from the <span id="S5.SS5.p3.1.1" class="ltx_text ltx_font_bold">same</span> image, depending on the words in the question.
Focusing on the right image region is crucial when answering unusual questions for an image (Row 1), questions about small image objects (Row 2), or when the most dominant image object partly occludes the question related region and can lead to a wrong answer (Row 3).</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p">Representative examples of questions that require image object identification are shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.5 Qualitative Results ‣ 5 Evaluation ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
We can observe that the focused attention enables the model to answer complex questions (Row 1, left) and counting questions (Row 1, right).
The question guided image object identification greatly simplifies the answering of questions like the ones shown in Row 2 and Row 3.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<table id="S5.F3.4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F3.2.2.2" class="ltx_tr">
<td id="S5.F3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/color_2.jpg" id="S5.F3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="198" alt="Refer to caption"></td>
<td id="S5.F3.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/color_4.jpg" id="S5.F3.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption"></td>
</tr>
<tr id="S5.F3.4.4.5.1" class="ltx_tr">
<td id="S5.F3.4.4.5.1.1" class="ltx_td ltx_align_left">
<span id="S5.F3.4.4.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F3.4.4.5.1.1.1.1" class="ltx_p"><span id="S5.F3.4.4.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">What color is the <span id="S5.F3.4.4.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">frisbee</span>?
<br class="ltx_break">- Red.
<br class="ltx_break"></span></span>
</span>
</td>
<td id="S5.F3.4.4.5.1.2" class="ltx_td ltx_align_left">
<span id="S5.F3.4.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F3.4.4.5.1.2.1.1" class="ltx_p"><span id="S5.F3.4.4.5.1.2.1.1.1" class="ltx_text" style="font-size:90%;">What color are the <span id="S5.F3.4.4.5.1.2.1.1.1.1" class="ltx_text ltx_font_bold">glass</span> items?
<br class="ltx_break">- Green.
<br class="ltx_break"></span></span>
</span>
</td>
</tr>
<tr id="S5.F3.4.4.4" class="ltx_tr">
<td id="S5.F3.3.3.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/color_1.jpg" id="S5.F3.3.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="191" alt="Refer to caption"></td>
<td id="S5.F3.4.4.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/color_3.jpg" id="S5.F3.4.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="192" alt="Refer to caption"></td>
</tr>
<tr id="S5.F3.4.4.6.2" class="ltx_tr">
<td id="S5.F3.4.4.6.2.1" class="ltx_td ltx_align_left">
<span id="S5.F3.4.4.6.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F3.4.4.6.2.1.1.1" class="ltx_p"><span id="S5.F3.4.4.6.2.1.1.1.1" class="ltx_text" style="font-size:90%;">What color is the <span id="S5.F3.4.4.6.2.1.1.1.1.1" class="ltx_text ltx_font_bold">mouse</span>?
<br class="ltx_break">- White.</span></span>
</span>
</td>
<td id="S5.F3.4.4.6.2.2" class="ltx_td ltx_align_left">
<span id="S5.F3.4.4.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F3.4.4.6.2.2.1.1" class="ltx_p"><span id="S5.F3.4.4.6.2.2.1.1.1" class="ltx_text" style="font-size:90%;">What color is the lady’s <span id="S5.F3.4.4.6.2.2.1.1.1.1" class="ltx_text ltx_font_bold">umbrella</span>?
<br class="ltx_break">- Blue</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Representative examples where focusing on the question related objects helps FDA answer “What color” type of questions.
The question words in bold have been matched with an image region.
The yellow region caption box contains the question word, followed by the region label, and in parenthesis their cosine similarity (see Section <a href="#S4.SS3" title="4.3 Focused Dynamic Attention Mechanism ‣ 4 Focused Dynamic Attention for VQA ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> for more details).
</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure">
<table id="S5.F4.6.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F4.2.2.2" class="ltx_tr">
<td id="S5.F4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_3.jpg" id="S5.F4.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="193" alt="Refer to caption"></td>
<td id="S5.F4.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_4.jpg" id="S5.F4.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="193" alt="Refer to caption"></td>
</tr>
<tr id="S5.F4.6.6.7.1" class="ltx_tr">
<td id="S5.F4.6.6.7.1.1" class="ltx_td ltx_align_left">
<span id="S5.F4.6.6.7.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F4.6.6.7.1.1.1.1" class="ltx_p"><span id="S5.F4.6.6.7.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Is this a birthday <span id="S5.F4.6.6.7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">cake</span>? 
<br class="ltx_break">- Yes.</span></span>
</span>
</td>
<td id="S5.F4.6.6.7.1.2" class="ltx_td ltx_align_left">
<span id="S5.F4.6.6.7.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F4.6.6.7.1.2.1.1" class="ltx_p"><span id="S5.F4.6.6.7.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Is someone in all likelihood, a <span id="S5.F4.6.6.7.1.2.1.1.1.1" class="ltx_text ltx_font_bold">zoo</span> fancier? - Yes.</span></span>
</span>
</td>
</tr>
<tr id="S5.F4.4.4.4" class="ltx_tr">
<td id="S5.F4.3.3.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_7.jpg" id="S5.F4.3.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption"></td>
<td id="S5.F4.4.4.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_8.jpg" id="S5.F4.4.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption"></td>
</tr>
<tr id="S5.F4.6.6.8.2" class="ltx_tr">
<td id="S5.F4.6.6.8.2.1" class="ltx_td ltx_align_left">
<span id="S5.F4.6.6.8.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F4.6.6.8.2.1.1.1" class="ltx_p"><span id="S5.F4.6.6.8.2.1.1.1.1" class="ltx_text" style="font-size:90%;">What <span id="S5.F4.6.6.8.2.1.1.1.1.1" class="ltx_text ltx_font_bold">fruit</span> is by the <span id="S5.F4.6.6.8.2.1.1.1.1.2" class="ltx_text ltx_font_bold">sink</span>? 
<br class="ltx_break">- Apples.</span></span>
</span>
</td>
<td id="S5.F4.6.6.8.2.2" class="ltx_td ltx_align_left">
<span id="S5.F4.6.6.8.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F4.6.6.8.2.2.1.1" class="ltx_p"><span id="S5.F4.6.6.8.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Is there a <span id="S5.F4.6.6.8.2.2.1.1.1.1" class="ltx_text ltx_font_bold">cookbook</span> in the picture?
<br class="ltx_break">- Yes.</span></span>
</span>
</td>
</tr>
<tr id="S5.F4.6.6.6" class="ltx_tr">
<td id="S5.F4.5.5.5.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_9.jpg" id="S5.F4.5.5.5.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption"></td>
<td id="S5.F4.6.6.6.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_10.jpg" id="S5.F4.6.6.6.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption"></td>
</tr>
<tr id="S5.F4.6.6.9.3" class="ltx_tr">
<td id="S5.F4.6.6.9.3.1" class="ltx_td ltx_align_left">
<span id="S5.F4.6.6.9.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F4.6.6.9.3.1.1.1" class="ltx_p"><span id="S5.F4.6.6.9.3.1.1.1.1" class="ltx_text" style="font-size:90%;">What type of <span id="S5.F4.6.6.9.3.1.1.1.1.1" class="ltx_text ltx_font_bold">vehicle</span> is pictured?
<br class="ltx_break">- Motorcycle.</span></span>
</span>
</td>
<td id="S5.F4.6.6.9.3.2" class="ltx_td ltx_align_left">
<span id="S5.F4.6.6.9.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F4.6.6.9.3.2.1.1" class="ltx_p"><span id="S5.F4.6.6.9.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Does the <span id="S5.F4.6.6.9.3.2.1.1.1.1" class="ltx_text ltx_font_bold">elephant</span> have tusks? 
<br class="ltx_break">- No.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Representative examples where the model focuses on different regions from the <span id="S5.F4.8.1" class="ltx_text ltx_font_bold">same</span> image, depending on the question.
The question words in bold have been matched with an image region.
The yellow region caption box contains the question word, followed by the region label, and in parenthesis their cosine similarity (see Section <a href="#S4.SS3" title="4.3 Focused Dynamic Attention Mechanism ‣ 4 Focused Dynamic Attention for VQA ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> for more details).
</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<table id="S5.F5.6.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F5.2.2.2" class="ltx_tr">
<td id="S5.F5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_1.jpg" id="S5.F5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="191" alt="Refer to caption"></td>
<td id="S5.F5.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/counting_1.jpg" id="S5.F5.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="193" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.6.6.7.1" class="ltx_tr">
<td id="S5.F5.6.6.7.1.1" class="ltx_td ltx_align_left">
<span id="S5.F5.6.6.7.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F5.6.6.7.1.1.1.1" class="ltx_p"><span id="S5.F5.6.6.7.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Is the <span id="S5.F5.6.6.7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">bus</span> in the middle of the <span id="S5.F5.6.6.7.1.1.1.1.1.2" class="ltx_text ltx_font_bold">intersection</span>? - Yes.</span></span>
</span>
</td>
<td id="S5.F5.6.6.7.1.2" class="ltx_td ltx_align_left">
<span id="S5.F5.6.6.7.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F5.6.6.7.1.2.1.1" class="ltx_p"><span id="S5.F5.6.6.7.1.2.1.1.1" class="ltx_text" style="font-size:90%;">How many <span id="S5.F5.6.6.7.1.2.1.1.1.1" class="ltx_text ltx_font_bold">cows</span> are present?
<br class="ltx_break">- 4.</span></span>
</span>
</td>
</tr>
<tr id="S5.F5.4.4.4" class="ltx_tr">
<td id="S5.F5.3.3.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_2.jpg" id="S5.F5.3.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="189" alt="Refer to caption"></td>
<td id="S5.F5.4.4.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_12.jpg" id="S5.F5.4.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="192" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.6.6.8.2" class="ltx_tr">
<td id="S5.F5.6.6.8.2.1" class="ltx_td ltx_align_left">
<span id="S5.F5.6.6.8.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F5.6.6.8.2.1.1.1" class="ltx_p"><span id="S5.F5.6.6.8.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Does this <span id="S5.F5.6.6.8.2.1.1.1.1.1" class="ltx_text ltx_font_bold">dessert</span> have any <span id="S5.F5.6.6.8.2.1.1.1.1.2" class="ltx_text ltx_font_bold">fruit</span> with it? - Yes.</span></span>
</span>
</td>
<td id="S5.F5.6.6.8.2.2" class="ltx_td ltx_align_left">
<span id="S5.F5.6.6.8.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F5.6.6.8.2.2.1.1" class="ltx_p"><span id="S5.F5.6.6.8.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Is this a <span id="S5.F5.6.6.8.2.2.1.1.1.1" class="ltx_text ltx_font_bold">carrot</span> <span id="S5.F5.6.6.8.2.2.1.1.1.2" class="ltx_text ltx_font_bold">cake</span>?
<br class="ltx_break">- Yes.</span></span>
</span>
</td>
</tr>
<tr id="S5.F5.6.6.6" class="ltx_tr">
<td id="S5.F5.5.5.5.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_6.jpg" id="S5.F5.5.5.5.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption"></td>
<td id="S5.F5.6.6.6.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1604.01485/assets/figure/sup/complex_5.jpg" id="S5.F5.6.6.6.2.g1" class="ltx_graphics ltx_img_landscape" width="278" height="210" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.6.6.9.3" class="ltx_tr">
<td id="S5.F5.6.6.9.3.1" class="ltx_td ltx_align_left">
<span id="S5.F5.6.6.9.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F5.6.6.9.3.1.1.1" class="ltx_p"><span id="S5.F5.6.6.9.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Is this an <span id="S5.F5.6.6.9.3.1.1.1.1.1" class="ltx_text ltx_font_bold">airport</span>?
<br class="ltx_break">- Yes.</span></span>
</span>
</td>
<td id="S5.F5.6.6.9.3.2" class="ltx_td ltx_align_left">
<span id="S5.F5.6.6.9.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F5.6.6.9.3.2.1.1" class="ltx_p"><span id="S5.F5.6.6.9.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Is the individual skiing or <span id="S5.F5.6.6.9.3.2.1.1.1.1" class="ltx_text ltx_font_bold">snowboarding</span>? - Snowboarding.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Representative examples of questions that require image object identification.
The question words in bold have been matched with an image region.
The yellow region caption box contains the question word, followed by the region label, and in parenthesis their cosine similarity (see Section <a href="#S4.SS3" title="4.3 Focused Dynamic Attention Mechanism ‣ 4 Focused Dynamic Attention for VQA ‣ A Focused Dynamic Attention Model for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> for more details).
</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we proposed a novel Focused Dynamic Attention (FDA) model to solve the challenging VQA problems.
FDA is built upon a generic object-centric attention model for extracting question related visual features from an image as well as a stack of multiple LSTM layers for feature fusion.
By only focusing on the identified regions specific for proposed questions, FDA was shown to be able to filter out overwhelming irrelevant informations from cluttered background or other regions, and thus substantially improved the quality of visual representations in the sense of answering proposed questions.
By fusing cleaned regional representation, global context and question representation via LSTM layers, FDA provided significant performance improvement over baselines on the VQA benchmark datasets, for both the open-ended and multiple-choices VQA tasks.
Excellent performance of FDA clearly demonstrates its stronger ability of modeling visual contents and also verifies paying more attention to visual part in VQA tasks could essentially improve the overall performance.
In the future, we are going to further explore along this research line and investigate different attention methods for visual information selection as well as better reasoning model for interpreting the relation between visual contents and questions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Geman, D., Geman, S., Hallonquist, N., Younes, L.:

</span>
<span class="ltx_bibblock">Visual turing test for computer vision systems.

</span>
<span class="ltx_bibblock">Proceedings of the National Academy of Sciences <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">112</span>(12)
(2015) 3618–3623

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Malinowski, M., Fritz, M.:

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems. (2014)
1682–1690

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ren, M., Kiros, R., Zemel, R.:

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems. (2015)
2935–2943

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., Xu, W.:

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual
image question.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems. (2015)
2287–2295

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Malinowski, M., Rohrbach, M., Fritz, M.:

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions
about images.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE International Conference on Computer
Vision. (2015) 1–9

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C.,
Parikh, D.:

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In: The IEEE International Conference on Computer Vision (ICCV).
(December 2015)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.:

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In: Computer Vision–ECCV 2014.

</span>
<span class="ltx_bibblock">Springer (2014) 740–755

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Noh, H., Seo, P.H., Han, B.:

</span>
<span class="ltx_bibblock">Image question answering using convolutional neural network with
dynamic parameter prediction.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.05756 (2015)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Wu, Q., Wang, P., Shen, C., Hengel, A.v.d., Dick, A.:

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.06973 (2015)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ma, L., Lu, Z., Li, H.:

</span>
<span class="ltx_bibblock">Learning to answer questions from image using convolutional neural
network.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1506.00333 (2015)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.,
Jackel, L.D.:

</span>
<span class="ltx_bibblock">Backpropagation applied to handwritten zip code recognition.

</span>
<span class="ltx_bibblock">Neural computation <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">1</span>(4) (1989) 541–551

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E.:

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2012)
1097–1105

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Simonyan, K., Zisserman, A.:

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1409.1556 (2014)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.:

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2015) 1–9

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hochreiter, S., Schmidhuber, J.:

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock">Neural computation <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">9</span>(8) (1997) 1735–1780

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhou, B., Tian, Y., Sukhbaatar, S., Szlam, A., Fergus, R.:

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1512.02167 (2015)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Shih, K.J., Singh, S., Hoiem, D.:

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.07394 (2015)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Chen, K., Wang, J., Chen, L.C., Gao, H., Xu, W., Nevatia, R.:

</span>
<span class="ltx_bibblock">Abc-cnn: An attention based convolutional neural network for visual
question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.05960 (2015)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yang, Z., He, X., Gao, J., Deng, L., Smola, A.:

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.02274 (2015)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Xu, H., Saenko, K.:

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.05234 (2015)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Zitnick, C.L., Dollár, P.:

</span>
<span class="ltx_bibblock">Edge boxes: Locating object proposals from edges.

</span>
<span class="ltx_bibblock">In: Computer Vision–ECCV 2014.

</span>
<span class="ltx_bibblock">Springer (2014) 391–405

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jiang, A., Wang, F., Porikli, F., Li, Y.:

</span>
<span class="ltx_bibblock">Compositional memory for visual question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1511.05676 (2015)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Andreas, J., Rohrbach, M., Darrell, T., Klein, D.:

</span>
<span class="ltx_bibblock">Learning to compose neural networks for question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1601.01705 (2016)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.:

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1512.03385 (2015)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Pont-Tuset, J., Arbeláez, P., Barron, J., Marques, F., Malik, J.:

</span>
<span class="ltx_bibblock">Multiscale combinatorial grouping for image segmentation and object
proposal generation.

</span>
<span class="ltx_bibblock">In: arXiv:1503.00848. (March 2015)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.:

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2013)
3111–3119

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jiasen Lu, Xiao Lin, D.B., Parikh, D.:

</span>
<span class="ltx_bibblock">Deeper lstm and normalized cnn visual question answering model.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/VT-vision-lab/VQA_LSTM_CNN</span> (2015)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1604.01484" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1604.01485" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1604.01485">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1604.01485" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1604.01486" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 14:43:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
