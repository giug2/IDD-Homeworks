<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems</title>
<!--Generated on Thu Oct  3 10:37:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="vertical federated learning,  distributed machine learning,  data privacy,  data security,  machine learning software,  fast prototyping" lang="en" name="keywords"/>
<base href="/html/2409.15558v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#S1" title="In Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction and motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#S2" title="In Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Stalactite</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#S3" title="In Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#S4" title="In Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Demo</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#S5" title="In Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">
<em class="ltx_emph ltx_font_italic" id="id1.id1">Stalactite</em>: Toolbox for Fast Prototyping of Vertical Federated Learning Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anastasiia Zakharova
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:nastyazakharova.nz@gmail.com">nastyazakharova.nz@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-7624-6790" title="ORCID identifier">0000-0002-7624-6790</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id2.1.id1">ITMO University</span><span class="ltx_text ltx_affiliation_city" id="id3.2.id2">Saint-Petersburg</span><span class="ltx_text ltx_affiliation_country" id="id4.3.id3">Russian Federation</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dmitriy Alexandrov
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mr.alexdmitriy@mail.ru">mr.alexdmitriy@mail.ru</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-3494-5315" title="ORCID identifier">0000-0002-3494-5315</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">ITMO University</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Saint-Petersburg</span><span class="ltx_text ltx_affiliation_country" id="id7.3.id3">Russian Federation</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Khodorchenko
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mariyaxod@yandex.ru">mariyaxod@yandex.ru</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-5446-5311" title="ORCID identifier">0000-0001-5446-5311</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id8.1.id1">ITMO University</span><span class="ltx_text ltx_affiliation_city" id="id9.2.id2">Saint-Petersburg</span><span class="ltx_text ltx_affiliation_country" id="id10.3.id3">Russian Federation</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nikolay Butakov
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:alipoov.nb@gmail.com">alipoov.nb@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-2705-1313" title="ORCID identifier">0000-0002-2705-1313</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">ITMO University</span><span class="ltx_text ltx_affiliation_city" id="id12.2.id2">Saint-Petersburg</span><span class="ltx_text ltx_affiliation_country" id="id13.3.id3">Russian Federation</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexey Vasilev
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:alexxl.vasilev@yandex.ru">alexxl.vasilev@yandex.ru</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0007-1415-2004" title="ORCID identifier">0009-0007-1415-2004</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id14.1.id1">Sber AI Lab</span><span class="ltx_text ltx_affiliation_city" id="id15.2.id2">Moscow</span><span class="ltx_text ltx_affiliation_country" id="id16.3.id3">Russian Federation</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maxim Savchenko
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:savvvan@gmail.com">savvvan@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0003-4180-9869" title="ORCID identifier">0009-0003-4180-9869</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Sber AI Lab</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Moscow</span><span class="ltx_text ltx_affiliation_country" id="id19.3.id3">Russian Federation</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Grigorievskiy
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:alex.grigorievskiy@gmail.com">alex.grigorievskiy@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4815-0641" title="ORCID identifier">0000-0003-4815-0641</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id20.1.id1">Independent Researcher</span><span class="ltx_text ltx_affiliation_city" id="id21.2.id2">Helsinki</span><span class="ltx_text ltx_affiliation_country" id="id22.3.id3">Finland</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id23.id1">Machine learning (ML) models trained on datasets owned
by different organizations and physically located in remote databases
offer benefits in many real-world use cases. State regulations
or business requirements often prevent data transfer to a central location, making it difficult to utilize standard machine learning algorithms.
Federated Learning (FL) is a technique that enables models to learn
from distributed datasets without revealing the original
data. Vertical Federated learning (VFL) is a type of FL
where data samples are divided by features across several data
owners. For instance, in a recommendation task, a user can interact with various sets of items, and the logs of these interactions
are stored by different organizations. In this demo paper, we present
<em class="ltx_emph ltx_font_italic" id="id23.id1.1">Stalactite</em> - an open-source framework for VFL that provides the
necessary functionality for building prototypes of VFL systems. It has
several advantages over the existing frameworks. In particular, it allows
researchers to focus on the algorithmic side rather than engineering and to easily deploy
learning in a distributed environment. It implements several VFL algorithms and has
a built-in homomorphic encryption layer. We demonstrate its use on a real-world recommendation datasets.</p>
</div>
<div class="ltx_keywords">vertical federated learning, distributed machine learning, data privacy, data security, machine learning software, fast prototyping
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>18th ACM Conference on Recommender Systems; October 14–18, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>18th ACM Conference on Recommender Systems (RecSys ’24), October 14–18, 2024, Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3640457.3691700</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0505-2/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction and motivation</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the last decade, many large organizations have positioned themselves as ecosystems, i.e., groups of companies that cover all user needs. Therefore, one of the possible options for improving the quality of recommendations is to enrich models with information from a related company with the same users. Often, companies from the same group may have different owners, so direct original data exchange may not be possible due to legal aspects. <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">Federated Learning</em> (FL) is usually used to exchange information and enrich models.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The term <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">Federated Learning</em> (FL) was coined in <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib15" title="">2017</a>)</cite> to describe a setup where different data owners contribute distinct data samples to an overall system. This type of FL is called horizontal FL (HFL). In scientific literature, the term <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">Federated Learning</em> typically refers to HFL. In contrast, Vertical Federated Learning (VFL) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib12" title="">2024</a>; Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib9" title="">2022</a>)</cite> is a setup where data is divided by features. Recognition of VFL is gradually increasing due to its relevant practical use cases. For example, in recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib6" title="">2024</a>)</cite>, different platforms may collect various parts of user interaction data. A closely related concept is Cross-Domain Recommender Systems <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib4" title="">2022</a>; Samra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib18" title="">2024</a>)</cite>. VFL has applications in finance <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib14" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib5" title="">2021</a>)</cite>, healthcare <cite class="ltx_cite ltx_citemacro_citep">(Shan, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib19" title="">2023</a>)</cite>, advertising <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib10" title="">2023</a>)</cite>, etc. Split learning <cite class="ltx_cite ltx_citemacro_citep">(Poirot et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib16" title="">2019</a>)</cite> is also a type of VFL. In this work, we have focused on the vertical federated learning case.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The development of FL software toolboxes has historically focused on horizontal FL, often leaving the needs of researchers working on vertical federated learning unmet. Some existing toolboxes offer limited or no support for VFL <cite class="ltx_cite ltx_citemacro_citep">(Ludwig et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib13" title="">2020</a>)</cite> (IBM). Even when toolboxes support VFL, the support is often limited in scope and requires substantial effort to implement new VFL algorithms. The root cause is that their architecture is primarily built with horizontal FL use cases in mind <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib8" title="">2020</a>; Beutel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib3" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Additionally, several toolboxes are designed for practical industrial use <cite class="ltx_cite ltx_citemacro_citep">(Roth et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib17" title="">2022</a>)</cite> (NVIDIA), <cite class="ltx_cite ltx_citemacro_citep">(Foley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib7" title="">2022</a>)</cite> (Intel),  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib11" title="">2021</a>; pad, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib2" title="">2021</a>)</cite> (Baidu), making them challenging for researchers to adopt. These industrial toolboxes are optimized for performance, often at the expense of code readability. Furthermore, they may require industrial-level infrastructure and significant engineering efforts to deploy effectively. In these toolboxes, the convenience of modification and implementation of new algorithms is often sacrificed in favor of speed and security. This trade-off can make it difficult for researchers to adapt these tools for experimental purposes or to integrate novel algorithms easily.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Recently, a specialized VFL toolbox for research, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">VFLARE</em> <cite class="ltx_cite ltx_citemacro_citep">(Zou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib22" title="">2024</a>)</cite>, has been introduced. It implements numerous VFL algorithms and attacks and contains several VFL datasets. Its current functionality is mainly developed for emulating VFL on a single machine, limiting its usefulness for analyzing algorithms in real distributed deployments. The distributed version of <em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">VFLARE</em> is still under development. In response to these limitations, we have developed <em class="ltx_emph ltx_font_italic" id="S1.p5.1.3">Stalactite</em>, a toolbox for fast prototyping VFL systems. The main goal of Stalactite is to allow researchers to focus on algorithms rather than engineering while facilitating the deployment of VFL algorithms in real distributed environments across the internet.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">VFL training consists of two phases: data matching and model training. The first phase aims to identify common samples across all participants. Once these common data samples are identified, the second phase involves training the ML model. In VFL, participants are typically divided into server and client roles. The server party usually holds the labels and controls the training process. Client parties contribute their data to the training process. Unlike HFL, where model parameters or gradients are exchanged between participants, VFL involves exchanging representations of distributed features or predictions. Stochastic Gradient Descent (SGD) or its variants are commonly used as optimization algorithms. Various techniques are employed to ensure data privacy among participants. Consequently, a single training iteration may require multiple exchanges between the server and clients <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib21" title="">2019</a>)</cite>. Given these complexities, a VFL toolbox must provide a flexible way to modify the main training loop to satisfy the specific requirements of different privacy-preserving techniques.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The main features of Stalactite are as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Well-designed abstractions that separate mathematical concepts from message exchange logic, facilitating the easy translation of VFL algorithms into code.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Multiple execution modes: multi-thread, multi-process, and distributed, with seamless switching between modes without requiring code modifications.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Convenient debugging of algorithm implementations, enabled by the flexibility in execution modes.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Comprehensive logging of payload, exchange time, and machine learning metrics during distributed execution.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S1.p7.2"><span class="ltx_text ltx_font_italic" id="S1.p7.2.1">Stalactite</span> source code is available on GitHub<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sb-ai-lab/Stalactite" title="">https://github.com/sb-ai-lab/Stalactite</a></span></span></span>.
An additional contribution to the paper is the presentation of a new real-world open-source dataset SBOL<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/datasets/alexxl/sbol-dataset" title="">https://www.kaggle.com/datasets/alexxl/sbol-dataset</a></span></span></span>, which has intersections in users with the dataset MegaMarket <cite class="ltx_cite ltx_citemacro_citep">(Shevchenko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib20" title="">2024</a>)</cite>. The characteristics of the SBOL dataset are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#S1.T1" title="Table 1 ‣ 1. Introduction and motivation ‣ Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span><span class="ltx_text ltx_font_bold" id="S1.T1.2.1">Statistics of the SBOL dataset for a period of 4 months. The dataset contains information about offers and purchases of banking products by users on certain days.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S1.T1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.3.1.1.1.1">Statistics</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S1.T1.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S1.T1.3.1.1.2.1">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S1.T1.3.2.1.1">Users</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S1.T1.3.2.1.2">190 439</td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S1.T1.3.3.2.1">Items</th>
<td class="ltx_td ltx_align_right" id="S1.T1.3.3.2.2">19</td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S1.T1.3.4.3.1">Interactions</th>
<td class="ltx_td ltx_align_right" id="S1.T1.3.4.3.2">1 056 889</td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S1.T1.3.5.4.1">Other features</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="S1.T1.3.5.4.2">1 345</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In the current demonstration, the SBOL dataset serves as the primary data source, while MegaMarket contains a subset of users from the main dataset with additional features. We focus on recommending a small set of banking products to users. This example is only a demonstration of Stalactite’s capabilities such as exchanging (possibly encrypted) intermediate computations between parties. The framework can be applied to a broader range of recommendation tasks within the VFL formulation. It is capable of handling a larger number of items, for instance, by implementing the recent algorithm described in <cite class="ltx_cite ltx_citemacro_citep">(Samra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#bib.bib18" title="">2024</a>)</cite>. This flexibility allows for more complex and comprehensive recommendation systems to be built using our tool.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Stalactite</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The architecture of the framework with its main components is presented on fig <a class="ltx_ref" href="https://arxiv.org/html/2409.15558v2#S2.F1" title="Figure 1 ‣ 2. Stalactite ‣ Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="499" id="S2.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Stalactite architecture with main components and communications between them.</figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The master component maintains its part of the data and target labels. It is responsible for matching the records’ IDs to form the shared space of rows, synchronizing all iterations in the training process, and calculating the loss. Member component, on the other hand, only holds its dataset and computes forward, and backward passes on its data. The special component Arbiter performs the distribution of encryption keys and calculation of the gradients concerning the master and members. It should be noted that the presence of this component is protocol-dependent and it may be absent if the protocol assumes direct communications between agents. Additionally, there are MlFLow and Prometheus components which are responsible for the collection of the training and inference metrics and statistics, allowing monitoring of the framework’s performance and algorithms quality.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The framework’s architecture can be divided into several main layers: communication layer, protocol layer, and models layer. They are implemented in isolation to make the customization and alternations possible if necessary.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_italic" id="S2.p4.1.1">The communication layer</span> is responsible for organizing data transfers between all participating entities including PartyMaster, PartyMember, and Arbiter. The main entity on this level is the PartyCommunicator which is responsible for a specific implementation of agent’s communication while providing a simple MPI-like send/receive interface to the agents. Currently, the framework offers two different implementations: gRPC-based server-client communication for the distributed setting with Protobuf interfaces and Safetensors serialization; and local in-process thread-based implementation for easy-to-use and easy-to-debug use in IDE. The latter one employs an in-memory queue for sending and receiving data. The combination of gRPC, Protobuf, and SafeTensors has been chosen due to several reasons. First, it saves the volume of data tensors being moved across the network. Second, it is efficient communication over the Internet network (we assume that data silos will be more likely allocated on independent stack holders hardware, not in the same local network). Third, it is flexible in terms of implementing various patterns of communication, including cases where one of the agents can be lost in comparison to traditional communicating frameworks like MPI or gloo optimized for local networks with high-speed connections. At the same time, the local mode strips out all complications caused by distributed settings and makes it easy to concentrate efforts on high-level details of protocol or ML model development and debugging. This architecture enables fast prototyping while preserving seamless switching to a distributed mode when necessary.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_italic" id="S2.p5.1.1">The protocol layer</span> is responsible for defining the logic of interactions and synchronization between agents, encrypting, and ensuring datasets’ non-disclosure for all participants. On this layer, we implement base classes for interactions of the agents in the case of classical ML algorithms, such as linear and logistic regressions, as well as neural networks-based algorithms enabled with a split-learning approach. Homomorphic encryption is also implemented on this layer.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1"><span class="ltx_text ltx_font_italic" id="S2.p6.1.1">The models’ layer</span> is responsible for integrating ML models into the framework, regardless of a specific protocol on the previous layer. This layer provides the necessary interfaces for models to be integrated and used by protocol implementations.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Setup</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Stalactite <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/sb-ai-lab/Stalactite</span></span></span> is available open-source on GitHub. Here you will find installation instructions using poetry. The documentation provides detailed information on each of the Stalactite modules. If you have any issues or questions about using the framework, you can check out the source code and contribution guidelines on GitHub.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Demo</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The Stalactite demo illustrates the extensive capabilities of the Stalactite framework through practical applications using independent real-world datasets, SBOL and MegaMarket, which share an ID space in the e-commerce domain. Data from these sources is utilized in a vertical federated learning (VFL) environment to demonstrate how distributed machine learning models can be trained without compromising data privacy. It is achieved by leveraging three cloud-based virtual machines to host distinct network agents. The demo guides through the setup of the repository, configuration of arbitered and arbiterless federated experiments, and execution using the Stalactite CLI to provide an example of machine learning lifecycle management. This includes data synchronization, model training, and result monitoring through integrated tools like MLflow and Grafana. It also provides various advanced features, such as plugin deployment for new algorithm integration and an IDE-supported local debugging mode, positioning Stalactite as a robust solution for VFL algorithm prototyping and distributed applications across diverse computing environments.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Stalactite provides the opportunity for fast VFL algorithm prototyping and probing. Users can implement ML algorithms, and models and customize the communication protocols for the data exchange between agents with optionally enabled Homomorphic Encryption. The framework allows changing the communication layer with ease(e.g. replacing the gRPC with in-memory exchanges, MPI, etc.) and enables user-friendly debugging for the developing solutions via IDE with easy transfer of those to the deployment via CLI. The proposed architecture of Stalactite allows upgrading it into a fully-fledged industrial VFL framework.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by the Analytical Center for the Government of the Russian Federation (IGK 000000D730324P540002), agreement No. 70-2021-00141

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">pad (2021)</span>
<span class="ltx_bibblock">
2021.

</span>
<span class="ltx_bibblock">PaddleFL.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/PaddlePaddle/PaddleFL.git" title="">https://github.com/PaddlePaddle/PaddleFL.git</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusmão, and Nicholas D. Lane. 2022.

</span>
<span class="ltx_bibblock">Flower: A Friendly Federated Learning Research Framework.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2007.14390 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.14390" title="">https://arxiv.org/abs/2007.14390</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Chaochao Chen, Huiwen Wu, Jiajie Su, Lingjuan Lyu, Xiaolin Zheng, and Li Wang. 2022.

</span>
<span class="ltx_bibblock">Differential Private Knowledge Transfer for Privacy-Preserving Cross-Domain Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the ACM Web Conference 2022</em> (Virtual Event, Lyon, France) <em class="ltx_emph ltx_font_italic" id="bib.bib4.4.2">(WWW ’22)</em>. Association for Computing Machinery, New York, NY, USA, 1455–1465.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3485447.3512192" title="">https://doi.org/10.1145/3485447.3512192</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chaochao Chen, Jun Zhou, Li Wang, Xibin Wu, Wenjing Fang, Jin Tan, Lei Wang, Alex X. Liu, Hao Wang, and Cheng Hong. 2021.

</span>
<span class="ltx_bibblock">When Homomorphic Encryption Marries Secret Sharing: Secure Large-Scale Sparse Logistic Regression and Applications in Risk Control. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em> (Virtual Event, Singapore) <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(KDD ’21)</em>. Association for Computing Machinery, New York, NY, USA, 2652–2662.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3447548.3467210" title="">https://doi.org/10.1145/3447548.3467210</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jamie Cui, Chaochao Chen, Lingjuan Lyu, Carl Yang, and Li Wang. 2024.

</span>
<span class="ltx_bibblock">Exploiting data sparsity in secure cross-platform social recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 35th International Conference on Neural Information Processing Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(NIPS ’21)</em>. Curran Associates Inc., Red Hook, NY, USA, Article 805, 11 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Foley et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Patrick Foley, Micah J Sheller, Brandon Edwards, Sarthak Pati, Walter Riviera, Mansi Sharma, Prakash Narayana Moorthy, Shi-han Wang, Jason Martin, Parsa Mirhaji, Prashant Shah, and Spyridon Bakas. 2022.

</span>
<span class="ltx_bibblock">OpenFL: the open federated learning library.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Physics in Medicine &amp; Biology</em> (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1088/1361-6560/ac97d9" title="">https://doi.org/10.1088/1361-6560/ac97d9</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock">FedML: A Research Library and Benchmark for Federated Machine Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2007.13518 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.13518" title="">https://arxiv.org/abs/2007.13518</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Afsana Khan, Marijn ten Thij, and Anna Wilbik. 2022.

</span>
<span class="ltx_bibblock">Vertical Federated Learning: A Structured Literature Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">ArXiv</em> abs/2212.00622 (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:254125648" title="">https://api.semanticscholar.org/CorpusID:254125648</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenjie Li, Qiaolin Xia, Junfeng Deng, Hao Cheng, Jiangming Liu, Kouying Xue, Yong Cheng, and Shu-Tao Xia. 2023.

</span>
<span class="ltx_bibblock">VFed-SSD: Towards Practical Vertical Federated Advertising.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2205.15987 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.15987" title="">https://arxiv.org/abs/2205.15987</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang. 2021.

</span>
<span class="ltx_bibblock">FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Journal of Machine Learning Research</em> 22, 226 (2021), 1–6.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v22/20-815.html" title="">http://jmlr.org/papers/v22/20-815.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and Qiang Yang. 2024.

</span>
<span class="ltx_bibblock">Vertical Federated Learning: Concepts, Advances, and Challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">IEEE Transactions on Knowledge and Data Engineering</em> PP (07 2024), 1–20.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TKDE.2024.3352628" title="">https://doi.org/10.1109/TKDE.2024.3352628</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ludwig et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali Anwar, Shashank Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu Sinn, Mark Purcell, Ambrish Rawat, Tran Minh, Naoise Holohan, Supriyo Chakraborty, Shalisha Whitherspoon, Dean Steuer, Laura Wynter, Hifaz Hassan, Sean Laguna, Mikhail Yurochkin, Mayank Agarwal, Ebube Chuba, and Annie Abay. 2020.

</span>
<span class="ltx_bibblock">IBM Federated Learning: an Enterprise Framework White Paper V0.1.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2007.10987 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.10987" title="">https://arxiv.org/abs/2007.10987</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Y. Luo, Z. Lu, X. Yin, S. Lu, and Y. Weng. 2023.

</span>
<span class="ltx_bibblock">Application Research of Vertical Federated Learning Technology in Banking Risk Control Model Strategy. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">2023 IEEE Intl Conf on Parallel &amp; Distributed Processing with Applications, Big Data &amp; Cloud Computing, Sustainable Computing &amp; Communications, Social Computing &amp; Networking (ISPA/BDCloud/SocialCom/SustainCom)</em>. IEEE Computer Society, Los Alamitos, CA, USA, 545–552.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00103" title="">https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00103</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks from Decentralized Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em> <em class="ltx_emph ltx_font_italic" id="bib.bib15.4.2">(Proceedings of Machine Learning Research, Vol. 54)</em>, Aarti Singh and Jerry Zhu (Eds.). PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v54/mcmahan17a.html" title="">https://proceedings.mlr.press/v54/mcmahan17a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poirot et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Maarten G. Poirot, Praneeth Vepakomma, Ken Chang, Jayashree Kalpathy-Cramer, Rajiv Gupta, and Ramesh Raskar. 2019.

</span>
<span class="ltx_bibblock">Split Learning for collaborative deep learning in healthcare.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">CoRR</em> abs/1912.12115 (2019).

</span>
<span class="ltx_bibblock">arXiv:1912.12115

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1912.12115" title="">http://arxiv.org/abs/1912.12115</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Holger R. Roth, Yan Cheng, Yuhong Wen, Isaac Yang, Ziyue Xu, Yuan-Ting Hsieh, Kristopher Kersten, Ahmed Harouni, Can Zhao, Kevin Lu, Zhihong Zhang, Wenqi Li, Andriy Myronenko, Dong Yang, Sean Yang, Nicola Rieke, Abood Quraini, Chester Chen, Daguang Xu, Nic Ma, Prerna Dogra, Mona Flores, and Andrew Feng. 2022.

</span>
<span class="ltx_bibblock">NVIDIA FLARE: Federated Learning from Simulation to Real-World.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2210.13291" title="">https://doi.org/10.48550/ARXIV.2210.13291</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samra et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Abdulaziz Samra, Evgeney Frolov, Alexey Vasilev, Alexander Grigorievskiy, and Anton Vakhrushev. 2024.

</span>
<span class="ltx_bibblock">Cross-Domain Latent Factors Sharing via Implicit Matrix Factorization. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 18th ACM Conference on Recommender Systems</em> (Bari, Italy) <em class="ltx_emph ltx_font_italic" id="bib.bib18.4.2">(RecSys ’24)</em>. Association for Computing Machinery, Bari, Italy.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3640457.3688143" title="">https://doi.org/10.1145/3640457.3688143</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan (2023)</span>
<span class="ltx_bibblock">
Jinyong Shan. 2023.

</span>
<span class="ltx_bibblock">IHVFL: a privacy-enhanced intention-hiding vertical federated learning framework for medical data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Cybersecurity</em> 6 (10 2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1186/s42400-023-00166-9" title="">https://doi.org/10.1186/s42400-023-00166-9</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shevchenko et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko, and Alexey Zaytsev. 2024.

</span>
<span class="ltx_bibblock">From Variability to Stability: Advancing RecSys Benchmarking Practices. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> (Barcelona, Spain) <em class="ltx_emph ltx_font_italic" id="bib.bib20.4.2">(KDD ’24)</em>. Association for Computing Machinery, New York, NY, USA, 5701–5712.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3637528.3671655" title="">https://doi.org/10.1145/3637528.3671655</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Shengwen Yang, Bing Ren, Xuhui Zhou, and Liping Liu. 2019.

</span>
<span class="ltx_bibblock">Parallel Distributed Logistic Regression for Vertical Federated Learning without Third-Party Coordinator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">ArXiv</em> abs/1911.09824 (2019).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:208248396" title="">https://api.semanticscholar.org/CorpusID:208248396</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tianyuan Zou, Zixuan Gu, Yu He, Hideaki Takahashi, Yang Liu, and Ya-Qin Zhang. 2024.

</span>
<span class="ltx_bibblock">VFLAIR: A Research Library and Benchmark for Vertical Federated Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.09827 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.09827" title="">https://arxiv.org/abs/2310.09827</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 10:37:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
