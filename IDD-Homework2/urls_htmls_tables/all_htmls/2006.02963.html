<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2006.02963] RarePlanes: Synthetic Data Takes Flight</title><meta property="og:description" content="RarePlanes is a unique open-source machine learning dataset that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of synthetic data to aid c…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RarePlanes: Synthetic Data Takes Flight">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RarePlanes: Synthetic Data Takes Flight">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2006.02963">

<!--Generated on Fri Mar  1 17:07:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">RarePlanes: Synthetic Data Takes Flight</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jacob Shermeyer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">IQT - CosmiQ Works, [jshermeyer, avanetten, dhogan, rlewis]@iqt.org
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thomas Hossler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">AI.Reverie, [thomas.hossler, daeil]@aireverie.com
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adam Van Etten
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">IQT - CosmiQ Works, [jshermeyer, avanetten, dhogan, rlewis]@iqt.org
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Hogan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">IQT - CosmiQ Works, [jshermeyer, avanetten, dhogan, rlewis]@iqt.org
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryan Lewis
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">IQT - CosmiQ Works, [jshermeyer, avanetten, dhogan, rlewis]@iqt.org
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daeil Kim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">AI.Reverie, [thomas.hossler, daeil]@aireverie.com
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.8" class="ltx_p">RarePlanes is a unique open-source machine learning dataset that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very-high resolution dataset built to test the value of synthetic data from an overhead perspective. Previous research has shown that synthetic data can reduce the amount of real training data needed and potentially improve performance for many tasks in the computer vision domain. The real portion of the dataset consists of <math id="id1.1.m1.1" class="ltx_Math" alttext="253" display="inline"><semantics id="id1.1.m1.1a"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">253</mn><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">253</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">253</annotation></semantics></math> Maxar WorldView-3 satellite scenes spanning <math id="id2.2.m2.1" class="ltx_Math" alttext="112" display="inline"><semantics id="id2.2.m2.1a"><mn id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">112</mn><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><cn type="integer" id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">112</cn></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">112</annotation></semantics></math> locations and <math id="id3.3.m3.2" class="ltx_Math" alttext="2,142\,{\rm km}^{2}" display="inline"><semantics id="id3.3.m3.2a"><mrow id="id3.3.m3.2.2.1" xref="id3.3.m3.2.2.2.cmml"><mn id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">2</mn><mo id="id3.3.m3.2.2.1.2" xref="id3.3.m3.2.2.2.cmml">,</mo><mrow id="id3.3.m3.2.2.1.1" xref="id3.3.m3.2.2.1.1.cmml"><mn id="id3.3.m3.2.2.1.1.2" xref="id3.3.m3.2.2.1.1.2.cmml">142</mn><mo lspace="0.170em" rspace="0em" id="id3.3.m3.2.2.1.1.1" xref="id3.3.m3.2.2.1.1.1.cmml">​</mo><msup id="id3.3.m3.2.2.1.1.3" xref="id3.3.m3.2.2.1.1.3.cmml"><mi id="id3.3.m3.2.2.1.1.3.2" xref="id3.3.m3.2.2.1.1.3.2.cmml">km</mi><mn id="id3.3.m3.2.2.1.1.3.3" xref="id3.3.m3.2.2.1.1.3.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="id3.3.m3.2b"><list id="id3.3.m3.2.2.2.cmml" xref="id3.3.m3.2.2.1"><cn type="integer" id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">2</cn><apply id="id3.3.m3.2.2.1.1.cmml" xref="id3.3.m3.2.2.1.1"><times id="id3.3.m3.2.2.1.1.1.cmml" xref="id3.3.m3.2.2.1.1.1"></times><cn type="integer" id="id3.3.m3.2.2.1.1.2.cmml" xref="id3.3.m3.2.2.1.1.2">142</cn><apply id="id3.3.m3.2.2.1.1.3.cmml" xref="id3.3.m3.2.2.1.1.3"><csymbol cd="ambiguous" id="id3.3.m3.2.2.1.1.3.1.cmml" xref="id3.3.m3.2.2.1.1.3">superscript</csymbol><ci id="id3.3.m3.2.2.1.1.3.2.cmml" xref="id3.3.m3.2.2.1.1.3.2">km</ci><cn type="integer" id="id3.3.m3.2.2.1.1.3.3.cmml" xref="id3.3.m3.2.2.1.1.3.3">2</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.2c">2,142\,{\rm km}^{2}</annotation></semantics></math> with <math id="id4.4.m4.2" class="ltx_Math" alttext="14,700" display="inline"><semantics id="id4.4.m4.2a"><mrow id="id4.4.m4.2.3.2" xref="id4.4.m4.2.3.1.cmml"><mn id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml">14</mn><mo id="id4.4.m4.2.3.2.1" xref="id4.4.m4.2.3.1.cmml">,</mo><mn id="id4.4.m4.2.2" xref="id4.4.m4.2.2.cmml">700</mn></mrow><annotation-xml encoding="MathML-Content" id="id4.4.m4.2b"><list id="id4.4.m4.2.3.1.cmml" xref="id4.4.m4.2.3.2"><cn type="integer" id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">14</cn><cn type="integer" id="id4.4.m4.2.2.cmml" xref="id4.4.m4.2.2">700</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.2c">14,700</annotation></semantics></math> hand-annotated aircraft. The accompanying synthetic dataset is generated via AI.Reverie’s simulation platform and features <math id="id5.5.m5.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="id5.5.m5.2a"><mrow id="id5.5.m5.2.3.2" xref="id5.5.m5.2.3.1.cmml"><mn id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml">50</mn><mo id="id5.5.m5.2.3.2.1" xref="id5.5.m5.2.3.1.cmml">,</mo><mn id="id5.5.m5.2.2" xref="id5.5.m5.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="id5.5.m5.2b"><list id="id5.5.m5.2.3.1.cmml" xref="id5.5.m5.2.3.2"><cn type="integer" id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1">50</cn><cn type="integer" id="id5.5.m5.2.2.cmml" xref="id5.5.m5.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.2c">50,000</annotation></semantics></math> synthetic satellite images simulating a total area of <math id="id6.6.m6.1" class="ltx_Math" alttext="9331.2\,\rm{km}^{2}" display="inline"><semantics id="id6.6.m6.1a"><mrow id="id6.6.m6.1.1" xref="id6.6.m6.1.1.cmml"><mn id="id6.6.m6.1.1.2" xref="id6.6.m6.1.1.2.cmml">9331.2</mn><mo lspace="0.170em" rspace="0em" id="id6.6.m6.1.1.1" xref="id6.6.m6.1.1.1.cmml">​</mo><msup id="id6.6.m6.1.1.3" xref="id6.6.m6.1.1.3.cmml"><mi id="id6.6.m6.1.1.3.2" xref="id6.6.m6.1.1.3.2.cmml">km</mi><mn id="id6.6.m6.1.1.3.3" xref="id6.6.m6.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="id6.6.m6.1b"><apply id="id6.6.m6.1.1.cmml" xref="id6.6.m6.1.1"><times id="id6.6.m6.1.1.1.cmml" xref="id6.6.m6.1.1.1"></times><cn type="float" id="id6.6.m6.1.1.2.cmml" xref="id6.6.m6.1.1.2">9331.2</cn><apply id="id6.6.m6.1.1.3.cmml" xref="id6.6.m6.1.1.3"><csymbol cd="ambiguous" id="id6.6.m6.1.1.3.1.cmml" xref="id6.6.m6.1.1.3">superscript</csymbol><ci id="id6.6.m6.1.1.3.2.cmml" xref="id6.6.m6.1.1.3.2">km</ci><cn type="integer" id="id6.6.m6.1.1.3.3.cmml" xref="id6.6.m6.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.1c">9331.2\,\rm{km}^{2}</annotation></semantics></math> with <math id="id7.7.m7.2" class="ltx_Math" alttext="\sim 630,000" display="inline"><semantics id="id7.7.m7.2a"><mrow id="id7.7.m7.2.3" xref="id7.7.m7.2.3.cmml"><mi id="id7.7.m7.2.3.2" xref="id7.7.m7.2.3.2.cmml"></mi><mo id="id7.7.m7.2.3.1" xref="id7.7.m7.2.3.1.cmml">∼</mo><mrow id="id7.7.m7.2.3.3.2" xref="id7.7.m7.2.3.3.1.cmml"><mn id="id7.7.m7.1.1" xref="id7.7.m7.1.1.cmml">630</mn><mo id="id7.7.m7.2.3.3.2.1" xref="id7.7.m7.2.3.3.1.cmml">,</mo><mn id="id7.7.m7.2.2" xref="id7.7.m7.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="id7.7.m7.2b"><apply id="id7.7.m7.2.3.cmml" xref="id7.7.m7.2.3"><csymbol cd="latexml" id="id7.7.m7.2.3.1.cmml" xref="id7.7.m7.2.3.1">similar-to</csymbol><csymbol cd="latexml" id="id7.7.m7.2.3.2.cmml" xref="id7.7.m7.2.3.2">absent</csymbol><list id="id7.7.m7.2.3.3.1.cmml" xref="id7.7.m7.2.3.3.2"><cn type="integer" id="id7.7.m7.1.1.cmml" xref="id7.7.m7.1.1">630</cn><cn type="integer" id="id7.7.m7.2.2.cmml" xref="id7.7.m7.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m7.2c">\sim 630,000</annotation></semantics></math> aircraft annotations. Both the real and synthetically generated aircraft feature <math id="id8.8.m8.1" class="ltx_Math" alttext="10" display="inline"><semantics id="id8.8.m8.1a"><mn id="id8.8.m8.1.1" xref="id8.8.m8.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="id8.8.m8.1b"><cn type="integer" id="id8.8.m8.1.1.cmml" xref="id8.8.m8.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m8.1c">10</annotation></semantics></math> fine grain attributes including: aircraft length, wingspan, wing-shape, wing-position, wingspan class, propulsion, number of engines, number of vertical-stabilizers, presence of canards, and aircraft role. Finally, we conduct extensive experiments to evaluate the real and synthetic datasets and compare performances. By doing so, we show the value of synthetic data for the task of detecting and classifying aircraft from an overhead perspective.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the last decade, computer vision research and the development of new algorithms has been driven largely by permissively licensed open datasets. Datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and PASCALVOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> (among others) remain critical drivers for advancement. Convolutional neural networks (CNNs), currently the leading class of algorithms for most vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, require a large amount of annotated observations. However, the development of such datasets is often manually intensive, time-consuming, and costly to create. An alternative approach to manually annotating training data is to create computer generated images and annotations (referred to as synthetic data). After creating realistic 3D environments, one can then generate thousands of images at virtually no cost. Such data has been shown to be effective for augmenting and replacing real data, thus reducing the burden of dataset curation. Synthetic datasets continue to be developed and have been notably helpful in various domains including: autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, facial recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, amodal analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> (see Section <a href="#S2.SS1" title="2.1 Synthetic Datasets ‣ 2 Related Work ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a> for further detail).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2006.02963/assets/figs/Maxar_Reverie_Jumbo.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="456" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">Example of the real and synthetic datasets present in RarePlanes.</span> The top two rows feature the real Maxar WorldView-3 satellite imagery and the bottom two rows show the synthetic data. The dataset features variable weather conditions, biomes, and ground surface types. </figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although synthetic datasets continue to become more prevalent, no expansive permissively licensed synthetic datasets exist in the context of overhead observation. Overhead imagery presents unique challenges for computer vision models such as: the detection of small visually-heterogeneous objects, varying look angles or lighting conditions, and different geographies that can feature distinct seasonal variability. As such, creating synthetic datasets from an overhead perspective is a significant challenge and simulators must attempt to closely mimic the complexities of a spaceborne or aerial sensor as well as the Earth’s ever-changing conditions. For example, to create a large and heterogeneous synthetic dataset, one must account for each sensors varying spatial resolution, changes in sensor look angle, the time of day of collection, shadowing, and changes in illumination due to the sun’s location relative to the sensor. Furthermore, the simulator must be able to account for other factors such as the ground appearance due to seasonal change, weather conditions, and varying geographies or biomes.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While synthetic datasets certainly have the potential to be beneficial, they require a paired real dataset with shared features to baseline performance and quantitatively test value. However, few permissively licensed overhead datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> exist that focus on detection or segmentation tasks and feature very-high resolution real imagery from an overhead perspective. Overhead datasets remain one of the best avenues for developing new computer vision methods that can adapt to limited sensor resolution, variable look angles, and locate tightly grouped, cluttered objects. Such methods can extend beyond the overhead space and be helpful in other domains such as face-id, autonomous driving, and surveillance.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.8" class="ltx_p">To address the limitations described above, we introduce the RarePlanes dataset. This dataset focuses on the detection of aircraft and their fine-grain attributes from an overhead perspective. It consists of both an expansive synthetic and real dataset. We use AI.Reverie’s novel platform which incorporates unreal engine<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to develop realistic synthetic data based off of real world airports. The platform ingests real world metadata and overhead images to procedurally generate 3D environments of real world locations. The weather, time of collection, sunlight intensity, look angle, biome, and distribution of aircraft model are among the multiple parameters that the simulator can modify to create diverse and heterogeneous data. The synthetic portion of RarePlanes consists of <math id="S1.p4.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S1.p4.1.m1.2a"><mrow id="S1.p4.1.m1.2.3.2" xref="S1.p4.1.m1.2.3.1.cmml"><mn id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">50</mn><mo id="S1.p4.1.m1.2.3.2.1" xref="S1.p4.1.m1.2.3.1.cmml">,</mo><mn id="S1.p4.1.m1.2.2" xref="S1.p4.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.2b"><list id="S1.p4.1.m1.2.3.1.cmml" xref="S1.p4.1.m1.2.3.2"><cn type="integer" id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">50</cn><cn type="integer" id="S1.p4.1.m1.2.2.cmml" xref="S1.p4.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.2c">50,000</annotation></semantics></math> images simulating a total area of <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="9331.2\,\rm{km}^{2}" display="inline"><semantics id="S1.p4.2.m2.1a"><mrow id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml"><mn id="S1.p4.2.m2.1.1.2" xref="S1.p4.2.m2.1.1.2.cmml">9331.2</mn><mo lspace="0.170em" rspace="0em" id="S1.p4.2.m2.1.1.1" xref="S1.p4.2.m2.1.1.1.cmml">​</mo><msup id="S1.p4.2.m2.1.1.3" xref="S1.p4.2.m2.1.1.3.cmml"><mi id="S1.p4.2.m2.1.1.3.2" xref="S1.p4.2.m2.1.1.3.2.cmml">km</mi><mn id="S1.p4.2.m2.1.1.3.3" xref="S1.p4.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><apply id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1"><times id="S1.p4.2.m2.1.1.1.cmml" xref="S1.p4.2.m2.1.1.1"></times><cn type="float" id="S1.p4.2.m2.1.1.2.cmml" xref="S1.p4.2.m2.1.1.2">9331.2</cn><apply id="S1.p4.2.m2.1.1.3.cmml" xref="S1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S1.p4.2.m2.1.1.3.1.cmml" xref="S1.p4.2.m2.1.1.3">superscript</csymbol><ci id="S1.p4.2.m2.1.1.3.2.cmml" xref="S1.p4.2.m2.1.1.3.2">km</ci><cn type="integer" id="S1.p4.2.m2.1.1.3.3.cmml" xref="S1.p4.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">9331.2\,\rm{km}^{2}</annotation></semantics></math> and <math id="S1.p4.3.m3.2" class="ltx_Math" alttext="\sim 630,000" display="inline"><semantics id="S1.p4.3.m3.2a"><mrow id="S1.p4.3.m3.2.3" xref="S1.p4.3.m3.2.3.cmml"><mi id="S1.p4.3.m3.2.3.2" xref="S1.p4.3.m3.2.3.2.cmml"></mi><mo id="S1.p4.3.m3.2.3.1" xref="S1.p4.3.m3.2.3.1.cmml">∼</mo><mrow id="S1.p4.3.m3.2.3.3.2" xref="S1.p4.3.m3.2.3.3.1.cmml"><mn id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">630</mn><mo id="S1.p4.3.m3.2.3.3.2.1" xref="S1.p4.3.m3.2.3.3.1.cmml">,</mo><mn id="S1.p4.3.m3.2.2" xref="S1.p4.3.m3.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.2b"><apply id="S1.p4.3.m3.2.3.cmml" xref="S1.p4.3.m3.2.3"><csymbol cd="latexml" id="S1.p4.3.m3.2.3.1.cmml" xref="S1.p4.3.m3.2.3.1">similar-to</csymbol><csymbol cd="latexml" id="S1.p4.3.m3.2.3.2.cmml" xref="S1.p4.3.m3.2.3.2">absent</csymbol><list id="S1.p4.3.m3.2.3.3.1.cmml" xref="S1.p4.3.m3.2.3.3.2"><cn type="integer" id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">630</cn><cn type="integer" id="S1.p4.3.m3.2.2.cmml" xref="S1.p4.3.m3.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.2c">\sim 630,000</annotation></semantics></math> annotations. The real portion consists of <math id="S1.p4.4.m4.1" class="ltx_Math" alttext="253" display="inline"><semantics id="S1.p4.4.m4.1a"><mn id="S1.p4.4.m4.1.1" xref="S1.p4.4.m4.1.1.cmml">253</mn><annotation-xml encoding="MathML-Content" id="S1.p4.4.m4.1b"><cn type="integer" id="S1.p4.4.m4.1.1.cmml" xref="S1.p4.4.m4.1.1">253</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.4.m4.1c">253</annotation></semantics></math> Maxar WorldView-3 satellite images spanning <math id="S1.p4.5.m5.1" class="ltx_Math" alttext="112" display="inline"><semantics id="S1.p4.5.m5.1a"><mn id="S1.p4.5.m5.1.1" xref="S1.p4.5.m5.1.1.cmml">112</mn><annotation-xml encoding="MathML-Content" id="S1.p4.5.m5.1b"><cn type="integer" id="S1.p4.5.m5.1.1.cmml" xref="S1.p4.5.m5.1.1">112</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.5.m5.1c">112</annotation></semantics></math> locations in <math id="S1.p4.6.m6.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S1.p4.6.m6.1a"><mn id="S1.p4.6.m6.1.1" xref="S1.p4.6.m6.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S1.p4.6.m6.1b"><cn type="integer" id="S1.p4.6.m6.1.1.cmml" xref="S1.p4.6.m6.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.6.m6.1c">22</annotation></semantics></math> countries and <math id="S1.p4.7.m7.2" class="ltx_Math" alttext="2,142\text{km}^{2}" display="inline"><semantics id="S1.p4.7.m7.2a"><mrow id="S1.p4.7.m7.2.2.1" xref="S1.p4.7.m7.2.2.2.cmml"><mn id="S1.p4.7.m7.1.1" xref="S1.p4.7.m7.1.1.cmml">2</mn><mo id="S1.p4.7.m7.2.2.1.2" xref="S1.p4.7.m7.2.2.2.cmml">,</mo><mrow id="S1.p4.7.m7.2.2.1.1" xref="S1.p4.7.m7.2.2.1.1.cmml"><mn id="S1.p4.7.m7.2.2.1.1.2" xref="S1.p4.7.m7.2.2.1.1.2.cmml">142</mn><mo lspace="0em" rspace="0em" id="S1.p4.7.m7.2.2.1.1.1" xref="S1.p4.7.m7.2.2.1.1.1.cmml">​</mo><msup id="S1.p4.7.m7.2.2.1.1.3" xref="S1.p4.7.m7.2.2.1.1.3.cmml"><mtext id="S1.p4.7.m7.2.2.1.1.3.2" xref="S1.p4.7.m7.2.2.1.1.3.2a.cmml">km</mtext><mn id="S1.p4.7.m7.2.2.1.1.3.3" xref="S1.p4.7.m7.2.2.1.1.3.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.7.m7.2b"><list id="S1.p4.7.m7.2.2.2.cmml" xref="S1.p4.7.m7.2.2.1"><cn type="integer" id="S1.p4.7.m7.1.1.cmml" xref="S1.p4.7.m7.1.1">2</cn><apply id="S1.p4.7.m7.2.2.1.1.cmml" xref="S1.p4.7.m7.2.2.1.1"><times id="S1.p4.7.m7.2.2.1.1.1.cmml" xref="S1.p4.7.m7.2.2.1.1.1"></times><cn type="integer" id="S1.p4.7.m7.2.2.1.1.2.cmml" xref="S1.p4.7.m7.2.2.1.1.2">142</cn><apply id="S1.p4.7.m7.2.2.1.1.3.cmml" xref="S1.p4.7.m7.2.2.1.1.3"><csymbol cd="ambiguous" id="S1.p4.7.m7.2.2.1.1.3.1.cmml" xref="S1.p4.7.m7.2.2.1.1.3">superscript</csymbol><ci id="S1.p4.7.m7.2.2.1.1.3.2a.cmml" xref="S1.p4.7.m7.2.2.1.1.3.2"><mtext id="S1.p4.7.m7.2.2.1.1.3.2.cmml" xref="S1.p4.7.m7.2.2.1.1.3.2">km</mtext></ci><cn type="integer" id="S1.p4.7.m7.2.2.1.1.3.3.cmml" xref="S1.p4.7.m7.2.2.1.1.3.3">2</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.7.m7.2c">2,142\text{km}^{2}</annotation></semantics></math> with <math id="S1.p4.8.m8.2" class="ltx_Math" alttext="\sim 14,700" display="inline"><semantics id="S1.p4.8.m8.2a"><mrow id="S1.p4.8.m8.2.3" xref="S1.p4.8.m8.2.3.cmml"><mi id="S1.p4.8.m8.2.3.2" xref="S1.p4.8.m8.2.3.2.cmml"></mi><mo id="S1.p4.8.m8.2.3.1" xref="S1.p4.8.m8.2.3.1.cmml">∼</mo><mrow id="S1.p4.8.m8.2.3.3.2" xref="S1.p4.8.m8.2.3.3.1.cmml"><mn id="S1.p4.8.m8.1.1" xref="S1.p4.8.m8.1.1.cmml">14</mn><mo id="S1.p4.8.m8.2.3.3.2.1" xref="S1.p4.8.m8.2.3.3.1.cmml">,</mo><mn id="S1.p4.8.m8.2.2" xref="S1.p4.8.m8.2.2.cmml">700</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.8.m8.2b"><apply id="S1.p4.8.m8.2.3.cmml" xref="S1.p4.8.m8.2.3"><csymbol cd="latexml" id="S1.p4.8.m8.2.3.1.cmml" xref="S1.p4.8.m8.2.3.1">similar-to</csymbol><csymbol cd="latexml" id="S1.p4.8.m8.2.3.2.cmml" xref="S1.p4.8.m8.2.3.2">absent</csymbol><list id="S1.p4.8.m8.2.3.3.1.cmml" xref="S1.p4.8.m8.2.3.3.2"><cn type="integer" id="S1.p4.8.m8.1.1.cmml" xref="S1.p4.8.m8.1.1">14</cn><cn type="integer" id="S1.p4.8.m8.2.2.cmml" xref="S1.p4.8.m8.2.2">700</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.8.m8.2c">\sim 14,700</annotation></semantics></math> hand annotated aircraft. Examples of the synthetic and real images are shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.2" class="ltx_p">RarePlanes also provides fine-grain labels with <math id="S1.p5.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S1.p5.1.m1.1a"><mn id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><cn type="integer" id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">10</annotation></semantics></math> distinct aircraft attributes and <math id="S1.p5.2.m2.1" class="ltx_Math" alttext="33" display="inline"><semantics id="S1.p5.2.m2.1a"><mn id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><cn type="integer" id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">33</annotation></semantics></math> different sub-attribute choices labeled for each aircraft. These include: aircraft length, wingspan, wing-shape, wing-position, Federal Aviation Administration (FAA) wingspan class <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, propulsion, number of engines, number of vertical-stabilizers, canards, and aircraft type or role. Although several other overhead detection datasets exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, no others have multiple fine-grain attributes that detail specific object features. Such fine-grain attributes have been particularly helpful for zero-shot learning applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and enable end users to create diverse custom classes. Using these combined attributes, anywhere from 1 to 110 classes can be created for individual research purposes. The dataset is available for free download through Amazon Web Services’ Open Data Program, with download instructions available at <a target="_blank" href="https://www.cosmiqworks.org/RarePlanes" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.cosmiqworks.org/RarePlanes</a>.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S1.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<td id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S1.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Gigapixels</span></td>
<td id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S1.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Classes</span></td>
<td id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S1.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Attributes</span></td>
<td id="S1.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S1.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Labels</span></td>
</tr>
<tr id="S1.T1.1.2.2" class="ltx_tr">
<td id="S1.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">Real</span></td>
<td id="S1.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">Synthetic</span></td>
</tr>
<tr id="S1.T1.1.3.3" class="ltx_tr">
<th id="S1.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SpaceNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<td id="S1.T1.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t">100.1</td>
<td id="S1.T1.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">1 to 8</td>
<td id="S1.T1.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">1</td>
<td id="S1.T1.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">859,982</td>
<td id="S1.T1.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">0</td>
</tr>
<tr id="S1.T1.1.4.4" class="ltx_tr">
<th id="S1.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xBD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S1.T1.1.4.4.2" class="ltx_td ltx_align_right">9.8</td>
<td id="S1.T1.1.4.4.3" class="ltx_td ltx_align_right">1 to 4</td>
<td id="S1.T1.1.4.4.4" class="ltx_td ltx_align_right">1</td>
<td id="S1.T1.1.4.4.5" class="ltx_td ltx_align_right">850,736</td>
<td id="S1.T1.1.4.4.6" class="ltx_td ltx_align_right">0</td>
</tr>
<tr id="S1.T1.1.5.5" class="ltx_tr">
<th id="S1.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xView <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<td id="S1.T1.1.5.5.2" class="ltx_td ltx_align_right">56.0</td>
<td id="S1.T1.1.5.5.3" class="ltx_td ltx_align_right">60</td>
<td id="S1.T1.1.5.5.4" class="ltx_td ltx_align_right">0</td>
<td id="S1.T1.1.5.5.5" class="ltx_td ltx_align_right">1,000,000</td>
<td id="S1.T1.1.5.5.6" class="ltx_td ltx_align_right">0</td>
</tr>
<tr id="S1.T1.1.6.6" class="ltx_tr">
<th id="S1.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">iSAID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</th>
<td id="S1.T1.1.6.6.2" class="ltx_td ltx_align_right">44.9</td>
<td id="S1.T1.1.6.6.3" class="ltx_td ltx_align_right">15</td>
<td id="S1.T1.1.6.6.4" class="ltx_td ltx_align_right">0</td>
<td id="S1.T1.1.6.6.5" class="ltx_td ltx_align_right">655,451</td>
<td id="S1.T1.1.6.6.6" class="ltx_td ltx_align_right">0</td>
</tr>
<tr id="S1.T1.1.7.7" class="ltx_tr">
<th id="S1.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FMOW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S1.T1.1.7.7.2" class="ltx_td ltx_align_right">1084.0</td>
<td id="S1.T1.1.7.7.3" class="ltx_td ltx_align_right">63</td>
<td id="S1.T1.1.7.7.4" class="ltx_td ltx_align_right">0</td>
<td id="S1.T1.1.7.7.5" class="ltx_td ltx_align_right">132716</td>
<td id="S1.T1.1.7.7.6" class="ltx_td ltx_align_right">0</td>
</tr>
<tr id="S1.T1.1.8.8" class="ltx_tr">
<th id="S1.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> + GTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S1.T1.1.8.8.2" class="ltx_td ltx_align_right ltx_border_t">537.5</td>
<td id="S1.T1.1.8.8.3" class="ltx_td ltx_align_right ltx_border_t">30/19</td>
<td id="S1.T1.1.8.8.4" class="ltx_td ltx_align_right ltx_border_t">0</td>
<td id="S1.T1.1.8.8.5" class="ltx_td ltx_align_right ltx_border_t">210,179</td>
<td id="S1.T1.1.8.8.6" class="ltx_td ltx_align_right ltx_border_t">510,4434</td>
</tr>
<tr id="S1.T1.1.9.9" class="ltx_tr">
<th id="S1.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">COCOA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> + SAIL-VOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S1.T1.1.9.9.2" class="ltx_td ltx_align_right">115.7</td>
<td id="S1.T1.1.9.9.3" class="ltx_td ltx_align_right">-/163</td>
<td id="S1.T1.1.9.9.4" class="ltx_td ltx_align_right">0</td>
<td id="S1.T1.1.9.9.5" class="ltx_td ltx_align_right">46,314</td>
<td id="S1.T1.1.9.9.6" class="ltx_td ltx_align_right">1,896,296</td>
</tr>
<tr id="S1.T1.1.10.10" class="ltx_tr">
<th id="S1.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Animals with Attributes 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</th>
<td id="S1.T1.1.10.10.2" class="ltx_td ltx_align_right ltx_border_t">24.7</td>
<td id="S1.T1.1.10.10.3" class="ltx_td ltx_align_right ltx_border_t">50</td>
<td id="S1.T1.1.10.10.4" class="ltx_td ltx_align_right ltx_border_t">85</td>
<td id="S1.T1.1.10.10.5" class="ltx_td ltx_align_right ltx_border_t">37,322</td>
<td id="S1.T1.1.10.10.6" class="ltx_td ltx_align_right ltx_border_t">0</td>
</tr>
<tr id="S1.T1.1.11.11" class="ltx_tr">
<th id="S1.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CompCars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</th>
<td id="S1.T1.1.11.11.2" class="ltx_td ltx_align_right">86.1</td>
<td id="S1.T1.1.11.11.3" class="ltx_td ltx_align_right">1,716</td>
<td id="S1.T1.1.11.11.4" class="ltx_td ltx_align_right">13</td>
<td id="S1.T1.1.11.11.5" class="ltx_td ltx_align_right">136,726</td>
<td id="S1.T1.1.11.11.6" class="ltx_td ltx_align_right">0</td>
</tr>
<tr id="S1.T1.1.12.12" class="ltx_tr">
<th id="S1.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_tt"><span id="S1.T1.1.12.12.1.1" class="ltx_text ltx_font_bold">RarePlanes (Ours)</span></th>
<td id="S1.T1.1.12.12.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt">187.1</td>
<td id="S1.T1.1.12.12.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt">1 to 110</td>
<td id="S1.T1.1.12.12.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt">10</td>
<td id="S1.T1.1.12.12.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt">14,707</td>
<td id="S1.T1.1.12.12.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt">629,551</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S1.T1.3.1" class="ltx_text ltx_font_bold">Comparison with other synthetic, attribute and overhead imagery datasets.</span> Our dataset has a similar scale as modern computer vision datasets and provides both a real and synthetic component. For SpaceNet (Buildings + Road Speed), xBD (Building Damage Scale), and RarePlanes we report the range of possible customizable classes that end-users can create using varieties of the dataset attributes.</figcaption>
</figure>
<section id="S1.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Contributions</h3>

<div id="S1.SSx1.p1" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">An expansive real and synthetic overhead computer vision dataset focused on the detection of aircraft and their features.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Annotations with fine-grain attributions that enable various CV tasks such as: detection, instance segmentation, or zero-shot learning.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experiments to evaluate the real and synthetic datasets and compare performances. By doing so, we show the value of synthetic images for the task of detecting and classifying aircraft from an overhead perspective.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">RarePlanes sits at the intersection of three distinct computer vision dataset domains: synthetic datasets, geospatial datasets, and fine-grain attribution datasets. These three domains are cornerstones around which computer vision research has continued to rapidly advance and grow. We summarize the key characteristics of modern synthetic, geospatial, and attribute datasets in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and compare them to the RarePlanes dataset.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Synthetic data has become prevalent across many computer vision domains and has shown value as a replacement for real data or to augment existing training datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Many synthetic datasets focus on the autonomous driving domain; including the Synthia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, GTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and vKITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> datasets. These synthetic datasets are often paired with real-world data such as Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, CamVid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, or KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to benchmark the value of synthetic data. Other notable synthetic datasets such as SUNCG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> or Matterport3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> focus on indoor scenes and include RGB-D data for depth estimation. Moreover, other datasets focus on addressing challenging occlusion (amodal) problems such as the expansive SAIL-VOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and DYCE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Finally, the Synthinel-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> Dataset is the only other dataset that bridges the synthetic/satellite domain. It features synthetic data from an overhead perspective with binary pixel masks of building footprints. Overall, combined synthetic and real datasets, similar to RarePlanes, have been helpful with several different tasks including: enhancing object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, or instance segmentation performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Furthermore, such datasets continue to inspire new domain adaptation (DA) techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. Such DA techniques could be particularly valuable for overhead applications as there remains a dearth of openly available training data and models trained on one location often do not generalize well to new areas.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2006.02963/assets/figs/Annotation_Styles.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="62" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.2.1" class="ltx_text ltx_font_bold">Three annotation styles within RarePlanes.</span> The dataset features three annotation styles including: ’Bounding Box’, ’Diamond Polygon’, and ’Full Instance Segmentation’ (synthetic only).</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Geospatial Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Geospatial and very-high resolution remote sensing datasets have continued to draw increased interest due to their relevancy to many computer vision challenges. Such datasets contain lower resolution images with tiny, closely grouped objects with varying aspect ratios, arbitrary orientations and high annotation density. The lessons learned from such datasets continue to inspire new computer vision approaches related to detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, super-resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and even bridges to natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. Some notable datasets include SpaceNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and xBD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which focus on foundational mapping and instance/semantic segmentation for problems such as building footprint and road network extraction or building damage assessment. Others such as xView <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, A large-scale dataset for object detection in aerial images (DOTA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and A Large-scale Dataset for Instance Segmentation in Aerial Images (iSAID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> focus on overhead object detection or instance segmentation, featuring multiple classes of different object types. The Functional Map of the World (FMOW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> dataset centers on the task of classification of smaller image chips from an overhead perspective. Other datasets such as CORE3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, MVS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the ISPRS benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and SatStereo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> target height extraction and 3D mapping from space. RarePlanes builds upon these existing datasets and contributes both synthetic and real data. Furthermore, RarePlanes adds 10 unique object attributes, which enable customizable classes, as well as three annotation styles per object (Bounding Box, Diamond Polygon, and Full-Instance (Synthetic Only)).</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fine-Grain Attribute Datasets</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Many datasets focus on identifying general objects in imagery, however, several others take an alternative approach and label unique attributes of each object. As previously stated, RarePlanes features 10 attributes and 33 sub-attributes. Such attribution has been particularly valuable for constructing new zero-shot learning methods and algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The Comprehensive Cars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> dataset is similar to RarePlanes and features attribute labels of 5 car attributes and 8 car-parts, as well as different look angles of vehicles. Several other similar datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> feature multiple classes with extensive ranges in attributes; most of which are geared toward zero-shot learning research.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The RarePlanes Dataset and Statistics</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Annotations, Features, and Attributes</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The RarePlanes dataset contains 14,707 real and 629,551 synthetic annotations of aircraft. Each aircraft is labeled in a diamond style with the nose, left-wing, tail, and right-wing being labeled in successive order (Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Synthetic Datasets ‣ 2 Related Work ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). This annotation style has the advantage of being: simplistic, easily reproducible, convertible to a bounding box, and ensures that aircraft are consistently annotated (other hand-annotated formats can often lead to imprecise labeling). Furthermore, this annotation style enables the calculation of aircraft length and wingspan by measuring between the first annotation node to the third and from the second to the fourth. We employ a professional labeling service to produce high-quality annotations for the real portion of the dataset. Two rounds of quality control are included in the process, a first one by the professional service and a second by the authors.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">After each aircraft is annotated in the diamond format, an expert geospatial team labels aircraft features. The features include attributes of aircraft <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">wings</span>, <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_bold">engines</span>, <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_bold">fuselage</span>, <span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_bold">tail</span>, and <span id="S3.SS1.p2.1.5" class="ltx_text ltx_font_bold">role</span> (Figure <a href="#S3.F7" title="Figure 7 ‣ 3.1 Annotations, Features, and Attributes ‣ 3 The RarePlanes Dataset and Statistics ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). We ultimately chose these attributes as they were visually distinctive from an overhead perspective and have been shown to be helpful in aiding to visually identifying the type or make of aircraft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Wings:</span> We label aircraft <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">Wing Shape:</span> (‘straight’, ‘swept’, ‘delta’, and ‘variable-swept’), <span id="S3.I1.i1.p1.1.3" class="ltx_text ltx_font_bold">Wing Position:</span> (‘high mounted’ and ‘mid/low mounted’), <span id="S3.I1.i1.p1.1.4" class="ltx_text ltx_font_bold">Wingspan in Meters:</span> (‘float’), and the <span id="S3.I1.i1.p1.1.5" class="ltx_text ltx_font_bold">FAA Aircraft Design Group Wingspan Class:</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> (‘1’ to ‘6’) which determines which airports can accommodate different sized aircraft. Examples of wing-shape and position can be seen in figure <a href="#S3.F3" title="Figure 3 ‣ 1st item ‣ 3.1 Annotations, Features, and Attributes ‣ 3 The RarePlanes Dataset and Statistics ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2006.02963/assets/figs/WingTypes3.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="299" height="152" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold">Wing Shapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> present in the RarePlanes dataset.</span> Note that the two left-most aircraft feature ‘high-mounted’ wings, with the two right-most aircraft featuring ‘mid/low mounted’ wings.</figcaption>
</figure>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Fuselage:</span> We label aircraft <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">Length in Meters:</span> (‘float’) and if the plane has <span id="S3.I1.i2.p1.1.3" class="ltx_text ltx_font_bold">Canards:</span> (‘yes’ or ‘no’). Canards are small fore-wings that are added to planes to increase maneuverability or reduce the load/airflow on the main wing.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Engines:</span> we label the <span id="S3.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">Number of Engines:</span> (‘0’ to ‘4’) and the <span id="S3.I1.i3.p1.1.3" class="ltx_text ltx_font_bold">Type of Propulsion:</span> (‘unpowered’, ‘jet’, ‘propeller’).</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/1EngineProp.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/2EngineProp.png" id="S3.F4.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/3EngineJet.png" id="S3.F4.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/4EngineJet.png" id="S3.F4.4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.6.1" class="ltx_text ltx_font_bold">Engine Types. From left to right:</span> A single engine propeller aircraft, a two engine propeller aircraft, a three engine jet aircraft, and a four engine jet aircraft. </figcaption>
</figure>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Tail:</span> We label the <span id="S3.I1.i4.p1.1.2" class="ltx_text ltx_font_bold">Number of Vertical Stabilizers:</span> (‘1’ or ‘2’) or tail fins that each aircraft possesses.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2006.02963/assets/figs/TailFins_Canards.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="225" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S3.F5.3.1" class="ltx_text ltx_font_bold">Examples of fuselage and tail attributes including canards and number of vertical stabilizers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> present in the RarePlanes dataset.</span> </figcaption>
</figure>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Role:</span> After labeling each attribute, we then use these attributes to classify the <span id="S3.I1.i5.p1.1.2" class="ltx_text ltx_font_bold">Role or Type:</span> of an aircraft into seven unique classes. These include: ’Civil Transport/Utility’ (‘Small’, ‘Medium, and ‘Large’ based upon wingspan), ‘Military Transport/Utility/AWAC’, ‘Military Bomber’, ‘Military Fighter/Interceptor/Attack’, and ‘Military Trainer’. Further detail on role definitions and can be found in the RarePlanes User Guide, hosted on AWS with the dataset.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/SmallCivil_150Scale.jpeg" id="S3.F6.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="75" height="75" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/MedCivil_150Scale.jpeg" id="S3.F6.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="75" height="75" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/LargeCivil_150Scale.jpeg" id="S3.F6.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="75" height="75" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2006.02963/assets/figs/MilFighter_150Scale.jpeg" id="S3.F6.4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="75" height="75" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2006.02963/assets/figs/MilBomber_150Scale.jpeg" id="S3.F6.5.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="75" height="75" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2006.02963/assets/figs/MilTrans_150Scale.jpeg" id="S3.F6.6.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="75" height="75" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S3.F6.9.1" class="ltx_text ltx_font_bold">Aircraft by role at 1:150 scale. Note the heterogeneous aircraft styles and variation in sizes. Top Row:</span> Small, Medium and Large Civil Transports. <span id="S3.F6.10.2" class="ltx_text ltx_font_bold">Bottom Row:</span> Military Fighter, Bomber, and Transport. </figcaption>
</figure>
</li>
</ul>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2006.02963/assets/figs/attribute_tree_vertical.jpg" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="275" height="440" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S3.F7.6.3" class="ltx_text ltx_font_bold">The <math id="S3.F7.4.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.F7.4.1.m1.1b"><mn id="S3.F7.4.1.m1.1.1" xref="S3.F7.4.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.F7.4.1.m1.1c"><cn type="integer" id="S3.F7.4.1.m1.1.1.cmml" xref="S3.F7.4.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.4.1.m1.1d">5</annotation></semantics></math> features, <math id="S3.F7.5.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.F7.5.2.m2.1b"><mn id="S3.F7.5.2.m2.1.1" xref="S3.F7.5.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.F7.5.2.m2.1c"><cn type="integer" id="S3.F7.5.2.m2.1.1.cmml" xref="S3.F7.5.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.5.2.m2.1d">10</annotation></semantics></math> attributes, and <math id="S3.F7.6.3.m3.1" class="ltx_Math" alttext="33" display="inline"><semantics id="S3.F7.6.3.m3.1b"><mn id="S3.F7.6.3.m3.1.1" xref="S3.F7.6.3.m3.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S3.F7.6.3.m3.1c"><cn type="integer" id="S3.F7.6.3.m3.1.1.cmml" xref="S3.F7.6.3.m3.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.6.3.m3.1d">33</annotation></semantics></math> sub-attributes contained in the RarePlanes dataset.</span> The dataset and associated codebase enables users to create custom classes using groupings of these attributes.</figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2006.02963/assets/figs/Real_Synth_Map.jpg" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="S3.F8.2.1" class="ltx_text ltx_font_bold">RarePlanes dataset locations.</span> The dataset features 112 real (blue points) and 15 synthetic locations (red points). Atlanta, Miami, and Salt Lake City feature both real and synthetic data.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Real Imagery and Locations</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.16" class="ltx_p">All electro-optical imagery is provided by the Maxar Worldview-3 satellite with a maximum ground sample distance (GSD) of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="0.31" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">0.31</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="float" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">0.31</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">0.31</annotation></semantics></math> to <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="0.39" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">0.39</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="float" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">0.39</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">0.39</annotation></semantics></math> meters depending upon sensor look-angle. The dataset consists of <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="253" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">253</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">253</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">253</annotation></semantics></math> unique scenes, spanning <math id="S3.SS2.p1.4.m4.2" class="ltx_Math" alttext="2,142\,\rm{km}^{2}" display="inline"><semantics id="S3.SS2.p1.4.m4.2a"><mrow id="S3.SS2.p1.4.m4.2.2.1" xref="S3.SS2.p1.4.m4.2.2.2.cmml"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">2</mn><mo id="S3.SS2.p1.4.m4.2.2.1.2" xref="S3.SS2.p1.4.m4.2.2.2.cmml">,</mo><mrow id="S3.SS2.p1.4.m4.2.2.1.1" xref="S3.SS2.p1.4.m4.2.2.1.1.cmml"><mn id="S3.SS2.p1.4.m4.2.2.1.1.2" xref="S3.SS2.p1.4.m4.2.2.1.1.2.cmml">142</mn><mo lspace="0.170em" rspace="0em" id="S3.SS2.p1.4.m4.2.2.1.1.1" xref="S3.SS2.p1.4.m4.2.2.1.1.1.cmml">​</mo><msup id="S3.SS2.p1.4.m4.2.2.1.1.3" xref="S3.SS2.p1.4.m4.2.2.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.2.2.1.1.3.2" xref="S3.SS2.p1.4.m4.2.2.1.1.3.2.cmml">km</mi><mn id="S3.SS2.p1.4.m4.2.2.1.1.3.3" xref="S3.SS2.p1.4.m4.2.2.1.1.3.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.2b"><list id="S3.SS2.p1.4.m4.2.2.2.cmml" xref="S3.SS2.p1.4.m4.2.2.1"><cn type="integer" id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">2</cn><apply id="S3.SS2.p1.4.m4.2.2.1.1.cmml" xref="S3.SS2.p1.4.m4.2.2.1.1"><times id="S3.SS2.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS2.p1.4.m4.2.2.1.1.1"></times><cn type="integer" id="S3.SS2.p1.4.m4.2.2.1.1.2.cmml" xref="S3.SS2.p1.4.m4.2.2.1.1.2">142</cn><apply id="S3.SS2.p1.4.m4.2.2.1.1.3.cmml" xref="S3.SS2.p1.4.m4.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.2.2.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.2.2.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.4.m4.2.2.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.2.2.1.1.3.2">km</ci><cn type="integer" id="S3.SS2.p1.4.m4.2.2.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.2.2.1.1.3.3">2</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.2c">2,142\,\rm{km}^{2}</annotation></semantics></math> with <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="112" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">112</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><cn type="integer" id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">112</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">112</annotation></semantics></math> locations in <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mn id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><cn type="integer" id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">22</annotation></semantics></math> countries. Locations were chosen by performing a stratified random sampling of OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> aerodromes of area <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="\geq 1\,\rm{km}^{2}" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mrow id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml"></mi><mo id="S3.SS2.p1.7.m7.1.1.1" xref="S3.SS2.p1.7.m7.1.1.1.cmml">≥</mo><mrow id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml"><mn id="S3.SS2.p1.7.m7.1.1.3.2" xref="S3.SS2.p1.7.m7.1.1.3.2.cmml">1</mn><mo lspace="0.170em" rspace="0em" id="S3.SS2.p1.7.m7.1.1.3.1" xref="S3.SS2.p1.7.m7.1.1.3.1.cmml">​</mo><msup id="S3.SS2.p1.7.m7.1.1.3.3" xref="S3.SS2.p1.7.m7.1.1.3.3.cmml"><mi id="S3.SS2.p1.7.m7.1.1.3.3.2" xref="S3.SS2.p1.7.m7.1.1.3.3.2.cmml">km</mi><mn id="S3.SS2.p1.7.m7.1.1.3.3.3" xref="S3.SS2.p1.7.m7.1.1.3.3.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><geq id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1.1"></geq><csymbol cd="latexml" id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2">absent</csymbol><apply id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3"><times id="S3.SS2.p1.7.m7.1.1.3.1.cmml" xref="S3.SS2.p1.7.m7.1.1.3.1"></times><cn type="integer" id="S3.SS2.p1.7.m7.1.1.3.2.cmml" xref="S3.SS2.p1.7.m7.1.1.3.2">1</cn><apply id="S3.SS2.p1.7.m7.1.1.3.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.1.1.3.3.1.cmml" xref="S3.SS2.p1.7.m7.1.1.3.3">superscript</csymbol><ci id="S3.SS2.p1.7.m7.1.1.3.3.2.cmml" xref="S3.SS2.p1.7.m7.1.1.3.3.2">km</ci><cn type="integer" id="S3.SS2.p1.7.m7.1.1.3.3.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">\geq 1\,\rm{km}^{2}</annotation></semantics></math> across the US and Europe using the Köppen climate zone as the stratification layer. We stratify by climate to increase seasonal diversity and geographic heterogeneity. Seven additional locations were manually chosen as they overlap with preexisting datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and we considered further revisits over these locations to potentially have additional value. We then chose individual satellite scenes by attempting to select scenes from different seasons for each location. Many locations have several scenes taken at different points in time, which may enable future investigation on the value of annotating the same areas using multiple images. The imagery is collected from variable look-angles (<math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="3.2" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mn id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">3.2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><cn type="float" id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">3.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">3.2</annotation></semantics></math> to <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="29.6^{\circ}" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><msup id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml"><mn id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">29.6</mn><mo id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">superscript</csymbol><cn type="float" id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2">29.6</cn><compose id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">29.6^{\circ}</annotation></semantics></math>), target azimuth angles (<math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="1.8" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><mn id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">1.8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><cn type="float" id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">1.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">1.8</annotation></semantics></math> to <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="359.7^{\circ}" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><msup id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml"><mn id="S3.SS2.p1.11.m11.1.1.2" xref="S3.SS2.p1.11.m11.1.1.2.cmml">359.7</mn><mo id="S3.SS2.p1.11.m11.1.1.3" xref="S3.SS2.p1.11.m11.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><apply id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m11.1.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">superscript</csymbol><cn type="float" id="S3.SS2.p1.11.m11.1.1.2.cmml" xref="S3.SS2.p1.11.m11.1.1.2">359.7</cn><compose id="S3.SS2.p1.11.m11.1.1.3.cmml" xref="S3.SS2.p1.11.m11.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">359.7^{\circ}</annotation></semantics></math>), and sun elevation angles (<math id="S3.SS2.p1.12.m12.1" class="ltx_Math" alttext="10.7" display="inline"><semantics id="S3.SS2.p1.12.m12.1a"><mn id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml">10.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><cn type="float" id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">10.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">10.7</annotation></semantics></math> to <math id="S3.SS2.p1.13.m13.1" class="ltx_Math" alttext="79.0^{\circ}" display="inline"><semantics id="S3.SS2.p1.13.m13.1a"><msup id="S3.SS2.p1.13.m13.1.1" xref="S3.SS2.p1.13.m13.1.1.cmml"><mn id="S3.SS2.p1.13.m13.1.1.2" xref="S3.SS2.p1.13.m13.1.1.2.cmml">79.0</mn><mo id="S3.SS2.p1.13.m13.1.1.3" xref="S3.SS2.p1.13.m13.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m13.1b"><apply id="S3.SS2.p1.13.m13.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m13.1.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1">superscript</csymbol><cn type="float" id="S3.SS2.p1.13.m13.1.1.2.cmml" xref="S3.SS2.p1.13.m13.1.1.2">79.0</cn><compose id="S3.SS2.p1.13.m13.1.1.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m13.1c">79.0^{\circ}</annotation></semantics></math>). Imagery is collected from all four seasons, with scenes featuring instances of cloud cover (<math id="S3.SS2.p1.14.m14.1" class="ltx_Math" alttext="12.6\%" display="inline"><semantics id="S3.SS2.p1.14.m14.1a"><mrow id="S3.SS2.p1.14.m14.1.1" xref="S3.SS2.p1.14.m14.1.1.cmml"><mn id="S3.SS2.p1.14.m14.1.1.2" xref="S3.SS2.p1.14.m14.1.1.2.cmml">12.6</mn><mo id="S3.SS2.p1.14.m14.1.1.1" xref="S3.SS2.p1.14.m14.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m14.1b"><apply id="S3.SS2.p1.14.m14.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1"><csymbol cd="latexml" id="S3.SS2.p1.14.m14.1.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p1.14.m14.1.1.2.cmml" xref="S3.SS2.p1.14.m14.1.1.2">12.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m14.1c">12.6\%</annotation></semantics></math>), snow (<math id="S3.SS2.p1.15.m15.1" class="ltx_Math" alttext="9.1\%" display="inline"><semantics id="S3.SS2.p1.15.m15.1a"><mrow id="S3.SS2.p1.15.m15.1.1" xref="S3.SS2.p1.15.m15.1.1.cmml"><mn id="S3.SS2.p1.15.m15.1.1.2" xref="S3.SS2.p1.15.m15.1.1.2.cmml">9.1</mn><mo id="S3.SS2.p1.15.m15.1.1.1" xref="S3.SS2.p1.15.m15.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.15.m15.1b"><apply id="S3.SS2.p1.15.m15.1.1.cmml" xref="S3.SS2.p1.15.m15.1.1"><csymbol cd="latexml" id="S3.SS2.p1.15.m15.1.1.1.cmml" xref="S3.SS2.p1.15.m15.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p1.15.m15.1.1.2.cmml" xref="S3.SS2.p1.15.m15.1.1.2">9.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.15.m15.1c">9.1\%</annotation></semantics></math>) and clear skies (<math id="S3.SS2.p1.16.m16.1" class="ltx_Math" alttext="78.3\%" display="inline"><semantics id="S3.SS2.p1.16.m16.1a"><mrow id="S3.SS2.p1.16.m16.1.1" xref="S3.SS2.p1.16.m16.1.1.cmml"><mn id="S3.SS2.p1.16.m16.1.1.2" xref="S3.SS2.p1.16.m16.1.1.2.cmml">78.3</mn><mo id="S3.SS2.p1.16.m16.1.1.1" xref="S3.SS2.p1.16.m16.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.16.m16.1b"><apply id="S3.SS2.p1.16.m16.1.1.cmml" xref="S3.SS2.p1.16.m16.1.1"><csymbol cd="latexml" id="S3.SS2.p1.16.m16.1.1.1.cmml" xref="S3.SS2.p1.16.m16.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p1.16.m16.1.1.2.cmml" xref="S3.SS2.p1.16.m16.1.1.2">78.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.16.m16.1c">78.3\%</annotation></semantics></math>). Combined together, this leads to high variability in illumination, shadowing, and lighting conditions. Consequently, the dataset should help to improve generalizability to new areas. Finally, background surfaces are quite diverse with grass, dirt, concrete, and asphalt surface types.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.8" class="ltx_p">The collection is composed of three different sets of data with different spatial resolutions: one panchromatic band (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="0.31-0.39" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">0.31</mn><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">−</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">0.39</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><minus id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></minus><cn type="float" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">0.31</cn><cn type="float" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">0.39</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">0.31-0.39</annotation></semantics></math>m), eight multi-spectral (coastal to NIR (<math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="400-954\mu" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mn id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">400</mn><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">−</mo><mrow id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mn id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">954</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.1" xref="S3.SS2.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml">μ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><minus id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></minus><cn type="integer" id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">400</cn><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><times id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.1"></times><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">954</cn><ci id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3">𝜇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">400-954\mu</annotation></semantics></math>m)) bands (<math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="1.24-1.56" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mn id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">1.24</mn><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">−</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">1.56</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><minus id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></minus><cn type="float" id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">1.24</cn><cn type="float" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">1.56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">1.24-1.56</annotation></semantics></math>m), and three RGB (<math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="448-692\mu" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mn id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">448</mn><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">−</mo><mrow id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mn id="S3.SS2.p2.4.m4.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.cmml">692</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.1" xref="S3.SS2.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml">μ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><minus id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></minus><cn type="integer" id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">448</cn><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><times id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.1"></times><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2">692</cn><ci id="S3.SS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3">𝜇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">448-692\mu</annotation></semantics></math>m) pan-sharpened bands (<math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="0.31-0.39" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mn id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">0.31</mn><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">−</mo><mn id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">0.39</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><minus id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></minus><cn type="float" id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">0.31</cn><cn type="float" id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">0.39</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">0.31-0.39</annotation></semantics></math>m). Each data product is atmospherically compensated to surface-reflectance values by Maxar’s AComp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and ortho-rectified using the SRTM DEM. RGB data is also converted to 8-bit. Areas containing non-valid imagery are set to <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mn id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><cn type="integer" id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">0</cn></annotation-xml></semantics></math>. We distribute both <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mn id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><times id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"></times><cn type="integer" id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">512</cn><cn type="integer" id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">512\times 512</annotation></semantics></math> pixel tiles (<math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mrow id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mn id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml">20</mn><mo id="S3.SS2.p2.8.m8.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><csymbol cd="latexml" id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">20\%</annotation></semantics></math> overlap) that contain aircraft as well as as the full images, cropped to the extent of the area of annotation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Data</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.7" class="ltx_p">The synthetic dataset contains <math id="S3.SS3.p1.1.m1.2" class="ltx_Math" alttext="629,551" display="inline"><semantics id="S3.SS3.p1.1.m1.2a"><mrow id="S3.SS3.p1.1.m1.2.3.2" xref="S3.SS3.p1.1.m1.2.3.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">629</mn><mo id="S3.SS3.p1.1.m1.2.3.2.1" xref="S3.SS3.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p1.1.m1.2.2" xref="S3.SS3.p1.1.m1.2.2.cmml">551</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.2b"><list id="S3.SS3.p1.1.m1.2.3.1.cmml" xref="S3.SS3.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">629</cn><cn type="integer" id="S3.SS3.p1.1.m1.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2">551</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.2c">629,551</annotation></semantics></math> annotations of aircraft across <math id="S3.SS3.p1.2.m2.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.3.2" xref="S3.SS3.p1.2.m2.2.3.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">50</mn><mo id="S3.SS3.p1.2.m2.2.3.2.1" xref="S3.SS3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><list id="S3.SS3.p1.2.m2.2.3.1.cmml" xref="S3.SS3.p1.2.m2.2.3.2"><cn type="integer" id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">50</cn><cn type="integer" id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">50,000</annotation></semantics></math> images with an extent of <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">1920</cn><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">1920\times 1080</annotation></semantics></math> pixels at 15 distinct locations, simulating a total area of <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="9331.2\,\rm{km}^{2}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mn id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">9331.2</mn><mo lspace="0.170em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">​</mo><msup id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.2" xref="S3.SS3.p1.4.m4.1.1.3.2.cmml">km</mi><mn id="S3.SS3.p1.4.m4.1.1.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><times id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1"></times><cn type="float" id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">9331.2</cn><apply id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2">km</ci><cn type="integer" id="S3.SS3.p1.4.m4.1.1.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">9331.2\,\rm{km}^{2}</annotation></semantics></math>. All synthetic data is created via AI.Reverie’s novel simulator software in a multi-step process. First, individual real-world locations are simulated using a proprietary procedural GIS framework integrated with Houdini <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. The framework ingests geospatial vector data (OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>) and overhead images to procedurally generate building and airport terminal models. Airport ground and runways are then generated through simple planar shape and texture projections. Once the simulated locations are generated, we import each location into unreal engine 4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Capture modules and settings are configured to spawn aircraft and simulate the environmental factors and biomes. Each image features a simulated GSD of <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mn id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><cn type="float" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">0.3</annotation></semantics></math> meters to closely approximate our real imagery and is collected from variable look-angles ranging between <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="5.0" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><mn id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">5.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><cn type="float" id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">5.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">5.0</annotation></semantics></math> to <math id="S3.SS3.p1.7.m7.1" class="ltx_Math" alttext="30.0^{\circ}" display="inline"><semantics id="S3.SS3.p1.7.m7.1a"><msup id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><mn id="S3.SS3.p1.7.m7.1.1.2" xref="S3.SS3.p1.7.m7.1.1.2.cmml">30.0</mn><mo id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">superscript</csymbol><cn type="float" id="S3.SS3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2">30.0</cn><compose id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">30.0^{\circ}</annotation></semantics></math> off-nadir. The imagery is evenly split across 5 distinct biomes including: ‘Alpine’, ‘Arctic’, ‘Temperate Evergreen Forests’, ‘Grasslands’, and ‘Tundra’. The biome parameter controls the type of vegetation, its density, as well as the ground textures. Four unique weather conditions are also evenly distributed across the dataset including: ‘Overcast’, ‘Clear Sky’, ‘Snow’, and ‘Rain’. Other parameters include the sunlight intensity, weather intensity, and the time of the day. Ultimately, this produces an expansive heterogeneous dataset with a wide variety of backgrounds. We believe that this dataset will be helpful in improving model generalizability to new areas and developing new algorithmic approaches that could move beyond aircraft detection.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments, Results, and Discussion</h2>

<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3" class="ltx_tr">
<th id="S4.T2.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">network</th>
<th id="S4.T2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">attribute</th>
<th id="S4.T2.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">dataset</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="C_{S}" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml">C</mi><mi id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">𝐶</ci><ci id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">C_{S}</annotation></semantics></math></th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="C_{M}" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><msub id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.m1.1.1.2.cmml">C</mi><mi id="S4.T2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.m1.1.1.2">𝐶</ci><ci id="S4.T2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">C_{M}</annotation></semantics></math></th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="C_{L}" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><msub id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml"><mi id="S4.T2.3.3.3.m1.1.1.2" xref="S4.T2.3.3.3.m1.1.1.2.cmml">C</mi><mi id="S4.T2.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.m1.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T2.3.3.3.m1.1.1.2.cmml" xref="S4.T2.3.3.3.m1.1.1.2">𝐶</ci><ci id="S4.T2.3.3.3.m1.1.1.3.cmml" xref="S4.T2.3.3.3.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">C_{L}</annotation></semantics></math></th>
<th id="S4.T2.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="S4.T2.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP50</th>
<th id="S4.T2.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AR</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.4.1" class="ltx_tr">
<th id="S4.T2.3.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">aircraft</td>
<td id="S4.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">Real</td>
<td id="S4.T2.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.3.4.1.5" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.3.4.1.6" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.3.4.1.7" class="ltx_td ltx_align_center ltx_border_t">73.32 (0.34)</td>
<td id="S4.T2.3.4.1.8" class="ltx_td ltx_align_center ltx_border_t">96.80 (0.02)</td>
<td id="S4.T2.3.4.1.9" class="ltx_td ltx_align_center ltx_border_t">77.16 (0.21)</td>
</tr>
<tr id="S4.T2.3.5.2" class="ltx_tr">
<th id="S4.T2.3.5.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.3.5.2.2" class="ltx_td ltx_align_center">aircraft</td>
<td id="S4.T2.3.5.2.3" class="ltx_td ltx_align_center">Synth.</td>
<td id="S4.T2.3.5.2.4" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.5.2.5" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.5.2.6" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.5.2.7" class="ltx_td ltx_align_center">54.86 (0.25)</td>
<td id="S4.T2.3.5.2.8" class="ltx_td ltx_align_center">87.03 (0.53)</td>
<td id="S4.T2.3.5.2.9" class="ltx_td ltx_align_center">60.67 (0.27)</td>
</tr>
<tr id="S4.T2.3.6.3" class="ltx_tr">
<th id="S4.T2.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.3.6.3.1.1" class="ltx_text">Faster R-CNN</span></th>
<td id="S4.T2.3.6.3.2" class="ltx_td ltx_align_center">aircraft</td>
<td id="S4.T2.3.6.3.3" class="ltx_td ltx_align_center">FT</td>
<td id="S4.T2.3.6.3.4" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.6.3.5" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.6.3.6" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.6.3.7" class="ltx_td ltx_align_center">69.16 (0.69)</td>
<td id="S4.T2.3.6.3.8" class="ltx_td ltx_align_center">95.29 (0.41)</td>
<td id="S4.T2.3.6.3.9" class="ltx_td ltx_align_center">73.03 (0.57)</td>
</tr>
<tr id="S4.T2.3.7.4" class="ltx_tr">
<td id="S4.T2.3.7.4.1" class="ltx_td ltx_align_center ltx_border_t">role</td>
<td id="S4.T2.3.7.4.2" class="ltx_td ltx_align_center ltx_border_t">Real</td>
<td id="S4.T2.3.7.4.3" class="ltx_td ltx_align_center ltx_border_t">66.68</td>
<td id="S4.T2.3.7.4.4" class="ltx_td ltx_align_center ltx_border_t">70.26</td>
<td id="S4.T2.3.7.4.5" class="ltx_td ltx_align_center ltx_border_t">67.68</td>
<td id="S4.T2.3.7.4.6" class="ltx_td ltx_align_center ltx_border_t">68.21 (0.4)</td>
<td id="S4.T2.3.7.4.7" class="ltx_td ltx_align_center ltx_border_t">92.16 (0.23)</td>
<td id="S4.T2.3.7.4.8" class="ltx_td ltx_align_center ltx_border_t">75.39 (0.40)</td>
</tr>
<tr id="S4.T2.3.8.5" class="ltx_tr">
<th id="S4.T2.3.8.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.3.8.5.2" class="ltx_td ltx_align_center">role</td>
<td id="S4.T2.3.8.5.3" class="ltx_td ltx_align_center">Synth.</td>
<td id="S4.T2.3.8.5.4" class="ltx_td ltx_align_center">27.70</td>
<td id="S4.T2.3.8.5.5" class="ltx_td ltx_align_center">37.09</td>
<td id="S4.T2.3.8.5.6" class="ltx_td ltx_align_center">42.85</td>
<td id="S4.T2.3.8.5.7" class="ltx_td ltx_align_center">35.88 (2.26)</td>
<td id="S4.T2.3.8.5.8" class="ltx_td ltx_align_center">59.09 (2.9)</td>
<td id="S4.T2.3.8.5.9" class="ltx_td ltx_align_center">53.82 (1.28)</td>
</tr>
<tr id="S4.T2.3.9.6" class="ltx_tr">
<th id="S4.T2.3.9.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.3.9.6.2" class="ltx_td ltx_align_center">role</td>
<td id="S4.T2.3.9.6.3" class="ltx_td ltx_align_center">FT</td>
<td id="S4.T2.3.9.6.4" class="ltx_td ltx_align_center">56.73</td>
<td id="S4.T2.3.9.6.5" class="ltx_td ltx_align_center">66.05</td>
<td id="S4.T2.3.9.6.6" class="ltx_td ltx_align_center">66.52</td>
<td id="S4.T2.3.9.6.7" class="ltx_td ltx_align_center">63.10 (0.78)</td>
<td id="S4.T2.3.9.6.8" class="ltx_td ltx_align_center">89.15 (0.22)</td>
<td id="S4.T2.3.9.6.9" class="ltx_td ltx_align_center">71.06 (0.75)</td>
</tr>
<tr id="S4.T2.3.10.7" class="ltx_tr">
<th id="S4.T2.3.10.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T2.3.10.7.2" class="ltx_td ltx_align_center ltx_border_t">aircraft</td>
<td id="S4.T2.3.10.7.3" class="ltx_td ltx_align_center ltx_border_t">Real</td>
<td id="S4.T2.3.10.7.4" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.3.10.7.5" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.3.10.7.6" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.3.10.7.7" class="ltx_td ltx_align_center ltx_border_t">73.67 (0.17)</td>
<td id="S4.T2.3.10.7.8" class="ltx_td ltx_align_center ltx_border_t">96.81 (0.03)</td>
<td id="S4.T2.3.10.7.9" class="ltx_td ltx_align_center ltx_border_t">76.46 (0.20)</td>
</tr>
<tr id="S4.T2.3.11.8" class="ltx_tr">
<th id="S4.T2.3.11.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.3.11.8.2" class="ltx_td ltx_align_center">aircraft</td>
<td id="S4.T2.3.11.8.3" class="ltx_td ltx_align_center">Synth.</td>
<td id="S4.T2.3.11.8.4" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.11.8.5" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.11.8.6" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.11.8.7" class="ltx_td ltx_align_center">56.28 (0.46)</td>
<td id="S4.T2.3.11.8.8" class="ltx_td ltx_align_center">87.54 (0.69)</td>
<td id="S4.T2.3.11.8.9" class="ltx_td ltx_align_center">60.71 (0.51)</td>
</tr>
<tr id="S4.T2.3.12.9" class="ltx_tr">
<th id="S4.T2.3.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.3.12.9.1.1" class="ltx_text">Mask R-CNN</span></th>
<td id="S4.T2.3.12.9.2" class="ltx_td ltx_align_center">aircraft</td>
<td id="S4.T2.3.12.9.3" class="ltx_td ltx_align_center">FT</td>
<td id="S4.T2.3.12.9.4" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.12.9.5" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.12.9.6" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.3.12.9.7" class="ltx_td ltx_align_center">70.51 (0.34)</td>
<td id="S4.T2.3.12.9.8" class="ltx_td ltx_align_center">94.73 (0.03)</td>
<td id="S4.T2.3.12.9.9" class="ltx_td ltx_align_center">73.72 (0.26)</td>
</tr>
<tr id="S4.T2.3.13.10" class="ltx_tr">
<td id="S4.T2.3.13.10.1" class="ltx_td ltx_align_center ltx_border_t">role</td>
<td id="S4.T2.3.13.10.2" class="ltx_td ltx_align_center ltx_border_t">Real</td>
<td id="S4.T2.3.13.10.3" class="ltx_td ltx_align_center ltx_border_t">65.60</td>
<td id="S4.T2.3.13.10.4" class="ltx_td ltx_align_center ltx_border_t">72.13</td>
<td id="S4.T2.3.13.10.5" class="ltx_td ltx_align_center ltx_border_t">70.97</td>
<td id="S4.T2.3.13.10.6" class="ltx_td ltx_align_center ltx_border_t">69.57 (0.47)</td>
<td id="S4.T2.3.13.10.7" class="ltx_td ltx_align_center ltx_border_t">91.89 (0.55)</td>
<td id="S4.T2.3.13.10.8" class="ltx_td ltx_align_center ltx_border_t">76.16 (0.30)</td>
</tr>
<tr id="S4.T2.3.14.11" class="ltx_tr">
<th id="S4.T2.3.14.11.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.3.14.11.2" class="ltx_td ltx_align_center">role</td>
<td id="S4.T2.3.14.11.3" class="ltx_td ltx_align_center">Synth.</td>
<td id="S4.T2.3.14.11.4" class="ltx_td ltx_align_center">29.12</td>
<td id="S4.T2.3.14.11.5" class="ltx_td ltx_align_center">41.78</td>
<td id="S4.T2.3.14.11.6" class="ltx_td ltx_align_center">47.47</td>
<td id="S4.T2.3.14.11.7" class="ltx_td ltx_align_center">39.46 (3.20)</td>
<td id="S4.T2.3.14.11.8" class="ltx_td ltx_align_center">62.31 (4.51)</td>
<td id="S4.T2.3.14.11.9" class="ltx_td ltx_align_center">57.33 (1.96)</td>
</tr>
<tr id="S4.T2.3.15.12" class="ltx_tr">
<th id="S4.T2.3.15.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<td id="S4.T2.3.15.12.2" class="ltx_td ltx_align_center ltx_border_bb">role</td>
<td id="S4.T2.3.15.12.3" class="ltx_td ltx_align_center ltx_border_bb">FT</td>
<td id="S4.T2.3.15.12.4" class="ltx_td ltx_align_center ltx_border_bb">58.96</td>
<td id="S4.T2.3.15.12.5" class="ltx_td ltx_align_center ltx_border_bb">70.02</td>
<td id="S4.T2.3.15.12.6" class="ltx_td ltx_align_center ltx_border_bb">72.33</td>
<td id="S4.T2.3.15.12.7" class="ltx_td ltx_align_center ltx_border_bb">67.11 (0.46)</td>
<td id="S4.T2.3.15.12.8" class="ltx_td ltx_align_center ltx_border_bb">90.03 (0.52)</td>
<td id="S4.T2.3.15.12.9" class="ltx_td ltx_align_center ltx_border_bb">74.40 (0.58)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.13.1" class="ltx_text ltx_font_bold">Results of the object detection and segmentation experiments.</span> We report models performance trained on the real dataset (Real) and the synthetic dataset (Synth.) as well as the fine tuning experiment (FT) using only <math id="S4.T2.8.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.T2.8.m1.1b"><mrow id="S4.T2.8.m1.1.1" xref="S4.T2.8.m1.1.1.cmml"><mn id="S4.T2.8.m1.1.1.2" xref="S4.T2.8.m1.1.1.2.cmml">10</mn><mo id="S4.T2.8.m1.1.1.1" xref="S4.T2.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.m1.1c"><apply id="S4.T2.8.m1.1.1.cmml" xref="S4.T2.8.m1.1.1"><csymbol cd="latexml" id="S4.T2.8.m1.1.1.1.cmml" xref="S4.T2.8.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.T2.8.m1.1.1.2.cmml" xref="S4.T2.8.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.m1.1d">10\%</annotation></semantics></math> of the real training dataset. We show the results of the single class experiments (‘aircraft’) and the three classes experiment: small (<math id="S4.T2.9.m2.1" class="ltx_Math" alttext="C_{S}" display="inline"><semantics id="S4.T2.9.m2.1b"><msub id="S4.T2.9.m2.1.1" xref="S4.T2.9.m2.1.1.cmml"><mi id="S4.T2.9.m2.1.1.2" xref="S4.T2.9.m2.1.1.2.cmml">C</mi><mi id="S4.T2.9.m2.1.1.3" xref="S4.T2.9.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.9.m2.1c"><apply id="S4.T2.9.m2.1.1.cmml" xref="S4.T2.9.m2.1.1"><csymbol cd="ambiguous" id="S4.T2.9.m2.1.1.1.cmml" xref="S4.T2.9.m2.1.1">subscript</csymbol><ci id="S4.T2.9.m2.1.1.2.cmml" xref="S4.T2.9.m2.1.1.2">𝐶</ci><ci id="S4.T2.9.m2.1.1.3.cmml" xref="S4.T2.9.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.m2.1d">C_{S}</annotation></semantics></math>), medium (<math id="S4.T2.10.m3.1" class="ltx_Math" alttext="C_{M}" display="inline"><semantics id="S4.T2.10.m3.1b"><msub id="S4.T2.10.m3.1.1" xref="S4.T2.10.m3.1.1.cmml"><mi id="S4.T2.10.m3.1.1.2" xref="S4.T2.10.m3.1.1.2.cmml">C</mi><mi id="S4.T2.10.m3.1.1.3" xref="S4.T2.10.m3.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.10.m3.1c"><apply id="S4.T2.10.m3.1.1.cmml" xref="S4.T2.10.m3.1.1"><csymbol cd="ambiguous" id="S4.T2.10.m3.1.1.1.cmml" xref="S4.T2.10.m3.1.1">subscript</csymbol><ci id="S4.T2.10.m3.1.1.2.cmml" xref="S4.T2.10.m3.1.1.2">𝐶</ci><ci id="S4.T2.10.m3.1.1.3.cmml" xref="S4.T2.10.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.m3.1d">C_{M}</annotation></semantics></math>), and large (<math id="S4.T2.11.m4.1" class="ltx_Math" alttext="C_{L}" display="inline"><semantics id="S4.T2.11.m4.1b"><msub id="S4.T2.11.m4.1.1" xref="S4.T2.11.m4.1.1.cmml"><mi id="S4.T2.11.m4.1.1.2" xref="S4.T2.11.m4.1.1.2.cmml">C</mi><mi id="S4.T2.11.m4.1.1.3" xref="S4.T2.11.m4.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.11.m4.1c"><apply id="S4.T2.11.m4.1.1.cmml" xref="S4.T2.11.m4.1.1"><csymbol cd="ambiguous" id="S4.T2.11.m4.1.1.1.cmml" xref="S4.T2.11.m4.1.1">subscript</csymbol><ci id="S4.T2.11.m4.1.1.2.cmml" xref="S4.T2.11.m4.1.1.2">𝐶</ci><ci id="S4.T2.11.m4.1.1.3.cmml" xref="S4.T2.11.m4.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.m4.1d">C_{L}</annotation></semantics></math>) civil transport aircraft. Performance is evaluated using the mean average precision (mAP) (IOU@[0.5:0.95]), the mAP50 (IOU@0.5) and the average recall (AR) metrics, as well as the class APs when applicable. For the Mask R-CNN instance segmentation experiments, we only report the segmentation AP. Each value reported is an average of 5 runs. The standard deviations for mAP, mAP50, and AR are also indicated.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we validate the synthetic dataset by running three experiments for two tasks: object detection and instance segmentation. For each task, we train a benchmark network on three subsets of data: on the real data only, on the synthetic data only, and perform a fine tuning experiment training on the synthetic data and then a portion (<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">∼</mo><mrow id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mn id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml">10</mn><mo id="S4.p1.1.m1.1.1.3.1" xref="S4.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p1.1.m1.1.1.3.2.cmml" xref="S4.p1.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\sim 10\%</annotation></semantics></math>) of the real dataset. Each experiment is validated on the test real dataset and the results are shown Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments, Results, and Discussion ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We ran these experiments for two attributes: aircraft (detection of an aircraft without classifying it) and civil role.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training and Testing Splits</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.3" class="ltx_p">For the real world data, given the size of the raw satellite scenes, we adopted a tiling approach. Each scene is split into 512x512 tiles containing at least one aircraft. Furthermore, we ensure that the training and test split contains at least one satellite scene per country to maximize geographic diversity. As the dataset contains satellite images captured over the same location at different dates, an airfield can appear in both splits but at different points in time. Moreover, we created a subset of the real training split for the fine tuning experiments. This subset contains roughly 10% of the images of the training split, created by drawing a 10% random sample of image tiles by location. All synthetic data is included in the training split to maintain a domain gap. For this study we train on <math id="S4.SS1.p1.1.m1.2" class="ltx_Math" alttext="45,000" display="inline"><semantics id="S4.SS1.p1.1.m1.2a"><mrow id="S4.SS1.p1.1.m1.2.3.2" xref="S4.SS1.p1.1.m1.2.3.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">45</mn><mo id="S4.SS1.p1.1.m1.2.3.2.1" xref="S4.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.2b"><list id="S4.SS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.2"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">45</cn><cn type="integer" id="S4.SS1.p1.1.m1.2.2.cmml" xref="S4.SS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.2c">45,000</annotation></semantics></math> of the <math id="S4.SS1.p1.2.m2.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S4.SS1.p1.2.m2.2a"><mrow id="S4.SS1.p1.2.m2.2.3.2" xref="S4.SS1.p1.2.m2.2.3.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">50</mn><mo id="S4.SS1.p1.2.m2.2.3.2.1" xref="S4.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.2b"><list id="S4.SS1.p1.2.m2.2.3.1.cmml" xref="S4.SS1.p1.2.m2.2.3.2"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">50</cn><cn type="integer" id="S4.SS1.p1.2.m2.2.2.cmml" xref="S4.SS1.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.2c">50,000</annotation></semantics></math> images, reserving <math id="S4.SS1.p1.3.m3.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S4.SS1.p1.3.m3.2a"><mrow id="S4.SS1.p1.3.m3.2.3.2" xref="S4.SS1.p1.3.m3.2.3.1.cmml"><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">5</mn><mo id="S4.SS1.p1.3.m3.2.3.2.1" xref="S4.SS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.3.m3.2.2" xref="S4.SS1.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.2b"><list id="S4.SS1.p1.3.m3.2.3.1.cmml" xref="S4.SS1.p1.3.m3.2.3.2"><cn type="integer" id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">5</cn><cn type="integer" id="S4.SS1.p1.3.m3.2.2.cmml" xref="S4.SS1.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.2c">5,000</annotation></semantics></math> images for cross-validation purposes. Note these cross-validation results are not reported here. Further details on the initial training and testing splits used in this study can be found at <a target="_blank" href="https://www.cosmiqworks.org/RarePlanes" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.cosmiqworks.org/RarePlanes</a>. Ultimately end users are encouraged to reorganize and create their own splits for their specific research or use-case(s).</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<table id="S4.F9.4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F9.4.4.4" class="ltx_tr">
<td id="S4.F9.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><img src="/html/2006.02963/assets/figs/predictions1.png" id="S4.F9.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="137" height="430" alt="Refer to caption"></td>
<td id="S4.F9.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><img src="/html/2006.02963/assets/figs/predictions2.png" id="S4.F9.2.2.2.2.g1" class="ltx_graphics ltx_img_portrait" width="137" height="430" alt="Refer to caption"></td>
<td id="S4.F9.3.3.3.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><img src="/html/2006.02963/assets/figs/predictions3.png" id="S4.F9.3.3.3.3.g1" class="ltx_graphics ltx_img_portrait" width="137" height="430" alt="Refer to caption"></td>
<td id="S4.F9.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><img src="/html/2006.02963/assets/figs/predictions4.png" id="S4.F9.4.4.4.4.g1" class="ltx_graphics ltx_img_portrait" width="137" height="430" alt="Refer to caption"></td>
</tr>
<tr id="S4.F9.4.4.5.1" class="ltx_tr">
<td id="S4.F9.4.4.5.1.1" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">(a)</td>
<td id="S4.F9.4.4.5.1.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">(b)</td>
<td id="S4.F9.4.4.5.1.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">(c)</td>
<td id="S4.F9.4.4.5.1.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="S4.F9.6.1" class="ltx_text ltx_font_bold">Example of aircraft detection results</span>. (a) ground truth, (b) model trained real dataset (c) model trained on synthetic dataset (d) model fine tuned on real subset.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In our experiments, we used a Resnet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> as the backbone for the Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> detection network. A similar backbone was used for the Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> instance segmentation network. Backbones are pre-trained using ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>weights and all experiments are conducted with the Detectron2 framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, using the default configurations for each network. The network was optimized with Stochastic Gradient Descent (SGD) using a learning rate of 0.001, weight decay of 0.0001 and a momentum of 0.9. Additionally, we used a linear warmup period over 1K iterations. We maintain a consistent learning rate for the fine tuning experiments. We found that decreasing the learning rate or freezing some of the layers in the backbone did not improve performance. The networks were trained on a NVIDIA Tesla V100 GPU with 12GB memory. Each network was trained until convergence, which was reached after around 60K iterations. We also applied basic pixel level augmentations, such as blurring and modifying the contrast or the brightness. Finally, we performed random cropping (512x512) when training on the synthetic dataset.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Discussion</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We evaluated our network performances using the COCO average precision (AP) metric. Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments, Results, and Discussion ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports the average precision for each class as well as the mAP, mAP50, and average recall (AR). Qualitative results are shown in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1 Training and Testing Splits ‣ 4 Experiments, Results, and Discussion ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.4" class="ltx_p">In the first set of experiments, we focused on the performance of the synthetic dataset only. As expected, we observe a drop in performances when training on the synthetic data only, due to the domain gap between the real and synthetic datasets. We observe that the model trained on the synthetic dataset tends to mislabel clutter or nearby objects as aircraft, as shown in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1 Training and Testing Splits ‣ 4 Experiments, Results, and Discussion ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Additionally, snow patches, ground markings, airport vehicles are sometimes detected as aircraft. This leads to a significantly lower AP (<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="55\%" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">55</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">55\%</annotation></semantics></math> to <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">75</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">75\%</annotation></semantics></math> of the real AP) when models are trained on the synthetic dataset only. However, the AR is not as sensitive to the domain gap (<math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">70</mn><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">70\%</annotation></semantics></math> to <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mrow id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mn id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">80</mn><mo id="S4.SS3.p2.4.m4.1.1.1" xref="S4.SS3.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.m4.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">80\%</annotation></semantics></math> of the real AR), meaning that the majority of aircraft are still detected when only the synthetic dataset is used. Similarly, we observe that the drop in AP50 is also lower relative to the AP metric. Ultimately, the AP50 metric may be more informative as we are most interested in accurately counting aircraft, rather than how well they are localized.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.2" class="ltx_p">Most importantly, when a small subset (<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">∼</mo><mrow id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml"><mn id="S4.SS3.p3.1.m1.1.1.3.2" xref="S4.SS3.p3.1.m1.1.1.3.2.cmml">10</mn><mo id="S4.SS3.p3.1.m1.1.1.3.1" xref="S4.SS3.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">absent</csymbol><apply id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.3.1.cmml" xref="S4.SS3.p3.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.1.m1.1.1.3.2.cmml" xref="S4.SS3.p3.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\sim 10\%</annotation></semantics></math>) of real data is added for fine tuning, we observe a significant gain in mAP, leading to similar performance (91 to <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="96\%" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mn id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">96</mn><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">96\%</annotation></semantics></math> of real mAP) to the models trained on the real dataset only. We hypothesize that the synthetic data helps to build a prior model for aircraft detection and eases transfer learning, thus greatly reducing the need for annotated real data. In Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1 Training and Testing Splits ‣ 4 Experiments, Results, and Discussion ‣ RarePlanes: Synthetic Data Takes Flight" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we see how fine tuning on the real subset removes some of the false positive predictions versus training on the synthetic dataset only. However, the false positive detection rate still remains slightly higher compared to training on the entire real training set. It’s important to note that the goal of these experiments is to define a baseline for future experimentation for other algorithms to improve upon, particularly within the area of domain adaptation.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.3" class="ltx_p">We present RarePlanes, a unique machine learning dataset that incorporates both real and synthetically generated satellite imagery. This dataset is critical for expanding and examining the value of synthetic data for overhead applications. Our benchmark experiments using the dataset found that blends of <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mn id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">90</mn><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">90\%</annotation></semantics></math> synthetic to <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mn id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">10</mn><mo id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">10\%</annotation></semantics></math> real can deliver nearly equivalent performance as <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mn id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">100</mn><mo id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">100\%</annotation></semantics></math> real data for the task of aircraft identification. Additionally, we believe that RarePlanes could be particularly valuable for developing new and extensible domain adaptation approaches. Finally, the detailed fine-grain attribution and annotation contained in the dataset may enable various CV tasks and future experiments in the fields of detection, instance segmentation, or zero-shot learning.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors thank the whole AI.Reverie team for making the creation of the synthetic dataset possible. We would especially like to thank Danny Gillies and Natasha Ruiz for their devoted help. Additionally we thank Christyn Zehnder at IQT for her contributions in launching and marketing the RarePlanes dataset.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas Geiger,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Augmented reality meets deep learning for car instance segmentation
in urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">British machine vision conference</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, volume 1, page 2, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Marc Bosch, Zachary Kurtz, Shea Hagstrom, and Myron Brown.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">A multiple view stereo benchmark for satellite imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Applied Imagery Pattern Recognition Workshop
(AIPR)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 1–9. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Gabriel J Brostow, Julien Fauqueur, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Semantic object classes in video: A high-definition ground truth
database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition Letters</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 30(2):88–97, 2009.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Myron Brown, Hirsh Goldberg, Kevin Foster, Andrea Leichtman, Sean Wang, Shea
Hagstrom, Marc Bosch, and Scott Almes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Large-scale public lidar and satellite image data set for urban
semantic labeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In Monte D. Turner and Gary W. Kamerman, editors, </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Laser Radar
Technology and Applications XXIII</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, volume 10636, pages 154 – 167.
International Society for Optics and Photonics, SPIE, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner,
Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Matterport3d: Learning from rgb-d data in indoor environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on 3D Vision (3DV)</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Functional Map of the World.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">. IEEE, Jun 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">The cityscapes dataset for semantic urban scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, June 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Gabriela Csurka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Domain adaptation for visual applications: A comprehensive survey,
2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer, Michael
Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski,
Sami Romdhani, Christian Theobalt, Volker Blanz, and Thomas Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">3d morphable face models – past, present and future, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Kiana Ehsani, Roozbeh Mottaghi, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Segan: Segmenting and generating the invisible.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Epic Games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Unreal engine.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Adam Van Etten, Dave Lindenbaum, and Todd M. Bacastow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Spacenet: A remote sensing dataset and challenge series, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew
Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes (voc) challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 88(2):303–338, 2010.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Describing objects by their attributes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 1778–1785. IEEE, 2009.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, and Shaogang
Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Recent advances in zero-shot recognition, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
A Gaidon, Q Wang, Y Cabon, and E Vig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Virtual worlds as proxy for multi-object tracking analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Vision meets robotics: The kitti dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Robotics Research (IJRR)</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Snorri Gudmundsson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">General aviation aircraft design: Applied Methods and
Procedures</span><span id="bib.bib19.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">Butterworth-Heinemann, 2013.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Ritwik Gupta, Bryce Goodman, Nirav Patel, Ricky Hosfelt, Sandra Sajeev, Eric
Heim, Jigar Doshi, Keane Lucas, Howie Choset, and Matthew Gaston.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Creating xbd: A dataset for assessing building damage from satellite
imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and Roberto
Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Understanding real world indoor scenes with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 4077–4085, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition, 2015.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
Saenko, Alexei A. Efros, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Cycada: Cycle-consistent adversarial domain adaptation, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Y.-T. Hu, H.-S. Chen, K. Hui, J.-B. Huang, and A. G. Schwing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">SAIL-VOS: Semantic Amodal Instance Level Video Object Segmentation
– A Synthetic Dataset and Baselines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Multimodal unsupervised image-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer Science</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, page 179–196, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Vladimir Iglovikov and Alexey Shvets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for
image segmentation, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Fanjie Kong, Bohao Huang, Kyle Bradbury, and Jordan Malof.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">The synthinel-1 dataset: a collection of high resolution synthetic
overhead imagery for building segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 Winter Conference on Applications of Computer Vision
(WACV)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Adam Kortylewski, Bernhard Egger, Andreas Morel-Forster, Andreas Schneider,
Thomas Gerig, Clemens Blumer, Corius Reyneke, and Thomas Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Can synthetic faces undo the damage of dataset bias to face
recognition and facial landmark detection?, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Adam Kortylewski, Andreas Schneider, Thomas Gerig, Bernhard Egger, Andreas
Morel-Forster, and Thomas Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Training deep face recognition systems with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.05891</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Free supervision from video games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 2955–2964, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael Laielli, Matthew
Klaric, Yaroslav Bulatov, and Brendan McCord.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">xView: Objects in context in overhead imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, abs/1802.07856, 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and
Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers,
Alexey Dosovitskiy, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">What makes good synthetic training data for learning disparity and
optical flow estimation?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 126(9):942–960, Apr
2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
OpenStreetMap contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Planet dump retrieved from https://planet.osm.org .
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.openstreetmap.org" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.openstreetmap.org</a><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
F. Pacifici, N. Longbotham, and W. J. Emery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">The importance of physical quantities for the analysis of
multitemporal and multiangular optical very high spatial resolution images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">,
52(10):6241–6256, Oct 2014.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Sonali Patil, Bharath Comandur, Tanmay Prakash, and Avinash C. Kak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">A new stereo benchmarking dataset for satellite images, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Genevieve Patterson, Chen Xu, Hang Su, and James Hays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">The sun attribute database: Beyond categories for deeper scene
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 108(1-2):59–81,
2014.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Learning deep object detectors from 3d models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 1278–1286, 2015.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Jakaria Rabbi, Nilanjan Ray, Matthias Schubert, Subir Chowdhury, and Dennis
Chao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Small-object detection in remote sensing images with end-to-end
edge-enhanced gan and object detector network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Remote Sensing</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 12(9):1432, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Param S. Rajpura, Hristo Bojinov, and Ravi S. Hegde.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Object detection using deep cnns trained on synthetic images, 2017.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Waseem Rawat and Zenghui Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Deep convolutional neural networks for image classification: A
comprehensive review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural computation</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 29(9):2352–2449, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks, 2015.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
D. Rendall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Jane’s Aircraft Recognition Guide</span><span id="bib.bib45.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">HarperCollins, 1996.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Stephan R. Richter, Zeeshan Hayder, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Playing for benchmarks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 2232–2241, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, pages 102–118.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M
Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 3234–3243, 2016.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Franz Rottensteiner, Gunho Sohn, Jaewook Jung, Markus Gerke, Caroline Baillard,
Sebastien Benitez, and Uwe Breitkopf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">The isprs benchmark on urban object classification and 3d building
reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
Information Sciences I-3 (2012), Nr. 1</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 1(1):293–298, 2012.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama
Chellappa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Learning from synthetic data: Addressing domain shift for semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 3752–3761, 2018.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Jacob Shermeyer, Daniel Hogan, Jason Brown, Adam Van Etten, Nicholas Weir,
Fabio Pacifici, Ronny Haensch, Alexei Bastidas, Scott Soenen, Todd Bacastow,
and Ryan Lewis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Spacenet 6: Multi-sensor all weather mapping dataset, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jacob Shermeyer and Adam Van Etten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">The effects of super-resolution on object detection performance in
satellite imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2019.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
SideFX.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Houdini.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas
Funkhouser.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Semantic scene completion from a single depth image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of 30th IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Steelpillow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Aircraft — Wikipedia, the free encyclopedia.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://commons.wikimedia.org/wiki/User:Steelpillow/Aircraft" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://commons.wikimedia.org/wiki/User:Steelpillow/Aircraft</a><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="font-size:90%;">[Online; accessed 30-April-2020].
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, June 2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang, Marshall Burke, David
Lobell, and Stefano Ermon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Learning to interpret satellite images in global scale using
wikipedia, 2019.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Adam Van Etten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">You only look twice: Rapid multi-scale object detection in satellite
imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1805.09512</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Perez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Advent: Adversarial entropy minimization for domain adaptation in
semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">, Jun 2019.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">The Caltech-UCSD Birds-200-2011 Dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">Technical Report CNS-TR-2011-001, California Institute of Technology,
2011.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
C. Wang, X. Bai, S. Wang, J. Zhou, and P. Ren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Multiscale visual attention networks for object detection in vhr
remote sensing images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Letters</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 16(2):310–314,
2019.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Syed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei Sun, Fahad
Shahbaz Khan, Fan Zhu, Ling Shao, Gui-Song Xia, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">isaid: A large-scale dataset for instance segmentation in aerial
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, pages 28–37, 2019.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Daniel Ward, Peyman Moghadam, and Nicolas Hudson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Deep leaf segmentation using synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.10931</span><span id="bib.bib63.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Nicholas Weir, David Lindenbaum, Alexei Bastidas, Adam Van Etten, Sean
McPherson, Jacob Shermeyer, Varun Kumar, and Hanlin Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Spacenet mvoi: a multi-view overhead imagery dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, pages 992–1001, 2019.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Detectron2.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/facebookresearch/detectron2</a><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Gui-Song Xia, Xiang Bai, Zhen Zhu Jian Ding, Serge Belongie, Jiebo Luo, Mihai
Datcu, Marcello Pelillo, and Liangpei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">DOTA: A Large-scale Dataset for Object Detection in Aerial Images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib66.4.2" class="ltx_text" style="font-size:90%;">, Nov. 2017.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and Zeynep Akata.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Zero-shot learning—a comprehensive evaluation of the good, the bad
and the ugly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib67.4.2" class="ltx_text" style="font-size:90%;">,
41(9):2251–2265, Sep 2019.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Linjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">A large-scale car dataset for fine-grained categorization and
verification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, pages 3973–3981, 2015.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo, Sun Xian,
and Kun Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Scrdet: Towards more robust detection for small, cluttered and
rotated objects, 2018.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Bo Zhao, Yanwei Fu, Rui Liang, Jiahong Wu, Yonggang Wang, and Yizhou Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">A large-scale attribute dataset for zero-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Object detection with deep learning: A review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on neural networks and learning systems</span><span id="bib.bib71.4.2" class="ltx_text" style="font-size:90%;">,
30(11):3212–3232, 2019.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Semantic amodal segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib72.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib72.5.3" class="ltx_text" style="font-size:90%;">, pages 1464–1472, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2006.02962" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2006.02963" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2006.02963">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2006.02963" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2006.02964" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 17:07:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
