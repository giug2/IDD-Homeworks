<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.13238] Personalized Wireless Federated Learning for Large Language Models</title><meta property="og:description" content="Large Language Models (LLMs) have revolutionized natural language processing tasks. However, their deployment in wireless networks still face challenges, i.e., a lack of privacy and security protection mechanisms. Fede…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Personalized Wireless Federated Learning for Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Personalized Wireless Federated Learning for Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.13238">

<!--Generated on Sun May  5 17:37:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Large Language Model,  Personalized Federated Learning,  Pre-training,  Fine-tuning.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\setlength</span>
<p id="p1.2" class="ltx_p">2em</p>
</div>
<h1 class="ltx_title ltx_title_document">Personalized Wireless Federated Learning for Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Feibo Jiang, <span id="id1.1.id1" class="ltx_text ltx_font_italic">Member, IEEE</span>, Li Dong, Siwei Tu, Yubo Peng, Kezhi Wang, <span id="id2.2.id2" class="ltx_text ltx_font_italic">Senior Member, IEEE</span>, Kun Yang, <span id="id3.3.id3" class="ltx_text ltx_font_italic">Fellow, IEEE</span>, Cunhua Pan, <span id="id4.4.id4" class="ltx_text ltx_font_italic">Senior Member, IEEE</span>, Dusit Niyato, <span id="id5.5.id5" class="ltx_text ltx_font_italic">Fellow, IEEE</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Large Language Models (LLMs) have revolutionized natural language processing tasks. However, their deployment in wireless networks still face challenges, i.e., a lack of privacy and security protection mechanisms. Federated Learning (FL) has emerged as a promising approach to address these challenges. Yet, it suffers from issues including inefficient handling with big and heterogeneous data, resource-intensive training, and high communication overhead.
To tackle these issues, we first compare different learning stages and their features of LLMs in wireless networks.
Next, we introduce two personalized wireless federated fine-tuning methods with low communication overhead, i.e., (1) Personalized Federated Instruction Tuning (PFIT), which employs reinforcement learning to fine-tune local LLMs with diverse reward models to achieve personalization;
(2) Personalized Federated Task Tuning (PFTT), which can leverage global adapters and local Low-Rank Adaptations (LoRA) to collaboratively fine-tune local LLMs, where the local LoRAs can be applied to achieve personalization without aggregation.
Finally, we perform simulations to demonstrate the effectiveness of the proposed two methods and comprehensively discuss open issues.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Large Language Model, Personalized Federated Learning, Pre-training, Fine-tuning.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the development of 6G communications, the application of Artificial Intelligence (AI) in the wireless networks is becoming increasingly important. One of the key features of 6G is the deep integration of AI with wireless networks, which can support more intelligent services and applications.
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) tasks by demonstrating impressive language understanding and generation capabilities, pushing the boundaries of AI research. LLMs can also provide more accurate understanding of user semantics and intentions, thereby offering personalized services to 6G users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, as LLM scales continue to expand, reaching hundreds of billions or even trillions of parameters, traditional publicly available datasets face challenges in meeting the demands for training future LLMs. In 6G networks, there could be a vast array of mobile devices, potentially accumulating significant volumes of user data. However, concerns regarding data security and information privacy may still prevent the users to share their personal data for the training of LLMs in wireless networks.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To leverage the vast amount of distributed and privatized data for training LLMs, Federated Learning (FL) has been adopted to offer a collaborative learning approach. This approach enables future LLMs to learn from a broader range of data sources while maintaining data security and privacy.
However, there are major challenges for training the LLM by wireless FL.</p>
</div>
<section id="S1.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS0.SSS1.4.1.1" class="ltx_text">I-</span>1 </span>Big and Heterogeneous Data</h4>

<div id="S1.SS0.SSS1.p1" class="ltx_para">
<p id="S1.SS0.SSS1.p1.1" class="ltx_p">LLMs require massive amounts of data from large and diverse data sources to train the model effectively. In wireless FL, the distributed structure of data is a critical challenge, as the data on each mobile device may be highly unbalanced, depending on their backgrounds, preferences, or behaviours. This can lead to slower convergence and poorer performance for training LLMs. Furthermore, diversity of data across mobile devices may introduce complex data-distribution in wireless networks.</p>
</div>
</section>
<section id="S1.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS0.SSS2.4.1.1" class="ltx_text">I-</span>2 </span>Resource-intensive Training</h4>

<div id="S1.SS0.SSS2.p1" class="ltx_para">
<p id="S1.SS0.SSS2.p1.1" class="ltx_p">Training an LLM is a resource-intensive task that demands high computational power and large memory. In wireless FL, the computation is decentralized, where individual devices may need to have sufficient resources to participate in the training process. However, not all contributing devices (such as smartphones or pads) have the necessary computational and storage resources, which can lead to slow training speed or even fail to train the LLM successfully,</p>
</div>
</section>
<section id="S1.SS0.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS0.SSS3.4.1.1" class="ltx_text">I-</span>3 </span>High Communication Overhead</h4>

<div id="S1.SS0.SSS3.p1" class="ltx_para">
<p id="S1.SS0.SSS3.p1.1" class="ltx_p">Wireless FL requires frequent communication between a central server and devices to update an LLM, which can incur high bandwidth and latency costs. For LLMs that have billions or even trillions of parameters, this could result in a huge amount of data to be transferred, leading to high communication overhead. Moreover, the communication cost increases with the number of communication rounds, the number of participating devices, and the model size. As such, reducing communication overhead without sacrificing model performance is a significant challenge.</p>
</div>
<div id="S1.SS0.SSS3.p2" class="ltx_para">
<p id="S1.SS0.SSS3.p2.1" class="ltx_p">Moreover, the demand for user-centric personalized LLMs has significantly increased. These LLMs have the ability to learn individual preferences and provide tailored results.
Personalized Federated Learning (PFL) is an extension of FL that recognizes potential data and resource heterogeneity among different clients and aims to learn a personalized model for each client <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Parameter-Efficient Fine-Tuning (PEFT) enables efficient adaptation to client-specific data and tasks by adjusting a minimal number of parameters for the pre-trained LLM, thereby reducing computational resource and communication overhead. Hence, PFL combined with PEFT can overcome the aforementioned challenges associated with training LLMs by FL.</p>
</div>
<div id="S1.SS0.SSS3.p3" class="ltx_para">
<p id="S1.SS0.SSS3.p3.1" class="ltx_p">Unlike FL for generative models such as generative adversarial networks and diffusion models, LLMs require consideration of alignment with human values and interaction with external knowledge during learning.
Hence, in the paper, we first introduce and compare different learning stages and their features of LLMs in wireless networks. We then summarize potential solutions of FL for various fine-tuning methods. Finally, we highlight advantages of PFL in fine-tuning LLM and propose two approaches for personalized wireless federated fine-tuning with low communication overhead as follows:</p>
</div>
<div id="S1.SS0.SSS3.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Personalized Federated Instruction Tuning (PFIT): We focus on Reinforcement Learning from Human Feedback (RLHF) and design two reward models to represent the helpfulness and safety by human feedback. By linearly combining the two reward models across different clients, personalized local models are obtained. Sparse self-attention is also employed to reduce communication overhead and accelerate the training speed of the federated instruction tuning.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Personalized Federated Task Tuning (PFTT): We combine two PEFT approaches, namely adapter and Low-Rank Adaptation (LoRA), to reduce communication overhead and accelerate the federated task fine-tuning. The adapter parameters of clients are sent to the server for global aggregation, while LoRA’s parameters are retained at the clients to maintain the personalization of local models.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS0.SSS3.p5" class="ltx_para">
<p id="S1.SS0.SSS3.p5.1" class="ltx_p">The remainder of this paper can be organized as follows. Section II describes the harmonizing of LLM and FL in wireless networks. Section III provides some potential solutions of federated fine-tuning for LLMs.
Section IV details the proposed personalized wireless federated fine-tuning schemes. Section V shows the simulation results. Section VI presents open issues and Section VII concludes this paper.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Harmonizing LLM and FL in Wireless Networks</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Unlike traditional deep learning, LLMs have a three-stage learning process: pre-training, fine-tuning and retrieval-augmented generation (RAG). As shown in Fig. <a href="#S2.F1" title="Figure 1 ‣ II Harmonizing LLM and FL in Wireless Networks ‣ Personalized Wireless Federated Learning for Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the pre-training stage provides a foundation of general language understanding, while the fine-tuning stage adapts this understanding to specific tasks or goals, and the RAG stage enhances this understanding and improves the accuracy of answers by retrieving information from external data sources. All learning stages of LLMs are also summarized and compared in Table <a href="#S2.T1" title="TABLE I ‣ II-C Retrieval-Augmented Generation Stage of LLM ‣ II Harmonizing LLM and FL in Wireless Networks ‣ Personalized Wireless Federated Learning for Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2404.13238/assets/FedLLM0_2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="726" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Illustration of the three-stage learning of LLMs in wireless networks.</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Pre-training Stage of LLM</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the pre-training stage, an LLM is deployed on a server and undergoes self-supervised learning using a substantial volume of unlabelled data. Numerous client devices transmit their unlabelled data that does not contain sensitive information to the server for learning purposes over the wireless network. The goal of pre-training is to teach patterns, grammar, semantics, and world knowledge to LLMs by predicting missing or masked words in sentences. For instance, in the BERT model, it is done using a technique called Masked Language Modelling (MLM), where a certain percentage of words in a sentence are randomly replaced with a special mask token, and the model has to predict the original word <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. During the pre-training stage, the LLM learns to capture contextual relationships and build a general understanding of language. A large number of parameters in the model enables it to encode a vast amount of information from the training corpus, resulting in a rich language representation.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">However, FL may not be necessary for the pre-training stage of LLMs due to the following reasons:</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><em id="S2.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">High Resource Requirements:</em> Pre-training of LLMs involves adjusting all their parameters, which requires significant computational resources. In FL, the model is trained across many devices or servers, with each device obtaining updates for the model based on its computation and storage resource <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Additionally, this distributed computation increases the communication overhead, as the update from each device needs to be aggregated to obtain the global model. Centralized training, on the other hand, can leverage powerful servers and optimized infrastructure to train LLMs more efficiently.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><em id="S2.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Privacy Concerns:</em> Pre-training of LLMs typically involves using a large, publicly available corpus of text, such as books, websites, and other online resources. Since this data is already publicly available and does not contain sensitive personal information, there are no data privacy concerns that necessitate the use of FL. FL is more applicable in scenarios where data is sensitive and decentralized, such as in healthcare or personal mobile devices, where privacy regulations or concerns prevent the data from being shared directly.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Fine-tuning Stage of LLM</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Once the LLM completes pre-training on the server side, it undergoes further fine-tuning on a smaller, more specific dataset. This dataset typically consists of local private data. The clients download the pre-trained parameters of the LLM from the server and perform fine-tuning on an extremely small subset of parameters to enhance the performance of the LLM. To ensure security and privacy of local data, the fine-tuning process is conducted locally on the client side, and the updated subset of parameters are transmitted back to the server through wireless networks.
This local dataset is often labelled, meaning that it comes with correct answers that the LLM should learn to predict. Fine-tuning requires less computational resources and smaller amount of data compared to pre-training. Fine-tuning of LLMs can be categorized into the following types:</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Instruction Tuning</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Instruction tuning is a strategy for fine tuning LLMs on a combination of task instructions so that the LLM can generate the correct output based on the instructions. Instruction tuning uses natural language instructions as inputs to query the LLM and guide its output. The instructions consist of sequences that pair instructions with corresponding examples, which can provide explicit and precise guidance for the LLM to generate texts that are consistent with the user’s intent and the data source. The goal of instruction learning is to improve the comprehension and generalization of LLMs on unseen tasks, as well as their helpfulness and safety.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Task Tuning</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Once the LLM has been pre-trained on the unlabelled data, it is further fine-tuned on specific downstream tasks using labelled data. Task tuning involves training the pre-trained model on a smaller task-specific dataset that is annotated with labels or target outputs. During the task tuning, the pre-trained model is adapted to the specific task by updating its parameters based on the labelled task data. The objective is to fine-tune the model’s representations and weights to better align with the target task’s objectives and improve its performance. The task tuning process helps the model generalize its pre-learned knowledge to the specific task, making it more task-specific and accurate.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">Pre-training requires massive resources and utilizes publicly available data, and therefore it may not be suitable for FL. However, FL could still be valuable in the fine-tuning stage where privacy is a concern or when the tuning data is inherently decentralized. The reasons are listed as follows:</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><em id="S2.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Low Resource Requirements</em>:
Fine-tuning is relatively efficient as it requires less data and computational resources for adjusting a small subset of parameters of LLMs compared to the pre-training, making it feasible to train LLMs even on resource-constrained devices.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><em id="S2.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Data Privacy Protection</em>: Fine-tuning often involves specific tuning data that could be sensitive. For example, the LLM might be fine-tuned on users’ interactions with a digital assistant, which could contain personal information. FL allows this fine-tuning to happen on the users’ own devices, ensuring that personal information remains secure and confidential.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Retrieval-Augmented Generation Stage of LLM</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">RAG is an approach that combines LLMs with information retrieval techniques to enhance performance of LLMs<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Due to the enormous training cost of LLMs, the learned knowledge has a temporal lag. For instance, the training data for GPT-3.5 is current up until January 2022, implying that it lacks knowledge of any events transpiring after January 2022. During the generation process, instead of relying solely on the pre-trained or fine-tuned model’s knowledge, retrieval mechanisms are employed to retrieve the latest relevant information from external sources such as Internet or local knowledge bases.
These knowledge bases are often deployed at the edge, and clients retrieve the latest local information by querying knowledge bases. The retrieved local information, along with the client’s request, is then sent to the server-side LLM by wireless networks. Moreover, LLMs can leverage the Internet to retrieve the latest relevant public information.
Then, this retrieved information is incorporated into the generated output of the LLM, ensuring that the generated content is contextually relevant and factually accurate.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">However, RAG may also not be suitable for FL due to the following reasons:</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p"><em id="S2.I3.i1.p1.1.1" class="ltx_emph ltx_font_italic">Additional Data Exposure</em>:
Sharing sensitive retrieval queries or accessing external resources across multiple clients may compromise the privacy and security of the distributed data.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p"><em id="S2.I3.i2.p1.1.1" class="ltx_emph ltx_font_italic">No Weight Update</em>: RAG enhances the performance of LLMs by incorporating the retrieved data into the prompt for in-context learning. It does not require updating parameters of LLMs, thus eliminating the need for local gradient descent in FL.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">LLMs not only need to align with human values during the learning process, but also interact with external knowledge, which represents a more complex learning paradigm. Due to the massive amount of insensitive data and extensive model parameters involved in the pre-training stage, it is more suitable for centralized cloud-based learning. In contrast, during the RAG stage, only local data embedding and querying are required, making it more suitable for local execution. Therefore, we primarily focus on designing PFLs for the fine-tuning stage.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Comparison of the three-stages learning of LLMs.</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.4.1" class="ltx_tr">
<td id="S2.T1.4.1.1" class="ltx_td ltx_align_top ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S2.T1.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.2.1.1" class="ltx_p" style="width:56.9pt;">Objective</span>
</span>
</td>
<td id="S2.T1.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.3.1.1" class="ltx_p" style="width:56.9pt;">Data Requirement</span>
</span>
</td>
<td id="S2.T1.4.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.4.1.1" class="ltx_p" style="width:56.9pt;">Learning Approach</span>
</span>
</td>
<td id="S2.T1.4.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.5.1.1" class="ltx_p" style="width:56.9pt;">Adjusting Parameters</span>
</span>
</td>
<td id="S2.T1.4.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.6.1.1" class="ltx_p" style="width:56.9pt;">Privacy</span>
</span>
</td>
<td id="S2.T1.4.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.7.1.1" class="ltx_p" style="width:71.1pt;">Resource Requirements</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.2" class="ltx_tr">
<td id="S2.T1.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.1.1.1" class="ltx_p" style="width:62.6pt;">Pre-training</span>
</span>
</td>
<td id="S2.T1.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.1.1" class="ltx_p" style="width:56.9pt;">Learning general language representation</span>
</span>
</td>
<td id="S2.T1.4.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.3.1.1" class="ltx_p" style="width:56.9pt;">Abundant and diverse datasets</span>
</span>
</td>
<td id="S2.T1.4.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.4.1.1" class="ltx_p" style="width:56.9pt;">Unsupervised learning</span>
</span>
</td>
<td id="S2.T1.4.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.5.1.1" class="ltx_p" style="width:56.9pt;">All parameters</span>
</span>
</td>
<td id="S2.T1.4.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.6.1.1" class="ltx_p" style="width:56.9pt;">Require public dataset</span>
</span>
</td>
<td id="S2.T1.4.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.7.1.1" class="ltx_p" style="width:71.1pt;">High computational and storage resources</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.3" class="ltx_tr">
<td id="S2.T1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.1.1.1" class="ltx_p" style="width:62.6pt;">Instruction tuning</span>
</span>
</td>
<td id="S2.T1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.2.1.1" class="ltx_p" style="width:56.9pt;">Generalization for many tasks</span>
</span>
</td>
<td id="S2.T1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.1.1" class="ltx_p" style="width:56.9pt;">Instruction data</span>
</span>
</td>
<td id="S2.T1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.4.1.1" class="ltx_p" style="width:56.9pt;">Supervised learning or Reinforcement learning</span>
</span>
</td>
<td id="S2.T1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.4.3.5.1.1.1" class="ltx_text" style="color:#000000;">Partial parameters (5%-10%)</span></span>
</span>
</td>
<td id="S2.T1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.6.1.1" class="ltx_p" style="width:56.9pt;">May require user instruction</span>
</span>
</td>
<td id="S2.T1.4.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.7.1.1" class="ltx_p" style="width:71.1pt;">Medium computational and storage resources</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<td id="S2.T1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.1.1.1" class="ltx_p" style="width:62.6pt;">Task tuning</span>
</span>
</td>
<td id="S2.T1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.2.1.1" class="ltx_p" style="width:56.9pt;">Personalization for specific tasks</span>
</span>
</td>
<td id="S2.T1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.3.1.1" class="ltx_p" style="width:56.9pt;">Task-specific data</span>
</span>
</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.1.1" class="ltx_p" style="width:56.9pt;">Supervised learning</span>
</span>
</td>
<td id="S2.T1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.4.4.5.1.1.1" class="ltx_text" style="color:#000000;">Few parameters (1%-2%)</span></span>
</span>
</td>
<td id="S2.T1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.6.1.1" class="ltx_p" style="width:56.9pt;">May require task data</span>
</span>
</td>
<td id="S2.T1.4.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.7.1.1" class="ltx_p" style="width:71.1pt;">Low computational and storage resources</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.5" class="ltx_tr">
<td id="S2.T1.4.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.4.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.1.1.1" class="ltx_p" style="width:62.6pt;">Retrieval-augmented generation</span>
</span>
</td>
<td id="S2.T1.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.2.1.1" class="ltx_p" style="width:56.9pt;">Enhance output with relevant information</span>
</span>
</td>
<td id="S2.T1.4.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.4.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.3.1.1" class="ltx_p" style="width:56.9pt;">Latest external data</span>
</span>
</td>
<td id="S2.T1.4.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.4.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.4.1.1" class="ltx_p" style="width:56.9pt;">Retrieval</span>
</span>
</td>
<td id="S2.T1.4.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.4.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.1.1" class="ltx_p" style="width:56.9pt;">No parameters</span>
</span>
</td>
<td id="S2.T1.4.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.4.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.6.1.1" class="ltx_p" style="width:56.9pt;">Require up-to-date public or user data</span>
</span>
</td>
<td id="S2.T1.4.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.4.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.7.1.1" class="ltx_p" style="width:71.1pt;">Low computational and high storage resources</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Potential solutions of wireless federated fine-tuning for LLMs</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present key techniques and potential solutions of federated fine-tuning for LLMs.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Federated Instruction Tuning</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Federated instruction tuning involves FL for instruction tuning of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which necessitates each client to possess sufficient computational resources to fine-tune a larger number of parameters compared with task fine-tuning methods. Additionally, it requires transmitting more model parameters over the network, which can be bandwidth-intensive and time-consuming. Furthermore, it is challenging to define a clear, algorithmic loss to measure the quality of the LLM’s output for local instructions. The following techniques hold the potential to address these challenges.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Sparse Attention</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">In traditional attention mechanisms, every token (word) attends to every other token, resulting in a quadratic complexity with respect to the input sequence length. However, in scenarios where the input sequence is long, this approach becomes computationally expensive and memory-intensive.
Sparse attention addresses this issue by introducing sparsity patterns, allowing tokens to attend only to a subset of other tokens rather than attending to all tokens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This reduces the computational and memory requirements, and communication overhead while preserving the LLM’s ability to capture relevant dependencies between tokens in FL. The sparsity pattern can be learned during the instruction tuning, enabling the LLM to dynamically determine the subset of attention parameters based on the wireless environment.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>RLHF</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">RLHF is a Reinforcement Learning (RL)-based fine-tuning technique that improves the performance of LLMs by incorporating human-generated feedback <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for difficult-to-assess objectives. This method involves a three-step process. Initially, an LLM is fine-tuned using supervised instruction learning, drawing from human-generated instructions or demonstrations. Next, a question and several model outputs are sampled. Human evaluators then rank these outputs from best to worst, and the ranked data is used to train a new reward model. Finally, the reward model calculates the reward for the LLM’s output, and the estimated reward is utilized to update the policy through RL. In FL, we can only adjust the parameters that are used to compute the generation probability distribution. These parameters control the probabilities of selecting different words or tokens during the generation process, which can influence the output of the LLM to better align with the expectations conveyed by human feedback (e.g., to fine-tune only the last few layers of LLMs).</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Federated Task Tuning</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Federated task tuning employs FL for fine-tuning downstream tasks of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which is relatively efficient as it requires less data and computational resources of cilents than that of the instruction tuning. It allows for task-specific adaptation but also risks overfitting or forgetting the original knowledge of local LLMs. PEFT is a set of techniques designed to adapt LLMs to specific downstream tasks with minimal changes to the original parameters. The following PEFT methods can help mitigate overfitting and catastrophic forgetting in LLMs.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Adapter</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Adapter tuning is introduced to add small, task-specific adapter modules to a pre-trained LLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. These adapter modules are inserted between the layers of the pre-trained LLM. They typically consist of a bottleneck structure: a down-projection, a non-linearity, and an up-projection. During fine-tuning, only the parameters of these adapter modules are updated, while the parameters of the pre-trained LLM are kept fixed. In FL, this approach significantly reduces the number of parameters that need to be updated in each client, while still allowing the LLM to adapt to the specific task with fewer resources. Moreover, when adaptation to wireless channel quality, we can define the dimensions of adapters adaptively, thereby dynamically adjusting the communication overhead for transmission in wireless channels.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>LoRA</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">LoRA adapts a pre-trained LLM to a new task by applying low-rank matrix decomposition to the model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. LoRA decomposes the original weight matrices into the product of two smaller matrices, and only updates the smaller matrices during fine-tuning. In FL, LoRA can reduce the number of parameters that need to be updated, thereby decreasing the communication and computational cost on the client. By preserving historical low-rank matrices, LoRA can prevent catastrophic forgetting of local models, while preserving its generation and generalization abilities.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><span id="S3.SS2.SSS2.p2.1.1" class="ltx_text" style="color:#000000;">Federated instruction tuning deepens the understanding of queries by learning from human instructions, while federated task tuning enhances the performance of downstream tasks by learning from different task data. Both approaches strengthen the distributed learning performance of LLMs from different perspectives.</span></p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Personalized federated fine-tuning</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Current Research Progress</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">There have been several joint research efforts on wireless FL and LLMs. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> combined split learning and FL to pretrain the BERT model. Reference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> proposed a federated instruction learning called Shepherd, which utilizes LoRA for fine-tuning LLMs through instruction data. Similarly, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> presented a low-parameter federated learning based on LoRA for task fine-tuning of LLMs. Reference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> addressed co-tuning and off Site-tuning of LLMs through a comprehensive FL open-source framework called Fate-LLM. However, all of these studies aim to train a unified LLM in a distributed manner, overlooking device variations, user preferences, and characteristics. As a result, they fail to achieve device-adaptive and user-centric LLMs.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Advantages of Personalized Federated Fine-tuning.</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">PFL allows for designing personalized LLMs that can adapt to the data of individual clients over wireless networks, which may improve user satisfaction and engagement levels. Advantages of applying PFL to LLMs include:
</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>Personalized User Data</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">PFL allows personalized learning from various user data, which can be beneficial in fine-tuning LLMs on non-identically and independently distributed (non-IID) data. PFL can learn a personalized LLM for each client that is tailored to its own data distribution, enabling the LLM to learn from a wider range of personalized contexts, features and patterns. This can lead to improved understanding and generation capabilities of LLMs for user private data.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>Customized Local Model</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">By allowing each client to have its own personalized LLM, PFL enables customization of the local tuning process to each device’s preference and constraints. This can lead to better suitability to personal computational, storage, and communication resources and improved model performance for all clients.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS3.4.1.1" class="ltx_text">IV-B</span>3 </span>Specific Communication Process</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">With PFL, the global aggregation of LLMs can be tailored to each client’s preference and requirement, avoiding unnecessary updates that may not be relevant to them. This reduces communication costs, making the fine-tuning process more efficient.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">Therefore, PFL provides flexibility in balancing the trade-off between learning shared knowledge and personalized knowledge for LLMs.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Personalized Federated Instruction Tuning</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We propose a PFIT method based on RL, in which each client has personalized requirements for helpfulness and safety of the LLM. Helpfulness emphasizes the quality and accuracy of generated content, such as grammatical correctness, logical coherence, and relevance of the responses. Safety, on the other hand, emphasizes the legality and ethicality of the generated content, such as the absence of sensitive or harmful information. For example, in Fig. <a href="#S4.F2" title="Figure 2 ‣ IV-C Personalized Federated Instruction Tuning ‣ IV Personalized federated fine-tuning ‣ Personalized Wireless Federated Learning for Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the LLM with high helpfulness would provide answers to user questions regarding employee information, including sensitive details. However, the LLM with high safety levels would refrain from answering sensitive employee information, prioritizing user privacy and data protection.
To achieve PFIT, we introduce three key innovations:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><em id="S4.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Double Reward Model</em>: We define two reward models to evaluate the helpfulness and safety of local instruction responses. The quality reward for different clients’ outputs is obtained by linearly weighting these two reward models, enabling personalized instruction fine-tuning of the LLM for different clients’ requirements.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><em id="S4.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Personalized Reward Function</em>: We design a personalized reward function that includes both the quality reward from two reward models and a negative regularization reward for global knowledge. The regularization reward is based on the Euclidean distance between local model parameters and global model parameters, promoting the knowledge sharing among clients in the PFL system.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><em id="S4.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Sparse Attention Update</em>: To encourage lightweight devices to participate in PFL, we only train the last two layers of the LLM. Additionally, we adopt a sparse attention mechanism to further reduce the computation complexity and communication overhead in self-attention layers during the fine-tuning phase of the LLM.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The specific workflow of the PFIT is as follows:</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Step 1</span>: Initialize the pre-trained LLM as the global model on the server and freeze the earlier parts of the LLM.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Step 2</span>: Each client uses the global LLM as the initial local LLM, sets a personalized reward function (In the red dashed box of Fig. <a href="#S4.F2" title="Figure 2 ‣ IV-C Personalized Federated Instruction Tuning ‣ IV Personalized federated fine-tuning ‣ Personalized Wireless Federated Learning for Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), and selects its own instruction data for local learning.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Step 3</span>: Based on the current local LLM, the client calculates the regularization reward related to the global LLM, evaluates the helpfulness and safety of responses, computes the quality reward for instructions, and then updates the unfrozen part of the local LLM using the PPO algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> according to the personalize reward function, including both quality reward and regularization reward.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p"><span id="S4.SS3.p6.1.1" class="ltx_text ltx_font_bold">Step 4</span>: The server aggregates the sparse tunable layers (the unfrozen parts of LLMs) from all clients, obtains the updated global LLM, and sends the global LLM (the unfrozen part) to all clients.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p"><span id="S4.SS3.p7.1.1" class="ltx_text ltx_font_bold">Step 5</span>: Steps 3-4 are repeated until the convergence criteria of the PFL system are met.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2404.13238/assets/FedLLM2_1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="354" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">The illustration of the proposed PFIT based on RL.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Personalized Federated Task Tuning</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We propose a PFTT method based on PEFT, in which a set of clients have different task goals, and each client possesses a set of non-IID task data. For instance, in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-D Personalized Federated Task Tuning ‣ IV Personalized federated fine-tuning ‣ Personalized Wireless Federated Learning for Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, different clients are tasked with film classification, but the distribution of labelled data varies across different clients. Some clients may have a higher proportion of science fiction and realistic film labels, while others may have a larger number of comedy and tragedy film labels. As a result, the LLM will excel at providing personalized movie classifications based on the local distribution of labelled data available to it.
To achieve PFTT, we introduce three key innovations:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><em id="S4.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Universal Adapter</em>: We incorporate adapters into the global pre-trained LLM to share task knowledge among different clients.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><em id="S4.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Local LoRA</em>: We introduce LoRA in the local LLMs to enable personalized local knowledge learning. The size of LoRA can be adjusted based on the data volume or computational resources available on each client.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><em id="S4.I2.i3.p1.1.1" class="ltx_emph ltx_font_italic">Partial Aggregation</em>: During global aggregation, only the adapter parameters are aggregated for global knowledge sharing, while the LoRA parameters are not aggregated for maintaining personalization. This approach enables personalized task tuning in heterogeneous devices.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The specific workflow of the PFTT is as follows:</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Step 1</span>: Initialize the pre-trained LLM as the global LLM on the server and insert adapters to the LLM.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Step 2</span>: Each client uses the global LLM as the initial local LLM and designs local LoRA parameters based on the data volume or computational resource of the local LLM.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p"><span id="S4.SS4.p5.1.1" class="ltx_text ltx_font_bold">Step 3</span>: Based on the current global LLM and local LoRA parameters, the client fine-tunes the LLM using local task data and updates the adapter and LoRA parameters over the wireless networks.</p>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p"><span id="S4.SS4.p6.1.1" class="ltx_text ltx_font_bold">Step 4</span>: The server aggregates the adapter parameters from all clients, obtains the updated global adapter parameters, and sends them to the clients.</p>
</div>
<div id="S4.SS4.p7" class="ltx_para">
<p id="S4.SS4.p7.1" class="ltx_p"><span id="S4.SS4.p7.1.1" class="ltx_text ltx_font_bold">Step 5</span>: Steps 3-4 are repeated until the convergence criteria of the system are met.</p>
</div>
<div id="S4.SS4.p8" class="ltx_para">
<p id="S4.SS4.p8.1" class="ltx_p"><span id="S4.SS4.p8.1.1" class="ltx_text" style="color:#000000;">Through PFL, both fine-tuning methods can achieve better performance for personalized LLMs.</span></p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2404.13238/assets/FedLLM1_1.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="354" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">The illustration of the proposed PFTT based on PEFT.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Simulation Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section provides two experiments for demonstrating the effectiveness of the proposed PFIT and PFTT.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Problem Formulation</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We consider a PFL system for LLMs. Suppose the system consists of four clients and one server, where each client possesses different local data and model preferences. The server has sufficient resources. The PFL system aims to achieve personalized fine-tuning of the local LLMs for all clients in wireless networks with Rayleigh channel, the communication round is set to 40 and the SNR is set to 5dB.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Simulation Settings</span>
</h3>

<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS1.4.1.1" class="ltx_text">V-B</span>1 </span>Settings for PFIT</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">We first employ the Alpaca dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to evaluate the validity of the proposed PFIT scheme. Secondly, we use GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as the local LLM with 40% sparse attention and adopt PPO as the local RL algorithm. During the fine-tuning process, we sample instructions from the dataset to generate the corresponding responses from GPT-2. Then, we incorporate the “instruction+response” into two reward models, with one model assessing the helpfulness score of the response and the other evaluating its safety score.
<span id="S5.SS2.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">We then utilize the reward score (i.e., helpfulness score plus safety score) and communication cost (i.e., the size of parameters to be aggregated) per round as evaluation metrics.</span></p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS2.4.1.1" class="ltx_text">V-B</span>2 </span>Settings for PFTT</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">We first employ the AG’s news corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as the evaluation dataset for PFTT. Additionally, we adopt a Dirichlet distribution to facilitate a non-IID data partition among clients. Next, we utilize RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as the local LLM, which is an improved BERT model pre-trained on a substantial corpus of English data in a self-supervised manner. We use 12 universal adapters for each client to exchange information among different clients. Subsequently, each client incorporates 10-12 local LoRAs, based on their local resources, to achieve local model personalization.
<span id="S5.SS2.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">Lastly, since the local LLM is responsible for sentence classification, we use the classification accuracy and communication (i.e., communication cost divided by transmission rate) delay per round as evaluation metrics.</span></p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Evaluation Results</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Fig. <a href="#S5.F4" title="Figure 4 ‣ V-C Evaluation Results ‣ V Simulation Results ‣ Personalized Wireless Federated Learning for Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the evaluation results for PFIT and its contenders, where SFL means to a fine-tuning method that uses only a single reward model (helpfulness metric) and incorporates 20% sparse attention. PFL, on the other hand, represents personalized fine-tuning without the sparse attention. Shepherd is an FL method that employs LoRA for instruction fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. We can see that PFIT enables the local model to obtain the highest rewards than SFL and Shepherd with the single reward model. Moreover, compared to PFL that does not use sparse attention, PFIT reduces communication overhead by 20%. Shepherd, utilizing LoRA for instruction fine-tuning, results in the lowest communication overhead. However, this approach also affects the LLM’s performance, resulting in lower rewards compared to PFIT.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2404.13238/assets/PFIT_exp3_2.jpg" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="243" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">Reward and communication cost for PFIT and its contenders.</span></figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Fig. <a href="#S5.F5" title="Figure 5 ‣ V-C Evaluation Results ‣ V Simulation Results ‣ Personalized Wireless Federated Learning for Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the evaluation results for PFTT and its benchmarks. In vanilla FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the parameters of both adapters and LoRAs all need to be uploaded.
FedBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is a federal split learning method, and FedLora <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is
a federated fine-tuning method that exclusively incorporates LoRA.
The results indicate that PFTT achieves the highest accuracy, which highlights the effectiveness of the LoRA-based personalized structure. Similarly, due to the fact that PFTT only requires the transmission of a part of fine-tuned parameters (universal adapters), it incurs the minimum communication overhead compared to other methods.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2404.13238/assets/PFTT_exp3_3.jpg" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="239" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">Accuracy and communication cost for PFTT and its contenders.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Open Issues</span>
</h2>

<section id="S6.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS0.SSS1.4.1.1" class="ltx_text">VI-</span>1 </span>Wireless Aggregation and Divergence</h4>

<div id="S6.SS0.SSS1.p1" class="ltx_para">
<p id="S6.SS0.SSS1.p1.1" class="ltx_p">In PFL, multiple participants collaborate to train a shared LLM. However, due to the possibility of signal quality fluctuations in wireless networks, mobile devices may experience communication interruptions and data loss. In the aggregation, these instabilities can lead to model divergence, where participants have different contributions to model updates. <span id="S6.SS0.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">Addressing the challenges of model aggregation and divergence in wireless networks requires asynchronous model aggregation strategies and fair client selection mechanisms to ensure the model effectively incorporates contributions from all participants while balancing the differences to ensure the reliability of model updates.</span></p>
</div>
</section>
<section id="S6.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS0.SSS2.4.1.1" class="ltx_text">VI-</span>2 </span>Personalization and Overfitting</h4>

<div id="S6.SS0.SSS2.p1" class="ltx_para">
<p id="S6.SS0.SSS2.p1.1" class="ltx_p">Personalization is one of the core objectives in PFL, aiming to customize the shared LLM to meet specific needs of each client. However, the introduction of personalization can potentially lead to overfitting problems. If the personalized requirements are overly detailed, the LLM may overfit to the data of specific clients, resulting in degraded performance on other clients or tasks. Resolving the challenges of personalization and overfitting necessitates appropriate regularization and control during the fine-tuning process to strike a balance between the degree of personalization and the LLM’s generalization.</p>
</div>
</section>
<section id="S6.SS0.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS0.SSS3.4.1.1" class="ltx_text">VI-</span>3 </span>Communication Efficiency and Model Accuracy</h4>

<div id="S6.SS0.SSS3.p1" class="ltx_para">
<p id="S6.SS0.SSS3.p1.1" class="ltx_p">PFL involves communication and collaboration among multiple participants. Communication overhead can be a significant challenge, particularly in scenarios with LLMs and numerous participants. Frequent communication can increase communication latency and resource consumption. Moreover, the unreliability or instability of communication can result in the loss or delay of model updates, which can have a direct impact on the accuracy and performance of the LLM. Addressing the issue requires designing efficient communication protocols and strategies to reduce communication overhead while ensuring reliable transmission of data and model parameters.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we have first summarized three learning stages of LLMs and discussed potential solutions for combining FL with LLMs. Next, we have proposed two PFLs for different fine-tuning methods. Specifically, we introduced PFIT for instruction fine-tuning of LLMs based on client preferences. We then designed two reward models based on helpfulness and safety, and we used RLHF to fine-tune the LLM based on diverse combinations of reward models from clients. Furthermore, we proposed PFTT to fine-tune downstream classification tasks based on locally non-IID data. In PFTT, we used global adapters to enable information exchange between devices and incorporated locals LoRA to customize the local LLM.
Finally, we carried out simulations to validate the effectiveness of these proposed methods.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Q. Duan <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Combining federated learning and edge computing toward
ubiquitous intelligence in 6g network: Challenges, recent advances, and
future directions,” <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">IEEE Communications Surveys and Tutorials</em>,
vol. 25, no. 4, pp. 2892–2950, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Z. Tan <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Towards personalized federated learning,”
<em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, vol. 34,
no. 12, pp. 9587–9603, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y. Tian <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Fedbert: When federated learning meets pre-training,”
<em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
vol. 13, no. 4, pp. 1–26, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Zhang <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Towards building the federated gpt: Federated
instruction tuning,” <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.05644</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
F. Jiang <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Large language model enhanced multi-agent systems for
6g communications,” <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.07850</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Roy <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Efficient content-based sparse attention with routing
transformers,” <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational
Linguistics</em>, vol. 9, pp. 53–68, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. R. McIntosh <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The inadequacy of reinforcement learning from
human feedback-radicalizing large language models via semantic
vulnerabilities,” <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on Cognitive and Developmental
Systems</em>, 2024.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Jiang, X. Liu, and C. Fan, “Low-parameter federated learning with large
language models,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.13896</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
N. Ding <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Parameter-efficient fine-tuning of large-scale
pre-trained language models,” <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, vol. 5,
no. 3, pp. 220–235, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T. Fan <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Fate-llm: A industrial grade federated learning
framework for large language models,” <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2310.10049</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Gu <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Proximal policy optimization with policy feedback,”
<em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on Systems, Man, and Cybernetics: Systems</em>, vol. 52,
no. 7, pp. 4600–4610, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Wang <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Self-instruct: Aligning language model with self
generated instructions,” <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10560</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Radford <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Language models are unsupervised multitask
learners,” <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">OpenAI blog</em>, vol. 1, no. 8, p. 9, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks for
text classification,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, vol. 28.   Curran Associates,
Inc., 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Liu <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Roberta: A robustly optimized bert pretraining
approach,” <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
</ul>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Biographies</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text ltx_font_bold">Feibo Jiang</span> (jiangfb@hunnu.edu.cn) received Ph.D. degree from the Central South University, China. He is currently an Associate Professor at Hunan Normal University, China.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p"><span id="Sx1.p2.1.1" class="ltx_text ltx_font_bold">Li Dong</span> (Dlj2017@hunnu.edu.cn) received Ph.D. degree from the Central South University, China. She is currently an Associate Professor at Hunan University of Technology and Business, China.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p"><span id="Sx1.p3.1.1" class="ltx_text ltx_font_bold">Siwei Tu</span> (tusiwei@hunnu.edu.cn) is currently pursuing the master’s degree with Hunan Normal University, China.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p"><span id="Sx1.p4.1.1" class="ltx_text ltx_font_bold">Yubo Peng</span> (pengyubo@hunnu.edu.cn) is currently pursuing the master’s degree with Hunan Normal University, China.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p"><span id="Sx1.p5.1.1" class="ltx_text ltx_font_bold">Kezhi Wang</span> (Kezhi.Wang@brunel.ac.uk) received Ph.D. degree from University of Warwick, U.K. in 2015. Currently he is a Senior Lecturer with the Department of Computer Science, Brunel University London, U.K.</p>
</div>
<div id="Sx1.p6" class="ltx_para">
<p id="Sx1.p6.1" class="ltx_p"><span id="Sx1.p6.1.1" class="ltx_text ltx_font_bold">Kun Yang</span> (kunyang@essex.ac.uk) received his PhD from the Department of Electronic &amp; Electrical Engineering of University College London (UCL), UK. He is currently a Chair Professor in the School of Computer Science &amp; Electronic Engineering, University of Essex, UK. He is also an affiliated professor at UESTC, China.</p>
</div>
<div id="Sx1.p7" class="ltx_para">
<p id="Sx1.p7.1" class="ltx_p"><span id="Sx1.p7.1.1" class="ltx_text ltx_font_bold">Cunhua Pan</span> (cpan@seu.edu.cn) received Ph.D. degrees from Southeast University, China, in 2015. He held a post-doctoral position at Queen Mary University of London, U.K., from 2016 and 2019. From 2019 to 2021, he was a Lecturer in the same university. From 2021, he is a full professor in Southeast University, China.</p>
</div>
<div id="Sx1.p8" class="ltx_para">
<p id="Sx1.p8.1" class="ltx_p"><span id="Sx1.p8.1.1" class="ltx_text ltx_font_bold">Dusit Niyato</span> (dniyato@ntu.edu.sg) received the Ph.D. degree in electrical and computer engineering from the University of Manitoba in Canada in 2008. He is a professor in the College of Computing and Data Science, Nanyang Technological University, 639798 Singapore.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.13237" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.13238" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.13238">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.13238" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.13239" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 17:37:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
