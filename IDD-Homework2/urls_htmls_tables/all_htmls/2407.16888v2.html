<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Nested Model for AI Design and Validation</title>
<!--Generated on Thu Aug  1 11:46:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.16888v2/"/></head>
<body>
<nav class="ltx_page_navbar">
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">[orcid=0009-0008-4823-9375]</p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">[orcid=0000-0001-9974-3231]</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">[orcid=0000-0003-4168-8254]</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">1]organization=Center for Artificial Intelligence in Public Health Research (ZKI-PH) at Robert Koch Institute,
addressline=Nordufer 20,
city=Berlin,
citysep=, postcode=13353,
country=Germany
</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">2]organization=Department of Mathematics and Computer Science, Freie Universität Berlin,
addressline=Arnimallee 14,
city=Berlin,
citysep=, postcode=14195,
country=Germany
3]organisation = Lead author</p>
</div>
<h1 class="ltx_title ltx_title_document">A Nested Model for AI Design and Validation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Akshat Dubey
</span><span class="ltx_author_notes">DubeyA@rki.de</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zewen Yang
</span><span class="ltx_author_notes">YangZ@rki.de</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Georges Hattab
</span><span class="ltx_author_notes">HattabG@rki.de
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The growing AI field faces trust, transparency, fairness, and discrimination challenges. Despite
the need for new regulations, there is a mismatch between regulatory science and AI, preventing a
consistent framework. A five-layer nested model for AI design and validation aims to address these
issues and streamline AI application design and validation, improving fairness, trust, and AI adoption.
This model aligns with regulations, addresses AI practitioners’ daily challenges, and offers prescriptive guidance for determining appropriate evaluation approaches by identifying unique validity threats.
We have three recommendations motivated by this model: authors should distinguish between layers when claiming contributions to clarify the specific areas in which the contribution is made and to avoid confusion, authors should explicitly state upstream assumptions to ensure that the context and limitations of their AI system are clearly understood, AI venues should promote thorough testing and validation of AI systems and their compliance with regulatory requirements.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Artificial Intelligence <span class="ltx_ERROR undefined" id="id2.id1">\sep</span>Machine Learning <span class="ltx_ERROR undefined" id="id3.id2">\sep</span>AI Regulations <span class="ltx_ERROR undefined" id="id4.id3">\sep</span>European Union <span class="ltx_ERROR undefined" id="id5.id4">\sep</span>AI Compliance <span class="ltx_ERROR undefined" id="id6.id5">\sep</span>AI Oversights <span class="ltx_ERROR undefined" id="id7.id6">\sep</span>Explainable Artificial Intelligence <span class="ltx_ERROR undefined" id="id8.id7">\sep</span>XAI <span class="ltx_ERROR undefined" id="id9.id8">\sep</span>Ethical AI <span class="ltx_ERROR undefined" id="id10.id9">\sep</span>Nested Model

</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">While artificial intelligence (AI) has grown tremendously in recent years, it has yet to reach its true potential in real-world use cases. This is due in part to a lack of trust and transparency, as well as fairness and fear of discrimination <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib4" title="">4</a></sup></cite>.
AI has unique strengths and weaknesses, so there will always be a need to develop new regulations and change old ones. But with the benefits of AI come significant ethical and legal risks.
As a result, there is an urgent need to address not only the regulatory policies that will facilitate the implementation of AI in real-world use cases but also how practitioners design and validate AI applications and workflows.
Several regulatory bodies from different countries are stepping in to establish a set of regulations for the implementation of AI in real-world applications. Researchers have shown an increased interest in unifying regulatory science and AI.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Surveys on the subject of AI and human-computer interaction have already been conducted. They draw attention to the drawbacks and difficulties with interactive machine learning and transparency in AI <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib51" title="">51</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib7" title="">7</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib6" title="">6</a></sup></cite>. Many of them offer a set of conceptual and design guidelines to help ensure that intelligent systems are understandable and that human users are held accountable. Using a framework, Mohseni <span class="ltx_text ltx_font_italic" id="Sx1.p2.1.2">et al.</span>. published an intriguing work that surveys and identifies the state of research on the junction of XAI and HCI <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p2.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib32" title="">32</a></sup></cite>.To bring the iterative design and evaluation cycles in diverse Explainable AI teams to a close, they created a framework with detailed design rules along with assessment techniques.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">AI systems are complex in nature and often involve multiple stakeholders. One of the key tools for managing complex systems is modularity. By distinguishing between activities that require extensive analysis and those that do not, modularity seeks to minimize the number of interdependencies that need to be examined. A specific type of modularity known as “layering" involves the arrangement of different system components into parallel hierarchies. Most research on nested models is based on the layering approach <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p3.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib55" title="">55</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">To address the complexity of AI systems, the authors break down governance issues into smaller, more manageable parts, encourage shared accountability among stakeholders, and provide a framework for the creation of laws, policies, standards, and other guidelines that can be used in concert to guide the responsible development and application of AI technologies <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p4.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib17" title="">17</a></sup></cite>.
The model consists of three layers, namely social &amp; legal, ethical, and technical (including algorithms and data), which can be developed independently.
They propose a modular approach that divides AI governance into social, legal, ethical, and technical layers to minimize risks and maximize benefits, making it more efficient and tractable.
The social and legal layer regulates AI by establishing institutions and norms within a legal framework.
The Ethical Layer addresses ethical concerns for AI systems, ensuring fairness, accountability, and transparency.
The Technical Layer, based on algorithms and data, promotes fairness and safeguards against discrimination.
These layers work together to manage the societal impacts of AI and ensure its trustworthiness, accountability, and auditability.
However, we argue that the technical layer can’t be addressed without addressing regulatory requirements.
For example, one of the most common requirements set by AI regulators is AI transparency. While addressing the three layers independently, the technical layer may or may not address the challenge of transparency if the technical layer is addressed first, leading to high uncertainty in development.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">Recent research from Wang<span class="ltx_text ltx_font_italic" id="Sx1.p5.1.1">et al.</span>presents a user-centric XAI as a nested model.
The study is limited to drug repurposing using graph neural network (GNN) <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p5.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib13" title="">13</a></sup></cite>. Although the authors address the domain context, which is very important for XAI, they don’t address the regulations, which is another important perspective <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p5.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib50" title="">50</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">Several research efforts from the fields of Human-Computer Interaction (HCI) and Explainable AI (XAI) have highlighted current bottlenecks in involving humans in understanding, decision-making, validation, etc. or having humans in the loop for AI applications and workflows.
There is an influential work by Liao <span class="ltx_text ltx_font_italic" id="Sx1.p6.1.1">et al.</span>, on the intersection of HCI and XAI in the form of XAI-Question Bank (XAI-QB). XAI-QB aims to solve the challenge of achieving full explicability through algorithm-informed prototypical questions <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p6.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib28" title="">28</a></sup></cite>.
XAI-QB was further advanced and was extended to include the prototypical questions for the end-users which they may come across while interacting through an interface <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p6.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib45" title="">45</a></sup></cite>.
While the need for guidelines and regulatory frameworks has been addressed by Lennerz <span class="ltx_text ltx_font_italic" id="Sx1.p6.1.4">et al.</span>, one of the problems is how AI is implemented and how results are communicated, which further hinders AI adoption <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p6.1.5.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib26" title="">26</a></sup></cite>.
The authors specifically mention that regulatory concepts are necessary for AI researchers, as these concepts allow risk and safety concerns to be addressed and understood by the regulations proposed by the US and Europe.</p>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">At the moment, however, the fields of regulatory science and AI are diverging, with no major overlap in sight.
A growing trend has been observed in discussions that express an urgent need for work at the intersection of regulatory science and AI <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p7.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib10" title="">10</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib19" title="">19</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib14" title="">14</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib31" title="">31</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib43" title="">43</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib37" title="">37</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib25" title="">25</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib12" title="">12</a></sup></cite>.
When it comes to regulating AI, many regulatory bodies are stepping in and making it mandatory to comply with laws that require explanations or interpretations to be given to users when confronted with algorithmic output <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p7.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib9" title="">9</a></sup></cite>.
The intersection of artificial intelligence (AI) and regulation requires massive work more than ever due to the emergence of new AI laws <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p7.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib23" title="">23</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib48" title="">48</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib53" title="">53</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib8" title="">8</a></sup></cite>. There is a growing trend of discussions that express an urgent need for the aforementioned intersection <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p7.1.4.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib29" title="">29</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib52" title="">52</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib42" title="">42</a></sup></cite>.
In response to the growing need for updated regulations to address ethical and legal risks that face obstacles due to a misalignment between regulatory science and AI, we have presented a structured, five-layer, nested model for the design and validation of AI that serves as a systematic guide for the assessment and validation of AI applications and workflows.
It facilitates the identification of appropriate evaluation methodologies by identifying unique threats within each layer, thereby mitigating the inherent tensions between technological innovation and regulatory imperatives.
In addition, the proposed model addresses concerns about fairness, trust, and the alignment of AI models with existing regulations.
To address the challenge of bridging the gap between AI practitioners and regulators, two preliminary case studies include researchers who aim to develop reliable, wise, and trustworthy human-centered AI through ethical and theoretical guidelines with management strategies or software engineering practices.
These include audit trails to enable analysis of failures, software engineering workflows, verification and validation testing, bias testing to enhance fairness, and even explainable user interfaces <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx1.p7.1.5.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib11" title="">11</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib44" title="">44</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib2" title="">2</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p8">
<p class="ltx_p" id="Sx1.p8.1">The proposed model for AI design and validation is the first of its kind, inspired by XAI-QB, which we extend to include the prototypical questions from a regulatory and domain perspective.
In parallel with the regulations, it addresses the issues that AI practitioners face on a daily basis.
The nested model consists of five layers, namely the regulation, the domain, the data, the model, and the prediction.
We have expanded regulatory and domain questions from the XAI-QB.
In addition, we have grouped the prototypical questions from the XAI-QB based on the layer of the nested model of AI design and validation that they need to address.
The five-layer nested model for AI design and validation, including but not limited to its regulatory and domain-aware layers and questions, are our major contributions.
The nested model for AI provides much-needed overlap and grounding, facilitating the design and validation of AI applications and workflows, and increasing fairness, trust, and AI adoption.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Regulation of AI</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Calls for appropriate regulation of AI have grown as awareness of its risks has increased.
This regulation aims to ensure that AI is legal, ethical, and robust while minimizing potential harm and increasing legal certainty.
Efforts are underway to establish global regulatory standards for AI, potentially leading to harmonization.
Collaboration between government, industry, and civil society is essential for the responsible use of AI.
Regulation is needed to protect consumers and society, provide a reliable framework for businesses, and understand the ethical and societal implications of AI.
The complexity and risks associated with AI underscore the urgency of establishing best practices and a comprehensive framework for AI regulation that takes into account ethical, legal, and societal impacts <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx2.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib47" title="">47</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib3" title="">3</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">The EU has taken a leading role in developing AI regulations, with a diverse regulatory landscape and a focus on protecting fundamental rights.
The General Data Protection Regulation (GDPR) includes provisions that address the legitimacy of algorithmic decision-making, emphasizing the right to human intervention and meaningful information for individuals.
The European Commission’s Ethics Guidelines for Trustworthy AI and the proposed AI Act aim to ensure the responsible and transparent use of AI, introducing principles such as human agency, technical robustness, privacy, transparency, and accountability.
These efforts aim to create a framework for trustworthy AI that benefits society and the environment <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx2.p2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib40" title="">40</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib34" title="">34</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib46" title="">46</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib1" title="">1</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">In the United States, AI regulation has relied primarily on self-regulation by industry stakeholders, leading to criticism for a lack of rigorous regulatory oversight. The Defense Advanced Research Projects Agency (DARPA) has initiated the first research program in XAI to address the challenge of opaque yet effective AI systems, with the goal of developing machine learning techniques for more explainable models. The National Institute of Standards and Technology (NIST) has also emphasized the importance of explainability, proposing principles to ensure that AI systems provide understandable explanations for their results. While the US regulatory landscape is focused on promoting innovation, the EU’s emphasis on XAI and ongoing policy discussions in the US and UK reflect a growing recognition of the importance of explainability in AI systems <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx2.p3.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib34" title="">34</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib22" title="">22</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib18" title="">18</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib38" title="">38</a></sup></cite>.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Nested Model for AI Design and Validation</h2>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Motivation</h3>
<div class="ltx_para" id="Sx3.SSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.p1.1">The EU is developing AI regulations with key requirements that align with the principles of Explainable AI (XAI), but addressing both ethical and technical requirements may require a focus on XAI and human-computer interaction (HCI) to ensure that AI upholds broader values such as accountability, human rights, and sustainable innovation.
At the intersection of XAI and HCI, the XAI-QB serves as a valuable tool for understanding user requirements for XAI.
It provides a set of algorithm-informed questions to achieve user-centered explainability in AI applications.
To address the perspective of domain and regulatory authorities, and inspired by both Munzner’s nested model for visualization and the XAI-QB, we propose a five-layer nested model for the design and validation of AI applications and workflows.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Our Work</h3>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">Our work has taken full advantage of the state of the art and the effectiveness of the nested model.
We extended the XAI-QB to include regulatory and domain-layer questions, aiming to help organizations address the needs of regulators when implementing AI.
This user-centric approach provides a hierarchical guide for stakeholders to elicit end-user and regulatory needs, while also highlighting technical barriers and emphasizing the importance of a human-centered approach to regulation using XAI.
The nested model supports specifying requirements for building AI applications and identifies opportunities for collaboration between the HCI and AI communities, industry practitioners, and academics to advance the field of AI.</p>
</div>
<figure class="ltx_figure" id="Sx3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="197" id="Sx3.F1.g1" src="x1.png" width="346"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Five-layer nested model for AI design and validation.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx2.p2">
<p class="ltx_p" id="Sx3.SSx2.p2.1">Our nested model (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx3.F1" title="Figure 1 ‣ Our Work ‣ Nested Model for AI Design and Validation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">1</span></a>) is divided into five distinct layers.
The model can be accompanied by questions.
These questions should be answered at each layer.
Although these questions can be modified or extended depending on the use case and user-specific requirements, they help to satisfy regulatory requirements.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p3">
<p class="ltx_p" id="Sx3.SSx2.p3.1">The regulation layer is responsible for making the AI workflow compliant.
At this layer, we categorize the rules into ethical and technical regulations.
Ethical rules should be specified and addressed at this layer.
This layer prohibits access to subsequent layers if the ethical rules are not addressed.
Once the ethical rules have been addressed with the appropriate infrastructure and various methodologies, the user should proceed to subsequent layers to address the technical rules.</p>
<ol class="ltx_enumerate" id="Sx3.I1">
<li class="ltx_item" id="Sx3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx3.I1.i1.p1">
<p class="ltx_p" id="Sx3.I1.i1.p1.1">Which regulation should be specified for this AI workflow?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx3.I1.i2.p1">
<p class="ltx_p" id="Sx3.I1.i2.p1.1">Which country has specified this regulation?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx3.I1.i3.p1">
<p class="ltx_p" id="Sx3.I1.i3.p1.1">Can you list out the key requirements set up by the regulation?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx3.I1.i4.p1">
<p class="ltx_p" id="Sx3.I1.i4.p1.1">Can you categorize the guidelines into ethical and technical ones?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx3.I1.i5.p1">
<p class="ltx_p" id="Sx3.I1.i5.p1.1">Who are the stakeholders involved in this AI workflow?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="Sx3.SSx2.p4">
<p class="ltx_p" id="Sx3.SSx2.p4.1">The domain layer ensures that any process taking place in the nested model is within the scope of that domain.
This nested model will allow for explainability at the domain layer. Different domains, in different settings, may have different answers to the XAI questions. At this layer, the domain expert is solely responsible for achieving certain goals related to the domain layer. Some of the goals may be to list the needs and requirements. After that, the domain expert should decide on the appropriate model validation metrics.</p>
<ol class="ltx_enumerate" id="Sx3.I2">
<li class="ltx_item" id="Sx3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx3.I2.i1.p1">
<p class="ltx_p" id="Sx3.I2.i1.p1.1">What are the specific requirements within the domain?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx3.I2.i2.p1">
<p class="ltx_p" id="Sx3.I2.i2.p1.1">Does the domain encompass high-stakes areas such as healthcare or finance?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx3.I2.i3.p1">
<p class="ltx_p" id="Sx3.I2.i3.p1.1">What are the potential risks associated with the domain?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx3.I2.i4.p1">
<p class="ltx_p" id="Sx3.I2.i4.p1.1">Are there any pre-existing assumptions that are necessary?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx3.I2.i5.p1">
<p class="ltx_p" id="Sx3.I2.i5.p1.1">Is feedback from domain experts a requirement for this process?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="Sx3.SSx2.p5">
<p class="ltx_p" id="Sx3.SSx2.p5.1">The data layer aims to explain the data by summarizing and analyzing the data and providing insights into the data.
This layer helps the user understand the data, the biases involved, how to mitigate the biases, the distribution, the limitations of the data, and what domain knowledge is contained in the data. This layer involves both the ML practitioner and the domain expert. The domain expert should list the limitations of the data, and both the ML practitioner and the domain expert should decide whether the data can be used for the specific application, The domain expert should provide the prior domain knowledge in the form of knowledge graphs to the ML practitioner, who will then incorporate this domain knowledge during training of the ML model. The data should be represented visually in the simplest way possible by the ML practitioner, so that the domain expert has a clear understanding of the data, which may help them to list the limitations or drawbacks of the data or the bias in the data, the ML practitioner will then mitigate the bias. One of the easy ways to detect and mitigate the bias in the data is to use the capabilities of IBM’s AI Fairness 360 (AIF360) library <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx3.SSx2.p5.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib5" title="">5</a></sup></cite>. This library can be used to remove the bias from both the data and the model. The data layer has the associated XAI questions.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p6">
<ol class="ltx_enumerate" id="Sx3.I3">
<li class="ltx_item" id="Sx3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx3.I3.i1.p1">
<p class="ltx_p" id="Sx3.I3.i1.p1.1">What type of information is contained within the data?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx3.I3.i2.p1">
<p class="ltx_p" id="Sx3.I3.i2.p1.1">What inferences can be drawn from this data?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx3.I3.i3.p1">
<p class="ltx_p" id="Sx3.I3.i3.p1.1">Which aspects of the data are the most significant?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx3.I3.i4.p1">
<p class="ltx_p" id="Sx3.I3.i4.p1.1">How is the information distributed within the data?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx3.I3.i5.p1">
<p class="ltx_p" id="Sx3.I3.i5.p1.1">Is it feasible to enhance the model’s performance by reducing the number of dimensions?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Sx3.I3.i6.p1">
<p class="ltx_p" id="Sx3.I3.i6.p1.1">Could the use of data summarization techniques provide a more effective explanation?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="Sx3.SSx2.p7">
<p class="ltx_p" id="Sx3.SSx2.p7.1">The model layer aims to explain the inner workings of the model, the parameters involved and their meaning, the interpretability at the model layer, and which model maintains the balance between performance and interpretability.
This layer involves an ML practitioner. One of the most common baselines to get started with model-layer explainability might be to answer whether the interpretable models can be used instead of black-box models <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx3.SSx2.p7.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib54" title="">54</a></sup></cite>. If the interpretable model can be used, then the hyper-parameter analysis should be done thoroughly to get the best result. There is often a trade-off between interpretability and performance. If performance is the most important goal at this layer, then post hoc methods can be used to achieve interpretability; otherwise, the interpretable models should undergo hyper-parameter tuning to get the best results without black box models.</p>
<ol class="ltx_enumerate" id="Sx3.I4">
<li class="ltx_item" id="Sx3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx3.I4.i1.p1">
<p class="ltx_p" id="Sx3.I4.i1.p1.1">What attributes render a parameter, objective, or action important to the system?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx3.I4.i2.p1">
<p class="ltx_p" id="Sx3.I4.i2.p1.1">At what point did the system assess a parameter, objective, or action, and when was it disregarded by the model?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx3.I4.i3.p1">
<p class="ltx_p" id="Sx3.I4.i3.p1.1">What are the repercussions of altering a decision or modifying a parameter?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx3.I4.i4.p1">
<p class="ltx_p" id="Sx3.I4.i4.p1.1">How was a specific action executed by the system?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx3.I4.i5.p1">
<p class="ltx_p" id="Sx3.I4.i5.p1.1">How are these model parameters, objectives, or actions interconnected?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I4.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Sx3.I4.i6.p1">
<p class="ltx_p" id="Sx3.I4.i6.p1.1">What elements does the system consider (or exclude) when making a decision?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I4.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="Sx3.I4.i7.p1">
<p class="ltx_p" id="Sx3.I4.i7.p1.1">What methods does the system employ or avoid to accomplish a goal or inference?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="Sx3.SSx2.p8">
<p class="ltx_p" id="Sx3.SSx2.p8.1">The prediction layer aims to explain the reason for a particular prediction, how certain inputs affect the prediction, whether the reason is sufficient for the conclusion or decision, what variables are involved behind the prediction, and how the prediction changes under certain considerations or criteria.
At this layer, the domain expert relies on the ML practitioner for results and a deeper understanding of the predictions. This layer would help answer the prediction-layer questions.</p>
<ol class="ltx_enumerate" id="Sx3.I5">
<li class="ltx_item" id="Sx3.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx3.I5.i1.p1">
<p class="ltx_p" id="Sx3.I5.i1.p1.1">What factors contribute to the importance of a parameter, objective, or action within the system?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx3.I5.i2.p1">
<p class="ltx_p" id="Sx3.I5.i2.p1.1">When was a parameter, objective, or action evaluated by the system, and when was it rejected by the model?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx3.I5.i3.p1">
<p class="ltx_p" id="Sx3.I5.i3.p1.1">What are the consequences of changing a decision or adjusting a parameter?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx3.I5.i4.p1">
<p class="ltx_p" id="Sx3.I5.i4.p1.1">How was a specific action carried out by the system?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I5.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx3.I5.i5.p1">
<p class="ltx_p" id="Sx3.I5.i5.p1.1">How are the parameters, objectives, or actions within the model interconnected?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I5.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Sx3.I5.i6.p1">
<p class="ltx_p" id="Sx3.I5.i6.p1.1">What elements does the system take into account (or disregard) when making a decision?</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I5.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="Sx3.I5.i7.p1">
<p class="ltx_p" id="Sx3.I5.i7.p1.1">What methods does the system utilize or avoid to achieve a specific goal or inference?</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx3">
<h3 class="ltx_title ltx_title_subsection">Addressing the question "How the nested model solve the problem?"</h3>
<div class="ltx_para" id="Sx3.SSx3.p1">
<ol class="ltx_enumerate" id="Sx3.I6">
<li class="ltx_item" id="Sx3.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx3.I6.i1.p1">
<p class="ltx_p" id="Sx3.I6.i1.p1.1">Once the regulations are followed or the key requirements have been introduced, only then can the next layer, i.e., the domain layer, be entered.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx3.I6.i2.p1">
<p class="ltx_p" id="Sx3.I6.i2.p1.1">The nested model prohibits entering further sub-layers until the goal of the previous layers has been achieved. For example, after accomplishing the specific goal of the domain layer, which is to define applications and requirements, the user can proceed to the data layer to accomplish certain goals within the scope of the data layer.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx3.I6.i3.p1">
<p class="ltx_p" id="Sx3.I6.i3.p1.1">The nested model is built on the foundation of the XAI-QB, which allows the user to answer specific questions that lead to a clear understanding of the rules, goals, needs, requirements, solutions, and conclusions.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I6.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx3.I6.i4.p1">
<p class="ltx_p" id="Sx3.I6.i4.p1.1">nested model conforms to regulatory guidelines. This leads to solving certain challenges that arise when using AI workflows. This enables AI to be compliant, trustworthy, accountable, non-discriminatory, appropriate for human equality, robust &amp; secure, and transparent AI that operates under human agency &amp; oversight.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I6.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx3.I6.i5.p1">
<p class="ltx_p" id="Sx3.I6.i5.p1.1">The needs of regulators are addressed in the form of prototypical questions. It uses the human-computer interaction approach to eliminate the threat posed by the lack of human agency and oversight.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I6.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Sx3.I6.i6.p1">
<p class="ltx_p" id="Sx3.I6.i6.p1.1">The nested model can be used as a guidance tool to support the need for specification work to create AI applications that meet the key requirements of regulatory authorities.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I6.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="Sx3.I6.i7.p1">
<p class="ltx_p" id="Sx3.I6.i7.p1.1">The nested model addresses "transparency and explainability" from both the regulatory and AI perspectives.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I6.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="Sx3.I6.i8.p1">
<p class="ltx_p" id="Sx3.I6.i8.p1.1">The nested model is a prescriptive guideline that bridges the gap between AI regulations and AI.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx4">
<h3 class="ltx_title ltx_title_subsection">Outcomes of the Nested Model for AI</h3>
<div class="ltx_para" id="Sx3.SSx4.p1">
<p class="ltx_p" id="Sx3.SSx4.p1.1">The outcomes of the Nested Model are as follows:</p>
<ul class="ltx_itemize" id="Sx3.I7">
<li class="ltx_item" id="Sx3.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I7.i1.p1">
<p class="ltx_p" id="Sx3.I7.i1.p1.1">It helps define a common baseline for the adoption of AI in real-life use cases.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I7.i2.p1">
<p class="ltx_p" id="Sx3.I7.i2.p1.1">It acts as a common intersection between regulatory authorities (and their regulations), XAI, and AI practitioners by bringing these divergent fields together.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I7.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I7.i3.p1">
<p class="ltx_p" id="Sx3.I7.i3.p1.1">Reduces the chances of AI implosion by addressing the issues and potential threats with each layer. Mitigates the problems of non-transparency, unfairness, and discrimination.</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I7.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I7.i4.p1">
<p class="ltx_p" id="Sx3.I7.i4.p1.1">Evaluates the AI workflow, not only through evaluation metrics but also through key requirements of the regulations.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Implementation</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">To implement the nested model for the design and validation of AI workflows for AI governance we need to define regulations into ethical and technical.
Modern regulations are a combination of both ethical and technical implementations. Ethical regulations and technical regulations in the context of AI can be distinguished based on their focus and objectives. Ethical regulations are often concerned with guiding the moral principles and values associated with AI development and deployment, while technical regulations focus on specific technical aspects and requirements to ensure the responsible and safe use of AI systems <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx4.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib33" title="">33</a></sup></cite>.</p>
<ul class="ltx_itemize" id="Sx4.I8">
<li class="ltx_item" id="Sx4.I8.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I8.i1.p1">
<p class="ltx_p" id="Sx4.I8.i1.p1.1">Ethical Requirements:
Focus: Ethical regulations are primarily concerned with promoting values, principles, and norms associated with responsible AI development and usage. This may include considerations of fairness, transparency, accountability, privacy, and the broader societal impact of AI technologies.
Objective: The primary goal is to ensure that AI systems align with ethical standards and do not cause harm or violate fundamental human rights. Ethical guidelines provide a framework for developers and organizations to make morally sound decisions throughout the AI life cycle. The ethical key requirements could be solved with methodologies listed by Al Alhamed <span class="ltx_text ltx_font_italic" id="Sx4.I8.i1.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx4.I8.i1.p1.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib2" title="">2</a></sup></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I8.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I8.i2.p1">
<p class="ltx_p" id="Sx4.I8.i2.p1.1">Technical Requirements:
Focus: Technical regulations, on the other hand, are more specific and detail-oriented. They focus on the technical aspects of AI systems, such as algorithms, data quality, safety measures, and other technical requirements.
Objective: The objective of technical regulations is to set standards and requirements that AI developers must follow to ensure the robustness, security, and reliability of AI systems. These regulations are often designed to prevent technical issues, biases, and potential risks associated with AI deployment.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="Sx4.p1.2">Using the above discussion as a reference, we define steps to implement the nested model, referencing questions at each layer as needed.
Steps to follow:</p>
<ol class="ltx_enumerate" id="Sx4.I9">
<li class="ltx_item" id="Sx4.I9.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx4.I9.i1.p1">
<p class="ltx_p" id="Sx4.I9.i1.p1.1">Define the regulations and key requirements at the layer of the regulation.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I9.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx4.I9.i2.p1">
<p class="ltx_p" id="Sx4.I9.i2.p1.1">Categorize the key requirements into ethical and technical requirements.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I9.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx4.I9.i3.p1">
<p class="ltx_p" id="Sx4.I9.i3.p1.1">At the layer of the regulation, address the ethical requirements first.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I9.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx4.I9.i4.p1">
<p class="ltx_p" id="Sx4.I9.i4.p1.1">After proceeding into the domain layer, the domain expert will list down the domain-specific requirements, which will act as a reference for AI practitioners.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I9.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx4.I9.i5.p1">
<p class="ltx_p" id="Sx4.I9.i5.p1.1">Map all the technical key requirements from the regulations to the sub-layer, namely, data, domain, and prediction (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx4.F2" title="Figure 2 ‣ Use Case: Europe Union Requirements for Trustworthy AI ‣ Implementation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I9.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Sx4.I9.i6.p1">
<p class="ltx_p" id="Sx4.I9.i6.p1.1">Address the specified technical key requirements at each sub-layer using appropriate methodologies.</p>
</div>
</li>
</ol>
</div>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Use Case: Europe Union Requirements for Trustworthy AI</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">Listing out the steps:</p>
<ol class="ltx_enumerate" id="Sx4.I10">
<li class="ltx_item" id="Sx4.I10.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx4.I10.i1.p1">
<p class="ltx_p" id="Sx4.I10.i1.p1.1">Define the regulations and key requirements at the layer of the regulation :
Based on fundamental rights and ethical principles, the Guidelines list seven key requirements that AI systems should meet to be trustworthy:</p>
<ul class="ltx_itemize" id="Sx4.I10.i1.I1">
<li class="ltx_item" id="Sx4.I10.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i1.I1.i1.p1">
<p class="ltx_p" id="Sx4.I10.i1.I1.i1.p1.1">Human agency and oversight</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i1.I1.i2.p1">
<p class="ltx_p" id="Sx4.I10.i1.I1.i2.p1.1">Technical robustness and safety</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i1.I1.i3.p1">
<p class="ltx_p" id="Sx4.I10.i1.I1.i3.p1.1">Privacy and data governance</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i1.I1.i4.p1">
<p class="ltx_p" id="Sx4.I10.i1.I1.i4.p1.1">Transparency</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i1.I1.i5.p1">
<p class="ltx_p" id="Sx4.I10.i1.I1.i5.p1.1">Diversity, non-discrimination, and fairness</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i1.I1.i6.p1">
<p class="ltx_p" id="Sx4.I10.i1.I1.i6.p1.1">Societal and environmental well-being</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i1.I1.i7.p1">
<p class="ltx_p" id="Sx4.I10.i1.I1.i7.p1.1">Accountability</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx4.I10.i2.p1">
<p class="ltx_p" id="Sx4.I10.i2.p1.1">Categorize the key requirements into ethical and technical requirements:
After the ethical and technical requirements are satisfied, they will result in accountability.</p>
<ul class="ltx_itemize" id="Sx4.I10.i2.I1">
<li class="ltx_item" id="Sx4.I10.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i2.I1.i1.p1">
<p class="ltx_p" id="Sx4.I10.i2.I1.i1.p1.1">Ethical requirements: Privacy and data governance, societal and environmental well-being, safety.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx4.I10.i2.I1.i2.p1">
<p class="ltx_p" id="Sx4.I10.i2.I1.i2.p1.1">Technical requirements: Human agency and oversight, technical robustness, transparency, diversity, non-discrimination, fairness</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx4.I10.i3.p1">
<p class="ltx_p" id="Sx4.I10.i3.p1.1">Address the ethical requirements.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx4.I10.i4.p1">
<p class="ltx_p" id="Sx4.I10.i4.p1.1">Domain experts list the domain-specific requirements.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx4.I10.i5.p1">
<p class="ltx_p" id="Sx4.I10.i5.p1.1">Map the technical requirements with the sub-layers namely Data, Model, and Prediction (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx4.F2" title="Figure 2 ‣ Use Case: Europe Union Requirements for Trustworthy AI ‣ Implementation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I10.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Sx4.I10.i6.p1">
<p class="ltx_p" id="Sx4.I10.i6.p1.1">Address the specific technical requirements at the specific sub-layer using appropriate methodologies. When all the key requirements (ethical and technical) are achieved, it results in accountability.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="Sx4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="134" id="Sx4.F2.g1" src="x2.png" width="322"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Mapping of technical requirements to the layers of the nested model for AI design and validation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Potential Threats</h3>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Regulation layer</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx1.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx1.p1.1">The regulation layer helps to ensure compliance with the AI workflow.
Regulatory threats need to be carefully considered (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx4.F3" title="Figure 3 ‣ Regulation layer ‣ Potential Threats ‣ Implementation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">3</span></a>).
As operational burdens and regulatory requirements increase, the AI infrastructure will need to also adapt; including data management, privacy, security, and transparency standards.
To ensure compliance and foster an environment conducive to evaluating, validating and ultimately adopting AI technologies, organizations must be prepared to address these issues collectively. The AI application must align with the expectations of both the AI regulatory body and the domain regulatory body for which the AI is being developed.</p>
</div>
<figure class="ltx_figure" id="Sx4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="125" id="Sx4.F3.g1" src="x3.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Potential threats at the regulation layer.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Domain layer</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx2.p1.1">The domain layer sets boundaries on processes that fall within the domain’s purview (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx4.F4" title="Figure 4 ‣ Domain layer ‣ Potential Threats ‣ Implementation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_figure" id="Sx4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="78" id="Sx4.F4.g1" src="x4.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Potential threats at the domain layer.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Data layer</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx3.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx3.p1.1">The data layer strives to explain the data by summarizing and analyzing the data and providing insights into the data (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx4.F5" title="Figure 5 ‣ Data layer ‣ Potential Threats ‣ Implementation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<figure class="ltx_figure" id="Sx4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="128" id="Sx4.F5.g1" src="x5.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Potential threats at the data layer.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx4">
<h4 class="ltx_title ltx_title_subsubsection">Model layer</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx4.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx4.p1.1">The model layer seeks to explain the inner workings of the model, the parameters involved and their meaning, the interpretability of the model, and whether the model maintains the balance between performance and interpretability (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx4.F6" title="Figure 6 ‣ Model layer ‣ Potential Threats ‣ Implementation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure class="ltx_figure" id="Sx4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="111" id="Sx4.F6.g1" src="x6.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Potential threats at the model layer.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx5">
<h4 class="ltx_title ltx_title_subsubsection">Prediction layer</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx5.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx5.p1.1">The prediction layer aims to explain the reason for a particular prediction, how certain inputs affect the prediction, whether the reason is sufficient for the conclusion or decision, what variables are involved behind the prediction, and how the prediction changes under certain considerations or criteria (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx4.F7" title="Figure 7 ‣ Prediction layer ‣ Potential Threats ‣ Implementation ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure class="ltx_figure" id="Sx4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="78" id="Sx4.F7.g1" src="x7.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Potential threats at the prediction layer.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Examples</h2>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">We now analyze several previous use cases in terms of our
model, to provide a concrete example.</p>
</div>
<section class="ltx_subsection" id="Sx5.SSx1">
<h3 class="ltx_title ltx_title_subsection">AI Model by Google LLC to detect retinopathy.</h3>
<div class="ltx_para" id="Sx5.SSx1.p1">
<p class="ltx_p" id="Sx5.SSx1.p1.1">Diabetic retinopathy is a disease of the retina caused by diabetes that leads to vision impairment or loss.
During the development of the workflow, the ethical and technical requirements were neither listed nor addressed.
Google’s deep learning model for the detection of diabetic retinopathy failed for several reasons. The model had been trained on high-quality, high-resolution eye scans, but in real-life clinics, the images captured by nurses differed in quality and lighting conditions, leading to a significant disparity between the training data and real-life data.
The model was approved by the Food and Drug Administration (FDA) of the United States (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx5.F8" title="Figure 8 ‣ AI Model by Google LLC to detect retinopathy. ‣ Examples ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
<div class="ltx_para" id="Sx5.SSx1.p2">
<p class="ltx_p" id="Sx5.SSx1.p2.1">While the FDA is essential to the regulation of AI in healthcare, additional specialized oversight mechanisms are needed to address the unique issues presented by AI technologies. This includes ensuring that AI applications are clear, safe, and efficient. Adaptive AI and machine learning (ML) technologies do not lend themselves well to the FDA’s traditional regulatory framework, as they can change over time in response to new data. This makes it difficult to ensure the effectiveness and safety of these dynamic systems. While there are still many unanswered questions, the FDA is currently addressing the regulatory issues raised by AI in healthcare. However, more comprehensive and transparent regulatory processes are required to handle the dynamic nature of AI as a technology <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.SSx1.p2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib16" title="">16</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib15" title="">15</a></sup></cite></p>
</div>
<div class="ltx_para" id="Sx5.SSx1.p3">
<p class="ltx_p" id="Sx5.SSx1.p3.1">Additionally, the model was not validated on real-life data, which could have been verified by domain experts.
This discrepancy between training data and real-life conditions, as well as the lack of validation of real-life data, contributed to the model’s failure to accurately detect diabetic retinopathy in a clinical setting <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.SSx1.p3.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib35" title="">35</a></sup></cite>.
The integration of user-centric explainable AI (XAI) approaches and collaboration with domain experts can enhance the evaluation of AI systems. These approaches prioritize understanding user needs, identifying explainability needs, fostering collaboration between domain experts and AI researchers, capturing domain knowledge, identifying model inaccuracies, addressing the explanations, addressing biases, developing adaptive explanations, and creating a comprehensive XAI evaluation framework <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.SSx1.p3.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib41" title="">41</a></sup></cite>. By involving domain experts in the design process, researchers can create more effective and understandable explanations, improve model accuracy and robustness, and address social biases in decision-making. XAI can also develop context-aware and adaptive explanations that match user mental models and expertise levels, ensuring relevance and understanding across different user groups.
By leveraging these approaches, researchers and practitioners can create more transparent and trustworthy AI systems that better serve their intended users, eventually improving adoption in the target knowledge domain <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.SSx1.p3.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib36" title="">36</a></sup></cite>.</p>
</div>
<figure class="ltx_figure" id="Sx5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="426" id="Sx5.F8.g1" src="x8.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Potential threats: Google AI model to detect diabetic retinopathy.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Zillow Group, Inc. House Price Forecasting Model</h3>
<div class="ltx_para" id="Sx5.SSx2.p1">
<p class="ltx_p" id="Sx5.SSx2.p1.1">Zillow suffered a significant loss of over $500 million in its home-flipping business due to the failure of its non-transparent proprietary forecasting algorithm <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.SSx2.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib49" title="">49</a></sup></cite>.
The loss was due to the algorithm’s inability to accurately predict home prices, resulting in overpayment for homes and financial volatility.
This raised concerns about the reliability of AI models in critical business decisions, prompting a reevaluation of the use of AI in high-stakes operations and highlighting the importance of considering potential limitations and risks.
The failure underscores the need for reliable and transparent AI algorithms and emphasizes thorough evaluation and risk assessment when integrating AI into decision-making processes.
The event serves as a cautionary tale for organizations relying on AI for critical business strategies, highlighting the potential consequences of inadequate algorithm performance and the need to maintain a critical perspective on AI’s capabilities and limitations (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx5.F9" title="Figure 9 ‣ Zillow Group, Inc. House Price Forecasting Model ‣ Examples ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<figure class="ltx_figure" id="Sx5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="Sx5.F9.g1" src="x9.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Potential threats: Zillow Group, Inc. ML model failed to forecast house prices.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx5.SSx3">
<h3 class="ltx_title ltx_title_subsection">The Feature Cloud Platform for Federated Learning </h3>
<div class="ltx_para" id="Sx5.SSx3.p1">
<p class="ltx_p" id="Sx5.SSx3.p1.1">This platform takes advantage of federated learning and is an impressive work by Holzinger <span class="ltx_text ltx_font_italic" id="Sx5.SSx3.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.SSx3.p1.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib24" title="">24</a></sup></cite> presents a comprehensive exploration of the integration of domain knowledge graphs into deep learning for improved interpretability and explainability using Graph Neural Networks (GNNs).
Federated Learning (FL) protects privacy by transmitting only model updates, reducing the risk of data breaches. Its decentralized approach enhances data security and compliance.
FL promotes collaboration between devices, improving generalization with diverse data sets.
In addition, it improves the learning algorithm by incorporating explanations and conceptual knowledge for better interpretability <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.SSx3.p1.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib27" title="">27</a></sup></cite>.
The authors focus on using a protein-protein interaction (PPI) network to enrich deep neural networks for classification, enabling the detection of disease sub-networks using explainable AI.
This work addresses the potential threats at each layer of the nested model and validates them accordingly (Refer figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#Sx5.F10" title="Figure 10 ‣ The Feature Cloud Platform for Federated Learning ‣ Examples ‣ A Nested Model for AI Design and Validation"><span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
<div class="ltx_para" id="Sx5.SSx3.p2">
<ol class="ltx_enumerate" id="Sx5.I11">
<li class="ltx_item" id="Sx5.I11.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx5.I11.i1.p1">
<p class="ltx_p" id="Sx5.I11.i1.p1.1">Regulations: GDPR and Europe Union Requirements for Trustworthy AI.</p>
<ul class="ltx_itemize" id="Sx5.I11.i1.I1">
<li class="ltx_item" id="Sx5.I11.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I11.i1.I1.i1.p1">
<p class="ltx_p" id="Sx5.I11.i1.I1.i1.p1.1">Ethical Requirements: Privacy and data governance,
societal and environmental well-being, safety.</p>
</div>
</li>
<li class="ltx_item" id="Sx5.I11.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I11.i1.I1.i2.p1">
<p class="ltx_p" id="Sx5.I11.i1.I1.i2.p1.1">Technical Requirements: Human agency and oversight, technical robustness, transparency, diversity, non-discrimination, fairness</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Sx5.I11.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx5.I11.i2.p1">
<p class="ltx_p" id="Sx5.I11.i2.p1.1">Addressing Ethical Requirements: Privacy, data governance, and safety are addressed through a federated learning approach. This use case accelerates biomedical research, which in turn benefits humanity and enables societal well-being.
In addition, by decentralizing the training process, federated learning has the potential to be more environmentally friendly, making it a promising approach for reducing the carbon footprint of machine learning model training <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx5.I11.i2.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib39" title="">39</a></sup></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Sx5.I11.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx5.I11.i3.p1">
<p class="ltx_p" id="Sx5.I11.i3.p1.1">Addressing the technical requirements: The framework included an expert-in-the-loop approach to develop AI workflows under the supervision of a domain expert, allowing for human agency and oversight. The domain expert evaluates data for biases related to non-discrimination and diversity.
Knowledge graphs are implemented to incorporate prior domain knowledge.
The potential threat at the data layer is validated. At the model layer, models are evaluated for fairness and technical robustness through continuous feedback from the domain expert.
Open-source models are preferred for training. Finally, the XAI is integrated to make the AI process transparent and trustworthy.</p>
</div>
<div class="ltx_para" id="Sx5.I11.i3.p2">
<p class="ltx_p" id="Sx5.I11.i3.p2.1">In addition, this work also answers the extended XAI-QB in a summarized fashion.
The questions answered are "What was not done and why?", "What problems were solved?", "What was difficult?", "What did we learn?", and "What are the limitations and future results?".</p>
</div>
</li>
</ol>
<p class="ltx_p" id="Sx5.SSx3.p2.1">After validating this work against our nested model, we concluded that this work presents an appropriate paper at the intersection of HCI and AI that meets the key requirements set by the EU Trustworthy AI Guidelines and GDPR.</p>
</div>
<figure class="ltx_figure" id="Sx5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="Sx5.F10.g1" src="x10.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Validation of The Feature Cloud Platform through the Nested Model for AI design and validation.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Discussion</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">We have discussed the potential threats and their validation methodologies, but we assume that the potential threats are a non-exhaustive list.
There can be many potential threats and validation methodologies that may exist at the given layer, and these potential threats and validation methodologies should be mapped to one of the layers of the nested model for AI design and validation.</p>
</div>
<div class="ltx_para" id="Sx6.p2">
<p class="ltx_p" id="Sx6.p2.1">Considering the need for the domain expert as an expert-in-the-loop, it is also possible that a single domain expert may not be able to list all the domain-specific requirements, model validation strategies, visualization techniques, etc., so we recommend multiple iterations of the nested model for AI design and validation.
In addition, the XAI-QB should be kept in mind and the specific sub-type of the topic can be extended according to the requirements.
For example, if a domain-specific data visualization is desired, then this question should be added to the XAI-QB in the data subtopic and then the appropriate methods should be used at the data level to address this question.
The advantage of this framework is that it allows users to evaluate the AI workflow in human terms in collaboration with XAI-QB, which is an algorithmic question bank.</p>
</div>
<div class="ltx_para" id="Sx6.p3">
<p class="ltx_p" id="Sx6.p3.1">In addition, there may be questions such as "Sometimes we don’t know the problem with the data until we start training".
One answer is that we are still at the data level because the issue with the data is still there in the training of the model.
After the training of the model, it reflects that there is a problem with the data, then technically you are using the ML model to analyze the data for more potential threats, this will act as a methodology for the identification of unknown or more potential threats.
Accordingly, a question should be added to the XAI-QB under the data subtopic that asks, "How do you identify unknown potential threats with the data that are not identifiable by normal bias identification methods?"
The possibility of unstated biases is an open question, and some may be implicit threats.</p>
</div>
<div class="ltx_para" id="Sx6.p4">
<p class="ltx_p" id="Sx6.p4.1">The process of navigating the nested model for AI enables validation.
We name this process as <span class="ltx_text ltx_font_italic" id="Sx6.p4.1.1">"eliminating the potential threats of one layer through the functionality of the next layer"</span>.
Successfully navigating through each layer of the nested model layer for AI means that one has successfully resolved the threats present at each layer.</p>
</div>
<div class="ltx_para" id="Sx6.p5">
<p class="ltx_p" id="Sx6.p5.1">Another important point is that the regulations don’t clearly differentiate between the terms used for ethical and technical key requirements.
For example, technical robustness could mean both robustness in terms of infrastructure and robustness in terms of AI or decision-making. So we assume that the requirements are specified from both technical and ethical perspectives.
Our nested model for AI design and validation is the only way to design the AI workflow and meet the listed key requirements according to the regulations.
Our model addresses the regulations through XAI-QB and HCI, allowing AI to be evaluated in human terms.</p>
</div>
<div class="ltx_para" id="Sx6.p6">
<p class="ltx_p" id="Sx6.p6.1">While we have not listed the full list of algorithms or methods to address every possible potential threat, we argue that the nested model for AI is sufficient to warrant a rethink at each step of the conceptualization, design, and validation steps.
The speed at which AI regulations will impact infrastructure, and the need to quickly comply with regulatory requirements, justifies the regulatory layer as an overarching gateway to validate AI systems and prepare them for the real world.
The threats to validity may vary depending on the choice of algorithms, and the potential threats may vary from use case to use case; our advice is to iterate the nested model for AI as often as necessary.</p>
</div>
<div class="ltx_para" id="Sx6.p7">
<p class="ltx_p" id="Sx6.p7.1">The nested model takes into account both the AI in production and the AI in development.
AI regulations are designed to address both the development and production or deployment phases of AI systems.
They aim to create a framework that promotes innovation while safeguarding against potential risks and ensuring ethical use of AI technology throughout its lifecycle.
One of the examples that aligns with this scenario is the EU AI Act.
It takes a comprehensive approach that covers the entire AI lifecycle.
The legislation establishes rules for AI developers, deployers, and end-users.
It also sets up governance structures at the national and international levels to oversee AI regulation.
Furthermore, it creates mechanisms for ongoing assessment and adaptation of regulations as AI technology evolves.
For instance, for AI in development, transparency is required in the AI model’s design and training process.
Additionally, the results and decision-making processes must be communicated effectively with the help of audience-centric XAI techniques.</p>
</div>
<div class="ltx_para" id="Sx6.p8">
<p class="ltx_p" id="Sx6.p8.1">There exist various regulations for the domains.
One such example is pubmed regulations for the biomedical domain.
Two seminal works focus on guidelines and best practices for health informatics and software development in healthcare, with a particular emphasis on artificial intelligence (AI) applications <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx6.p8.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib20" title="">20</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib21" title="">21</a></sup></cite>.
These articles highlight the importance of creating reproducible and reusable biomedical software, fostering technology transfer in health informatics, and leveraging AI’s potential in medical research and precision medicine.
Furthermore, the articles address the software life cycle in health informatics and the role of AI in supporting clinical decision-making, which can enhance patient monitoring, diagnostics, and prognostics.
They also discuss crucial aspects often considered in healthcare software regulations, such as ensuring software reliability, maintaining data privacy and security, validating AI algorithms for clinical use, and implementing proper documentation and version control.
It is crucial to acknowledge that the specific regulations pertaining to health informatics and AI in healthcare may vary by country and jurisdiction.
Therefore, it is essential for developers and healthcare providers to consult with the relevant regulatory bodies to ensure that they are aware of the most up-to-date and applicable regulations when implementing health informatics solutions.</p>
</div>
<div class="ltx_para" id="Sx6.p9">
<p class="ltx_p" id="Sx6.p9.1">One could argue that questioning is not sufficient. However, formulating and asking the right questions brings up potential problems and assumptions from regulation all the way to predictions of an AI system. If a certain question or questions cannot be answered truthfully, or without reasonable doubt, the question(s) shall remain open to show transparency and potential pitfalls along every subsequent layer of the nested model for AI design and validation. Naturally, certain questions require a hard limit, so do not proceed to subsequent layers. This validation is meant to resolve any downstream problems when developing AI systems.</p>
</div>
<div class="ltx_para" id="Sx6.p10">
<p class="ltx_p" id="Sx6.p10.1">The future work would focus on developing an audience-centric XAI that takes into account the background of the end-users.
The development of audience-centric XAI necessitates a comprehensive approach that prioritizes user needs and contexts.
Key requirements include the implementation of user-centric design principles to tailor explanations for diverse user groups, the provision of real-time and actionable insights, and the utilization of scenario-based methods to elicit specific user requirements.
The integration of human-computer interaction principles is of paramount importance for the creation of effective explanation interfaces.
Furthermore, the assurance of compliance with legal and regulatory standards is of critical significance.
By meeting these requirements, XAI systems can deliver customized, meaningful explanations that enhance user understanding and trust across various contexts, while also fulfilling legal obligations.
This approach aims to bridge the gap between complex AI systems and end-users, making AI decisions more transparent and interpretable for different audiences. <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx6.p10.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib30" title="">30</a></sup></cite>
It is not uncommon for there to be debate surrounding the provision of overly detailed explanations, which could potentially be exploited or manipulated by malicious actors.
For instance, counterfactual explanations may be employed to identify adversarial samples with greater ease.
Despite this, XAI systems remain susceptible to the biases inherent in the data and algorithms utilized, given that humans set the parameters for these systems <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="Sx6.p10.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16888v2#bib.bib41" title="">41</a></sup></cite>.
One potential solution is to involve multiple domain experts in lieu of a single expert.</p>
</div>
</section>
<section class="ltx_section" id="Sx7">
<h2 class="ltx_title ltx_title_section">Limitations of study</h2>
<div class="ltx_para" id="Sx7.p1">
<p class="ltx_p" id="Sx7.p1.1">The achievement of international harmonization in AI regulation represents a significant challenge, given the diverse regulatory approaches that exist across countries and regions.
Nevertheless, there are indications of progress, including a growing consensus on high-risk applications and various international initiatives working towards harmonization.
The key challenges that must be addressed include the definition of AI, the balancing of innovation with risk mitigation, and the navigation of international tensions.
On a more positive note, there is an increasing political will for regulation, signs of trans-Atlantic convergence, and the potential influence of EU regulations globally.
Potential solutions include the establishment of global governance institutions, the adaptation of best practices from other high-tech sectors, and the development of open-source AI software.
While full harmonization remains a challenging objective, there is a growing consensus towards more coordinated approaches to AI regulation through continued dialogue, cooperation, and flexible frameworks adaptable to this rapidly evolving technology.
Our nested model is designed to facilitate international harmonization for AI regulations.
The essential requirements identified by regulatory bodies in different countries may vary, but the underlying fundamental principle to satisfy these requirements remains consistent.</p>
</div>
</section>
<section class="ltx_section" id="Sx8">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>
<div class="ltx_para" id="Sx8.p1">
<p class="ltx_p" id="Sx8.p1.1">In conclusion, our work is the first to bring together the disparate fields of AI and regulation.
Our work establishes a common baseline for designing and validating AI under the umbrella of regulation, taking into account the complexity of the domain and the need for domain experts.
Through this work, we are addressing discussions around the need to work at the intersection of AI and regulation to increase adoption in high-stakes domains such as healthcare.
Adopting the nested model for AI design and validation will help realize the full potential of today’s state-of-the-art AI techniques in a complex, vast, but interconnected, and globalized world.</p>
</div>
</section>
<section class="ltx_section" id="Sx9">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2>
<div class="ltx_para" id="Sx9.p1">
<p class="ltx_p" id="Sx9.p1.1">Conceptualization, A.D., G.H.; Methodology. A.D., G.H.; Writing – Original Draft, A.D., Z.Y., and G.H.; Writing – Review &amp; Editing, A.D., Z.Y., and G.H.; Supervision, Z.Y. and G.H., Project Administration, G.H.</p>
</div>
</section>
<section class="ltx_section" id="Sx10">
<h2 class="ltx_title ltx_title_section">Declaration of Interests</h2>
<div class="ltx_para" id="Sx10.p1">
<p class="ltx_p" id="Sx10.p1.1">The authors declare no competing interests.</p>
</div>
<div class="ltx_para" id="Sx10.p2">
<span class="ltx_ERROR undefined" id="Sx10.p2.1">\printcredits</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx11">
<h2 class="ltx_title ltx_title_section">STAR<math alttext="\star" class="ltx_Math" display="inline" id="Sx11.1.m1.1"><semantics id="Sx11.1.m1.1b"><mo id="Sx11.1.m1.1.1" xref="Sx11.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="Sx11.1.m1.1c"><ci id="Sx11.1.m1.1.1.cmml" xref="Sx11.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx11.1.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="Sx11.1.m1.1e">⋆</annotation></semantics></math> METHODS</h2>
</section>
<section class="ltx_section" id="Sx12">
<h2 class="ltx_title ltx_title_section">Resource availability</h2>
<section class="ltx_subsection" id="Sx12.SSx1">
<h3 class="ltx_title ltx_title_subsection">Lead contact</h3>
<div class="ltx_para" id="Sx12.SSx1.p1">
<p class="ltx_p" id="Sx12.SSx1.p1.1">Further information and requests should be directed to and will be fulfilled by the lead contact, Akshat Dubey (DubeyA@rki.de)</p>
</div>
</section>
<section class="ltx_subsection" id="Sx12.SSx2">
<h3 class="ltx_title ltx_title_subsection">Materials availability</h3>
<div class="ltx_para" id="Sx12.SSx2.p1">
<p class="ltx_p" id="Sx12.SSx2.p1.1">Given the nature of the research, no datasets or codes are used or produced.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx12.SSx3">
<h3 class="ltx_title ltx_title_subsection">Data and code availability</h3>
<div class="ltx_para" id="Sx12.SSx3.p1">
<p class="ltx_p" id="Sx12.SSx3.p1.1">No data and code are involved in the research conducted.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx13">
<h2 class="ltx_title ltx_title_section">Method details</h2>
<div class="ltx_para" id="Sx13.p1">
<p class="ltx_p" id="Sx13.p1.1">The research is focused on the evolving landscape of AI regulations. Although artificial intelligence (AI) has experienced significant growth in recent years, it has yet to achieve its full potential in real-world applications. This research is divided into four sections. The initial section examines the advantages of AI and elucidates the reasons behind the relatively slow rate of adoption of AI in critical and high-stakes domains. The advent of AI has brought with it a number of significant ethical and legal concerns. Consequently, there is a pressing need to address not only the regulatory policies that will facilitate the implementation of AI in real-world use cases but also how practitioners design and validate AI applications and workflows. The second part of the research is concerned with the potential of an audience-centric approach to AI, in conjunction with explainable artificial intelligence, to address significant concerns with AI regulations and satisfy the guidelines set out by regulatory authorities. The third section of the research presents the XAI-QB, which is a question bank comprising algorithm-informed questions. These questions serve to assess and validate the AI, thereby fostering trust, faithfulness, and transparency in AI through the application of design principles. In the fourth part, we have developed a nested model for AI design and validation on top of XAI-QB. The nested model contains five distinct layers: regulations, domain, data, model, and predictions. At each level, specific prototypical questions must be addressed, with due consideration given to the guidelines set forth by regulatory authorities. Additionally, illustrative examples have been provided.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Act 2021</span>
<span class="ltx_bibblock">
Act, A.I., 2021.

</span>
<span class="ltx_bibblock">Proposal for a regulation of the european parliament and the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts.

</span>
<span class="ltx_bibblock">EUR-Lex-52021PC0206 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al Alahmed et al. 2023</span>
<span class="ltx_bibblock">
Al Alahmed, Y., Abadla, R., Ameen, N., Shteiwi, A., 2023.

</span>
<span class="ltx_bibblock">Bridging the gap between ethical ai implementations.

</span>
<span class="ltx_bibblock">International Journal of Membrane Science and Technology 10, 3034–3046.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.15379/ijmst.v10i3.2953" title="">10.15379/ijmst.v10i3.2953</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Almeida et al. 2021</span>
<span class="ltx_bibblock">
de Almeida, P.G.R., dos Santos, C.D., Farias, J.S., 2021.

</span>
<span class="ltx_bibblock">Artificial intelligence regulation: a framework for governance.

</span>
<span class="ltx_bibblock">Ethics and Information Technology 23, 505–525.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bedué and Fritzsche 2022</span>
<span class="ltx_bibblock">
Bedué, P., Fritzsche, A., 2022.

</span>
<span class="ltx_bibblock">Can we trust ai? an empirical investigation of trust requirements and guide to successful ai adoption.

</span>
<span class="ltx_bibblock">Journal of Enterprise Information Management 35, 530–549.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellamy et al. 2018</span>
<span class="ltx_bibblock">
Bellamy, R.K.E., Dey, K., Hind, M., Hoffman, S.C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., Nagar, S., Ramamurthy, K.N., Richards, J., Saha, D., Sattigeri, P., Singh, M., Varshney, K.R., Zhang, Y., 2018.

</span>
<span class="ltx_bibblock">AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1810.01943" title="">https://arxiv.org/abs/1810.01943</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellotti and Edwards 2001</span>
<span class="ltx_bibblock">
Bellotti, V., Edwards, K., 2001.

</span>
<span class="ltx_bibblock">Intelligibility and accountability: human considerations in context-aware systems.

</span>
<span class="ltx_bibblock">Human–Computer Interaction 16, 193–212.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilgic and Mooney 2005</span>
<span class="ltx_bibblock">
Bilgic, M., Mooney, R.J., 2005.

</span>
<span class="ltx_bibblock">Explaining recommendations: Satisfaction vs. promotion, in: Beyond personalization workshop, IUI, p. 153.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black and Murray 2019</span>
<span class="ltx_bibblock">
Black, J., Murray, A.D., 2019.

</span>
<span class="ltx_bibblock">Regulating ai and machine learning: setting the regulatory agenda.

</span>
<span class="ltx_bibblock">European journal of law and technology 10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Broniatowski et al. 2021</span>
<span class="ltx_bibblock">
Broniatowski, D.A., et al., 2021.

</span>
<span class="ltx_bibblock">Psychological foundations of explainability and interpretability in artificial intelligence.

</span>
<span class="ltx_bibblock">NIST, Tech. Rep .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buiten 2019</span>
<span class="ltx_bibblock">
Buiten, M.C., 2019.

</span>
<span class="ltx_bibblock">Towards intelligent regulation of artificial intelligence.

</span>
<span class="ltx_bibblock">European Journal of Risk Regulation 10, 41–59.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakraborty et al. 2021</span>
<span class="ltx_bibblock">
Chakraborty, J., Majumder, S., Menzies, T., 2021.

</span>
<span class="ltx_bibblock">Bias in machine learning software: Why? how? what to do?, in: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 429–440.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clarke 2019</span>
<span class="ltx_bibblock">
Clarke, R., 2019.

</span>
<span class="ltx_bibblock">Regulatory alternatives for ai.

</span>
<span class="ltx_bibblock">Computer Law &amp; Security Review 35, 398–409.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corso et al. 2024</span>
<span class="ltx_bibblock">
Corso, G., Stark, H., Jegelka, S., Jaakkola, T., Barzilay, R., 2024.

</span>
<span class="ltx_bibblock">Graph neural networks.

</span>
<span class="ltx_bibblock">Nature Reviews Methods Primers 4, 17.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erdélyi and Goldsmith 2018</span>
<span class="ltx_bibblock">
Erdélyi, O.J., Goldsmith, J., 2018.

</span>
<span class="ltx_bibblock">Regulating artificial intelligence: Proposal for a global solution, in: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 95–101.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FedSoc 2023</span>
<span class="ltx_bibblock">
FedSoc, 2023.

</span>
<span class="ltx_bibblock">The problem with ai licensing and an “fda for algorithms”.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://fedsoc.org/commentary/fedsoc-blog/the-problem-with-ai-licensing-an-fda-for-algorithms" title="">https://fedsoc.org/commentary/fedsoc-blog/the-problem-with-ai-licensing-an-fda-for-algorithms</a> [Accessed: 08.07.2024 (DD.MM.YYYY)].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Food and  FDA</span>
<span class="ltx_bibblock">
Food, U., (FDA), D.A., 2024.

</span>
<span class="ltx_bibblock">Artificial intelligence and machine learning in software as a medical device.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device" title="">https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device</a> [Accessed: 08.07.2024 (DD.MM.YYYY)].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gasser and Almeida 2017</span>
<span class="ltx_bibblock">
Gasser, U., Almeida, V.A., 2017.

</span>
<span class="ltx_bibblock">A layered model for ai governance.

</span>
<span class="ltx_bibblock">IEEE Internet Computing 21, 58–62.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunning and Aha 2019</span>
<span class="ltx_bibblock">
Gunning, D., Aha, D., 2019.

</span>
<span class="ltx_bibblock">Darpa’s explainable artificial intelligence (xai) program.

</span>
<span class="ltx_bibblock">AI magazine 40, 44–58.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harvey and Gowda 2021</span>
<span class="ltx_bibblock">
Harvey, H.B., Gowda, V., 2021.

</span>
<span class="ltx_bibblock">Regulatory issues and challenges to artificial intelligence adoption.

</span>
<span class="ltx_bibblock">Radiologic Clinics 59, 1075–1083.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hauschild et al. 2021</span>
<span class="ltx_bibblock">
Hauschild, A.C., Eick, L., Wienbeck, J., Heider, D., 2021.

</span>
<span class="ltx_bibblock">Fostering reproducibility, reusability, and technology transfer in health informatics.

</span>
<span class="ltx_bibblock">Iscience 24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hauschild et al. 2022</span>
<span class="ltx_bibblock">
Hauschild, A.C., Martin, R., Holst, S.C., Wienbeck, J., Heider, D., 2022.

</span>
<span class="ltx_bibblock">Guideline for software life cycle in health informatics.

</span>
<span class="ltx_bibblock">Iscience 25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heer 2018</span>
<span class="ltx_bibblock">
Heer, J., 2018.

</span>
<span class="ltx_bibblock">The partnership on ai.

</span>
<span class="ltx_bibblock">AI Matters 4, 25–26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann-Riem 2020</span>
<span class="ltx_bibblock">
Hoffmann-Riem, W., 2020.

</span>
<span class="ltx_bibblock">Artificial intelligence as a challenge for law and regulation.

</span>
<span class="ltx_bibblock">Regulating artificial intelligence , 1–29.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holzinger et al. 2023</span>
<span class="ltx_bibblock">
Holzinger, A., Saranti, A., Hauschild, A.C., Beinecke, J., Heider, D., Roettger, R., Mueller, H., Baumbach, J., Pfeifer, B., 2023.

</span>
<span class="ltx_bibblock">Human-in-the-loop integration with domain-knowledge graphs for explainable federated deep learning, in: International Cross-Domain Conference for Machine Learning and Knowledge Extraction, Springer. pp. 45–64.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Larson et al. 2021</span>
<span class="ltx_bibblock">
Larson, D.B., Harvey, H., Rubin, D.L., Irani, N., Justin, R.T., Langlotz, C.P., 2021.

</span>
<span class="ltx_bibblock">Regulatory frameworks for development and evaluation of artificial intelligence–based diagnostic imaging algorithms: summary and recommendations.

</span>
<span class="ltx_bibblock">Journal of the American College of Radiology 18, 413–424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lennerz et al. 2022</span>
<span class="ltx_bibblock">
Lennerz, J.K., Green, U., Williamson, D.F., Mahmood, F., 2022.

</span>
<span class="ltx_bibblock">A unifying force for the realization of medical ai.

</span>
<span class="ltx_bibblock">NPJ Digital Medicine 5, 172.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. 2020</span>
<span class="ltx_bibblock">
Li, L., Fan, Y., Tse, M., Lin, K.Y., 2020.

</span>
<span class="ltx_bibblock">A review of applications in federated learning.

</span>
<span class="ltx_bibblock">Computers &amp; Industrial Engineering 149, 106854.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. 2020</span>
<span class="ltx_bibblock">
Liao, Q.V., Gruen, D., Miller, S., 2020.

</span>
<span class="ltx_bibblock">Questioning the ai: Informing design practices for explainable ai user experiences, in: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, Association for Computing Machinery, New York, NY, USA. p. 1–15.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3313831.3376590" title="">https://doi.org/10.1145/3313831.3376590</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.1145/3313831.3376590" title="">10.1145/3313831.3376590</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lilkov 2021</span>
<span class="ltx_bibblock">
Lilkov, D., 2021.

</span>
<span class="ltx_bibblock">Regulating artificial intelligence in the eu: A risky game.

</span>
<span class="ltx_bibblock">European View 20, 166–174.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maxwell and Dumas 2023</span>
<span class="ltx_bibblock">
Maxwell, W., Dumas, B., 2023.

</span>
<span class="ltx_bibblock">Meaningful xai based on user-centric design methodology: Combining legal and human-computer interaction (hci) approaches to achieve meaningful algorithmic explainability.

</span>
<span class="ltx_bibblock">Available at SSRN 4520754 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Misra et al. 2020</span>
<span class="ltx_bibblock">
Misra, S.K., Das, S., Gupta, S., Sharma, S.K., 2020.

</span>
<span class="ltx_bibblock">Public policy and regulatory challenges of artificial intelligence (ai), in: Re-imagining Diffusion and Adoption of Information Technology and Systems: A Continuing Conversation: IFIP WG 8.6 International Conference on Transfer and Diffusion of IT, TDIT 2020, Tiruchirappalli, India, December 18–19, 2020, Proceedings, Part I, Springer. pp. 100–111.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohseni et al. 2021</span>
<span class="ltx_bibblock">
Mohseni, S., Zarei, N., Ragan, E.D., 2021.

</span>
<span class="ltx_bibblock">A multidisciplinary survey and framework for design and evaluation of explainable ai systems.

</span>
<span class="ltx_bibblock">ACM Transactions on Interactive Intelligent Systems (TiiS) 11, 1–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller 2023</span>
<span class="ltx_bibblock">
Müller, V.C., 2023.

</span>
<span class="ltx_bibblock">Ethics of Artificial Intelligence and Robotics, in: Zalta, E.N., Nodelman, U. (Eds.), The Stanford Encyclopedia of Philosophy. Fall 2023 ed.. Metaphysics Research Lab, Stanford University.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nannini et al. 2023</span>
<span class="ltx_bibblock">
Nannini, L., Balayn, A., Smith, A.L., 2023.

</span>
<span class="ltx_bibblock">Explainability in ai policies: A critical review of communications, reports, regulations, and standards in the eu, us, and uk, in: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pp. 1198–1212.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">page 2020</span>
<span class="ltx_bibblock">
page, W.D.H., 2020.

</span>
<span class="ltx_bibblock">Google’s medical ai was super accurate in a lab. real life was a different story. .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parikh et al. 2019</span>
<span class="ltx_bibblock">
Parikh, R.B., Teeple, S., Navathe, A.S., 2019.

</span>
<span class="ltx_bibblock">Addressing bias in artificial intelligence in health care.

</span>
<span class="ltx_bibblock">Jama 322, 2377–2378.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park 2018</span>
<span class="ltx_bibblock">
Park, S.H., 2018.

</span>
<span class="ltx_bibblock">Regulatory approval versus clinical validation of artificial intelligence diagnostic tools.

</span>
<span class="ltx_bibblock">Radiology 288, 910–911.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phillips et al. 2020</span>
<span class="ltx_bibblock">
Phillips, P.J., Hahn, C.A., Fontana, P.C., Broniatowski, D.A., Przybocki, M.A., 2020.

</span>
<span class="ltx_bibblock">Four principles of explainable artificial intelligence.

</span>
<span class="ltx_bibblock">Gaithersburg, Maryland 18.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. 2023</span>
<span class="ltx_bibblock">
Qiu, X., Parcollet, T., Fernandez-Marques, J., Gusmao, P.P., Gao, Y., Beutel, D.J., Topal, T., Mathur, A., Lane, N.D., 2023.

</span>
<span class="ltx_bibblock">A first look into the carbon footprint of federated learning.

</span>
<span class="ltx_bibblock">Journal of Machine Learning Research 24, 1–23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Regulation 2016</span>
<span class="ltx_bibblock">
Regulation, P., 2016.

</span>
<span class="ltx_bibblock">Regulation (eu) 2016/679 of the european parliament and of the council.

</span>
<span class="ltx_bibblock">Regulation (eu) 679, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roselli et al. 2019</span>
<span class="ltx_bibblock">
Roselli, D., Matthews, J., Talagala, N., 2019.

</span>
<span class="ltx_bibblock">Managing bias in ai, in: Companion proceedings of the 2019 world wide web conference, pp. 539–544.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salgado-Criado and Fernández-Aller 2021</span>
<span class="ltx_bibblock">
Salgado-Criado, J., Fernández-Aller, C., 2021.

</span>
<span class="ltx_bibblock">A wide human-rights approach to artificial intelligence regulation in europe.

</span>
<span class="ltx_bibblock">IEEE Technology and Society Magazine 40, 55–65.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scherer 2015</span>
<span class="ltx_bibblock">
Scherer, M.U., 2015.

</span>
<span class="ltx_bibblock">Regulating artificial intelligence systems: Risks, challenges, competencies, and strategies.

</span>
<span class="ltx_bibblock">Harv. JL &amp; Tech. 29, 353.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shneiderman 2020</span>
<span class="ltx_bibblock">
Shneiderman, B., 2020.

</span>
<span class="ltx_bibblock">Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered ai systems.

</span>
<span class="ltx_bibblock">ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 1–31.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sipos et al. 2023</span>
<span class="ltx_bibblock">
Sipos, L., Schäfer, U., Glinka, K., Müller-Birn, C., 2023.

</span>
<span class="ltx_bibblock">Identifying explanation needs of end-users: Applying and extending the xai question bank, in: Proceedings of Mensch und Computer 2023, pp. 492–497.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smuha 2019a</span>
<span class="ltx_bibblock">
Smuha, N.A., 2019a.

</span>
<span class="ltx_bibblock">The eu approach to ethics guidelines for trustworthy artificial intelligence.

</span>
<span class="ltx_bibblock">Computer Law Review International 20, 97–106.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smuha 2019b</span>
<span class="ltx_bibblock">
Smuha, N.A., 2019b.

</span>
<span class="ltx_bibblock">From a ’race to ai’ to a ’race to ai regulation’ - regulatory competition for artificial intelligence.

</span>
<span class="ltx_bibblock">IO: Regulation URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:213222579" title="">https://api.semanticscholar.org/CorpusID:213222579</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stuurman and Lachaud 2022</span>
<span class="ltx_bibblock">
Stuurman, K., Lachaud, E., 2022.

</span>
<span class="ltx_bibblock">Regulating ai. a label to complete the proposed act on artificial intelligence.

</span>
<span class="ltx_bibblock">Computer Law &amp; Security Review 44, 105657.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Troncoso et al. 2023</span>
<span class="ltx_bibblock">
Troncoso, I., Fu, R., Malik, N., Proserpio, D., 2023.

</span>
<span class="ltx_bibblock">Algorithm failures and consumers’ response: Evidence from zillow.

</span>
<span class="ltx_bibblock">Available at SSRN 4520172 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. 2022</span>
<span class="ltx_bibblock">
Wang, Q., Huang, K., Chandak, P., Zitnik, M., Gehlenborg, N., 2022.

</span>
<span class="ltx_bibblock">Extending the nested model for user-centric xai: A design study on gnn-based drug repurposing.

</span>
<span class="ltx_bibblock">IEEE Transactions on Visualization and Computer Graphics 29, 1266–1276.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weller 2019</span>
<span class="ltx_bibblock">
Weller, A., 2019.

</span>
<span class="ltx_bibblock">Transparency: motivations and challenges, in: Explainable AI: interpreting, explaining and visualizing deep learning. Springer, pp. 23–40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White and Lidskog 2022</span>
<span class="ltx_bibblock">
White, J.M., Lidskog, R., 2022.

</span>
<span class="ltx_bibblock">Ignorance and the regulation of artificial intelligence.

</span>
<span class="ltx_bibblock">Journal of Risk Research 25, 488–500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong 2021</span>
<span class="ltx_bibblock">
Wong, A., 2021.

</span>
<span class="ltx_bibblock">Ethics and regulation of artificial intelligence, in: Artificial Intelligence for Knowledge Management: 8th IFIP WG 12.6 International Workshop, AI4KM 2021, Held at IJCAI 2020, Yokohama, Japan, January 7–8, 2021, Revised Selected Papers 8, Springer. pp. 1–18.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. 2024</span>
<span class="ltx_bibblock">
Yang, Z., Dai, X., Dubey, A., Hirche, S., Hattab, G., 2024.

</span>
<span class="ltx_bibblock">Whom to trust? elective learning for distributed gaussian process regression, in: Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC. p. 2020–2028.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo 2013</span>
<span class="ltx_bibblock">
Yoo, C.S., 2013.

</span>
<span class="ltx_bibblock">Protocol layering and internet policy.

</span>
<span class="ltx_bibblock">University of Pennsylvania Law Review , 1707–1771.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug  1 11:46:36 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
