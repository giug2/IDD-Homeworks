<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages</title>
<!--Generated on Fri Sep  6 17:13:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.04512v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S1" title="In Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S2" title="In Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3" title="In Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.SS1" title="In 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Chain of Translation Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.SS2" title="In 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Datasets Used</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.SS3" title="In 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Methodology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S4" title="In Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S4.SS1" title="In 4 Results and Discussion ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Classification Task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S4.SS2" title="In 4 Results and Discussion ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Generation Task</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S5" title="In Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Future Work and Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_font_bold ltx_title_document">Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tejas Deshpande<sup class="ltx_sup" id="id1.1.id1">1, *</sup>, Nidhi Kowtal<sup class="ltx_sup" id="id2.2.id2">1, *</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id3.3.id3">Raviraj Joshi<sup class="ltx_sup" id="id3.3.id3.1"><span class="ltx_text ltx_font_medium" id="id3.3.id3.1.1">2,3</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id4.4.id4">1</sup> Pune Institute of Computer Technology, Pune, Maharashtra India 
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.id5">2</sup> Indian Institute of Technology Madras, Chennai, Tamil Nadu India
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.id6">3</sup> L3Cube Labs, Pune
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id7.7.id7">{tejasdeshpande1112, kowtalnidhi}@gmail.com</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.8.id8">ravirajoshi@gmail.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">This paper introduces Chain of Translation Prompting (CoTR), a novel strategy designed to enhance the performance of language models in low-resource languages. CoTR restructures prompts to first translate the input context from a low-resource language into a higher-resource language, such as English. The specified task like generation, classification, or any other NLP function is then performed on the translated text, with the option to translate the output back to the original language if needed. All these steps are specified in a single prompt. We demonstrate the effectiveness of this method through a case study on the low-resource Indic language Marathi. The CoTR strategy is applied to various tasks, including sentiment analysis, hate speech classification, subject classification and text generation, and its efficacy is showcased by comparing it with regular prompting methods. Our results underscore the potential of translation-based prompting strategies to significantly improve multilingual LLM performance in low-resource languages, offering valuable insights for future research and applications. We specifically see the highest accuracy improvements with the hate speech detection task. The technique also has the potential to enhance the quality of synthetic data generation for underrepresented languages using LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Tejas Deshpande<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1, *</sup>, Nidhi Kowtal<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">1, *</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">Raviraj Joshi<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.2.1.1.1.1">2,3</span></sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">1</sup> Pune Institute of Computer Technology, Pune, Maharashtra India</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.1">2</sup> Indian Institute of Technology Madras, Chennai, Tamil Nadu India</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1"><sup class="ltx_sup" id="p1.1.2.1.1.5.5.1.1">3</sup> L3Cube Labs, Pune</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.6.6">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.6.6.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.6.6.1.1">{tejasdeshpande1112, kowtalnidhi}@gmail.com</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.7.7">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.7.7.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.7.7.1.1">ravirajoshi@gmail.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Natural Language Processing (NLP) has made significant progress in recent years, with models capable of understanding, creating, and translating human language across a wide range of tasks and languages. However since high-resource languages like English, Spanish, and Chinese have access to a wealth of annotated datasets and linguistic resources, most of this development has been focused on those languages. Low-resource languages, on the other hand, have a lot more difficulties since they lack large-scale and high-quality datasets <cite class="ltx_cite ltx_citemacro_cite">Thabah and Purkayastha (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib46" title="">2021</a>)</cite>. Training effective NLP models are challenging due to this data scarcity, which frequently leads to subpar performance and poor generalization. Low-resource languages have distinct grammatical structures, linguistic diversity, and cultural quirks that make it more difficult to create accurate models and limit their use in practical contexts <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib50" title="">2023</a>)</cite>. Multilingual LLMs have limitations on processing the prompts in low-resource languages <cite class="ltx_cite ltx_citemacro_cite">Sanjib Narzary (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib43" title="">2022</a>)</cite>. This is because the amount of data used to train or fine-tune the model is very less. As a result, speakers of low-resource languages are frequently excluded from the benefits of advanced NLP technologies, highlighting the crucial need for novel techniques to close this gap.

<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="300" id="S1.F1.g1" src="extracted/5838669/short_prompt.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A brief overview of the Chain of Translation Prompting (CoTR) for an annotation task. The technique modifies the input prompt to encapsulate the translation of the non-English input context to English, followed by performing the target task on the translated text.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, Multilingual LLMs are good at translation tasks, as it is common practice to include parallel corpora during the pre-training stage <cite class="ltx_cite ltx_citemacro_cite">Xiang Zhang (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib49" title="">2023</a>)</cite>. We can leverage the ability of multilingual LLMs to improve responses for low-resource languages. In our study, we apply this approach to Marathi, an Indo-Aryan language spoken by about 83 million people, primarily in the Indian state of Maharashtra. Marathi is one of these low-resource languages <cite class="ltx_cite ltx_citemacro_cite">Joshi (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib19" title="">2022b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib18" title="">a</a>)</cite>.
Despite its large speaker base, Marathi lacks digital resources, annotated corpora, and computational tools. The language’s complex syntax challenges the development of precise NLP models, and limited Marathi-specific datasets and pre-trained models hinder the adoption of language technologies <cite class="ltx_cite ltx_citemacro_cite">Luong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib29" title="">2023</a>)</cite>. Therefore, new approaches are needed to enhance Marathi NLP performance and enable its speakers to benefit from AI advancements.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we investigate new prompting strategies to enhance Marathi language processing capabilities in models such as GPT-4o, GPT-4o Mini, Llama3-8B, Llama3-405B, and Gemma-9B. Our research introduces a novel strategy called "Chain of Translation Prompting (CoTR)", which we evaluate against direct Marathi prompting. We apply this method to sentiment analysis, hate speech classification, and subject categorization across three datasets: MahaSent <cite class="ltx_cite ltx_citemacro_cite">Pingle et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib41" title="">2023</a>); Kulkarni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib22" title="">2021</a>)</cite>, MahaHate <cite class="ltx_cite ltx_citemacro_cite">Patil et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib38" title="">2022</a>)</cite>, and MahaNews-SHC <cite class="ltx_cite ltx_citemacro_cite">Mittal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib33" title="">2023</a>); Aishwarya et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib3" title="">2023</a>)</cite> respectively. Additionally, we assess its effectiveness in generating headlines using the CSEBUETNLP XLSum dataset. Our findings reveal that translating Marathi input into English and then performing classification or text generation using a single prompt yields superior results compared to directly processing the Marathi text with a standard prompt. This work significantly contributes to multilingual NLP by demonstrating the potential of translation-based prompting strategies, particularly with a single prompt, to enhance NLP performance in low-resource languages.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The main contributions of this work are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce Chain of Translation Prompting (CoTR) as a method for performing input context translation during LLM response generation. Our results demonstrate that CoTR consistently outperforms standard prompting strategies across a variety of models and datasets.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We benchmark various open and closed LLMs, including GPT-4o, GPT-4o mini, Llama 3.1 405B, Llama 3.1 8B, and Gemma 2 9B, on tasks such as Marathi Sentiment Analysis, Hate Speech Detection, News Categorization, and News Headline Generation. In terms of performance, closed LLMs consistently rank higher: GPT-4o &gt; GPT-4o mini &gt; Llama 3.1 405B &gt; Gemma 2 9B &gt; Llama 3.1 8B. We observe that CoTR is particularly beneficial for smaller models with higher error rates.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The CoTR prompting strategy shows the most significant improvements in complex tasks like hate speech detection and sentiment analysis.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="518" id="S1.F2.g1" src="extracted/5838669/prompt.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Prompt for Classification Task</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="328" id="S1.F3.g1" src="extracted/5838669/generation_prompt.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prompt for Generation Task</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Natural language processing has improved significantly with the creation of sophisticated models like GPT-4, Llama3, and others. Nonetheless, insufficient representation and scarce data availability in pre-trained models continue to pose problems for low-resource languages<cite class="ltx_cite ltx_citemacro_cite">Panteleimon Krasadakis (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib36" title="">2024</a>)</cite>. Language diversity and data scarcity in low-resource contexts have shown to be challenges for traditional NLP techniques, which has prompted a quest for novel approaches that can make better use of already-existing data. <cite class="ltx_cite ltx_citemacro_cite">Michael A. Hedderich (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib32" title="">2021</a>)</cite> research highlighted the significance of creating NLP tools that are especially suited for low-resource languages while taking linguistic and cultural quirks into account.

<br class="ltx_break"/>A crucial component of developing NLP models for low-resource languages is dataset curation. In addition to collecting data, curators of datasets such as MahaSent, MahaHate, MahaNews-SHC, and CSEBUETNLP XLSum make sure that the data accurately reflects the linguistic diversity and cultural context of the language. Projects like <cite class="ltx_cite ltx_citemacro_cite">Narzary et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib35" title="">2022</a>)</cite> have brought attention to how crucial it is to provide high-quality datasets that accurately represent language use in everyday situations.

<br class="ltx_break"/>In multilingual natural language processing, cross-lingual transfer methods have demonstrated potential, especially when applied to low-resource language tasks. According to research like that of <cite class="ltx_cite ltx_citemacro_cite">Melvin Johnson (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib31" title="">2017</a>)</cite>, the concept of sharing parameters across languages allows models to acquire representations that function well in a variety of languages. This idea is important because it enables language models to use their English language skills to complete tasks in Marathi through the use of translation-based prompting, which is a type of cross-lingual transfer. Cross-lingual skills are supported by recent advances in multilingual models, such as mBERT <cite class="ltx_cite ltx_citemacro_cite">Jacob Devlin (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib16" title="">2019</a>)</cite> and XLM-R <cite class="ltx_cite ltx_citemacro_cite">Alexis Conneau (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib5" title="">2018</a>)</cite>, which lay a strong platform for further gains in low-resource language processing.

<br class="ltx_break"/>Prompting strategies have become an effective way to train large language models (LLMs) for particular tasks without requiring a lot of fine-tuning. According to <cite class="ltx_cite ltx_citemacro_cite">Tom B. Brown (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib48" title="">2020</a>)</cite>, well-crafted prompts can direct models such as GPT-3 to carry out a range of NLP tasks effectively. More research has been done on the subject of quick engineering’s potential to induce desired behaviors in LLMs even in situations with limited resources by <cite class="ltx_cite ltx_citemacro_cite">Pengfei Liu (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib39" title="">2021</a>)</cite>. These methods have shown to be useful, particularly for languages and activities for which there is little to no direct training data.

<br class="ltx_break"/>Prompting is being used more and more for tasks like sentiment analysis and hate speech detection, which are essential for keeping an eye on public conversation and guaranteeing secure online spaces. Research on Pattern-Exploiting Training (PET) for such tasks was first presented by <cite class="ltx_cite ltx_citemacro_cite">Timo Schick (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib47" title="">2021</a>)</cite>, who showed how prompts could direct models to make context-based, nuanced predictions. This method is consistent with the findings of <cite class="ltx_cite ltx_citemacro_cite">Shijun Shi (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib45" title="">2024</a>)</cite>, who also highlighted the benefit of model prompting for text categorization tasks in a variety of languages and domains.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Chain of Translation Prompting</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our study introduces a novel approach called "Chain of Translation Prompting" aimed at enhancing the processing of Marathi, a low-resource language, using advanced language models like GPT-4o, GPT-4o Mini, Llama3-8B, Llama3-405B, and Gemma-9B. Recognizing the strong translation capabilities of these models, we leverage their ability to translate Marathi into English for improved processing. Directly prompting language models in Marathi has posed several challenges, primarily due to the scarcity of quality training data and the models’ limitations in comprehending underrepresented languages. These challenges often result in sub-optimal performance on tasks such as sentiment analysis, hate speech classification, news categorization, and headline generation. Below, we outline the step-by-step methodology employed in our approach.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Data Collection and Preparation:</span>
We used datasets specific to Marathi language tasks, including MahaSent for sentiment analysis, MahaHate for hate speech classification, and MahaNews-SHC for subject categorization. For generative tasks, we used the CSEBUETNLP XLSum dataset to generate headlines.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Chain of Translation Prompting (CoTR) Technique:</span>
Our methodology adapts a conventional translation approach used in developing low-resource NLP systems but applies it within the framework of large language model (LLM) prompts. Specifically, our method involves prompting the LLM to first translate the input text from Marathi into English, and then to execute the desired task on the translated English text.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Task Execution:</span></p>
<ul class="ltx_itemize" id="S3.I1.i3.I1">
<li class="ltx_item" id="S3.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.I1.i1.p1.1.1">Sentiment Analysis, Hate Speech Classification, and Subject Categorization</span>: For these classification tasks, the models categorize each sentence into predefined classes based on the task’s requirements.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.I1.i2.p1.1.1">Generative Task</span>: We used GPT-4o, GPT-4o Mini, and Llama3-405b for the headline generation task. The three prompting strategies used for generating headlines are described below.</p>
<ol class="ltx_enumerate" id="S3.I1.i3.I1.i2.I1">
<li class="ltx_item" id="S3.I1.i3.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S3.I1.i3.I1.i2.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i3.I1.i2.I1.i1.p1.1">Without Translation: In this approach, headlines were generated directly from the original Marathi articles without any translation. This method aimed to assess the model’s capability to generate concise and impactful headlines in the source language.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="S3.I1.i3.I1.i2.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i3.I1.i2.I1.i2.p1.1">Full Translation: Here, the entire Marathi article was first translated into English. Headlines were then generated based on the translated English text. The generated English headlines were subsequently translated back into Marathi to evaluate their fidelity and relevance.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3.I1.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="S3.I1.i3.I1.i2.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.I1.i2.I1.i3.p1.1">Half Translation: Given the length and complexity of the articles, the half-translation method was employed to streamline the process. In this approach, English headlines were generated based on the Marathi articles without full translation. These English headlines were then translated back into Marathi. This method aimed to balance efficiency and accuracy by avoiding the need for extensive translation of the entire article.</p>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Comparison with Direct Prompting</span>:
To evaluate the effectiveness of the Chain of Translation Prompting, we compare its results against the traditional method of directly prompting the models to process the Marathi text without performing translation.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Evaluation Metrics</span>:
The performance of the models is measured using conventional metrics, such as the ROUGE-L score for generative tasks. The ROUGE-L score assesses the quality of the generated text, like summaries or headlines, by calculating the overlap with reference text. It evaluates precision and recall by calculating the longest common subsequence (LCS) between the reference text and the generated output. ROUGE-L focuses on capturing the longest word sequences found in both texts, providing insights into the preservation of critical information and coherence.</p>
</div>
<div class="ltx_para" id="S3.I1.i5.p2">
<p class="ltx_p" id="S3.I1.i5.p2.1">For classification tasks, the model outputs are compared with ground truths, and the error percentage is reported.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S3.F4.g1" src="extracted/5838669/classification1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Classification Task using Chain of Translation Prompting</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S3.F5.g1" src="extracted/5838669/generative.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Generative Task using Chain of Translation Prompting</figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results on the MahaNews, MahaHate Dataset</figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="424" id="S3.T1.g1" src="extracted/5838669/table_classification.png" width="598"/>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Datasets Used</h3>
<div class="ltx_para" id="S3.SS2.p1">
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">MahaSent-GT<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaSent-MD" title="">https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaSent-MD</a></span></span></span>:

<br class="ltx_break"/>We used a subset of the L3Cube-MahaSent-MD dataset <cite class="ltx_cite ltx_citemacro_cite">Pingle et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib41" title="">2023</a>)</cite>, which contains 14,000 annotated Marathi tweets. Three sentiment labels Positive, Negative, and Neutral are present in the dataset. In particular, we employed the MahaSent-GT portion of this dataset for sentiment analysis.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">MahaHate<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaHate" title="">https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaHate</a></span></span></span>:

<br class="ltx_break"/>We used the L3Cube-MahaHate collection’s MahaHate 2-Class dataset for our classification task <cite class="ltx_cite ltx_citemacro_cite">Patil et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib38" title="">2022</a>)</cite>. It contains around 37500 annotated Marathi sentences. This dataset is divided into two categories: hate and non-hate. We employed the MahaHate 2-Class set for our classification task.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">MahaNews-SHC<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaNews-SHC" title="">https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaNews-SHC</a></span></span></span>:

<br class="ltx_break"/>We analyzed Marathi news articles using the L3Cube-MahaNews-SHC dataset <cite class="ltx_cite ltx_citemacro_cite">Mittal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#bib.bib33" title="">2023</a>)</cite>. This dataset contains approximately 54,000 news articles spanning a wide range of topics and was used for the news classification task.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">XLSum<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/csebuetnlp/xlsum" title="">https://huggingface.co/datasets/csebuetnlp/xlsum</a></span></span></span>:

<br class="ltx_break"/>We focused on Marathi text headline creation for our study using the CSEBUETNLP XLSum dataset. The dataset offers a wide range of news stories linked with their associated headlines. Our objective was to enhance the accuracy of automated headline creation for Marathi news articles by utilizing this dataset.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Methodology</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We performed our classification task on GPT-4o, GPT-4o Mini, Llama3-8B, Llama3-405B, and Gemma-9B.</p>
<ol class="ltx_enumerate" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i1.p1.1.1">GPT-4o:</span>
<br class="ltx_break"/>GPT-4o is developed by OpenAI, with  1.8 trillion parameters (unofficial). It is a closed-source model and accessible through APIs provided by OpenAI. GPT-4o builds on the advancements of its previous versions, offering enhanced capabilities in natural language understanding, generation, and reasoning across a wide range of tasks.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i2.p1.1.1">GPT-4o Mini:</span>
<br class="ltx_break"/>GPT-4o Mini is a smaller, more lightweight version of GPT-4o. This model is closed-source. GPT-4o Mini is engineered to balance computational efficiency with performance, making it suitable for applications requiring faster inference times and lower resource consumption while maintaining a high level of language understanding.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i3.p1.1.1">Llama 3.1 8B / 405B:</span>
<br class="ltx_break"/>Llama 3.1 (Large Language Model for Multilingual Applications) is the third iteration in the Meta Llama series, designed with multiple variants, including a 405 billion parameter version and an 8 billion parameter version. These models are typically open-source. Llama3 models are optimized for multilingual tasks, incorporating vast and diverse datasets to improve performance across different languages.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i4.p1.1.1">Gemma-2 9B:</span>
<br class="ltx_break"/>Gemma-2 9B is an open-source language model with 9 billion parameters from Google. It strikes a balance between model size and performance, offering robust capabilities for both academic and practical applications.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.1">Model</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.2.1.1">Without Translation</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.1.3.1.1">Half Translation</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.4.1">
<span class="ltx_p" id="S3.T2.1.1.1.4.1.1">Full Translation</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.2.1.1.1.1">GPT-4o</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.2.1.2.1.1">33.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.3.1">
<span class="ltx_p" id="S3.T2.1.2.1.3.1.1">44</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.4.1">
<span class="ltx_p" id="S3.T2.1.2.1.4.1.1">49</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.1.1">
<span class="ltx_p" id="S3.T2.1.3.2.1.1.1">GPT-4o mini</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.2.1">
<span class="ltx_p" id="S3.T2.1.3.2.2.1.1">21.34</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.3.1">
<span class="ltx_p" id="S3.T2.1.3.2.3.1.1">21.72</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.4.1">
<span class="ltx_p" id="S3.T2.1.3.2.4.1.1">22.22</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.1.1">
<span class="ltx_p" id="S3.T2.1.4.3.1.1.1">llama3-405b</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.2.1">
<span class="ltx_p" id="S3.T2.1.4.3.2.1.1">20.27</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.3.1">
<span class="ltx_p" id="S3.T2.1.4.3.3.1.1">20.34</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.4.1">
<span class="ltx_p" id="S3.T2.1.4.3.4.1.1">21.13</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Rouge-L score in percentage for 3 approaches on the headline generation task on CSEBUETNLP XLSum Dataset</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Error percentage in the classification task across 5 models (these are the weighted averages and the numbers are percentages)</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.1.1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1.1" style="width:56.9pt;">Model</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.1.2">Dataset</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.3.1">
<span class="ltx_p" id="S3.T3.1.1.1.3.1.1" style="width:64.0pt;">Standard Prompt</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.4.1">
<span class="ltx_p" id="S3.T3.1.1.1.4.1.1" style="width:64.0pt;">CoTR Prompt</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.5.1">
<span class="ltx_p" id="S3.T3.1.1.1.5.1.1" style="width:56.9pt;">Average Standard Prompt</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.6.1">
<span class="ltx_p" id="S3.T3.1.1.1.6.1.1" style="width:56.9pt;">Average CoTR Prompt</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.2.1.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.1.1">
<span class="ltx_p" id="S3.T3.1.2.1.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.2.1.1.1.1.1">gpt-4o</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.2.1.2">MahaSent-GT</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.3.1">
<span class="ltx_p" id="S3.T3.1.2.1.3.1.1" style="width:64.0pt;">20.38</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.4.1">
<span class="ltx_p" id="S3.T3.1.2.1.4.1.1" style="width:64.0pt;">18.44</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.2.1.5" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.5.1">
<span class="ltx_p" id="S3.T3.1.2.1.5.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.2.1.5.1.1.1">13.57</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.2.1.6" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.6.1">
<span class="ltx_p" id="S3.T3.1.2.1.6.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.2.1.6.1.1.1">11.25</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.3.2.1">MahaNews-SHC</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.2.2.1">
<span class="ltx_p" id="S3.T3.1.3.2.2.1.1" style="width:64.0pt;">3.06</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.2.3.1">
<span class="ltx_p" id="S3.T3.1.3.2.3.1.1" style="width:64.0pt;">2.04</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.4.3.1">MahaHate</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.4.3.2.1">
<span class="ltx_p" id="S3.T3.1.4.3.2.1.1" style="width:64.0pt;">16.83</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.4.3.3.1">
<span class="ltx_p" id="S3.T3.1.4.3.3.1.1" style="width:64.0pt;">12.87</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.4">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.5.4.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.1.1">
<span class="ltx_p" id="S3.T3.1.5.4.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.5.4.1.1.1.1">gpt-4o mini</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.5.4.2">MahaSent-GT</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.3.1">
<span class="ltx_p" id="S3.T3.1.5.4.3.1.1" style="width:64.0pt;">20.38</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.4.1">
<span class="ltx_p" id="S3.T3.1.5.4.4.1.1" style="width:64.0pt;">19.41</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.5.4.5" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.5.1">
<span class="ltx_p" id="S3.T3.1.5.4.5.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.5.4.5.1.1.1">20.19</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.5.4.6" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.6.1">
<span class="ltx_p" id="S3.T3.1.5.4.6.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.5.4.6.1.1.1">15.23</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.6.5.1">MahaNews-SHC</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.6.5.2.1">
<span class="ltx_p" id="S3.T3.1.6.5.2.1.1" style="width:64.0pt;">6.12</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.6.5.3.1">
<span class="ltx_p" id="S3.T3.1.6.5.3.1.1" style="width:64.0pt;">4.08</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.7.6.1">MahaHate</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.6.2.1">
<span class="ltx_p" id="S3.T3.1.7.6.2.1.1" style="width:64.0pt;">33.66</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.6.3.1">
<span class="ltx_p" id="S3.T3.1.7.6.3.1.1" style="width:64.0pt;">21.78</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8.7">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.8.7.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.8.7.1.1">
<span class="ltx_p" id="S3.T3.1.8.7.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.8.7.1.1.1.1">llama3-405b</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.8.7.2">MahaSent-GT</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.8.7.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.8.7.3.1">
<span class="ltx_p" id="S3.T3.1.8.7.3.1.1" style="width:64.0pt;">31.06</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.8.7.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.8.7.4.1">
<span class="ltx_p" id="S3.T3.1.8.7.4.1.1" style="width:64.0pt;">27.18</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.8.7.5" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.8.7.5.1">
<span class="ltx_p" id="S3.T3.1.8.7.5.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.8.7.5.1.1.1">19.86</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.8.7.6" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.8.7.6.1">
<span class="ltx_p" id="S3.T3.1.8.7.6.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.8.7.6.1.1.1">16.22</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.9.8.1">MahaNews-SHC</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.9.8.2.1">
<span class="ltx_p" id="S3.T3.1.9.8.2.1.1" style="width:64.0pt;">7.14</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.9.8.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.9.8.3.1">
<span class="ltx_p" id="S3.T3.1.9.8.3.1.1" style="width:64.0pt;">6.12</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.10.9.1">MahaHate</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.10.9.2.1">
<span class="ltx_p" id="S3.T3.1.10.9.2.1.1" style="width:64.0pt;">20.89</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.10.9.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.10.9.3.1">
<span class="ltx_p" id="S3.T3.1.10.9.3.1.1" style="width:64.0pt;">14.85</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.11.10">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.11.10.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.10.1.1">
<span class="ltx_p" id="S3.T3.1.11.10.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.11.10.1.1.1.1">llama3-8b</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.11.10.2">MahaSent-GT</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.11.10.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.10.3.1">
<span class="ltx_p" id="S3.T3.1.11.10.3.1.1" style="width:64.0pt;">35.92</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.11.10.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.10.4.1">
<span class="ltx_p" id="S3.T3.1.11.10.4.1.1" style="width:64.0pt;">27.18</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.11.10.5" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.10.5.1">
<span class="ltx_p" id="S3.T3.1.11.10.5.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.11.10.5.1.1.1">29.13</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.11.10.6" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.10.6.1">
<span class="ltx_p" id="S3.T3.1.11.10.6.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.11.10.6.1.1.1">23.84</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.12.11.1">MahaNews-SHC</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.12.11.2.1">
<span class="ltx_p" id="S3.T3.1.12.11.2.1.1" style="width:64.0pt;">10.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.12.11.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.12.11.3.1">
<span class="ltx_p" id="S3.T3.1.12.11.3.1.1" style="width:64.0pt;">7.14</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.13.12.1">MahaHate</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.13.12.2.1">
<span class="ltx_p" id="S3.T3.1.13.12.2.1.1" style="width:64.0pt;">40.59</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.13.12.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.13.12.3.1">
<span class="ltx_p" id="S3.T3.1.13.12.3.1.1" style="width:64.0pt;">36.63</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.14.13">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.14.13.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.14.13.1.1">
<span class="ltx_p" id="S3.T3.1.14.13.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.14.13.1.1.1.1">gemma9b</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.14.13.2">MahaSent-GT</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.14.13.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.14.13.3.1">
<span class="ltx_p" id="S3.T3.1.14.13.3.1.1" style="width:64.0pt;">33.98</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.14.13.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.14.13.4.1">
<span class="ltx_p" id="S3.T3.1.14.13.4.1.1" style="width:64.0pt;">27.18</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.14.13.5" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.14.13.5.1">
<span class="ltx_p" id="S3.T3.1.14.13.5.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.14.13.5.1.1.1">22.18</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.14.13.6" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.14.13.6.1">
<span class="ltx_p" id="S3.T3.1.14.13.6.1.1" style="width:56.9pt;"><span class="ltx_text" id="S3.T3.1.14.13.6.1.1.1">22.51</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.15.14.1">MahaNews-SHC</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.15.14.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.15.14.2.1">
<span class="ltx_p" id="S3.T3.1.15.14.2.1.1" style="width:64.0pt;">10.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.15.14.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.15.14.3.1">
<span class="ltx_p" id="S3.T3.1.15.14.3.1.1" style="width:64.0pt;">10.20</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.16.15.1">MahaHate</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.16.15.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.16.15.2.1">
<span class="ltx_p" id="S3.T3.1.16.15.2.1.1" style="width:64.0pt;">21.78</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.16.15.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.16.15.3.1">
<span class="ltx_p" id="S3.T3.1.16.15.3.1.1" style="width:64.0pt;">29.70</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.T2" title="Table 2 ‣ 3.3 Evaluation Methodology ‣ 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.T3" title="Table 3 ‣ 3.3 Evaluation Methodology ‣ 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_tag">3</span></a> show the analysis done on Standard Prompting and Chain of Translation Prompting.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Classification Task</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Approximately 100 sentences were selected from MahaSent-GT, MahaNews-SHC, and MahaHate. The large language models categorize each of the sentences into a predefined category. These results were compared with the ground truth values to calculate the error rate.
The error rate was calculated with the direct prompting approach and Chain of Translation prompting approach
The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.T3" title="Table 3 ‣ 3.3 Evaluation Methodology ‣ 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_tag">3</span></a>.

<br class="ltx_break"/>In the CoTR prompting approach, the error rate has reduced by 2.32% in the GPT-4o model, by 3.64% llama3-405b, by 5.29% in llama3-8b and by 4.96% in GPT-4o Mini. The error rate is slightly increased by 0.33% in the Gemma-9B model.

<br class="ltx_break"/>The error rate has been reduced by almost 5% in llama3-8b and gpt4 mini models. Specifically, the CoTR prompting approach has significantly improved hate speech identification across all models except for Gemma-9B. In the hate speech classification task, Gemma-9B often failed to correctly translate hateful comments and, in some cases, omitted those parts entirely. However, compared to standard prompting, the number of misclassifications for the "Non-hate" class was lower when using CoTR.

<br class="ltx_break"/>One sample detection with traditional prompting versus CoTR prompting from each of the four models has been attached in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.T1" title="Table 1 ‣ 3.1 Chain of Translation Prompting ‣ 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_tag">1</span></a>, where the output with CoTR prompting is the same as the ground truth.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Generation Task</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The headlines from the Marathi news text were generated using traditional prompting and CoTR prompting (with half and full translation). The headlines were compared against the manually assigned headline and the Rouge-L score metric was used to calculate their similarity with the manually assigned headline.
The Rouge-L score for traditional prompting and CoTR prompting (half and full translation) are given in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.04512v1#S3.T2" title="Table 2 ‣ 3.3 Evaluation Methodology ‣ 3 Methodology ‣ Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages"><span class="ltx_text ltx_ref_tag">2</span></a>
<br class="ltx_break"/>We observed that GPT-4o delivered the best performance among all the models. GPT-4o Mini struggled to identify fine details in the articles, while Llama3-405B occasionally failed to provide the results in the specified format and produced some inaccurate translations. Overall, GPT-4o Mini and Llama-405B yielded similar outcomes.

<br class="ltx_break"/>In general, we observe the following performance ranking for Marathi tasks: GPT-4o &gt; GPT-4o Mini &gt; Llama 3.1 405B &gt; Gemma 2 9B &gt; Llama 3.1 8B. The CoTR approach proves especially useful with smaller models and for complex tasks like hate speech detection and sentiment analysis.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Rouge-L score for 3 approaches on the headline generation task</figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="S4.T4.g1" src="extracted/5838669/table_generation.png" width="598"/>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future Work and Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In summary, our study demonstrates that various prompting strategies, particularly the Chain of Translation (CoTR) method, effectively enhance Marathi language processing tasks. By applying these techniques to various classification and generation tasks, we have expanded the potential for more reliable and accurate NLP applications in Marathi. While CoTR improves model performance, it does so at the cost of generating more tokens.

<br class="ltx_break"/>In the future, we aim to enhance performance on Marathi language tasks by combining Chain of Thought (CoT) and Chain of Translation (CoTR) prompting strategies. Our goal is to achieve context-aware and precise responses for complex tasks like sentiment analysis, hate speech detection, and subject classification. CoT allows models to break down complex tasks into simpler steps, while CoTR leverages translation from Marathi to English, where more accurate models can be employed. Together, these strategies should create a robust framework that improves model performance and reliability in Marathi NLP tasks.

<br class="ltx_break"/>This approach can further be used for other low-resource Indic languages.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was done under the mentorship of Mr. Raviraj Joshi (Mentor, L3Cube Pune). We would like to express our gratitude towards him for his continuous support and encouragement.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn and Brohan (2022)</span>
<span class="ltx_bibblock">
Michael Ahn and Anthony Brohan. 2022.

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2204.01691</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahuja et al. (2023)</span>
<span class="ltx_bibblock">
Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2023.

</span>
<span class="ltx_bibblock">Mega: Multilingual evaluation of generative ai.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, page 4232–4267, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aishwarya et al. (2023)</span>
<span class="ltx_bibblock">
Mirashi Aishwarya, Sonavane Srushti, Lingayat Purva, Padhiyar Tejas, and Joshi Raviraj. 2023.

</span>
<span class="ltx_bibblock">L3cube-indicnews: News-based short text and long document classification datasets in indic languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 20th International Conference on Natural Language Processing (ICON)</em>, pages 442–449.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akhtar et al. (2016)</span>
<span class="ltx_bibblock">
Md Shad Akhtar, Ayush Kumar, Asif Ekbal, and Pushpak Bhattacharyya. 2016.

</span>
<span class="ltx_bibblock">A hybrid deep learning architecture for sentiment analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</em>, page 482–493, Osaka, Japan. The COLING 2016 Organizing Committee.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alexis Conneau (2018)</span>
<span class="ltx_bibblock">
Naman Goyal Alexis Conneau, Kartikay Khandelwal. 2018.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Andreas, Dan Klein, and Sergey Levine. 2018.

</span>
<span class="ltx_bibblock">Learning with latent language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of NAACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et al. (2023)</span>
<span class="ltx_bibblock">
Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock">Revisiting machine translation for cross-lingual classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, page 6489–6499, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang et al. (2023)</span>
<span class="ltx_bibblock">
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, page 675–718, Nusa Dua, Bali. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022.

</span>
<span class="ltx_bibblock">Can rationalization improve robustness?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of NAACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2107.03374</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2022)</span>
<span class="ltx_bibblock">
Nikhil Goyal, Harsh Trivedi, and Prithiviraj Sen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://neurips.cc/Conferences/2022/Schedule?showEvent=34426" title="">Prompting techniques for improving performance on low-resource nlp tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2022)</span>
<span class="ltx_bibblock">
Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022.

</span>
<span class="ltx_bibblock">Dream: Uncovering mental models behind language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of NAACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halike et al. (2023)</span>
<span class="ltx_bibblock">
Ayiguli Halike, Aishan Wumaier, and Tuergen Yibulayin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.mdpi.com/2076-3417/13/7/4636" title="">Zero-shot relation triple extraction with prompts for low-resource languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Applied Sciences</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hase and Bansal (2022)</span>
<span class="ltx_bibblock">
Peter Hase and Mohit Bansal. 2022.

</span>
<span class="ltx_bibblock">When can models learn from explanations? a formal framework for understanding the roles of explanation data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacob Devlin (2019)</span>
<span class="ltx_bibblock">
Kenton Lee Jacob Devlin, Ming-Wei Chang. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jie et al. (2022)</span>
<span class="ltx_bibblock">
Zhanming Jie, Jierui Li, and Wei Lu. 2022.

</span>
<span class="ltx_bibblock">Learning to reason deductively: Math word problem solving as complex relation extraction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2203.10316</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi (2022a)</span>
<span class="ltx_bibblock">
Raviraj Joshi. 2022a.

</span>
<span class="ltx_bibblock">L3cube-mahacorpus and mahabert: Marathi monolingual corpus, marathi bert language models, and resources.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference</em>, pages 97–101.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi (2022b)</span>
<span class="ltx_bibblock">
Raviraj Joshi. 2022b.

</span>
<span class="ltx_bibblock">L3cube-mahanlp: Marathi natural language processing datasets, models, and library.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2205.14728</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khurana et al. (2022)</span>
<span class="ltx_bibblock">
Sameer Khurana, Ashwin Ghosh, and Sreelakshmi Nair. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://jair.org/index.php/jair/article/view/13422" title="">Using prompt-based learning for enhanced low-resource language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Journal of Artificial Intelligence Research</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koutsikakis et al. (2022)</span>
<span class="ltx_bibblock">
John Koutsikakis, Konstantinos Papagiannopoulos, and Antonis Papadakis. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://jair.org/index.php/jair/article/view/13421" title="">Prompting strategies for zero-shot text classification in low-resource languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Journal of Artificial Intelligence Research</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kulkarni et al. (2021)</span>
<span class="ltx_bibblock">
Atharva Kulkarni, Meet Mandhane, Manali Likhitkar, Gayatri Kshirsagar, and Raviraj Joshi. 2021.

</span>
<span class="ltx_bibblock">L3cubemahasent: A marathi tweet-based sentiment analysis dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</em>, pages 213–220.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lampinen et al. (2022)</span>
<span class="ltx_bibblock">
Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022.

</span>
<span class="ltx_bibblock">Can language models learn from explanations in context?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2204.02329</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. (2021)</span>
<span class="ltx_bibblock">
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. 2021.

</span>
<span class="ltx_bibblock">Mwptoolkit: An open-source framework for deep learning-based math word problem solvers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2109.00799</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2107.13586</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
X. Liu, H. Wu, L. Shen, S. Zhang, and M. Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.567" title="">Empirical evaluation of multilingual language models for low-resource languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luong et al. (2023)</span>
<span class="ltx_bibblock">
Minh-Thang Luong, Quoc V. Le, and Thang Luong. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://transacl.org/ojs/index.php/tacl/article/view/2112" title="">Multilingual neural machine translation with a special focus on low-resource languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Transactions of the Association for Computational Linguistics (TACL)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marasovic et al. (2022)</span>
<span class="ltx_bibblock">
Ana Marasovic, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022.

</span>
<span class="ltx_bibblock">Few-shot self-rationalization with natural language prompts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">NAACL Findings</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melvin Johnson (2017)</span>
<span class="ltx_bibblock">
Quoc V. Le Melvin Johnson, Mike Schuster. 2017.

</span>
<span class="ltx_bibblock">Google’s multilingual neural machine translation system: Enabling zero-shot translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michael A. Hedderich (2021)</span>
<span class="ltx_bibblock">
Heike Adel Michael A. Hedderich, Lukas Lange. 2021.

</span>
<span class="ltx_bibblock">A survey on recent approaches for natural language processing in low-resource scenarios.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittal et al. (2023)</span>
<span class="ltx_bibblock">
Saloni Mittal, Vidula Magdum, Sharayu Hiwarkhedkar, Omkar Dhekane, and Raviraj Joshi. 2023.

</span>
<span class="ltx_bibblock">L3cube-mahanews: News-based short text and long document classification datasets in marathi.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">International Conference on Speech and Language Technologies for Low-resource Languages</em>, pages 52–63. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad Farrukh Bashir (2023)</span>
<span class="ltx_bibblock">
Abdul Rehman Javed Muhammad Farrukh Bashir. 2023.

</span>
<span class="ltx_bibblock">Context-aware emotion detection from low-resource urdu language using deep neural network.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">ACM Journals</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narzary et al. (2022)</span>
<span class="ltx_bibblock">
Sanjib Narzary, Maharaj Brahma, and Mwnthai Narzary. 2022.

</span>
<span class="ltx_bibblock">Generating monolingual dataset for low resource language bodo from old books using google keep.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panteleimon Krasadakis (2024)</span>
<span class="ltx_bibblock">
Vassilios S. Verykios Panteleimon Krasadakis, Evangelos Sakkopoulos. 2024.

</span>
<span class="ltx_bibblock">A survey on challenges and advances in natural language processing with a focus on legal informatics and low-resource languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of MDPI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al. (2021)</span>
<span class="ltx_bibblock">
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.

</span>
<span class="ltx_bibblock">Are nlp models really able to solve simple math word problems?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of NAACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et al. (2022)</span>
<span class="ltx_bibblock">
Hrushikesh Patil, Abhishek Velankar, and Raviraj Joshi. 2022.

</span>
<span class="ltx_bibblock">L3cube-mahahate: A tweet-based marathi hate speech detection dataset and bert models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the Third Workshop on Threat, Aggression and Cyberbullying (TRAC 2022)</em>, pages 1–9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pengfei Liu (2021)</span>
<span class="ltx_bibblock">
Jinlan Fu Pengfei Liu, Weizhe Yuan. 2021.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pi et al. (2022)</span>
<span class="ltx_bibblock">
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock">Reasoning like program executors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2201.11473</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pingle et al. (2023)</span>
<span class="ltx_bibblock">
Aabha Pingle, Aditya Vyawahare, Isha Joshi, Rahul Tangsali, and Raviraj Joshi. 2023.

</span>
<span class="ltx_bibblock">L3cube-mahasent-md: A multi-domain marathi sentiment analysis dataset and transformer models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation</em>, pages 274–281.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et al. (2021)</span>
<span class="ltx_bibblock">
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis and insights from training gopher.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2112.11446</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanjib Narzary (2022)</span>
<span class="ltx_bibblock">
Mwnthai Narzary Sanjib Narzary, Maharaj Brahma. 2022.

</span>
<span class="ltx_bibblock">Generating monolingual dataset for low resource language bodo from old books using google keep.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao and Rush (2021)</span>
<span class="ltx_bibblock">
Teven Le Scao and Alexander Rush. 2021.

</span>
<span class="ltx_bibblock">How many data points is a prompt worth?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of NAACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shijun Shi (2024)</span>
<span class="ltx_bibblock">
Jie Xi Shijun Shi, Kai Hu. 2024.

</span>
<span class="ltx_bibblock">Robust scientific text classification using prompt tuning based on data augmentation with l2 regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Science Direct</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thabah and Purkayastha (2021)</span>
<span class="ltx_bibblock">
N. Donald Jefferson Thabah and Bipul Syam Purkayastha. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://link.springer.com/chapter/10.1007/978-981-33-4084-8_1" title="">Low resource neural machine translation from english to khasi: A transformer-based approach</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Low Resource Neural Machine Translation from English to Khasi: A Transformer-Based Approach</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Timo Schick (2021)</span>
<span class="ltx_bibblock">
Hinrich Schütze Timo Schick. 2021.

</span>
<span class="ltx_bibblock">Exploiting cloze questions for few shot text classification and natural language inference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tom B. Brown (2020)</span>
<span class="ltx_bibblock">
Nick Ryder Tom B. Brown, Benjamin Mann. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang Zhang (2023)</span>
<span class="ltx_bibblock">
Bradley Hauer Xiang Zhang, Senyu Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.491/" title="">Don’t trust chatgpt when your question is not in english: A study of multilingual abilities and types of llms</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Yuqing Yang, Jie Fu, and Pascal Poupart. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.acl-main.123" title="">Prompt learning for low-resource language understanding with pretrained models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1"></p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep  6 17:13:56 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
