<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.18072] Image Captioning with Multi-Context Synthetic Data</title><meta property="og:description" content="Image captioning requires numerous annotated image-text pairs, resulting in substantial annotation costs. Recently, large models (e.g. diffusion models and large language models) have excelled in producing high-qualityâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Image Captioning with Multi-Context Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Image Captioning with Multi-Context Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.18072">

<!--Generated on Thu Feb 29 04:36:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Image Captioning with Multi-Context Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Feipeng Ma<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">1</span></sup> â€ƒYizhou Zhou<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">2</span></sup> â€ƒFengyun Rao<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">2</span></sup> â€ƒYueyi Zhang<sup id="id9.9.id4" class="ltx_sup"><span id="id9.9.id4.1" class="ltx_text ltx_font_italic">1,3</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span> â€ƒXiaoyan Sun<sup id="id10.10.id5" class="ltx_sup"><span id="id10.10.id5.1" class="ltx_text ltx_font_italic">1,3</span></sup>
</span><span class="ltx_author_notes">This work was performed while Feipeng Ma was an intern with WeChat, Tencent Inc.Corresponding authors: {zhyuey, sunxiaoyan}@ustc.edu.cn</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Image captioning requires numerous annotated image-text pairs, resulting in substantial annotation costs. Recently, large models (e.g. diffusion models and large language models) have excelled in producing high-quality images and text. This potential can be harnessed to create synthetic image-text pairs for training captioning models. Synthetic data can improve cost and time efficiency in data collection, allow for customization to specific domains, bootstrap generalization capability for zero-shot performance, and circumvent privacy concerns associated with real-world data. However, existing methods struggle to attain satisfactory performance solely through synthetic data. We identify the issue as generated images from simple descriptions mostly capture a solitary perspective with limited context, failing to align with the intricate scenes prevalent in real-world imagery. To tackle this, we present an innovative pipeline that introduces multi-context data generation. Beginning with an initial text corpus, our approach employs a large language model to extract multiple sentences portraying the same scene from diverse viewpoints. These sentences are then condensed into a single sentence with multiple contexts. Subsequently, we generate intricate images using the condensed captions through diffusion models. Our model is exclusively trained on synthetic image-text pairs crafted through this process. The effectiveness of our pipeline is validated through experimental results in both the in-domain and cross-domain settings, where it achieves state-of-the-art performance on well-known datasets such as MSCOCO, Flickr30k, and NoCaps.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The realm of image captioning, which aims to craft informative textual descriptions for provided images, has witnessed remarkable progress. The crux of the challenge in image captioning hinges on comprehending the interplay between images and text, a dependency strongly rooted in the image-text pairs. Two approaches emerge to tackle this hurdle: one involves utilizing readily existing paired image-text data, while the other entails creating pairs from independent data sources.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">There are two primary sources of existing paired image-text data: human-annotated and web-crawled.
Human-annotated data, employed in many studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Dai and Lin <a href="#bib.bib10" title="" class="ltx_ref">2017</a>; Anderson etÂ al. <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>, can lead to significant improvements. However, the small size of available data limits its scalability and restricts domain generality.
Web-crawled data often suffer from low quality due to inaccurate correlations between images and text.
As a result, modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Kang etÂ al. <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> trained on web-crawled data can exhibit limited performance in zero-shot settings.
Moreover, large-scale crawling of image-text pairs possibly involves privacy and copyright issues.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">In the absence of paired image-text data, unsupervised strategies often forge noisy image-text pairs from independent datasets for model bootstrappingÂ <cite class="ltx_cite ltx_citemacro_citep">(Feng etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> or domain alignmentÂ <cite class="ltx_cite ltx_citemacro_citep">(Laina, Rupprecht, and Navab <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, leveraging a pretrained object detector. Moreover, <cite class="ltx_cite ltx_citemacro_citet">Meng etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite> suggest establishing object-text pairs by associating objects with sentences, rather than seeking candidates within the image collection. These techniques operate under the assumption that a pretrained detector can consistently discern visual concepts, thus establishing connections between disparate images and text. However, this assumption might not hold universally.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">Inspired by <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al. <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Zhao etÂ al. <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>, which effectively employ diffusion model-driven synthetic data in image classification and segmentation, we have noticed the progress text-to-image models have achieved in crafting high-quality images from textual descriptions. This opens the door to the use of diffusion models for constructing image-text pairs in the image captioning domain. In comparison to human-labeled and web-crawled datasets,
synthetic data offer efficiency in cost and time, enable customization for specific domains, bootstrap generalization capability for zero-shot performance, and sidestep privacy issues linked to real-world data.
Customization for specific domains, referring to the in-domain ability, involves generating data tailored to specific domains, such as particular objects, attributes, or scenarios. Generalization capabilities, pertaining to cross-domain capability, entail generating synthetic data encompassing a broader range of scenarios, not limited to a specific objective.
However, there exists no prior work that specifically addresses the image captioning task solely using synthetic data generated via diffusion models.</p>
</div>
<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2305.18072/assets/x1.png" id="Sx1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of the natural image, the uni-context images, and the multi-context image.</figcaption>
</figure>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">The method to train an image captioning model using synthetic data involves two essential steps: (1) generating images along with captions through a diffusion model, utilizing established image captioning datasets. (2) subsequently training the image captioning model with the newly created synthetic dataset. However, a significant limitation arises when utilizing synthetic images, as they often lack the contextual depth necessary for ensuring precise image captioning accuracy. Our analysis highlights that synthetic images, originating from readily available basic captions, tend to exhibit constrained contexts, resulting in the omission of intricate and multifaceted scenes. These images are specifically referred to as â€œuni-contextâ€ images. In contrast, natural images inherently encompass a multi-contextual essence, portraying a diverse array of objects, arrangements, interactions, and encapsulating complex and elaborate scenes. In this context, we provide a collection of examples for a comprehensive comparison between uni-context and multi-context images, depicted in FigureÂ <a href="#Sx1.F1" title="Figure 1 â€£ Introduction â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Notably, employing complex captions enables diffusion models to generate multi-context images, aligning with multi-faceted captions. Our focus for the image captioning task centers around the generation of multi-context captions to facilitate the synthesis of images with diverse contextual characteristics.</p>
</div>
<div id="Sx1.p6" class="ltx_para">
<p id="Sx1.p6.1" class="ltx_p">In this paper, we propose a pipeline for <span id="Sx1.p6.1.1" class="ltx_text ltx_font_bold">I</span>mage <span id="Sx1.p6.1.2" class="ltx_text ltx_font_bold">C</span>aptioning with Multi-Context <span id="Sx1.p6.1.3" class="ltx_text ltx_font_bold">S</span>ynthetic <span id="Sx1.p6.1.4" class="ltx_text ltx_font_bold">D</span>ata (<span id="Sx1.p6.1.5" class="ltx_text ltx_font_bold">ICSD</span>).
Our pipeline starts with a text corpus containing accessible simple captions from diverse sources such as datasets, web crawls, and generated content. Comprising two key stages, the pipeline initiates with the generation stage and moves on to the training stage. <span id="Sx1.p6.1.6" class="ltx_text ltx_font_bold">The generation stage</span> begins with obtaining complex captions. To optimally harness the corpus, we suggest selecting simple captions that might collectively depict the same scene rather than focusing on a small subset of direct complex captions. This process, termed selection and summarization, not only taps into the corpusâ€™ potential but also generates varied combinations of simple captions for diverse scenes. However, existing methods face challenges as they lack suitable metrics to determine if captions portray the same scene, and they need to be adaptable across different domains due to varied corpus sources. Leveraging the strengths of Large Language Models (LLMs) with their expansive knowledge and generalization abilities, we employ LLMs to execute selection and summarization tasks through provided instructions. Initially, we cluster captions based on text feature similarity, treating each caption as a query to construct LLM input candidates. The subsequent step instructs LLMs to pick captions from these clusters that could potentially form a complex scene. These chosen captions are then condensed into a single comprehensive caption. Subsequently, we generate multi-context images employing a generative model aided by these summarized captions.
Moving into <span id="Sx1.p6.1.7" class="ltx_text ltx_font_bold">the training stage</span>, our approach involves training models based on multi-context images derived from summarized sentences and the captions present in the corpus.
Each multi-context image is associated with its corresponding selected sentences, offering multiple related descriptions for every multi-context image. The training of our model relies solely on this synthetic data.</p>
</div>
<div id="Sx1.p7" class="ltx_para">
<p id="Sx1.p7.1" class="ltx_p">Our main contributions are summarized as follows:</p>
</div>
<div id="Sx1.p8" class="ltx_para ltx_noindent">
<p id="Sx1.p8.1" class="ltx_p">(1) Pioneering the utilization of synthetic data in image captioning through the synergistic application of diffusion models and LLMs, introducing a novel approach in this field.</p>
</div>
<div id="Sx1.p9" class="ltx_para ltx_noindent">
<p id="Sx1.p9.1" class="ltx_p">(2) Addressing the deficiency in complexity found in synthetic images generated from basic captions by analyzing the multi-context nature of natural images. We introduce a multi-context data generation pipeline tailored for enhancing image captioning.</p>
</div>
<div id="Sx1.p10" class="ltx_para ltx_noindent">
<p id="Sx1.p10.1" class="ltx_p">(3) Demonstrating the efficacy of our exclusively synthetic data-driven approach, we attain state-of-the-art performance in in-domain and cross-domain image captioning across three datasets: MSCOCO, Flickr30k and NoCaps.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Related Work</h2>

<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Supervised Image Captioning</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">Conventional image captioning methods treat the task as a form of translationÂ <cite class="ltx_cite ltx_citemacro_citep">(Vinyals etÂ al. <a href="#bib.bib45" title="" class="ltx_ref">2015</a>; Karpathy and Fei-Fei <a href="#bib.bib20" title="" class="ltx_ref">2015</a>)</cite>. These methods typically comprise a CNN-based encoder for image encoding and an RNN-based decoder for caption generation. Recent approachesÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang, Xu, and Sun <a href="#bib.bib46" title="" class="ltx_ref">2022</a>; Barraco etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> adopt transformers architecture, yielding promising results. Additionally, research efforts integrate objectÂ <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al. <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Song etÂ al. <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>, segmentationÂ <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al. <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, gaze patternsÂ <cite class="ltx_cite ltx_citemacro_citep">(Alahmadi and Hahn <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>, and attributesÂ <cite class="ltx_cite ltx_citemacro_citep">(Fang etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> to enhance image captioning models. Due to limited annotated data, studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2022</a>; Hu etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2022</a>; Wang etÂ al. <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite> pretrain models on expansive web-crawled datasets, followed by fine-tuning on smaller human-annotated datasets. Although these methods benefit from pretraining, their performance still heavily depends on the fine-tuning phase, which entails human-annotated data.</p>
</div>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Unsupervised Image Captioning</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">Unsupervised image captioning seeks to train captioning models without the need for human-annotated data. Prior work utilizes independent image sources and text corpora for training, often leveraging object detectors to establish an initial link between the two modalities. <cite class="ltx_cite ltx_citemacro_citet">Feng etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> pioneer this field by introducing policy gradient to reward generated captions aligned with correct visual concepts. Subsequently, <cite class="ltx_cite ltx_citemacro_citet">Laina, Rupprecht, and Navab (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> propose a shared multi-modal space constructed through visual concepts to align images and text. <cite class="ltx_cite ltx_citemacro_citet">Meng etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite> suggest harvesting objects corresponding to given sentences instead of finding candidate images. Nonetheless, these approaches depend heavily on object detectors, overlooking object attributes and relationships, constrained by detector generalization. Recent text-only training methods focus on training text decoder to reconstruct text from CLIP text encoder-derived features. During inference, they align image features extracted by CLIPâ€™s image encoder with text features in the same space. <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> introduce a training-free mechanism using training text features to project visual embeddings into text embedding space at inference. <cite class="ltx_cite ltx_citemacro_citet">Nukrai, Mokady, and Globerson (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Gu, Clark, and Kembhavi (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> propose noise injection training to reduce the modality gap during inference. However, these methods rely on CLIPâ€™s cross-modality capacity and struggle to transfer to new domains without fine-tuning CLIP.</p>
</div>
</section>
<section id="Sx2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Applications of Diffusion Models</h3>

<div id="Sx2.SSx3.p1" class="ltx_para">
<p id="Sx2.SSx3.p1.1" class="ltx_p">Diffusion models excel in generative capacities, spanning image creation, video synthesis, and text generationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ho, Jain, and Abbeel <a href="#bib.bib17" title="" class="ltx_ref">2020</a>; Dhariwal and Nichol <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Villegas etÂ al. <a href="#bib.bib44" title="" class="ltx_ref">2023</a>; Chen, Zhang, and Hinton <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>. Conditional versions enhance control and produce premium outcomes, extending their usefulness, as seen in text-to-image generation with models like DALL-E 2, Imagen, and Stable DiffusionÂ <cite class="ltx_cite ltx_citemacro_citep">(Ramesh etÂ al. <a href="#bib.bib35" title="" class="ltx_ref">2022</a>; Saharia etÂ al. <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Rombach etÂ al. <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>. Synthetic data from GLIDE demonstrated efficacy in image classificationÂ <cite class="ltx_cite ltx_citemacro_citep">(Nichol etÂ al. <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; He etÂ al. <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, with further improvements achieved through ImageNet fine-tuningÂ <cite class="ltx_cite ltx_citemacro_citep">(Azizi etÂ al. <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>.
X-PasteÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al. <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite> leverages Stable Diffusion and CLIP to obtain synthetic images with accurate categories, which are transformed into instances for image segmentation.
These tasks need high-quality synthetic images but with less focus on matching meaning exactly. Only the object linked to a single label should appear in the image, without considering the whole scene. The text-to-image diffusion model manages this basic requirement. Unlike these tasks, image captioning requires intricate scenes in synthetic images that can be described from various perspectives.
Because diffusion models cannot generate multi-context images from simple sentences, creating suitable training data for image captioning becomes quite a challenge.</p>
</div>
<figure id="Sx2.F2" class="ltx_figure"><img src="/html/2305.18072/assets/x2.png" id="Sx2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="430" height="239" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Overview of our proposed ICSD pipeline. The pipeline comprises two stages: the generation stage and the training stage. In the generation stage, we commence by performing the grouping of simple captions within the corpus. Next, LLMs are employed to select captions that depict the same scene from multiple perspectives, which are extracted from the obtained candidate sets. These selected captions are then condensed into a single sentence through summarization. These condensed sentences play a pivotal role in generating multi-context images using stable diffusion. Finally, in the training stage, we exclusively train the image captioning model on the synthetic multi-context image-text pairs.
</figcaption>
</figure>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Method</h2>

<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Overview</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.1" class="ltx_p">Our pipeline, presented in FigureÂ <a href="#Sx2.F2" title="Figure 2 â€£ Applications of Diffusion Models â€£ Related Work â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, comprises two stages: the generation stage and the training stage.
The generation stage begins with a given text corpus which comes from multiple sources for in-domain or cross-domain settings. For in-domain setting, we utilize human-annotated text to generate in-domain data. For cross-domain setting, we employ web-crawled text or text generated from LLMs with rich knowledge, to produce large-scale cross-domain data.
The generation stage consists of three steps: (1) Grouping of simple captions. In this step, for each simple caption acting as a query, we retrieve the most similar captions from the text corpus. These retrieved captions are then combined with the query caption to form a group. The captions in the same group possibly describe the same scene from diverse perspectives.
(2) LLM-based selection and summarization. Providing the group of simple captions, which includes captions that potentially describe the same scene from diverse perspectives, we meticulously design prompt.
These prompts guide LLMs to select simple captions that coherently align with a particular scene and summarize them into one sentence for image generation.
(3) Finally, we employ stable diffusion to generate images with the summarized captions.
In the training stage, we train the image captioning model solely on the synthetic multi-context image-text pairs obtained from the generation stage.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Generation Stage</h3>

<div id="Sx3.SSx2.p1" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p1.2" class="ltx_p"><span id="Sx3.SSx2.p1.2.1" class="ltx_text ltx_font_bold">Grouping of Simple Captions.</span>
Given a text corpus <math id="Sx3.SSx2.p1.1.m1.4" class="ltx_Math" alttext="T=\{t_{1},t_{2},...,t_{N}\}" display="inline"><semantics id="Sx3.SSx2.p1.1.m1.4a"><mrow id="Sx3.SSx2.p1.1.m1.4.4" xref="Sx3.SSx2.p1.1.m1.4.4.cmml"><mi id="Sx3.SSx2.p1.1.m1.4.4.5" xref="Sx3.SSx2.p1.1.m1.4.4.5.cmml">T</mi><mo id="Sx3.SSx2.p1.1.m1.4.4.4" xref="Sx3.SSx2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="Sx3.SSx2.p1.1.m1.4.4.3.3" xref="Sx3.SSx2.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="Sx3.SSx2.p1.1.m1.4.4.3.3.4" xref="Sx3.SSx2.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="Sx3.SSx2.p1.1.m1.2.2.1.1.1" xref="Sx3.SSx2.p1.1.m1.2.2.1.1.1.cmml"><mi id="Sx3.SSx2.p1.1.m1.2.2.1.1.1.2" xref="Sx3.SSx2.p1.1.m1.2.2.1.1.1.2.cmml">t</mi><mn id="Sx3.SSx2.p1.1.m1.2.2.1.1.1.3" xref="Sx3.SSx2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="Sx3.SSx2.p1.1.m1.4.4.3.3.5" xref="Sx3.SSx2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="Sx3.SSx2.p1.1.m1.3.3.2.2.2" xref="Sx3.SSx2.p1.1.m1.3.3.2.2.2.cmml"><mi id="Sx3.SSx2.p1.1.m1.3.3.2.2.2.2" xref="Sx3.SSx2.p1.1.m1.3.3.2.2.2.2.cmml">t</mi><mn id="Sx3.SSx2.p1.1.m1.3.3.2.2.2.3" xref="Sx3.SSx2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="Sx3.SSx2.p1.1.m1.4.4.3.3.6" xref="Sx3.SSx2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="Sx3.SSx2.p1.1.m1.1.1" xref="Sx3.SSx2.p1.1.m1.1.1.cmml">â€¦</mi><mo id="Sx3.SSx2.p1.1.m1.4.4.3.3.7" xref="Sx3.SSx2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="Sx3.SSx2.p1.1.m1.4.4.3.3.3" xref="Sx3.SSx2.p1.1.m1.4.4.3.3.3.cmml"><mi id="Sx3.SSx2.p1.1.m1.4.4.3.3.3.2" xref="Sx3.SSx2.p1.1.m1.4.4.3.3.3.2.cmml">t</mi><mi id="Sx3.SSx2.p1.1.m1.4.4.3.3.3.3" xref="Sx3.SSx2.p1.1.m1.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="Sx3.SSx2.p1.1.m1.4.4.3.3.8" xref="Sx3.SSx2.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.1.m1.4b"><apply id="Sx3.SSx2.p1.1.m1.4.4.cmml" xref="Sx3.SSx2.p1.1.m1.4.4"><eq id="Sx3.SSx2.p1.1.m1.4.4.4.cmml" xref="Sx3.SSx2.p1.1.m1.4.4.4"></eq><ci id="Sx3.SSx2.p1.1.m1.4.4.5.cmml" xref="Sx3.SSx2.p1.1.m1.4.4.5">ğ‘‡</ci><set id="Sx3.SSx2.p1.1.m1.4.4.3.4.cmml" xref="Sx3.SSx2.p1.1.m1.4.4.3.3"><apply id="Sx3.SSx2.p1.1.m1.2.2.1.1.1.cmml" xref="Sx3.SSx2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.1.m1.2.2.1.1.1.1.cmml" xref="Sx3.SSx2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.1.m1.2.2.1.1.1.2.cmml" xref="Sx3.SSx2.p1.1.m1.2.2.1.1.1.2">ğ‘¡</ci><cn type="integer" id="Sx3.SSx2.p1.1.m1.2.2.1.1.1.3.cmml" xref="Sx3.SSx2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="Sx3.SSx2.p1.1.m1.3.3.2.2.2.cmml" xref="Sx3.SSx2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.1.m1.3.3.2.2.2.1.cmml" xref="Sx3.SSx2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="Sx3.SSx2.p1.1.m1.3.3.2.2.2.2.cmml" xref="Sx3.SSx2.p1.1.m1.3.3.2.2.2.2">ğ‘¡</ci><cn type="integer" id="Sx3.SSx2.p1.1.m1.3.3.2.2.2.3.cmml" xref="Sx3.SSx2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="Sx3.SSx2.p1.1.m1.1.1.cmml" xref="Sx3.SSx2.p1.1.m1.1.1">â€¦</ci><apply id="Sx3.SSx2.p1.1.m1.4.4.3.3.3.cmml" xref="Sx3.SSx2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.1.m1.4.4.3.3.3.1.cmml" xref="Sx3.SSx2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="Sx3.SSx2.p1.1.m1.4.4.3.3.3.2.cmml" xref="Sx3.SSx2.p1.1.m1.4.4.3.3.3.2">ğ‘¡</ci><ci id="Sx3.SSx2.p1.1.m1.4.4.3.3.3.3.cmml" xref="Sx3.SSx2.p1.1.m1.4.4.3.3.3.3">ğ‘</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.1.m1.4c">T=\{t_{1},t_{2},...,t_{N}\}</annotation></semantics></math> with <math id="Sx3.SSx2.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx3.SSx2.p1.2.m2.1a"><mi id="Sx3.SSx2.p1.2.m2.1.1" xref="Sx3.SSx2.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.2.m2.1b"><ci id="Sx3.SSx2.p1.2.m2.1.1.cmml" xref="Sx3.SSx2.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.2.m2.1c">N</annotation></semantics></math> captions,
directly utilizing the whole text corpus as input is infeasible.
This is because the input context length limitation in LLMs prevents the use of the full corpus.
To address this issue, we propose to partition the text corpus into multiple groups of simple captions, with each group serving as a candidate set for selection and summarization.
Considering the large size of the text corpus, we form a group for each simple caption by retrieving instead of clustering algorithms.
Since captions describing the same scene often exhibit substantial semantic similarity with shared visual concepts, we employ CLIP to extract caption features and calculate the cosine similarity between each query caption and others within the corpus:</p>
<table id="Sx3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx3.E1.m1.4" class="ltx_Math" alttext="s_{ij}=\frac{f(t_{i})\cdot f(t_{j})}{||f(t_{i})||\,||f(t_{j})||}" display="block"><semantics id="Sx3.E1.m1.4a"><mrow id="Sx3.E1.m1.4.5" xref="Sx3.E1.m1.4.5.cmml"><msub id="Sx3.E1.m1.4.5.2" xref="Sx3.E1.m1.4.5.2.cmml"><mi id="Sx3.E1.m1.4.5.2.2" xref="Sx3.E1.m1.4.5.2.2.cmml">s</mi><mrow id="Sx3.E1.m1.4.5.2.3" xref="Sx3.E1.m1.4.5.2.3.cmml"><mi id="Sx3.E1.m1.4.5.2.3.2" xref="Sx3.E1.m1.4.5.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.5.2.3.1" xref="Sx3.E1.m1.4.5.2.3.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.4.5.2.3.3" xref="Sx3.E1.m1.4.5.2.3.3.cmml">j</mi></mrow></msub><mo id="Sx3.E1.m1.4.5.1" xref="Sx3.E1.m1.4.5.1.cmml">=</mo><mfrac id="Sx3.E1.m1.4.4" xref="Sx3.E1.m1.4.4.cmml"><mrow id="Sx3.E1.m1.2.2.2" xref="Sx3.E1.m1.2.2.2.cmml"><mrow id="Sx3.E1.m1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.cmml"><mrow id="Sx3.E1.m1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx3.E1.m1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.E1.m1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="Sx3.E1.m1.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">t</mi><mi id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo rspace="0.055em" stretchy="false" id="Sx3.E1.m1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="Sx3.E1.m1.1.1.1.1.2" xref="Sx3.E1.m1.1.1.1.1.2.cmml">â‹…</mo><mi id="Sx3.E1.m1.1.1.1.1.3" xref="Sx3.E1.m1.1.1.1.1.3.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.2.2.2.3" xref="Sx3.E1.m1.2.2.2.3.cmml">â€‹</mo><mrow id="Sx3.E1.m1.2.2.2.2.1" xref="Sx3.E1.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="Sx3.E1.m1.2.2.2.2.1.2" xref="Sx3.E1.m1.2.2.2.2.1.1.cmml">(</mo><msub id="Sx3.E1.m1.2.2.2.2.1.1" xref="Sx3.E1.m1.2.2.2.2.1.1.cmml"><mi id="Sx3.E1.m1.2.2.2.2.1.1.2" xref="Sx3.E1.m1.2.2.2.2.1.1.2.cmml">t</mi><mi id="Sx3.E1.m1.2.2.2.2.1.1.3" xref="Sx3.E1.m1.2.2.2.2.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="Sx3.E1.m1.2.2.2.2.1.3" xref="Sx3.E1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mrow id="Sx3.E1.m1.4.4.4" xref="Sx3.E1.m1.4.4.4.cmml"><mrow id="Sx3.E1.m1.3.3.3.1.1" xref="Sx3.E1.m1.3.3.3.1.2.cmml"><mo stretchy="false" id="Sx3.E1.m1.3.3.3.1.1.2" xref="Sx3.E1.m1.3.3.3.1.2.1.cmml">â€–</mo><mrow id="Sx3.E1.m1.3.3.3.1.1.1" xref="Sx3.E1.m1.3.3.3.1.1.1.cmml"><mi id="Sx3.E1.m1.3.3.3.1.1.1.3" xref="Sx3.E1.m1.3.3.3.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.3.3.3.1.1.1.2" xref="Sx3.E1.m1.3.3.3.1.1.1.2.cmml">â€‹</mo><mrow id="Sx3.E1.m1.3.3.3.1.1.1.1.1" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.E1.m1.3.3.3.1.1.1.1.1.2" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="Sx3.E1.m1.3.3.3.1.1.1.1.1.1" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.2" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.2.cmml">t</mi><mi id="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.3" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx3.E1.m1.3.3.3.1.1.1.1.1.3" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="Sx3.E1.m1.3.3.3.1.1.3" xref="Sx3.E1.m1.3.3.3.1.2.1.cmml">â€–</mo></mrow><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.4.4.3" xref="Sx3.E1.m1.4.4.4.3.cmml">â€‹</mo><mrow id="Sx3.E1.m1.4.4.4.2.1" xref="Sx3.E1.m1.4.4.4.2.2.cmml"><mo stretchy="false" id="Sx3.E1.m1.4.4.4.2.1.2" xref="Sx3.E1.m1.4.4.4.2.2.1.cmml">â€–</mo><mrow id="Sx3.E1.m1.4.4.4.2.1.1" xref="Sx3.E1.m1.4.4.4.2.1.1.cmml"><mi id="Sx3.E1.m1.4.4.4.2.1.1.3" xref="Sx3.E1.m1.4.4.4.2.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.4.4.2.1.1.2" xref="Sx3.E1.m1.4.4.4.2.1.1.2.cmml">â€‹</mo><mrow id="Sx3.E1.m1.4.4.4.2.1.1.1.1" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.E1.m1.4.4.4.2.1.1.1.1.2" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.cmml">(</mo><msub id="Sx3.E1.m1.4.4.4.2.1.1.1.1.1" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.2" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.2.cmml">t</mi><mi id="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.3" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="Sx3.E1.m1.4.4.4.2.1.1.1.1.3" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="Sx3.E1.m1.4.4.4.2.1.3" xref="Sx3.E1.m1.4.4.4.2.2.1.cmml">â€–</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E1.m1.4b"><apply id="Sx3.E1.m1.4.5.cmml" xref="Sx3.E1.m1.4.5"><eq id="Sx3.E1.m1.4.5.1.cmml" xref="Sx3.E1.m1.4.5.1"></eq><apply id="Sx3.E1.m1.4.5.2.cmml" xref="Sx3.E1.m1.4.5.2"><csymbol cd="ambiguous" id="Sx3.E1.m1.4.5.2.1.cmml" xref="Sx3.E1.m1.4.5.2">subscript</csymbol><ci id="Sx3.E1.m1.4.5.2.2.cmml" xref="Sx3.E1.m1.4.5.2.2">ğ‘ </ci><apply id="Sx3.E1.m1.4.5.2.3.cmml" xref="Sx3.E1.m1.4.5.2.3"><times id="Sx3.E1.m1.4.5.2.3.1.cmml" xref="Sx3.E1.m1.4.5.2.3.1"></times><ci id="Sx3.E1.m1.4.5.2.3.2.cmml" xref="Sx3.E1.m1.4.5.2.3.2">ğ‘–</ci><ci id="Sx3.E1.m1.4.5.2.3.3.cmml" xref="Sx3.E1.m1.4.5.2.3.3">ğ‘—</ci></apply></apply><apply id="Sx3.E1.m1.4.4.cmml" xref="Sx3.E1.m1.4.4"><divide id="Sx3.E1.m1.4.4.5.cmml" xref="Sx3.E1.m1.4.4"></divide><apply id="Sx3.E1.m1.2.2.2.cmml" xref="Sx3.E1.m1.2.2.2"><times id="Sx3.E1.m1.2.2.2.3.cmml" xref="Sx3.E1.m1.2.2.2.3"></times><apply id="Sx3.E1.m1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1"><ci id="Sx3.E1.m1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.1.1.2">â‹…</ci><apply id="Sx3.E1.m1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1"><times id="Sx3.E1.m1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.1.1.1.2"></times><ci id="Sx3.E1.m1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.1.1.1.3">ğ‘“</ci><apply id="Sx3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="Sx3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><ci id="Sx3.E1.m1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.1.1.3">ğ‘“</ci></apply><apply id="Sx3.E1.m1.2.2.2.2.1.1.cmml" xref="Sx3.E1.m1.2.2.2.2.1"><csymbol cd="ambiguous" id="Sx3.E1.m1.2.2.2.2.1.1.1.cmml" xref="Sx3.E1.m1.2.2.2.2.1">subscript</csymbol><ci id="Sx3.E1.m1.2.2.2.2.1.1.2.cmml" xref="Sx3.E1.m1.2.2.2.2.1.1.2">ğ‘¡</ci><ci id="Sx3.E1.m1.2.2.2.2.1.1.3.cmml" xref="Sx3.E1.m1.2.2.2.2.1.1.3">ğ‘—</ci></apply></apply><apply id="Sx3.E1.m1.4.4.4.cmml" xref="Sx3.E1.m1.4.4.4"><times id="Sx3.E1.m1.4.4.4.3.cmml" xref="Sx3.E1.m1.4.4.4.3"></times><apply id="Sx3.E1.m1.3.3.3.1.2.cmml" xref="Sx3.E1.m1.3.3.3.1.1"><csymbol cd="latexml" id="Sx3.E1.m1.3.3.3.1.2.1.cmml" xref="Sx3.E1.m1.3.3.3.1.1.2">norm</csymbol><apply id="Sx3.E1.m1.3.3.3.1.1.1.cmml" xref="Sx3.E1.m1.3.3.3.1.1.1"><times id="Sx3.E1.m1.3.3.3.1.1.1.2.cmml" xref="Sx3.E1.m1.3.3.3.1.1.1.2"></times><ci id="Sx3.E1.m1.3.3.3.1.1.1.3.cmml" xref="Sx3.E1.m1.3.3.3.1.1.1.3">ğ‘“</ci><apply id="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1">subscript</csymbol><ci id="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.3.3.3.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply><apply id="Sx3.E1.m1.4.4.4.2.2.cmml" xref="Sx3.E1.m1.4.4.4.2.1"><csymbol cd="latexml" id="Sx3.E1.m1.4.4.4.2.2.1.cmml" xref="Sx3.E1.m1.4.4.4.2.1.2">norm</csymbol><apply id="Sx3.E1.m1.4.4.4.2.1.1.cmml" xref="Sx3.E1.m1.4.4.4.2.1.1"><times id="Sx3.E1.m1.4.4.4.2.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.2.1.1.2"></times><ci id="Sx3.E1.m1.4.4.4.2.1.1.3.cmml" xref="Sx3.E1.m1.4.4.4.2.1.1.3">ğ‘“</ci><apply id="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.cmml" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1">subscript</csymbol><ci id="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.2">ğ‘¡</ci><ci id="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.4.4.4.2.1.1.1.1.1.3">ğ‘—</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E1.m1.4c">s_{ij}=\frac{f(t_{i})\cdot f(t_{j})}{||f(t_{i})||\,||f(t_{j})||}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="Sx3.SSx2.p1.12" class="ltx_p">where <math id="Sx3.SSx2.p1.3.m1.1" class="ltx_Math" alttext="s_{ij}" display="inline"><semantics id="Sx3.SSx2.p1.3.m1.1a"><msub id="Sx3.SSx2.p1.3.m1.1.1" xref="Sx3.SSx2.p1.3.m1.1.1.cmml"><mi id="Sx3.SSx2.p1.3.m1.1.1.2" xref="Sx3.SSx2.p1.3.m1.1.1.2.cmml">s</mi><mrow id="Sx3.SSx2.p1.3.m1.1.1.3" xref="Sx3.SSx2.p1.3.m1.1.1.3.cmml"><mi id="Sx3.SSx2.p1.3.m1.1.1.3.2" xref="Sx3.SSx2.p1.3.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.3.m1.1.1.3.1" xref="Sx3.SSx2.p1.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="Sx3.SSx2.p1.3.m1.1.1.3.3" xref="Sx3.SSx2.p1.3.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.3.m1.1b"><apply id="Sx3.SSx2.p1.3.m1.1.1.cmml" xref="Sx3.SSx2.p1.3.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.3.m1.1.1.1.cmml" xref="Sx3.SSx2.p1.3.m1.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.3.m1.1.1.2.cmml" xref="Sx3.SSx2.p1.3.m1.1.1.2">ğ‘ </ci><apply id="Sx3.SSx2.p1.3.m1.1.1.3.cmml" xref="Sx3.SSx2.p1.3.m1.1.1.3"><times id="Sx3.SSx2.p1.3.m1.1.1.3.1.cmml" xref="Sx3.SSx2.p1.3.m1.1.1.3.1"></times><ci id="Sx3.SSx2.p1.3.m1.1.1.3.2.cmml" xref="Sx3.SSx2.p1.3.m1.1.1.3.2">ğ‘–</ci><ci id="Sx3.SSx2.p1.3.m1.1.1.3.3.cmml" xref="Sx3.SSx2.p1.3.m1.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.3.m1.1c">s_{ij}</annotation></semantics></math> is the cosine similarity between <math id="Sx3.SSx2.p1.4.m2.1" class="ltx_Math" alttext="f(t_{i})" display="inline"><semantics id="Sx3.SSx2.p1.4.m2.1a"><mrow id="Sx3.SSx2.p1.4.m2.1.1" xref="Sx3.SSx2.p1.4.m2.1.1.cmml"><mi id="Sx3.SSx2.p1.4.m2.1.1.3" xref="Sx3.SSx2.p1.4.m2.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.4.m2.1.1.2" xref="Sx3.SSx2.p1.4.m2.1.1.2.cmml">â€‹</mo><mrow id="Sx3.SSx2.p1.4.m2.1.1.1.1" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.SSx2.p1.4.m2.1.1.1.1.2" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.cmml">(</mo><msub id="Sx3.SSx2.p1.4.m2.1.1.1.1.1" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.cmml"><mi id="Sx3.SSx2.p1.4.m2.1.1.1.1.1.2" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.2.cmml">t</mi><mi id="Sx3.SSx2.p1.4.m2.1.1.1.1.1.3" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx3.SSx2.p1.4.m2.1.1.1.1.3" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.4.m2.1b"><apply id="Sx3.SSx2.p1.4.m2.1.1.cmml" xref="Sx3.SSx2.p1.4.m2.1.1"><times id="Sx3.SSx2.p1.4.m2.1.1.2.cmml" xref="Sx3.SSx2.p1.4.m2.1.1.2"></times><ci id="Sx3.SSx2.p1.4.m2.1.1.3.cmml" xref="Sx3.SSx2.p1.4.m2.1.1.3">ğ‘“</ci><apply id="Sx3.SSx2.p1.4.m2.1.1.1.1.1.cmml" xref="Sx3.SSx2.p1.4.m2.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.4.m2.1.1.1.1.1.1.cmml" xref="Sx3.SSx2.p1.4.m2.1.1.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.4.m2.1.1.1.1.1.2.cmml" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.2">ğ‘¡</ci><ci id="Sx3.SSx2.p1.4.m2.1.1.1.1.1.3.cmml" xref="Sx3.SSx2.p1.4.m2.1.1.1.1.1.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.4.m2.1c">f(t_{i})</annotation></semantics></math> and <math id="Sx3.SSx2.p1.5.m3.1" class="ltx_Math" alttext="f(t_{j})" display="inline"><semantics id="Sx3.SSx2.p1.5.m3.1a"><mrow id="Sx3.SSx2.p1.5.m3.1.1" xref="Sx3.SSx2.p1.5.m3.1.1.cmml"><mi id="Sx3.SSx2.p1.5.m3.1.1.3" xref="Sx3.SSx2.p1.5.m3.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.5.m3.1.1.2" xref="Sx3.SSx2.p1.5.m3.1.1.2.cmml">â€‹</mo><mrow id="Sx3.SSx2.p1.5.m3.1.1.1.1" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.SSx2.p1.5.m3.1.1.1.1.2" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.cmml">(</mo><msub id="Sx3.SSx2.p1.5.m3.1.1.1.1.1" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.cmml"><mi id="Sx3.SSx2.p1.5.m3.1.1.1.1.1.2" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.2.cmml">t</mi><mi id="Sx3.SSx2.p1.5.m3.1.1.1.1.1.3" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="Sx3.SSx2.p1.5.m3.1.1.1.1.3" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.5.m3.1b"><apply id="Sx3.SSx2.p1.5.m3.1.1.cmml" xref="Sx3.SSx2.p1.5.m3.1.1"><times id="Sx3.SSx2.p1.5.m3.1.1.2.cmml" xref="Sx3.SSx2.p1.5.m3.1.1.2"></times><ci id="Sx3.SSx2.p1.5.m3.1.1.3.cmml" xref="Sx3.SSx2.p1.5.m3.1.1.3">ğ‘“</ci><apply id="Sx3.SSx2.p1.5.m3.1.1.1.1.1.cmml" xref="Sx3.SSx2.p1.5.m3.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.5.m3.1.1.1.1.1.1.cmml" xref="Sx3.SSx2.p1.5.m3.1.1.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.5.m3.1.1.1.1.1.2.cmml" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.2">ğ‘¡</ci><ci id="Sx3.SSx2.p1.5.m3.1.1.1.1.1.3.cmml" xref="Sx3.SSx2.p1.5.m3.1.1.1.1.1.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.5.m3.1c">f(t_{j})</annotation></semantics></math>, <math id="Sx3.SSx2.p1.6.m4.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="Sx3.SSx2.p1.6.m4.1a"><msub id="Sx3.SSx2.p1.6.m4.1.1" xref="Sx3.SSx2.p1.6.m4.1.1.cmml"><mi id="Sx3.SSx2.p1.6.m4.1.1.2" xref="Sx3.SSx2.p1.6.m4.1.1.2.cmml">t</mi><mi id="Sx3.SSx2.p1.6.m4.1.1.3" xref="Sx3.SSx2.p1.6.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.6.m4.1b"><apply id="Sx3.SSx2.p1.6.m4.1.1.cmml" xref="Sx3.SSx2.p1.6.m4.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.6.m4.1.1.1.cmml" xref="Sx3.SSx2.p1.6.m4.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.6.m4.1.1.2.cmml" xref="Sx3.SSx2.p1.6.m4.1.1.2">ğ‘¡</ci><ci id="Sx3.SSx2.p1.6.m4.1.1.3.cmml" xref="Sx3.SSx2.p1.6.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.6.m4.1c">t_{i}</annotation></semantics></math> is the query caption, <math id="Sx3.SSx2.p1.7.m5.1" class="ltx_Math" alttext="t_{j}" display="inline"><semantics id="Sx3.SSx2.p1.7.m5.1a"><msub id="Sx3.SSx2.p1.7.m5.1.1" xref="Sx3.SSx2.p1.7.m5.1.1.cmml"><mi id="Sx3.SSx2.p1.7.m5.1.1.2" xref="Sx3.SSx2.p1.7.m5.1.1.2.cmml">t</mi><mi id="Sx3.SSx2.p1.7.m5.1.1.3" xref="Sx3.SSx2.p1.7.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.7.m5.1b"><apply id="Sx3.SSx2.p1.7.m5.1.1.cmml" xref="Sx3.SSx2.p1.7.m5.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.7.m5.1.1.1.cmml" xref="Sx3.SSx2.p1.7.m5.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.7.m5.1.1.2.cmml" xref="Sx3.SSx2.p1.7.m5.1.1.2">ğ‘¡</ci><ci id="Sx3.SSx2.p1.7.m5.1.1.3.cmml" xref="Sx3.SSx2.p1.7.m5.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.7.m5.1c">t_{j}</annotation></semantics></math> is another caption in corpus, <math id="Sx3.SSx2.p1.8.m6.1" class="ltx_Math" alttext="f(\cdot)" display="inline"><semantics id="Sx3.SSx2.p1.8.m6.1a"><mrow id="Sx3.SSx2.p1.8.m6.1.2" xref="Sx3.SSx2.p1.8.m6.1.2.cmml"><mi id="Sx3.SSx2.p1.8.m6.1.2.2" xref="Sx3.SSx2.p1.8.m6.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.8.m6.1.2.1" xref="Sx3.SSx2.p1.8.m6.1.2.1.cmml">â€‹</mo><mrow id="Sx3.SSx2.p1.8.m6.1.2.3.2" xref="Sx3.SSx2.p1.8.m6.1.2.cmml"><mo stretchy="false" id="Sx3.SSx2.p1.8.m6.1.2.3.2.1" xref="Sx3.SSx2.p1.8.m6.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.8.m6.1.1" xref="Sx3.SSx2.p1.8.m6.1.1.cmml">â‹…</mo><mo stretchy="false" id="Sx3.SSx2.p1.8.m6.1.2.3.2.2" xref="Sx3.SSx2.p1.8.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.8.m6.1b"><apply id="Sx3.SSx2.p1.8.m6.1.2.cmml" xref="Sx3.SSx2.p1.8.m6.1.2"><times id="Sx3.SSx2.p1.8.m6.1.2.1.cmml" xref="Sx3.SSx2.p1.8.m6.1.2.1"></times><ci id="Sx3.SSx2.p1.8.m6.1.2.2.cmml" xref="Sx3.SSx2.p1.8.m6.1.2.2">ğ‘“</ci><ci id="Sx3.SSx2.p1.8.m6.1.1.cmml" xref="Sx3.SSx2.p1.8.m6.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.8.m6.1c">f(\cdot)</annotation></semantics></math> represents the text encoder of CLIP.
For query caption <math id="Sx3.SSx2.p1.9.m7.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="Sx3.SSx2.p1.9.m7.1a"><msub id="Sx3.SSx2.p1.9.m7.1.1" xref="Sx3.SSx2.p1.9.m7.1.1.cmml"><mi id="Sx3.SSx2.p1.9.m7.1.1.2" xref="Sx3.SSx2.p1.9.m7.1.1.2.cmml">t</mi><mi id="Sx3.SSx2.p1.9.m7.1.1.3" xref="Sx3.SSx2.p1.9.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.9.m7.1b"><apply id="Sx3.SSx2.p1.9.m7.1.1.cmml" xref="Sx3.SSx2.p1.9.m7.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.9.m7.1.1.1.cmml" xref="Sx3.SSx2.p1.9.m7.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.9.m7.1.1.2.cmml" xref="Sx3.SSx2.p1.9.m7.1.1.2">ğ‘¡</ci><ci id="Sx3.SSx2.p1.9.m7.1.1.3.cmml" xref="Sx3.SSx2.p1.9.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.9.m7.1c">t_{i}</annotation></semantics></math>, we retrieve the top <math id="Sx3.SSx2.p1.10.m8.1" class="ltx_Math" alttext="k" display="inline"><semantics id="Sx3.SSx2.p1.10.m8.1a"><mi id="Sx3.SSx2.p1.10.m8.1.1" xref="Sx3.SSx2.p1.10.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.10.m8.1b"><ci id="Sx3.SSx2.p1.10.m8.1.1.cmml" xref="Sx3.SSx2.p1.10.m8.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.10.m8.1c">k</annotation></semantics></math> similar sentences to form a group <math id="Sx3.SSx2.p1.11.m9.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="Sx3.SSx2.p1.11.m9.1a"><msub id="Sx3.SSx2.p1.11.m9.1.1" xref="Sx3.SSx2.p1.11.m9.1.1.cmml"><mi id="Sx3.SSx2.p1.11.m9.1.1.2" xref="Sx3.SSx2.p1.11.m9.1.1.2.cmml">G</mi><mi id="Sx3.SSx2.p1.11.m9.1.1.3" xref="Sx3.SSx2.p1.11.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.11.m9.1b"><apply id="Sx3.SSx2.p1.11.m9.1.1.cmml" xref="Sx3.SSx2.p1.11.m9.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.11.m9.1.1.1.cmml" xref="Sx3.SSx2.p1.11.m9.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.11.m9.1.1.2.cmml" xref="Sx3.SSx2.p1.11.m9.1.1.2">ğº</ci><ci id="Sx3.SSx2.p1.11.m9.1.1.3.cmml" xref="Sx3.SSx2.p1.11.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.11.m9.1c">G_{i}</annotation></semantics></math> including <math id="Sx3.SSx2.p1.12.m10.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="Sx3.SSx2.p1.12.m10.1a"><msub id="Sx3.SSx2.p1.12.m10.1.1" xref="Sx3.SSx2.p1.12.m10.1.1.cmml"><mi id="Sx3.SSx2.p1.12.m10.1.1.2" xref="Sx3.SSx2.p1.12.m10.1.1.2.cmml">t</mi><mi id="Sx3.SSx2.p1.12.m10.1.1.3" xref="Sx3.SSx2.p1.12.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.12.m10.1b"><apply id="Sx3.SSx2.p1.12.m10.1.1.cmml" xref="Sx3.SSx2.p1.12.m10.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.12.m10.1.1.1.cmml" xref="Sx3.SSx2.p1.12.m10.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.12.m10.1.1.2.cmml" xref="Sx3.SSx2.p1.12.m10.1.1.2">ğ‘¡</ci><ci id="Sx3.SSx2.p1.12.m10.1.1.3.cmml" xref="Sx3.SSx2.p1.12.m10.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.12.m10.1c">t_{i}</annotation></semantics></math>:</p>
<table id="Sx3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx3.E2.m1.4" class="ltx_Math" alttext="G_{i}=\{t_{i},t_{m},\dots,t_{n}\}" display="block"><semantics id="Sx3.E2.m1.4a"><mrow id="Sx3.E2.m1.4.4" xref="Sx3.E2.m1.4.4.cmml"><msub id="Sx3.E2.m1.4.4.5" xref="Sx3.E2.m1.4.4.5.cmml"><mi id="Sx3.E2.m1.4.4.5.2" xref="Sx3.E2.m1.4.4.5.2.cmml">G</mi><mi id="Sx3.E2.m1.4.4.5.3" xref="Sx3.E2.m1.4.4.5.3.cmml">i</mi></msub><mo id="Sx3.E2.m1.4.4.4" xref="Sx3.E2.m1.4.4.4.cmml">=</mo><mrow id="Sx3.E2.m1.4.4.3.3" xref="Sx3.E2.m1.4.4.3.4.cmml"><mo stretchy="false" id="Sx3.E2.m1.4.4.3.3.4" xref="Sx3.E2.m1.4.4.3.4.cmml">{</mo><msub id="Sx3.E2.m1.2.2.1.1.1" xref="Sx3.E2.m1.2.2.1.1.1.cmml"><mi id="Sx3.E2.m1.2.2.1.1.1.2" xref="Sx3.E2.m1.2.2.1.1.1.2.cmml">t</mi><mi id="Sx3.E2.m1.2.2.1.1.1.3" xref="Sx3.E2.m1.2.2.1.1.1.3.cmml">i</mi></msub><mo id="Sx3.E2.m1.4.4.3.3.5" xref="Sx3.E2.m1.4.4.3.4.cmml">,</mo><msub id="Sx3.E2.m1.3.3.2.2.2" xref="Sx3.E2.m1.3.3.2.2.2.cmml"><mi id="Sx3.E2.m1.3.3.2.2.2.2" xref="Sx3.E2.m1.3.3.2.2.2.2.cmml">t</mi><mi id="Sx3.E2.m1.3.3.2.2.2.3" xref="Sx3.E2.m1.3.3.2.2.2.3.cmml">m</mi></msub><mo id="Sx3.E2.m1.4.4.3.3.6" xref="Sx3.E2.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="Sx3.E2.m1.1.1" xref="Sx3.E2.m1.1.1.cmml">â€¦</mi><mo id="Sx3.E2.m1.4.4.3.3.7" xref="Sx3.E2.m1.4.4.3.4.cmml">,</mo><msub id="Sx3.E2.m1.4.4.3.3.3" xref="Sx3.E2.m1.4.4.3.3.3.cmml"><mi id="Sx3.E2.m1.4.4.3.3.3.2" xref="Sx3.E2.m1.4.4.3.3.3.2.cmml">t</mi><mi id="Sx3.E2.m1.4.4.3.3.3.3" xref="Sx3.E2.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="Sx3.E2.m1.4.4.3.3.8" xref="Sx3.E2.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E2.m1.4b"><apply id="Sx3.E2.m1.4.4.cmml" xref="Sx3.E2.m1.4.4"><eq id="Sx3.E2.m1.4.4.4.cmml" xref="Sx3.E2.m1.4.4.4"></eq><apply id="Sx3.E2.m1.4.4.5.cmml" xref="Sx3.E2.m1.4.4.5"><csymbol cd="ambiguous" id="Sx3.E2.m1.4.4.5.1.cmml" xref="Sx3.E2.m1.4.4.5">subscript</csymbol><ci id="Sx3.E2.m1.4.4.5.2.cmml" xref="Sx3.E2.m1.4.4.5.2">ğº</ci><ci id="Sx3.E2.m1.4.4.5.3.cmml" xref="Sx3.E2.m1.4.4.5.3">ğ‘–</ci></apply><set id="Sx3.E2.m1.4.4.3.4.cmml" xref="Sx3.E2.m1.4.4.3.3"><apply id="Sx3.E2.m1.2.2.1.1.1.cmml" xref="Sx3.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="Sx3.E2.m1.2.2.1.1.1.1.cmml" xref="Sx3.E2.m1.2.2.1.1.1">subscript</csymbol><ci id="Sx3.E2.m1.2.2.1.1.1.2.cmml" xref="Sx3.E2.m1.2.2.1.1.1.2">ğ‘¡</ci><ci id="Sx3.E2.m1.2.2.1.1.1.3.cmml" xref="Sx3.E2.m1.2.2.1.1.1.3">ğ‘–</ci></apply><apply id="Sx3.E2.m1.3.3.2.2.2.cmml" xref="Sx3.E2.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="Sx3.E2.m1.3.3.2.2.2.1.cmml" xref="Sx3.E2.m1.3.3.2.2.2">subscript</csymbol><ci id="Sx3.E2.m1.3.3.2.2.2.2.cmml" xref="Sx3.E2.m1.3.3.2.2.2.2">ğ‘¡</ci><ci id="Sx3.E2.m1.3.3.2.2.2.3.cmml" xref="Sx3.E2.m1.3.3.2.2.2.3">ğ‘š</ci></apply><ci id="Sx3.E2.m1.1.1.cmml" xref="Sx3.E2.m1.1.1">â€¦</ci><apply id="Sx3.E2.m1.4.4.3.3.3.cmml" xref="Sx3.E2.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="Sx3.E2.m1.4.4.3.3.3.1.cmml" xref="Sx3.E2.m1.4.4.3.3.3">subscript</csymbol><ci id="Sx3.E2.m1.4.4.3.3.3.2.cmml" xref="Sx3.E2.m1.4.4.3.3.3.2">ğ‘¡</ci><ci id="Sx3.E2.m1.4.4.3.3.3.3.cmml" xref="Sx3.E2.m1.4.4.3.3.3.3">ğ‘›</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E2.m1.4c">G_{i}=\{t_{i},t_{m},\dots,t_{n}\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="Sx3.SSx2.p1.22" class="ltx_p">where the cardinality of <math id="Sx3.SSx2.p1.13.m1.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="Sx3.SSx2.p1.13.m1.1a"><msub id="Sx3.SSx2.p1.13.m1.1.1" xref="Sx3.SSx2.p1.13.m1.1.1.cmml"><mi id="Sx3.SSx2.p1.13.m1.1.1.2" xref="Sx3.SSx2.p1.13.m1.1.1.2.cmml">G</mi><mi id="Sx3.SSx2.p1.13.m1.1.1.3" xref="Sx3.SSx2.p1.13.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.13.m1.1b"><apply id="Sx3.SSx2.p1.13.m1.1.1.cmml" xref="Sx3.SSx2.p1.13.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.13.m1.1.1.1.cmml" xref="Sx3.SSx2.p1.13.m1.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.13.m1.1.1.2.cmml" xref="Sx3.SSx2.p1.13.m1.1.1.2">ğº</ci><ci id="Sx3.SSx2.p1.13.m1.1.1.3.cmml" xref="Sx3.SSx2.p1.13.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.13.m1.1c">G_{i}</annotation></semantics></math> is <math id="Sx3.SSx2.p1.14.m2.1" class="ltx_Math" alttext="k+1" display="inline"><semantics id="Sx3.SSx2.p1.14.m2.1a"><mrow id="Sx3.SSx2.p1.14.m2.1.1" xref="Sx3.SSx2.p1.14.m2.1.1.cmml"><mi id="Sx3.SSx2.p1.14.m2.1.1.2" xref="Sx3.SSx2.p1.14.m2.1.1.2.cmml">k</mi><mo id="Sx3.SSx2.p1.14.m2.1.1.1" xref="Sx3.SSx2.p1.14.m2.1.1.1.cmml">+</mo><mn id="Sx3.SSx2.p1.14.m2.1.1.3" xref="Sx3.SSx2.p1.14.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.14.m2.1b"><apply id="Sx3.SSx2.p1.14.m2.1.1.cmml" xref="Sx3.SSx2.p1.14.m2.1.1"><plus id="Sx3.SSx2.p1.14.m2.1.1.1.cmml" xref="Sx3.SSx2.p1.14.m2.1.1.1"></plus><ci id="Sx3.SSx2.p1.14.m2.1.1.2.cmml" xref="Sx3.SSx2.p1.14.m2.1.1.2">ğ‘˜</ci><cn type="integer" id="Sx3.SSx2.p1.14.m2.1.1.3.cmml" xref="Sx3.SSx2.p1.14.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.14.m2.1c">k+1</annotation></semantics></math>, and <math id="Sx3.SSx2.p1.15.m3.1" class="ltx_Math" alttext="s_{ii}\geq s_{im}\geq...\geq s_{in}" display="inline"><semantics id="Sx3.SSx2.p1.15.m3.1a"><mrow id="Sx3.SSx2.p1.15.m3.1.1" xref="Sx3.SSx2.p1.15.m3.1.1.cmml"><msub id="Sx3.SSx2.p1.15.m3.1.1.2" xref="Sx3.SSx2.p1.15.m3.1.1.2.cmml"><mi id="Sx3.SSx2.p1.15.m3.1.1.2.2" xref="Sx3.SSx2.p1.15.m3.1.1.2.2.cmml">s</mi><mrow id="Sx3.SSx2.p1.15.m3.1.1.2.3" xref="Sx3.SSx2.p1.15.m3.1.1.2.3.cmml"><mi id="Sx3.SSx2.p1.15.m3.1.1.2.3.2" xref="Sx3.SSx2.p1.15.m3.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.15.m3.1.1.2.3.1" xref="Sx3.SSx2.p1.15.m3.1.1.2.3.1.cmml">â€‹</mo><mi id="Sx3.SSx2.p1.15.m3.1.1.2.3.3" xref="Sx3.SSx2.p1.15.m3.1.1.2.3.3.cmml">i</mi></mrow></msub><mo id="Sx3.SSx2.p1.15.m3.1.1.3" xref="Sx3.SSx2.p1.15.m3.1.1.3.cmml">â‰¥</mo><msub id="Sx3.SSx2.p1.15.m3.1.1.4" xref="Sx3.SSx2.p1.15.m3.1.1.4.cmml"><mi id="Sx3.SSx2.p1.15.m3.1.1.4.2" xref="Sx3.SSx2.p1.15.m3.1.1.4.2.cmml">s</mi><mrow id="Sx3.SSx2.p1.15.m3.1.1.4.3" xref="Sx3.SSx2.p1.15.m3.1.1.4.3.cmml"><mi id="Sx3.SSx2.p1.15.m3.1.1.4.3.2" xref="Sx3.SSx2.p1.15.m3.1.1.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.15.m3.1.1.4.3.1" xref="Sx3.SSx2.p1.15.m3.1.1.4.3.1.cmml">â€‹</mo><mi id="Sx3.SSx2.p1.15.m3.1.1.4.3.3" xref="Sx3.SSx2.p1.15.m3.1.1.4.3.3.cmml">m</mi></mrow></msub><mo id="Sx3.SSx2.p1.15.m3.1.1.5" xref="Sx3.SSx2.p1.15.m3.1.1.5.cmml">â‰¥</mo><mi mathvariant="normal" id="Sx3.SSx2.p1.15.m3.1.1.6" xref="Sx3.SSx2.p1.15.m3.1.1.6.cmml">â€¦</mi><mo id="Sx3.SSx2.p1.15.m3.1.1.7" xref="Sx3.SSx2.p1.15.m3.1.1.7.cmml">â‰¥</mo><msub id="Sx3.SSx2.p1.15.m3.1.1.8" xref="Sx3.SSx2.p1.15.m3.1.1.8.cmml"><mi id="Sx3.SSx2.p1.15.m3.1.1.8.2" xref="Sx3.SSx2.p1.15.m3.1.1.8.2.cmml">s</mi><mrow id="Sx3.SSx2.p1.15.m3.1.1.8.3" xref="Sx3.SSx2.p1.15.m3.1.1.8.3.cmml"><mi id="Sx3.SSx2.p1.15.m3.1.1.8.3.2" xref="Sx3.SSx2.p1.15.m3.1.1.8.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.15.m3.1.1.8.3.1" xref="Sx3.SSx2.p1.15.m3.1.1.8.3.1.cmml">â€‹</mo><mi id="Sx3.SSx2.p1.15.m3.1.1.8.3.3" xref="Sx3.SSx2.p1.15.m3.1.1.8.3.3.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.15.m3.1b"><apply id="Sx3.SSx2.p1.15.m3.1.1.cmml" xref="Sx3.SSx2.p1.15.m3.1.1"><and id="Sx3.SSx2.p1.15.m3.1.1a.cmml" xref="Sx3.SSx2.p1.15.m3.1.1"></and><apply id="Sx3.SSx2.p1.15.m3.1.1b.cmml" xref="Sx3.SSx2.p1.15.m3.1.1"><geq id="Sx3.SSx2.p1.15.m3.1.1.3.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.3"></geq><apply id="Sx3.SSx2.p1.15.m3.1.1.2.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.2"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.15.m3.1.1.2.1.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.2">subscript</csymbol><ci id="Sx3.SSx2.p1.15.m3.1.1.2.2.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.2.2">ğ‘ </ci><apply id="Sx3.SSx2.p1.15.m3.1.1.2.3.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.2.3"><times id="Sx3.SSx2.p1.15.m3.1.1.2.3.1.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.2.3.1"></times><ci id="Sx3.SSx2.p1.15.m3.1.1.2.3.2.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.2.3.2">ğ‘–</ci><ci id="Sx3.SSx2.p1.15.m3.1.1.2.3.3.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.2.3.3">ğ‘–</ci></apply></apply><apply id="Sx3.SSx2.p1.15.m3.1.1.4.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.15.m3.1.1.4.1.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.4">subscript</csymbol><ci id="Sx3.SSx2.p1.15.m3.1.1.4.2.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.4.2">ğ‘ </ci><apply id="Sx3.SSx2.p1.15.m3.1.1.4.3.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.4.3"><times id="Sx3.SSx2.p1.15.m3.1.1.4.3.1.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.4.3.1"></times><ci id="Sx3.SSx2.p1.15.m3.1.1.4.3.2.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.4.3.2">ğ‘–</ci><ci id="Sx3.SSx2.p1.15.m3.1.1.4.3.3.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.4.3.3">ğ‘š</ci></apply></apply></apply><apply id="Sx3.SSx2.p1.15.m3.1.1c.cmml" xref="Sx3.SSx2.p1.15.m3.1.1"><geq id="Sx3.SSx2.p1.15.m3.1.1.5.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.5"></geq><share href="#Sx3.SSx2.p1.15.m3.1.1.4.cmml" id="Sx3.SSx2.p1.15.m3.1.1d.cmml" xref="Sx3.SSx2.p1.15.m3.1.1"></share><ci id="Sx3.SSx2.p1.15.m3.1.1.6.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.6">â€¦</ci></apply><apply id="Sx3.SSx2.p1.15.m3.1.1e.cmml" xref="Sx3.SSx2.p1.15.m3.1.1"><geq id="Sx3.SSx2.p1.15.m3.1.1.7.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.7"></geq><share href="#Sx3.SSx2.p1.15.m3.1.1.6.cmml" id="Sx3.SSx2.p1.15.m3.1.1f.cmml" xref="Sx3.SSx2.p1.15.m3.1.1"></share><apply id="Sx3.SSx2.p1.15.m3.1.1.8.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.8"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.15.m3.1.1.8.1.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.8">subscript</csymbol><ci id="Sx3.SSx2.p1.15.m3.1.1.8.2.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.8.2">ğ‘ </ci><apply id="Sx3.SSx2.p1.15.m3.1.1.8.3.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.8.3"><times id="Sx3.SSx2.p1.15.m3.1.1.8.3.1.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.8.3.1"></times><ci id="Sx3.SSx2.p1.15.m3.1.1.8.3.2.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.8.3.2">ğ‘–</ci><ci id="Sx3.SSx2.p1.15.m3.1.1.8.3.3.cmml" xref="Sx3.SSx2.p1.15.m3.1.1.8.3.3">ğ‘›</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.15.m3.1c">s_{ii}\geq s_{im}\geq...\geq s_{in}</annotation></semantics></math>.
This results in <math id="Sx3.SSx2.p1.16.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx3.SSx2.p1.16.m4.1a"><mi id="Sx3.SSx2.p1.16.m4.1.1" xref="Sx3.SSx2.p1.16.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.16.m4.1b"><ci id="Sx3.SSx2.p1.16.m4.1.1.cmml" xref="Sx3.SSx2.p1.16.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.16.m4.1c">N</annotation></semantics></math> groups, corresponding to the number of captions within the text corpus, with these groups containing overlapping captions.
We use a greedy algorithm to minimize redundancy and ensure that a small number of groups cover all corpus captions. The algorithm works as follows: (1) initialize a set <math id="Sx3.SSx2.p1.17.m5.1" class="ltx_Math" alttext="C" display="inline"><semantics id="Sx3.SSx2.p1.17.m5.1a"><mi id="Sx3.SSx2.p1.17.m5.1.1" xref="Sx3.SSx2.p1.17.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.17.m5.1b"><ci id="Sx3.SSx2.p1.17.m5.1.1.cmml" xref="Sx3.SSx2.p1.17.m5.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.17.m5.1c">C</annotation></semantics></math> identical to the corpus <math id="Sx3.SSx2.p1.18.m6.1" class="ltx_Math" alttext="T" display="inline"><semantics id="Sx3.SSx2.p1.18.m6.1a"><mi id="Sx3.SSx2.p1.18.m6.1.1" xref="Sx3.SSx2.p1.18.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.18.m6.1b"><ci id="Sx3.SSx2.p1.18.m6.1.1.cmml" xref="Sx3.SSx2.p1.18.m6.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.18.m6.1c">T</annotation></semantics></math>. (2) repeatedly find and remove from <math id="Sx3.SSx2.p1.19.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="Sx3.SSx2.p1.19.m7.1a"><mi id="Sx3.SSx2.p1.19.m7.1.1" xref="Sx3.SSx2.p1.19.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.19.m7.1b"><ci id="Sx3.SSx2.p1.19.m7.1.1.cmml" xref="Sx3.SSx2.p1.19.m7.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.19.m7.1c">C</annotation></semantics></math> the group <math id="Sx3.SSx2.p1.20.m8.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="Sx3.SSx2.p1.20.m8.1a"><msub id="Sx3.SSx2.p1.20.m8.1.1" xref="Sx3.SSx2.p1.20.m8.1.1.cmml"><mi id="Sx3.SSx2.p1.20.m8.1.1.2" xref="Sx3.SSx2.p1.20.m8.1.1.2.cmml">G</mi><mi id="Sx3.SSx2.p1.20.m8.1.1.3" xref="Sx3.SSx2.p1.20.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.20.m8.1b"><apply id="Sx3.SSx2.p1.20.m8.1.1.cmml" xref="Sx3.SSx2.p1.20.m8.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.p1.20.m8.1.1.1.cmml" xref="Sx3.SSx2.p1.20.m8.1.1">subscript</csymbol><ci id="Sx3.SSx2.p1.20.m8.1.1.2.cmml" xref="Sx3.SSx2.p1.20.m8.1.1.2">ğº</ci><ci id="Sx3.SSx2.p1.20.m8.1.1.3.cmml" xref="Sx3.SSx2.p1.20.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.20.m8.1c">G_{i}</annotation></semantics></math> with the most overlap with <math id="Sx3.SSx2.p1.21.m9.1" class="ltx_Math" alttext="C" display="inline"><semantics id="Sx3.SSx2.p1.21.m9.1a"><mi id="Sx3.SSx2.p1.21.m9.1.1" xref="Sx3.SSx2.p1.21.m9.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.21.m9.1b"><ci id="Sx3.SSx2.p1.21.m9.1.1.cmml" xref="Sx3.SSx2.p1.21.m9.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.21.m9.1c">C</annotation></semantics></math>, until <math id="Sx3.SSx2.p1.22.m10.1" class="ltx_Math" alttext="C" display="inline"><semantics id="Sx3.SSx2.p1.22.m10.1a"><mi id="Sx3.SSx2.p1.22.m10.1.1" xref="Sx3.SSx2.p1.22.m10.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.22.m10.1b"><ci id="Sx3.SSx2.p1.22.m10.1.1.cmml" xref="Sx3.SSx2.p1.22.m10.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.22.m10.1c">C</annotation></semantics></math> is empty. Finally, we can find groups that can cover the entire corpus.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p2.1" class="ltx_p"><span id="Sx3.SSx2.p2.1.1" class="ltx_text ltx_font_bold">Selection and Summarization via LLMs.</span>
Selection and summarization pose significant challenges for existing technology. Selection aims to select simple captions that are able to describe the same image from various perspectives. The challenges of selection are (1) the demand for common sense: the selection process should incorporate the knowledge of natural scenes to decide what kind of objects should appear together in the scene and the given descriptions should not conflict.
(2) the lack of metrics: the current metrics of text similarity are not designed for our target; the similarity metrics can not identify the descriptions that depict the same image.
Summarization aims to combine the selected simple captions into one complex caption for image generation. The challenge is that traditional text summarization approaches are ill-suited for our scenario since they aim to extract key information from long documents. And our summarization also demands strong generalization capabilities in open domains, since the corpus is so diverse.
Fortunately, the potency of LLMs empowers us to tackle these intricate challenges. LLMs are pretrained on a large scale of data, showing advancement in common knowledge and generalization ability. The instruction-following ability of LLMs makes it possible for us to formulate the selection and summarization task through language.
So we employ LLMs to tackle both tasks. We consider each group of captions as a candidate set for describing a specific image. We then formulate a prompt that enables us to accomplish both selection and summarization through LLMs. The prompt template is provided in Appendix C.</p>
</div>
<div id="Sx3.SSx2.p3" class="ltx_para">
<p id="Sx3.SSx2.p3.1" class="ltx_p">To avoid the hallucination problem of LLMs, we incorporate the chain of thought techniqueÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al. <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite> into our design. We instruct the LLMs to first select sentences and subsequently summarize them into a single sentence. Additionally, we specifically prompt LLMs to provide the index of the chosen sentences, instead of generating these sentences anew, which may lead to hallucination problems.</p>
</div>
<div id="Sx3.SSx2.p4" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p4.1" class="ltx_p"><span id="Sx3.SSx2.p4.1.1" class="ltx_text ltx_font_bold">Image Generation with Stable Diffusion.</span>
In this crucial step, we harness the power of stable diffusion to generate synthetic images based on the summarized sentences derived from the text corpus.
We refrain from using any prompt engineering on the captions that are inputted to stable diffusion, aiming to minimize the influence of human intervention during large-scale generation. Consequently, we can acquire a substantial volume of multi-context images.</p>
</div>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Training Stage</h3>

<div id="Sx3.SSx3.p1" class="ltx_para ltx_noindent">
<p id="Sx3.SSx3.p1.1" class="ltx_p"><span id="Sx3.SSx3.p1.1.1" class="ltx_text ltx_font_bold">Architecture and data.</span>
We follow BLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> to adopt the encoder-decoder architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al. <a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite>.
The encoder is initialized from the weights of ViT-B/32, while the decoder is initialized from the weights of BERT-baseÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al. <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>.
We exclusively utilize synthetic data for training, specifically focusing on multi-context image-text pairs.</p>
</div>
<figure id="Sx3.T1" class="ltx_table">
<div id="Sx3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:455.3pt;height:174.9pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.8pt,20.9pt) scale(0.806078083808782,0.806078083808782) ;">
<table id="Sx3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T1.1.1.1" class="ltx_tr">
<td id="Sx3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="Sx3.T1.1.1.1.1.1" class="ltx_text">Methods</span></td>
<td id="Sx3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Data</td>
<td id="Sx3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">MSCOCO</td>
<td id="Sx3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">Flickr30k</td>
</tr>
<tr id="Sx3.T1.1.1.2" class="ltx_tr">
<td id="Sx3.T1.1.1.2.1" class="ltx_td ltx_align_center">I.</td>
<td id="Sx3.T1.1.1.2.2" class="ltx_td ltx_align_center">T.</td>
<td id="Sx3.T1.1.1.2.3" class="ltx_td ltx_align_center">B@4</td>
<td id="Sx3.T1.1.1.2.4" class="ltx_td ltx_align_center">M</td>
<td id="Sx3.T1.1.1.2.5" class="ltx_td ltx_align_center">R</td>
<td id="Sx3.T1.1.1.2.6" class="ltx_td ltx_align_center">C</td>
<td id="Sx3.T1.1.1.2.7" class="ltx_td ltx_align_center">B@4</td>
<td id="Sx3.T1.1.1.2.8" class="ltx_td ltx_align_center">M</td>
<td id="Sx3.T1.1.1.2.9" class="ltx_td ltx_align_center">R</td>
<td id="Sx3.T1.1.1.2.10" class="ltx_td ltx_align_center">C</td>
</tr>
<tr id="Sx3.T1.1.1.3" class="ltx_tr">
<td id="Sx3.T1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><cite class="ltx_cite ltx_citemacro_citet">Feng etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="Sx3.T1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="Sx3.T1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="Sx3.T1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">18.6</td>
<td id="Sx3.T1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">17.9</td>
<td id="Sx3.T1.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">43.1</td>
<td id="Sx3.T1.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">54.9</td>
<td id="Sx3.T1.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx3.T1.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx3.T1.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx3.T1.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="Sx3.T1.1.1.4" class="ltx_tr">
<td id="Sx3.T1.1.1.4.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Laina, Rupprecht, and Navab (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="Sx3.T1.1.1.4.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.4.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.4.4" class="ltx_td ltx_align_center">19.3</td>
<td id="Sx3.T1.1.1.4.5" class="ltx_td ltx_align_center">20.2</td>
<td id="Sx3.T1.1.1.4.6" class="ltx_td ltx_align_center">45.0</td>
<td id="Sx3.T1.1.1.4.7" class="ltx_td ltx_align_center">61.8</td>
<td id="Sx3.T1.1.1.4.8" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.4.9" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.4.10" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.4.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx3.T1.1.1.5" class="ltx_tr">
<td id="Sx3.T1.1.1.5.1" class="ltx_td ltx_align_left">ESPER-StyleÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al. <a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx3.T1.1.1.5.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.5.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.5.4" class="ltx_td ltx_align_center">21.9</td>
<td id="Sx3.T1.1.1.5.5" class="ltx_td ltx_align_center">21.9</td>
<td id="Sx3.T1.1.1.5.6" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.5.7" class="ltx_td ltx_align_center">78.2</td>
<td id="Sx3.T1.1.1.5.8" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.5.9" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.5.10" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.5.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx3.T1.1.1.6" class="ltx_tr">
<td id="Sx3.T1.1.1.6.1" class="ltx_td ltx_align_left">ZeroCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Tewel etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx3.T1.1.1.6.2" class="ltx_td"></td>
<td id="Sx3.T1.1.1.6.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.6.4" class="ltx_td ltx_align_center">7.0</td>
<td id="Sx3.T1.1.1.6.5" class="ltx_td ltx_align_center">15.4</td>
<td id="Sx3.T1.1.1.6.6" class="ltx_td ltx_align_center">31.8</td>
<td id="Sx3.T1.1.1.6.7" class="ltx_td ltx_align_center">34.5</td>
<td id="Sx3.T1.1.1.6.8" class="ltx_td ltx_align_center">5.4</td>
<td id="Sx3.T1.1.1.6.9" class="ltx_td ltx_align_center">11.8</td>
<td id="Sx3.T1.1.1.6.10" class="ltx_td ltx_align_center">27.3</td>
<td id="Sx3.T1.1.1.6.11" class="ltx_td ltx_align_center">16.8</td>
</tr>
<tr id="Sx3.T1.1.1.7" class="ltx_tr">
<td id="Sx3.T1.1.1.7.1" class="ltx_td ltx_align_left">MagicÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx3.T1.1.1.7.2" class="ltx_td"></td>
<td id="Sx3.T1.1.1.7.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.7.4" class="ltx_td ltx_align_center">12.9</td>
<td id="Sx3.T1.1.1.7.5" class="ltx_td ltx_align_center">17.4</td>
<td id="Sx3.T1.1.1.7.6" class="ltx_td ltx_align_center">39.9</td>
<td id="Sx3.T1.1.1.7.7" class="ltx_td ltx_align_center">49.3</td>
<td id="Sx3.T1.1.1.7.8" class="ltx_td ltx_align_center">6.4</td>
<td id="Sx3.T1.1.1.7.9" class="ltx_td ltx_align_center">13.1</td>
<td id="Sx3.T1.1.1.7.10" class="ltx_td ltx_align_center">31.6</td>
<td id="Sx3.T1.1.1.7.11" class="ltx_td ltx_align_center">20.4</td>
</tr>
<tr id="Sx3.T1.1.1.8" class="ltx_tr">
<td id="Sx3.T1.1.1.8.1" class="ltx_td ltx_align_left">CLIPReÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx3.T1.1.1.8.2" class="ltx_td"></td>
<td id="Sx3.T1.1.1.8.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.8.4" class="ltx_td ltx_align_center">4.9</td>
<td id="Sx3.T1.1.1.8.5" class="ltx_td ltx_align_center">11.4</td>
<td id="Sx3.T1.1.1.8.6" class="ltx_td ltx_align_center">29.0</td>
<td id="Sx3.T1.1.1.8.7" class="ltx_td ltx_align_center">13.6</td>
<td id="Sx3.T1.1.1.8.8" class="ltx_td ltx_align_center">5.2</td>
<td id="Sx3.T1.1.1.8.9" class="ltx_td ltx_align_center">11.6</td>
<td id="Sx3.T1.1.1.8.10" class="ltx_td ltx_align_center">27.6</td>
<td id="Sx3.T1.1.1.8.11" class="ltx_td ltx_align_center">10.0</td>
</tr>
<tr id="Sx3.T1.1.1.9" class="ltx_tr">
<td id="Sx3.T1.1.1.9.1" class="ltx_td ltx_align_left">CapDecÂ <cite class="ltx_cite ltx_citemacro_citep">(Nukrai, Mokady, and Globerson <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx3.T1.1.1.9.2" class="ltx_td"></td>
<td id="Sx3.T1.1.1.9.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.9.4" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.9.4.1" class="ltx_text ltx_framed ltx_framed_underline">26.4</span></td>
<td id="Sx3.T1.1.1.9.5" class="ltx_td ltx_align_center">25.1</td>
<td id="Sx3.T1.1.1.9.6" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.9.6.1" class="ltx_text ltx_framed ltx_framed_underline">51.8</span></td>
<td id="Sx3.T1.1.1.9.7" class="ltx_td ltx_align_center">91.8</td>
<td id="Sx3.T1.1.1.9.8" class="ltx_td ltx_align_center">17.7</td>
<td id="Sx3.T1.1.1.9.9" class="ltx_td ltx_align_center">20.0</td>
<td id="Sx3.T1.1.1.9.10" class="ltx_td ltx_align_center">43.9</td>
<td id="Sx3.T1.1.1.9.11" class="ltx_td ltx_align_center">39.1</td>
</tr>
<tr id="Sx3.T1.1.1.10" class="ltx_tr">
<td id="Sx3.T1.1.1.10.1" class="ltx_td ltx_align_left">DeCap*Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx3.T1.1.1.10.2" class="ltx_td"></td>
<td id="Sx3.T1.1.1.10.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.10.4" class="ltx_td ltx_align_center">25.3</td>
<td id="Sx3.T1.1.1.10.5" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.10.5.1" class="ltx_text ltx_framed ltx_framed_underline">25.2</span></td>
<td id="Sx3.T1.1.1.10.6" class="ltx_td ltx_align_center">51.1</td>
<td id="Sx3.T1.1.1.10.7" class="ltx_td ltx_align_center">92.9</td>
<td id="Sx3.T1.1.1.10.8" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.10.8.1" class="ltx_text ltx_framed ltx_framed_underline">20.0</span></td>
<td id="Sx3.T1.1.1.10.9" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.10.9.1" class="ltx_text ltx_font_bold">21.7</span></td>
<td id="Sx3.T1.1.1.10.10" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.10.10.1" class="ltx_text ltx_framed ltx_framed_underline">46.2</span></td>
<td id="Sx3.T1.1.1.10.11" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.10.11.1" class="ltx_text ltx_framed ltx_framed_underline">49.3</span></td>
</tr>
<tr id="Sx3.T1.1.1.11" class="ltx_tr">
<td id="Sx3.T1.1.1.11.1" class="ltx_td ltx_align_left">CLOSEÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu, Clark, and Kembhavi <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx3.T1.1.1.11.2" class="ltx_td"></td>
<td id="Sx3.T1.1.1.11.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="Sx3.T1.1.1.11.4" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.11.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.11.6" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.11.7" class="ltx_td ltx_align_center"><span id="Sx3.T1.1.1.11.7.1" class="ltx_text ltx_framed ltx_framed_underline">95.3</span></td>
<td id="Sx3.T1.1.1.11.8" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.11.9" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.11.10" class="ltx_td ltx_align_center">-</td>
<td id="Sx3.T1.1.1.11.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx3.T1.1.1.12" class="ltx_tr">
<td id="Sx3.T1.1.1.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">ICSD</td>
<td id="Sx3.T1.1.1.12.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="Sx3.T1.1.1.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ“</td>
<td id="Sx3.T1.1.1.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.4.1" class="ltx_text ltx_font_bold">29.9</span></td>
<td id="Sx3.T1.1.1.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.5.1" class="ltx_text ltx_font_bold">25.4</span></td>
<td id="Sx3.T1.1.1.12.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.6.1" class="ltx_text ltx_font_bold">52.7</span></td>
<td id="Sx3.T1.1.1.12.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.7.1" class="ltx_text ltx_font_bold">96.6</span></td>
<td id="Sx3.T1.1.1.12.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.8.1" class="ltx_text ltx_font_bold">25.2</span></td>
<td id="Sx3.T1.1.1.12.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.9.1" class="ltx_text ltx_framed ltx_framed_underline">20.6</span></td>
<td id="Sx3.T1.1.1.12.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.10.1" class="ltx_text ltx_font_bold">46.7</span></td>
<td id="Sx3.T1.1.1.12.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx3.T1.1.1.12.11.1" class="ltx_text ltx_font_bold">54.3</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The results of in-domain image captioning on MSCOCO and Flickr30k. â€œI.â€ and â€œT.â€ denote the need for external image data and text data, respectively.
â€œ*â€ means results reproduced using the provided code. B@4: BLEU@4; M: METEOR; R: ROUGE; C: CIDEr.
Numbers in <span id="Sx3.T1.2.1" class="ltx_text ltx_font_bold">bold</span> and <span id="Sx3.T1.3.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span> text represent the best and second-best results, respectively.
</figcaption>
</figure>
<div id="Sx3.SSx3.p2" class="ltx_para ltx_noindent">
<p id="Sx3.SSx3.p2.7" class="ltx_p"><span id="Sx3.SSx3.p2.7.1" class="ltx_text ltx_font_bold">Objective.</span> We utilize synthetic data to train our model using Cross Entropy Loss:</p>
<table id="Sx3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx3.E3.m1.3" class="ltx_Math" alttext="\mathcal{L}=-\sum_{i=1}^{n}\log(P(y_{i}|y_{1:i-1},v))" display="block"><semantics id="Sx3.E3.m1.3a"><mrow id="Sx3.E3.m1.3.3" xref="Sx3.E3.m1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.E3.m1.3.3.3" xref="Sx3.E3.m1.3.3.3.cmml">â„’</mi><mo id="Sx3.E3.m1.3.3.2" xref="Sx3.E3.m1.3.3.2.cmml">=</mo><mrow id="Sx3.E3.m1.3.3.1" xref="Sx3.E3.m1.3.3.1.cmml"><mo id="Sx3.E3.m1.3.3.1a" xref="Sx3.E3.m1.3.3.1.cmml">âˆ’</mo><mrow id="Sx3.E3.m1.3.3.1.1" xref="Sx3.E3.m1.3.3.1.1.cmml"><munderover id="Sx3.E3.m1.3.3.1.1.2" xref="Sx3.E3.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="Sx3.E3.m1.3.3.1.1.2.2.2" xref="Sx3.E3.m1.3.3.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="Sx3.E3.m1.3.3.1.1.2.2.3" xref="Sx3.E3.m1.3.3.1.1.2.2.3.cmml"><mi id="Sx3.E3.m1.3.3.1.1.2.2.3.2" xref="Sx3.E3.m1.3.3.1.1.2.2.3.2.cmml">i</mi><mo id="Sx3.E3.m1.3.3.1.1.2.2.3.1" xref="Sx3.E3.m1.3.3.1.1.2.2.3.1.cmml">=</mo><mn id="Sx3.E3.m1.3.3.1.1.2.2.3.3" xref="Sx3.E3.m1.3.3.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="Sx3.E3.m1.3.3.1.1.2.3" xref="Sx3.E3.m1.3.3.1.1.2.3.cmml">n</mi></munderover><mrow id="Sx3.E3.m1.3.3.1.1.1.1" xref="Sx3.E3.m1.3.3.1.1.1.2.cmml"><mi id="Sx3.E3.m1.2.2" xref="Sx3.E3.m1.2.2.cmml">log</mi><mo id="Sx3.E3.m1.3.3.1.1.1.1a" xref="Sx3.E3.m1.3.3.1.1.1.2.cmml">â¡</mo><mrow id="Sx3.E3.m1.3.3.1.1.1.1.1" xref="Sx3.E3.m1.3.3.1.1.1.2.cmml"><mo stretchy="false" id="Sx3.E3.m1.3.3.1.1.1.1.1.2" xref="Sx3.E3.m1.3.3.1.1.1.2.cmml">(</mo><mrow id="Sx3.E3.m1.3.3.1.1.1.1.1.1" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mi id="Sx3.E3.m1.3.3.1.1.1.1.1.1.3" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mrow id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">âˆ’</mo><mn id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub><mo id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="Sx3.E3.m1.1.1" xref="Sx3.E3.m1.1.1.cmml">v</mi></mrow></mrow><mo stretchy="false" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.3" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="Sx3.E3.m1.3.3.1.1.1.1.1.3" xref="Sx3.E3.m1.3.3.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E3.m1.3b"><apply id="Sx3.E3.m1.3.3.cmml" xref="Sx3.E3.m1.3.3"><eq id="Sx3.E3.m1.3.3.2.cmml" xref="Sx3.E3.m1.3.3.2"></eq><ci id="Sx3.E3.m1.3.3.3.cmml" xref="Sx3.E3.m1.3.3.3">â„’</ci><apply id="Sx3.E3.m1.3.3.1.cmml" xref="Sx3.E3.m1.3.3.1"><minus id="Sx3.E3.m1.3.3.1.2.cmml" xref="Sx3.E3.m1.3.3.1"></minus><apply id="Sx3.E3.m1.3.3.1.1.cmml" xref="Sx3.E3.m1.3.3.1.1"><apply id="Sx3.E3.m1.3.3.1.1.2.cmml" xref="Sx3.E3.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="Sx3.E3.m1.3.3.1.1.2.1.cmml" xref="Sx3.E3.m1.3.3.1.1.2">superscript</csymbol><apply id="Sx3.E3.m1.3.3.1.1.2.2.cmml" xref="Sx3.E3.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="Sx3.E3.m1.3.3.1.1.2.2.1.cmml" xref="Sx3.E3.m1.3.3.1.1.2">subscript</csymbol><sum id="Sx3.E3.m1.3.3.1.1.2.2.2.cmml" xref="Sx3.E3.m1.3.3.1.1.2.2.2"></sum><apply id="Sx3.E3.m1.3.3.1.1.2.2.3.cmml" xref="Sx3.E3.m1.3.3.1.1.2.2.3"><eq id="Sx3.E3.m1.3.3.1.1.2.2.3.1.cmml" xref="Sx3.E3.m1.3.3.1.1.2.2.3.1"></eq><ci id="Sx3.E3.m1.3.3.1.1.2.2.3.2.cmml" xref="Sx3.E3.m1.3.3.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="Sx3.E3.m1.3.3.1.1.2.2.3.3.cmml" xref="Sx3.E3.m1.3.3.1.1.2.2.3.3">1</cn></apply></apply><ci id="Sx3.E3.m1.3.3.1.1.2.3.cmml" xref="Sx3.E3.m1.3.3.1.1.2.3">ğ‘›</ci></apply><apply id="Sx3.E3.m1.3.3.1.1.1.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1"><log id="Sx3.E3.m1.2.2.cmml" xref="Sx3.E3.m1.2.2"></log><apply id="Sx3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1"><times id="Sx3.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.2"></times><ci id="Sx3.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.3">ğ‘ƒ</ci><apply id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">ğ‘¦</ci><ci id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><list id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><apply id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¦</ci><apply id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3"><ci id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2">1</cn><apply id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><minus id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">ğ‘–</ci><cn type="integer" id="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="Sx3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply><ci id="Sx3.E3.m1.1.1.cmml" xref="Sx3.E3.m1.1.1">ğ‘£</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E3.m1.3c">\mathcal{L}=-\sum_{i=1}^{n}\log(P(y_{i}|y_{1:i-1},v))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="Sx3.SSx3.p2.6" class="ltx_p">where <math id="Sx3.SSx3.p2.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="Sx3.SSx3.p2.1.m1.1a"><mi id="Sx3.SSx3.p2.1.m1.1.1" xref="Sx3.SSx3.p2.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p2.1.m1.1b"><ci id="Sx3.SSx3.p2.1.m1.1.1.cmml" xref="Sx3.SSx3.p2.1.m1.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p2.1.m1.1c">P</annotation></semantics></math> denotes the probability distribution from the language decoder, <math id="Sx3.SSx3.p2.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="Sx3.SSx3.p2.2.m2.1a"><msub id="Sx3.SSx3.p2.2.m2.1.1" xref="Sx3.SSx3.p2.2.m2.1.1.cmml"><mi id="Sx3.SSx3.p2.2.m2.1.1.2" xref="Sx3.SSx3.p2.2.m2.1.1.2.cmml">y</mi><mi id="Sx3.SSx3.p2.2.m2.1.1.3" xref="Sx3.SSx3.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p2.2.m2.1b"><apply id="Sx3.SSx3.p2.2.m2.1.1.cmml" xref="Sx3.SSx3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p2.2.m2.1.1.1.cmml" xref="Sx3.SSx3.p2.2.m2.1.1">subscript</csymbol><ci id="Sx3.SSx3.p2.2.m2.1.1.2.cmml" xref="Sx3.SSx3.p2.2.m2.1.1.2">ğ‘¦</ci><ci id="Sx3.SSx3.p2.2.m2.1.1.3.cmml" xref="Sx3.SSx3.p2.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p2.2.m2.1c">y_{i}</annotation></semantics></math> denotes the ground-truth word at time step <math id="Sx3.SSx3.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx3.SSx3.p2.3.m3.1a"><mi id="Sx3.SSx3.p2.3.m3.1.1" xref="Sx3.SSx3.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p2.3.m3.1b"><ci id="Sx3.SSx3.p2.3.m3.1.1.cmml" xref="Sx3.SSx3.p2.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p2.3.m3.1c">i</annotation></semantics></math>, <math id="Sx3.SSx3.p2.4.m4.1" class="ltx_Math" alttext="y_{1:i-1}" display="inline"><semantics id="Sx3.SSx3.p2.4.m4.1a"><msub id="Sx3.SSx3.p2.4.m4.1.1" xref="Sx3.SSx3.p2.4.m4.1.1.cmml"><mi id="Sx3.SSx3.p2.4.m4.1.1.2" xref="Sx3.SSx3.p2.4.m4.1.1.2.cmml">y</mi><mrow id="Sx3.SSx3.p2.4.m4.1.1.3" xref="Sx3.SSx3.p2.4.m4.1.1.3.cmml"><mn id="Sx3.SSx3.p2.4.m4.1.1.3.2" xref="Sx3.SSx3.p2.4.m4.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="Sx3.SSx3.p2.4.m4.1.1.3.1" xref="Sx3.SSx3.p2.4.m4.1.1.3.1.cmml">:</mo><mrow id="Sx3.SSx3.p2.4.m4.1.1.3.3" xref="Sx3.SSx3.p2.4.m4.1.1.3.3.cmml"><mi id="Sx3.SSx3.p2.4.m4.1.1.3.3.2" xref="Sx3.SSx3.p2.4.m4.1.1.3.3.2.cmml">i</mi><mo id="Sx3.SSx3.p2.4.m4.1.1.3.3.1" xref="Sx3.SSx3.p2.4.m4.1.1.3.3.1.cmml">âˆ’</mo><mn id="Sx3.SSx3.p2.4.m4.1.1.3.3.3" xref="Sx3.SSx3.p2.4.m4.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p2.4.m4.1b"><apply id="Sx3.SSx3.p2.4.m4.1.1.cmml" xref="Sx3.SSx3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p2.4.m4.1.1.1.cmml" xref="Sx3.SSx3.p2.4.m4.1.1">subscript</csymbol><ci id="Sx3.SSx3.p2.4.m4.1.1.2.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.2">ğ‘¦</ci><apply id="Sx3.SSx3.p2.4.m4.1.1.3.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.3"><ci id="Sx3.SSx3.p2.4.m4.1.1.3.1.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.3.1">:</ci><cn type="integer" id="Sx3.SSx3.p2.4.m4.1.1.3.2.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.3.2">1</cn><apply id="Sx3.SSx3.p2.4.m4.1.1.3.3.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.3.3"><minus id="Sx3.SSx3.p2.4.m4.1.1.3.3.1.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.3.3.1"></minus><ci id="Sx3.SSx3.p2.4.m4.1.1.3.3.2.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.3.3.2">ğ‘–</ci><cn type="integer" id="Sx3.SSx3.p2.4.m4.1.1.3.3.3.cmml" xref="Sx3.SSx3.p2.4.m4.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p2.4.m4.1c">y_{1:i-1}</annotation></semantics></math> refers to the prior words, <math id="Sx3.SSx3.p2.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="Sx3.SSx3.p2.5.m5.1a"><mi id="Sx3.SSx3.p2.5.m5.1.1" xref="Sx3.SSx3.p2.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p2.5.m5.1b"><ci id="Sx3.SSx3.p2.5.m5.1.1.cmml" xref="Sx3.SSx3.p2.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p2.5.m5.1c">n</annotation></semantics></math> stands for the length of the ground-truth sentence, and <math id="Sx3.SSx3.p2.6.m6.1" class="ltx_Math" alttext="v" display="inline"><semantics id="Sx3.SSx3.p2.6.m6.1a"><mi id="Sx3.SSx3.p2.6.m6.1.1" xref="Sx3.SSx3.p2.6.m6.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p2.6.m6.1b"><ci id="Sx3.SSx3.p2.6.m6.1.1.cmml" xref="Sx3.SSx3.p2.6.m6.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p2.6.m6.1c">v</annotation></semantics></math> is the synthetic image.</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiments</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">We conduct experiments in two settings: in-domain and cross-domain image captioning. In the in-domain setting, the training and test data are derived from the same dataset. In the cross-domain setting, the training and test data are sampled from different datasets, requiring the model to effectively generalize across diverse data sources.</p>
</div>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Settings</h3>

<div id="Sx4.SSx1.p1" class="ltx_para ltx_noindent">
<p id="Sx4.SSx1.p1.1" class="ltx_p"><span id="Sx4.SSx1.p1.1.1" class="ltx_text ltx_font_bold">Datasets.</span>
For in-domain image captioning, we utilize MSCOCOÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al. <a href="#bib.bib27" title="" class="ltx_ref">2014</a>)</cite> and Flickr30kÂ <cite class="ltx_cite ltx_citemacro_citep">(Young etÂ al. <a href="#bib.bib51" title="" class="ltx_ref">2014</a>)</cite> datasets. MSCOCO contains 123,287 images, each annotated with five captions. FollowingÂ <cite class="ltx_cite ltx_citemacro_citep">(Karpathy and Fei-Fei <a href="#bib.bib20" title="" class="ltx_ref">2015</a>)</cite>, we split MSCOCO into 118,287 for training, 4,000 for validation, and 1,000 for testing. Flickr30k contains 31,783 images, with each image accompanied by five captions.
Regarding cross-domain image captioning, we train our model on SS1MÂ <cite class="ltx_cite ltx_citemacro_citep">(Feng etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> dataset and evaluate its performance using MSCOCO and NoCapsÂ <cite class="ltx_cite ltx_citemacro_citep">(Agrawal etÂ al. <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>. SS1M is a web-scraped text corpus and contains 2,322,628 image descriptions from Shutterstock using eighty object class names in MSCOCO as keywords. In line with DeCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, we exclude sentences containing more than fifteen words on SS1M.
We use the validation set of NoCaps to evaluate performance in three settings: in-domain, near-domain, and out-of-domain.
For all datasets, we solely utilize their text during training and do not acquire any natural images.</p>
</div>
<div id="Sx4.SSx1.p2" class="ltx_para">
<p id="Sx4.SSx1.p2.1" class="ltx_p">To measure the quality of generated captions, we follow prior studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Dai etÂ al. <a href="#bib.bib9" title="" class="ltx_ref">2017</a>; Meng etÂ al. <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite> and employ metrics such as BLEUÂ <cite class="ltx_cite ltx_citemacro_citep">(Papineni etÂ al. <a href="#bib.bib33" title="" class="ltx_ref">2002</a>)</cite>, METEORÂ <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Lavie <a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite>, ROUGEÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin <a href="#bib.bib26" title="" class="ltx_ref">2004</a>)</cite>, and CIDEr-DÂ <cite class="ltx_cite ltx_citemacro_citep">(Vedantam, LawrenceÂ Zitnick, and Parikh <a href="#bib.bib43" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<figure id="Sx4.T2" class="ltx_table">
<div id="Sx4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:404.7pt;height:102.5pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-83.9pt,21.1pt) scale(0.706891942353698,0.706891942353698) ;">
<table id="Sx4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx4.T2.1.1.1" class="ltx_tr">
<td id="Sx4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="Sx4.T2.1.1.1.1.1" class="ltx_text">Methods</span></td>
<td id="Sx4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="Sx4.T2.1.1.1.2.1" class="ltx_text">Dataset</span></td>
<td id="Sx4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">MSCOCO</td>
<td id="Sx4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">NoCaps val (CIDEr)</td>
</tr>
<tr id="Sx4.T2.1.1.2" class="ltx_tr">
<td id="Sx4.T2.1.1.2.1" class="ltx_td ltx_align_center">B@4</td>
<td id="Sx4.T2.1.1.2.2" class="ltx_td ltx_align_center">M</td>
<td id="Sx4.T2.1.1.2.3" class="ltx_td ltx_align_center">R</td>
<td id="Sx4.T2.1.1.2.4" class="ltx_td ltx_align_center">C</td>
<td id="Sx4.T2.1.1.2.5" class="ltx_td ltx_align_center">In</td>
<td id="Sx4.T2.1.1.2.6" class="ltx_td ltx_align_center">Near</td>
<td id="Sx4.T2.1.1.2.7" class="ltx_td ltx_align_center">Out</td>
<td id="Sx4.T2.1.1.2.8" class="ltx_td ltx_align_center">Overall</td>
</tr>
<tr id="Sx4.T2.1.1.3" class="ltx_tr">
<td id="Sx4.T2.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">ZeroCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Tewel etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx4.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">2.6</td>
<td id="Sx4.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">11.5</td>
<td id="Sx4.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">14.6</td>
<td id="Sx4.T2.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T2.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T2.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T2.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="Sx4.T2.1.1.4" class="ltx_tr">
<td id="Sx4.T2.1.1.4.1" class="ltx_td ltx_align_left">ConZICÂ <cite class="ltx_cite ltx_citemacro_citep">(Zeng etÂ al. <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx4.T2.1.1.4.2" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.4.3" class="ltx_td ltx_align_center">1.3</td>
<td id="Sx4.T2.1.1.4.4" class="ltx_td ltx_align_center">11.5</td>
<td id="Sx4.T2.1.1.4.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.4.6" class="ltx_td ltx_align_center">12.8</td>
<td id="Sx4.T2.1.1.4.7" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.4.8" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.4.9" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.4.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx4.T2.1.1.5" class="ltx_tr">
<td id="Sx4.T2.1.1.5.1" class="ltx_td ltx_align_left">CLIPReÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx4.T2.1.1.5.2" class="ltx_td ltx_align_center">CC3M-text</td>
<td id="Sx4.T2.1.1.5.3" class="ltx_td ltx_align_center">4.6</td>
<td id="Sx4.T2.1.1.5.4" class="ltx_td ltx_align_center">13.3</td>
<td id="Sx4.T2.1.1.5.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.5.6" class="ltx_td ltx_align_center">25.6</td>
<td id="Sx4.T2.1.1.5.7" class="ltx_td ltx_align_center">23.3</td>
<td id="Sx4.T2.1.1.5.8" class="ltx_td ltx_align_center">26.8</td>
<td id="Sx4.T2.1.1.5.9" class="ltx_td ltx_align_center">36.5</td>
<td id="Sx4.T2.1.1.5.10" class="ltx_td ltx_align_center">28.2</td>
</tr>
<tr id="Sx4.T2.1.1.6" class="ltx_tr">
<td id="Sx4.T2.1.1.6.1" class="ltx_td ltx_align_left">DeCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx4.T2.1.1.6.2" class="ltx_td ltx_align_center">CC3M-text</td>
<td id="Sx4.T2.1.1.6.3" class="ltx_td ltx_align_center">8.8</td>
<td id="Sx4.T2.1.1.6.4" class="ltx_td ltx_align_center">16.0</td>
<td id="Sx4.T2.1.1.6.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.6.6" class="ltx_td ltx_align_center">42.1</td>
<td id="Sx4.T2.1.1.6.7" class="ltx_td ltx_align_center">34.8</td>
<td id="Sx4.T2.1.1.6.8" class="ltx_td ltx_align_center">37.7</td>
<td id="Sx4.T2.1.1.6.9" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.6.9.1" class="ltx_text ltx_font_bold">49.9</span></td>
<td id="Sx4.T2.1.1.6.10" class="ltx_td ltx_align_center">39.7</td>
</tr>
<tr id="Sx4.T2.1.1.7" class="ltx_tr">
<td id="Sx4.T2.1.1.7.1" class="ltx_td ltx_align_left">DeCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx4.T2.1.1.7.2" class="ltx_td ltx_align_center">SS1M</td>
<td id="Sx4.T2.1.1.7.3" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.7.3.1" class="ltx_text ltx_framed ltx_framed_underline">8.9</span></td>
<td id="Sx4.T2.1.1.7.4" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.7.4.1" class="ltx_text ltx_framed ltx_framed_underline">17.5</span></td>
<td id="Sx4.T2.1.1.7.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.1.7.6" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.7.6.1" class="ltx_text ltx_framed ltx_framed_underline">50.6</span></td>
<td id="Sx4.T2.1.1.7.7" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.7.7.1" class="ltx_text ltx_framed ltx_framed_underline">41.9</span></td>
<td id="Sx4.T2.1.1.7.8" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.7.8.1" class="ltx_text ltx_framed ltx_framed_underline">41.7</span></td>
<td id="Sx4.T2.1.1.7.9" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.7.9.1" class="ltx_text ltx_framed ltx_framed_underline">46.2</span></td>
<td id="Sx4.T2.1.1.7.10" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.7.10.1" class="ltx_text ltx_font_bold">42.7</span></td>
</tr>
<tr id="Sx4.T2.1.1.8" class="ltx_tr">
<td id="Sx4.T2.1.1.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">ICSD</td>
<td id="Sx4.T2.1.1.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">SS1M</td>
<td id="Sx4.T2.1.1.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T2.1.1.8.3.1" class="ltx_text ltx_font_bold">13.6</span></td>
<td id="Sx4.T2.1.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T2.1.1.8.4.1" class="ltx_text ltx_font_bold">18.3</span></td>
<td id="Sx4.T2.1.1.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T2.1.1.8.5.1" class="ltx_text ltx_font_bold">38.0</span></td>
<td id="Sx4.T2.1.1.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T2.1.1.8.6.1" class="ltx_text ltx_font_bold">54.2</span></td>
<td id="Sx4.T2.1.1.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T2.1.1.8.7.1" class="ltx_text ltx_font_bold">42.9</span></td>
<td id="Sx4.T2.1.1.8.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T2.1.1.8.8.1" class="ltx_text ltx_font_bold">44.3</span></td>
<td id="Sx4.T2.1.1.8.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">35.6</td>
<td id="Sx4.T2.1.1.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T2.1.1.8.10.1" class="ltx_text ltx_font_bold">42.7</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The results of cross-domain image captioning on MSCOCO and NoCaps.
</figcaption>
</figure>
<div id="Sx4.SSx1.p3" class="ltx_para ltx_noindent">
<p id="Sx4.SSx1.p3.2" class="ltx_p"><span id="Sx4.SSx1.p3.2.1" class="ltx_text ltx_font_bold">Implementation Details.</span>
During the generation stage, we use different group sizes for various datasets: 30 for MSCOCO, 20 for Flickr30k, and 10 for SS1M. We employ the GPT-3.5-turbo model for selecting and summarizing captions via API access. For image generation, we utilize Stable Diffusion v1.4 at a 512<math id="Sx4.SSx1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx4.SSx1.p3.1.m1.1a"><mo id="Sx4.SSx1.p3.1.m1.1.1" xref="Sx4.SSx1.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.p3.1.m1.1b"><times id="Sx4.SSx1.p3.1.m1.1.1.cmml" xref="Sx4.SSx1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.p3.1.m1.1c">\times</annotation></semantics></math>512 resolution with 20 sampling steps, and we speed up the diffusion modelâ€™s sampling process using DPM-SolverÂ <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al. <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>.
We train the model for 30 epochs using AdamÂ <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba <a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite> and a batch size of 36. The learning rate is 1e-5, and a warm-up strategy is applied during training.
Additionally, the input synthetic images are resized to 384<math id="Sx4.SSx1.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx4.SSx1.p3.2.m2.1a"><mo id="Sx4.SSx1.p3.2.m2.1.1" xref="Sx4.SSx1.p3.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.p3.2.m2.1b"><times id="Sx4.SSx1.p3.2.m2.1.1.cmml" xref="Sx4.SSx1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.p3.2.m2.1c">\times</annotation></semantics></math>384. For inference, we follow the BLIP to use beam search with a beam size of 3.
All experiments are conducted using eight NVIDIA A100 GPUs.</p>
</div>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Comparisons with State-of-the-art Models</h3>

<div id="Sx4.SSx2.p1" class="ltx_para ltx_noindent">
<p id="Sx4.SSx2.p1.1" class="ltx_p"><span id="Sx4.SSx2.p1.1.1" class="ltx_text ltx_font_bold">In-domain Image Captioning.</span>
We perform in-domain image captioning on MSCOCO and Flickr30k datasets, comparing our ICSD with state-of-the-art unsupervised methods:
<cite class="ltx_cite ltx_citemacro_citet">Feng etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Laina, Rupprecht, and Navab (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> train models on independent image and text data, using visual concepts to establish connections between images and text. ZeroCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Tewel etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>, MagicÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>, and ESPER-StyleÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al. <a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> incorporate GPT-2Â <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al. <a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite> as the language decoder. CLIPReÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> is a CLIP-based method for retrieving captions. CapDecÂ <cite class="ltx_cite ltx_citemacro_citep">(Nukrai, Mokady, and Globerson <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, DeCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> and CLOSEÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu, Clark, and Kembhavi <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> conduct text-only training, leveraging the powerful cross-modal capability of CLIP.</p>
</div>
<div id="Sx4.SSx2.p2" class="ltx_para">
<p id="Sx4.SSx2.p2.1" class="ltx_p">The comparison results for MSCOCO and Flickr30k datasets are presented in TableÂ <a href="#Sx3.T1" title="Table 1 â€£ Training Stage â€£ Method â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We generate 150,000 multi-context images for MSCOCO and 140,000 images for Flickr30k, with each synthetic image paired with 5 to 10 captions. Our method significantly outperforms other unsupervised approaches across the majority of metrics. In the B@4 metric, our ICSD surpasses previous state-of-the-art methods by 13.3% on MSCOCO and 26.0% on Flickr30k.</p>
</div>
<div id="Sx4.SSx2.p3" class="ltx_para ltx_noindent">
<p id="Sx4.SSx2.p3.1" class="ltx_p"><span id="Sx4.SSx2.p3.1.1" class="ltx_text ltx_font_bold">Cross-domain Image Captioning.</span>
The captions of MSCOCO and Flickr30k can naturally be grouped, as each image has at least five descriptions in these human-annotated datasets. To evaluate the effectiveness of our ICSD, we train the model on the web-crawled SS1M captions, which are not inherently grouped, and perform cross-domain image captioning on MSCOCO and NoCaps.
We create 150,000 multi-context images for SS1M. Due to SS1Mâ€™s large scale and the API call limitations of GPT-3.5-turbo, we additionally generate uni-context images for each caption.
We compare our method with several other approaches in this experimental setting: (1) ZeroCap and ConZICÂ <cite class="ltx_cite ltx_citemacro_citep">(Zeng etÂ al. <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite> directly use pretrained vision-language models without fine-tuning; (2) CLIPRe and DeCap are trained on the large CC3M-text corpusÂ <cite class="ltx_cite ltx_citemacro_citep">(Changpinyo etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>; (3) DeCap and <cite class="ltx_cite ltx_citemacro_citet">Feng etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> also employ SS1M to train the models.</p>
</div>
<div id="Sx4.SSx2.p4" class="ltx_para">
<p id="Sx4.SSx2.p4.1" class="ltx_p">The results in TableÂ <a href="#Sx4.T2" title="Table 2 â€£ Settings â€£ Experiments â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrate the effectiveness of our method.
When evaluating ICSD on MSCOCO, our method achieves obvious improvements across all metrics. Especially on BLEU and CIDEr metrics, improved from 8.9 to 13.6 and 50.6 to 54.2, respectively.
This implies that the effectiveness of our method is not limited to in-domain image captioning, it remains efficient even when applied to a wide range of data collected from the web.
In the NoCaps evaluation, our method performs well on in-domain and near-domain sets but not as strongly on the out-domain set. This discrepancy arises because SS1M is collected based on the objects of MSCOCO, which is not strictly designed for cross-domain setting, affecting its performance on the out-domain set of NoCaps. However, our method is able to address this limitation by utilizing LLMs to generate a corpus containing diverse objects for generalization and applying our pipeline with this corpus. We report the results of this experiment in Appendix A.</p>
</div>
<figure id="Sx4.F3" class="ltx_figure"><img src="/html/2305.18072/assets/x3.png" id="Sx4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Comparisons of captions generated by CLIPCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Mokady, Hertz, and Bermano <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>, CapDecÂ <cite class="ltx_cite ltx_citemacro_citep">(Nukrai, Mokady, and Globerson <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, and our proposed ICSD, using exemplary images from MSCOCO dataset.
</figcaption>
</figure>
</section>
<section id="Sx4.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Ablation Study</h3>

<div id="Sx4.SSx3.p1" class="ltx_para ltx_noindent">
<p id="Sx4.SSx3.p1.1" class="ltx_p"><span id="Sx4.SSx3.p1.1.1" class="ltx_text ltx_font_bold">The effect of components of ICSD.</span>
TableÂ <a href="#Sx4.T3" title="Table 3 â€£ Ablation Study â€£ Experiments â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results of an ablation study evaluating each component on MSCOCO. The <span id="Sx4.SSx3.p1.1.2" class="ltx_text ltx_font_italic">Baseline</span> indicates that we train the model using only uni-context image-text pairs.
We then investigate three variations of selection and summarization: (1) selection without summarization (<span id="Sx4.SSx3.p1.1.3" class="ltx_text ltx_font_italic">Sel. w.o. Sum.</span>), where we employ LLMs to select captions for training but do not merge them into a single sentence for multi-context image generation.
The captions selected from the same group are associated with the uni-context image, which is generated with query caption of the group.
(2) summarization without selection (<span id="Sx4.SSx3.p1.1.4" class="ltx_text ltx_font_italic">Sum. w.o. Sel.</span>), in which LLMs directly summarize the top five similar captions in the group, potentially leading to errors since the most similar captions are not necessarily conflict-free. (3) both selection and summarization (<span id="Sx4.SSx3.p1.1.5" class="ltx_text ltx_font_italic">Sel. &amp; Sum.</span>), representing our ICSD approach.
To further emphasize the importance of selection in collecting a high-quality caption set for summarization, we conduct experiments using the ground-truth group (<span id="Sx4.SSx3.p1.1.6" class="ltx_text ltx_font_italic">w. GTG</span>) as a substitute for selection. MSCOCO is a human-annotated dataset where each image has at least five captions, inherently containing captions written for the same image by different annotators. In the <span id="Sx4.SSx3.p1.1.7" class="ltx_text ltx_font_italic">w. GTG</span> experiment, we use these captions, written for the same image, for summarization.</p>
</div>
<div id="Sx4.SSx3.p2" class="ltx_para">
<p id="Sx4.SSx3.p2.1" class="ltx_p">The results of <span id="Sx4.SSx3.p2.1.1" class="ltx_text ltx_font_italic">Baseline</span> exhibit limited performance across most metrics. In comparison with previous state-of-the-art methods, the Meteor and CIDEr scores are lower than DeCap by 0.5 and 2.1, respectively, while the ROUGE-L score is lower than CapDec by 0.4. The limited performance indicates that the uni-context images, generated by single captions, are insufficient for image captioning.</p>
</div>
<div id="Sx4.SSx3.p3" class="ltx_para">
<p id="Sx4.SSx3.p3.1" class="ltx_p">The <span id="Sx4.SSx3.p3.1.1" class="ltx_text ltx_font_italic">Sel. w.o. Sum.</span> approach results in improved performance, particularly in the BLEU and CIDEr metrics. This improvement can be attributed to the fact that <span id="Sx4.SSx3.p3.1.2" class="ltx_text ltx_font_italic">Sel. w.o. Sum.</span> employs LLMs to select captions that may describe the same scene and associate them with the image generated using the query caption. This process constructs pseudo multi-context pairs in which the uni-context image is associated with multiple captions, albeit with less accurate correlations.</p>
</div>
<div id="Sx4.SSx3.p4" class="ltx_para">
<p id="Sx4.SSx3.p4.1" class="ltx_p">The <span id="Sx4.SSx3.p4.1.1" class="ltx_text ltx_font_italic">Sum. w.o. Sel.</span> presents another type of pseudo multi-context image-text pair. By directly summarizing similar captions into a single caption through LLMs and generating multi-context images, this approach may introduce errors, as the similarity of captions cannot guarantee that they are conflict-free when describing the same scene. Consequently, the summarized caption may not be fully compatible with the original simple captions. However, the generated images are multi-context images, resulting in improved performance compared to <span id="Sx4.SSx3.p4.1.2" class="ltx_text ltx_font_italic">Sel. w.o. Sum.</span> method. This observation demonstrates the importance of multi-context images for image captioning, but the inaccurate correlations lead to inferior performance in this approach.</p>
</div>
<div id="Sx4.SSx3.p5" class="ltx_para">
<p id="Sx4.SSx3.p5.1" class="ltx_p">The performance of our proposed ICSD approach, which incorporates both <span id="Sx4.SSx3.p5.1.1" class="ltx_text ltx_font_italic">Sel. &amp; Sum.</span>, is significantly improved. As the selection process can gather compatible captions for summarization, which is absent in the <span id="Sx4.SSx3.p5.1.2" class="ltx_text ltx_font_italic">Sum. w.o. Sel.</span> method, summarization can create multi-context captions for multi-context image generation, an aspect that is lacking in the <span id="Sx4.SSx3.p5.1.3" class="ltx_text ltx_font_italic">Sel. w.o. Sum.</span> approach.
The results indicate that by combining selection and summarization, our ICSD can generate multi-context data that are highly beneficial for image captioning. To further explore our method, we use the ground-truth grouping of MSCOCO and summarize captions.
This strategy notably enhances performance across all metrics, underscoring the importance of effective grouping.</p>
</div>
<figure id="Sx4.T3" class="ltx_table">
<div id="Sx4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:202.4pt;height:96.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.4pt,5.9pt) scale(0.890549161686173,0.890549161686173) ;">
<table id="Sx4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx4.T3.1.1.1" class="ltx_tr">
<td id="Sx4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td id="Sx4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">B@4</td>
<td id="Sx4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">M</td>
<td id="Sx4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">R</td>
<td id="Sx4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">C</td>
</tr>
<tr id="Sx4.T3.1.1.2" class="ltx_tr">
<td id="Sx4.T3.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Baseline</td>
<td id="Sx4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">26.9</td>
<td id="Sx4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">24.7</td>
<td id="Sx4.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="Sx4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">90.8</td>
</tr>
<tr id="Sx4.T3.1.1.3" class="ltx_tr">
<td id="Sx4.T3.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Sel. w.o. Sum.</td>
<td id="Sx4.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">29.5</td>
<td id="Sx4.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">24.7</td>
<td id="Sx4.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">52.3</td>
<td id="Sx4.T3.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">94.2</td>
</tr>
<tr id="Sx4.T3.1.1.4" class="ltx_tr">
<td id="Sx4.T3.1.1.4.1" class="ltx_td ltx_align_left">Sum. w.o. Sel.</td>
<td id="Sx4.T3.1.1.4.2" class="ltx_td ltx_align_center">29.6</td>
<td id="Sx4.T3.1.1.4.3" class="ltx_td ltx_align_center">25.0</td>
<td id="Sx4.T3.1.1.4.4" class="ltx_td ltx_align_center">52.9</td>
<td id="Sx4.T3.1.1.4.5" class="ltx_td ltx_align_center">95.8</td>
</tr>
<tr id="Sx4.T3.1.1.5" class="ltx_tr">
<td id="Sx4.T3.1.1.5.1" class="ltx_td ltx_align_left">Sel. &amp; Sum. (ICSD)</td>
<td id="Sx4.T3.1.1.5.2" class="ltx_td ltx_align_center">29.9</td>
<td id="Sx4.T3.1.1.5.3" class="ltx_td ltx_align_center">25.4</td>
<td id="Sx4.T3.1.1.5.4" class="ltx_td ltx_align_center">52.7</td>
<td id="Sx4.T3.1.1.5.5" class="ltx_td ltx_align_center">96.6</td>
</tr>
<tr id="Sx4.T3.1.1.6" class="ltx_tr">
<td id="Sx4.T3.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">ICSD <span id="Sx4.T3.1.1.6.1.1" class="ltx_text ltx_font_italic">w. GTG</span>
</td>
<td id="Sx4.T3.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T3.1.1.6.2.1" class="ltx_text ltx_font_bold">30.2</span></td>
<td id="Sx4.T3.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T3.1.1.6.3.1" class="ltx_text ltx_font_bold">25.7</span></td>
<td id="Sx4.T3.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T3.1.1.6.4.1" class="ltx_text ltx_font_bold">53.0</span></td>
<td id="Sx4.T3.1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx4.T3.1.1.6.5.1" class="ltx_text ltx_font_bold">97.3</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The effect of components of ICSD on MSCOCO.
</figcaption>
</figure>
<div id="Sx4.SSx3.p6" class="ltx_para ltx_noindent">
<p id="Sx4.SSx3.p6.1" class="ltx_p"><span id="Sx4.SSx3.p6.1.1" class="ltx_text ltx_font_bold">The impact of the number of multi-context images.</span>
We conduct experiments using different numbers of multi-context images for training and evaluate the model on the test split of MSCOCO. TableÂ <a href="#Sx4.T4" title="Table 4 â€£ Ablation Study â€£ Experiments â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results, where we increase the number of multi-context images from 10,000 to 200,000.
The results demonstrate that incorporating more multi-context images during training can improve the performance of in-domain image captioning. Particularly, we observe significant gains in the B@4 and CIDEr metrics when increasing the number of multi-context images from 10,000 to 50,000. The best performance achieved thus far is obtained when using 150,000 multi-context images.
However, expanding the number of multi-context images to 200,000 yields only marginal improvements, primarily due to the constrained diversity of the text corpus.</p>
</div>
<div id="Sx4.SSx3.p7" class="ltx_para ltx_noindent">
<p id="Sx4.SSx3.p7.1" class="ltx_p"><span id="Sx4.SSx3.p7.1.1" class="ltx_text ltx_font_bold">Visualization.</span>
Our model, trained on synthetic data, is capable of generating accurate captions for natural images. In FigureÂ <a href="#Sx4.F3" title="Figure 3 â€£ Comparisons with State-of-the-art Models â€£ Experiments â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present examples of generated captions on the test split of MSCOCO, comparing our approach with CLIPCap and CapDec. The incorrect portions of the captions are highlighted in red, while the improvements made by our method are emphasized in green.
For the first image, our method accurately describes the location and the number of people. Regarding the second and third images, our approach outperforms the others by capturing more detailed descriptions, such as the colors â€œblack and whiteâ€ and â€œbrown and whiteâ€.</p>
</div>
<div id="Sx4.SSx3.p8" class="ltx_para">
<p id="Sx4.SSx3.p8.1" class="ltx_p">In Appendix B, we further explore a range of alternative methods, including retrieving images instead of generating them, as well as creating a detailed caption from a given input caption, as opposed to merging multiple simple captions.</p>
</div>
<figure id="Sx4.T4" class="ltx_table">
<div id="Sx4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:177.1pt;height:109.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.9pt,-0.6pt) scale(1.0108331702925,1.0108331702925) ;">
<table id="Sx4.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx4.T4.1.1.1" class="ltx_tr">
<td id="Sx4.T4.1.1.1.1" class="ltx_td ltx_align_right ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">Number</td>
<td id="Sx4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">B@4</td>
<td id="Sx4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">M</td>
<td id="Sx4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">R</td>
<td id="Sx4.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">C</td>
</tr>
<tr id="Sx4.T4.1.1.2" class="ltx_tr">
<td id="Sx4.T4.1.1.2.1" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">10,000</td>
<td id="Sx4.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">28.0</td>
<td id="Sx4.T4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">23.8</td>
<td id="Sx4.T4.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">51.1</td>
<td id="Sx4.T4.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">87.9</td>
</tr>
<tr id="Sx4.T4.1.1.3" class="ltx_tr">
<td id="Sx4.T4.1.1.3.1" class="ltx_td ltx_align_right" style="padding-top:-1pt;padding-bottom:-1pt;">50,000</td>
<td id="Sx4.T4.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">29.3</td>
<td id="Sx4.T4.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">24.7</td>
<td id="Sx4.T4.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">52.3</td>
<td id="Sx4.T4.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">93.5</td>
</tr>
<tr id="Sx4.T4.1.1.4" class="ltx_tr">
<td id="Sx4.T4.1.1.4.1" class="ltx_td ltx_align_right" style="padding-top:-1pt;padding-bottom:-1pt;">100,000</td>
<td id="Sx4.T4.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">29.4</td>
<td id="Sx4.T4.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">25.0</td>
<td id="Sx4.T4.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">52.3</td>
<td id="Sx4.T4.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">95.1</td>
</tr>
<tr id="Sx4.T4.1.1.5" class="ltx_tr">
<td id="Sx4.T4.1.1.5.1" class="ltx_td ltx_align_right" style="padding-top:-1pt;padding-bottom:-1pt;">150,000</td>
<td id="Sx4.T4.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">29.9</td>
<td id="Sx4.T4.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="Sx4.T4.1.1.5.3.1" class="ltx_text ltx_font_bold">25.4</span></td>
<td id="Sx4.T4.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="Sx4.T4.1.1.5.4.1" class="ltx_text ltx_font_bold">52.7</span></td>
<td id="Sx4.T4.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="Sx4.T4.1.1.5.5.1" class="ltx_text ltx_font_bold">96.6</span></td>
</tr>
<tr id="Sx4.T4.1.1.6" class="ltx_tr">
<td id="Sx4.T4.1.1.6.1" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;">200,000</td>
<td id="Sx4.T4.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="Sx4.T4.1.1.6.2.1" class="ltx_text ltx_font_bold">30.1</span></td>
<td id="Sx4.T4.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;">25.1</td>
<td id="Sx4.T4.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="Sx4.T4.1.1.6.4.1" class="ltx_text ltx_font_bold">52.7</span></td>
<td id="Sx4.T4.1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;">96.5</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The impact of the number of multi-context images.</figcaption>
</figure>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">We observe that synthetic images generated from single captions lack the ability to be described from multiple perspectives, unlike real-world images. To address this issue, we propose a pipeline called ICSD that generates multi-context training data by combining LLMs and diffusion models for image captioning. The pipeline has two stages: generation and training. In the generation stage, we group captions in the corpus and select diverse perspectives using LLMs. These perspectives are summarized into a single sentence, which is then used to generate multi-context images through diffusion models. This results in high-quality synthetic multi-context image-text pairs where each image can be described from various perspectives. In the training stage, we train image captioning models using the synthetic data generated in the generation stage. Extensive experiments on in-domain and cross-domain image captioning demonstrate the effectiveness of our ICSD pipeline.</p>
</div>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">This work was in part supported by the National Natural Science Foundation of China under grants 62032006 and 62021001.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2019)</span>
<span class="ltx_bibblock">
Agrawal, H.; Desai, K.; Wang, Y.; Chen, X.; Jain, R.; Johnson, M.; Batra, D.; Parikh, D.; Lee, S.; and Anderson, P. 2019.

</span>
<span class="ltx_bibblock">Nocaps: Novel object captioning at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 8948â€“8957.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alahmadi and Hahn (2022)</span>
<span class="ltx_bibblock">
Alahmadi, R.; and Hahn, J. 2022.

</span>
<span class="ltx_bibblock">Improve Image Captioning by Estimating the Gazing Patterns from the Caption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 1025â€“1034.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson etÂ al. (2018)</span>
<span class="ltx_bibblock">
Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 6077â€“6086.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Azizi, S.; Kornblith, S.; Saharia, C.; Norouzi, M.; and Fleet, D.Â J. 2023.

</span>
<span class="ltx_bibblock">Synthetic Data from Diffusion Models Improves ImageNet Classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Banerjee, S.; and Lavie, A. 2005.

</span>
<span class="ltx_bibblock">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>, 65â€“72.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barraco etÂ al. (2022)</span>
<span class="ltx_bibblock">
Barraco, M.; Cornia, M.; Cascianelli, S.; Baraldi, L.; and Cucchiara, R. 2022.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of CLIP features for image captioning: an experimental analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CVPR Workshops</em>, 4662â€“4670.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo etÂ al. (2021)</span>
<span class="ltx_bibblock">
Changpinyo, S.; Sharma, P.; Ding, N.; and Soricut, R. 2021.

</span>
<span class="ltx_bibblock">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 3558â€“3568.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen, Zhang, and Hinton (2023)</span>
<span class="ltx_bibblock">
Chen, T.; Zhang, R.; and Hinton, G. 2023.

</span>
<span class="ltx_bibblock">Analog bits: Generating discrete data using diffusion models with self-conditioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2017)</span>
<span class="ltx_bibblock">
Dai, B.; Fidler, S.; Urtasun, R.; and Lin, D. 2017.

</span>
<span class="ltx_bibblock">Towards diverse and natural image descriptions via a conditional gan.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2970â€“2979.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai and Lin (2017)</span>
<span class="ltx_bibblock">
Dai, B.; and Lin, D. 2017.

</span>
<span class="ltx_bibblock">Contrastive learning for image captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 30.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 4171â€“4186.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal and Nichol (2021)</span>
<span class="ltx_bibblock">
Dhariwal, P.; and Nichol, A. 2021.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 34: 8780â€“8794.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2022.

</span>
<span class="ltx_bibblock">Injecting semantic concepts into end-to-end image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 18009â€“18019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2019)</span>
<span class="ltx_bibblock">
Feng, Y.; Ma, L.; Liu, W.; and Luo, J. 2019.

</span>
<span class="ltx_bibblock">Unsupervised image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 4125â€“4134.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu, Clark, and Kembhavi (2023)</span>
<span class="ltx_bibblock">
Gu, S.; Clark, C.; and Kembhavi, A. 2023.

</span>
<span class="ltx_bibblock">I Canâ€™t Believe Thereâ€™s No Images! Learning Visual Tasks Using only Language Supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2672â€“2683.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2023)</span>
<span class="ltx_bibblock">
He, R.; Sun, S.; Yu, X.; Xue, C.; Zhang, W.; Torr, P.; Bai, S.; and Qi, X. 2023.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for image recognition?

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho, Jain, and Abbeel (2020)</span>
<span class="ltx_bibblock">
Ho, J.; Jain, A.; and Abbeel, P. 2020.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 33: 6840â€“6851.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2022.

</span>
<span class="ltx_bibblock">Scaling up vision-language pre-training for image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 17980â€“17989.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Kang, W.; Mun, J.; Lee, S.; and Roh, B. 2023.

</span>
<span class="ltx_bibblock">Noise-aware learning from web-crawled image-text data for image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2942â€“2952.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy and Fei-Fei (2015)</span>
<span class="ltx_bibblock">
Karpathy, A.; and Fei-Fei, L. 2015.

</span>
<span class="ltx_bibblock">Deep visual-semantic alignments for generating image descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 3128â€“3137.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Kingma, D.Â P.; and Ba, J. 2015.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2017)</span>
<span class="ltx_bibblock">
Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, D.Â A.; etÂ al. 2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laina, Rupprecht, and Navab (2019)</span>
<span class="ltx_bibblock">
Laina, I.; Rupprecht, C.; and Navab, N. 2019.

</span>
<span class="ltx_bibblock">Towards unsupervised image captioning with shared multimodal embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 7414â€“7424.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2022)</span>
<span class="ltx_bibblock">
Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ICML</em>, volume 162, 12888â€“12900. PMLR.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Li, W.; Zhu, L.; Wen, L.; and Yang, Y. 2023.

</span>
<span class="ltx_bibblock">DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Lin, C.-Y. 2004.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 74â€“81.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; DollÃ¡r, P.; and Zitnick, C.Â L. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 740â€“755. Springer.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Lu, C.; Zhou, Y.; Bao, F.; Chen, J.; Li, C.; and Zhu, J. 2022.

</span>
<span class="ltx_bibblock">Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 35: 5775â€“5787.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Meng, Z.; Yang, D.; Cao, X.; Shah, A.; and Lim, S.-N. 2022.

</span>
<span class="ltx_bibblock">Object-Centric Unsupervised Image Captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 219â€“235. Springer.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mokady, Hertz, and Bermano (2021)</span>
<span class="ltx_bibblock">
Mokady, R.; Hertz, A.; and Bermano, A.Â H. 2021.

</span>
<span class="ltx_bibblock">Clipcap: Clip prefix for image captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09734</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol etÂ al. (2022)</span>
<span class="ltx_bibblock">
Nichol, A.Â Q.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin, P.; Mcgrew, B.; Sutskever, I.; and Chen, M. 2022.

</span>
<span class="ltx_bibblock">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ICML</em>, volume 162, 16784â€“16804. PMLR.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nukrai, Mokady, and Globerson (2022)</span>
<span class="ltx_bibblock">
Nukrai, D.; Mokady, R.; and Globerson, A. 2022.

</span>
<span class="ltx_bibblock">Text-Only Training for Image Captioning using Noise-Injected CLIP.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 4055â€“4063.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 311â€“318.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019.

</span>
<span class="ltx_bibblock">Language Models are Unsupervised Multitask Learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, 1(8): 9.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M. 2022.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.06125</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach etÂ al. (2022)</span>
<span class="ltx_bibblock">
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 10684â€“10695.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia etÂ al. (2022)</span>
<span class="ltx_bibblock">
Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E.Â L.; Ghasemipour, K.; GontijoÂ Lopes, R.; KaragolÂ Ayan, B.; Salimans, T.; etÂ al. 2022.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 35: 36479â€“36494.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann etÂ al. (2022)</span>
<span class="ltx_bibblock">
Schuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C.; Wightman, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis, C.; Wortsman, M.; etÂ al. 2022.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 35: 25278â€“25294.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song etÂ al. (2021)</span>
<span class="ltx_bibblock">
Song, Z.; Zhou, X.; Dong, L.; Tan, J.; and Guo, L. 2021.

</span>
<span class="ltx_bibblock">Direction relation transformer for image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ACM MM</em>, 5056â€“5064.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2022)</span>
<span class="ltx_bibblock">
Su, Y.; Lan, T.; Liu, Y.; Liu, F.; Yogatama, D.; Wang, Y.; Kong, L.; and Collier, N. 2022.

</span>
<span class="ltx_bibblock">Language models can see: plugging visual controls in text generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.02655</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tewel etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tewel, Y.; Shalev, Y.; Schwartz, I.; and Wolf, L. 2022.

</span>
<span class="ltx_bibblock">Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 17918â€“17928.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.Â N.; Kaiser, Å.; and Polosukhin, I. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 30.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam, LawrenceÂ Zitnick, and Parikh (2015)</span>
<span class="ltx_bibblock">
Vedantam, R.; LawrenceÂ Zitnick, C.; and Parikh, D. 2015.

</span>
<span class="ltx_bibblock">Cider: Consensus-based image description evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 4566â€“4575.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villegas etÂ al. (2023)</span>
<span class="ltx_bibblock">
Villegas, R.; Babaeizadeh, M.; Kindermans, P.-J.; Moraldo, H.; Zhang, H.; Saffar, M.Â T.; Castro, S.; Kunze, J.; and Erhan, D. 2023.

</span>
<span class="ltx_bibblock">Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vinyals etÂ al. (2015)</span>
<span class="ltx_bibblock">
Vinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.

</span>
<span class="ltx_bibblock">Show and tell: A neural image caption generator.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang, Xu, and Sun (2022)</span>
<span class="ltx_bibblock">
Wang, Y.; Xu, J.; and Sun, Y. 2022.

</span>
<span class="ltx_bibblock">End-to-end transformer based model for image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, volumeÂ 36, 2585â€“2594.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wang, Z.; Yu, J.; Yu, A.Â W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2022.

</span>
<span class="ltx_bibblock">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q.Â V.; Zhou, D.; etÂ al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 35: 24824â€“24837.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wu, M.; Zhang, X.; Sun, X.; Zhou, Y.; Chen, C.; Gu, J.; Sun, X.; and Ji, R. 2022.

</span>
<span class="ltx_bibblock">Difnet: Boosting visual information flow for image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 18020â€“18029.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yao, L.; Huang, R.; Hou, L.; Lu, G.; Niu, M.; Xu, H.; Liang, X.; Li, Z.; Jiang, X.; and Xu, C. 2021.

</span>
<span class="ltx_bibblock">FILIP: Fine-grained Interactive Language-Image Pre-Training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young etÂ al. (2014)</span>
<span class="ltx_bibblock">
Young, P.; Lai, A.; Hodosh, M.; and Hockenmaier, J. 2014.

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 2: 67â€“78.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yu, Y.; Chung, J.; Yun, H.; Hessel, J.; Park, J.; Lu, X.; Ammanabrolu, P.; Zellers, R.; Bras, R.Â L.; Kim, G.; etÂ al. 2022.

</span>
<span class="ltx_bibblock">Multimodal knowledge alignment with reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12630</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zeng, Z.; Zhang, H.; Wang, Z.; Lu, R.; Wang, D.; and Chen, B. 2023.

</span>
<span class="ltx_bibblock">ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 23465â€“23476.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhao, H.; Sheng, D.; Bao, J.; Chen, D.; Chen, D.; Wen, F.; Yuan, L.; Liu, C.; Zhou, W.; Chu, Q.; etÂ al. 2023.

</span>
<span class="ltx_bibblock">X-Paste: Revisit Copy-Paste at Scale with CLIP and StableDiffusion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">ICML</em>, volume 202, 42098â€“42109. PMLR.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="Sx6.T5" class="ltx_table">
<div id="Sx6.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:404.7pt;height:126pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-88.2pt,27.3pt) scale(0.696334173538346,0.696334173538346) ;">
<table id="Sx6.T5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx6.T5.1.1.1" class="ltx_tr">
<td id="Sx6.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="Sx6.T5.1.1.1.1.1" class="ltx_text">Methods</span></td>
<td id="Sx6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="Sx6.T5.1.1.1.2.1" class="ltx_text">Dataset</span></td>
<td id="Sx6.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">MSCOCO</td>
<td id="Sx6.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">NoCaps val (CIDEr)</td>
</tr>
<tr id="Sx6.T5.1.1.2" class="ltx_tr">
<td id="Sx6.T5.1.1.2.1" class="ltx_td ltx_align_center">B@4</td>
<td id="Sx6.T5.1.1.2.2" class="ltx_td ltx_align_center">M</td>
<td id="Sx6.T5.1.1.2.3" class="ltx_td ltx_align_center">R</td>
<td id="Sx6.T5.1.1.2.4" class="ltx_td ltx_align_center">C</td>
<td id="Sx6.T5.1.1.2.5" class="ltx_td ltx_align_center">In</td>
<td id="Sx6.T5.1.1.2.6" class="ltx_td ltx_align_center">Near</td>
<td id="Sx6.T5.1.1.2.7" class="ltx_td ltx_align_center">Out</td>
<td id="Sx6.T5.1.1.2.8" class="ltx_td ltx_align_center">Overall</td>
</tr>
<tr id="Sx6.T5.1.1.3" class="ltx_tr">
<td id="Sx6.T5.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">ZeroCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Tewel etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx6.T5.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx6.T5.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">2.6</td>
<td id="Sx6.T5.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">11.5</td>
<td id="Sx6.T5.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx6.T5.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">14.6</td>
<td id="Sx6.T5.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx6.T5.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx6.T5.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx6.T5.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="Sx6.T5.1.1.4" class="ltx_tr">
<td id="Sx6.T5.1.1.4.1" class="ltx_td ltx_align_left">ConZICÂ <cite class="ltx_cite ltx_citemacro_citep">(Zeng etÂ al. <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx6.T5.1.1.4.2" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.4.3" class="ltx_td ltx_align_center">1.3</td>
<td id="Sx6.T5.1.1.4.4" class="ltx_td ltx_align_center">11.5</td>
<td id="Sx6.T5.1.1.4.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.4.6" class="ltx_td ltx_align_center">12.8</td>
<td id="Sx6.T5.1.1.4.7" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.4.8" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.4.9" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.4.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx6.T5.1.1.5" class="ltx_tr">
<td id="Sx6.T5.1.1.5.1" class="ltx_td ltx_align_left">CLIPReÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="Sx6.T5.1.1.5.2" class="ltx_td ltx_align_center">CC3M-text</td>
<td id="Sx6.T5.1.1.5.3" class="ltx_td ltx_align_center">4.6</td>
<td id="Sx6.T5.1.1.5.4" class="ltx_td ltx_align_center">13.3</td>
<td id="Sx6.T5.1.1.5.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.5.6" class="ltx_td ltx_align_center">25.6</td>
<td id="Sx6.T5.1.1.5.7" class="ltx_td ltx_align_center">23.3</td>
<td id="Sx6.T5.1.1.5.8" class="ltx_td ltx_align_center">26.8</td>
<td id="Sx6.T5.1.1.5.9" class="ltx_td ltx_align_center">36.5</td>
<td id="Sx6.T5.1.1.5.10" class="ltx_td ltx_align_center">28.2</td>
</tr>
<tr id="Sx6.T5.1.1.6" class="ltx_tr">
<td id="Sx6.T5.1.1.6.1" class="ltx_td ltx_align_left">DeCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx6.T5.1.1.6.2" class="ltx_td ltx_align_center">CC3M-text</td>
<td id="Sx6.T5.1.1.6.3" class="ltx_td ltx_align_center">8.8</td>
<td id="Sx6.T5.1.1.6.4" class="ltx_td ltx_align_center">16.0</td>
<td id="Sx6.T5.1.1.6.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.6.6" class="ltx_td ltx_align_center">42.1</td>
<td id="Sx6.T5.1.1.6.7" class="ltx_td ltx_align_center">34.8</td>
<td id="Sx6.T5.1.1.6.8" class="ltx_td ltx_align_center">37.7</td>
<td id="Sx6.T5.1.1.6.9" class="ltx_td ltx_align_center"><span id="Sx6.T5.1.1.6.9.1" class="ltx_text ltx_font_bold">49.9</span></td>
<td id="Sx6.T5.1.1.6.10" class="ltx_td ltx_align_center">39.7</td>
</tr>
<tr id="Sx6.T5.1.1.7" class="ltx_tr">
<td id="Sx6.T5.1.1.7.1" class="ltx_td ltx_align_left">DeCapÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx6.T5.1.1.7.2" class="ltx_td ltx_align_center">SS1M</td>
<td id="Sx6.T5.1.1.7.3" class="ltx_td ltx_align_center">8.9</td>
<td id="Sx6.T5.1.1.7.4" class="ltx_td ltx_align_center">17.5</td>
<td id="Sx6.T5.1.1.7.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx6.T5.1.1.7.6" class="ltx_td ltx_align_center">50.6</td>
<td id="Sx6.T5.1.1.7.7" class="ltx_td ltx_align_center">41.9</td>
<td id="Sx6.T5.1.1.7.8" class="ltx_td ltx_align_center">41.7</td>
<td id="Sx6.T5.1.1.7.9" class="ltx_td ltx_align_center">46.2</td>
<td id="Sx6.T5.1.1.7.10" class="ltx_td ltx_align_center">42.7</td>
</tr>
<tr id="Sx6.T5.1.1.8" class="ltx_tr">
<td id="Sx6.T5.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">ICSD</td>
<td id="Sx6.T5.1.1.8.2" class="ltx_td ltx_align_center ltx_border_t">SS1M</td>
<td id="Sx6.T5.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t">13.6</td>
<td id="Sx6.T5.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx6.T5.1.1.8.4.1" class="ltx_text ltx_framed ltx_framed_underline">18.3</span></td>
<td id="Sx6.T5.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t">38.0</td>
<td id="Sx6.T5.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx6.T5.1.1.8.6.1" class="ltx_text ltx_framed ltx_framed_underline">54.2</span></td>
<td id="Sx6.T5.1.1.8.7" class="ltx_td ltx_align_center ltx_border_t">42.9</td>
<td id="Sx6.T5.1.1.8.8" class="ltx_td ltx_align_center ltx_border_t">44.3</td>
<td id="Sx6.T5.1.1.8.9" class="ltx_td ltx_align_center ltx_border_t">35.6</td>
<td id="Sx6.T5.1.1.8.10" class="ltx_td ltx_align_center ltx_border_t">42.7</td>
</tr>
<tr id="Sx6.T5.1.1.9" class="ltx_tr">
<td id="Sx6.T5.1.1.9.1" class="ltx_td ltx_align_left">ICSD</td>
<td id="Sx6.T5.1.1.9.2" class="ltx_td ltx_align_center">GenT</td>
<td id="Sx6.T5.1.1.9.3" class="ltx_td ltx_align_center"><span id="Sx6.T5.1.1.9.3.1" class="ltx_text ltx_framed ltx_framed_underline">15.0</span></td>
<td id="Sx6.T5.1.1.9.4" class="ltx_td ltx_align_center">18.2</td>
<td id="Sx6.T5.1.1.9.5" class="ltx_td ltx_align_center"><span id="Sx6.T5.1.1.9.5.1" class="ltx_text ltx_font_bold">41.4</span></td>
<td id="Sx6.T5.1.1.9.6" class="ltx_td ltx_align_center">47.2</td>
<td id="Sx6.T5.1.1.9.7" class="ltx_td ltx_align_center"><span id="Sx6.T5.1.1.9.7.1" class="ltx_text ltx_framed ltx_framed_underline">51.3</span></td>
<td id="Sx6.T5.1.1.9.8" class="ltx_td ltx_align_center"><span id="Sx6.T5.1.1.9.8.1" class="ltx_text ltx_framed ltx_framed_underline">47.0</span></td>
<td id="Sx6.T5.1.1.9.9" class="ltx_td ltx_align_center">40.3</td>
<td id="Sx6.T5.1.1.9.10" class="ltx_td ltx_align_center"><span id="Sx6.T5.1.1.9.10.1" class="ltx_text ltx_framed ltx_framed_underline">46.4</span></td>
</tr>
<tr id="Sx6.T5.1.1.10" class="ltx_tr">
<td id="Sx6.T5.1.1.10.1" class="ltx_td ltx_align_left ltx_border_bb">ICSD</td>
<td id="Sx6.T5.1.1.10.2" class="ltx_td ltx_align_center ltx_border_bb">SS1M+GenT</td>
<td id="Sx6.T5.1.1.10.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.3.1" class="ltx_text ltx_font_bold">16.0</span></td>
<td id="Sx6.T5.1.1.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.4.1" class="ltx_text ltx_font_bold">18.9</span></td>
<td id="Sx6.T5.1.1.10.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.5.1" class="ltx_text ltx_framed ltx_framed_underline">40.9</span></td>
<td id="Sx6.T5.1.1.10.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.6.1" class="ltx_text ltx_font_bold">58.3</span></td>
<td id="Sx6.T5.1.1.10.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.7.1" class="ltx_text ltx_font_bold">54.3</span></td>
<td id="Sx6.T5.1.1.10.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.8.1" class="ltx_text ltx_font_bold">54.0</span></td>
<td id="Sx6.T5.1.1.10.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.9.1" class="ltx_text ltx_framed ltx_framed_underline">48.3</span></td>
<td id="Sx6.T5.1.1.10.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx6.T5.1.1.10.10.1" class="ltx_text ltx_font_bold">53.5</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Cross-domain image captioning on MSCOCO Karpathy-test split and NoCaps validation set.
â€œI.â€ and â€œT.â€ denote image data and text data, respectively.
B@4: BLEU@4; M: METEOR; R: ROUGE; C: CIDEr.
Numbers in <span id="Sx6.T5.2.1" class="ltx_text ltx_font_bold">bold</span> and <span id="Sx6.T5.3.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span> text represent the best and second-best results, respectively.
</figcaption>
</figure>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">A â€ƒCross-Domain Image Captioning</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">We evaluate our method on cross-domain image captioning to assess its generalization capability. As shown in TableÂ <a href="#Sx6.T5" title="Table 5 â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, our ICSD, trained on the text of SS1M, performs well on in-domain and near-domain sets of NoCaps, but its performance on the out-domain set is limited. We identified the core issue as the SS1M dataset being collected based on the objects of MSCOCO, which is not designed for cross-domain settings, thus affecting its performance on the out-domain set of NoCaps.
Since our ICSD can be applied to various text corpora, including web-crawled text and text generated from LLMs, we can address this issue by employing LLMs to generate diverse text, thereby enhancing the generalization capability.</p>
</div>
<section id="Sx7.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Generate Text Corpus with LLMs</h3>

<div id="Sx7.SSx1.p1" class="ltx_para">
<p id="Sx7.SSx1.p1.1" class="ltx_p">To achieve cross-domain image captioning, a large and diverse text corpus is essential. Our pipeline can effectively utilize LLMs to obtain such a text corpus.</p>
</div>
<div id="Sx7.SSx1.p2" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p2.1" class="ltx_p"><span id="Sx7.SSx1.p2.1.1" class="ltx_text ltx_font_bold">Object Collections.</span>
We first construct a set of objects for generating a text corpus. Unlike SS1M, which is based on the 80 objects from the MSCOCO dataset, our aim is to collect a large-scale array of objects that are not limited to a specific domain. Our objects are sourced from existing datasets or generated using LLMs. Specifically, we utilize the 80 objects from MSCOCO and randomly collect 2,500 objects from the Visual GenomeÂ <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al. <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>. Additionally, we employ LLMs to generate 400 common objects. As a result, we gather nearly 3,000 objects for text corpus generation.
These objects are not collected for a specific domain but rather serve a more general purpose in cross-domain image captioning.</p>
</div>
<div id="Sx7.SSx1.p3" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p3.1" class="ltx_p"><span id="Sx7.SSx1.p3.1.1" class="ltx_text ltx_font_bold">Text Corpus Generation.</span>
We use the collected objects as the foundation for corpus generation. We instruct LLMs to generate high-quality text based on the given objects. To achieve better results, we randomly select 80 objects as context for LLMs, rather than providing all objects at once.
The prompt is:</p>
</div>
<div id="Sx7.SSx1.p4" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p4.1" class="ltx_p"><span id="Sx7.SSx1.p4.1.1" class="ltx_text ltx_font_italic">Given 100 objects [ person, man, woman, people, â€¦]
Using the provided objects to generate captions that describe real-world images in an objective manner.
The generated sentences are captions for images, the scene described in the sentence must be reasonably realistic.
The generate 100 sentences should follow these requirements: </span></p>
</div>
<div id="Sx7.SSx1.p5" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p5.1" class="ltx_p"><span id="Sx7.SSx1.p5.1.1" class="ltx_text ltx_font_italic">1. Descriptive: The captions provide clear and accurate descriptions of the objects and actions in the scene.</span></p>
</div>
<div id="Sx7.SSx1.p6" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p6.1" class="ltx_p"><span id="Sx7.SSx1.p6.1.1" class="ltx_text ltx_font_italic">2. Concise: The captions are no longer than 15 words, but more than 8 words. </span></p>
</div>
<div id="Sx7.SSx1.p7" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p7.1" class="ltx_p"><span id="Sx7.SSx1.p7.1.1" class="ltx_text ltx_font_italic">3. Objective: The descriptions focus on the factual aspects of the scene, avoiding subjective interpretations or emotions.</span></p>
</div>
<div id="Sx7.SSx1.p8" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p8.1" class="ltx_p"><span id="Sx7.SSx1.p8.1.1" class="ltx_text ltx_font_italic">4. Present tense: The captions describe events happening in the present moment, not past or future events.</span></p>
</div>
<div id="Sx7.SSx1.p9" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p9.1" class="ltx_p"><span id="Sx7.SSx1.p9.1.1" class="ltx_text ltx_font_italic">5. No adverbs: The sentences do not contain adverbs, making the descriptions more straightforward.</span></p>
</div>
<div id="Sx7.SSx1.p10" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p10.1" class="ltx_p"><span id="Sx7.SSx1.p10.1.1" class="ltx_text ltx_font_italic">6. Avoid starting with certain phrases: The captions do not begin with phrases like â€˜Thereâ€™, â€˜An imageâ€™, or â€˜A photo ofâ€™.</span></p>
</div>
<div id="Sx7.SSx1.p11" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p11.1" class="ltx_p"><span id="Sx7.SSx1.p11.1.1" class="ltx_text ltx_font_italic">Please output the sentences with â€˜;â€™ as the separator.</span></p>
</div>
<div id="Sx7.SSx1.p12" class="ltx_para ltx_noindent">
<p id="Sx7.SSx1.p12.1" class="ltx_p"><span id="Sx7.SSx1.p12.1.1" class="ltx_text ltx_font_bold">Post-processing.</span>
We then apply post-processing to ensure the quality of the generated text, removing sentences that are longer than 15 words or shorter than 8 words. Ultimately, we construct a <span id="Sx7.SSx1.p12.1.2" class="ltx_text ltx_font_bold">Gen</span>erated <span id="Sx7.SSx1.p12.1.3" class="ltx_text ltx_font_bold">T</span>ext corpus containing 450,000 sentences, which we refer to as <span id="Sx7.SSx1.p12.1.4" class="ltx_text ltx_font_bold">GenT</span>.</p>
</div>
</section>
<section id="Sx7.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Experimental Results</h3>

<div id="Sx7.SSx2.p1" class="ltx_para">
<p id="Sx7.SSx2.p1.1" class="ltx_p">In TableÂ <a href="#Sx6.T5" title="Table 5 â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we apply our pipeline to GenT to develop an image captioning model with enhanced generalization capability. We create 50,000 multi-context images with 3 to 8 captions for each image We observe significant performance improvements on NoCaps, particularly the CIDEr score on the out-domain set, which increases from 35.6 to 40.3. Since GenT is generated from a large number of objects by LLMs, our pipeline effectively leverages GenT, resulting in robust generalization capability for cross-domain image captioning.</p>
</div>
<div id="Sx7.SSx2.p2" class="ltx_para">
<p id="Sx7.SSx2.p2.1" class="ltx_p">Additionally, we combine SS1M and GenT for our method, leading to substantial performance improvements on both MSCOCO and NoCaps. On MSCOCO, the BLEU@4 score increases from 13.6 to 16.0, and the CIDEr score rises from 54.2 to 58.3. On NoCaps, the CIDEr scores across all three domains improve significantly, with increases of over 10 points. Notably, the CIDEr score on the out-domain set increases from 35.6 to 48.3, demonstrating a strong generalization capability.</p>
</div>
</section>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">B â€ƒMore Analysis of the Proposed ICSD</h2>

<div id="Sx8.p1" class="ltx_para ltx_noindent">
<p id="Sx8.p1.1" class="ltx_p"><span id="Sx8.p1.1.1" class="ltx_text ltx_font_bold">Can retrieving images from a corpus be an alternative to generating images with diffusion models?</span> A major concern is whether obtaining synthetic images by diffusion models is necessary. An alternative way is to use condensed captions to retrieve corresponding images from an accessible image corpus, which requires less GPU memory and time. In TableÂ <a href="#Sx9.T6" title="Table 6 â€£ C Prompt for Selection and Summarization â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we implement the retrieval-based method by sampling 1 million images from laion-2bÂ <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann etÂ al. <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite> and using CLIP to retrieve images for condensed captions.
The results of the retrieval-based method are inferior to our ICSD.
The reason is that accurately retrieving images that match detailed descriptions is difficult.
For example, CLIP suffers from aligning images with detailed textÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al. <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="Sx8.p2" class="ltx_para ltx_noindent">
<p id="Sx8.p2.1" class="ltx_p"><span id="Sx8.p2.1.1" class="ltx_text ltx_font_bold">Can generating detailed caption from an input caption replace merging simple captions by LLMs?</span>
Asking LLMs to make up a more detailed scene description that matches an input caption is a simpler method. In TableÂ <a href="#Sx9.T6" title="Table 6 â€£ C Prompt for Selection and Summarization â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we conduct experiments of this generative-based method on MSCOCO, generate a condensed caption for each simple caption, and generate images from condensed captions for training. The generative-based method achieves 28.9 and 94.7 on B@4 and CIDEr, lower than our ICSD. However, these methods are not in conflict, they serve different scenarios. Our ICSD focuses on in-domain settings, aligning images with the target text corpus distribution, while the generative-based method emphasizes generalization without considering specific text distribution.</p>
</div>
</section>
<section id="Sx9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">C â€ƒPrompt for Selection and Summarization</h2>

<div id="Sx9.p1" class="ltx_para">
<p id="Sx9.p1.1" class="ltx_p">In this section, we provide the prompt template used for selection and summarization:</p>
</div>
<div id="Sx9.p2" class="ltx_para ltx_noindent">
<p id="Sx9.p2.1" class="ltx_p"><span id="Sx9.p2.1.1" class="ltx_text ltx_font_italic">Select and summary sentences in the given sentences set. You should find 3 to 8 sentences that form a description from the same or different views of the same image. The meanings of the selected sentences in a group should not conflict with each other. Summarize the sentences as one not exceed 50 words to describe the scene in objective style. The summary sentence must be objective and concise, without ambiguity and uncertainty. Return the selected index and the summarized sentence in json format like â€˜indexâ€™: list,â€˜summaryâ€™: str. Return directly the json format results without explanation. The given sentences set are: 1. â€¦ 2. â€¦</span></p>
</div>
<div id="Sx9.p3" class="ltx_para">
<p id="Sx9.p3.1" class="ltx_p">ndidate set and instruct LLMs to perform selection and summarization. To avoid the hallucination problem, we guide LLMs to first select sentences and then summarize them into a single sentence. Furthermore, we ask LLMs to output the index of the selected sentences, rather than generating these sentences anew.</p>
</div>
<figure id="Sx9.T6" class="ltx_table">
<div id="Sx9.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:232.7pt;height:66.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.3pt,2.7pt) scale(0.92565562188789,0.92565562188789) ;">
<table id="Sx9.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx9.T6.1.1.1" class="ltx_tr">
<td id="Sx9.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td id="Sx9.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">B@4</td>
<td id="Sx9.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">M</td>
<td id="Sx9.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">R</td>
<td id="Sx9.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">C</td>
</tr>
<tr id="Sx9.T6.1.1.2" class="ltx_tr">
<td id="Sx9.T6.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Retrieval-based method</td>
<td id="Sx9.T6.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">27.3</td>
<td id="Sx9.T6.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">23.4</td>
<td id="Sx9.T6.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">50.4</td>
<td id="Sx9.T6.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">85.7</td>
</tr>
<tr id="Sx9.T6.1.1.3" class="ltx_tr">
<td id="Sx9.T6.1.1.3.1" class="ltx_td ltx_align_left">Generative-based method</td>
<td id="Sx9.T6.1.1.3.2" class="ltx_td ltx_align_center">28.9</td>
<td id="Sx9.T6.1.1.3.3" class="ltx_td ltx_align_center">25.1</td>
<td id="Sx9.T6.1.1.3.4" class="ltx_td ltx_align_center">52.2</td>
<td id="Sx9.T6.1.1.3.5" class="ltx_td ltx_align_center">94.7</td>
</tr>
<tr id="Sx9.T6.1.1.4" class="ltx_tr">
<td id="Sx9.T6.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">ICSD(Ours)</td>
<td id="Sx9.T6.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx9.T6.1.1.4.2.1" class="ltx_text ltx_font_bold">29.9</span></td>
<td id="Sx9.T6.1.1.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx9.T6.1.1.4.3.1" class="ltx_text ltx_font_bold">25.4</span></td>
<td id="Sx9.T6.1.1.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx9.T6.1.1.4.4.1" class="ltx_text ltx_font_bold">52.7</span></td>
<td id="Sx9.T6.1.1.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx9.T6.1.1.4.5.1" class="ltx_text ltx_font_bold">96.6</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison with retrieval-based and generative-based methods.
</figcaption>
</figure>
</section>
<section id="Sx10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">D â€ƒVisualization</h2>

<div id="Sx10.p1" class="ltx_para">
<p id="Sx10.p1.1" class="ltx_p">We present some cases of selection and summarization in FigureÂ <a href="#Sx10.F4" title="Figure 4 â€£ D Visualization â€£ Image Captioning with Multi-Context Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, with each row representing a specific instance.
The columns in the figure are: (1) Selected Captions: captions selected from the initial grouping by LLMs. (2) Natural Images: The real images corresponding to a specific selected caption. (3) Summarized Captions: The summarized captions of selected captions by LLMs. (4) Synthetic Images: The synthetic images generated with summarized captions through Stable Diffusion.
In selected captions, multiple captions correspond to the same natural image, and we choose this natural image to represent the ground truth scene.
Overall, we observe that the synthetic images are very close to the corresponding natural images in terms of scene and can be described by multiple selected captions.
This observation verifies our hypothesis that by combining LLMs and Stable Diffusion, we are able to obtain synthetic images that can be described from multiple perspectives, and such synthetic images are closer to natural images.</p>
</div>
<figure id="Sx10.F4" class="ltx_figure"><img src="/html/2305.18072/assets/x4.png" id="Sx10.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="244" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualization of selection and summarization with the corresponding natural images and synthetic images.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.18071" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.18072" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.18072">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.18072" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.18073" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 04:36:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
