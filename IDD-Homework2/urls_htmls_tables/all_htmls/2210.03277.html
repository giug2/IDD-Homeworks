<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2210.03277] Rethinking Normalization Methods in Federated Learning</title><meta property="og:description" content="Federated learning (FL) is a popular distributed learning framework that can reduce privacy risks by not explicitly sharing private data. In this work, we explicitly uncover external covariate shift problem in FL, whicâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Rethinking Normalization Methods in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Rethinking Normalization Methods in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2210.03277">

<!--Generated on Thu Mar 14 00:44:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Rethinking Normalization Methods in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zhixu Du
<br class="ltx_break">Duke University 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">zhixu.du@duke.edu</span> 
<br class="ltx_break">&amp;Jingwei Sun<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break">Duke University 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">jingwei.sun@duke.edu</span> 
<br class="ltx_break">&amp;Ang Li 
<br class="ltx_break">Duke University 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">ang.li630@duke.edu</span> 
<br class="ltx_break">&amp;Pin-Yu Chen 
<br class="ltx_break">IBM Research AI 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">pin-yu.chen@ibm.com</span> 
<br class="ltx_break">&amp;Jianyi Zhang 
<br class="ltx_break">Duke University 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">jianyi.zhang@duke.edu</span> 
<br class="ltx_break">&amp;Hai "Helen" Li 
<br class="ltx_break">Duke University 
<br class="ltx_break"><span id="id6.6.id6" class="ltx_text ltx_font_typewriter">hai.li@duke.edu</span> 
<br class="ltx_break">&amp;Yiran Chen 
<br class="ltx_break">Duke University 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">yiran.chen@duke.edu</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Equal contribution</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Federated learning (FL) is a popular distributed learning framework that can reduce privacy risks by not explicitly sharing private data. In this work, we explicitly uncover <span id="id8.id1.1" class="ltx_text ltx_font_bold">external covariate shift</span> problem in FL, which is caused by the independent local training processes on different devices. We demonstrate that external covariate shifts will lead to the obliteration of some devicesâ€™ contributions to the global model. Further, we show that normalization layers are indispensable in FL since their inherited properties can alleviate the problem of obliterating some devicesâ€™ contributions. However, recent works have shown that batch normalization, which is one of the standard components in many deep neural networks, will incur accuracy drop of the global model in FL. The essential reason for the failure of batch normalization in FL is poorly studied. We unveil that external covariate shift is the key reason why batch normalization is ineffective in FL. We also show that layer normalization is a better choice in FL which can mitigate the external covariate shift and improve the performance of the global model. We conduct experiments on CIFAR10 under non-IID settings. The results demonstrate that models with layer normalization converge fastest and achieve the best or comparable accuracy for three different model architectures.

<br class="ltx_break">
<br class="ltx_break"><span id="id8.id1.2" class="ltx_text ltx_font_bold">Keywords:</span> Federated Learning, Batch normalization, Layer normalization</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL)Â <cite class="ltx_cite ltx_citemacro_cite">McMahan etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>); Tang etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> is a popular distributed learning approach that enables a large number of devices to train a shared model in a federated fashion without explicitly sharing their local data.
In order to reduce communication cost, most FL methods enable participating devices to conduct multiple steps of training before uploading their local models to the central server for aggregation. However, multiple steps of local training on edge devices would cause <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">internal covariate shift</span>Â <cite class="ltx_cite ltx_citemacro_cite">Ioffe and Szegedy (<a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite> on local models, which is a known problem in the centralized (non-FL) setting.
Internal covariate shift describes the phenomenon that during the training of deep neural networks (DNN), each layerâ€™s input distribution varies due to the parameter changes of preceding layers.
Such an issue requires the internal neurons in a given layer to adapt to varying input distributions, and hence slows down the convergence of model training.
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2210.03277/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Internal and external covariate shift.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Internal covariate shift has been well studied in the centralized learning scenarios and an effective approach to mitigate this issue is batch normalization. Further, batch normalization has many good properties which will stable the training process exploited by previous work. In FL systems, participating devices perform several batches of local training in each communication round, thus, internal covariate shift raises a concern for the local training. In FL, the updates of model parameters vary across devices during local training. Without any constrains, the internal covariate shift across devices will be varied, leading to gaps of statistics information given the same channel among different devices. We name this unique phenomenon in FL as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">external covariate shift</span>. Due to external covariate shift, the model neurons of a given channel on one device need to adapt to the feature distribution of the same channel on other devices, which slows down the convergence of global model training. Further, external covariate shift may also lead to large discrepancy in the norm of weights and may obliterate contribution from devices with weights of small norm.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">We show in this paper that inherited good properties of normalization will shed light on solving external covariate shift. However, existing worksÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Hsieh etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> show that batch normalization will incur the accuracy drop of global model in FL. These works simply attribute the failure of batch normalization in FL to the discrepancies of local data distributions across devices. In this work, we show our key observation that the ineffectiveness of batch normalization in FL is not only caused by the data distribution discrepancies, but also resulted from the diverged internal covariate shift among different devices due to the stochastic training process. Batch normalization drops the accuracy of global model when applied to solve external covariate shift because the feature distribution of the global model after aggregation is not predictable. Further, we also show that layer normalization does not suffer from the problem and can server as the placement of batch normalization in FL.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The experiment results demonstrate that layer normalization can effectively mitigate the external covariate shift and speedup the convergence of the global model training. In particular, layer normalization achieves the fastest convergence and best or comparable accuracy upon convergence on three different model architectures.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Our key contributions are summarized as follows:</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To the best of our knowledge, this is the first work to explicitly reveal <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">external covariate shift</span> in FL, which is an important issue that affects the convergence of FL training.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a simple yet effective placement of batch normalization in Federated Learning, i.e., layer normalization, which can effectively mitigate the external covariate shift and speedup the convergence of FL training.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Internal covariate shift and Activation normalization</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">In the training of deep neural networks, each layerâ€™s input distribution keeps changing due to updates of parameters in the preceding layer. Consequently, layers are forced to keep adapting to the varying input distributions, leading to slow convergence. The issue is more severe when networks get deeper, because the covariate shift will be amplified layer by layer. This phenomenon is called <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">internal covariate shift</span>Â <cite class="ltx_cite ltx_citemacro_cite">Ioffe and Szegedy (<a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>. Activation normalization is proposed to alleviate the internal covirate shift.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.4" class="ltx_p">Activation normalization methods have become one of the most important components in Deep Neural Networks (DNNs) aiming at alleviating internal covariate shift. The normalization layer is usually inserted between neural networksâ€™ output layer and activation functions. Among activation normalization methods, batch normalizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ioffe and Szegedy, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>, group normalizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Wu and He, <a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> and layer normalizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ba etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> are the most commonly used. Activation normalization methods are of great help in stabilizing the training of DNNs and producing well-conditioned training landscapesÂ <cite class="ltx_cite ltx_citemacro_citep">(Martens and Grosse, <a href="#bib.bib11" title="" class="ltx_ref">2015</a>)</cite>. Specifically, activation normalization methods usually have two running statistics <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="{\bm{\mu}}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">{\bm{\mu}}</annotation></semantics></math> and <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\bm{\sigma}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\bm{\sigma}</annotation></semantics></math> and two trainable parameters <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\gamma</annotation></semantics></math> and <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">\beta</annotation></semantics></math> for scaling and shifting, following the same formula,</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\textnormal{AN}({\mathbf{x}})=\frac{{\mathbf{x}}-{\bm{\mu}}}{\sqrt{\bm{\sigma}^{2}+\epsilon}}\times\gamma+\beta," display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.2" xref="S2.E1.m1.2.2.1.1.2.cmml"><mtext id="S2.E1.m1.2.2.1.1.2.2" xref="S2.E1.m1.2.2.1.1.2.2a.cmml">AN</mtext><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.2.1" xref="S2.E1.m1.2.2.1.1.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.1.1.2.3.2" xref="S2.E1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.2.3.2.1" xref="S2.E1.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">ğ±</mi><mo stretchy="false" id="S2.E1.m1.2.2.1.1.2.3.2.2" xref="S2.E1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.2.2.1.1.3" xref="S2.E1.m1.2.2.1.1.3.cmml"><mrow id="S2.E1.m1.2.2.1.1.3.2" xref="S2.E1.m1.2.2.1.1.3.2.cmml"><mfrac id="S2.E1.m1.2.2.1.1.3.2.2" xref="S2.E1.m1.2.2.1.1.3.2.2.cmml"><mrow id="S2.E1.m1.2.2.1.1.3.2.2.2" xref="S2.E1.m1.2.2.1.1.3.2.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.3.2.2.2.2" xref="S2.E1.m1.2.2.1.1.3.2.2.2.2.cmml">ğ±</mi><mo id="S2.E1.m1.2.2.1.1.3.2.2.2.1" xref="S2.E1.m1.2.2.1.1.3.2.2.2.1.cmml">âˆ’</mo><mi id="S2.E1.m1.2.2.1.1.3.2.2.2.3" xref="S2.E1.m1.2.2.1.1.3.2.2.2.3.cmml">ğ</mi></mrow><msqrt id="S2.E1.m1.2.2.1.1.3.2.2.3" xref="S2.E1.m1.2.2.1.1.3.2.2.3.cmml"><mrow id="S2.E1.m1.2.2.1.1.3.2.2.3.2" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.cmml"><msup id="S2.E1.m1.2.2.1.1.3.2.2.3.2.2" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.2" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.2.cmml">ğˆ</mi><mn id="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.3" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.3.cmml">2</mn></msup><mo id="S2.E1.m1.2.2.1.1.3.2.2.3.2.1" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.1.cmml">+</mo><mi id="S2.E1.m1.2.2.1.1.3.2.2.3.2.3" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.3.cmml">Ïµ</mi></mrow></msqrt></mfrac><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.2.2.1.1.3.2.1" xref="S2.E1.m1.2.2.1.1.3.2.1.cmml">Ã—</mo><mi id="S2.E1.m1.2.2.1.1.3.2.3" xref="S2.E1.m1.2.2.1.1.3.2.3.cmml">Î³</mi></mrow><mo id="S2.E1.m1.2.2.1.1.3.1" xref="S2.E1.m1.2.2.1.1.3.1.cmml">+</mo><mi id="S2.E1.m1.2.2.1.1.3.3" xref="S2.E1.m1.2.2.1.1.3.3.cmml">Î²</mi></mrow></mrow><mo id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1"><eq id="S2.E1.m1.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1"></eq><apply id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.2"><times id="S2.E1.m1.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.2.1"></times><ci id="S2.E1.m1.2.2.1.1.2.2a.cmml" xref="S2.E1.m1.2.2.1.1.2.2"><mtext id="S2.E1.m1.2.2.1.1.2.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2">AN</mtext></ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ±</ci></apply><apply id="S2.E1.m1.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.3"><plus id="S2.E1.m1.2.2.1.1.3.1.cmml" xref="S2.E1.m1.2.2.1.1.3.1"></plus><apply id="S2.E1.m1.2.2.1.1.3.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2"><times id="S2.E1.m1.2.2.1.1.3.2.1.cmml" xref="S2.E1.m1.2.2.1.1.3.2.1"></times><apply id="S2.E1.m1.2.2.1.1.3.2.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2"><divide id="S2.E1.m1.2.2.1.1.3.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2"></divide><apply id="S2.E1.m1.2.2.1.1.3.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.2"><minus id="S2.E1.m1.2.2.1.1.3.2.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.2.1"></minus><ci id="S2.E1.m1.2.2.1.1.3.2.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.2.2">ğ±</ci><ci id="S2.E1.m1.2.2.1.1.3.2.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.2.3">ğ</ci></apply><apply id="S2.E1.m1.2.2.1.1.3.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3"><root id="S2.E1.m1.2.2.1.1.3.2.2.3a.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3"></root><apply id="S2.E1.m1.2.2.1.1.3.2.2.3.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2"><plus id="S2.E1.m1.2.2.1.1.3.2.2.3.2.1.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.1"></plus><apply id="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.2">superscript</csymbol><ci id="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.2">ğˆ</ci><cn type="integer" id="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.2.3">2</cn></apply><ci id="S2.E1.m1.2.2.1.1.3.2.2.3.2.3.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2.3.2.3">italic-Ïµ</ci></apply></apply></apply><ci id="S2.E1.m1.2.2.1.1.3.2.3.cmml" xref="S2.E1.m1.2.2.1.1.3.2.3">ğ›¾</ci></apply><ci id="S2.E1.m1.2.2.1.1.3.3.cmml" xref="S2.E1.m1.2.2.1.1.3.3">ğ›½</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\textnormal{AN}({\mathbf{x}})=\frac{{\mathbf{x}}-{\bm{\mu}}}{\sqrt{\bm{\sigma}^{2}+\epsilon}}\times\gamma+\beta,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.9" class="ltx_p">where AN stands for Activation Normalization and <math id="S2.SS1.p2.5.m1.1" class="ltx_Math" alttext="{\mathbf{x}}" display="inline"><semantics id="S2.SS1.p2.5.m1.1a"><mi id="S2.SS1.p2.5.m1.1.1" xref="S2.SS1.p2.5.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m1.1b"><ci id="S2.SS1.p2.5.m1.1.1.cmml" xref="S2.SS1.p2.5.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m1.1c">{\mathbf{x}}</annotation></semantics></math> is the input of the normalization. Different normalization techniques computes <math id="S2.SS1.p2.6.m2.1" class="ltx_Math" alttext="{\bm{\mu}}" display="inline"><semantics id="S2.SS1.p2.6.m2.1a"><mi id="S2.SS1.p2.6.m2.1.1" xref="S2.SS1.p2.6.m2.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m2.1b"><ci id="S2.SS1.p2.6.m2.1.1.cmml" xref="S2.SS1.p2.6.m2.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m2.1c">{\bm{\mu}}</annotation></semantics></math> and <math id="S2.SS1.p2.7.m3.1" class="ltx_Math" alttext="\bm{\sigma}" display="inline"><semantics id="S2.SS1.p2.7.m3.1a"><mi id="S2.SS1.p2.7.m3.1.1" xref="S2.SS1.p2.7.m3.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m3.1b"><ci id="S2.SS1.p2.7.m3.1.1.cmml" xref="S2.SS1.p2.7.m3.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m3.1c">\bm{\sigma}</annotation></semantics></math> differently. Batch normalization uses mini-batch mean and mini-batch variance as <math id="S2.SS1.p2.8.m4.1" class="ltx_Math" alttext="{\bm{\mu}}" display="inline"><semantics id="S2.SS1.p2.8.m4.1a"><mi id="S2.SS1.p2.8.m4.1.1" xref="S2.SS1.p2.8.m4.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m4.1b"><ci id="S2.SS1.p2.8.m4.1.1.cmml" xref="S2.SS1.p2.8.m4.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m4.1c">{\bm{\mu}}</annotation></semantics></math> and <math id="S2.SS1.p2.9.m5.1" class="ltx_Math" alttext="\bm{\sigma}" display="inline"><semantics id="S2.SS1.p2.9.m5.1a"><mi id="S2.SS1.p2.9.m5.1.1" xref="S2.SS1.p2.9.m5.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.9.m5.1b"><ci id="S2.SS1.p2.9.m5.1.1.cmml" xref="S2.SS1.p2.9.m5.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.9.m5.1c">\bm{\sigma}</annotation></semantics></math>, while layer normalization and group normalization use mean and variance across channels or features of a data sample.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">Previous worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Ioffe and Szegedy, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>; Ba etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> have exploited many well-inherited properties of batch normalization and layer normalization, and some of them are important in Federated Learning, which will be detailed in the following sections.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Scale invariant property of normalization</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.7" class="ltx_p">The scale invariant property of batch normalization (BN) and layer normalization (LN) has been widely studiedÂ <cite class="ltx_cite ltx_citemacro_citep">(Neyshabur etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2016</a>; Sun etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; VanÂ Laarhoven, <a href="#bib.bib18" title="" class="ltx_ref">2017</a>; Arora etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>. The property equips BN and LN layers with ability to automatically tune the learning rate for the layer that are antecedent to the normalization layer. Formally,</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.4" class="ltx_Math" alttext="\textnormal{BN}({\mathbf{h}};{\bm{W}})=\textnormal{BN}({\mathbf{h}};a{\bm{W}})," display="block"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.3" xref="S2.E2.m1.4.4.1.1.3.cmml"><mtext id="S2.E2.m1.4.4.1.1.3.2" xref="S2.E2.m1.4.4.1.1.3.2a.cmml">BN</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.3.1" xref="S2.E2.m1.4.4.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E2.m1.4.4.1.1.3.3.2" xref="S2.E2.m1.4.4.1.1.3.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.3.2.1" xref="S2.E2.m1.4.4.1.1.3.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">ğ¡</mi><mo id="S2.E2.m1.4.4.1.1.3.3.2.2" xref="S2.E2.m1.4.4.1.1.3.3.1.cmml">;</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">ğ‘¾</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.3.2.3" xref="S2.E2.m1.4.4.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.2" xref="S2.E2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.cmml"><mtext id="S2.E2.m1.4.4.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.3a.cmml">BN</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">ğ¡</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml">;</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml">ğ‘¾</mi></mrow><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.4" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1"><eq id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.2"></eq><apply id="S2.E2.m1.4.4.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.3"><times id="S2.E2.m1.4.4.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.1"></times><ci id="S2.E2.m1.4.4.1.1.3.2a.cmml" xref="S2.E2.m1.4.4.1.1.3.2"><mtext id="S2.E2.m1.4.4.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2">BN</mtext></ci><list id="S2.E2.m1.4.4.1.1.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğ¡</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ğ‘¾</ci></list></apply><apply id="S2.E2.m1.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2"></times><ci id="S2.E2.m1.4.4.1.1.1.3a.cmml" xref="S2.E2.m1.4.4.1.1.1.3"><mtext id="S2.E2.m1.4.4.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.3">BN</mtext></ci><list id="S2.E2.m1.4.4.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1"><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">ğ¡</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1"></times><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3">ğ‘¾</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\textnormal{BN}({\mathbf{h}};{\bm{W}})=\textnormal{BN}({\mathbf{h}};a{\bm{W}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.3" class="ltx_p">where <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="{\bm{W}}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">ğ‘¾</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğ‘¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">{\bm{W}}</annotation></semantics></math> is the weight parameters of the preceding layer, <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="{\mathbf{h}}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">{\mathbf{h}}</annotation></semantics></math> is the input to the layer and <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">a</annotation></semantics></math> is a non-zero scalar. The same also applies for LN. In back-propagation,</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.5" class="ltx_Math" alttext="\frac{\partial\textnormal{BN}({\mathbf{h}};a{\bm{W}})}{\partial a{\bm{W}}}=\frac{1}{a}\times\frac{\partial\textnormal{BN}({\mathbf{h}};{\bm{W}})}{\partial{\bm{W}}}." display="block"><semantics id="S2.E3.m1.5a"><mrow id="S2.E3.m1.5.5.1" xref="S2.E3.m1.5.5.1.1.cmml"><mrow id="S2.E3.m1.5.5.1.1" xref="S2.E3.m1.5.5.1.1.cmml"><mfrac id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><mrow id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml"><mo rspace="0em" id="S2.E3.m1.2.2.2.3" xref="S2.E3.m1.2.2.2.3.cmml">âˆ‚</mo><mrow id="S2.E3.m1.2.2.2.2" xref="S2.E3.m1.2.2.2.2.cmml"><mtext id="S2.E3.m1.2.2.2.2.3" xref="S2.E3.m1.2.2.2.2.3a.cmml">BN</mtext><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.cmml">â€‹</mo><mrow id="S2.E3.m1.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.1.2.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.2.2.1.1.2" xref="S2.E3.m1.2.2.2.2.1.2.cmml">(</mo><mi id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml">ğ¡</mi><mo id="S2.E3.m1.2.2.2.2.1.1.3" xref="S2.E3.m1.2.2.2.2.1.2.cmml">;</mo><mrow id="S2.E3.m1.2.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.2.1.1.1.cmml"><mi id="S2.E3.m1.2.2.2.2.1.1.1.2" xref="S2.E3.m1.2.2.2.2.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.2.1.1.1.1" xref="S2.E3.m1.2.2.2.2.1.1.1.1.cmml">â€‹</mo><mi id="S2.E3.m1.2.2.2.2.1.1.1.3" xref="S2.E3.m1.2.2.2.2.1.1.1.3.cmml">ğ‘¾</mi></mrow><mo stretchy="false" id="S2.E3.m1.2.2.2.2.1.1.4" xref="S2.E3.m1.2.2.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><mrow id="S2.E3.m1.2.2.4" xref="S2.E3.m1.2.2.4.cmml"><mo rspace="0em" id="S2.E3.m1.2.2.4.1" xref="S2.E3.m1.2.2.4.1.cmml">âˆ‚</mo><mrow id="S2.E3.m1.2.2.4.2" xref="S2.E3.m1.2.2.4.2.cmml"><mi id="S2.E3.m1.2.2.4.2.2" xref="S2.E3.m1.2.2.4.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.4.2.1" xref="S2.E3.m1.2.2.4.2.1.cmml">â€‹</mo><mi id="S2.E3.m1.2.2.4.2.3" xref="S2.E3.m1.2.2.4.2.3.cmml">ğ‘¾</mi></mrow></mrow></mfrac><mo id="S2.E3.m1.5.5.1.1.1" xref="S2.E3.m1.5.5.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.5.5.1.1.2" xref="S2.E3.m1.5.5.1.1.2.cmml"><mfrac id="S2.E3.m1.5.5.1.1.2.2" xref="S2.E3.m1.5.5.1.1.2.2.cmml"><mn id="S2.E3.m1.5.5.1.1.2.2.2" xref="S2.E3.m1.5.5.1.1.2.2.2.cmml">1</mn><mi id="S2.E3.m1.5.5.1.1.2.2.3" xref="S2.E3.m1.5.5.1.1.2.2.3.cmml">a</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S2.E3.m1.5.5.1.1.2.1" xref="S2.E3.m1.5.5.1.1.2.1.cmml">Ã—</mo><mfrac id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml"><mrow id="S2.E3.m1.4.4.2" xref="S2.E3.m1.4.4.2.cmml"><mo rspace="0em" id="S2.E3.m1.4.4.2.3" xref="S2.E3.m1.4.4.2.3.cmml">âˆ‚</mo><mrow id="S2.E3.m1.4.4.2.4" xref="S2.E3.m1.4.4.2.4.cmml"><mtext id="S2.E3.m1.4.4.2.4.2" xref="S2.E3.m1.4.4.2.4.2a.cmml">BN</mtext><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.2.4.1" xref="S2.E3.m1.4.4.2.4.1.cmml">â€‹</mo><mrow id="S2.E3.m1.4.4.2.4.3.2" xref="S2.E3.m1.4.4.2.4.3.1.cmml"><mo stretchy="false" id="S2.E3.m1.4.4.2.4.3.2.1" xref="S2.E3.m1.4.4.2.4.3.1.cmml">(</mo><mi id="S2.E3.m1.3.3.1.1" xref="S2.E3.m1.3.3.1.1.cmml">ğ¡</mi><mo id="S2.E3.m1.4.4.2.4.3.2.2" xref="S2.E3.m1.4.4.2.4.3.1.cmml">;</mo><mi id="S2.E3.m1.4.4.2.2" xref="S2.E3.m1.4.4.2.2.cmml">ğ‘¾</mi><mo stretchy="false" id="S2.E3.m1.4.4.2.4.3.2.3" xref="S2.E3.m1.4.4.2.4.3.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S2.E3.m1.4.4.4" xref="S2.E3.m1.4.4.4.cmml"><mo rspace="0em" id="S2.E3.m1.4.4.4.1" xref="S2.E3.m1.4.4.4.1.cmml">âˆ‚</mo><mi id="S2.E3.m1.4.4.4.2" xref="S2.E3.m1.4.4.4.2.cmml">ğ‘¾</mi></mrow></mfrac></mrow></mrow><mo lspace="0em" id="S2.E3.m1.5.5.1.2" xref="S2.E3.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.5b"><apply id="S2.E3.m1.5.5.1.1.cmml" xref="S2.E3.m1.5.5.1"><eq id="S2.E3.m1.5.5.1.1.1.cmml" xref="S2.E3.m1.5.5.1.1.1"></eq><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2"><divide id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2"></divide><apply id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2"><partialdiff id="S2.E3.m1.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.3"></partialdiff><apply id="S2.E3.m1.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2"><times id="S2.E3.m1.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2"></times><ci id="S2.E3.m1.2.2.2.2.3a.cmml" xref="S2.E3.m1.2.2.2.2.3"><mtext id="S2.E3.m1.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.2.3">BN</mtext></ci><list id="S2.E3.m1.2.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1"><ci id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1">ğ¡</ci><apply id="S2.E3.m1.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.1"><times id="S2.E3.m1.2.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.1.1"></times><ci id="S2.E3.m1.2.2.2.2.1.1.1.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.1.2">ğ‘</ci><ci id="S2.E3.m1.2.2.2.2.1.1.1.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.1.3">ğ‘¾</ci></apply></list></apply></apply><apply id="S2.E3.m1.2.2.4.cmml" xref="S2.E3.m1.2.2.4"><partialdiff id="S2.E3.m1.2.2.4.1.cmml" xref="S2.E3.m1.2.2.4.1"></partialdiff><apply id="S2.E3.m1.2.2.4.2.cmml" xref="S2.E3.m1.2.2.4.2"><times id="S2.E3.m1.2.2.4.2.1.cmml" xref="S2.E3.m1.2.2.4.2.1"></times><ci id="S2.E3.m1.2.2.4.2.2.cmml" xref="S2.E3.m1.2.2.4.2.2">ğ‘</ci><ci id="S2.E3.m1.2.2.4.2.3.cmml" xref="S2.E3.m1.2.2.4.2.3">ğ‘¾</ci></apply></apply></apply><apply id="S2.E3.m1.5.5.1.1.2.cmml" xref="S2.E3.m1.5.5.1.1.2"><times id="S2.E3.m1.5.5.1.1.2.1.cmml" xref="S2.E3.m1.5.5.1.1.2.1"></times><apply id="S2.E3.m1.5.5.1.1.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2"><divide id="S2.E3.m1.5.5.1.1.2.2.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2"></divide><cn type="integer" id="S2.E3.m1.5.5.1.1.2.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2">1</cn><ci id="S2.E3.m1.5.5.1.1.2.2.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3">ğ‘</ci></apply><apply id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.4.4"><divide id="S2.E3.m1.4.4.3.cmml" xref="S2.E3.m1.4.4"></divide><apply id="S2.E3.m1.4.4.2.cmml" xref="S2.E3.m1.4.4.2"><partialdiff id="S2.E3.m1.4.4.2.3.cmml" xref="S2.E3.m1.4.4.2.3"></partialdiff><apply id="S2.E3.m1.4.4.2.4.cmml" xref="S2.E3.m1.4.4.2.4"><times id="S2.E3.m1.4.4.2.4.1.cmml" xref="S2.E3.m1.4.4.2.4.1"></times><ci id="S2.E3.m1.4.4.2.4.2a.cmml" xref="S2.E3.m1.4.4.2.4.2"><mtext id="S2.E3.m1.4.4.2.4.2.cmml" xref="S2.E3.m1.4.4.2.4.2">BN</mtext></ci><list id="S2.E3.m1.4.4.2.4.3.1.cmml" xref="S2.E3.m1.4.4.2.4.3.2"><ci id="S2.E3.m1.3.3.1.1.cmml" xref="S2.E3.m1.3.3.1.1">ğ¡</ci><ci id="S2.E3.m1.4.4.2.2.cmml" xref="S2.E3.m1.4.4.2.2">ğ‘¾</ci></list></apply></apply><apply id="S2.E3.m1.4.4.4.cmml" xref="S2.E3.m1.4.4.4"><partialdiff id="S2.E3.m1.4.4.4.1.cmml" xref="S2.E3.m1.4.4.4.1"></partialdiff><ci id="S2.E3.m1.4.4.4.2.cmml" xref="S2.E3.m1.4.4.4.2">ğ‘¾</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.5c">\frac{\partial\textnormal{BN}({\mathbf{h}};a{\bm{W}})}{\partial a{\bm{W}}}=\frac{1}{a}\times\frac{\partial\textnormal{BN}({\mathbf{h}};{\bm{W}})}{\partial{\bm{W}}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.6" class="ltx_p">If the weight <math id="S2.SS2.p1.4.m1.1" class="ltx_Math" alttext="a{\bm{W}}" display="inline"><semantics id="S2.SS2.p1.4.m1.1a"><mrow id="S2.SS2.p1.4.m1.1.1" xref="S2.SS2.p1.4.m1.1.1.cmml"><mi id="S2.SS2.p1.4.m1.1.1.2" xref="S2.SS2.p1.4.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m1.1.1.1" xref="S2.SS2.p1.4.m1.1.1.1.cmml">â€‹</mo><mi id="S2.SS2.p1.4.m1.1.1.3" xref="S2.SS2.p1.4.m1.1.1.3.cmml">ğ‘¾</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m1.1b"><apply id="S2.SS2.p1.4.m1.1.1.cmml" xref="S2.SS2.p1.4.m1.1.1"><times id="S2.SS2.p1.4.m1.1.1.1.cmml" xref="S2.SS2.p1.4.m1.1.1.1"></times><ci id="S2.SS2.p1.4.m1.1.1.2.cmml" xref="S2.SS2.p1.4.m1.1.1.2">ğ‘</ci><ci id="S2.SS2.p1.4.m1.1.1.3.cmml" xref="S2.SS2.p1.4.m1.1.1.3">ğ‘¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m1.1c">a{\bm{W}}</annotation></semantics></math> is large, then in back-propagation the gradient will be shrink by a factor of <math id="S2.SS2.p1.5.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS2.p1.5.m2.1a"><mi id="S2.SS2.p1.5.m2.1.1" xref="S2.SS2.p1.5.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m2.1b"><ci id="S2.SS2.p1.5.m2.1.1.cmml" xref="S2.SS2.p1.5.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m2.1c">a</annotation></semantics></math>. On the other hand, if <math id="S2.SS2.p1.6.m3.1" class="ltx_Math" alttext="a{\bm{W}}" display="inline"><semantics id="S2.SS2.p1.6.m3.1a"><mrow id="S2.SS2.p1.6.m3.1.1" xref="S2.SS2.p1.6.m3.1.1.cmml"><mi id="S2.SS2.p1.6.m3.1.1.2" xref="S2.SS2.p1.6.m3.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.6.m3.1.1.1" xref="S2.SS2.p1.6.m3.1.1.1.cmml">â€‹</mo><mi id="S2.SS2.p1.6.m3.1.1.3" xref="S2.SS2.p1.6.m3.1.1.3.cmml">ğ‘¾</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m3.1b"><apply id="S2.SS2.p1.6.m3.1.1.cmml" xref="S2.SS2.p1.6.m3.1.1"><times id="S2.SS2.p1.6.m3.1.1.1.cmml" xref="S2.SS2.p1.6.m3.1.1.1"></times><ci id="S2.SS2.p1.6.m3.1.1.2.cmml" xref="S2.SS2.p1.6.m3.1.1.2">ğ‘</ci><ci id="S2.SS2.p1.6.m3.1.1.3.cmml" xref="S2.SS2.p1.6.m3.1.1.3">ğ‘¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m3.1c">a{\bm{W}}</annotation></semantics></math> is small, the gradient will be enlarged.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Further, the scale invariant property leads to an equilibrium on the norm of weights. The following lemma shows that the gradient on batch normalization and layer normalization layer is always orthogonal to the weight parameters.</p>
</div>
<div id="Thmlemma1" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="Thmlemma1.1.1.1" class="ltx_text ltx_font_bold">Lemma 1</span></span><span id="Thmlemma1.2.2" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma1.p1" class="ltx_para">
<p id="Thmlemma1.p1.4" class="ltx_p"><span id="Thmlemma1.p1.4.4" class="ltx_text ltx_font_italic">If function <math id="Thmlemma1.p1.1.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="Thmlemma1.p1.1.1.m1.1a"><mi id="Thmlemma1.p1.1.1.m1.1.1" xref="Thmlemma1.p1.1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="Thmlemma1.p1.1.1.m1.1b"><ci id="Thmlemma1.p1.1.1.m1.1.1.cmml" xref="Thmlemma1.p1.1.1.m1.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmlemma1.p1.1.1.m1.1c">f</annotation></semantics></math> satisfies that <math id="Thmlemma1.p1.2.2.m2.2" class="ltx_Math" alttext="f(\lambda{\bm{W}})=f({\bm{W}})" display="inline"><semantics id="Thmlemma1.p1.2.2.m2.2a"><mrow id="Thmlemma1.p1.2.2.m2.2.2" xref="Thmlemma1.p1.2.2.m2.2.2.cmml"><mrow id="Thmlemma1.p1.2.2.m2.2.2.1" xref="Thmlemma1.p1.2.2.m2.2.2.1.cmml"><mi id="Thmlemma1.p1.2.2.m2.2.2.1.3" xref="Thmlemma1.p1.2.2.m2.2.2.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="Thmlemma1.p1.2.2.m2.2.2.1.2" xref="Thmlemma1.p1.2.2.m2.2.2.1.2.cmml">â€‹</mo><mrow id="Thmlemma1.p1.2.2.m2.2.2.1.1.1" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.2" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.cmml">(</mo><mrow id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.cmml"><mi id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.2" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.1" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.1.cmml">â€‹</mo><mi id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.3" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.3.cmml">ğ–</mi></mrow><mo stretchy="false" id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.3" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Thmlemma1.p1.2.2.m2.2.2.2" xref="Thmlemma1.p1.2.2.m2.2.2.2.cmml">=</mo><mrow id="Thmlemma1.p1.2.2.m2.2.2.3" xref="Thmlemma1.p1.2.2.m2.2.2.3.cmml"><mi id="Thmlemma1.p1.2.2.m2.2.2.3.2" xref="Thmlemma1.p1.2.2.m2.2.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="Thmlemma1.p1.2.2.m2.2.2.3.1" xref="Thmlemma1.p1.2.2.m2.2.2.3.1.cmml">â€‹</mo><mrow id="Thmlemma1.p1.2.2.m2.2.2.3.3.2" xref="Thmlemma1.p1.2.2.m2.2.2.3.cmml"><mo stretchy="false" id="Thmlemma1.p1.2.2.m2.2.2.3.3.2.1" xref="Thmlemma1.p1.2.2.m2.2.2.3.cmml">(</mo><mi id="Thmlemma1.p1.2.2.m2.1.1" xref="Thmlemma1.p1.2.2.m2.1.1.cmml">ğ–</mi><mo stretchy="false" id="Thmlemma1.p1.2.2.m2.2.2.3.3.2.2" xref="Thmlemma1.p1.2.2.m2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmlemma1.p1.2.2.m2.2b"><apply id="Thmlemma1.p1.2.2.m2.2.2.cmml" xref="Thmlemma1.p1.2.2.m2.2.2"><eq id="Thmlemma1.p1.2.2.m2.2.2.2.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.2"></eq><apply id="Thmlemma1.p1.2.2.m2.2.2.1.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.1"><times id="Thmlemma1.p1.2.2.m2.2.2.1.2.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.1.2"></times><ci id="Thmlemma1.p1.2.2.m2.2.2.1.3.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.1.3">ğ‘“</ci><apply id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1"><times id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.1.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.1"></times><ci id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.2.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.2">ğœ†</ci><ci id="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.3.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.1.1.1.1.3">ğ–</ci></apply></apply><apply id="Thmlemma1.p1.2.2.m2.2.2.3.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.3"><times id="Thmlemma1.p1.2.2.m2.2.2.3.1.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.3.1"></times><ci id="Thmlemma1.p1.2.2.m2.2.2.3.2.cmml" xref="Thmlemma1.p1.2.2.m2.2.2.3.2">ğ‘“</ci><ci id="Thmlemma1.p1.2.2.m2.1.1.cmml" xref="Thmlemma1.p1.2.2.m2.1.1">ğ–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmlemma1.p1.2.2.m2.2c">f(\lambda{\bm{W}})=f({\bm{W}})</annotation></semantics></math> for all non-zero scalar <math id="Thmlemma1.p1.3.3.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="Thmlemma1.p1.3.3.m3.1a"><mi id="Thmlemma1.p1.3.3.m3.1.1" xref="Thmlemma1.p1.3.3.m3.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="Thmlemma1.p1.3.3.m3.1b"><ci id="Thmlemma1.p1.3.3.m3.1.1.cmml" xref="Thmlemma1.p1.3.3.m3.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmlemma1.p1.3.3.m3.1c">\lambda</annotation></semantics></math> and <math id="Thmlemma1.p1.4.4.m4.1" class="ltx_Math" alttext="\nabla f({\bm{W}})" display="inline"><semantics id="Thmlemma1.p1.4.4.m4.1a"><mrow id="Thmlemma1.p1.4.4.m4.1.2" xref="Thmlemma1.p1.4.4.m4.1.2.cmml"><mrow id="Thmlemma1.p1.4.4.m4.1.2.2" xref="Thmlemma1.p1.4.4.m4.1.2.2.cmml"><mo rspace="0.167em" id="Thmlemma1.p1.4.4.m4.1.2.2.1" xref="Thmlemma1.p1.4.4.m4.1.2.2.1.cmml">âˆ‡</mo><mi id="Thmlemma1.p1.4.4.m4.1.2.2.2" xref="Thmlemma1.p1.4.4.m4.1.2.2.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="Thmlemma1.p1.4.4.m4.1.2.1" xref="Thmlemma1.p1.4.4.m4.1.2.1.cmml">â€‹</mo><mrow id="Thmlemma1.p1.4.4.m4.1.2.3.2" xref="Thmlemma1.p1.4.4.m4.1.2.cmml"><mo stretchy="false" id="Thmlemma1.p1.4.4.m4.1.2.3.2.1" xref="Thmlemma1.p1.4.4.m4.1.2.cmml">(</mo><mi id="Thmlemma1.p1.4.4.m4.1.1" xref="Thmlemma1.p1.4.4.m4.1.1.cmml">ğ–</mi><mo stretchy="false" id="Thmlemma1.p1.4.4.m4.1.2.3.2.2" xref="Thmlemma1.p1.4.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmlemma1.p1.4.4.m4.1b"><apply id="Thmlemma1.p1.4.4.m4.1.2.cmml" xref="Thmlemma1.p1.4.4.m4.1.2"><times id="Thmlemma1.p1.4.4.m4.1.2.1.cmml" xref="Thmlemma1.p1.4.4.m4.1.2.1"></times><apply id="Thmlemma1.p1.4.4.m4.1.2.2.cmml" xref="Thmlemma1.p1.4.4.m4.1.2.2"><ci id="Thmlemma1.p1.4.4.m4.1.2.2.1.cmml" xref="Thmlemma1.p1.4.4.m4.1.2.2.1">âˆ‡</ci><ci id="Thmlemma1.p1.4.4.m4.1.2.2.2.cmml" xref="Thmlemma1.p1.4.4.m4.1.2.2.2">ğ‘“</ci></apply><ci id="Thmlemma1.p1.4.4.m4.1.1.cmml" xref="Thmlemma1.p1.4.4.m4.1.1">ğ–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmlemma1.p1.4.4.m4.1c">\nabla f({\bm{W}})</annotation></semantics></math> exists, then</span></p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.2" class="ltx_Math" alttext="{\bm{W}}^{\top}\nabla f({\bm{W}})=0," display="block"><semantics id="S2.Ex1.m1.2a"><mrow id="S2.Ex1.m1.2.2.1" xref="S2.Ex1.m1.2.2.1.1.cmml"><mrow id="S2.Ex1.m1.2.2.1.1" xref="S2.Ex1.m1.2.2.1.1.cmml"><mrow id="S2.Ex1.m1.2.2.1.1.2" xref="S2.Ex1.m1.2.2.1.1.2.cmml"><msup id="S2.Ex1.m1.2.2.1.1.2.2" xref="S2.Ex1.m1.2.2.1.1.2.2.cmml"><mi id="S2.Ex1.m1.2.2.1.1.2.2.2" xref="S2.Ex1.m1.2.2.1.1.2.2.2.cmml">ğ‘¾</mi><mo id="S2.Ex1.m1.2.2.1.1.2.2.3" xref="S2.Ex1.m1.2.2.1.1.2.2.3.cmml">âŠ¤</mo></msup><mo lspace="0.167em" rspace="0em" id="S2.Ex1.m1.2.2.1.1.2.1" xref="S2.Ex1.m1.2.2.1.1.2.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.2.2.1.1.2.3" xref="S2.Ex1.m1.2.2.1.1.2.3.cmml"><mo rspace="0.167em" id="S2.Ex1.m1.2.2.1.1.2.3.1" xref="S2.Ex1.m1.2.2.1.1.2.3.1.cmml">âˆ‡</mo><mi id="S2.Ex1.m1.2.2.1.1.2.3.2" xref="S2.Ex1.m1.2.2.1.1.2.3.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.2.2.1.1.2.1a" xref="S2.Ex1.m1.2.2.1.1.2.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.2.2.1.1.2.4.2" xref="S2.Ex1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.2.2.1.1.2.4.2.1" xref="S2.Ex1.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">ğ‘¾</mi><mo stretchy="false" id="S2.Ex1.m1.2.2.1.1.2.4.2.2" xref="S2.Ex1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.2.2.1.1.1" xref="S2.Ex1.m1.2.2.1.1.1.cmml">=</mo><mn id="S2.Ex1.m1.2.2.1.1.3" xref="S2.Ex1.m1.2.2.1.1.3.cmml">0</mn></mrow><mo id="S2.Ex1.m1.2.2.1.2" xref="S2.Ex1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.2b"><apply id="S2.Ex1.m1.2.2.1.1.cmml" xref="S2.Ex1.m1.2.2.1"><eq id="S2.Ex1.m1.2.2.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.1"></eq><apply id="S2.Ex1.m1.2.2.1.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2"><times id="S2.Ex1.m1.2.2.1.1.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.2.1"></times><apply id="S2.Ex1.m1.2.2.1.1.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.2.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2">superscript</csymbol><ci id="S2.Ex1.m1.2.2.1.1.2.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2">ğ‘¾</ci><csymbol cd="latexml" id="S2.Ex1.m1.2.2.1.1.2.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.3">top</csymbol></apply><apply id="S2.Ex1.m1.2.2.1.1.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.2.3"><ci id="S2.Ex1.m1.2.2.1.1.2.3.1.cmml" xref="S2.Ex1.m1.2.2.1.1.2.3.1">âˆ‡</ci><ci id="S2.Ex1.m1.2.2.1.1.2.3.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2.3.2">ğ‘“</ci></apply><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">ğ‘¾</ci></apply><cn type="integer" id="S2.Ex1.m1.2.2.1.1.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.2c">{\bm{W}}^{\top}\nabla f({\bm{W}})=0,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="Thmlemma1.p1.7" class="ltx_p"><span id="Thmlemma1.p1.7.3" class="ltx_text ltx_font_italic">where <math id="Thmlemma1.p1.5.1.m1.1" class="ltx_Math" alttext="\nabla f({\bm{W}})" display="inline"><semantics id="Thmlemma1.p1.5.1.m1.1a"><mrow id="Thmlemma1.p1.5.1.m1.1.2" xref="Thmlemma1.p1.5.1.m1.1.2.cmml"><mrow id="Thmlemma1.p1.5.1.m1.1.2.2" xref="Thmlemma1.p1.5.1.m1.1.2.2.cmml"><mo rspace="0.167em" id="Thmlemma1.p1.5.1.m1.1.2.2.1" xref="Thmlemma1.p1.5.1.m1.1.2.2.1.cmml">âˆ‡</mo><mi id="Thmlemma1.p1.5.1.m1.1.2.2.2" xref="Thmlemma1.p1.5.1.m1.1.2.2.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="Thmlemma1.p1.5.1.m1.1.2.1" xref="Thmlemma1.p1.5.1.m1.1.2.1.cmml">â€‹</mo><mrow id="Thmlemma1.p1.5.1.m1.1.2.3.2" xref="Thmlemma1.p1.5.1.m1.1.2.cmml"><mo stretchy="false" id="Thmlemma1.p1.5.1.m1.1.2.3.2.1" xref="Thmlemma1.p1.5.1.m1.1.2.cmml">(</mo><mi id="Thmlemma1.p1.5.1.m1.1.1" xref="Thmlemma1.p1.5.1.m1.1.1.cmml">ğ–</mi><mo stretchy="false" id="Thmlemma1.p1.5.1.m1.1.2.3.2.2" xref="Thmlemma1.p1.5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmlemma1.p1.5.1.m1.1b"><apply id="Thmlemma1.p1.5.1.m1.1.2.cmml" xref="Thmlemma1.p1.5.1.m1.1.2"><times id="Thmlemma1.p1.5.1.m1.1.2.1.cmml" xref="Thmlemma1.p1.5.1.m1.1.2.1"></times><apply id="Thmlemma1.p1.5.1.m1.1.2.2.cmml" xref="Thmlemma1.p1.5.1.m1.1.2.2"><ci id="Thmlemma1.p1.5.1.m1.1.2.2.1.cmml" xref="Thmlemma1.p1.5.1.m1.1.2.2.1">âˆ‡</ci><ci id="Thmlemma1.p1.5.1.m1.1.2.2.2.cmml" xref="Thmlemma1.p1.5.1.m1.1.2.2.2">ğ‘“</ci></apply><ci id="Thmlemma1.p1.5.1.m1.1.1.cmml" xref="Thmlemma1.p1.5.1.m1.1.1">ğ–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmlemma1.p1.5.1.m1.1c">\nabla f({\bm{W}})</annotation></semantics></math> is the gradient of <math id="Thmlemma1.p1.6.2.m2.1" class="ltx_Math" alttext="f" display="inline"><semantics id="Thmlemma1.p1.6.2.m2.1a"><mi id="Thmlemma1.p1.6.2.m2.1.1" xref="Thmlemma1.p1.6.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="Thmlemma1.p1.6.2.m2.1b"><ci id="Thmlemma1.p1.6.2.m2.1.1.cmml" xref="Thmlemma1.p1.6.2.m2.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmlemma1.p1.6.2.m2.1c">f</annotation></semantics></math> with regard to <math id="Thmlemma1.p1.7.3.m3.1" class="ltx_Math" alttext="{\bm{W}}" display="inline"><semantics id="Thmlemma1.p1.7.3.m3.1a"><mi id="Thmlemma1.p1.7.3.m3.1.1" xref="Thmlemma1.p1.7.3.m3.1.1.cmml">ğ–</mi><annotation-xml encoding="MathML-Content" id="Thmlemma1.p1.7.3.m3.1b"><ci id="Thmlemma1.p1.7.3.m3.1.1.cmml" xref="Thmlemma1.p1.7.3.m3.1.1">ğ–</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmlemma1.p1.7.3.m3.1c">{\bm{W}}</annotation></semantics></math>.</span></p>
</div>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.2" class="ltx_p">Lemma 1 can be easily proved by differentiating with regard to <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\lambda</annotation></semantics></math> on both sides of the equation and set <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="\lambda=1" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mi id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">Î»</mi><mo id="S2.SS2.p3.2.m2.1.1.1" xref="S2.SS2.p3.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.p3.2.m2.1.1.3" xref="S2.SS2.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><eq id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1.1"></eq><ci id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">ğœ†</ci><cn type="integer" id="S2.SS2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">\lambda=1</annotation></semantics></math>. By applying Lemma 1 and the rule of weights update</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.2" class="ltx_Math" alttext="{\bm{W}}_{t+1}={\bm{W}}_{t}+\eta\frac{\partial f({\bm{W}}_{t})}{\partial{\bm{W}}_{t}}," display="block"><semantics id="S2.E4.m1.2a"><mrow id="S2.E4.m1.2.2.1" xref="S2.E4.m1.2.2.1.1.cmml"><mrow id="S2.E4.m1.2.2.1.1" xref="S2.E4.m1.2.2.1.1.cmml"><msub id="S2.E4.m1.2.2.1.1.2" xref="S2.E4.m1.2.2.1.1.2.cmml"><mi id="S2.E4.m1.2.2.1.1.2.2" xref="S2.E4.m1.2.2.1.1.2.2.cmml">ğ‘¾</mi><mrow id="S2.E4.m1.2.2.1.1.2.3" xref="S2.E4.m1.2.2.1.1.2.3.cmml"><mi id="S2.E4.m1.2.2.1.1.2.3.2" xref="S2.E4.m1.2.2.1.1.2.3.2.cmml">t</mi><mo id="S2.E4.m1.2.2.1.1.2.3.1" xref="S2.E4.m1.2.2.1.1.2.3.1.cmml">+</mo><mn id="S2.E4.m1.2.2.1.1.2.3.3" xref="S2.E4.m1.2.2.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.E4.m1.2.2.1.1.1" xref="S2.E4.m1.2.2.1.1.1.cmml">=</mo><mrow id="S2.E4.m1.2.2.1.1.3" xref="S2.E4.m1.2.2.1.1.3.cmml"><msub id="S2.E4.m1.2.2.1.1.3.2" xref="S2.E4.m1.2.2.1.1.3.2.cmml"><mi id="S2.E4.m1.2.2.1.1.3.2.2" xref="S2.E4.m1.2.2.1.1.3.2.2.cmml">ğ‘¾</mi><mi id="S2.E4.m1.2.2.1.1.3.2.3" xref="S2.E4.m1.2.2.1.1.3.2.3.cmml">t</mi></msub><mo id="S2.E4.m1.2.2.1.1.3.1" xref="S2.E4.m1.2.2.1.1.3.1.cmml">+</mo><mrow id="S2.E4.m1.2.2.1.1.3.3" xref="S2.E4.m1.2.2.1.1.3.3.cmml"><mi id="S2.E4.m1.2.2.1.1.3.3.2" xref="S2.E4.m1.2.2.1.1.3.3.2.cmml">Î·</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.2.1.1.3.3.1" xref="S2.E4.m1.2.2.1.1.3.3.1.cmml">â€‹</mo><mfrac id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.cmml"><mo rspace="0em" id="S2.E4.m1.1.1.1.2" xref="S2.E4.m1.1.1.1.2.cmml">âˆ‚</mo><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E4.m1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S2.E4.m1.1.1.3" xref="S2.E4.m1.1.1.3.cmml"><mo rspace="0em" id="S2.E4.m1.1.1.3.1" xref="S2.E4.m1.1.1.3.1.cmml">âˆ‚</mo><msub id="S2.E4.m1.1.1.3.2" xref="S2.E4.m1.1.1.3.2.cmml"><mi id="S2.E4.m1.1.1.3.2.2" xref="S2.E4.m1.1.1.3.2.2.cmml">ğ‘¾</mi><mi id="S2.E4.m1.1.1.3.2.3" xref="S2.E4.m1.1.1.3.2.3.cmml">t</mi></msub></mrow></mfrac></mrow></mrow></mrow><mo id="S2.E4.m1.2.2.1.2" xref="S2.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.2b"><apply id="S2.E4.m1.2.2.1.1.cmml" xref="S2.E4.m1.2.2.1"><eq id="S2.E4.m1.2.2.1.1.1.cmml" xref="S2.E4.m1.2.2.1.1.1"></eq><apply id="S2.E4.m1.2.2.1.1.2.cmml" xref="S2.E4.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.1.2.1.cmml" xref="S2.E4.m1.2.2.1.1.2">subscript</csymbol><ci id="S2.E4.m1.2.2.1.1.2.2.cmml" xref="S2.E4.m1.2.2.1.1.2.2">ğ‘¾</ci><apply id="S2.E4.m1.2.2.1.1.2.3.cmml" xref="S2.E4.m1.2.2.1.1.2.3"><plus id="S2.E4.m1.2.2.1.1.2.3.1.cmml" xref="S2.E4.m1.2.2.1.1.2.3.1"></plus><ci id="S2.E4.m1.2.2.1.1.2.3.2.cmml" xref="S2.E4.m1.2.2.1.1.2.3.2">ğ‘¡</ci><cn type="integer" id="S2.E4.m1.2.2.1.1.2.3.3.cmml" xref="S2.E4.m1.2.2.1.1.2.3.3">1</cn></apply></apply><apply id="S2.E4.m1.2.2.1.1.3.cmml" xref="S2.E4.m1.2.2.1.1.3"><plus id="S2.E4.m1.2.2.1.1.3.1.cmml" xref="S2.E4.m1.2.2.1.1.3.1"></plus><apply id="S2.E4.m1.2.2.1.1.3.2.cmml" xref="S2.E4.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.1.3.2.1.cmml" xref="S2.E4.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S2.E4.m1.2.2.1.1.3.2.2.cmml" xref="S2.E4.m1.2.2.1.1.3.2.2">ğ‘¾</ci><ci id="S2.E4.m1.2.2.1.1.3.2.3.cmml" xref="S2.E4.m1.2.2.1.1.3.2.3">ğ‘¡</ci></apply><apply id="S2.E4.m1.2.2.1.1.3.3.cmml" xref="S2.E4.m1.2.2.1.1.3.3"><times id="S2.E4.m1.2.2.1.1.3.3.1.cmml" xref="S2.E4.m1.2.2.1.1.3.3.1"></times><ci id="S2.E4.m1.2.2.1.1.3.3.2.cmml" xref="S2.E4.m1.2.2.1.1.3.3.2">ğœ‚</ci><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><divide id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1"></divide><apply id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><partialdiff id="S2.E4.m1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.2"></partialdiff><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1"><times id="S2.E4.m1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2"></times><ci id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3">ğ‘“</ci><apply id="S2.E4.m1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.2">ğ‘¾</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.3">ğ‘¡</ci></apply></apply></apply><apply id="S2.E4.m1.1.1.3.cmml" xref="S2.E4.m1.1.1.3"><partialdiff id="S2.E4.m1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.3.1"></partialdiff><apply id="S2.E4.m1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.2.1.cmml" xref="S2.E4.m1.1.1.3.2">subscript</csymbol><ci id="S2.E4.m1.1.1.3.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2">ğ‘¾</ci><ci id="S2.E4.m1.1.1.3.2.3.cmml" xref="S2.E4.m1.1.1.3.2.3">ğ‘¡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.2c">{\bm{W}}_{t+1}={\bm{W}}_{t}+\eta\frac{\partial f({\bm{W}}_{t})}{\partial{\bm{W}}_{t}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p3.3" class="ltx_p">we can derive that</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.2" class="ltx_Math" alttext="\lVert{\bm{W}}_{t+1}\rVert_{2}=\lVert{\bm{W}}_{t}\rVert_{2}+\eta^{2}\lVert\frac{\partial f({\bm{W}}_{t})}{\partial{\bm{W}}_{t}}\rVert_{2}," display="block"><semantics id="S2.E5.m1.2a"><mrow id="S2.E5.m1.2.2.1" xref="S2.E5.m1.2.2.1.1.cmml"><mrow id="S2.E5.m1.2.2.1.1" xref="S2.E5.m1.2.2.1.1.cmml"><msub id="S2.E5.m1.2.2.1.1.1" xref="S2.E5.m1.2.2.1.1.1.cmml"><mrow id="S2.E5.m1.2.2.1.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S2.E5.m1.2.2.1.1.1.1.1.2" xref="S2.E5.m1.2.2.1.1.1.1.2.1.cmml">âˆ¥</mo><msub id="S2.E5.m1.2.2.1.1.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.2.2.1.1.1.1.1.1.2" xref="S2.E5.m1.2.2.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mrow id="S2.E5.m1.2.2.1.1.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S2.E5.m1.2.2.1.1.1.1.1.1.3.2" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S2.E5.m1.2.2.1.1.1.1.1.1.3.1" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">+</mo><mn id="S2.E5.m1.2.2.1.1.1.1.1.1.3.3" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo fence="true" lspace="0em" id="S2.E5.m1.2.2.1.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S2.E5.m1.2.2.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.3.cmml">2</mn></msub><mo rspace="0.1389em" id="S2.E5.m1.2.2.1.1.3" xref="S2.E5.m1.2.2.1.1.3.cmml">=</mo><mrow id="S2.E5.m1.2.2.1.1.2" xref="S2.E5.m1.2.2.1.1.2.cmml"><msub id="S2.E5.m1.2.2.1.1.2.1" xref="S2.E5.m1.2.2.1.1.2.1.cmml"><mrow id="S2.E5.m1.2.2.1.1.2.1.1.1" xref="S2.E5.m1.2.2.1.1.2.1.1.2.cmml"><mo fence="true" lspace="0.1389em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.1.1.1.2" xref="S2.E5.m1.2.2.1.1.2.1.1.2.1.cmml">âˆ¥</mo><msub id="S2.E5.m1.2.2.1.1.2.1.1.1.1" xref="S2.E5.m1.2.2.1.1.2.1.1.1.1.cmml"><mi id="S2.E5.m1.2.2.1.1.2.1.1.1.1.2" xref="S2.E5.m1.2.2.1.1.2.1.1.1.1.2.cmml">ğ‘¾</mi><mi id="S2.E5.m1.2.2.1.1.2.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.2.1.1.1.1.3.cmml">t</mi></msub><mo fence="true" lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.1.1.1.3" xref="S2.E5.m1.2.2.1.1.2.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S2.E5.m1.2.2.1.1.2.1.3" xref="S2.E5.m1.2.2.1.1.2.1.3.cmml">2</mn></msub><mo id="S2.E5.m1.2.2.1.1.2.2" xref="S2.E5.m1.2.2.1.1.2.2.cmml">+</mo><mrow id="S2.E5.m1.2.2.1.1.2.3" xref="S2.E5.m1.2.2.1.1.2.3.cmml"><msup id="S2.E5.m1.2.2.1.1.2.3.2" xref="S2.E5.m1.2.2.1.1.2.3.2.cmml"><mi id="S2.E5.m1.2.2.1.1.2.3.2.2" xref="S2.E5.m1.2.2.1.1.2.3.2.2.cmml">Î·</mi><mn id="S2.E5.m1.2.2.1.1.2.3.2.3" xref="S2.E5.m1.2.2.1.1.2.3.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.3.1" xref="S2.E5.m1.2.2.1.1.2.3.1.cmml">â€‹</mo><msub id="S2.E5.m1.2.2.1.1.2.3.3" xref="S2.E5.m1.2.2.1.1.2.3.3.cmml"><mrow id="S2.E5.m1.2.2.1.1.2.3.3.2.2" xref="S2.E5.m1.2.2.1.1.2.3.3.2.1.cmml"><mo fence="true" rspace="0em" id="S2.E5.m1.2.2.1.1.2.3.3.2.2.1" xref="S2.E5.m1.2.2.1.1.2.3.3.2.1.1.cmml">âˆ¥</mo><mfrac id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mo rspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">âˆ‚</mo><mrow id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mi id="S2.E5.m1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S2.E5.m1.1.1.3" xref="S2.E5.m1.1.1.3.cmml"><mo rspace="0em" id="S2.E5.m1.1.1.3.1" xref="S2.E5.m1.1.1.3.1.cmml">âˆ‚</mo><msub id="S2.E5.m1.1.1.3.2" xref="S2.E5.m1.1.1.3.2.cmml"><mi id="S2.E5.m1.1.1.3.2.2" xref="S2.E5.m1.1.1.3.2.2.cmml">ğ‘¾</mi><mi id="S2.E5.m1.1.1.3.2.3" xref="S2.E5.m1.1.1.3.2.3.cmml">t</mi></msub></mrow></mfrac><mo fence="true" lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.3.3.2.2.2" xref="S2.E5.m1.2.2.1.1.2.3.3.2.1.1.cmml">âˆ¥</mo></mrow><mn id="S2.E5.m1.2.2.1.1.2.3.3.3" xref="S2.E5.m1.2.2.1.1.2.3.3.3.cmml">2</mn></msub></mrow></mrow></mrow><mo id="S2.E5.m1.2.2.1.2" xref="S2.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.2b"><apply id="S2.E5.m1.2.2.1.1.cmml" xref="S2.E5.m1.2.2.1"><eq id="S2.E5.m1.2.2.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.3"></eq><apply id="S2.E5.m1.2.2.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1">subscript</csymbol><apply id="S2.E5.m1.2.2.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.2.2.1.1.1.1.2.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S2.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.2">ğ‘¾</ci><apply id="S2.E5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3"><plus id="S2.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.1"></plus><ci id="S2.E5.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S2.E5.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply><cn type="integer" id="S2.E5.m1.2.2.1.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.1.3">2</cn></apply><apply id="S2.E5.m1.2.2.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.2"><plus id="S2.E5.m1.2.2.1.1.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2"></plus><apply id="S2.E5.m1.2.2.1.1.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.1.2.cmml" xref="S2.E5.m1.2.2.1.1.2.1">subscript</csymbol><apply id="S2.E5.m1.2.2.1.1.2.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.2.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.2.2.1.1.2.1.1.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S2.E5.m1.2.2.1.1.2.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.2.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.2.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.2.1.1.1.1.2">ğ‘¾</ci><ci id="S2.E5.m1.2.2.1.1.2.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.2.1.1.1.1.3">ğ‘¡</ci></apply></apply><cn type="integer" id="S2.E5.m1.2.2.1.1.2.1.3.cmml" xref="S2.E5.m1.2.2.1.1.2.1.3">2</cn></apply><apply id="S2.E5.m1.2.2.1.1.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.3"><times id="S2.E5.m1.2.2.1.1.2.3.1.cmml" xref="S2.E5.m1.2.2.1.1.2.3.1"></times><apply id="S2.E5.m1.2.2.1.1.2.3.2.cmml" xref="S2.E5.m1.2.2.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.3.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.3.2">superscript</csymbol><ci id="S2.E5.m1.2.2.1.1.2.3.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.3.2.2">ğœ‚</ci><cn type="integer" id="S2.E5.m1.2.2.1.1.2.3.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.3.2.3">2</cn></apply><apply id="S2.E5.m1.2.2.1.1.2.3.3.cmml" xref="S2.E5.m1.2.2.1.1.2.3.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.3.3.1.cmml" xref="S2.E5.m1.2.2.1.1.2.3.3">subscript</csymbol><apply id="S2.E5.m1.2.2.1.1.2.3.3.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.3.3.2.2"><csymbol cd="latexml" id="S2.E5.m1.2.2.1.1.2.3.3.2.1.1.cmml" xref="S2.E5.m1.2.2.1.1.2.3.3.2.2.1">delimited-âˆ¥âˆ¥</csymbol><apply id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1"><divide id="S2.E5.m1.1.1.2.cmml" xref="S2.E5.m1.1.1"></divide><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><partialdiff id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></partialdiff><apply id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1"><times id="S2.E5.m1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2"></times><ci id="S2.E5.m1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3">ğ‘“</ci><apply id="S2.E5.m1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.2">ğ‘¾</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.3">ğ‘¡</ci></apply></apply></apply><apply id="S2.E5.m1.1.1.3.cmml" xref="S2.E5.m1.1.1.3"><partialdiff id="S2.E5.m1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.3.1"></partialdiff><apply id="S2.E5.m1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.3.2.1.cmml" xref="S2.E5.m1.1.1.3.2">subscript</csymbol><ci id="S2.E5.m1.1.1.3.2.2.cmml" xref="S2.E5.m1.1.1.3.2.2">ğ‘¾</ci><ci id="S2.E5.m1.1.1.3.2.3.cmml" xref="S2.E5.m1.1.1.3.2.3">ğ‘¡</ci></apply></apply></apply></apply><cn type="integer" id="S2.E5.m1.2.2.1.1.2.3.3.3.cmml" xref="S2.E5.m1.2.2.1.1.2.3.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.2c">\lVert{\bm{W}}_{t+1}\rVert_{2}=\lVert{\bm{W}}_{t}\rVert_{2}+\eta^{2}\lVert\frac{\partial f({\bm{W}}_{t})}{\partial{\bm{W}}_{t}}\rVert_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p3.4" class="ltx_p">where the middle term disappears because of the property in Lemma 1. The auto-tuning effect in Eq. <a href="#S2.E3" title="In 2.2 Scale invariant property of normalization â€£ 2 Preliminaries â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> will force the norm of weights to converge to an equilibrium. When the norm of weights is large, the norm of gradients will be small and vice versa. We will discuss why this property is important in Federated Learning in the next section.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>External Covariate Shift and Adaptive-Balancing of Weights</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>External convariate shift</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">In Federated Learning, participating devices train their local models for multiple steps in each communication round with their own private data. In this process, we observe that the statistics of channels is significantly diverse between devices. This phenomenon is caused by the independent local training process on different devices, resulting in different internal covariate shifts. We name this phenomenon as <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">external covariate shift</span>, which is a unique problem in FL.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">The external covariate shift phenomenon slows down the convergence of the global model in Federated Learning, since the feature distribution varies after aggregation and neurons have to keep adapting the updated feature distribution. We attribute this feature distribution shifts as the key reason that batch normalization drops the global accuracy, which will be detailed in SectionÂ <a href="#S3.SS3" title="3.3 Failure of batch normalization â€£ 3 External Covariate Shift and Adaptive-Balancing of Weights â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.16" class="ltx_p">In addition to the obstacle to convergence caused by the heterogeneous feature distributions, the external covariate shift phenomenon also harms the aggregation step in the federated training process. To describe why the aggregation step is harmed by the external covariate shift, we consider a toy example. For simplicity, we use the fully connected (FC) layer as an example for analysis. Note that such an analysis can be naturally extended to other types of layers. Specifically, a FC layer is represented as:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="{\bm{y}}={\bm{W}}{\bm{x}}+{\bm{b}}," display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml">ğ’š</mi><mo id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><mrow id="S3.E6.m1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.3.2.2.cmml">ğ‘¾</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.2.1" xref="S3.E6.m1.1.1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.3.2.3.cmml">ğ’™</mi></mrow><mo id="S3.E6.m1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.3.1.cmml">+</mo><mi id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml">ğ’ƒ</mi></mrow></mrow><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"></eq><ci id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2">ğ’š</ci><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><plus id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.1"></plus><apply id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2"><times id="S3.E6.m1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1"></times><ci id="S3.E6.m1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.2">ğ‘¾</ci><ci id="S3.E6.m1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3">ğ’™</ci></apply><ci id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3">ğ’ƒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">{\bm{y}}={\bm{W}}{\bm{x}}+{\bm{b}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.15" class="ltx_p">where <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="{\bm{x}}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">ğ’™</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ’™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">{\bm{x}}</annotation></semantics></math> is the input to this layer, <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="{\bm{W}}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">ğ‘¾</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ğ‘¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">{\bm{W}}</annotation></semantics></math> and <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="{\bm{b}}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">ğ’ƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğ’ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">{\bm{b}}</annotation></semantics></math> are weight and bias, <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">ğ’š</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ğ’š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">{\bm{y}}</annotation></semantics></math> is the feature of this layer. We assume that the input features <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="{\bm{x}}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">ğ’™</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">ğ’™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">{\bm{x}}</annotation></semantics></math> for this layer are whitened (independently distributed with zero mean and unit variance), then <math id="S3.SS1.p3.6.m6.2" class="ltx_Math" alttext="\mu[{\bm{y}}]=\mu[{\bm{b}}]" display="inline"><semantics id="S3.SS1.p3.6.m6.2a"><mrow id="S3.SS1.p3.6.m6.2.3" xref="S3.SS1.p3.6.m6.2.3.cmml"><mrow id="S3.SS1.p3.6.m6.2.3.2" xref="S3.SS1.p3.6.m6.2.3.2.cmml"><mi id="S3.SS1.p3.6.m6.2.3.2.2" xref="S3.SS1.p3.6.m6.2.3.2.2.cmml">Î¼</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.6.m6.2.3.2.1" xref="S3.SS1.p3.6.m6.2.3.2.1.cmml">â€‹</mo><mrow id="S3.SS1.p3.6.m6.2.3.2.3.2" xref="S3.SS1.p3.6.m6.2.3.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.6.m6.2.3.2.3.2.1" xref="S3.SS1.p3.6.m6.2.3.2.3.1.1.cmml">[</mo><mi id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">ğ’š</mi><mo stretchy="false" id="S3.SS1.p3.6.m6.2.3.2.3.2.2" xref="S3.SS1.p3.6.m6.2.3.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S3.SS1.p3.6.m6.2.3.1" xref="S3.SS1.p3.6.m6.2.3.1.cmml">=</mo><mrow id="S3.SS1.p3.6.m6.2.3.3" xref="S3.SS1.p3.6.m6.2.3.3.cmml"><mi id="S3.SS1.p3.6.m6.2.3.3.2" xref="S3.SS1.p3.6.m6.2.3.3.2.cmml">Î¼</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.6.m6.2.3.3.1" xref="S3.SS1.p3.6.m6.2.3.3.1.cmml">â€‹</mo><mrow id="S3.SS1.p3.6.m6.2.3.3.3.2" xref="S3.SS1.p3.6.m6.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.6.m6.2.3.3.3.2.1" xref="S3.SS1.p3.6.m6.2.3.3.3.1.1.cmml">[</mo><mi id="S3.SS1.p3.6.m6.2.2" xref="S3.SS1.p3.6.m6.2.2.cmml">ğ’ƒ</mi><mo stretchy="false" id="S3.SS1.p3.6.m6.2.3.3.3.2.2" xref="S3.SS1.p3.6.m6.2.3.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.2b"><apply id="S3.SS1.p3.6.m6.2.3.cmml" xref="S3.SS1.p3.6.m6.2.3"><eq id="S3.SS1.p3.6.m6.2.3.1.cmml" xref="S3.SS1.p3.6.m6.2.3.1"></eq><apply id="S3.SS1.p3.6.m6.2.3.2.cmml" xref="S3.SS1.p3.6.m6.2.3.2"><times id="S3.SS1.p3.6.m6.2.3.2.1.cmml" xref="S3.SS1.p3.6.m6.2.3.2.1"></times><ci id="S3.SS1.p3.6.m6.2.3.2.2.cmml" xref="S3.SS1.p3.6.m6.2.3.2.2">ğœ‡</ci><apply id="S3.SS1.p3.6.m6.2.3.2.3.1.cmml" xref="S3.SS1.p3.6.m6.2.3.2.3.2"><csymbol cd="latexml" id="S3.SS1.p3.6.m6.2.3.2.3.1.1.cmml" xref="S3.SS1.p3.6.m6.2.3.2.3.2.1">delimited-[]</csymbol><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">ğ’š</ci></apply></apply><apply id="S3.SS1.p3.6.m6.2.3.3.cmml" xref="S3.SS1.p3.6.m6.2.3.3"><times id="S3.SS1.p3.6.m6.2.3.3.1.cmml" xref="S3.SS1.p3.6.m6.2.3.3.1"></times><ci id="S3.SS1.p3.6.m6.2.3.3.2.cmml" xref="S3.SS1.p3.6.m6.2.3.3.2">ğœ‡</ci><apply id="S3.SS1.p3.6.m6.2.3.3.3.1.cmml" xref="S3.SS1.p3.6.m6.2.3.3.3.2"><csymbol cd="latexml" id="S3.SS1.p3.6.m6.2.3.3.3.1.1.cmml" xref="S3.SS1.p3.6.m6.2.3.3.3.2.1">delimited-[]</csymbol><ci id="S3.SS1.p3.6.m6.2.2.cmml" xref="S3.SS1.p3.6.m6.2.2">ğ’ƒ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.2c">\mu[{\bm{y}}]=\mu[{\bm{b}}]</annotation></semantics></math> and <math id="S3.SS1.p3.7.m7.2" class="ltx_Math" alttext="\sigma[{\bm{y}}]=||{\bm{W}}||_{2}" display="inline"><semantics id="S3.SS1.p3.7.m7.2a"><mrow id="S3.SS1.p3.7.m7.2.3" xref="S3.SS1.p3.7.m7.2.3.cmml"><mrow id="S3.SS1.p3.7.m7.2.3.2" xref="S3.SS1.p3.7.m7.2.3.2.cmml"><mi id="S3.SS1.p3.7.m7.2.3.2.2" xref="S3.SS1.p3.7.m7.2.3.2.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.7.m7.2.3.2.1" xref="S3.SS1.p3.7.m7.2.3.2.1.cmml">â€‹</mo><mrow id="S3.SS1.p3.7.m7.2.3.2.3.2" xref="S3.SS1.p3.7.m7.2.3.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.7.m7.2.3.2.3.2.1" xref="S3.SS1.p3.7.m7.2.3.2.3.1.1.cmml">[</mo><mi id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml">ğ’š</mi><mo stretchy="false" id="S3.SS1.p3.7.m7.2.3.2.3.2.2" xref="S3.SS1.p3.7.m7.2.3.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S3.SS1.p3.7.m7.2.3.1" xref="S3.SS1.p3.7.m7.2.3.1.cmml">=</mo><msub id="S3.SS1.p3.7.m7.2.3.3" xref="S3.SS1.p3.7.m7.2.3.3.cmml"><mrow id="S3.SS1.p3.7.m7.2.3.3.2.2" xref="S3.SS1.p3.7.m7.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.SS1.p3.7.m7.2.3.3.2.2.1" xref="S3.SS1.p3.7.m7.2.3.3.2.1.1.cmml">â€–</mo><mi id="S3.SS1.p3.7.m7.2.2" xref="S3.SS1.p3.7.m7.2.2.cmml">ğ‘¾</mi><mo stretchy="false" id="S3.SS1.p3.7.m7.2.3.3.2.2.2" xref="S3.SS1.p3.7.m7.2.3.3.2.1.1.cmml">â€–</mo></mrow><mn id="S3.SS1.p3.7.m7.2.3.3.3" xref="S3.SS1.p3.7.m7.2.3.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.2b"><apply id="S3.SS1.p3.7.m7.2.3.cmml" xref="S3.SS1.p3.7.m7.2.3"><eq id="S3.SS1.p3.7.m7.2.3.1.cmml" xref="S3.SS1.p3.7.m7.2.3.1"></eq><apply id="S3.SS1.p3.7.m7.2.3.2.cmml" xref="S3.SS1.p3.7.m7.2.3.2"><times id="S3.SS1.p3.7.m7.2.3.2.1.cmml" xref="S3.SS1.p3.7.m7.2.3.2.1"></times><ci id="S3.SS1.p3.7.m7.2.3.2.2.cmml" xref="S3.SS1.p3.7.m7.2.3.2.2">ğœ</ci><apply id="S3.SS1.p3.7.m7.2.3.2.3.1.cmml" xref="S3.SS1.p3.7.m7.2.3.2.3.2"><csymbol cd="latexml" id="S3.SS1.p3.7.m7.2.3.2.3.1.1.cmml" xref="S3.SS1.p3.7.m7.2.3.2.3.2.1">delimited-[]</csymbol><ci id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">ğ’š</ci></apply></apply><apply id="S3.SS1.p3.7.m7.2.3.3.cmml" xref="S3.SS1.p3.7.m7.2.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.2.3.3.1.cmml" xref="S3.SS1.p3.7.m7.2.3.3">subscript</csymbol><apply id="S3.SS1.p3.7.m7.2.3.3.2.1.cmml" xref="S3.SS1.p3.7.m7.2.3.3.2.2"><csymbol cd="latexml" id="S3.SS1.p3.7.m7.2.3.3.2.1.1.cmml" xref="S3.SS1.p3.7.m7.2.3.3.2.2.1">norm</csymbol><ci id="S3.SS1.p3.7.m7.2.2.cmml" xref="S3.SS1.p3.7.m7.2.2">ğ‘¾</ci></apply><cn type="integer" id="S3.SS1.p3.7.m7.2.3.3.3.cmml" xref="S3.SS1.p3.7.m7.2.3.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.2c">\sigma[{\bm{y}}]=||{\bm{W}}||_{2}</annotation></semantics></math> where <math id="S3.SS1.p3.8.m8.1" class="ltx_math_unparsed" alttext="||\cdot||_{2}" display="inline"><semantics id="S3.SS1.p3.8.m8.1a"><mrow id="S3.SS1.p3.8.m8.1b"><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS1.p3.8.m8.1.1">|</mo><mo fence="false" stretchy="false" id="S3.SS1.p3.8.m8.1.2">|</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.8.m8.1.3">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS1.p3.8.m8.1.4">|</mo><msub id="S3.SS1.p3.8.m8.1.5"><mo fence="false" stretchy="false" id="S3.SS1.p3.8.m8.1.5.2">|</mo><mn id="S3.SS1.p3.8.m8.1.5.3">2</mn></msub></mrow><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">||\cdot||_{2}</annotation></semantics></math> denotes the Euclidean norm. For two sets of weights <math id="S3.SS1.p3.9.m9.1" class="ltx_Math" alttext="{\bm{W}}_{1}" display="inline"><semantics id="S3.SS1.p3.9.m9.1a"><msub id="S3.SS1.p3.9.m9.1.1" xref="S3.SS1.p3.9.m9.1.1.cmml"><mi id="S3.SS1.p3.9.m9.1.1.2" xref="S3.SS1.p3.9.m9.1.1.2.cmml">ğ‘¾</mi><mn id="S3.SS1.p3.9.m9.1.1.3" xref="S3.SS1.p3.9.m9.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m9.1b"><apply id="S3.SS1.p3.9.m9.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.9.m9.1.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p3.9.m9.1.1.2.cmml" xref="S3.SS1.p3.9.m9.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.SS1.p3.9.m9.1.1.3.cmml" xref="S3.SS1.p3.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m9.1c">{\bm{W}}_{1}</annotation></semantics></math> and <math id="S3.SS1.p3.10.m10.1" class="ltx_Math" alttext="{\bm{W}}_{2}" display="inline"><semantics id="S3.SS1.p3.10.m10.1a"><msub id="S3.SS1.p3.10.m10.1.1" xref="S3.SS1.p3.10.m10.1.1.cmml"><mi id="S3.SS1.p3.10.m10.1.1.2" xref="S3.SS1.p3.10.m10.1.1.2.cmml">ğ‘¾</mi><mn id="S3.SS1.p3.10.m10.1.1.3" xref="S3.SS1.p3.10.m10.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.m10.1b"><apply id="S3.SS1.p3.10.m10.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m10.1.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p3.10.m10.1.1.2.cmml" xref="S3.SS1.p3.10.m10.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.SS1.p3.10.m10.1.1.3.cmml" xref="S3.SS1.p3.10.m10.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.m10.1c">{\bm{W}}_{2}</annotation></semantics></math>, if their feature deviations have significant discrepancies, i.e., <math id="S3.SS1.p3.11.m11.2" class="ltx_Math" alttext="\sigma[{\bm{y}}_{1}]\gg\sigma[{\bm{y}}_{2}]" display="inline"><semantics id="S3.SS1.p3.11.m11.2a"><mrow id="S3.SS1.p3.11.m11.2.2" xref="S3.SS1.p3.11.m11.2.2.cmml"><mrow id="S3.SS1.p3.11.m11.1.1.1" xref="S3.SS1.p3.11.m11.1.1.1.cmml"><mi id="S3.SS1.p3.11.m11.1.1.1.3" xref="S3.SS1.p3.11.m11.1.1.1.3.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.11.m11.1.1.1.2" xref="S3.SS1.p3.11.m11.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p3.11.m11.1.1.1.1.1" xref="S3.SS1.p3.11.m11.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.11.m11.1.1.1.1.1.2" xref="S3.SS1.p3.11.m11.1.1.1.1.2.1.cmml">[</mo><msub id="S3.SS1.p3.11.m11.1.1.1.1.1.1" xref="S3.SS1.p3.11.m11.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.11.m11.1.1.1.1.1.1.2" xref="S3.SS1.p3.11.m11.1.1.1.1.1.1.2.cmml">ğ’š</mi><mn id="S3.SS1.p3.11.m11.1.1.1.1.1.1.3" xref="S3.SS1.p3.11.m11.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS1.p3.11.m11.1.1.1.1.1.3" xref="S3.SS1.p3.11.m11.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.SS1.p3.11.m11.2.2.3" xref="S3.SS1.p3.11.m11.2.2.3.cmml">â‰«</mo><mrow id="S3.SS1.p3.11.m11.2.2.2" xref="S3.SS1.p3.11.m11.2.2.2.cmml"><mi id="S3.SS1.p3.11.m11.2.2.2.3" xref="S3.SS1.p3.11.m11.2.2.2.3.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.11.m11.2.2.2.2" xref="S3.SS1.p3.11.m11.2.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.p3.11.m11.2.2.2.1.1" xref="S3.SS1.p3.11.m11.2.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.11.m11.2.2.2.1.1.2" xref="S3.SS1.p3.11.m11.2.2.2.1.2.1.cmml">[</mo><msub id="S3.SS1.p3.11.m11.2.2.2.1.1.1" xref="S3.SS1.p3.11.m11.2.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.11.m11.2.2.2.1.1.1.2" xref="S3.SS1.p3.11.m11.2.2.2.1.1.1.2.cmml">ğ’š</mi><mn id="S3.SS1.p3.11.m11.2.2.2.1.1.1.3" xref="S3.SS1.p3.11.m11.2.2.2.1.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="S3.SS1.p3.11.m11.2.2.2.1.1.3" xref="S3.SS1.p3.11.m11.2.2.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.11.m11.2b"><apply id="S3.SS1.p3.11.m11.2.2.cmml" xref="S3.SS1.p3.11.m11.2.2"><csymbol cd="latexml" id="S3.SS1.p3.11.m11.2.2.3.cmml" xref="S3.SS1.p3.11.m11.2.2.3">much-greater-than</csymbol><apply id="S3.SS1.p3.11.m11.1.1.1.cmml" xref="S3.SS1.p3.11.m11.1.1.1"><times id="S3.SS1.p3.11.m11.1.1.1.2.cmml" xref="S3.SS1.p3.11.m11.1.1.1.2"></times><ci id="S3.SS1.p3.11.m11.1.1.1.3.cmml" xref="S3.SS1.p3.11.m11.1.1.1.3">ğœ</ci><apply id="S3.SS1.p3.11.m11.1.1.1.1.2.cmml" xref="S3.SS1.p3.11.m11.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.11.m11.1.1.1.1.2.1.cmml" xref="S3.SS1.p3.11.m11.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.p3.11.m11.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.11.m11.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.11.m11.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.11.m11.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.11.m11.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.11.m11.1.1.1.1.1.1.2">ğ’š</ci><cn type="integer" id="S3.SS1.p3.11.m11.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.11.m11.1.1.1.1.1.1.3">1</cn></apply></apply></apply><apply id="S3.SS1.p3.11.m11.2.2.2.cmml" xref="S3.SS1.p3.11.m11.2.2.2"><times id="S3.SS1.p3.11.m11.2.2.2.2.cmml" xref="S3.SS1.p3.11.m11.2.2.2.2"></times><ci id="S3.SS1.p3.11.m11.2.2.2.3.cmml" xref="S3.SS1.p3.11.m11.2.2.2.3">ğœ</ci><apply id="S3.SS1.p3.11.m11.2.2.2.1.2.cmml" xref="S3.SS1.p3.11.m11.2.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p3.11.m11.2.2.2.1.2.1.cmml" xref="S3.SS1.p3.11.m11.2.2.2.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.p3.11.m11.2.2.2.1.1.1.cmml" xref="S3.SS1.p3.11.m11.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.11.m11.2.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.11.m11.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.11.m11.2.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.11.m11.2.2.2.1.1.1.2">ğ’š</ci><cn type="integer" id="S3.SS1.p3.11.m11.2.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.11.m11.2.2.2.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.11.m11.2c">\sigma[{\bm{y}}_{1}]\gg\sigma[{\bm{y}}_{2}]</annotation></semantics></math>, then we can derive that <math id="S3.SS1.p3.12.m12.2" class="ltx_Math" alttext="||{\bm{W}}_{1}||_{2}\gg||{\bm{W}}_{2}||_{2}" display="inline"><semantics id="S3.SS1.p3.12.m12.2a"><mrow id="S3.SS1.p3.12.m12.2.2" xref="S3.SS1.p3.12.m12.2.2.cmml"><msub id="S3.SS1.p3.12.m12.1.1.1" xref="S3.SS1.p3.12.m12.1.1.1.cmml"><mrow id="S3.SS1.p3.12.m12.1.1.1.1.1" xref="S3.SS1.p3.12.m12.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.12.m12.1.1.1.1.1.2" xref="S3.SS1.p3.12.m12.1.1.1.1.2.1.cmml">â€–</mo><msub id="S3.SS1.p3.12.m12.1.1.1.1.1.1" xref="S3.SS1.p3.12.m12.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.12.m12.1.1.1.1.1.1.2" xref="S3.SS1.p3.12.m12.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mn id="S3.SS1.p3.12.m12.1.1.1.1.1.1.3" xref="S3.SS1.p3.12.m12.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS1.p3.12.m12.1.1.1.1.1.3" xref="S3.SS1.p3.12.m12.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.SS1.p3.12.m12.1.1.1.3" xref="S3.SS1.p3.12.m12.1.1.1.3.cmml">2</mn></msub><mo id="S3.SS1.p3.12.m12.2.2.3" xref="S3.SS1.p3.12.m12.2.2.3.cmml">â‰«</mo><msub id="S3.SS1.p3.12.m12.2.2.2" xref="S3.SS1.p3.12.m12.2.2.2.cmml"><mrow id="S3.SS1.p3.12.m12.2.2.2.1.1" xref="S3.SS1.p3.12.m12.2.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.12.m12.2.2.2.1.1.2" xref="S3.SS1.p3.12.m12.2.2.2.1.2.1.cmml">â€–</mo><msub id="S3.SS1.p3.12.m12.2.2.2.1.1.1" xref="S3.SS1.p3.12.m12.2.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.12.m12.2.2.2.1.1.1.2" xref="S3.SS1.p3.12.m12.2.2.2.1.1.1.2.cmml">ğ‘¾</mi><mn id="S3.SS1.p3.12.m12.2.2.2.1.1.1.3" xref="S3.SS1.p3.12.m12.2.2.2.1.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="S3.SS1.p3.12.m12.2.2.2.1.1.3" xref="S3.SS1.p3.12.m12.2.2.2.1.2.1.cmml">â€–</mo></mrow><mn id="S3.SS1.p3.12.m12.2.2.2.3" xref="S3.SS1.p3.12.m12.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.12.m12.2b"><apply id="S3.SS1.p3.12.m12.2.2.cmml" xref="S3.SS1.p3.12.m12.2.2"><csymbol cd="latexml" id="S3.SS1.p3.12.m12.2.2.3.cmml" xref="S3.SS1.p3.12.m12.2.2.3">much-greater-than</csymbol><apply id="S3.SS1.p3.12.m12.1.1.1.cmml" xref="S3.SS1.p3.12.m12.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m12.1.1.1.2.cmml" xref="S3.SS1.p3.12.m12.1.1.1">subscript</csymbol><apply id="S3.SS1.p3.12.m12.1.1.1.1.2.cmml" xref="S3.SS1.p3.12.m12.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.12.m12.1.1.1.1.2.1.cmml" xref="S3.SS1.p3.12.m12.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS1.p3.12.m12.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.12.m12.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m12.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.12.m12.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.12.m12.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.12.m12.1.1.1.1.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.SS1.p3.12.m12.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.12.m12.1.1.1.1.1.1.3">1</cn></apply></apply><cn type="integer" id="S3.SS1.p3.12.m12.1.1.1.3.cmml" xref="S3.SS1.p3.12.m12.1.1.1.3">2</cn></apply><apply id="S3.SS1.p3.12.m12.2.2.2.cmml" xref="S3.SS1.p3.12.m12.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m12.2.2.2.2.cmml" xref="S3.SS1.p3.12.m12.2.2.2">subscript</csymbol><apply id="S3.SS1.p3.12.m12.2.2.2.1.2.cmml" xref="S3.SS1.p3.12.m12.2.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p3.12.m12.2.2.2.1.2.1.cmml" xref="S3.SS1.p3.12.m12.2.2.2.1.1.2">norm</csymbol><apply id="S3.SS1.p3.12.m12.2.2.2.1.1.1.cmml" xref="S3.SS1.p3.12.m12.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m12.2.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.12.m12.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.12.m12.2.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.12.m12.2.2.2.1.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.SS1.p3.12.m12.2.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.12.m12.2.2.2.1.1.1.3">2</cn></apply></apply><cn type="integer" id="S3.SS1.p3.12.m12.2.2.2.3.cmml" xref="S3.SS1.p3.12.m12.2.2.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.12.m12.2c">||{\bm{W}}_{1}||_{2}\gg||{\bm{W}}_{2}||_{2}</annotation></semantics></math>. Thus, the essence of external covariate shift describes the shift of model parameterâ€™s norm. Considering these two sets of weights belonging to two local models involved in FL, then the contribution of <math id="S3.SS1.p3.13.m13.1" class="ltx_Math" alttext="{\bm{W}}_{2}" display="inline"><semantics id="S3.SS1.p3.13.m13.1a"><msub id="S3.SS1.p3.13.m13.1.1" xref="S3.SS1.p3.13.m13.1.1.cmml"><mi id="S3.SS1.p3.13.m13.1.1.2" xref="S3.SS1.p3.13.m13.1.1.2.cmml">ğ‘¾</mi><mn id="S3.SS1.p3.13.m13.1.1.3" xref="S3.SS1.p3.13.m13.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.13.m13.1b"><apply id="S3.SS1.p3.13.m13.1.1.cmml" xref="S3.SS1.p3.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.13.m13.1.1.1.cmml" xref="S3.SS1.p3.13.m13.1.1">subscript</csymbol><ci id="S3.SS1.p3.13.m13.1.1.2.cmml" xref="S3.SS1.p3.13.m13.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.SS1.p3.13.m13.1.1.3.cmml" xref="S3.SS1.p3.13.m13.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.13.m13.1c">{\bm{W}}_{2}</annotation></semantics></math> will be obliterated by <math id="S3.SS1.p3.14.m14.1" class="ltx_Math" alttext="{\bm{W}}_{1}" display="inline"><semantics id="S3.SS1.p3.14.m14.1a"><msub id="S3.SS1.p3.14.m14.1.1" xref="S3.SS1.p3.14.m14.1.1.cmml"><mi id="S3.SS1.p3.14.m14.1.1.2" xref="S3.SS1.p3.14.m14.1.1.2.cmml">ğ‘¾</mi><mn id="S3.SS1.p3.14.m14.1.1.3" xref="S3.SS1.p3.14.m14.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.14.m14.1b"><apply id="S3.SS1.p3.14.m14.1.1.cmml" xref="S3.SS1.p3.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.14.m14.1.1.1.cmml" xref="S3.SS1.p3.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.p3.14.m14.1.1.2.cmml" xref="S3.SS1.p3.14.m14.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.SS1.p3.14.m14.1.1.3.cmml" xref="S3.SS1.p3.14.m14.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.14.m14.1c">{\bm{W}}_{1}</annotation></semantics></math> as shown in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3.1 External convariate shift â€£ 3 External Covariate Shift and Adaptive-Balancing of Weights â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and the same effect applies to the bias, which is more related to <math id="S3.SS1.p3.15.m15.1" class="ltx_Math" alttext="\mu[{\bm{y}}]" display="inline"><semantics id="S3.SS1.p3.15.m15.1a"><mrow id="S3.SS1.p3.15.m15.1.2" xref="S3.SS1.p3.15.m15.1.2.cmml"><mi id="S3.SS1.p3.15.m15.1.2.2" xref="S3.SS1.p3.15.m15.1.2.2.cmml">Î¼</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.15.m15.1.2.1" xref="S3.SS1.p3.15.m15.1.2.1.cmml">â€‹</mo><mrow id="S3.SS1.p3.15.m15.1.2.3.2" xref="S3.SS1.p3.15.m15.1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.15.m15.1.2.3.2.1" xref="S3.SS1.p3.15.m15.1.2.3.1.1.cmml">[</mo><mi id="S3.SS1.p3.15.m15.1.1" xref="S3.SS1.p3.15.m15.1.1.cmml">ğ’š</mi><mo stretchy="false" id="S3.SS1.p3.15.m15.1.2.3.2.2" xref="S3.SS1.p3.15.m15.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.15.m15.1b"><apply id="S3.SS1.p3.15.m15.1.2.cmml" xref="S3.SS1.p3.15.m15.1.2"><times id="S3.SS1.p3.15.m15.1.2.1.cmml" xref="S3.SS1.p3.15.m15.1.2.1"></times><ci id="S3.SS1.p3.15.m15.1.2.2.cmml" xref="S3.SS1.p3.15.m15.1.2.2">ğœ‡</ci><apply id="S3.SS1.p3.15.m15.1.2.3.1.cmml" xref="S3.SS1.p3.15.m15.1.2.3.2"><csymbol cd="latexml" id="S3.SS1.p3.15.m15.1.2.3.1.1.cmml" xref="S3.SS1.p3.15.m15.1.2.3.2.1">delimited-[]</csymbol><ci id="S3.SS1.p3.15.m15.1.1.cmml" xref="S3.SS1.p3.15.m15.1.1">ğ’š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.15.m15.1c">\mu[{\bm{y}}]</annotation></semantics></math>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2210.03277/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="267" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The contribution of the model with a smaller norm will be obliterated by the model with a larger norm during aggregation.</figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">In FL, it is reasonable that weights on different devices are diverse due to non-IID data, but they should have similar norms <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="||{\bm{W}}||_{2}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.2" xref="S3.SS1.p4.1.m1.1.2.cmml"><mrow id="S3.SS1.p4.1.m1.1.2.2.2" xref="S3.SS1.p4.1.m1.1.2.2.1.cmml"><mo stretchy="false" id="S3.SS1.p4.1.m1.1.2.2.2.1" xref="S3.SS1.p4.1.m1.1.2.2.1.1.cmml">â€–</mo><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">ğ‘¾</mi><mo stretchy="false" id="S3.SS1.p4.1.m1.1.2.2.2.2" xref="S3.SS1.p4.1.m1.1.2.2.1.1.cmml">â€–</mo></mrow><mn id="S3.SS1.p4.1.m1.1.2.3" xref="S3.SS1.p4.1.m1.1.2.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.2.1.cmml" xref="S3.SS1.p4.1.m1.1.2">subscript</csymbol><apply id="S3.SS1.p4.1.m1.1.2.2.1.cmml" xref="S3.SS1.p4.1.m1.1.2.2.2"><csymbol cd="latexml" id="S3.SS1.p4.1.m1.1.2.2.1.1.cmml" xref="S3.SS1.p4.1.m1.1.2.2.2.1">norm</csymbol><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ğ‘¾</ci></apply><cn type="integer" id="S3.SS1.p4.1.m1.1.2.3.cmml" xref="S3.SS1.p4.1.m1.1.2.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">||{\bm{W}}||_{2}</annotation></semantics></math>, otherwise the aggregated model would be dominated by a small part of local models with larger weight norms.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Adaptive-balancing on norm of weights</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The scale invariant property inherited in the batch normalization and layer normalization is able to alleviate the obliteration problem in aggregation caused by external covariate shift. In Section <a href="#S2.SS2" title="2.2 Scale invariant property of normalization â€£ 2 Preliminaries â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, we demonstrate that the batch and layer normalization can automatically tune the learning rate of the weights of the previous layer. Consequently, the norm of weights will reach certain equilibrium and this process is regardless of the input distribution. Since in federated learning, we start training with the same initialization, the scale invariant property provides some guarantee that the norm of weights across different devices will not diverge far away from initialization. Formally, we iteratively apply Eq.Â <a href="#S2.E5" title="In 2.2 Scale invariant property of normalization â€£ 2 Preliminaries â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we can see that</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.2" class="ltx_Math" alttext="\lVert{\bm{W}}_{t+1}\rVert_{2}=\lVert{\bm{W}}_{0}\rVert_{2}+\eta^{2}\sum_{i=0}^{t}\lVert\frac{\partial f({\bm{W}}_{i})}{\partial{\bm{W}}_{i}}\rVert_{2}," display="block"><semantics id="S3.E7.m1.2a"><mrow id="S3.E7.m1.2.2.1" xref="S3.E7.m1.2.2.1.1.cmml"><mrow id="S3.E7.m1.2.2.1.1" xref="S3.E7.m1.2.2.1.1.cmml"><msub id="S3.E7.m1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.1.cmml"><mrow id="S3.E7.m1.2.2.1.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.E7.m1.2.2.1.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.2.1.cmml">âˆ¥</mo><msub id="S3.E7.m1.2.2.1.1.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.2.2.1.1.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mrow id="S3.E7.m1.2.2.1.1.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.E7.m1.2.2.1.1.1.1.1.1.3.1" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3.1.cmml">+</mo><mn id="S3.E7.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo fence="true" lspace="0em" id="S3.E7.m1.2.2.1.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S3.E7.m1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.3.cmml">2</mn></msub><mo rspace="0.1389em" id="S3.E7.m1.2.2.1.1.3" xref="S3.E7.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E7.m1.2.2.1.1.2" xref="S3.E7.m1.2.2.1.1.2.cmml"><msub id="S3.E7.m1.2.2.1.1.2.1" xref="S3.E7.m1.2.2.1.1.2.1.cmml"><mrow id="S3.E7.m1.2.2.1.1.2.1.1.1" xref="S3.E7.m1.2.2.1.1.2.1.1.2.cmml"><mo fence="true" lspace="0.1389em" rspace="0em" id="S3.E7.m1.2.2.1.1.2.1.1.1.2" xref="S3.E7.m1.2.2.1.1.2.1.1.2.1.cmml">âˆ¥</mo><msub id="S3.E7.m1.2.2.1.1.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.2.1.1.1.1.cmml"><mi id="S3.E7.m1.2.2.1.1.2.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.2.1.1.1.1.2.cmml">ğ‘¾</mi><mn id="S3.E7.m1.2.2.1.1.2.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.2.1.1.1.1.3.cmml">0</mn></msub><mo fence="true" lspace="0em" rspace="0em" id="S3.E7.m1.2.2.1.1.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.2.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S3.E7.m1.2.2.1.1.2.1.3" xref="S3.E7.m1.2.2.1.1.2.1.3.cmml">2</mn></msub><mo id="S3.E7.m1.2.2.1.1.2.2" xref="S3.E7.m1.2.2.1.1.2.2.cmml">+</mo><mrow id="S3.E7.m1.2.2.1.1.2.3" xref="S3.E7.m1.2.2.1.1.2.3.cmml"><msup id="S3.E7.m1.2.2.1.1.2.3.2" xref="S3.E7.m1.2.2.1.1.2.3.2.cmml"><mi id="S3.E7.m1.2.2.1.1.2.3.2.2" xref="S3.E7.m1.2.2.1.1.2.3.2.2.cmml">Î·</mi><mn id="S3.E7.m1.2.2.1.1.2.3.2.3" xref="S3.E7.m1.2.2.1.1.2.3.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.E7.m1.2.2.1.1.2.3.1" xref="S3.E7.m1.2.2.1.1.2.3.1.cmml">â€‹</mo><mrow id="S3.E7.m1.2.2.1.1.2.3.3" xref="S3.E7.m1.2.2.1.1.2.3.3.cmml"><munderover id="S3.E7.m1.2.2.1.1.2.3.3.1" xref="S3.E7.m1.2.2.1.1.2.3.3.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E7.m1.2.2.1.1.2.3.3.1.2.2" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.cmml"><mi id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.2" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.2.cmml">i</mi><mo id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.1" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.3" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.3.cmml">0</mn></mrow><mi id="S3.E7.m1.2.2.1.1.2.3.3.1.3" xref="S3.E7.m1.2.2.1.1.2.3.3.1.3.cmml">t</mi></munderover><msub id="S3.E7.m1.2.2.1.1.2.3.3.2" xref="S3.E7.m1.2.2.1.1.2.3.3.2.cmml"><mrow id="S3.E7.m1.2.2.1.1.2.3.3.2.2.2" xref="S3.E7.m1.2.2.1.1.2.3.3.2.2.1.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S3.E7.m1.2.2.1.1.2.3.3.2.2.2.1" xref="S3.E7.m1.2.2.1.1.2.3.3.2.2.1.1.cmml">âˆ¥</mo><mfrac id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml"><mo rspace="0em" id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.2.cmml">âˆ‚</mo><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E7.m1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E7.m1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.2.cmml">ğ‘¾</mi><mi id="S3.E7.m1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mo rspace="0em" id="S3.E7.m1.1.1.3.1" xref="S3.E7.m1.1.1.3.1.cmml">âˆ‚</mo><msub id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><mi id="S3.E7.m1.1.1.3.2.2" xref="S3.E7.m1.1.1.3.2.2.cmml">ğ‘¾</mi><mi id="S3.E7.m1.1.1.3.2.3" xref="S3.E7.m1.1.1.3.2.3.cmml">i</mi></msub></mrow></mfrac><mo fence="true" lspace="0em" rspace="0em" id="S3.E7.m1.2.2.1.1.2.3.3.2.2.2.2" xref="S3.E7.m1.2.2.1.1.2.3.3.2.2.1.1.cmml">âˆ¥</mo></mrow><mn id="S3.E7.m1.2.2.1.1.2.3.3.2.3" xref="S3.E7.m1.2.2.1.1.2.3.3.2.3.cmml">2</mn></msub></mrow></mrow></mrow></mrow><mo id="S3.E7.m1.2.2.1.2" xref="S3.E7.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.2b"><apply id="S3.E7.m1.2.2.1.1.cmml" xref="S3.E7.m1.2.2.1"><eq id="S3.E7.m1.2.2.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.3"></eq><apply id="S3.E7.m1.2.2.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1">subscript</csymbol><apply id="S3.E7.m1.2.2.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.E7.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.1.2">ğ‘¾</ci><apply id="S3.E7.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3"><plus id="S3.E7.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3.1"></plus><ci id="S3.E7.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.E7.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply><cn type="integer" id="S3.E7.m1.2.2.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.3">2</cn></apply><apply id="S3.E7.m1.2.2.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.2"><plus id="S3.E7.m1.2.2.1.1.2.2.cmml" xref="S3.E7.m1.2.2.1.1.2.2"></plus><apply id="S3.E7.m1.2.2.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.2.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.2.1.2.cmml" xref="S3.E7.m1.2.2.1.1.2.1">subscript</csymbol><apply id="S3.E7.m1.2.2.1.1.2.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.2.2.1.1.2.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.2.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.E7.m1.2.2.1.1.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.2.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.2.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.2.2.1.1.2.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.2.1.1.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.E7.m1.2.2.1.1.2.1.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.2.1.1.1.1.3">0</cn></apply></apply><cn type="integer" id="S3.E7.m1.2.2.1.1.2.1.3.cmml" xref="S3.E7.m1.2.2.1.1.2.1.3">2</cn></apply><apply id="S3.E7.m1.2.2.1.1.2.3.cmml" xref="S3.E7.m1.2.2.1.1.2.3"><times id="S3.E7.m1.2.2.1.1.2.3.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.1"></times><apply id="S3.E7.m1.2.2.1.1.2.3.2.cmml" xref="S3.E7.m1.2.2.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.2.3.2.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.2">superscript</csymbol><ci id="S3.E7.m1.2.2.1.1.2.3.2.2.cmml" xref="S3.E7.m1.2.2.1.1.2.3.2.2">ğœ‚</ci><cn type="integer" id="S3.E7.m1.2.2.1.1.2.3.2.3.cmml" xref="S3.E7.m1.2.2.1.1.2.3.2.3">2</cn></apply><apply id="S3.E7.m1.2.2.1.1.2.3.3.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3"><apply id="S3.E7.m1.2.2.1.1.2.3.3.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.2.3.3.1.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1">superscript</csymbol><apply id="S3.E7.m1.2.2.1.1.2.3.3.1.2.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.2.3.3.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1">subscript</csymbol><sum id="S3.E7.m1.2.2.1.1.2.3.3.1.2.2.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.2"></sum><apply id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3"><eq id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.1"></eq><ci id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.2.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.3.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1.2.3.3">0</cn></apply></apply><ci id="S3.E7.m1.2.2.1.1.2.3.3.1.3.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.1.3">ğ‘¡</ci></apply><apply id="S3.E7.m1.2.2.1.1.2.3.3.2.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.2.3.3.2.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.2">subscript</csymbol><apply id="S3.E7.m1.2.2.1.1.2.3.3.2.2.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.2.2.2"><csymbol cd="latexml" id="S3.E7.m1.2.2.1.1.2.3.3.2.2.1.1.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.2.2.2.1">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><divide id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1"></divide><apply id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><partialdiff id="S3.E7.m1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.2"></partialdiff><apply id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.2"></times><ci id="S3.E7.m1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.3">ğ‘“</ci><apply id="S3.E7.m1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.2">ğ‘¾</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><partialdiff id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3.1"></partialdiff><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2">ğ‘¾</ci><ci id="S3.E7.m1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.3.2.3">ğ‘–</ci></apply></apply></apply></apply><cn type="integer" id="S3.E7.m1.2.2.1.1.2.3.3.2.3.cmml" xref="S3.E7.m1.2.2.1.1.2.3.3.2.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.2c">\lVert{\bm{W}}_{t+1}\rVert_{2}=\lVert{\bm{W}}_{0}\rVert_{2}+\eta^{2}\sum_{i=0}^{t}\lVert\frac{\partial f({\bm{W}}_{i})}{\partial{\bm{W}}_{i}}\rVert_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.1" class="ltx_p">where <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="{\bm{W}}_{0}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ğ‘¾</mi><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ‘¾</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">{\bm{W}}_{0}</annotation></semantics></math> is the initialization. Combined with Eq.Â <a href="#S2.E3" title="In 2.2 Scale invariant property of normalization â€£ 2 Preliminaries â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we have the norm of gradient will be similar since otherwise the auto tuning property will automatically magnify or reduce the norm of weights. The adaptive-balancing property makes the normalization module indispensable in Federated Learning. However, the batch normalization has some issues in the distributed training scenario.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Failure of batch normalization</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Batch normalization has been the most commonly used normalization layer in deep neural networks that satisfies the scale invariant property. However, recent studiesÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Hsieh etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> have shown that applying batch normalization to FL incurs accuracy drop. Though previous arts have raised the problem, a convincing explanation is lacking, where the previous work simply blame the data heterogeneity. Although different data samples will generate totally different features, it is still reasonable that the whole channel of outputs has similar statistics. This is the foundational assumption of batch normalization, because it indeed normalizes the output of different samples by applying the same statistics in a given channel. In particular, for two devices holding data sampled from different distributions, the neurons in their models may follow totally different distributions, but the statistics of the given channel is not necessarily different between two devices. In this case, batch normalization can still be applied to effectively address the internal covariate shift in FL.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">Therefore, we explain the phenomena by lens of external covariate shift. Due to external covariate shift, model neurons have to adapt to the new input distributions after aggregation. This is caused by the different input distributions of the corresponding neurons on other devices, which we call <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">external neurons</span>. After applying batch normalization, different devices will have varying running statistics, and the central server can not obtain correct running statistics by simply averaging local statistics. Wrongly obtained batch normalization statistics that mismatch feature statistics will lead to information loss or introduces extra noise to the featuresÂ <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> especially after activation functions. Therefore, based on our observation, we identify the key reason that why batch normalization causes accuracy drop in FL is that the statistics of the same channel are trained to be different between devices during local training.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">We verify our observation through a toy experiment. The histograms of output from different channels on two devices are shown in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.3 Failure of batch normalization â€£ 3 External Covariate Shift and Adaptive-Balancing of Weights â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> on experiments on MNIST. In particular, we train two identical models from the same initialization with two local training dataset to simulate two devices in a federated learning round. For simplicity, we apply a base model with 3 convolutional layers followed by batch normalization layers respectively, and for each layer there is only one channel. To avoid the influence of non-IID data, the two local training dataset are the same except having different mean values, which can be easily normalized by batch normalization.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div id="S3.F3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:231.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-142.6pt,79.8pt) scale(0.590909047132776,0.590909047132776) ;"><img src="/html/2210.03277/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="741" height="416" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Histograms of output before and after BatchNorm in different channels on two devices.</figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">Note that the output after batch normalization (the red curve) is the input of the succeeding layer. The results show that two devices have totally different feature statistics for the same channel (layers) with training from the same initialized model.
For the second and third layer, the input distributions are normalized to similar statistics (the red curves in layer1 and layer2 figures), but the output channels (the black curves in layer2 and layer3 figures) still show a significant statistical discrepancy.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>CNN Model architecture.</figcaption>
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">5<math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><times id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\times</annotation></semantics></math> 5 Conv 3-6</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.3.1" class="ltx_tr">
<td id="S3.T1.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t">NormLayer</td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<td id="S3.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">5<math id="S3.T1.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.2.2.1.m1.1a"><mo id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><times id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">\times</annotation></semantics></math> 5 Conv 6-16</td>
</tr>
<tr id="S3.T1.2.4.2" class="ltx_tr">
<td id="S3.T1.2.4.2.1" class="ltx_td ltx_align_center ltx_border_t">NormLayer</td>
</tr>
<tr id="S3.T1.2.5.3" class="ltx_tr">
<td id="S3.T1.2.5.3.1" class="ltx_td ltx_align_center ltx_border_t">FC-120</td>
</tr>
<tr id="S3.T1.2.6.4" class="ltx_tr">
<td id="S3.T1.2.6.4.1" class="ltx_td ltx_align_center ltx_border_t">NormLayer</td>
</tr>
<tr id="S3.T1.2.7.5" class="ltx_tr">
<td id="S3.T1.2.7.5.1" class="ltx_td ltx_align_center ltx_border_t">FC-84</td>
</tr>
<tr id="S3.T1.2.8.6" class="ltx_tr">
<td id="S3.T1.2.8.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">FC-10</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Success of layer normalization</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">Given the failure of batch normalization, layer normalization becomes the good choice of the normalization layer in Federated Learning. Since layer normalization normalizes across channel in a sample-wise manner, it does not suffer from the statistics discrepancy across devices. Further, layer normalization possesses all the advantages discussed in SectionÂ <a href="#S2" title="2 Preliminaries â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In next section, we verify our analysis through experiments and we show that layer normalization is the best among all baselines.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In our experiments, we utilize FedAVGÂ <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> as the algorithm and apply different normalization methods on different architectures, i.e., VGG-11, ResNet-18 and a simple CNN. Each experiment is run with three different random seeds and standard deviation is reported. Experiments are conducted on a server with two Intel Xeon E5-2687W CPUs and four Nvidia TITAN RTX GPUs.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We use CIFAR10 dataset and construct non-IID dataset by following the configurations inÂ <cite class="ltx_cite ltx_citemacro_cite">McMahan etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>. The data is distributed across 100 devices. Each device holds 2 random classes with 100 samples per class.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Hyperparameter configurations.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px2.p1.3" class="ltx_p">We set local epoch <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">E</annotation></semantics></math> as 10 and batch size <math id="S4.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mi id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">B</annotation></semantics></math> as 64. We apply SGD optimizer and set the learning rate <math id="S4.SS1.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.3.m3.1a"><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.3.m3.1b"><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.3.m3.1c">\eta</annotation></semantics></math> to 0.01. In each communication round, there are 10 randomly sampled devices participate in the training. The architecture of CNN model is presented in Table<a href="#S3.T1" title="Table 1 â€£ 3.3 Failure of batch normalization â€£ 3 External Covariate Shift and Adaptive-Balancing of Weights â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. By default, we perform training with 5000 communication rounds.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">We compare the performance of different normalization methods, group normalization, layer normalization, instance normalization and batch normalization. Further, we run experiments on fixed batch normalization where the training statistics are not aggregated in the aggregation step. We also provide results with no normalization as baselines.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Results</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">The comparison of convergence speed is shown in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and the accuracy upon convergence is shown in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ Rethinking Normalization Methods in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The results show that layer normalization converges the fastest and to the highest accuracy or comparable to the highest accuracy for all three architectures. Note that, in all experiments, group normalization achieves similar results as layer normalization since these two methods are very similar. For VGG-11, the layer normalization achieves 11.96% and 27.01% improvements compared with batch normalization and no normalization, respectively. For ResNet and CNN, layer normalization also achieves remarkable improvements compared with batch normalization. Note that, for ResNet-18, the no normalization achieves the best result which is slightly higher than layer normalization, we account this for the residual connection. Since the residual connection, the input to the next block is not too far away from the input to the last block, which prevents the weights deviating from the initialization. However, the hyper-parameters for ResNet with no normalization must be selected carefully to obtain a stable training process.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy on CIFAR10 with different normalization methods on different architectures. Standard deviation is computed with regard to three different seeds.</figcaption>
<table id="S4.T2.20" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.3.1" class="ltx_text">Architecture</span></td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.4.1" class="ltx_text">Method</span></td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">Accuracy(<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\%</annotation></semantics></math>)Â (<math id="S4.T2.2.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.m2.1a"><mo stretchy="false" id="S4.T2.2.2.2.m2.1.1" xref="S4.T2.2.2.2.m2.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m2.1b"><ci id="S4.T2.2.2.2.m2.1.1.cmml" xref="S4.T2.2.2.2.m2.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m2.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr id="S4.T2.20.21.1" class="ltx_tr">
<td id="S4.T2.20.21.1.1" class="ltx_td ltx_align_center">@5000 rounds</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="6"><span id="S4.T2.3.3.2.1" class="ltx_text">VGG-11</span></td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_left ltx_border_tt">No normalization</td>
<td id="S4.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_tt"><math id="S4.T2.3.3.1.m1.1" class="ltx_Math" alttext="36.51_{\pm 2.48}" display="inline"><semantics id="S4.T2.3.3.1.m1.1a"><msub id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml"><mn id="S4.T2.3.3.1.m1.1.1.2" xref="S4.T2.3.3.1.m1.1.1.2.cmml">36.51</mn><mrow id="S4.T2.3.3.1.m1.1.1.3" xref="S4.T2.3.3.1.m1.1.1.3.cmml"><mo id="S4.T2.3.3.1.m1.1.1.3a" xref="S4.T2.3.3.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.3.3.1.m1.1.1.3.2" xref="S4.T2.3.3.1.m1.1.1.3.2.cmml">2.48</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><apply id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.3.3.1.m1.1.1.2.cmml" xref="S4.T2.3.3.1.m1.1.1.2">36.51</cn><apply id="S4.T2.3.3.1.m1.1.1.3.cmml" xref="S4.T2.3.3.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.3.3.1.m1.1.1.3.1.cmml" xref="S4.T2.3.3.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.3.3.1.m1.1.1.3.2.cmml" xref="S4.T2.3.3.1.m1.1.1.3.2">2.48</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">36.51_{\pm 2.48}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.4.4" class="ltx_tr">
<td id="S4.T2.4.4.2" class="ltx_td ltx_align_left">Group-Normalization</td>
<td id="S4.T2.4.4.1" class="ltx_td ltx_align_center"><math id="S4.T2.4.4.1.m1.1" class="ltx_Math" alttext="63.10_{\pm 1.68}" display="inline"><semantics id="S4.T2.4.4.1.m1.1a"><msub id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml"><mn id="S4.T2.4.4.1.m1.1.1.2" xref="S4.T2.4.4.1.m1.1.1.2.cmml">63.10</mn><mrow id="S4.T2.4.4.1.m1.1.1.3" xref="S4.T2.4.4.1.m1.1.1.3.cmml"><mo id="S4.T2.4.4.1.m1.1.1.3a" xref="S4.T2.4.4.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.4.4.1.m1.1.1.3.2" xref="S4.T2.4.4.1.m1.1.1.3.2.cmml">1.68</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><apply id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.4.4.1.m1.1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.4.4.1.m1.1.1.2.cmml" xref="S4.T2.4.4.1.m1.1.1.2">63.10</cn><apply id="S4.T2.4.4.1.m1.1.1.3.cmml" xref="S4.T2.4.4.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.4.4.1.m1.1.1.3.1.cmml" xref="S4.T2.4.4.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.4.4.1.m1.1.1.3.2.cmml" xref="S4.T2.4.4.1.m1.1.1.3.2">1.68</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">63.10_{\pm 1.68}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.5.5" class="ltx_tr">
<td id="S4.T2.5.5.2" class="ltx_td ltx_align_left">Layer-Normalization</td>
<td id="S4.T2.5.5.1" class="ltx_td ltx_align_center"><math id="S4.T2.5.5.1.m1.1" class="ltx_Math" alttext="63.52_{\pm 1.44}" display="inline"><semantics id="S4.T2.5.5.1.m1.1a"><msub id="S4.T2.5.5.1.m1.1.1" xref="S4.T2.5.5.1.m1.1.1.cmml"><mn id="S4.T2.5.5.1.m1.1.1.2" xref="S4.T2.5.5.1.m1.1.1.2.cmml">63.52</mn><mrow id="S4.T2.5.5.1.m1.1.1.3" xref="S4.T2.5.5.1.m1.1.1.3.cmml"><mo id="S4.T2.5.5.1.m1.1.1.3a" xref="S4.T2.5.5.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.5.5.1.m1.1.1.3.2" xref="S4.T2.5.5.1.m1.1.1.3.2.cmml">1.44</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.m1.1b"><apply id="S4.T2.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.5.5.1.m1.1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.5.5.1.m1.1.1.2.cmml" xref="S4.T2.5.5.1.m1.1.1.2">63.52</cn><apply id="S4.T2.5.5.1.m1.1.1.3.cmml" xref="S4.T2.5.5.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.5.5.1.m1.1.1.3.1.cmml" xref="S4.T2.5.5.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.5.5.1.m1.1.1.3.2.cmml" xref="S4.T2.5.5.1.m1.1.1.3.2">1.44</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.m1.1c">63.52_{\pm 1.44}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<td id="S4.T2.6.6.2" class="ltx_td ltx_align_left">Instance-Normalization</td>
<td id="S4.T2.6.6.1" class="ltx_td ltx_align_center"><math id="S4.T2.6.6.1.m1.1" class="ltx_Math" alttext="53.46_{\pm 0.34}" display="inline"><semantics id="S4.T2.6.6.1.m1.1a"><msub id="S4.T2.6.6.1.m1.1.1" xref="S4.T2.6.6.1.m1.1.1.cmml"><mn id="S4.T2.6.6.1.m1.1.1.2" xref="S4.T2.6.6.1.m1.1.1.2.cmml">53.46</mn><mrow id="S4.T2.6.6.1.m1.1.1.3" xref="S4.T2.6.6.1.m1.1.1.3.cmml"><mo id="S4.T2.6.6.1.m1.1.1.3a" xref="S4.T2.6.6.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.6.6.1.m1.1.1.3.2" xref="S4.T2.6.6.1.m1.1.1.3.2.cmml">0.34</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.1.m1.1b"><apply id="S4.T2.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.6.6.1.m1.1.1.1.cmml" xref="S4.T2.6.6.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.6.6.1.m1.1.1.2.cmml" xref="S4.T2.6.6.1.m1.1.1.2">53.46</cn><apply id="S4.T2.6.6.1.m1.1.1.3.cmml" xref="S4.T2.6.6.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.6.6.1.m1.1.1.3.1.cmml" xref="S4.T2.6.6.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.6.6.1.m1.1.1.3.2.cmml" xref="S4.T2.6.6.1.m1.1.1.3.2">0.34</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.1.m1.1c">53.46_{\pm 0.34}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.7.7" class="ltx_tr">
<td id="S4.T2.7.7.2" class="ltx_td ltx_align_left">Batch-Normalization</td>
<td id="S4.T2.7.7.1" class="ltx_td ltx_align_center"><math id="S4.T2.7.7.1.m1.1" class="ltx_Math" alttext="51.56_{\pm 2.22}" display="inline"><semantics id="S4.T2.7.7.1.m1.1a"><msub id="S4.T2.7.7.1.m1.1.1" xref="S4.T2.7.7.1.m1.1.1.cmml"><mn id="S4.T2.7.7.1.m1.1.1.2" xref="S4.T2.7.7.1.m1.1.1.2.cmml">51.56</mn><mrow id="S4.T2.7.7.1.m1.1.1.3" xref="S4.T2.7.7.1.m1.1.1.3.cmml"><mo id="S4.T2.7.7.1.m1.1.1.3a" xref="S4.T2.7.7.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.7.7.1.m1.1.1.3.2" xref="S4.T2.7.7.1.m1.1.1.3.2.cmml">2.22</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.1.m1.1b"><apply id="S4.T2.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.7.7.1.m1.1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.7.7.1.m1.1.1.2.cmml" xref="S4.T2.7.7.1.m1.1.1.2">51.56</cn><apply id="S4.T2.7.7.1.m1.1.1.3.cmml" xref="S4.T2.7.7.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.7.7.1.m1.1.1.3.1.cmml" xref="S4.T2.7.7.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.7.7.1.m1.1.1.3.2.cmml" xref="S4.T2.7.7.1.m1.1.1.3.2">2.22</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.1.m1.1c">51.56_{\pm 2.22}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.8.8" class="ltx_tr">
<td id="S4.T2.8.8.2" class="ltx_td ltx_align_left">Fixed Batch-Normalization</td>
<td id="S4.T2.8.8.1" class="ltx_td ltx_align_center"><math id="S4.T2.8.8.1.m1.1" class="ltx_Math" alttext="50.93_{\pm 3.07}" display="inline"><semantics id="S4.T2.8.8.1.m1.1a"><msub id="S4.T2.8.8.1.m1.1.1" xref="S4.T2.8.8.1.m1.1.1.cmml"><mn id="S4.T2.8.8.1.m1.1.1.2" xref="S4.T2.8.8.1.m1.1.1.2.cmml">50.93</mn><mrow id="S4.T2.8.8.1.m1.1.1.3" xref="S4.T2.8.8.1.m1.1.1.3.cmml"><mo id="S4.T2.8.8.1.m1.1.1.3a" xref="S4.T2.8.8.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.8.8.1.m1.1.1.3.2" xref="S4.T2.8.8.1.m1.1.1.3.2.cmml">3.07</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.1.m1.1b"><apply id="S4.T2.8.8.1.m1.1.1.cmml" xref="S4.T2.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.8.8.1.m1.1.1.1.cmml" xref="S4.T2.8.8.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.8.8.1.m1.1.1.2.cmml" xref="S4.T2.8.8.1.m1.1.1.2">50.93</cn><apply id="S4.T2.8.8.1.m1.1.1.3.cmml" xref="S4.T2.8.8.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.8.8.1.m1.1.1.3.1.cmml" xref="S4.T2.8.8.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.8.8.1.m1.1.1.3.2.cmml" xref="S4.T2.8.8.1.m1.1.1.3.2">3.07</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.1.m1.1c">50.93_{\pm 3.07}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.9.9" class="ltx_tr">
<td id="S4.T2.9.9.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="6"><span id="S4.T2.9.9.2.1" class="ltx_text">ResNet-18</span></td>
<td id="S4.T2.9.9.3" class="ltx_td ltx_align_left ltx_border_tt">No normalization</td>
<td id="S4.T2.9.9.1" class="ltx_td ltx_align_center ltx_border_tt"><math id="S4.T2.9.9.1.m1.1" class="ltx_Math" alttext="60.73_{\pm 0.53}" display="inline"><semantics id="S4.T2.9.9.1.m1.1a"><msub id="S4.T2.9.9.1.m1.1.1" xref="S4.T2.9.9.1.m1.1.1.cmml"><mn id="S4.T2.9.9.1.m1.1.1.2" xref="S4.T2.9.9.1.m1.1.1.2.cmml">60.73</mn><mrow id="S4.T2.9.9.1.m1.1.1.3" xref="S4.T2.9.9.1.m1.1.1.3.cmml"><mo id="S4.T2.9.9.1.m1.1.1.3a" xref="S4.T2.9.9.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.9.9.1.m1.1.1.3.2" xref="S4.T2.9.9.1.m1.1.1.3.2.cmml">0.53</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.1.m1.1b"><apply id="S4.T2.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.9.9.1.m1.1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.9.9.1.m1.1.1.2.cmml" xref="S4.T2.9.9.1.m1.1.1.2">60.73</cn><apply id="S4.T2.9.9.1.m1.1.1.3.cmml" xref="S4.T2.9.9.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.9.9.1.m1.1.1.3.1.cmml" xref="S4.T2.9.9.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.9.9.1.m1.1.1.3.2.cmml" xref="S4.T2.9.9.1.m1.1.1.3.2">0.53</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.1.m1.1c">60.73_{\pm 0.53}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.10.10" class="ltx_tr">
<td id="S4.T2.10.10.2" class="ltx_td ltx_align_left">Group-Normalization</td>
<td id="S4.T2.10.10.1" class="ltx_td ltx_align_center"><math id="S4.T2.10.10.1.m1.1" class="ltx_Math" alttext="59.01_{\pm 0.44}" display="inline"><semantics id="S4.T2.10.10.1.m1.1a"><msub id="S4.T2.10.10.1.m1.1.1" xref="S4.T2.10.10.1.m1.1.1.cmml"><mn id="S4.T2.10.10.1.m1.1.1.2" xref="S4.T2.10.10.1.m1.1.1.2.cmml">59.01</mn><mrow id="S4.T2.10.10.1.m1.1.1.3" xref="S4.T2.10.10.1.m1.1.1.3.cmml"><mo id="S4.T2.10.10.1.m1.1.1.3a" xref="S4.T2.10.10.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.10.10.1.m1.1.1.3.2" xref="S4.T2.10.10.1.m1.1.1.3.2.cmml">0.44</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.1.m1.1b"><apply id="S4.T2.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.10.10.1.m1.1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.10.10.1.m1.1.1.2.cmml" xref="S4.T2.10.10.1.m1.1.1.2">59.01</cn><apply id="S4.T2.10.10.1.m1.1.1.3.cmml" xref="S4.T2.10.10.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.10.10.1.m1.1.1.3.1.cmml" xref="S4.T2.10.10.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.10.10.1.m1.1.1.3.2.cmml" xref="S4.T2.10.10.1.m1.1.1.3.2">0.44</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.1.m1.1c">59.01_{\pm 0.44}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.11.11" class="ltx_tr">
<td id="S4.T2.11.11.2" class="ltx_td ltx_align_left">Layer-Normalization</td>
<td id="S4.T2.11.11.1" class="ltx_td ltx_align_center"><math id="S4.T2.11.11.1.m1.1" class="ltx_Math" alttext="59.70_{\pm 0.21}" display="inline"><semantics id="S4.T2.11.11.1.m1.1a"><msub id="S4.T2.11.11.1.m1.1.1" xref="S4.T2.11.11.1.m1.1.1.cmml"><mn id="S4.T2.11.11.1.m1.1.1.2" xref="S4.T2.11.11.1.m1.1.1.2.cmml">59.70</mn><mrow id="S4.T2.11.11.1.m1.1.1.3" xref="S4.T2.11.11.1.m1.1.1.3.cmml"><mo id="S4.T2.11.11.1.m1.1.1.3a" xref="S4.T2.11.11.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.11.11.1.m1.1.1.3.2" xref="S4.T2.11.11.1.m1.1.1.3.2.cmml">0.21</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.1.m1.1b"><apply id="S4.T2.11.11.1.m1.1.1.cmml" xref="S4.T2.11.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.11.11.1.m1.1.1.1.cmml" xref="S4.T2.11.11.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.11.11.1.m1.1.1.2.cmml" xref="S4.T2.11.11.1.m1.1.1.2">59.70</cn><apply id="S4.T2.11.11.1.m1.1.1.3.cmml" xref="S4.T2.11.11.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.11.11.1.m1.1.1.3.1.cmml" xref="S4.T2.11.11.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.11.11.1.m1.1.1.3.2.cmml" xref="S4.T2.11.11.1.m1.1.1.3.2">0.21</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.1.m1.1c">59.70_{\pm 0.21}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.12.12" class="ltx_tr">
<td id="S4.T2.12.12.2" class="ltx_td ltx_align_left">Instance-Normalization</td>
<td id="S4.T2.12.12.1" class="ltx_td ltx_align_center"><math id="S4.T2.12.12.1.m1.1" class="ltx_Math" alttext="51.78_{\pm 0.47}" display="inline"><semantics id="S4.T2.12.12.1.m1.1a"><msub id="S4.T2.12.12.1.m1.1.1" xref="S4.T2.12.12.1.m1.1.1.cmml"><mn id="S4.T2.12.12.1.m1.1.1.2" xref="S4.T2.12.12.1.m1.1.1.2.cmml">51.78</mn><mrow id="S4.T2.12.12.1.m1.1.1.3" xref="S4.T2.12.12.1.m1.1.1.3.cmml"><mo id="S4.T2.12.12.1.m1.1.1.3a" xref="S4.T2.12.12.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.12.12.1.m1.1.1.3.2" xref="S4.T2.12.12.1.m1.1.1.3.2.cmml">0.47</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.1.m1.1b"><apply id="S4.T2.12.12.1.m1.1.1.cmml" xref="S4.T2.12.12.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.12.12.1.m1.1.1.1.cmml" xref="S4.T2.12.12.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.12.12.1.m1.1.1.2.cmml" xref="S4.T2.12.12.1.m1.1.1.2">51.78</cn><apply id="S4.T2.12.12.1.m1.1.1.3.cmml" xref="S4.T2.12.12.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.12.12.1.m1.1.1.3.1.cmml" xref="S4.T2.12.12.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.12.12.1.m1.1.1.3.2.cmml" xref="S4.T2.12.12.1.m1.1.1.3.2">0.47</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.1.m1.1c">51.78_{\pm 0.47}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.13.13" class="ltx_tr">
<td id="S4.T2.13.13.2" class="ltx_td ltx_align_left">Batch-Normalization</td>
<td id="S4.T2.13.13.1" class="ltx_td ltx_align_center"><math id="S4.T2.13.13.1.m1.1" class="ltx_Math" alttext="34.38_{\pm 2.07}" display="inline"><semantics id="S4.T2.13.13.1.m1.1a"><msub id="S4.T2.13.13.1.m1.1.1" xref="S4.T2.13.13.1.m1.1.1.cmml"><mn id="S4.T2.13.13.1.m1.1.1.2" xref="S4.T2.13.13.1.m1.1.1.2.cmml">34.38</mn><mrow id="S4.T2.13.13.1.m1.1.1.3" xref="S4.T2.13.13.1.m1.1.1.3.cmml"><mo id="S4.T2.13.13.1.m1.1.1.3a" xref="S4.T2.13.13.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.13.13.1.m1.1.1.3.2" xref="S4.T2.13.13.1.m1.1.1.3.2.cmml">2.07</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.1.m1.1b"><apply id="S4.T2.13.13.1.m1.1.1.cmml" xref="S4.T2.13.13.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.13.13.1.m1.1.1.1.cmml" xref="S4.T2.13.13.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.13.13.1.m1.1.1.2.cmml" xref="S4.T2.13.13.1.m1.1.1.2">34.38</cn><apply id="S4.T2.13.13.1.m1.1.1.3.cmml" xref="S4.T2.13.13.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.13.13.1.m1.1.1.3.1.cmml" xref="S4.T2.13.13.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.13.13.1.m1.1.1.3.2.cmml" xref="S4.T2.13.13.1.m1.1.1.3.2">2.07</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.1.m1.1c">34.38_{\pm 2.07}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.14.14" class="ltx_tr">
<td id="S4.T2.14.14.2" class="ltx_td ltx_align_left">Fixed Batch-Normalization</td>
<td id="S4.T2.14.14.1" class="ltx_td ltx_align_center"><math id="S4.T2.14.14.1.m1.1" class="ltx_Math" alttext="33.85_{\pm 1.25}" display="inline"><semantics id="S4.T2.14.14.1.m1.1a"><msub id="S4.T2.14.14.1.m1.1.1" xref="S4.T2.14.14.1.m1.1.1.cmml"><mn id="S4.T2.14.14.1.m1.1.1.2" xref="S4.T2.14.14.1.m1.1.1.2.cmml">33.85</mn><mrow id="S4.T2.14.14.1.m1.1.1.3" xref="S4.T2.14.14.1.m1.1.1.3.cmml"><mo id="S4.T2.14.14.1.m1.1.1.3a" xref="S4.T2.14.14.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.14.14.1.m1.1.1.3.2" xref="S4.T2.14.14.1.m1.1.1.3.2.cmml">1.25</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.1.m1.1b"><apply id="S4.T2.14.14.1.m1.1.1.cmml" xref="S4.T2.14.14.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.14.14.1.m1.1.1.1.cmml" xref="S4.T2.14.14.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.14.14.1.m1.1.1.2.cmml" xref="S4.T2.14.14.1.m1.1.1.2">33.85</cn><apply id="S4.T2.14.14.1.m1.1.1.3.cmml" xref="S4.T2.14.14.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.14.14.1.m1.1.1.3.1.cmml" xref="S4.T2.14.14.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.14.14.1.m1.1.1.3.2.cmml" xref="S4.T2.14.14.1.m1.1.1.3.2">1.25</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.1.m1.1c">33.85_{\pm 1.25}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.15.15" class="ltx_tr">
<td id="S4.T2.15.15.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" rowspan="6"><span id="S4.T2.15.15.2.1" class="ltx_text">CNN</span></td>
<td id="S4.T2.15.15.3" class="ltx_td ltx_align_left ltx_border_tt">No normalization</td>
<td id="S4.T2.15.15.1" class="ltx_td ltx_align_center ltx_border_tt"><math id="S4.T2.15.15.1.m1.1" class="ltx_Math" alttext="49.96_{\pm 1.80}" display="inline"><semantics id="S4.T2.15.15.1.m1.1a"><msub id="S4.T2.15.15.1.m1.1.1" xref="S4.T2.15.15.1.m1.1.1.cmml"><mn id="S4.T2.15.15.1.m1.1.1.2" xref="S4.T2.15.15.1.m1.1.1.2.cmml">49.96</mn><mrow id="S4.T2.15.15.1.m1.1.1.3" xref="S4.T2.15.15.1.m1.1.1.3.cmml"><mo id="S4.T2.15.15.1.m1.1.1.3a" xref="S4.T2.15.15.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.15.15.1.m1.1.1.3.2" xref="S4.T2.15.15.1.m1.1.1.3.2.cmml">1.80</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.1.m1.1b"><apply id="S4.T2.15.15.1.m1.1.1.cmml" xref="S4.T2.15.15.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.15.15.1.m1.1.1.1.cmml" xref="S4.T2.15.15.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.15.15.1.m1.1.1.2.cmml" xref="S4.T2.15.15.1.m1.1.1.2">49.96</cn><apply id="S4.T2.15.15.1.m1.1.1.3.cmml" xref="S4.T2.15.15.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.15.15.1.m1.1.1.3.1.cmml" xref="S4.T2.15.15.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.15.15.1.m1.1.1.3.2.cmml" xref="S4.T2.15.15.1.m1.1.1.3.2">1.80</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.1.m1.1c">49.96_{\pm 1.80}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.16.16" class="ltx_tr">
<td id="S4.T2.16.16.2" class="ltx_td ltx_align_left">Group-Normalization</td>
<td id="S4.T2.16.16.1" class="ltx_td ltx_align_center"><math id="S4.T2.16.16.1.m1.1" class="ltx_Math" alttext="50.86_{\pm 1.08}" display="inline"><semantics id="S4.T2.16.16.1.m1.1a"><msub id="S4.T2.16.16.1.m1.1.1" xref="S4.T2.16.16.1.m1.1.1.cmml"><mn id="S4.T2.16.16.1.m1.1.1.2" xref="S4.T2.16.16.1.m1.1.1.2.cmml">50.86</mn><mrow id="S4.T2.16.16.1.m1.1.1.3" xref="S4.T2.16.16.1.m1.1.1.3.cmml"><mo id="S4.T2.16.16.1.m1.1.1.3a" xref="S4.T2.16.16.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.16.16.1.m1.1.1.3.2" xref="S4.T2.16.16.1.m1.1.1.3.2.cmml">1.08</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.1.m1.1b"><apply id="S4.T2.16.16.1.m1.1.1.cmml" xref="S4.T2.16.16.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.16.16.1.m1.1.1.1.cmml" xref="S4.T2.16.16.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.16.16.1.m1.1.1.2.cmml" xref="S4.T2.16.16.1.m1.1.1.2">50.86</cn><apply id="S4.T2.16.16.1.m1.1.1.3.cmml" xref="S4.T2.16.16.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.16.16.1.m1.1.1.3.1.cmml" xref="S4.T2.16.16.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.16.16.1.m1.1.1.3.2.cmml" xref="S4.T2.16.16.1.m1.1.1.3.2">1.08</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.1.m1.1c">50.86_{\pm 1.08}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.17.17" class="ltx_tr">
<td id="S4.T2.17.17.2" class="ltx_td ltx_align_left">Layer-Normalization</td>
<td id="S4.T2.17.17.1" class="ltx_td ltx_align_center"><math id="S4.T2.17.17.1.m1.1" class="ltx_Math" alttext="52.02_{\pm 0.76}" display="inline"><semantics id="S4.T2.17.17.1.m1.1a"><msub id="S4.T2.17.17.1.m1.1.1" xref="S4.T2.17.17.1.m1.1.1.cmml"><mn id="S4.T2.17.17.1.m1.1.1.2" xref="S4.T2.17.17.1.m1.1.1.2.cmml">52.02</mn><mrow id="S4.T2.17.17.1.m1.1.1.3" xref="S4.T2.17.17.1.m1.1.1.3.cmml"><mo id="S4.T2.17.17.1.m1.1.1.3a" xref="S4.T2.17.17.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.17.17.1.m1.1.1.3.2" xref="S4.T2.17.17.1.m1.1.1.3.2.cmml">0.76</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.1.m1.1b"><apply id="S4.T2.17.17.1.m1.1.1.cmml" xref="S4.T2.17.17.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.17.17.1.m1.1.1.1.cmml" xref="S4.T2.17.17.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.17.17.1.m1.1.1.2.cmml" xref="S4.T2.17.17.1.m1.1.1.2">52.02</cn><apply id="S4.T2.17.17.1.m1.1.1.3.cmml" xref="S4.T2.17.17.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.17.17.1.m1.1.1.3.1.cmml" xref="S4.T2.17.17.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.17.17.1.m1.1.1.3.2.cmml" xref="S4.T2.17.17.1.m1.1.1.3.2">0.76</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.1.m1.1c">52.02_{\pm 0.76}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.18.18" class="ltx_tr">
<td id="S4.T2.18.18.2" class="ltx_td ltx_align_left">Instance-Normalization</td>
<td id="S4.T2.18.18.1" class="ltx_td ltx_align_center"><math id="S4.T2.18.18.1.m1.1" class="ltx_Math" alttext="48.18_{\pm 1.17}" display="inline"><semantics id="S4.T2.18.18.1.m1.1a"><msub id="S4.T2.18.18.1.m1.1.1" xref="S4.T2.18.18.1.m1.1.1.cmml"><mn id="S4.T2.18.18.1.m1.1.1.2" xref="S4.T2.18.18.1.m1.1.1.2.cmml">48.18</mn><mrow id="S4.T2.18.18.1.m1.1.1.3" xref="S4.T2.18.18.1.m1.1.1.3.cmml"><mo id="S4.T2.18.18.1.m1.1.1.3a" xref="S4.T2.18.18.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.18.18.1.m1.1.1.3.2" xref="S4.T2.18.18.1.m1.1.1.3.2.cmml">1.17</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.1.m1.1b"><apply id="S4.T2.18.18.1.m1.1.1.cmml" xref="S4.T2.18.18.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.18.18.1.m1.1.1.1.cmml" xref="S4.T2.18.18.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.18.18.1.m1.1.1.2.cmml" xref="S4.T2.18.18.1.m1.1.1.2">48.18</cn><apply id="S4.T2.18.18.1.m1.1.1.3.cmml" xref="S4.T2.18.18.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.18.18.1.m1.1.1.3.1.cmml" xref="S4.T2.18.18.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.18.18.1.m1.1.1.3.2.cmml" xref="S4.T2.18.18.1.m1.1.1.3.2">1.17</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.1.m1.1c">48.18_{\pm 1.17}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.19.19" class="ltx_tr">
<td id="S4.T2.19.19.2" class="ltx_td ltx_align_left">Batch-Normalization</td>
<td id="S4.T2.19.19.1" class="ltx_td ltx_align_center"><math id="S4.T2.19.19.1.m1.1" class="ltx_Math" alttext="33.29_{\pm 0.66}" display="inline"><semantics id="S4.T2.19.19.1.m1.1a"><msub id="S4.T2.19.19.1.m1.1.1" xref="S4.T2.19.19.1.m1.1.1.cmml"><mn id="S4.T2.19.19.1.m1.1.1.2" xref="S4.T2.19.19.1.m1.1.1.2.cmml">33.29</mn><mrow id="S4.T2.19.19.1.m1.1.1.3" xref="S4.T2.19.19.1.m1.1.1.3.cmml"><mo id="S4.T2.19.19.1.m1.1.1.3a" xref="S4.T2.19.19.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.19.19.1.m1.1.1.3.2" xref="S4.T2.19.19.1.m1.1.1.3.2.cmml">0.66</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.1.m1.1b"><apply id="S4.T2.19.19.1.m1.1.1.cmml" xref="S4.T2.19.19.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.19.19.1.m1.1.1.1.cmml" xref="S4.T2.19.19.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.19.19.1.m1.1.1.2.cmml" xref="S4.T2.19.19.1.m1.1.1.2">33.29</cn><apply id="S4.T2.19.19.1.m1.1.1.3.cmml" xref="S4.T2.19.19.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.19.19.1.m1.1.1.3.1.cmml" xref="S4.T2.19.19.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.19.19.1.m1.1.1.3.2.cmml" xref="S4.T2.19.19.1.m1.1.1.3.2">0.66</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.1.m1.1c">33.29_{\pm 0.66}</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.20.20" class="ltx_tr">
<td id="S4.T2.20.20.2" class="ltx_td ltx_align_left ltx_border_bb">Fixed Batch-Normalization</td>
<td id="S4.T2.20.20.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T2.20.20.1.m1.1" class="ltx_Math" alttext="33.66_{\pm 1.34}" display="inline"><semantics id="S4.T2.20.20.1.m1.1a"><msub id="S4.T2.20.20.1.m1.1.1" xref="S4.T2.20.20.1.m1.1.1.cmml"><mn id="S4.T2.20.20.1.m1.1.1.2" xref="S4.T2.20.20.1.m1.1.1.2.cmml">33.66</mn><mrow id="S4.T2.20.20.1.m1.1.1.3" xref="S4.T2.20.20.1.m1.1.1.3.cmml"><mo id="S4.T2.20.20.1.m1.1.1.3a" xref="S4.T2.20.20.1.m1.1.1.3.cmml">Â±</mo><mn id="S4.T2.20.20.1.m1.1.1.3.2" xref="S4.T2.20.20.1.m1.1.1.3.2.cmml">1.34</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.1.m1.1b"><apply id="S4.T2.20.20.1.m1.1.1.cmml" xref="S4.T2.20.20.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.20.20.1.m1.1.1.1.cmml" xref="S4.T2.20.20.1.m1.1.1">subscript</csymbol><cn type="float" id="S4.T2.20.20.1.m1.1.1.2.cmml" xref="S4.T2.20.20.1.m1.1.1.2">33.66</cn><apply id="S4.T2.20.20.1.m1.1.1.3.cmml" xref="S4.T2.20.20.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.20.20.1.m1.1.1.3.1.cmml" xref="S4.T2.20.20.1.m1.1.1.3">plus-or-minus</csymbol><cn type="float" id="S4.T2.20.20.1.m1.1.1.3.2.cmml" xref="S4.T2.20.20.1.m1.1.1.3.2">1.34</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.1.m1.1c">33.66_{\pm 1.34}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2210.03277/assets/x4.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>CNN</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2210.03277/assets/x5.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>VGG-11</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2210.03277/assets/x6.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>ResNet-18</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We show the convergence of different normalization method. Layer normalization achieves fastest or comparable convergence and best or comparable accuracy. </figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Batch Normalization in Deep Neural Networks</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">Batch Normalization (BatchNorm) was proposed to solve the internal covariate shift problem in training deep neural networks. It has been shown that BatchNorm can effectively speedup and improve the robustness of model training. Relevant works have explained why BatchNorm improve both convergence and generalization in training neural networks. <cite class="ltx_cite ltx_citemacro_cite">Luo etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> investigated an explicit regularization form of BN and illustrates the advantage of applying BatchNorm in a single layer perceptron. <cite class="ltx_cite ltx_citemacro_cite">Santurkar etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite> demonstrates that BatchNorm makes the optimization landscape significantly smoother and this smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. <cite class="ltx_cite ltx_citemacro_cite">Morcos etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> empirically shows that BatchNorm improves generalization by implicitly discouraging single direction reliance of the model. In addition to improving generalization and convergence, BatchNorm is also appliedÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> to tackle the domain adaptation problem. However, recent worksÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Hsieh etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> show that BatchNorm incurs the accuracy drop of the global model in FL.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Normalization in Federated Learning</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">BatchNorm is shown to be ineffective in FL, and recent works propose several alternatives. FedBNÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> applies local batch normalization to alleviate the feature shift before averaging models by not uploading and averaging local batch normalization parameters during central aggregation. However, FedBN is limited to personalized FL scenarios. <cite class="ltx_cite ltx_citemacro_cite">Hsieh etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> demonstrates that group normalization (GroupNorm) can improve the convergence of FL. Nevertheless, GroupNorm is instance-based normalization, which is highly sensitive to the noise on data samples. HeteroFL<cite class="ltx_cite ltx_citemacro_cite">Diao etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> applied statistic BatchNorm to solve the privacy concern by not tracking running estimates and simply normalize batch data.
Although these alternatives empirically shows better performance than BatchNorm in FL, the essential reasons why BatchNorm is ineffective in FL are still poorly studied.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this paper, we explicitly identify the external covariate shift problem in FL, which is caused by not only non-IID data but also independent training processes on different devices. We also demonstrate that severe external covariate shift even obliterates some devicesâ€™ contributions to the global model, which will significantly degrade FL training performance. We further present the importance of scale invariant property of normalization layer to the Federated Learning, i.e., prevent norm of weights on different devices from deviating the initialization. We empirically and theoretically explain that external covariate shift is the key reason why batch normalization incurs accuracy drop of the global model in FL and we show that layer normalization does not suffer the problem. The experimental results demonstrate that layer normalization converges much faster than other normalization methods and achieve the best or comparable to the best accuracy.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora etÂ al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sanjeev Arora, Zhiyuan
Li, and Kaifeng Lyu. 2019.

</span>
<span class="ltx_bibblock">Theoretical Analysis of Auto Rate-Tuning by Batch
Normalization. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">International Conference on
Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
JimmyÂ Lei Ba, JamieÂ Ryan
Kiros, and GeoffreyÂ E Hinton.
2016.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1607.06450</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Enmao Diao, Jie Ding,
and Vahid Tarokh. 2020.

</span>
<span class="ltx_bibblock">HeteroFL: Computation and communication efficient
federated learning for heterogeneous clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.01264</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Shang-Hua Gao, Qi Han,
Duo Li, Ming-Ming Cheng, and
Pai Peng. 2021.

</span>
<span class="ltx_bibblock">Representative batch normalization with feature
calibration. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
8669â€“8679.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kevin Hsieh, Amar
Phanishayee, Onur Mutlu, and Phillip
Gibbons. 2020.

</span>
<span class="ltx_bibblock">The non-iid data quagmire of decentralized machine
learning. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>. PMLR, 4387â€“4398.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ioffe and Szegedy (2015)</span>
<span class="ltx_bibblock">
Sergey Ioffe and
Christian Szegedy. 2015.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network
training by reducing internal covariate shift. In
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.
PMLR, 448â€“456.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiaoxiao Li, Meirui
Jiang, Xiaofei Zhang, Michael Kamp,
and Qi Dou. 2021.

</span>
<span class="ltx_bibblock">Fedbn: Federated learning on non-iid features via
local batch normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.07623</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yanghao Li, Naiyan Wang,
Jianping Shi, Xiaodi Hou, and
Jiaying Liu. 2018.

</span>
<span class="ltx_bibblock">Adaptive batch normalization for practical domain
adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em> 80
(2018), 109â€“117.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Ping Luo, Xinjiang Wang,
Wenqi Shao, and Zhanglin Peng.
2018.

</span>
<span class="ltx_bibblock">Towards understanding regularization in batch
normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.00846</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martens and Grosse (2015)</span>
<span class="ltx_bibblock">
James Martens and Roger
Grosse. 2015.

</span>
<span class="ltx_bibblock">Optimizing neural networks with kronecker-factored
approximate curvature. In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International conference
on machine learning</em>. PMLR, 2408â€“2417.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and BlaiseÂ Aguera y Arcas.
2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Artificial
intelligence and statistics</em>. PMLR, 1273â€“1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morcos etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
AriÂ S Morcos, DavidÂ GT
Barrett, NeilÂ C Rabinowitz, and Matthew
Botvinick. 2018.

</span>
<span class="ltx_bibblock">On the importance of single directions for
generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.06959</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neyshabur etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Behnam Neyshabur, Ryota
Tomioka, Ruslan Salakhutdinov, and
Nathan Srebro. 2016.

</span>
<span class="ltx_bibblock">Data-Dependent Path Normalization in Neural
Networks.. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">ICLR (Poster)</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/1511.06747" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1511.06747</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santurkar etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Shibani Santurkar,
Dimitris Tsipras, Andrew Ilyas, and
Aleksander MÄ…dry. 2018.

</span>
<span class="ltx_bibblock">How does batch normalization help optimization?.
In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd international conference
on neural information processing systems</em>. 2488â€“2498.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jiacheng Sun, Xiangyong
Cao, Hanwen Liang, Weiran Huang,
Zewei Chen, and Zhenguo Li.
2020.

</span>
<span class="ltx_bibblock">New interpretations of normalization methods in
deep learning. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, Vol.Â 34.
5875â€“5882.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Minxue Tang, Xuefei Ning,
Yitu Wang, Yu Wang, and
Yiran Chen. 2021.

</span>
<span class="ltx_bibblock">Fedgp: Correlation-based active client selection
for heterogeneous federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.13822</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanÂ Laarhoven (2017)</span>
<span class="ltx_bibblock">
Twan VanÂ Laarhoven.
2017.

</span>
<span class="ltx_bibblock">L2 regularization versus batch and weight
normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.05350</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and He (2018)</span>
<span class="ltx_bibblock">
Yuxin Wu and Kaiming
He. 2018.

</span>
<span class="ltx_bibblock">Group normalization. In
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer
vision (ECCV)</em>. 3â€“19.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2210.03276" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2210.03277" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2210.03277">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2210.03277" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2210.03279" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 00:44:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
