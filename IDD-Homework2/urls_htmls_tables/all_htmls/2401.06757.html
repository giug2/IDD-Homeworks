<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.06757] Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction</title><meta property="og:description" content="Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating acc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.06757">

<!--Generated on Tue Feb 27 09:51:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Muhammad Naveed Riaz<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Maciej Wielgosz<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Abel García Romera<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, and Antonio M. López<sup id="id9.9.id4" class="ltx_sup"><span id="id9.9.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes">Corresponding author: <span id="id10.10.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">nriaz@cvc.uab.cat</span><math id="id5.5.m1.1" class="ltx_Math" alttext="(1)" display="inline"><semantics id="id5.5.m1.1a"><mrow id="id5.5.m1.1.2.2"><mo stretchy="false" id="id5.5.m1.1.2.2.1">(</mo><mn id="id5.5.m1.1.1" xref="id5.5.m1.1.1.cmml">1</mn><mo stretchy="false" id="id5.5.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="id5.5.m1.1b"><cn type="integer" id="id5.5.m1.1.1.cmml" xref="id5.5.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m1.1c">(1)</annotation></semantics></math> Naveed, Abel, and Antonio are with the Dpt. Ciències de la Computació and the CVC, at Univ. Autònoma de Barcelona (UAB).Maciej acknowledges the funding from the European Union’s Horizon 2020 research and innovation programme under Marie Skłodowska-Curie grant agreement No. 801342 (Tecniospring INDUSTRY) and the Government of Catalonia’s Agency for Business Competitiveness (ACCIÓ). This allowed to develop ARCANE. This work only expresses the opinion of the author and neither the European Union nor ACCIÓ are liable for the use made of the information provided.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to predict crossing intentions. ARCANE, PedSynth, and PedGNN will be publicly released<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><math id="footnote1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="footnote1.m1.1b"><mo mathsize="70%" id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><lt id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">&lt;</annotation></semantics></math><span id="footnote1.1" class="ltx_text" style="font-size:70%;">URL address to be provided with the camera-ready version of the paper<math id="footnote1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="footnote1.1.m1.1b"><mo id="footnote1.1.m1.1.1" xref="footnote1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="footnote1.1.m1.1c"><gt id="footnote1.1.m1.1.1.cmml" xref="footnote1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="footnote1.1.m1.1d">&gt;</annotation></semantics></math></span></span></span></span>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.3.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As evidenced in an early Google self-driving car report <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, the 10% of their self-driving malfunctions on streets were due to incorrect behavior predictions of other road users, including pedestrians.</p>
</div>
<figure id="S1.F1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2401.06757/assets/figures/joined-2e.png" id="S1.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="239" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2401.06757/assets/figures/joined-1e.png" id="S1.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="239" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Summary of two video clips from PedSynth. Top rows: a pedestrian crosses the road perpendicularly to the ego-vehicle moving direction. Bottom rows: a pedestrian change the intention of crossing the road at mid-lane. In both examples, the pedestrians enter the road at locations not enabled for crossing.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While there have been significant efforts to improve the accuracy of pedestrian intention prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, there is still ample room for improvement. Currently, two datasets, JAAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and PIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, are being used to benchmark such prediction models. In these datasets, the core ground truth (GT) consists of labeling if pedestrians are crossing or are going to cross in front of the ego vehicle. As for other onboard perception tasks (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, object detection and tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, monocular depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>), synthetic datasets have been proposed to train C/NC prediction models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. We propose to go beyond these datasets by introducing a framework, named ARCANE<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>ARCANE stands for <em id="footnote2.1" class="ltx_emph ltx_font_italic"><span id="footnote2.1.1" class="ltx_text ltx_font_bold">a</span>dve<span id="footnote2.1.2" class="ltx_text ltx_font_bold">r</span>sarial</em> <em id="footnote2.2" class="ltx_emph ltx_font_italic"><span id="footnote2.2.1" class="ltx_text ltx_font_bold">c</span>ases</em> for <em id="footnote2.3" class="ltx_emph ltx_font_italic"><span id="footnote2.3.1" class="ltx_text ltx_font_bold">a</span>uto<span id="footnote2.3.2" class="ltx_text ltx_font_bold">n</span>omous</em> <em id="footnote2.4" class="ltx_emph ltx_font_italic">v<span id="footnote2.4.1" class="ltx_text ltx_font_bold">e</span>hicles</em>, the generic project supporting the development of the framework.</span></span></span>, where traffic scenarios of pedestrian behavior can be programmatically defined. This opens the possibility of introducing underrepresented vehicle-to-pedestrian traffic situations. For being aligned with the research community, ARCANE has been developed on top of the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. As an example, we have used ARCANE to generate PedSynth which is a large and diverse synthetic dataset with pedestrian C/NC labels. Note that this type of labeling is not provided by the CARLA simulator, but it is generated by ARCANE. PedSynth consists of 947 video clips of pedestrian C/NC situations. Each video clip runs <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="\sim 20" display="inline"><semantics id="S1.p2.1.m1.1a"><mrow id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml"></mi><mo id="S1.p2.1.m1.1.1.1" xref="S1.p2.1.m1.1.1.1.cmml">∼</mo><mn id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="latexml" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\sim 20</annotation></semantics></math>s at 30fps, so resulting in approximately 5 H and 26 min of labeled videos. Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows several frames of two video clips from PedSynth. On the other hand, users can generate their own datasets by working with ARCANE.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Focusing on the demanding hardware requirements for onboard perception, we also propose a lightweight model for C/NC prediction, named PedGNN. This model has a 27KB GPU memory footprint and runs on <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\sim 0.6" display="inline"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml"></mi><mo id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">∼</mo><mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">absent</csymbol><cn type="float" id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\sim 0.6</annotation></semantics></math>ms on an NVIDIA GTX 1080 GPU. Compared to a state-of-the-art C/NC prediction model, here named PedGraph+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, PedGNN is one order of magnitude smaller and one order of magnitude faster. Even though, PedGNN outperforms PedGraph+ in terms of the class-balanced F1-score classification metric. PedGNN is based on a GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to predict crossing intentions (see Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Note that, so far, the spatiotemporal analysis of pedestrian skeletons has been shown as one of the most relevant sources of information to predict pedestrian crossing intentions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Using PedGNN and PedSynth to complement the training sets of both JAAD and PIE, allows us to outperform pedestrian C/NC prediction in the respective testing sets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Pedestrian intention prediction</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Pioneering approaches cast C/NC prediction as a trajectory prediction problem, which requires the explicit estimation of the future location, speed, and acceleration of the observed pedestrians <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In practice, the corresponding dynamic models were difficult to adjust and require to extract the silhouette of the pedestrians, dense depth, and dense optical flow with ego-motion compensation. On the other hand, Schneemann and Heinemann <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> concluded that pedestrians’ posture and body movement are essential to take faster C/NC predictions. Accordingly, methods relying on the temporal evolution of pedestrian skeletons gained popularity, especially due to the increasing accuracy of the deep models adjusting them in 2D images, <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (becoming OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>) and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> (becoming AlphaPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>). For instance, a sequence of pedestrian skeletons was used as input to classical lightweight and fast machine learning models such as a random forest C/NC classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and as input to more resources-consuming but accurate deep models such as a Graph Convolutional Network (GNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Skeleton extraction and C/NC prediction have been also tackled as a joint multi-task problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Semantic and contextual information is also considered in different works. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, it is proposed a deep model based on recurrent neural networks (RNNs) and attention modules, which takes as input the ego-vehicle speed, the bounding box (BB), skeleton, and local context (RGB crops) of pedestrians. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, RNNs and attention modules are also used to train two intermediate deep architectures whose output is fused (mid/late fusion) to provide C/NC predictions. One architecture considers the ego-vehicle speed, the BB, and the skeleton of pedestrians. The other considers local and global (scene semantic segmentation) contexts. Finally, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> became a state-of-the-art model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, here named PedGraph+, by incorporating the ego-vehicle speed and pedestrian local context to the initial skeleton-based GNN architecture.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_italic">In this paper:</em> As these C/NC prediction approaches, we also rely on an off-the-self model to obtain the pedestrian skeletons that PedGNN requires as input. Since these skeletons are structured as graphs, we think that GNNs are natural architectures to work with them, <em id="S2.SS1.p3.1.2" class="ltx_emph ltx_font_italic">i.e.</em>, as done by PedGraph+. However, we want PedGNN to be more lightweight and faster. Thus, we use a different GNN architecture than PedGraph+.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.7.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.8.2" class="ltx_text ltx_font_italic">Synthetic datasets focusing on C/NC prediction</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As with other vision-based tasks, C/NC prediction research started with relatively small and non-naturalistic datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Fortunately, larger and naturalistic datasets such as JAAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and PIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> appeared progressively, so helping to accelerate this research. It was also a matter of time to use synthetic data to support C/NC prediction research. In fact, onboard pedestrian detection was one of the first tasks for which a model was trained on synthetic images to perform later in real-world images, this was done more than a decade ago <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Since then, there have been many works leveraging synthetic data to support the training of perception models or performing simulations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>; being synth-to-real domain adaptation a core ingredient to encourage the use of synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Focusing on pedestrians, synthetic data has been mainly generated and used for the tasks of detection and tracking either onboard or from static infrastructure locations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>; where the required GT for each pedestrian consists of a 2D/3D BB, pixel-level segmentation and depth, an ID, and, eventually, a body skeleton. In addition to this kind of GT, for collecting samples to develop C/NC prediction models we must control pedestrian behavior in the simulator, <em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, to force C/NC situations as we wish, and we must label each frame accordingly as in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Recent attempts to do so <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> rely on the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the CP2A dataset was introduced with 220K video clips with per-frame C/NC labels, where 25% of the clips contain crossing (c) examples. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the Virtual-Pedcross-4667 dataset was introduced with 4,667 video clips specially prepared to cover a variety of weather and lighting conditions and per-frame C/NC labels, where 61% of the clips contain crossing (c) examples. On the other hand, these datasets lack some corner cases like the one shown as the bottom example in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Overall, the experiments provided in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> encourage the use of synthetic data to train C/NC models.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><em id="S2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">In this paper:</em> We contribute to the use of synthetic data to develop C/NC prediction models. As <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> we rely on the CARLA simulator. However, rather than only providing a specific synthetic dataset, we introduce ARCANE, a framework prepared to programmatically generate synthetic datasets of pedestrian C/NC videos. As an example, we have used ARCANE to generate PedSynth, which consists of
947 video clips recorded under different weather and lighting conditions over 400 locations in CARLA cities, with <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\sim 398" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml"></mi><mo id="S2.SS2.p3.1.m1.1.1.1" xref="S2.SS2.p3.1.m1.1.1.1.cmml">∼</mo><mn id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml">398</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3">398</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\sim 398</annotation></semantics></math>K frames with C/NC labels. Moreover, using PedGNN, we show that PedSynth is a good complement for the training sets of both JAAD and PIE, so boosting C/NC prediction performance in the respective testing sets.</p>
</div>
<figure id="S2.F2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2401.06757/assets/figures/Model-1o.png" id="S2.SS2.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="214" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">To perform C/NC predictions PedGNN processes sequences of pedestrian skeletons. To process onboard sequences while driving, we use a temporal sliding window of a 1-frame step. PedGNN consists of a graph convolutional gated recurrent unit (GConvGRU), followed by a block of three (ReLU + Fully connected) layers, and a final Softmax. Synthetic datasets with C/NC examples can be used for training PedGNN. For instance, in this paper, we use PedSynth, a synthetic dataset that we have generated using ARCANE, a framework that we introduce in this paper too (see Fig. <a href="#S2.F3" title="Figure 3 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2401.06757/assets/figures/3_2nd.png" id="S2.SS2.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="220" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Block diagram of ARCANE dataset generator. </span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methods</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Figure <a href="#S2.F2" title="Figure 2 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the role of the main contributions of this paper: ARCANE, PedSynth, and PedGNN, which we present in the following subsections.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">The ARCANE framework</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">ARCANE framework is built on the top of CARLA simulator. It enables the generation of different types of video clips through the parameterization of the distribution of pedestrian models, pedestrian velocity, and onboard camera settings. It is possible to create scenarios in a single Python file, which can establish the trajectories of pedestrians in the scene. ARCANE also contains a series of mechanisms that allow for the filtering of not useful videos (<em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, when a pedestrian is not visible). ARCANE is structured around five primary modules, as shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Generator</span>: This module manages the simultaneous execution of multiple batch generator objects. It oversees the randomization of the data generation process and finalizes the process once completed. The module maintains a count of the number of generated videos, ensuring the target quantity is achieved. In the event of crashes, the module has a predefined number of retry attempts to prevent endless generations.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Batch Generator</span>: This module spawns pedestrians in a city, positions cameras, and regulates the data generation process. An integral part of its role involves verifying the presence of pedestrians in the generated videos. This is done via semantic segmentation checks and skeleton existence verification. It supervises the C/NC labeling process too, <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, marking pedestrians as crossing (C) if they are entering a driving area.
These tasks are fulfilled by interacting with Karma.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Karma</span>: This module acts as a facade for the CARLA API. It automates the process of creating the virtual environment in CARLA, pedestrian spawning, and other tasks, by relying on CARLA-based scenario runner functionalities.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">CARLA Engine</span>: This module serves as an instance of the CARLA simulator, which is supplemented with extra routines to ensure its operation within a Docker container during the data generation phase. The module is equipped to restart the container in case of any simulation crashes.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Based on these modules, ARCANE allows for both <em id="S3.SS1.p6.1.1" class="ltx_emph ltx_font_italic">simple</em> and <em id="S3.SS1.p6.1.2" class="ltx_emph ltx_font_italic">advanced usage</em>. Advanced usage refers to the possibility of programmatically defining dynamic traffic scenarios. For instance, leveraging CARLA cities it is possible to write a Python code to choreograph the behavior of pedestrians in these towns, so forcing situations interesting for C/NC prediction. This is what we have done for generating the PedSynth dataset, as illustrated in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Given one of such user-defined Python files to generate traffic scenarios, it is possible to generate variations by generic parameters (<em id="S3.SS1.p6.1.3" class="ltx_emph ltx_font_italic">i.e.</em>, scenario agnostic) which allow controlling the types of pedestrians to be included, their speed, the pedestrian density, <em id="S3.SS1.p6.1.4" class="ltx_emph ltx_font_italic">etc.</em> These parameters are included in a configuration file, named <span id="S3.SS1.p6.1.5" class="ltx_text ltx_font_typewriter">config.yaml</span> in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Therefore, this configuration file enables simple usage provided we are satisfied with the traffic scenarios in place.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Overall, the development of ARCANE took approximately half a year. The code repository includes 3,267 lines of Python code in 47 files, supplemented by a multitude of additional files of other formats.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.11.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.12.2" class="ltx_text" style="font-size:90%;">Features of the datasets used in this paper.</span></figcaption>
<table id="S3.T1.9.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.9.9.10.1" class="ltx_tr">
<th id="S3.T1.9.9.10.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.10.1.1.1" class="ltx_text" style="font-size:90%;">Feature</span></th>
<th id="S3.T1.9.9.10.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.10.1.2.1" class="ltx_text" style="font-size:90%;">JAAD</span></th>
<th id="S3.T1.9.9.10.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.10.1.3.1" class="ltx_text" style="font-size:90%;">PIE</span></th>
<th id="S3.T1.9.9.10.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.10.1.4.1" class="ltx_text" style="font-size:90%;">PedSynth</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.9.9.11.1" class="ltx_tr">
<td id="S3.T1.9.9.11.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.11.1.1.1" class="ltx_text" style="font-size:90%;">Video clips with C/NC labels</span></td>
<td id="S3.T1.9.9.11.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.11.1.2.1" class="ltx_text" style="font-size:90%;">323</span></td>
<td id="S3.T1.9.9.11.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.11.1.3.1" class="ltx_text" style="font-size:90%;">55</span></td>
<td id="S3.T1.9.9.11.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.11.1.4.1" class="ltx_text" style="font-size:90%;">947</span></td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.3.3.3.4.1" class="ltx_text" style="font-size:90%;">Video clips length (s)</span></td>
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim 5-15" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S3.T1.1.1.1.1.m1.1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S3.T1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S3.T1.1.1.1.1.m1.1.1.3.2" xref="S3.T1.1.1.1.1.m1.1.1.3.2.cmml">5</mn><mo mathsize="90%" id="S3.T1.1.1.1.1.m1.1.1.3.1" xref="S3.T1.1.1.1.1.m1.1.1.3.1.cmml">−</mo><mn mathsize="90%" id="S3.T1.1.1.1.1.m1.1.1.3.3" xref="S3.T1.1.1.1.1.m1.1.1.3.3.cmml">15</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.T1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S3.T1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3"><minus id="S3.T1.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3.1"></minus><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3.2">5</cn><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3.3">15</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\sim 5-15</annotation></semantics></math></td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><math id="S3.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\sim 600" display="inline"><semantics id="S3.T1.2.2.2.2.m1.1a"><mrow id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml"><mi id="S3.T1.2.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.2.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S3.T1.2.2.2.2.m1.1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S3.T1.2.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.2.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S3.T1.2.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.T1.2.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.T1.2.2.2.2.m1.1.1.3.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\sim 600</annotation></semantics></math></td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><math id="S3.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\sim 20" display="inline"><semantics id="S3.T1.3.3.3.3.m1.1a"><mrow id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml"><mi id="S3.T1.3.3.3.3.m1.1.1.2" xref="S3.T1.3.3.3.3.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S3.T1.3.3.3.3.m1.1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S3.T1.3.3.3.3.m1.1.1.3" xref="S3.T1.3.3.3.3.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><apply id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1"><csymbol cd="latexml" id="S3.T1.3.3.3.3.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.T1.3.3.3.3.m1.1.1.2.cmml" xref="S3.T1.3.3.3.3.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.T1.3.3.3.3.m1.1.1.3.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">\sim 20</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.9.9.12.2" class="ltx_tr">
<td id="S3.T1.9.9.12.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.12.2.1.1" class="ltx_text" style="font-size:90%;">Frames per second (fps)</span></td>
<td id="S3.T1.9.9.12.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.12.2.2.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="S3.T1.9.9.12.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.12.2.3.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="S3.T1.9.9.12.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.12.2.4.1" class="ltx_text" style="font-size:90%;">30</span></td>
</tr>
<tr id="S3.T1.6.6.6" class="ltx_tr">
<td id="S3.T1.6.6.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.6.6.6.4.1" class="ltx_text" style="font-size:90%;">Frame resolution (pix)</span></td>
<td id="S3.T1.4.4.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><math id="S3.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="1920\!\!\times\!\!1080" display="inline"><semantics id="S3.T1.4.4.4.1.m1.1a"><mrow id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml"><mpadded width="1.892em"><mn mathsize="90%" id="S3.T1.4.4.4.1.m1.1.1.2" xref="S3.T1.4.4.4.1.m1.1.1.2.cmml">1920</mn></mpadded><mpadded width="0.670em"><mo mathsize="90%" id="S3.T1.4.4.4.1.m1.1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.1.cmml">×</mo></mpadded><mn mathsize="90%" id="S3.T1.4.4.4.1.m1.1.1.3" xref="S3.T1.4.4.4.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><apply id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1"><times id="S3.T1.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.4.4.4.1.m1.1.1.2.cmml" xref="S3.T1.4.4.4.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T1.4.4.4.1.m1.1.1.3.cmml" xref="S3.T1.4.4.4.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">1920\!\!\times\!\!1080</annotation></semantics></math></td>
<td id="S3.T1.5.5.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><math id="S3.T1.5.5.5.2.m1.1" class="ltx_Math" alttext="1920\!\!\times\!\!1080" display="inline"><semantics id="S3.T1.5.5.5.2.m1.1a"><mrow id="S3.T1.5.5.5.2.m1.1.1" xref="S3.T1.5.5.5.2.m1.1.1.cmml"><mpadded width="1.892em"><mn mathsize="90%" id="S3.T1.5.5.5.2.m1.1.1.2" xref="S3.T1.5.5.5.2.m1.1.1.2.cmml">1920</mn></mpadded><mpadded width="0.670em"><mo mathsize="90%" id="S3.T1.5.5.5.2.m1.1.1.1" xref="S3.T1.5.5.5.2.m1.1.1.1.cmml">×</mo></mpadded><mn mathsize="90%" id="S3.T1.5.5.5.2.m1.1.1.3" xref="S3.T1.5.5.5.2.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.2.m1.1b"><apply id="S3.T1.5.5.5.2.m1.1.1.cmml" xref="S3.T1.5.5.5.2.m1.1.1"><times id="S3.T1.5.5.5.2.m1.1.1.1.cmml" xref="S3.T1.5.5.5.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.5.5.5.2.m1.1.1.2.cmml" xref="S3.T1.5.5.5.2.m1.1.1.2">1920</cn><cn type="integer" id="S3.T1.5.5.5.2.m1.1.1.3.cmml" xref="S3.T1.5.5.5.2.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.2.m1.1c">1920\!\!\times\!\!1080</annotation></semantics></math></td>
<td id="S3.T1.6.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><math id="S3.T1.6.6.6.3.m1.1" class="ltx_Math" alttext="1600\!\!\times\!\!600" display="inline"><semantics id="S3.T1.6.6.6.3.m1.1a"><mrow id="S3.T1.6.6.6.3.m1.1.1" xref="S3.T1.6.6.6.3.m1.1.1.cmml"><mpadded width="1.892em"><mn mathsize="90%" id="S3.T1.6.6.6.3.m1.1.1.2" xref="S3.T1.6.6.6.3.m1.1.1.2.cmml">1600</mn></mpadded><mpadded width="0.670em"><mo mathsize="90%" id="S3.T1.6.6.6.3.m1.1.1.1" xref="S3.T1.6.6.6.3.m1.1.1.1.cmml">×</mo></mpadded><mn mathsize="90%" id="S3.T1.6.6.6.3.m1.1.1.3" xref="S3.T1.6.6.6.3.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.3.m1.1b"><apply id="S3.T1.6.6.6.3.m1.1.1.cmml" xref="S3.T1.6.6.6.3.m1.1.1"><times id="S3.T1.6.6.6.3.m1.1.1.1.cmml" xref="S3.T1.6.6.6.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.6.6.6.3.m1.1.1.2.cmml" xref="S3.T1.6.6.6.3.m1.1.1.2">1600</cn><cn type="integer" id="S3.T1.6.6.6.3.m1.1.1.3.cmml" xref="S3.T1.6.6.6.3.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.3.m1.1c">1600\!\!\times\!\!600</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.9.9.9" class="ltx_tr">
<td id="S3.T1.9.9.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.9.4.1" class="ltx_text" style="font-size:90%;">Frames with C/NC labels</span></td>
<td id="S3.T1.7.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">
<math id="S3.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="\sim 75" display="inline"><semantics id="S3.T1.7.7.7.1.m1.1a"><mrow id="S3.T1.7.7.7.1.m1.1.1" xref="S3.T1.7.7.7.1.m1.1.1.cmml"><mi id="S3.T1.7.7.7.1.m1.1.1.2" xref="S3.T1.7.7.7.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S3.T1.7.7.7.1.m1.1.1.1" xref="S3.T1.7.7.7.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S3.T1.7.7.7.1.m1.1.1.3" xref="S3.T1.7.7.7.1.m1.1.1.3.cmml">75</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.1.m1.1b"><apply id="S3.T1.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.7.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.T1.7.7.7.1.m1.1.1.2.cmml" xref="S3.T1.7.7.7.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.T1.7.7.7.1.m1.1.1.3.cmml" xref="S3.T1.7.7.7.1.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.1.m1.1c">\sim 75</annotation></semantics></math><span id="S3.T1.7.7.7.1.1" class="ltx_text" style="font-size:90%;">K</span>
</td>
<td id="S3.T1.8.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">
<math id="S3.T1.8.8.8.2.m1.1" class="ltx_Math" alttext="\sim 293" display="inline"><semantics id="S3.T1.8.8.8.2.m1.1a"><mrow id="S3.T1.8.8.8.2.m1.1.1" xref="S3.T1.8.8.8.2.m1.1.1.cmml"><mi id="S3.T1.8.8.8.2.m1.1.1.2" xref="S3.T1.8.8.8.2.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S3.T1.8.8.8.2.m1.1.1.1" xref="S3.T1.8.8.8.2.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S3.T1.8.8.8.2.m1.1.1.3" xref="S3.T1.8.8.8.2.m1.1.1.3.cmml">293</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.2.m1.1b"><apply id="S3.T1.8.8.8.2.m1.1.1.cmml" xref="S3.T1.8.8.8.2.m1.1.1"><csymbol cd="latexml" id="S3.T1.8.8.8.2.m1.1.1.1.cmml" xref="S3.T1.8.8.8.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.T1.8.8.8.2.m1.1.1.2.cmml" xref="S3.T1.8.8.8.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.T1.8.8.8.2.m1.1.1.3.cmml" xref="S3.T1.8.8.8.2.m1.1.1.3">293</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.2.m1.1c">\sim 293</annotation></semantics></math><span id="S3.T1.8.8.8.2.1" class="ltx_text" style="font-size:90%;">K</span>
</td>
<td id="S3.T1.9.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">
<math id="S3.T1.9.9.9.3.m1.1" class="ltx_Math" alttext="\sim 398" display="inline"><semantics id="S3.T1.9.9.9.3.m1.1a"><mrow id="S3.T1.9.9.9.3.m1.1.1" xref="S3.T1.9.9.9.3.m1.1.1.cmml"><mi id="S3.T1.9.9.9.3.m1.1.1.2" xref="S3.T1.9.9.9.3.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S3.T1.9.9.9.3.m1.1.1.1" xref="S3.T1.9.9.9.3.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S3.T1.9.9.9.3.m1.1.1.3" xref="S3.T1.9.9.9.3.m1.1.1.3.cmml">398</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.3.m1.1b"><apply id="S3.T1.9.9.9.3.m1.1.1.cmml" xref="S3.T1.9.9.9.3.m1.1.1"><csymbol cd="latexml" id="S3.T1.9.9.9.3.m1.1.1.1.cmml" xref="S3.T1.9.9.9.3.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.T1.9.9.9.3.m1.1.1.2.cmml" xref="S3.T1.9.9.9.3.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.T1.9.9.9.3.m1.1.1.3.cmml" xref="S3.T1.9.9.9.3.m1.1.1.3">398</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.3.m1.1c">\sim 398</annotation></semantics></math><span id="S3.T1.9.9.9.3.1" class="ltx_text" style="font-size:90%;">K</span>
</td>
</tr>
<tr id="S3.T1.9.9.13.3" class="ltx_tr">
<td id="S3.T1.9.9.13.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.13.3.1.1" class="ltx_text" style="font-size:90%;">Semantic segmentation</span></td>
<td id="S3.T1.9.9.13.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.13.3.2.1" class="ltx_text" style="font-size:90%;">no</span></td>
<td id="S3.T1.9.9.13.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.13.3.3.1" class="ltx_text" style="font-size:90%;">no</span></td>
<td id="S3.T1.9.9.13.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.13.3.4.1" class="ltx_text" style="font-size:90%;">yes</span></td>
</tr>
<tr id="S3.T1.9.9.14.4" class="ltx_tr">
<td id="S3.T1.9.9.14.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.14.4.1.1" class="ltx_text" style="font-size:90%;">Pedestrian skeleton</span></td>
<td id="S3.T1.9.9.14.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.14.4.2.1" class="ltx_text" style="font-size:90%;">no</span></td>
<td id="S3.T1.9.9.14.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.14.4.3.1" class="ltx_text" style="font-size:90%;">no</span></td>
<td id="S3.T1.9.9.14.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.14.4.4.1" class="ltx_text" style="font-size:90%;">yes</span></td>
</tr>
<tr id="S3.T1.9.9.15.5" class="ltx_tr">
<td id="S3.T1.9.9.15.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.15.5.1.1" class="ltx_text" style="font-size:90%;">Weather variability</span></td>
<td id="S3.T1.9.9.15.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.15.5.2.1" class="ltx_text" style="font-size:90%;">yes</span></td>
<td id="S3.T1.9.9.15.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.15.5.3.1" class="ltx_text" style="font-size:90%;">no</span></td>
<td id="S3.T1.9.9.15.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S3.T1.9.9.15.5.4.1" class="ltx_text" style="font-size:90%;">yes</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">The PedSynth dataset</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">We have written a Python code, named <span id="S3.SS2.p1.4.1" class="ltx_text ltx_font_typewriter">PedSynth Scenarios</span> in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which is consumed in ARCANE to generate video clips with C/NC labels according to the settings provided through the <span id="S3.SS2.p1.4.2" class="ltx_text ltx_font_typewriter">config.yaml</span> file. With this information, ARCANE has generated the PedSynth dataset. It covers <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\sim 400" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∼</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\sim 400</annotation></semantics></math> locations from different CARLA cities, thus, including different city styles and road lanes. Varying pedestrians and environmental conditions, we have generated 947 video clips with C/NC labels, resulting in a total of <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\sim 398" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"></mi><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">∼</mo><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">398</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">398</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\sim 398</annotation></semantics></math>K frames with C/NC labels. Table <a href="#S3.T1" title="TABLE I ‣ III-A The ARCANE framework ‣ III Methods ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the main features of PedSynth compared to the real-world datasets JAAD and PIE. We can see how PedSynth contains <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\sim 100" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"></mi><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">∼</mo><mn id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\sim 100</annotation></semantics></math>K more frames with C/NC labels than PIE and more than <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\sim 300" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml"></mi><mo id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">∼</mo><mn id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">300</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\sim 300</annotation></semantics></math>K compared to JAAD. Note that, as in real-world datasets, PedSynth’s videos include frames with no pedestrians, where C/NC prediction models should not rise false warnings. Beyond C/NC labels, we can also leverage GT already present in the CARLA simulator itself, such as pixel-level class semantics (semantic segmentation), pedestrian skeletons, <em id="S3.SS2.p1.4.3" class="ltx_emph ltx_font_italic">etc.</em> We provide more detailed information about ARCANE and PedSynth in the technical report <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2401.06757/assets/figures/Skeleton_final.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="180" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Pedestrian skeleton as expected by PedGNN. We consider 19 joints connected as an undirected graph.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">PedGNN model</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As we have mentioned in Section <a href="#S2" title="II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, the temporal evolution of pedestrian pose is considered core information to determine C/NC intentions. Today, there are robust deep models able to provide human-body skeletons from 2D images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Even by using hand-crafted features and traditional machine learning models, the temporal evolution of 2D-fitted pedestrian skeletons was shown to be effective to determine C/NC intentions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Therefore, as per the state-of-the-art literature, for our C/NC prediction model, we also assume that a sequence of 2D-fitted pedestrian skeletons is used as input to determine C/NC intentions. Figure <a href="#S3.F4" title="Figure 4 ‣ III-B The PedSynth dataset ‣ III Methods ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the joints we consider and their connections. They cover the head, arms, trunk, and legs. The poses of hands and feet are not considered since they cannot be clearly perceived by an onboard camera, and most likely they are irrelevant for determining C/NC intentions. To process sequences of images in a continuous manner, we use a temporal sliding window approach with a frame step to be adjusted experimentally according to the frame rate of the onboard camera (<em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, we use a 1-frame step for cameras working at 30fps).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Since pedestrian skeletons can be represented as undirected graphs, natural deep architectures to process them are GNNs (graph neural networks). In fact, since, for each pedestrian, we work with a sequence of skeletons, a graph convolutional gated recurrent unit (GConvGRU) is a very convenient model for C/NC prediction. Therefore, we adopt it by using the implementation in the PyTorch Geometric (PyG) library<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/pyg-team/pytorch_geometric" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pyg-team/pytorch_geometric</a></span></span></span>. The output of the GConvGRU is flattened and processed by three consecutive blocks of (ReLU + FC) layers, which feed Softmax to obtain the C/NC prediction (see Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Synthetic datasets focusing on C/NC prediction ‣ II Related Work ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.9" class="ltx_p">As input information at a graph node, we use the <math id="S3.SS3.p3.1.m1.2" class="ltx_Math" alttext="(x_{j},y_{j})" display="inline"><semantics id="S3.SS3.p3.1.m1.2a"><mrow id="S3.SS3.p3.1.m1.2.2.2" xref="S3.SS3.p3.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p3.1.m1.2.2.2.3" xref="S3.SS3.p3.1.m1.2.2.3.cmml">(</mo><msub id="S3.SS3.p3.1.m1.1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p3.1.m1.1.1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS3.p3.1.m1.2.2.2.4" xref="S3.SS3.p3.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS3.p3.1.m1.2.2.2.2" xref="S3.SS3.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p3.1.m1.2.2.2.2.2" xref="S3.SS3.p3.1.m1.2.2.2.2.2.cmml">y</mi><mi id="S3.SS3.p3.1.m1.2.2.2.2.3" xref="S3.SS3.p3.1.m1.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS3.p3.1.m1.2.2.2.5" xref="S3.SS3.p3.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.2b"><interval closure="open" id="S3.SS3.p3.1.m1.2.2.3.cmml" xref="S3.SS3.p3.1.m1.2.2.2"><apply id="S3.SS3.p3.1.m1.1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.2">𝑥</ci><ci id="S3.SS3.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.3">𝑗</ci></apply><apply id="S3.SS3.p3.1.m1.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2">𝑦</ci><ci id="S3.SS3.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.3">𝑗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.2c">(x_{j},y_{j})</annotation></semantics></math> coordinates of the joint <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">j</annotation></semantics></math> associated with the node and its fitting confidence <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">c</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝑐</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">c_{j}</annotation></semantics></math> as provided by the skeleton fitting model in use. As is recommended for GNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and for skeleton-based C/NC prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, <math id="S3.SS3.p3.4.m4.2" class="ltx_Math" alttext="(x_{j},y_{j})" display="inline"><semantics id="S3.SS3.p3.4.m4.2a"><mrow id="S3.SS3.p3.4.m4.2.2.2" xref="S3.SS3.p3.4.m4.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p3.4.m4.2.2.2.3" xref="S3.SS3.p3.4.m4.2.2.3.cmml">(</mo><msub id="S3.SS3.p3.4.m4.1.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.1.1.2" xref="S3.SS3.p3.4.m4.1.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p3.4.m4.1.1.1.1.3" xref="S3.SS3.p3.4.m4.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS3.p3.4.m4.2.2.2.4" xref="S3.SS3.p3.4.m4.2.2.3.cmml">,</mo><msub id="S3.SS3.p3.4.m4.2.2.2.2" xref="S3.SS3.p3.4.m4.2.2.2.2.cmml"><mi id="S3.SS3.p3.4.m4.2.2.2.2.2" xref="S3.SS3.p3.4.m4.2.2.2.2.2.cmml">y</mi><mi id="S3.SS3.p3.4.m4.2.2.2.2.3" xref="S3.SS3.p3.4.m4.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS3.p3.4.m4.2.2.2.5" xref="S3.SS3.p3.4.m4.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.2b"><interval closure="open" id="S3.SS3.p3.4.m4.2.2.3.cmml" xref="S3.SS3.p3.4.m4.2.2.2"><apply id="S3.SS3.p3.4.m4.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.2">𝑥</ci><ci id="S3.SS3.p3.4.m4.1.1.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.3">𝑗</ci></apply><apply id="S3.SS3.p3.4.m4.2.2.2.2.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.2.2.2.2.1.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p3.4.m4.2.2.2.2.2.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2.2">𝑦</ci><ci id="S3.SS3.p3.4.m4.2.2.2.2.3.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2.3">𝑗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.2c">(x_{j},y_{j})</annotation></semantics></math> are normalized at each frame to the range <math id="S3.SS3.p3.5.m5.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.SS3.p3.5.m5.2a"><mrow id="S3.SS3.p3.5.m5.2.3.2" xref="S3.SS3.p3.5.m5.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p3.5.m5.2.3.2.1" xref="S3.SS3.p3.5.m5.2.3.1.cmml">[</mo><mn id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">0</mn><mo id="S3.SS3.p3.5.m5.2.3.2.2" xref="S3.SS3.p3.5.m5.2.3.1.cmml">,</mo><mn id="S3.SS3.p3.5.m5.2.2" xref="S3.SS3.p3.5.m5.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS3.p3.5.m5.2.3.2.3" xref="S3.SS3.p3.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.2b"><interval closure="closed" id="S3.SS3.p3.5.m5.2.3.1.cmml" xref="S3.SS3.p3.5.m5.2.3.2"><cn type="integer" id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">0</cn><cn type="integer" id="S3.SS3.p3.5.m5.2.2.cmml" xref="S3.SS3.p3.5.m5.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.2c">[0,1]</annotation></semantics></math>. Thus, we work with normalized 2D coordinates <math id="S3.SS3.p3.6.m6.2" class="ltx_Math" alttext="(\hat{x}_{j},\hat{y}_{j})" display="inline"><semantics id="S3.SS3.p3.6.m6.2a"><mrow id="S3.SS3.p3.6.m6.2.2.2" xref="S3.SS3.p3.6.m6.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p3.6.m6.2.2.2.3" xref="S3.SS3.p3.6.m6.2.2.3.cmml">(</mo><msub id="S3.SS3.p3.6.m6.1.1.1.1" xref="S3.SS3.p3.6.m6.1.1.1.1.cmml"><mover accent="true" id="S3.SS3.p3.6.m6.1.1.1.1.2" xref="S3.SS3.p3.6.m6.1.1.1.1.2.cmml"><mi id="S3.SS3.p3.6.m6.1.1.1.1.2.2" xref="S3.SS3.p3.6.m6.1.1.1.1.2.2.cmml">x</mi><mo id="S3.SS3.p3.6.m6.1.1.1.1.2.1" xref="S3.SS3.p3.6.m6.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.6.m6.1.1.1.1.3" xref="S3.SS3.p3.6.m6.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS3.p3.6.m6.2.2.2.4" xref="S3.SS3.p3.6.m6.2.2.3.cmml">,</mo><msub id="S3.SS3.p3.6.m6.2.2.2.2" xref="S3.SS3.p3.6.m6.2.2.2.2.cmml"><mover accent="true" id="S3.SS3.p3.6.m6.2.2.2.2.2" xref="S3.SS3.p3.6.m6.2.2.2.2.2.cmml"><mi id="S3.SS3.p3.6.m6.2.2.2.2.2.2" xref="S3.SS3.p3.6.m6.2.2.2.2.2.2.cmml">y</mi><mo id="S3.SS3.p3.6.m6.2.2.2.2.2.1" xref="S3.SS3.p3.6.m6.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.6.m6.2.2.2.2.3" xref="S3.SS3.p3.6.m6.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS3.p3.6.m6.2.2.2.5" xref="S3.SS3.p3.6.m6.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.2b"><interval closure="open" id="S3.SS3.p3.6.m6.2.2.3.cmml" xref="S3.SS3.p3.6.m6.2.2.2"><apply id="S3.SS3.p3.6.m6.1.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m6.1.1.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p3.6.m6.1.1.1.1.2.cmml" xref="S3.SS3.p3.6.m6.1.1.1.1.2"><ci id="S3.SS3.p3.6.m6.1.1.1.1.2.1.cmml" xref="S3.SS3.p3.6.m6.1.1.1.1.2.1">^</ci><ci id="S3.SS3.p3.6.m6.1.1.1.1.2.2.cmml" xref="S3.SS3.p3.6.m6.1.1.1.1.2.2">𝑥</ci></apply><ci id="S3.SS3.p3.6.m6.1.1.1.1.3.cmml" xref="S3.SS3.p3.6.m6.1.1.1.1.3">𝑗</ci></apply><apply id="S3.SS3.p3.6.m6.2.2.2.2.cmml" xref="S3.SS3.p3.6.m6.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m6.2.2.2.2.1.cmml" xref="S3.SS3.p3.6.m6.2.2.2.2">subscript</csymbol><apply id="S3.SS3.p3.6.m6.2.2.2.2.2.cmml" xref="S3.SS3.p3.6.m6.2.2.2.2.2"><ci id="S3.SS3.p3.6.m6.2.2.2.2.2.1.cmml" xref="S3.SS3.p3.6.m6.2.2.2.2.2.1">^</ci><ci id="S3.SS3.p3.6.m6.2.2.2.2.2.2.cmml" xref="S3.SS3.p3.6.m6.2.2.2.2.2.2">𝑦</ci></apply><ci id="S3.SS3.p3.6.m6.2.2.2.2.3.cmml" xref="S3.SS3.p3.6.m6.2.2.2.2.3">𝑗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.2c">(\hat{x}_{j},\hat{y}_{j})</annotation></semantics></math> which add invariance to ego-vehicle to pedestrian distance variations. Overall, the input to PedGNN has dimensions (<math id="S3.SS3.p3.7.m7.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S3.SS3.p3.7.m7.1a"><msub id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml"><mi id="S3.SS3.p3.7.m7.1.1.2" xref="S3.SS3.p3.7.m7.1.1.2.cmml">N</mi><mi id="S3.SS3.p3.7.m7.1.1.3" xref="S3.SS3.p3.7.m7.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><apply id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.7.m7.1.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p3.7.m7.1.1.2.cmml" xref="S3.SS3.p3.7.m7.1.1.2">𝑁</ci><ci id="S3.SS3.p3.7.m7.1.1.3.cmml" xref="S3.SS3.p3.7.m7.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">N_{F}</annotation></semantics></math>,19,3), where <math id="S3.SS3.p3.8.m8.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S3.SS3.p3.8.m8.1a"><msub id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml"><mi id="S3.SS3.p3.8.m8.1.1.2" xref="S3.SS3.p3.8.m8.1.1.2.cmml">N</mi><mi id="S3.SS3.p3.8.m8.1.1.3" xref="S3.SS3.p3.8.m8.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><apply id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.8.m8.1.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS3.p3.8.m8.1.1.2.cmml" xref="S3.SS3.p3.8.m8.1.1.2">𝑁</ci><ci id="S3.SS3.p3.8.m8.1.1.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">N_{F}</annotation></semantics></math> is the number of frames used to perform C/NC predictions, which is determined experimentally during the training of PedGNN. Obviously, the 19 comes from the number of joints, and 3 from the information per joint, <em id="S3.SS3.p3.9.1" class="ltx_emph ltx_font_italic">i.e.</em>, <math id="S3.SS3.p3.9.m9.3" class="ltx_Math" alttext="(\hat{x}_{j},\hat{y}_{j},c_{j})" display="inline"><semantics id="S3.SS3.p3.9.m9.3a"><mrow id="S3.SS3.p3.9.m9.3.3.3" xref="S3.SS3.p3.9.m9.3.3.4.cmml"><mo stretchy="false" id="S3.SS3.p3.9.m9.3.3.3.4" xref="S3.SS3.p3.9.m9.3.3.4.cmml">(</mo><msub id="S3.SS3.p3.9.m9.1.1.1.1" xref="S3.SS3.p3.9.m9.1.1.1.1.cmml"><mover accent="true" id="S3.SS3.p3.9.m9.1.1.1.1.2" xref="S3.SS3.p3.9.m9.1.1.1.1.2.cmml"><mi id="S3.SS3.p3.9.m9.1.1.1.1.2.2" xref="S3.SS3.p3.9.m9.1.1.1.1.2.2.cmml">x</mi><mo id="S3.SS3.p3.9.m9.1.1.1.1.2.1" xref="S3.SS3.p3.9.m9.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.9.m9.1.1.1.1.3" xref="S3.SS3.p3.9.m9.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS3.p3.9.m9.3.3.3.5" xref="S3.SS3.p3.9.m9.3.3.4.cmml">,</mo><msub id="S3.SS3.p3.9.m9.2.2.2.2" xref="S3.SS3.p3.9.m9.2.2.2.2.cmml"><mover accent="true" id="S3.SS3.p3.9.m9.2.2.2.2.2" xref="S3.SS3.p3.9.m9.2.2.2.2.2.cmml"><mi id="S3.SS3.p3.9.m9.2.2.2.2.2.2" xref="S3.SS3.p3.9.m9.2.2.2.2.2.2.cmml">y</mi><mo id="S3.SS3.p3.9.m9.2.2.2.2.2.1" xref="S3.SS3.p3.9.m9.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.9.m9.2.2.2.2.3" xref="S3.SS3.p3.9.m9.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.SS3.p3.9.m9.3.3.3.6" xref="S3.SS3.p3.9.m9.3.3.4.cmml">,</mo><msub id="S3.SS3.p3.9.m9.3.3.3.3" xref="S3.SS3.p3.9.m9.3.3.3.3.cmml"><mi id="S3.SS3.p3.9.m9.3.3.3.3.2" xref="S3.SS3.p3.9.m9.3.3.3.3.2.cmml">c</mi><mi id="S3.SS3.p3.9.m9.3.3.3.3.3" xref="S3.SS3.p3.9.m9.3.3.3.3.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS3.p3.9.m9.3.3.3.7" xref="S3.SS3.p3.9.m9.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m9.3b"><vector id="S3.SS3.p3.9.m9.3.3.4.cmml" xref="S3.SS3.p3.9.m9.3.3.3"><apply id="S3.SS3.p3.9.m9.1.1.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.9.m9.1.1.1.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p3.9.m9.1.1.1.1.2.cmml" xref="S3.SS3.p3.9.m9.1.1.1.1.2"><ci id="S3.SS3.p3.9.m9.1.1.1.1.2.1.cmml" xref="S3.SS3.p3.9.m9.1.1.1.1.2.1">^</ci><ci id="S3.SS3.p3.9.m9.1.1.1.1.2.2.cmml" xref="S3.SS3.p3.9.m9.1.1.1.1.2.2">𝑥</ci></apply><ci id="S3.SS3.p3.9.m9.1.1.1.1.3.cmml" xref="S3.SS3.p3.9.m9.1.1.1.1.3">𝑗</ci></apply><apply id="S3.SS3.p3.9.m9.2.2.2.2.cmml" xref="S3.SS3.p3.9.m9.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.9.m9.2.2.2.2.1.cmml" xref="S3.SS3.p3.9.m9.2.2.2.2">subscript</csymbol><apply id="S3.SS3.p3.9.m9.2.2.2.2.2.cmml" xref="S3.SS3.p3.9.m9.2.2.2.2.2"><ci id="S3.SS3.p3.9.m9.2.2.2.2.2.1.cmml" xref="S3.SS3.p3.9.m9.2.2.2.2.2.1">^</ci><ci id="S3.SS3.p3.9.m9.2.2.2.2.2.2.cmml" xref="S3.SS3.p3.9.m9.2.2.2.2.2.2">𝑦</ci></apply><ci id="S3.SS3.p3.9.m9.2.2.2.2.3.cmml" xref="S3.SS3.p3.9.m9.2.2.2.2.3">𝑗</ci></apply><apply id="S3.SS3.p3.9.m9.3.3.3.3.cmml" xref="S3.SS3.p3.9.m9.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.9.m9.3.3.3.3.1.cmml" xref="S3.SS3.p3.9.m9.3.3.3.3">subscript</csymbol><ci id="S3.SS3.p3.9.m9.3.3.3.3.2.cmml" xref="S3.SS3.p3.9.m9.3.3.3.3.2">𝑐</ci><ci id="S3.SS3.p3.9.m9.3.3.3.3.3.cmml" xref="S3.SS3.p3.9.m9.3.3.3.3.3">𝑗</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m9.3c">(\hat{x}_{j},\hat{y}_{j},c_{j})</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Finally, we remark that we focus on having a lightweight and fast C/NC prediction model. PedGNN shows a memory footprint of 27KB and an inference time of <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\sim 0.6" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml"></mi><mo id="S3.SS3.p4.1.m1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.cmml">∼</mo><mn id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">absent</csymbol><cn type="float" id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\sim 0.6</annotation></semantics></math>ms on an NVIDIA GTX 1080 GPU. As we will see in Section <a href="#S4" title="IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, this is one order of magnitude of improvement over other state-of-the-art methods such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental Results</span>
</h2>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.23.11.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.20.10" class="ltx_text" style="font-size:90%;">Let’s a <em id="S4.T2.20.10.1" class="ltx_emph ltx_font_italic">sample</em> be a particular pedestrian completing a C/NC sequence. Thus, different samples can overlap in the same frame. Let <math id="S4.T2.11.1.m1.1" class="ltx_Math" alttext="N_{F}^{S}" display="inline"><semantics id="S4.T2.11.1.m1.1b"><msubsup id="S4.T2.11.1.m1.1.1" xref="S4.T2.11.1.m1.1.1.cmml"><mi id="S4.T2.11.1.m1.1.1.2.2" xref="S4.T2.11.1.m1.1.1.2.2.cmml">N</mi><mi id="S4.T2.11.1.m1.1.1.2.3" xref="S4.T2.11.1.m1.1.1.2.3.cmml">F</mi><mi id="S4.T2.11.1.m1.1.1.3" xref="S4.T2.11.1.m1.1.1.3.cmml">S</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.T2.11.1.m1.1c"><apply id="S4.T2.11.1.m1.1.1.cmml" xref="S4.T2.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.11.1.m1.1.1.1.cmml" xref="S4.T2.11.1.m1.1.1">superscript</csymbol><apply id="S4.T2.11.1.m1.1.1.2.cmml" xref="S4.T2.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.11.1.m1.1.1.2.1.cmml" xref="S4.T2.11.1.m1.1.1">subscript</csymbol><ci id="S4.T2.11.1.m1.1.1.2.2.cmml" xref="S4.T2.11.1.m1.1.1.2.2">𝑁</ci><ci id="S4.T2.11.1.m1.1.1.2.3.cmml" xref="S4.T2.11.1.m1.1.1.2.3">𝐹</ci></apply><ci id="S4.T2.11.1.m1.1.1.3.cmml" xref="S4.T2.11.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.1.m1.1d">N_{F}^{S}</annotation></semantics></math> be the number of labeled frames of the C/NC sequence of sample <math id="S4.T2.12.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.T2.12.2.m2.1b"><mi id="S4.T2.12.2.m2.1.1" xref="S4.T2.12.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.T2.12.2.m2.1c"><ci id="S4.T2.12.2.m2.1.1.cmml" xref="S4.T2.12.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.2.m2.1d">S</annotation></semantics></math>. Let <math id="S4.T2.13.3.m3.1" class="ltx_Math" alttext="N_{S}" display="inline"><semantics id="S4.T2.13.3.m3.1b"><msub id="S4.T2.13.3.m3.1.1" xref="S4.T2.13.3.m3.1.1.cmml"><mi id="S4.T2.13.3.m3.1.1.2" xref="S4.T2.13.3.m3.1.1.2.cmml">N</mi><mi id="S4.T2.13.3.m3.1.1.3" xref="S4.T2.13.3.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.13.3.m3.1c"><apply id="S4.T2.13.3.m3.1.1.cmml" xref="S4.T2.13.3.m3.1.1"><csymbol cd="ambiguous" id="S4.T2.13.3.m3.1.1.1.cmml" xref="S4.T2.13.3.m3.1.1">subscript</csymbol><ci id="S4.T2.13.3.m3.1.1.2.cmml" xref="S4.T2.13.3.m3.1.1.2">𝑁</ci><ci id="S4.T2.13.3.m3.1.1.3.cmml" xref="S4.T2.13.3.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.3.m3.1d">N_{S}</annotation></semantics></math> be the number of samples in a particular subset of videos. For each dataset and split subset, this table reports <math id="S4.T2.14.4.m4.3" class="ltx_Math" alttext="\#l=\sum_{s=1}^{N_{S}}\sum_{f=1}^{N_{F}^{s}}gt(s,f,l)" display="inline"><semantics id="S4.T2.14.4.m4.3b"><mrow id="S4.T2.14.4.m4.3.4" xref="S4.T2.14.4.m4.3.4.cmml"><mrow id="S4.T2.14.4.m4.3.4.2" xref="S4.T2.14.4.m4.3.4.2.cmml"><mi mathvariant="normal" id="S4.T2.14.4.m4.3.4.2.2" xref="S4.T2.14.4.m4.3.4.2.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S4.T2.14.4.m4.3.4.2.1" xref="S4.T2.14.4.m4.3.4.2.1.cmml">​</mo><mi id="S4.T2.14.4.m4.3.4.2.3" xref="S4.T2.14.4.m4.3.4.2.3.cmml">l</mi></mrow><mo rspace="0.111em" id="S4.T2.14.4.m4.3.4.1" xref="S4.T2.14.4.m4.3.4.1.cmml">=</mo><mrow id="S4.T2.14.4.m4.3.4.3" xref="S4.T2.14.4.m4.3.4.3.cmml"><msubsup id="S4.T2.14.4.m4.3.4.3.1" xref="S4.T2.14.4.m4.3.4.3.1.cmml"><mo rspace="0em" id="S4.T2.14.4.m4.3.4.3.1.2.2" xref="S4.T2.14.4.m4.3.4.3.1.2.2.cmml">∑</mo><mrow id="S4.T2.14.4.m4.3.4.3.1.2.3" xref="S4.T2.14.4.m4.3.4.3.1.2.3.cmml"><mi id="S4.T2.14.4.m4.3.4.3.1.2.3.2" xref="S4.T2.14.4.m4.3.4.3.1.2.3.2.cmml">s</mi><mo id="S4.T2.14.4.m4.3.4.3.1.2.3.1" xref="S4.T2.14.4.m4.3.4.3.1.2.3.1.cmml">=</mo><mn id="S4.T2.14.4.m4.3.4.3.1.2.3.3" xref="S4.T2.14.4.m4.3.4.3.1.2.3.3.cmml">1</mn></mrow><msub id="S4.T2.14.4.m4.3.4.3.1.3" xref="S4.T2.14.4.m4.3.4.3.1.3.cmml"><mi id="S4.T2.14.4.m4.3.4.3.1.3.2" xref="S4.T2.14.4.m4.3.4.3.1.3.2.cmml">N</mi><mi id="S4.T2.14.4.m4.3.4.3.1.3.3" xref="S4.T2.14.4.m4.3.4.3.1.3.3.cmml">S</mi></msub></msubsup><mrow id="S4.T2.14.4.m4.3.4.3.2" xref="S4.T2.14.4.m4.3.4.3.2.cmml"><msubsup id="S4.T2.14.4.m4.3.4.3.2.1" xref="S4.T2.14.4.m4.3.4.3.2.1.cmml"><mo id="S4.T2.14.4.m4.3.4.3.2.1.2.2" xref="S4.T2.14.4.m4.3.4.3.2.1.2.2.cmml">∑</mo><mrow id="S4.T2.14.4.m4.3.4.3.2.1.2.3" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3.cmml"><mi id="S4.T2.14.4.m4.3.4.3.2.1.2.3.2" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3.2.cmml">f</mi><mo id="S4.T2.14.4.m4.3.4.3.2.1.2.3.1" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3.1.cmml">=</mo><mn id="S4.T2.14.4.m4.3.4.3.2.1.2.3.3" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3.3.cmml">1</mn></mrow><msubsup id="S4.T2.14.4.m4.3.4.3.2.1.3" xref="S4.T2.14.4.m4.3.4.3.2.1.3.cmml"><mi id="S4.T2.14.4.m4.3.4.3.2.1.3.2.2" xref="S4.T2.14.4.m4.3.4.3.2.1.3.2.2.cmml">N</mi><mi id="S4.T2.14.4.m4.3.4.3.2.1.3.2.3" xref="S4.T2.14.4.m4.3.4.3.2.1.3.2.3.cmml">F</mi><mi id="S4.T2.14.4.m4.3.4.3.2.1.3.3" xref="S4.T2.14.4.m4.3.4.3.2.1.3.3.cmml">s</mi></msubsup></msubsup><mrow id="S4.T2.14.4.m4.3.4.3.2.2" xref="S4.T2.14.4.m4.3.4.3.2.2.cmml"><mi id="S4.T2.14.4.m4.3.4.3.2.2.2" xref="S4.T2.14.4.m4.3.4.3.2.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.T2.14.4.m4.3.4.3.2.2.1" xref="S4.T2.14.4.m4.3.4.3.2.2.1.cmml">​</mo><mi id="S4.T2.14.4.m4.3.4.3.2.2.3" xref="S4.T2.14.4.m4.3.4.3.2.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T2.14.4.m4.3.4.3.2.2.1b" xref="S4.T2.14.4.m4.3.4.3.2.2.1.cmml">​</mo><mrow id="S4.T2.14.4.m4.3.4.3.2.2.4.2" xref="S4.T2.14.4.m4.3.4.3.2.2.4.1.cmml"><mo stretchy="false" id="S4.T2.14.4.m4.3.4.3.2.2.4.2.1" xref="S4.T2.14.4.m4.3.4.3.2.2.4.1.cmml">(</mo><mi id="S4.T2.14.4.m4.1.1" xref="S4.T2.14.4.m4.1.1.cmml">s</mi><mo id="S4.T2.14.4.m4.3.4.3.2.2.4.2.2" xref="S4.T2.14.4.m4.3.4.3.2.2.4.1.cmml">,</mo><mi id="S4.T2.14.4.m4.2.2" xref="S4.T2.14.4.m4.2.2.cmml">f</mi><mo id="S4.T2.14.4.m4.3.4.3.2.2.4.2.3" xref="S4.T2.14.4.m4.3.4.3.2.2.4.1.cmml">,</mo><mi id="S4.T2.14.4.m4.3.3" xref="S4.T2.14.4.m4.3.3.cmml">l</mi><mo stretchy="false" id="S4.T2.14.4.m4.3.4.3.2.2.4.2.4" xref="S4.T2.14.4.m4.3.4.3.2.2.4.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.14.4.m4.3c"><apply id="S4.T2.14.4.m4.3.4.cmml" xref="S4.T2.14.4.m4.3.4"><eq id="S4.T2.14.4.m4.3.4.1.cmml" xref="S4.T2.14.4.m4.3.4.1"></eq><apply id="S4.T2.14.4.m4.3.4.2.cmml" xref="S4.T2.14.4.m4.3.4.2"><times id="S4.T2.14.4.m4.3.4.2.1.cmml" xref="S4.T2.14.4.m4.3.4.2.1"></times><ci id="S4.T2.14.4.m4.3.4.2.2.cmml" xref="S4.T2.14.4.m4.3.4.2.2">#</ci><ci id="S4.T2.14.4.m4.3.4.2.3.cmml" xref="S4.T2.14.4.m4.3.4.2.3">𝑙</ci></apply><apply id="S4.T2.14.4.m4.3.4.3.cmml" xref="S4.T2.14.4.m4.3.4.3"><apply id="S4.T2.14.4.m4.3.4.3.1.cmml" xref="S4.T2.14.4.m4.3.4.3.1"><csymbol cd="ambiguous" id="S4.T2.14.4.m4.3.4.3.1.1.cmml" xref="S4.T2.14.4.m4.3.4.3.1">superscript</csymbol><apply id="S4.T2.14.4.m4.3.4.3.1.2.cmml" xref="S4.T2.14.4.m4.3.4.3.1"><csymbol cd="ambiguous" id="S4.T2.14.4.m4.3.4.3.1.2.1.cmml" xref="S4.T2.14.4.m4.3.4.3.1">subscript</csymbol><sum id="S4.T2.14.4.m4.3.4.3.1.2.2.cmml" xref="S4.T2.14.4.m4.3.4.3.1.2.2"></sum><apply id="S4.T2.14.4.m4.3.4.3.1.2.3.cmml" xref="S4.T2.14.4.m4.3.4.3.1.2.3"><eq id="S4.T2.14.4.m4.3.4.3.1.2.3.1.cmml" xref="S4.T2.14.4.m4.3.4.3.1.2.3.1"></eq><ci id="S4.T2.14.4.m4.3.4.3.1.2.3.2.cmml" xref="S4.T2.14.4.m4.3.4.3.1.2.3.2">𝑠</ci><cn type="integer" id="S4.T2.14.4.m4.3.4.3.1.2.3.3.cmml" xref="S4.T2.14.4.m4.3.4.3.1.2.3.3">1</cn></apply></apply><apply id="S4.T2.14.4.m4.3.4.3.1.3.cmml" xref="S4.T2.14.4.m4.3.4.3.1.3"><csymbol cd="ambiguous" id="S4.T2.14.4.m4.3.4.3.1.3.1.cmml" xref="S4.T2.14.4.m4.3.4.3.1.3">subscript</csymbol><ci id="S4.T2.14.4.m4.3.4.3.1.3.2.cmml" xref="S4.T2.14.4.m4.3.4.3.1.3.2">𝑁</ci><ci id="S4.T2.14.4.m4.3.4.3.1.3.3.cmml" xref="S4.T2.14.4.m4.3.4.3.1.3.3">𝑆</ci></apply></apply><apply id="S4.T2.14.4.m4.3.4.3.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2"><apply id="S4.T2.14.4.m4.3.4.3.2.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1"><csymbol cd="ambiguous" id="S4.T2.14.4.m4.3.4.3.2.1.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1">superscript</csymbol><apply id="S4.T2.14.4.m4.3.4.3.2.1.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1"><csymbol cd="ambiguous" id="S4.T2.14.4.m4.3.4.3.2.1.2.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1">subscript</csymbol><sum id="S4.T2.14.4.m4.3.4.3.2.1.2.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.2.2"></sum><apply id="S4.T2.14.4.m4.3.4.3.2.1.2.3.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3"><eq id="S4.T2.14.4.m4.3.4.3.2.1.2.3.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3.1"></eq><ci id="S4.T2.14.4.m4.3.4.3.2.1.2.3.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3.2">𝑓</ci><cn type="integer" id="S4.T2.14.4.m4.3.4.3.2.1.2.3.3.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.2.3.3">1</cn></apply></apply><apply id="S4.T2.14.4.m4.3.4.3.2.1.3.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.3"><csymbol cd="ambiguous" id="S4.T2.14.4.m4.3.4.3.2.1.3.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.3">superscript</csymbol><apply id="S4.T2.14.4.m4.3.4.3.2.1.3.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.3"><csymbol cd="ambiguous" id="S4.T2.14.4.m4.3.4.3.2.1.3.2.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.3">subscript</csymbol><ci id="S4.T2.14.4.m4.3.4.3.2.1.3.2.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.3.2.2">𝑁</ci><ci id="S4.T2.14.4.m4.3.4.3.2.1.3.2.3.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.3.2.3">𝐹</ci></apply><ci id="S4.T2.14.4.m4.3.4.3.2.1.3.3.cmml" xref="S4.T2.14.4.m4.3.4.3.2.1.3.3">𝑠</ci></apply></apply><apply id="S4.T2.14.4.m4.3.4.3.2.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2.2"><times id="S4.T2.14.4.m4.3.4.3.2.2.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.2.1"></times><ci id="S4.T2.14.4.m4.3.4.3.2.2.2.cmml" xref="S4.T2.14.4.m4.3.4.3.2.2.2">𝑔</ci><ci id="S4.T2.14.4.m4.3.4.3.2.2.3.cmml" xref="S4.T2.14.4.m4.3.4.3.2.2.3">𝑡</ci><vector id="S4.T2.14.4.m4.3.4.3.2.2.4.1.cmml" xref="S4.T2.14.4.m4.3.4.3.2.2.4.2"><ci id="S4.T2.14.4.m4.1.1.cmml" xref="S4.T2.14.4.m4.1.1">𝑠</ci><ci id="S4.T2.14.4.m4.2.2.cmml" xref="S4.T2.14.4.m4.2.2">𝑓</ci><ci id="S4.T2.14.4.m4.3.3.cmml" xref="S4.T2.14.4.m4.3.3">𝑙</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.4.m4.3d">\#l=\sum_{s=1}^{N_{S}}\sum_{f=1}^{N_{F}^{s}}gt(s,f,l)</annotation></semantics></math>, where <math id="S4.T2.15.5.m5.1" class="ltx_Math" alttext="l\in\{\mbox{C, NC}\}" display="inline"><semantics id="S4.T2.15.5.m5.1b"><mrow id="S4.T2.15.5.m5.1.2" xref="S4.T2.15.5.m5.1.2.cmml"><mi id="S4.T2.15.5.m5.1.2.2" xref="S4.T2.15.5.m5.1.2.2.cmml">l</mi><mo id="S4.T2.15.5.m5.1.2.1" xref="S4.T2.15.5.m5.1.2.1.cmml">∈</mo><mrow id="S4.T2.15.5.m5.1.2.3.2" xref="S4.T2.15.5.m5.1.2.3.1.cmml"><mo stretchy="false" id="S4.T2.15.5.m5.1.2.3.2.1" xref="S4.T2.15.5.m5.1.2.3.1.cmml">{</mo><mtext id="S4.T2.15.5.m5.1.1" xref="S4.T2.15.5.m5.1.1a.cmml">C, NC</mtext><mo stretchy="false" id="S4.T2.15.5.m5.1.2.3.2.2" xref="S4.T2.15.5.m5.1.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.15.5.m5.1c"><apply id="S4.T2.15.5.m5.1.2.cmml" xref="S4.T2.15.5.m5.1.2"><in id="S4.T2.15.5.m5.1.2.1.cmml" xref="S4.T2.15.5.m5.1.2.1"></in><ci id="S4.T2.15.5.m5.1.2.2.cmml" xref="S4.T2.15.5.m5.1.2.2">𝑙</ci><set id="S4.T2.15.5.m5.1.2.3.1.cmml" xref="S4.T2.15.5.m5.1.2.3.2"><ci id="S4.T2.15.5.m5.1.1a.cmml" xref="S4.T2.15.5.m5.1.1"><mtext id="S4.T2.15.5.m5.1.1.cmml" xref="S4.T2.15.5.m5.1.1">C, NC</mtext></ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.5.m5.1d">l\in\{\mbox{C, NC}\}</annotation></semantics></math> is the label, and <math id="S4.T2.16.6.m6.3" class="ltx_Math" alttext="gt(s,f,l)=1" display="inline"><semantics id="S4.T2.16.6.m6.3b"><mrow id="S4.T2.16.6.m6.3.4" xref="S4.T2.16.6.m6.3.4.cmml"><mrow id="S4.T2.16.6.m6.3.4.2" xref="S4.T2.16.6.m6.3.4.2.cmml"><mi id="S4.T2.16.6.m6.3.4.2.2" xref="S4.T2.16.6.m6.3.4.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.T2.16.6.m6.3.4.2.1" xref="S4.T2.16.6.m6.3.4.2.1.cmml">​</mo><mi id="S4.T2.16.6.m6.3.4.2.3" xref="S4.T2.16.6.m6.3.4.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T2.16.6.m6.3.4.2.1b" xref="S4.T2.16.6.m6.3.4.2.1.cmml">​</mo><mrow id="S4.T2.16.6.m6.3.4.2.4.2" xref="S4.T2.16.6.m6.3.4.2.4.1.cmml"><mo stretchy="false" id="S4.T2.16.6.m6.3.4.2.4.2.1" xref="S4.T2.16.6.m6.3.4.2.4.1.cmml">(</mo><mi id="S4.T2.16.6.m6.1.1" xref="S4.T2.16.6.m6.1.1.cmml">s</mi><mo id="S4.T2.16.6.m6.3.4.2.4.2.2" xref="S4.T2.16.6.m6.3.4.2.4.1.cmml">,</mo><mi id="S4.T2.16.6.m6.2.2" xref="S4.T2.16.6.m6.2.2.cmml">f</mi><mo id="S4.T2.16.6.m6.3.4.2.4.2.3" xref="S4.T2.16.6.m6.3.4.2.4.1.cmml">,</mo><mi id="S4.T2.16.6.m6.3.3" xref="S4.T2.16.6.m6.3.3.cmml">l</mi><mo stretchy="false" id="S4.T2.16.6.m6.3.4.2.4.2.4" xref="S4.T2.16.6.m6.3.4.2.4.1.cmml">)</mo></mrow></mrow><mo id="S4.T2.16.6.m6.3.4.1" xref="S4.T2.16.6.m6.3.4.1.cmml">=</mo><mn id="S4.T2.16.6.m6.3.4.3" xref="S4.T2.16.6.m6.3.4.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.16.6.m6.3c"><apply id="S4.T2.16.6.m6.3.4.cmml" xref="S4.T2.16.6.m6.3.4"><eq id="S4.T2.16.6.m6.3.4.1.cmml" xref="S4.T2.16.6.m6.3.4.1"></eq><apply id="S4.T2.16.6.m6.3.4.2.cmml" xref="S4.T2.16.6.m6.3.4.2"><times id="S4.T2.16.6.m6.3.4.2.1.cmml" xref="S4.T2.16.6.m6.3.4.2.1"></times><ci id="S4.T2.16.6.m6.3.4.2.2.cmml" xref="S4.T2.16.6.m6.3.4.2.2">𝑔</ci><ci id="S4.T2.16.6.m6.3.4.2.3.cmml" xref="S4.T2.16.6.m6.3.4.2.3">𝑡</ci><vector id="S4.T2.16.6.m6.3.4.2.4.1.cmml" xref="S4.T2.16.6.m6.3.4.2.4.2"><ci id="S4.T2.16.6.m6.1.1.cmml" xref="S4.T2.16.6.m6.1.1">𝑠</ci><ci id="S4.T2.16.6.m6.2.2.cmml" xref="S4.T2.16.6.m6.2.2">𝑓</ci><ci id="S4.T2.16.6.m6.3.3.cmml" xref="S4.T2.16.6.m6.3.3">𝑙</ci></vector></apply><cn type="integer" id="S4.T2.16.6.m6.3.4.3.cmml" xref="S4.T2.16.6.m6.3.4.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.6.m6.3d">gt(s,f,l)=1</annotation></semantics></math> if <math id="S4.T2.17.7.m7.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.T2.17.7.m7.1b"><mi id="S4.T2.17.7.m7.1.1" xref="S4.T2.17.7.m7.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.T2.17.7.m7.1c"><ci id="S4.T2.17.7.m7.1.1.cmml" xref="S4.T2.17.7.m7.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.7.m7.1d">l</annotation></semantics></math> matches the C/NC GT associated to the pedestrian sample <math id="S4.T2.18.8.m8.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.T2.18.8.m8.1b"><mi id="S4.T2.18.8.m8.1.1" xref="S4.T2.18.8.m8.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.T2.18.8.m8.1c"><ci id="S4.T2.18.8.m8.1.1.cmml" xref="S4.T2.18.8.m8.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.8.m8.1d">s</annotation></semantics></math> at frame <math id="S4.T2.19.9.m9.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.T2.19.9.m9.1b"><mi id="S4.T2.19.9.m9.1.1" xref="S4.T2.19.9.m9.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.T2.19.9.m9.1c"><ci id="S4.T2.19.9.m9.1.1.cmml" xref="S4.T2.19.9.m9.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.9.m9.1d">f</annotation></semantics></math>, and <math id="S4.T2.20.10.m10.3" class="ltx_Math" alttext="gt(s,f,l)=0" display="inline"><semantics id="S4.T2.20.10.m10.3b"><mrow id="S4.T2.20.10.m10.3.4" xref="S4.T2.20.10.m10.3.4.cmml"><mrow id="S4.T2.20.10.m10.3.4.2" xref="S4.T2.20.10.m10.3.4.2.cmml"><mi id="S4.T2.20.10.m10.3.4.2.2" xref="S4.T2.20.10.m10.3.4.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.T2.20.10.m10.3.4.2.1" xref="S4.T2.20.10.m10.3.4.2.1.cmml">​</mo><mi id="S4.T2.20.10.m10.3.4.2.3" xref="S4.T2.20.10.m10.3.4.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T2.20.10.m10.3.4.2.1b" xref="S4.T2.20.10.m10.3.4.2.1.cmml">​</mo><mrow id="S4.T2.20.10.m10.3.4.2.4.2" xref="S4.T2.20.10.m10.3.4.2.4.1.cmml"><mo stretchy="false" id="S4.T2.20.10.m10.3.4.2.4.2.1" xref="S4.T2.20.10.m10.3.4.2.4.1.cmml">(</mo><mi id="S4.T2.20.10.m10.1.1" xref="S4.T2.20.10.m10.1.1.cmml">s</mi><mo id="S4.T2.20.10.m10.3.4.2.4.2.2" xref="S4.T2.20.10.m10.3.4.2.4.1.cmml">,</mo><mi id="S4.T2.20.10.m10.2.2" xref="S4.T2.20.10.m10.2.2.cmml">f</mi><mo id="S4.T2.20.10.m10.3.4.2.4.2.3" xref="S4.T2.20.10.m10.3.4.2.4.1.cmml">,</mo><mi id="S4.T2.20.10.m10.3.3" xref="S4.T2.20.10.m10.3.3.cmml">l</mi><mo stretchy="false" id="S4.T2.20.10.m10.3.4.2.4.2.4" xref="S4.T2.20.10.m10.3.4.2.4.1.cmml">)</mo></mrow></mrow><mo id="S4.T2.20.10.m10.3.4.1" xref="S4.T2.20.10.m10.3.4.1.cmml">=</mo><mn id="S4.T2.20.10.m10.3.4.3" xref="S4.T2.20.10.m10.3.4.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.20.10.m10.3c"><apply id="S4.T2.20.10.m10.3.4.cmml" xref="S4.T2.20.10.m10.3.4"><eq id="S4.T2.20.10.m10.3.4.1.cmml" xref="S4.T2.20.10.m10.3.4.1"></eq><apply id="S4.T2.20.10.m10.3.4.2.cmml" xref="S4.T2.20.10.m10.3.4.2"><times id="S4.T2.20.10.m10.3.4.2.1.cmml" xref="S4.T2.20.10.m10.3.4.2.1"></times><ci id="S4.T2.20.10.m10.3.4.2.2.cmml" xref="S4.T2.20.10.m10.3.4.2.2">𝑔</ci><ci id="S4.T2.20.10.m10.3.4.2.3.cmml" xref="S4.T2.20.10.m10.3.4.2.3">𝑡</ci><vector id="S4.T2.20.10.m10.3.4.2.4.1.cmml" xref="S4.T2.20.10.m10.3.4.2.4.2"><ci id="S4.T2.20.10.m10.1.1.cmml" xref="S4.T2.20.10.m10.1.1">𝑠</ci><ci id="S4.T2.20.10.m10.2.2.cmml" xref="S4.T2.20.10.m10.2.2">𝑓</ci><ci id="S4.T2.20.10.m10.3.3.cmml" xref="S4.T2.20.10.m10.3.3">𝑙</ci></vector></apply><cn type="integer" id="S4.T2.20.10.m10.3.4.3.cmml" xref="S4.T2.20.10.m10.3.4.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.10.m10.3d">gt(s,f,l)=0</annotation></semantics></math> otherwise.</span></figcaption>
<table id="S4.T2.24" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.24.1.1" class="ltx_tr">
<th id="S4.T2.24.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.1.1.1.1" class="ltx_text" style="font-size:90%;">Int.</span></th>
<th id="S4.T2.24.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;" colspan="3"><span id="S4.T2.24.1.1.2.1" class="ltx_text" style="font-size:90%;">JAAD</span></th>
<th id="S4.T2.24.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;" colspan="3"><span id="S4.T2.24.1.1.3.1" class="ltx_text" style="font-size:90%;">PIE</span></th>
<th id="S4.T2.24.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;" colspan="3"><span id="S4.T2.24.1.1.4.1" class="ltx_text" style="font-size:90%;">PedSynth</span></th>
</tr>
<tr id="S4.T2.24.2.2" class="ltx_tr">
<th id="S4.T2.24.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.1.1" class="ltx_text" style="font-size:90%;">Label</span></th>
<th id="S4.T2.24.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.2.1" class="ltx_text" style="font-size:90%;">Train</span></th>
<th id="S4.T2.24.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.3.1" class="ltx_text" style="font-size:90%;">Val.</span></th>
<th id="S4.T2.24.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.4.1" class="ltx_text" style="font-size:90%;">Test</span></th>
<th id="S4.T2.24.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.5.1" class="ltx_text" style="font-size:90%;">Train</span></th>
<th id="S4.T2.24.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.6.1" class="ltx_text" style="font-size:90%;">Val.</span></th>
<th id="S4.T2.24.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.7.1" class="ltx_text" style="font-size:90%;">Test</span></th>
<th id="S4.T2.24.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.8.1" class="ltx_text" style="font-size:90%;">Train</span></th>
<th id="S4.T2.24.2.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.9.1" class="ltx_text" style="font-size:90%;">Val.</span></th>
<th id="S4.T2.24.2.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.2.2.10.1" class="ltx_text" style="font-size:90%;">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.24.3.1" class="ltx_tr">
<th id="S4.T2.24.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.1.1" class="ltx_text" style="font-size:90%;">#C</span></th>
<td id="S4.T2.24.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.2.1" class="ltx_text" style="font-size:90%;">39.7K</span></td>
<td id="S4.T2.24.3.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.3.1" class="ltx_text" style="font-size:90%;">6.3K</span></td>
<td id="S4.T2.24.3.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_r ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.4.1" class="ltx_text" style="font-size:90%;">32.8K</span></td>
<td id="S4.T2.24.3.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.5.1" class="ltx_text" style="font-size:90%;">116.2K</span></td>
<td id="S4.T2.24.3.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.6.1" class="ltx_text" style="font-size:90%;">18.1K</span></td>
<td id="S4.T2.24.3.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_r ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.7.1" class="ltx_text" style="font-size:90%;">130.9K</span></td>
<td id="S4.T2.24.3.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.8.1" class="ltx_text" style="font-size:90%;">155.1K</span></td>
<td id="S4.T2.24.3.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.9.1" class="ltx_text" style="font-size:90%;">50.4K</span></td>
<td id="S4.T2.24.3.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.3.1.10.1" class="ltx_text" style="font-size:90%;">50.5K</span></td>
</tr>
<tr id="S4.T2.24.4.2" class="ltx_tr">
<th id="S4.T2.24.4.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.1.1" class="ltx_text" style="font-size:90%;">#NC</span></th>
<td id="S4.T2.24.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.2.1" class="ltx_text" style="font-size:90%;">7.9K</span></td>
<td id="S4.T2.24.4.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.3.1" class="ltx_text" style="font-size:90%;">1.5K</span></td>
<td id="S4.T2.24.4.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.4.1" class="ltx_text" style="font-size:90%;">8.4K</span></td>
<td id="S4.T2.24.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.5.1" class="ltx_text" style="font-size:90%;">110.7K</span></td>
<td id="S4.T2.24.4.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.6.1" class="ltx_text" style="font-size:90%;">22.1K</span></td>
<td id="S4.T2.24.4.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.7.1" class="ltx_text" style="font-size:90%;">76.8K</span></td>
<td id="S4.T2.24.4.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.8.1" class="ltx_text" style="font-size:90%;">82.3K</span></td>
<td id="S4.T2.24.4.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.9.1" class="ltx_text" style="font-size:90%;">29.9K</span></td>
<td id="S4.T2.24.4.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S4.T2.24.4.2.10.1" class="ltx_text" style="font-size:90%;">29.5K</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Datasets, metrics, frameworks, pose estimation</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.3" class="ltx_p">For our experiments, as real-world datasets we use the de-facto standards for C/NC prediction, <em id="S4.SS1.p1.3.1" class="ltx_emph ltx_font_italic">i.e.</em>, JAAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and PIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. We use JAAD <em id="S4.SS1.p1.3.2" class="ltx_emph ltx_font_italic">all</em> version. As a synthetic dataset, we use<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>At the moment of elaborating our work, CP2A and Virtual-Pedcross-4667 datasets do not seem downloadable anymore.</span></span></span> our PedSynth. Table <a href="#S3.T1" title="TABLE I ‣ III-A The ARCANE framework ‣ III Methods ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes their main features.
For JAAD and PIE datasets we also use their standard Train/Val./Test split. For PedSynth we performed a random split and fix it for all the experiments. Specifically, <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\sim 80" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">∼</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\sim 80</annotation></semantics></math>% of PedSynth is allocated for training, <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\sim 10" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml"></mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">∼</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\sim 10</annotation></semantics></math>% for validation, and another <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\sim 10" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml"></mi><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">∼</mo><mn id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">absent</csymbol><cn type="integer" id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\sim 10</annotation></semantics></math>% for testing. Table <a href="#S4.T2" title="TABLE II ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides information on the corresponding splits in terms of C/NC labeling.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To report our results, we apply the metrics used in C/NC prediction literature, <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, standard <em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Accuracy</em>, <em id="S4.SS1.p2.1.3" class="ltx_emph ltx_font_italic">Precision</em>, <em id="S4.SS1.p2.1.4" class="ltx_emph ltx_font_italic">Recall</em>, and <em id="S4.SS1.p2.1.5" class="ltx_emph ltx_font_italic">F1-score</em>. For training models and running inferences, we use PyTorch.
Since PedGNN is based on pedestrian skeletons adjusted on 2D images, we use the state-of-the-art deep model named AlphaPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as an off-the-shelf method. AlphaPose does not return the 19 joints we use for PedGNN. Compared to Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B The PedSynth dataset ‣ III Methods ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, AlphaPose does not provide the Neck and CHip joints. To compute the Neck coordinates we average LShoulder and RShoulder coordinates. Analogously, to compute the CHip coordinates we average LHip and RHip.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Training protocol</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">As the training optimizer, we use AdamW with binary cross-entropy loss and default parameters except for the learning rate, <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="l_{r}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">l</mi><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑙</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">l_{r}</annotation></semantics></math>. We perform training runs for a maximum number of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="M_{E}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">M</mi><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑀</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">M_{E}</annotation></semantics></math> epochs. We also apply a 50% dropout. In this optimization process, a training sample consists of a sequence of skeletons from the same pedestrian. In other words, for C/NC pedestrian prediction, we consider <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><msub id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">N</mi><mi id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">𝑁</ci><ci id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">N_{F}</annotation></semantics></math> consecutive frames. We use a training batch size of 500 samples. Since skeleton information has a really low memory footprint, this batch size fits well in a single 24GB memory GPU. Since some experiments rely on training images from different datasets, we utilized PyTorch’s ConcatDataset and WeightedRandomSampler functions to ensure equal dataset sampling per batch.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.11" class="ltx_p">To train a C/NC prediction model, we test different values for <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">N</mi><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑁</ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">N_{F}</annotation></semantics></math> and <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="l_{r}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><msub id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">l</mi><mi id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑙</ci><ci id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">l_{r}</annotation></semantics></math>. Regarding <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><msub id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">N</mi><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝑁</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">N_{F}</annotation></semantics></math>, we consider values in the range <math id="S4.SS2.p2.4.m4.3" class="ltx_Math" alttext="[4,\ldots,32]" display="inline"><semantics id="S4.SS2.p2.4.m4.3a"><mrow id="S4.SS2.p2.4.m4.3.4.2" xref="S4.SS2.p2.4.m4.3.4.1.cmml"><mo stretchy="false" id="S4.SS2.p2.4.m4.3.4.2.1" xref="S4.SS2.p2.4.m4.3.4.1.cmml">[</mo><mn id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">4</mn><mo id="S4.SS2.p2.4.m4.3.4.2.2" xref="S4.SS2.p2.4.m4.3.4.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p2.4.m4.2.2" xref="S4.SS2.p2.4.m4.2.2.cmml">…</mi><mo id="S4.SS2.p2.4.m4.3.4.2.3" xref="S4.SS2.p2.4.m4.3.4.1.cmml">,</mo><mn id="S4.SS2.p2.4.m4.3.3" xref="S4.SS2.p2.4.m4.3.3.cmml">32</mn><mo stretchy="false" id="S4.SS2.p2.4.m4.3.4.2.4" xref="S4.SS2.p2.4.m4.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.3b"><list id="S4.SS2.p2.4.m4.3.4.1.cmml" xref="S4.SS2.p2.4.m4.3.4.2"><cn type="integer" id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">4</cn><ci id="S4.SS2.p2.4.m4.2.2.cmml" xref="S4.SS2.p2.4.m4.2.2">…</ci><cn type="integer" id="S4.SS2.p2.4.m4.3.3.cmml" xref="S4.SS2.p2.4.m4.3.3">32</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.3c">[4,\ldots,32]</annotation></semantics></math> with step=2. Since all datasets were recorded at 30fps (Table <a href="#S3.T1" title="TABLE I ‣ III-A The ARCANE framework ‣ III Methods ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>), this is equivalent to considering a temporal window from <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="\sim 133" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml"></mi><mo id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">∼</mo><mn id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">133</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">absent</csymbol><cn type="integer" id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">133</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\sim 133</annotation></semantics></math>ms to <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="\sim 1067" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml"></mi><mo id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">∼</mo><mn id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">1067</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">absent</csymbol><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">1067</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">\sim 1067</annotation></semantics></math>ms. Regarding <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="l_{r}" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><msub id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml"><mi id="S4.SS2.p2.7.m7.1.1.2" xref="S4.SS2.p2.7.m7.1.1.2.cmml">l</mi><mi id="S4.SS2.p2.7.m7.1.1.3" xref="S4.SS2.p2.7.m7.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2">𝑙</ci><ci id="S4.SS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">l_{r}</annotation></semantics></math>, we consider values in <math id="S4.SS2.p2.8.m8.4" class="ltx_Math" alttext="\{0.001,0.005,0.0002,0.0005\}" display="inline"><semantics id="S4.SS2.p2.8.m8.4a"><mrow id="S4.SS2.p2.8.m8.4.5.2" xref="S4.SS2.p2.8.m8.4.5.1.cmml"><mo stretchy="false" id="S4.SS2.p2.8.m8.4.5.2.1" xref="S4.SS2.p2.8.m8.4.5.1.cmml">{</mo><mn id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml">0.001</mn><mo id="S4.SS2.p2.8.m8.4.5.2.2" xref="S4.SS2.p2.8.m8.4.5.1.cmml">,</mo><mn id="S4.SS2.p2.8.m8.2.2" xref="S4.SS2.p2.8.m8.2.2.cmml">0.005</mn><mo id="S4.SS2.p2.8.m8.4.5.2.3" xref="S4.SS2.p2.8.m8.4.5.1.cmml">,</mo><mn id="S4.SS2.p2.8.m8.3.3" xref="S4.SS2.p2.8.m8.3.3.cmml">0.0002</mn><mo id="S4.SS2.p2.8.m8.4.5.2.4" xref="S4.SS2.p2.8.m8.4.5.1.cmml">,</mo><mn id="S4.SS2.p2.8.m8.4.4" xref="S4.SS2.p2.8.m8.4.4.cmml">0.0005</mn><mo stretchy="false" id="S4.SS2.p2.8.m8.4.5.2.5" xref="S4.SS2.p2.8.m8.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.4b"><set id="S4.SS2.p2.8.m8.4.5.1.cmml" xref="S4.SS2.p2.8.m8.4.5.2"><cn type="float" id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1">0.001</cn><cn type="float" id="S4.SS2.p2.8.m8.2.2.cmml" xref="S4.SS2.p2.8.m8.2.2">0.005</cn><cn type="float" id="S4.SS2.p2.8.m8.3.3.cmml" xref="S4.SS2.p2.8.m8.3.3">0.0002</cn><cn type="float" id="S4.SS2.p2.8.m8.4.4.cmml" xref="S4.SS2.p2.8.m8.4.4">0.0005</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.4c">\{0.001,0.005,0.0002,0.0005\}</annotation></semantics></math>. While training a model, we assess its F1-score at the end of each epoch with the help of the validation set associated with the targeted training set. We have set <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="M_{E}=100" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><mrow id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml"><msub id="S4.SS2.p2.9.m9.1.1.2" xref="S4.SS2.p2.9.m9.1.1.2.cmml"><mi id="S4.SS2.p2.9.m9.1.1.2.2" xref="S4.SS2.p2.9.m9.1.1.2.2.cmml">M</mi><mi id="S4.SS2.p2.9.m9.1.1.2.3" xref="S4.SS2.p2.9.m9.1.1.2.3.cmml">E</mi></msub><mo id="S4.SS2.p2.9.m9.1.1.1" xref="S4.SS2.p2.9.m9.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.9.m9.1.1.3" xref="S4.SS2.p2.9.m9.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><apply id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1"><eq id="S4.SS2.p2.9.m9.1.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1.1"></eq><apply id="S4.SS2.p2.9.m9.1.1.2.cmml" xref="S4.SS2.p2.9.m9.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.9.m9.1.1.2.1.cmml" xref="S4.SS2.p2.9.m9.1.1.2">subscript</csymbol><ci id="S4.SS2.p2.9.m9.1.1.2.2.cmml" xref="S4.SS2.p2.9.m9.1.1.2.2">𝑀</ci><ci id="S4.SS2.p2.9.m9.1.1.2.3.cmml" xref="S4.SS2.p2.9.m9.1.1.2.3">𝐸</ci></apply><cn type="integer" id="S4.SS2.p2.9.m9.1.1.3.cmml" xref="S4.SS2.p2.9.m9.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">M_{E}=100</annotation></semantics></math>. To apply the C/NC prediction models we use a temporal sliding window with a step of 1 frame. Across the considered <math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><msub id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml"><mi id="S4.SS2.p2.10.m10.1.1.2" xref="S4.SS2.p2.10.m10.1.1.2.cmml">N</mi><mi id="S4.SS2.p2.10.m10.1.1.3" xref="S4.SS2.p2.10.m10.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><apply id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.10.m10.1.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1">subscript</csymbol><ci id="S4.SS2.p2.10.m10.1.1.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2">𝑁</ci><ci id="S4.SS2.p2.10.m10.1.1.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">N_{F}</annotation></semantics></math> and <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="l_{r}" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><msub id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mi id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">l</mi><mi id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.11.m11.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1">subscript</csymbol><ci id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2">𝑙</ci><ci id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">l_{r}</annotation></semantics></math> values and epochs, we take as the final model the best-performing one.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">Depending on the size of the training set, obtaining a trained model requires from <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\sim 3-4" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml"></mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">∼</mo><mrow id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml"><mn id="S4.SS2.p3.1.m1.1.1.3.2" xref="S4.SS2.p3.1.m1.1.1.3.2.cmml">3</mn><mo id="S4.SS2.p3.1.m1.1.1.3.1" xref="S4.SS2.p3.1.m1.1.1.3.1.cmml">−</mo><mn id="S4.SS2.p3.1.m1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">absent</csymbol><apply id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><minus id="S4.SS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3.1"></minus><cn type="integer" id="S4.SS2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.2">3</cn><cn type="integer" id="S4.SS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\sim 3-4</annotation></semantics></math>h (single training dataset) to <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="\sim 6-9" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mi id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml"></mi><mo id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">∼</mo><mrow id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml"><mn id="S4.SS2.p3.2.m2.1.1.3.2" xref="S4.SS2.p3.2.m2.1.1.3.2.cmml">6</mn><mo id="S4.SS2.p3.2.m2.1.1.3.1" xref="S4.SS2.p3.2.m2.1.1.3.1.cmml">−</mo><mn id="S4.SS2.p3.2.m2.1.1.3.3" xref="S4.SS2.p3.2.m2.1.1.3.3.cmml">9</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">absent</csymbol><apply id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3"><minus id="S4.SS2.p3.2.m2.1.1.3.1.cmml" xref="S4.SS2.p3.2.m2.1.1.3.1"></minus><cn type="integer" id="S4.SS2.p3.2.m2.1.1.3.2.cmml" xref="S4.SS2.p3.2.m2.1.1.3.2">6</cn><cn type="integer" id="S4.SS2.p3.2.m2.1.1.3.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\sim 6-9</annotation></semantics></math>h (multiple training datasets) in a desktop PC with an NVIDIA RTX 3090 GPU. We remark that, for doing these experiments, pedestrian skeletons are computed and recorded on the hard disk once.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">C/NC prediction performance with PedGNN.</span></figcaption>
<table id="S4.T3.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">Train</span></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Test</span></th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T3.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.m1.1.1.2.cmml">N</mi><mi mathsize="80%" id="S4.T3.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2">𝑁</ci><ci id="S4.T3.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">N_{F}</annotation></semantics></math></th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Precision</span></th>
<th id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.6.1" class="ltx_text" style="font-size:80%;">Recall</span></th>
<th id="S4.T3.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.7.1" class="ltx_text" style="font-size:80%;">F1-score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">JAAD</span></th>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">JAAD</span></th>
<th id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">18</span></th>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80.32</span></td>
<td id="S4.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.1.5.1" class="ltx_text" style="font-size:80%;">84.72</span></td>
<td id="S4.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">87.91</span></td>
<td id="S4.T3.1.1.2.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.1.7.1" class="ltx_text" style="font-size:80%;">85.29</span></td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">PedSynth</span></th>
<th id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">JAAD</span></th>
<th id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.3.2.4.1" class="ltx_text" style="font-size:80%;">78.59</span></td>
<td id="S4.T3.1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.3.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.48</span></td>
<td id="S4.T3.1.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.3.2.6.1" class="ltx_text" style="font-size:80%;">84.62</span></td>
<td id="S4.T3.1.1.3.2.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.1.3.2.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">85.45</span></td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<th id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.1.4.3.1.1" class="ltx_text" style="font-size:80%;">PIE</span></th>
<th id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">PIE</span></th>
<th id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.4.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">68.11</span></td>
<td id="S4.T3.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.4.3.5.1" class="ltx_text" style="font-size:80%;">66.98</span></td>
<td id="S4.T3.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.4.3.6.1" class="ltx_text" style="font-size:80%;">68.36</span></td>
<td id="S4.T3.1.1.4.3.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.1.4.3.7.1" class="ltx_text" style="font-size:80%;">69.81</span></td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<th id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.1.1.5.4.1.1" class="ltx_text" style="font-size:80%;">PedSynth</span></th>
<th id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">PIE</span></th>
<th id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">16</span></th>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.5.4.4.1" class="ltx_text" style="font-size:80%;">62.74</span></td>
<td id="S4.T3.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.5.4.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">69.37</span></td>
<td id="S4.T3.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.5.4.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">79.31</span></td>
<td id="S4.T3.1.1.5.4.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.5.4.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">74.01</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">Performance when combining different datasets for training PedGNN. J: JAAD, P: PIE, S: PedSynth.</span></figcaption>
<table id="S4.T4.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.1.1.1.2.1" class="ltx_text" style="font-size:80%;">Train</span></th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Test</span></th>
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><msub id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T4.1.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.1.m1.1.1.2.cmml">N</mi><mi mathsize="80%" id="S4.T4.1.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T4.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2">𝑁</ci><ci id="S4.T4.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">N_{F}</annotation></semantics></math></th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></th>
<th id="S4.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Precision</span></th>
<th id="S4.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.6.1" class="ltx_text" style="font-size:80%;">Recall</span></th>
<th id="S4.T4.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.7.1" class="ltx_text" style="font-size:80%;">F1-score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<th id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">18</span></th>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.1.4.1" class="ltx_text" style="font-size:80%;">80.32</span></td>
<td id="S4.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.1.5.1" class="ltx_text" style="font-size:80%;">84.72</span></td>
<td id="S4.T4.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.1.6.1" class="ltx_text" style="font-size:80%;">87.91</span></td>
<td id="S4.T4.1.1.2.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.1.7.1" class="ltx_text" style="font-size:80%;">85.29</span></td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<th id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">S</span></th>
<th id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.3.2.4.1" class="ltx_text" style="font-size:80%;">78.59</span></td>
<td id="S4.T4.1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.3.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.48</span></td>
<td id="S4.T4.1.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.3.2.6.1" class="ltx_text" style="font-size:80%;">84.62</span></td>
<td id="S4.T4.1.1.3.2.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.3.2.7.1" class="ltx_text" style="font-size:80%;">85.45</span></td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<th id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.4.3.1.1" class="ltx_text" style="font-size:80%;">J + P</span></th>
<th id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.4.3.4.1" class="ltx_text" style="font-size:80%;">72.36</span></td>
<td id="S4.T4.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.4.3.5.1" class="ltx_text" style="font-size:80%;">74.22</span></td>
<td id="S4.T4.1.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.4.3.6.1" class="ltx_text" style="font-size:80%;">89.18</span></td>
<td id="S4.T4.1.1.4.3.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.4.3.7.1" class="ltx_text" style="font-size:80%;">81.20</span></td>
</tr>
<tr id="S4.T4.1.1.5.4" class="ltx_tr">
<th id="S4.T4.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.5.4.1.1" class="ltx_text" style="font-size:80%;">J + S</span></th>
<th id="S4.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S4.T4.1.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">86.22</span></td>
<td id="S4.T4.1.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.4.5.1" class="ltx_text" style="font-size:80%;">77.35</span></td>
<td id="S4.T4.1.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.4.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">96.19</span></td>
<td id="S4.T4.1.1.5.4.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.5.4.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">85.96</span></td>
</tr>
<tr id="S4.T4.1.1.6.5" class="ltx_tr">
<th id="S4.T4.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.6.5.1.1" class="ltx_text" style="font-size:80%;">J + S + P</span></th>
<th id="S4.T4.1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.6.5.2.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.6.5.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T4.1.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.5.4.1" class="ltx_text" style="font-size:80%;">74.41</span></td>
<td id="S4.T4.1.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.5.5.1" class="ltx_text" style="font-size:80%;">76.73</span></td>
<td id="S4.T4.1.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.5.6.1" class="ltx_text" style="font-size:80%;">88.00</span></td>
<td id="S4.T4.1.1.6.5.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.6.5.7.1" class="ltx_text" style="font-size:80%;">81.98</span></td>
</tr>
<tr id="S4.T4.1.1.7.6" class="ltx_tr">
<th id="S4.T4.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.1.7.6.1.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T4.1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.1.7.6.2.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T4.1.1.7.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.1.7.6.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T4.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.4.1" class="ltx_text" style="font-size:80%;">68.11</span></td>
<td id="S4.T4.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.5.1" class="ltx_text" style="font-size:80%;">66.98</span></td>
<td id="S4.T4.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.6.1" class="ltx_text" style="font-size:80%;">68.36</span></td>
<td id="S4.T4.1.1.7.6.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.7.1" class="ltx_text" style="font-size:80%;">69.81</span></td>
</tr>
<tr id="S4.T4.1.1.8.7" class="ltx_tr">
<th id="S4.T4.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.8.7.1.1" class="ltx_text" style="font-size:80%;">S</span></th>
<th id="S4.T4.1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.8.7.2.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T4.1.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.8.7.3.1" class="ltx_text" style="font-size:80%;">16</span></th>
<td id="S4.T4.1.1.8.7.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.7.4.1" class="ltx_text" style="font-size:80%;">62.74</span></td>
<td id="S4.T4.1.1.8.7.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.7.5.1" class="ltx_text" style="font-size:80%;">69.37</span></td>
<td id="S4.T4.1.1.8.7.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.8.7.6.1" class="ltx_text" style="font-size:80%;">79.31</span></td>
<td id="S4.T4.1.1.8.7.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.8.7.7.1" class="ltx_text" style="font-size:80%;">74.01</span></td>
</tr>
<tr id="S4.T4.1.1.9.8" class="ltx_tr">
<th id="S4.T4.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.9.8.1.1" class="ltx_text" style="font-size:80%;">P + J</span></th>
<th id="S4.T4.1.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.9.8.2.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T4.1.1.9.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.9.8.3.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S4.T4.1.1.9.8.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.9.8.4.1" class="ltx_text" style="font-size:80%;">69.26</span></td>
<td id="S4.T4.1.1.9.8.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.9.8.5.1" class="ltx_text" style="font-size:80%;">77.03</span></td>
<td id="S4.T4.1.1.9.8.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.9.8.6.1" class="ltx_text" style="font-size:80%;">75.26</span></td>
<td id="S4.T4.1.1.9.8.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.9.8.7.1" class="ltx_text" style="font-size:80%;">76.13</span></td>
</tr>
<tr id="S4.T4.1.1.10.9" class="ltx_tr">
<th id="S4.T4.1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.10.9.1.1" class="ltx_text" style="font-size:80%;">P + S</span></th>
<th id="S4.T4.1.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.10.9.2.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T4.1.1.10.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.1.10.9.3.1" class="ltx_text" style="font-size:80%;">16</span></th>
<td id="S4.T4.1.1.10.9.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.10.9.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">70.52</span></td>
<td id="S4.T4.1.1.10.9.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.10.9.5.1" class="ltx_text" style="font-size:80%;">74.80</span></td>
<td id="S4.T4.1.1.10.9.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.10.9.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">82.73</span></td>
<td id="S4.T4.1.1.10.9.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.10.9.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">79.12</span></td>
</tr>
<tr id="S4.T4.1.1.11.10" class="ltx_tr">
<th id="S4.T4.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.1.1.11.10.1.1" class="ltx_text" style="font-size:80%;">P + S + J</span></th>
<th id="S4.T4.1.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.1.1.11.10.2.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T4.1.1.11.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.1.1.11.10.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T4.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.11.10.4.1" class="ltx_text" style="font-size:80%;">69.34</span></td>
<td id="S4.T4.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.11.10.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">78.24</span></td>
<td id="S4.T4.1.1.11.10.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.11.10.6.1" class="ltx_text" style="font-size:80%;">70.20</span></td>
<td id="S4.T4.1.1.11.10.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.11.10.7.1" class="ltx_text" style="font-size:80%;">76.35</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">TABLE V</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">PedSynth (S) as testing dataset. J: JAAD, P: PIE.</span></figcaption>
<table id="S4.T5.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T5.1.1.1.2.1" class="ltx_text" style="font-size:80%;">Train</span></th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T5.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Test</span></th>
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><msub id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T5.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.m1.1.1.2.cmml">N</mi><mi mathsize="80%" id="S4.T5.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T5.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.m1.1.1.2">𝑁</ci><ci id="S4.T5.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">N_{F}</annotation></semantics></math></th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></th>
<th id="S4.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Precision</span></th>
<th id="S4.T5.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.6.1" class="ltx_text" style="font-size:80%;">Recall</span></th>
<th id="S4.T5.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.7.1" class="ltx_text" style="font-size:80%;">F1-score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<th id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">S</span></th>
<th id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">72.23</span></td>
<td id="S4.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">77.33</span></td>
<td id="S4.T5.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.2.1.6.1" class="ltx_text" style="font-size:80%;">83.97</span></td>
<td id="S4.T5.1.1.2.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T5.1.1.2.1.7.1" class="ltx_text" style="font-size:80%;">80.97</span></td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<th id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">S</span></th>
<th id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.3.2.4.1" class="ltx_text" style="font-size:80%;">69.66</span></td>
<td id="S4.T5.1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.3.2.5.1" class="ltx_text" style="font-size:80%;">74.54</span></td>
<td id="S4.T5.1.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.3.2.6.1" class="ltx_text" style="font-size:80%;">82.47</span></td>
<td id="S4.T5.1.1.3.2.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.1.1.3.2.7.1" class="ltx_text" style="font-size:80%;">78.30</span></td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<th id="S4.T5.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.1.4.3.1.1" class="ltx_text" style="font-size:80%;">J + P</span></th>
<th id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">S</span></th>
<th id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">08</span></th>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.3.4.1" class="ltx_text" style="font-size:80%;">71.40</span></td>
<td id="S4.T5.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.3.5.1" class="ltx_text" style="font-size:80%;">73.11</span></td>
<td id="S4.T5.1.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">85.74</span></td>
<td id="S4.T5.1.1.4.3.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.1.1.4.3.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">82.98</span></td>
</tr>
<tr id="S4.T5.1.1.5.4" class="ltx_tr">
<th id="S4.T5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.1.5.4.1.1" class="ltx_text" style="font-size:80%;">J + S</span></th>
<th id="S4.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">J</span></th>
<th id="S4.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S4.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.5.4.4.1" class="ltx_text" style="font-size:80%;">86.22</span></td>
<td id="S4.T5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.5.4.5.1" class="ltx_text" style="font-size:80%;">77.35</span></td>
<td id="S4.T5.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.5.4.6.1" class="ltx_text" style="font-size:80%;">96.19</span></td>
<td id="S4.T5.1.1.5.4.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T5.1.1.5.4.7.1" class="ltx_text" style="font-size:80%;">85.96</span></td>
</tr>
<tr id="S4.T5.1.1.6.5" class="ltx_tr">
<th id="S4.T5.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T5.1.1.6.5.1.1" class="ltx_text" style="font-size:80%;">P + S</span></th>
<th id="S4.T5.1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T5.1.1.6.5.2.1" class="ltx_text" style="font-size:80%;">P</span></th>
<th id="S4.T5.1.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T5.1.1.6.5.3.1" class="ltx_text" style="font-size:80%;">16</span></th>
<td id="S4.T5.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.6.5.4.1" class="ltx_text" style="font-size:80%;">70.52</span></td>
<td id="S4.T5.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.6.5.5.1" class="ltx_text" style="font-size:80%;">74.80</span></td>
<td id="S4.T5.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.6.5.6.1" class="ltx_text" style="font-size:80%;">82.73</span></td>
<td id="S4.T5.1.1.6.5.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.6.5.7.1" class="ltx_text" style="font-size:80%;">79.12</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.15.5.1" class="ltx_text" style="font-size:90%;">TABLE VI</span>: </span><span id="S4.T6.8.4" class="ltx_text" style="font-size:90%;">J: JAAD, P: PIE, S: PedSynth. GT refers to using ground truth skeletons from CARLA. PedGraph+<sup id="S4.T6.8.4.1" class="ltx_sup">⋆</sup> refers to PedGraph+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> but only considers pedestrian skeletons (from AlphaPose) as input information (<math id="S4.T6.6.2.m2.1" class="ltx_Math" alttext="N_{F}=32" display="inline"><semantics id="S4.T6.6.2.m2.1b"><mrow id="S4.T6.6.2.m2.1.1" xref="S4.T6.6.2.m2.1.1.cmml"><msub id="S4.T6.6.2.m2.1.1.2" xref="S4.T6.6.2.m2.1.1.2.cmml"><mi id="S4.T6.6.2.m2.1.1.2.2" xref="S4.T6.6.2.m2.1.1.2.2.cmml">N</mi><mi id="S4.T6.6.2.m2.1.1.2.3" xref="S4.T6.6.2.m2.1.1.2.3.cmml">F</mi></msub><mo id="S4.T6.6.2.m2.1.1.1" xref="S4.T6.6.2.m2.1.1.1.cmml">=</mo><mn id="S4.T6.6.2.m2.1.1.3" xref="S4.T6.6.2.m2.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.6.2.m2.1c"><apply id="S4.T6.6.2.m2.1.1.cmml" xref="S4.T6.6.2.m2.1.1"><eq id="S4.T6.6.2.m2.1.1.1.cmml" xref="S4.T6.6.2.m2.1.1.1"></eq><apply id="S4.T6.6.2.m2.1.1.2.cmml" xref="S4.T6.6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.T6.6.2.m2.1.1.2.1.cmml" xref="S4.T6.6.2.m2.1.1.2">subscript</csymbol><ci id="S4.T6.6.2.m2.1.1.2.2.cmml" xref="S4.T6.6.2.m2.1.1.2.2">𝑁</ci><ci id="S4.T6.6.2.m2.1.1.2.3.cmml" xref="S4.T6.6.2.m2.1.1.2.3">𝐹</ci></apply><cn type="integer" id="S4.T6.6.2.m2.1.1.3.cmml" xref="S4.T6.6.2.m2.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.2.m2.1d">N_{F}=32</annotation></semantics></math> is used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>). For PIE, PedGraph+<sup id="S4.T6.8.4.2" class="ltx_sup">⋆</sup> only considers the <math id="S4.T6.8.4.m4.1" class="ltx_Math" alttext="\sim 30\%" display="inline"><semantics id="S4.T6.8.4.m4.1b"><mrow id="S4.T6.8.4.m4.1.1" xref="S4.T6.8.4.m4.1.1.cmml"><mi id="S4.T6.8.4.m4.1.1.2" xref="S4.T6.8.4.m4.1.1.2.cmml"></mi><mo id="S4.T6.8.4.m4.1.1.1" xref="S4.T6.8.4.m4.1.1.1.cmml">∼</mo><mrow id="S4.T6.8.4.m4.1.1.3" xref="S4.T6.8.4.m4.1.1.3.cmml"><mn id="S4.T6.8.4.m4.1.1.3.2" xref="S4.T6.8.4.m4.1.1.3.2.cmml">30</mn><mo id="S4.T6.8.4.m4.1.1.3.1" xref="S4.T6.8.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.8.4.m4.1c"><apply id="S4.T6.8.4.m4.1.1.cmml" xref="S4.T6.8.4.m4.1.1"><csymbol cd="latexml" id="S4.T6.8.4.m4.1.1.1.cmml" xref="S4.T6.8.4.m4.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.T6.8.4.m4.1.1.2.cmml" xref="S4.T6.8.4.m4.1.1.2">absent</csymbol><apply id="S4.T6.8.4.m4.1.1.3.cmml" xref="S4.T6.8.4.m4.1.1.3"><csymbol cd="latexml" id="S4.T6.8.4.m4.1.1.3.1.cmml" xref="S4.T6.8.4.m4.1.1.3.1">percent</csymbol><cn type="integer" id="S4.T6.8.4.m4.1.1.3.2.cmml" xref="S4.T6.8.4.m4.1.1.3.2">30</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.4.m4.1d">\sim 30\%</annotation></semantics></math> of C/NC cases, which we denote as P*.</span></figcaption>
<table id="S4.T6.11.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.9.1.1" class="ltx_tr">
<th id="S4.T6.9.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.9.1.1.2.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S4.T6.9.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.9.1.1.3.1" class="ltx_text" style="font-size:80%;">Train</span></th>
<th id="S4.T6.9.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.9.1.1.4.1" class="ltx_text" style="font-size:80%;">Test</span></th>
<th id="S4.T6.9.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><math id="S4.T6.9.1.1.1.m1.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.T6.9.1.1.1.m1.1a"><msub id="S4.T6.9.1.1.1.m1.1.1" xref="S4.T6.9.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T6.9.1.1.1.m1.1.1.2" xref="S4.T6.9.1.1.1.m1.1.1.2.cmml">N</mi><mi mathsize="80%" id="S4.T6.9.1.1.1.m1.1.1.3" xref="S4.T6.9.1.1.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.9.1.1.1.m1.1b"><apply id="S4.T6.9.1.1.1.m1.1.1.cmml" xref="S4.T6.9.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.9.1.1.1.m1.1.1.1.cmml" xref="S4.T6.9.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T6.9.1.1.1.m1.1.1.2.cmml" xref="S4.T6.9.1.1.1.m1.1.1.2">𝑁</ci><ci id="S4.T6.9.1.1.1.m1.1.1.3.cmml" xref="S4.T6.9.1.1.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.9.1.1.1.m1.1c">N_{F}</annotation></semantics></math></th>
<th id="S4.T6.9.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.9.1.1.5.1" class="ltx_text" style="font-size:80%;">Accuracy</span></th>
<th id="S4.T6.9.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.9.1.1.6.1" class="ltx_text" style="font-size:80%;">Precision</span></th>
<th id="S4.T6.9.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.9.1.1.7.1" class="ltx_text" style="font-size:80%;">Recall</span></th>
<th id="S4.T6.9.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.9.1.1.8.1" class="ltx_text" style="font-size:80%;">F1-score</span></th>
</tr>
<tr id="S4.T6.11.3.4.1" class="ltx_tr">
<th id="S4.T6.11.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.1.1" class="ltx_text" style="font-size:80%;">PedGNN</span></th>
<th id="S4.T6.11.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.2.1" class="ltx_text" style="font-size:80%;">S (GT)</span></th>
<th id="S4.T6.11.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.3.1" class="ltx_text" style="font-size:80%;">S (GT)</span></th>
<th id="S4.T6.11.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.4.1" class="ltx_text" style="font-size:80%;">08</span></th>
<th id="S4.T6.11.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.5.1" class="ltx_text" style="font-size:80%;">89.29</span></th>
<th id="S4.T6.11.3.4.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.6.1" class="ltx_text" style="font-size:80%;">95.85</span></th>
<th id="S4.T6.11.3.4.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.7.1" class="ltx_text" style="font-size:80%;">88.69</span></th>
<th id="S4.T6.11.3.4.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.4.1.8.1" class="ltx_text" style="font-size:80%;">92.14</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.11.3.5.1" class="ltx_tr">
<td id="S4.T6.11.3.5.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.1.1" class="ltx_text" style="font-size:80%;">PedGNN</span></td>
<td id="S4.T6.11.3.5.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.2.1" class="ltx_text" style="font-size:80%;">J</span></td>
<td id="S4.T6.11.3.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.3.1" class="ltx_text" style="font-size:80%;">J</span></td>
<td id="S4.T6.11.3.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.4.1" class="ltx_text" style="font-size:80%;">18</span></td>
<td id="S4.T6.11.3.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.5.1" class="ltx_text" style="font-size:80%;">80.32</span></td>
<td id="S4.T6.11.3.5.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">84.72</span></td>
<td id="S4.T6.11.3.5.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">87.91</span></td>
<td id="S4.T6.11.3.5.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.5.1.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">85.29</span></td>
</tr>
<tr id="S4.T6.10.2.2" class="ltx_tr">
<td id="S4.T6.10.2.2.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T6.10.2.2.1.1" class="ltx_text" style="font-size:80%;">PedGraph+</span><sup id="S4.T6.10.2.2.1.2" class="ltx_sup"><span id="S4.T6.10.2.2.1.2.1" class="ltx_text" style="font-size:80%;">⋆</span></sup>
</td>
<td id="S4.T6.10.2.2.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.10.2.2.2.1" class="ltx_text" style="font-size:80%;">J</span></td>
<td id="S4.T6.10.2.2.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.10.2.2.3.1" class="ltx_text" style="font-size:80%;">J</span></td>
<td id="S4.T6.10.2.2.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.10.2.2.4.1" class="ltx_text" style="font-size:80%;">32</span></td>
<td id="S4.T6.10.2.2.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.10.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">83.85</span></td>
<td id="S4.T6.10.2.2.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.10.2.2.6.1" class="ltx_text" style="font-size:80%;">53.76</span></td>
<td id="S4.T6.10.2.2.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.10.2.2.7.1" class="ltx_text" style="font-size:80%;">59.21</span></td>
<td id="S4.T6.10.2.2.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.10.2.2.8.1" class="ltx_text" style="font-size:80%;">56.36</span></td>
</tr>
<tr id="S4.T6.11.3.6.2" class="ltx_tr">
<td id="S4.T6.11.3.6.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.1.1" class="ltx_text" style="font-size:80%;">PedGNN</span></td>
<td id="S4.T6.11.3.6.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.2.1" class="ltx_text" style="font-size:80%;">P</span></td>
<td id="S4.T6.11.3.6.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.3.1" class="ltx_text" style="font-size:80%;">P</span></td>
<td id="S4.T6.11.3.6.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.4.1" class="ltx_text" style="font-size:80%;">08</span></td>
<td id="S4.T6.11.3.6.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.5.1" class="ltx_text" style="font-size:80%;">68.11</span></td>
<td id="S4.T6.11.3.6.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.6.1" class="ltx_text" style="font-size:80%;">66.98</span></td>
<td id="S4.T6.11.3.6.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">68.36</span></td>
<td id="S4.T6.11.3.6.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.6.2.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">69.81</span></td>
</tr>
<tr id="S4.T6.11.3.3" class="ltx_tr">
<td id="S4.T6.11.3.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T6.11.3.3.1.1" class="ltx_text" style="font-size:80%;">PedGraph+</span><sup id="S4.T6.11.3.3.1.2" class="ltx_sup"><span id="S4.T6.11.3.3.1.2.1" class="ltx_text" style="font-size:80%;">⋆</span></sup>
</td>
<td id="S4.T6.11.3.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.3.2.1" class="ltx_text" style="font-size:80%;">P*</span></td>
<td id="S4.T6.11.3.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.3.3.1" class="ltx_text" style="font-size:80%;">P*</span></td>
<td id="S4.T6.11.3.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.3.4.1" class="ltx_text" style="font-size:80%;">32</span></td>
<td id="S4.T6.11.3.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.3.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">79.15</span></td>
<td id="S4.T6.11.3.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">77.91</span></td>
<td id="S4.T6.11.3.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.3.7.1" class="ltx_text" style="font-size:80%;">36.51</span></td>
<td id="S4.T6.11.3.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.11.3.3.8.1" class="ltx_text" style="font-size:80%;">49.72</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">User</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.2.1.1" class="ltx_text" style="font-size:90%;">TABLE VII</span>: </span><span id="S4.T7.3.2" class="ltx_text" style="font-size:90%;">Memory footprint and inference time of PedGNN and different models from the state-of-the-art. All times are computed on an NVIDIA GTX 1080 GPU. We have extracted these times from the respective papers.</span></figcaption>
<table id="S4.T7.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.4.1.1" class="ltx_tr">
<th id="S4.T7.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">Model</th>
<th id="S4.T7.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">Size (MB)</th>
<th id="S4.T7.4.1.1.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">Inference time (ms)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.4.2.1" class="ltx_tr">
<td id="S4.T7.4.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">PCPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S4.T7.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">118.8</td>
<td id="S4.T7.4.2.1.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">38.6</td>
</tr>
<tr id="S4.T7.4.3.2" class="ltx_tr">
<td id="S4.T7.4.3.2.1" class="ltx_td ltx_align_left" style="padding-left:7.0pt;padding-right:7.0pt;">Global PCPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S4.T7.4.3.2.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">374.2</td>
<td id="S4.T7.4.3.2.3" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:7.0pt;padding-right:7.0pt;">70.83</td>
</tr>
<tr id="S4.T7.4.4.3" class="ltx_tr">
<td id="S4.T7.4.4.3.1" class="ltx_td ltx_align_left" style="padding-left:7.0pt;padding-right:7.0pt;">FUSSI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S4.T7.4.4.3.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">8.4</td>
<td id="S4.T7.4.4.3.3" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:7.0pt;padding-right:7.0pt;">34.92</td>
</tr>
<tr id="S4.T7.4.5.4" class="ltx_tr">
<td id="S4.T7.4.5.4.1" class="ltx_td ltx_align_left" style="padding-left:7.0pt;padding-right:7.0pt;">PedGraph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S4.T7.4.5.4.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.22</td>
<td id="S4.T7.4.5.4.3" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:7.0pt;padding-right:7.0pt;">29.01</td>
</tr>
<tr id="S4.T7.4.6.5" class="ltx_tr">
<td id="S4.T7.4.6.5.1" class="ltx_td ltx_align_left" style="padding-left:7.0pt;padding-right:7.0pt;">PedGraph+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S4.T7.4.6.5.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.28</td>
<td id="S4.T7.4.6.5.3" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:7.0pt;padding-right:7.0pt;">5.47</td>
</tr>
<tr id="S4.T7.4.7.6" class="ltx_tr">
<td id="S4.T7.4.7.6.1" class="ltx_td ltx_align_left" style="padding-left:7.0pt;padding-right:7.0pt;">TEP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S4.T7.4.7.6.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">12.8</td>
<td id="S4.T7.4.7.6.3" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:7.0pt;padding-right:7.0pt;">2.85</td>
</tr>
<tr id="S4.T7.4.8.7" class="ltx_tr">
<td id="S4.T7.4.8.7.1" class="ltx_td ltx_align_left" style="padding-left:7.0pt;padding-right:7.0pt;">V-PedCross <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S4.T7.4.8.7.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">4.8</td>
<td id="S4.T7.4.8.7.3" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
</tr>
<tr id="S4.T7.4.9.8" class="ltx_tr">
<td id="S4.T7.4.9.8.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">PedGNN (Ours)</td>
<td id="S4.T7.4.9.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T7.4.9.8.2.1" class="ltx_text ltx_font_bold">0.027</span></td>
<td id="S4.T7.4.9.8.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T7.4.9.8.3.1" class="ltx_text ltx_font_bold">0.58</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Experiments and discussion</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p">We start the experiments by evaluating how effective PedSynth is training our PedGNN model to perform on the JAAD and PIE testing sets. Table <a href="#S4.T3" title="TABLE III ‣ IV-B Training protocol ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> shows the results. The <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">N</mi><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑁</ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">N_{F}</annotation></semantics></math> value refers to the number of input frames (per pedestrian skeletons) that was best for each case according to the previously described training protocol. Training on PedSynth requires considering more frames than using the respective JAAD/PIE training data. However, this does not affect the prediction latency since, as we have mentioned before, we use a temporal sliding window of a 1-frame step. Moreover, in an NVIDIA GTX 1080 GPU, for <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="N_{F}=32" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><msub id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2.2" xref="S4.SS3.p1.2.m2.1.1.2.2.cmml">N</mi><mi id="S4.SS3.p1.2.m2.1.1.2.3" xref="S4.SS3.p1.2.m2.1.1.2.3.cmml">F</mi></msub><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><eq id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></eq><apply id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2.2">𝑁</ci><ci id="S4.SS3.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.p1.2.m2.1.1.2.3">𝐹</ci></apply><cn type="integer" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">N_{F}=32</annotation></semantics></math> PedGNN only takes <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="\sim 0.6" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mi id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml"></mi><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">∼</mo><mn id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">absent</csymbol><cn type="float" id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">\sim 0.6</annotation></semantics></math>ms to perform the inference. In terms of accuracy, training on the respective real-world datasets is better than training on PedSynth. However, we can see how it is not the case for F1-score, which is a more unbiased metric than accuracy when the testing data distribution presents a class unbalanced. Table <a href="#S4.T2" title="TABLE II ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows that this is the case here since both JAAD and PIE have testing sets clearly biased toward the crossing (C) class. Thus, we think PedSynth is an effective dataset for training C/NC prediction models.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.3" class="ltx_p">At this point, it is worth commenting that, as shown in Table <a href="#S3.T1" title="TABLE I ‣ III-A The ARCANE framework ‣ III Methods ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, PedSynth provides a skeleton GT for each pedestrian (coming from the CARLA simulator). The joints used by PedGNN are a subset of those provided by the CARLA simulator, so the mapping is straightforward. Moreover, fitting confidences <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">c</mi><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝑐</ci><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">c_{j}</annotation></semantics></math> can be set to 1 since skeletons are perfectly fitting pedestrians. Therefore, this raises the question of using such skeletons as GT for training with PedSynth instead of applying AlphaPose to the synthetic pedestrians. We did the corresponding experiments, however, F1-score dropped <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="\sim 10" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml"></mi><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">∼</mo><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\sim 10</annotation></semantics></math> points when testing on JAAD and <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="\sim 6" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml"></mi><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">∼</mo><mn id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">absent</csymbol><cn type="integer" id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">\sim 6</annotation></semantics></math> for PIE. In other words, a synth-to-real domain gap is induced by the use of different skeleton sources at training (GT) and testing (Alphapose) time. We leave future work to investigate more in deep the underlying reasons for the domain gap and keep using AlphaPose for all the training runs involving PedSynth.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.2" class="ltx_p">Sometimes we may have real-world training data labeled for C/NC prediction, as is the case of PIE and JAAD. Then, it is also interesting to see if the synthetic data at hand can act as a complement, giving rise to better-performing models. Note that by using the same skeleton fitting method we avoid the synth-to-real domain gap provided this method performs well in the real and synthetic domains. According to our experiments, AlphaPose fulfills so. Therefore, we have combined PedSynth training data with JAAD and/or PIE training data for assessing the complementarity of these datasets. Table <a href="#S4.T4" title="TABLE IV ‣ IV-B Training protocol ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the corresponding results, where we also include those in Table <a href="#S4.T3" title="TABLE III ‣ IV-B Training protocol ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> for easier comparison. We can see that, when testing in JAAD, combining JAAD and PedSynth gives better results than using JAAD or PedSynth alone and even than combining JAAD and PIE. In fact, it seems that PIE produces negative transfer when combining the three datasets. On the other hand, since the testing set of PIE has around <math id="S4.SS3.p3.1.m1.1" class="ltx_math_unparsed" alttext="{4\times}" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1b"><mn id="S4.SS3.p3.1.m1.1.1">4</mn><mo lspace="0.222em" id="S4.SS3.p3.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">{4\times}</annotation></semantics></math> more C-frames than JAAD, and around <math id="S4.SS3.p3.2.m2.1" class="ltx_math_unparsed" alttext="{9\times}" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1b"><mn id="S4.SS3.p3.2.m2.1.1">9</mn><mo lspace="0.222em" id="S4.SS3.p3.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">{9\times}</annotation></semantics></math> more NC-frames, results on PIE are of special interest. We can see that, when testing in PIE, combining PIE and PedSynth gives rise to the best results in terms of accuracy and F1-score. Thus, we think that PedSynth can complement real-world datasets for training purposes.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2401.06757/assets/figures/1f.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2401.06757/assets/figures/2f.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2401.06757/assets/figures/4e.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2401.06757/assets/figures/1e.png" id="S4.F5.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Performance of PedGNN trained on JAAD+PedSynth and tested on JAAD. Cases (a) and (b) are fully successful, while in cases (c) and (d) there are C/NC prediction discrepancies with the labels provided by human labelers (GT). Time in each sequence runs from top-left to bottom-right.</span></figcaption>
</figure>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">We can also assume that we use PedSynth for testing purposes. Table <a href="#S4.T5" title="TABLE V ‣ IV-B Training protocol ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> shows the results corresponding to training with the real-world training sets and testing in the PedSynth testing set. For easier visual comparison, we also include the best models obtained when testing in real-world testing sets. These correspond to training on the respective training set plus the training set of PedSynth. We can see that performance metrics report comparable values when testing in real-world sets and in our synthetic set. Thus, we think PedSynth can play the role of the testing set too; in other words, it is not easier than their real-world counterparts.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.2" class="ltx_p">At this point, we put the focus on the PedGNN model. On the one hand, we assess its potential by using PedSynth and the associated pedestrian GT skeletons, so that results are not influenced by the skeleton fitting method in place (here AlphaPose). Moreover, we compare our results with the state-of-the-art method on C/NC prediction here named PedGraph+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Table <a href="#S4.T6" title="TABLE VI ‣ IV-B Training protocol ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the results. For PedGraph+ we have copied the results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> when only AlphaPose-based skeletons are considered as input. However, for the PIE dataset, only a portion of the data is considered in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, roughly the <math id="S4.SS3.p5.1.m1.1" class="ltx_Math" alttext="30\%" display="inline"><semantics id="S4.SS3.p5.1.m1.1a"><mrow id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml"><mn id="S4.SS3.p5.1.m1.1.1.2" xref="S4.SS3.p5.1.m1.1.1.2.cmml">30</mn><mo id="S4.SS3.p5.1.m1.1.1.1" xref="S4.SS3.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><apply id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p5.1.m1.1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p5.1.m1.1.1.2.cmml" xref="S4.SS3.p5.1.m1.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">30\%</annotation></semantics></math>. We can observe that PedGNN has great potential of providing good performance, which can be seen when training and testing with perfect pedestrian skeletons (GT). Note that F1-score is <math id="S4.SS3.p5.2.m2.1" class="ltx_Math" alttext="\sim 92\%" display="inline"><semantics id="S4.SS3.p5.2.m2.1a"><mrow id="S4.SS3.p5.2.m2.1.1" xref="S4.SS3.p5.2.m2.1.1.cmml"><mi id="S4.SS3.p5.2.m2.1.1.2" xref="S4.SS3.p5.2.m2.1.1.2.cmml"></mi><mo id="S4.SS3.p5.2.m2.1.1.1" xref="S4.SS3.p5.2.m2.1.1.1.cmml">∼</mo><mrow id="S4.SS3.p5.2.m2.1.1.3" xref="S4.SS3.p5.2.m2.1.1.3.cmml"><mn id="S4.SS3.p5.2.m2.1.1.3.2" xref="S4.SS3.p5.2.m2.1.1.3.2.cmml">92</mn><mo id="S4.SS3.p5.2.m2.1.1.3.1" xref="S4.SS3.p5.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.2.m2.1b"><apply id="S4.SS3.p5.2.m2.1.1.cmml" xref="S4.SS3.p5.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p5.2.m2.1.1.1.cmml" xref="S4.SS3.p5.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.p5.2.m2.1.1.2.cmml" xref="S4.SS3.p5.2.m2.1.1.2">absent</csymbol><apply id="S4.SS3.p5.2.m2.1.1.3.cmml" xref="S4.SS3.p5.2.m2.1.1.3"><csymbol cd="latexml" id="S4.SS3.p5.2.m2.1.1.3.1.cmml" xref="S4.SS3.p5.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.p5.2.m2.1.1.3.2.cmml" xref="S4.SS3.p5.2.m2.1.1.3.2">92</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.2.m2.1c">\sim 92\%</annotation></semantics></math>. Of course, there is room for improvement. Compared to PedGraph+ assuming the same input data (AlphaPose-based skeletons), we can see how PedGraph+ performs better in terms of accuracy, but significantly worse when using the more representative F1-score metric. Moreover, we can see in Table <a href="#S4.T7" title="TABLE VII ‣ IV-B Training protocol ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> how in terms of memory footprint and inference speed PedGNN is significantly more lightweight and faster.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">Finally, as an example of qualitative results, Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-C Experiments and discussion ‣ IV Experimental Results ‣ Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the performance of PedGNN trained on JAAD+PedSynth and tested on JAAD. In case (a), while the ego-vehicle turns to the right, the intention of a pedestrian that started to cross in the left is properly predicted from the very beginning. In case (b), while the ego-vehicle
moves straight forward, a pedestrian standing still at the border of the road
is properly predicted as a non-crossing pedestrian.
In cases (c) and (d), PedGNN requires more frames to reach the proper prediction.
In case (c) the pedestrian seems to take the crossing decision later than in case (a), so predicting the intentions required some additional time. In case (d), the pedestrian seems to start crossing in front of the ego-vehicle in a parking area, but finally, it does not. In fact, for us it is unclear if the GT is right, after all, it is based on human labelers and, therefore, there is subjectivity. For instance, if the initial frames have been labeled after looking at what the pedestrian did at the final ones, this would be like using the future to predict the present/past, which cannot be done by the temporal sliding window mechanism used to process onboard continuous image sequences.
</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we have introduced our framework ARCANE which allows the generation of synthetic datasets labeled for the pedestrian C/NC prediction task. It works on top of the CARLA simulator so being aligned with the autonomous driving research community. Advanced users can programmatically design their pedestrian C/NC scenarios, the rest can adjust a configuration file to use existing scenarios. For example, we have generated the PedSynth dataset by using ARCANE. It is diverse and contains a large amount of pedestrian C/NC cases. We have shown its usefulness by running an extensive set of experiments. We have seen that it can play the role of the training set alone, it can complement real-world training sets, and it can play the role of the testing set. Most experiments are based on our model PedGNN, also introduced in this paper. It processes sequences of pedestrian skeletons to produce C/NC predictions. Our experiments show that PedGNN produces state-of-the-art results, despite being significantly more lightweight and faster than previous C/NC prediction models. In future work, we plan to use PedSynth and PedGNN to address the synth-to-real unsupervised domain adaptation problem for the pedestrian C/NC prediction task.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Lina Achaji, Julien Moreau, Thibault Fouqueray, Francois Aioun, and Francois
Charpillet.

</span>
<span class="ltx_bibblock">Is attention to bounding boxes all you need for pedestrian action
prediction?

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Intelligent Vehicles Symposium (IV)</span>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Jie Bai, Xing Fang, Jianwu Fang, Jianru Xue, and Changwei Yuan.

</span>
<span class="ltx_bibblock">Deep virtual-to-real distillation for pedestrian crossing prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Intelligent Transportation Systems Conference (ITSC)</span>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Smail Bouhsain, Saeed Saadatnejad, and Alexandre Alahi.

</span>
<span class="ltx_bibblock">Pedestrian intention prediction: A multi-task perspective.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Symposium of the European Association for Research in
Transportation (hEART)</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Yohann Cabon, Naila Murray, and Martin Humenberger.

</span>
<span class="ltx_bibblock">Virtual KITTI 2.

</span>
<span class="ltx_bibblock">arXiv:2001.10773, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Pablo Rodrigo Gantier Cadena, Yeqiang Qian, Chunxiang Wang, and Ming Yang.

</span>
<span class="ltx_bibblock">Pedestrian graph +: A fast pedestrian crossing prediction model based
on graph convolutional networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Intelligent Transportation Systems</span>,
23:21050–21061, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Pablo Rodrigo Gantier Cadena, Ming Yang, Yeqiang Qian, and Chunxiang Wang.

</span>
<span class="ltx_bibblock">Pedestrian graph: Pedestrian crossing prediction based on 2D pose
estimation and graph convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Intelligent Transportation Systems Conference (ITSC)</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yase Sheikh.

</span>
<span class="ltx_bibblock">OpenPose: Realtime multi-person 2d pose estimation using part
affinity fields.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Pattern Analysis and Machine Intelligence</span>,
43:172–186, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Realtime multi-person 2D pose estimation using part affinity
fields.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Computer Vision and Pattern Recognition
(CVPR)</span>, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ernest Cheung, Anson Wong, Aniket Bera, and Dinesh Manocha.

</span>
<span class="ltx_bibblock">MixedPeds: Pedestrian detection in unannotated videos using
synthetically generated human-agents for training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">AAAI Conference on Artificial Intelligence</span>, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Gabriela Csurka, Riccardo Volpi, and Boris Chidlovskii.

</span>
<span class="ltx_bibblock">Semantic image segmentation: Two decades of research.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Foundations and Trends in Computer Graphics and Vision</span>,
14:1–162, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio M. López, and
Vladlen Koltun.

</span>
<span class="ltx_bibblock">CARLA: An open urban driving simulator.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Conference on Robot Learning (CoRL)</span>, 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Volker Eiselein Erik Bochinski and Tomas Sikora.

</span>
<span class="ltx_bibblock">Training a convolutional neural network for multi-class object
detection using solely virtual world data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">International Conference on Advanced Video and Signal Based
Surveillance (AVSS)</span>, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu,
Yong-Lu Li, and Cewu Lu.

</span>
<span class="ltx_bibblock">AlphaPose: Whole-body regional multi-person pose estimation and
tracking in real-time.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Pattern Analysis and Machine Intelligence</span>,
45:7157–7173, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.

</span>
<span class="ltx_bibblock">RMPE: Regional multi-person pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision (ICCV)</span>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Zhijie Fang and Antonio M. López.

</span>
<span class="ltx_bibblock">Is the pedestrian going to cross? answering by 2d pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Intelligent Vehicles Symposium (IV)</span>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Joseph Gesnouin, Steve Pechberti, Bogdan Stanciulescu, and Fabien Moutarde.

</span>
<span class="ltx_bibblock">TrouSPI-Net: Spatio-temporal attention on parallel atrous
convolutions and U-GRUs for skeletal pedestrian crossing prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International Conference on Automatic Face and Gesture
Recognition(FG)</span>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Akhil Gurram, Ahmet F. Tuna, Fengyi Shen, Onay Urfalioglu, and Antonio M.
López.

</span>
<span class="ltx_bibblock">Monocular depth estimation through virtual-world supervision and
real-world SFM self-supervision.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Intelligent Transportation Systems</span>,
23:12738–12751, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Je-Seok Ham, Kangmin Bae, and Jinyoung Moon.

</span>
<span class="ltx_bibblock">MCIP: Multi-stream network for pedestrian crossing intention
prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)–Workshops</span>,
2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Hironori Hattori, Vishnu N. Boddeti, Kris Kitani, and Takeo Kanade.

</span>
<span class="ltx_bibblock">Learning scene-specific pedestrian detectors without real data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Computer Vision and Pattern Recognition
(CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Christoph G. Keller and Dariu M. Gavrila.

</span>
<span class="ltx_bibblock">Will the pedestrian cross? a study on pedestrian path prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Intelligent Transportation Systems</span>,
15:494–506, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wonhui Kim, Manikandasriram Srinivasan Ramanagopal, Charlie Barto, Ming-Yuan
Yu, Karl Rosaen, Nicholas Goumas, Ram Vasudevan, and Matthew
Johnson-Roberson.

</span>
<span class="ltx_bibblock">PedX: Benchmark dataset for metric 3-D pose estimation of
pedestrians in complex urban intersections.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, 4:1940–1947, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Thomas N. Kipf and Max Welling.

</span>
<span class="ltx_bibblock">Semi-supervised classification with graph convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representation (ICLR)</span>,
2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Iuliia Kotseruba, Amir Rasouli, and John K. Tsotsos.

</span>
<span class="ltx_bibblock">Benchmark for evaluating pedestrian action prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Winter conf. on Applications of Computer Vision (WACV)</span>,
2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Google Auto LLC.

</span>
<span class="ltx_bibblock">Google self-driving car testing report on disengagements of
autonomous mode, December 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Javier Marin, David Gerónimo, David Vázquez, and Antonio M. López.

</span>
<span class="ltx_bibblock">Learning appearance in virtual scenarios for pedestrian detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Computer Vision and Pattern Recognition
(CVPR)</span>, 2010.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Francesco Piccoli, Rajarathnam Balakrishnan, Maria Jesus Perez, Moraldeepsingh
Sachdeo, Carlos Nunez, Matthew Tang, Kajsa Andreasson, Kalle Bjurek, Ria Dass
Raj, Ebba Davidsson, Colin Eriksson, Victor Hagman, Jonas Sjoberg, Ying Li,
L. Srikar Muppirisetty, and Sohini Roychowdhury.

</span>
<span class="ltx_bibblock">FuSSI-Net: Fusion of spatio-temporal skeletons for intention
prediction network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Asilomar Conference on Signals, Systems, and Computers</span>,
2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Amir Rasouli, Iuliia Kotseruba, Toni Kunic, and John K. Tsotsos.

</span>
<span class="ltx_bibblock">PIE: A large-scale dataset and models for pedestrian intention
estimation and trajectory prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision (ICCV)</span>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Amir Rasouli, Iuliia Kotseruba, and John K Tsotsos.

</span>
<span class="ltx_bibblock">are they going to cross? a benchmark dataset and baseline for
pedestrian crosswalk behavior.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision
(ICCV)–Workshops</span>, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Haziq Razali, Taylor Mordan, and Alexandre Alahi.

</span>
<span class="ltx_bibblock">Pedestrian intention prediction: A convolutional bottom-up multi-task
approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Transportation Research Part C: Emerging Technologies</span>,
130:103259, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Playing for data: Ground truth from computer games.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Germán Ros, Laura Sellart, Joanna Materzynska, David Vázquez, and Antonio
M. López.

</span>
<span class="ltx_bibblock">The SYNTHIA dataset: a large collection of synthetic images for
semantic segmentation of urban scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Computer Vision and Pattern Recognition
(CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Friederike Schneemann and Patrick Heinemann.

</span>
<span class="ltx_bibblock">Context-based detection of pedestrian crossing intention for
autonomous driving in urban environments.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Intelligent Robots and Systems (IROS)</span>, 2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Nicolas Schneider and Dariu M. Gavrila.

</span>
<span class="ltx_bibblock">Pedestrian path prediction with recursive bayesian filters: A
comparative study.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">German Conference on Pattern Recognition (GCPR)</span>, 2013.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor.

</span>
<span class="ltx_bibblock">AirSim: High-fidelity visual and physical simulation for autonomous
vehicles.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Field and Service Robotics (FSR)</span>, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Thomas Stauner, Frederik Blank, Michael Fürst, Johannes Günther,
Korbinian Hagn, Philipp Heidenreich, Markus Huber, Bastian Knerr, Thomas
Schulik, and Karl-Ferdinand Lei.

</span>
<span class="ltx_bibblock">SynPeDS: A synthetic dataset for pedestrian detection in urban
traffic scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">ACM Computer Science in Cars Symposium</span>, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Fan Wang, Jie Bai, and Jianwu Fang.

</span>
<span class="ltx_bibblock">Pedestrian crossing prediction based on invariant feature extraction
of cross-spectral images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">International Conference on Autonomous Unmanned Systems
(ICAUS)</span>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Mei Wang and Weihong Deng.

</span>
<span class="ltx_bibblock">Deep visual domain adaptation: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 312:135–153, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Maciej Wielgosz, Antonio M López, and Muhammad Naveed Riaz.

</span>
<span class="ltx_bibblock">CARLA-BSP: a simulated dataset with pedestrians.

</span>
<span class="ltx_bibblock">arXiv:2305.00204, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Magnus Wrenninge and Jonas Unger.

</span>
<span class="ltx_bibblock">Synscapes: A photorealistic synthetic dataset for street scene
parsing.

</span>
<span class="ltx_bibblock">arXiv:1810.08705, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Dongfang Yang, Haolin Zhang, Ekim Yurtsever, Keith A. Redmill, and Umit
Ozguner.

</span>
<span class="ltx_bibblock">Predicting pedestrian crossing intention with feature fusion and
spatio-temporal attention.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Intelligent Transportation Systems</span>,
7:221–230, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.06756" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.06757" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.06757">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.06757" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.06758" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 09:51:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
