<article class="ltx_document ltx_authors_1line ltx_pruned_first">
 <h1 class="ltx_title ltx_title_document">
  MultiPLY: A Multisensory Object-Centric
  <br class="ltx_break"/>
  Embodied Large Language Model in 3D World
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Yining Hong
    <sup class="ltx_sup" id="id12.10.id1">
     <span class="ltx_text ltx_font_italic" id="id12.10.id1.1">
      2,3
     </span>
    </sup>
    ,
Zishuo Zheng
    <sup class="ltx_sup" id="id13.11.id2">
     <span class="ltx_text ltx_font_italic" id="id13.11.id2.1">
      1
     </span>
    </sup>
    ,
Peihao Chen
    <sup class="ltx_sup" id="id14.12.id3">
     <span class="ltx_text ltx_font_italic" id="id14.12.id3.1">
      1
     </span>
    </sup>
    ,
Yian Wang
    <sup class="ltx_sup" id="id15.13.id4">
     <span class="ltx_text ltx_font_italic" id="id15.13.id4.1">
      1
     </span>
    </sup>
    ,
Junyan Li
    <sup class="ltx_sup" id="id16.14.id5">
     <span class="ltx_text ltx_font_italic" id="id16.14.id5.1">
      1
     </span>
    </sup>
    ,
Chuang Gan
    <sup class="ltx_sup" id="id17.15.id6">
     <span class="ltx_text ltx_font_italic" id="id17.15.id6.1">
      1,3
     </span>
    </sup>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id18.16.id7">
     1
    </sup>
    UMass Amherst,
    <sup class="ltx_sup" id="id19.17.id8">
     2
    </sup>
    UCLA,
    <sup class="ltx_sup" id="id20.18.id9">
     3
    </sup>
    MIT-IBM Watson AI Lab
    <br class="ltx_break"/>
    <br class="ltx_break"/>
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://vis-www.cs.umass.edu/multiply" target="_blank" title="">
     https://vis-www.cs.umass.edu/multiply
    </a>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id21.id1">
   Human beings possess the capability to multiply a mélange of multisensory cues while actively exploring and interacting with the 3D world.
Current multi-modal large language models, however, passively absorb sensory data as inputs,
lacking the capacity to actively interact with the objects in the 3D environment and
dynamically collect their multisensory information. To usher in the study of this area, we propose MultiPLY, a multisensory embodied large language model that could incorporate multisensory interactive data, including visual, audio, tactile, and thermal information into large language models, thereby establishing the correlation among words, actions, and percepts. To this end, we first collect Multisensory Universe, a large-scale multisensory interaction dataset comprising 500k data by deploying an LLM-powered embodied agent to engage with the 3D environment. To perform instruction tuning with pre-trained LLM on such generated data, we first encode the 3D scene as abstracted object-centric representations, and then introduce action tokens denoting that the embodied agent takes certain actions within the environment, as well as state tokens that represent the multisensory state observations of the agent at each time step. In the inference time, MultiPLY could generate action tokens, instructing the agent to take the action in the environment and obtain the next multisensory state observation. The observation is then appended back to the LLM via state tokens to generate subsequent text or action tokens. We demonstrate that MultiPLY outperforms baselines by a large margin through a diverse set of embodied tasks involving object retrieval, tool use, multisensory captioning, and task decomposition.
  </p>
 </div>
 <div class="ltx_logical-block" id="id11">
  <div class="ltx_para" id="id11.p1">
   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="195" id="id10.g1" src="/html/2401.08577/assets/x1.png" width="442"/>
  </div>
  <figure class="ltx_figure ltx_align_center" id="S0.F1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">
      Figure 1
     </span>
     :
    </span>
    <span class="ltx_text" id="S0.F1.4.2" style="font-size:90%;">
     We propose MultiPLY, a multisensory embodied LLM that encodes object-centric multisensory representations (
     <span class="ltx_text ltx_font_italic" id="S0.F1.4.2.1">
      e.g.,
     </span>
     visual, audio, tactile, and thermal), by deploying an embodied agent to engage with the 3D environment. MultiPLY excels at multiple tasks including multisensory captioning, question answering, dialogue, manipulation, navigation, tool use, task decomposition, and so on.
    </span>
   </figcaption>
  </figure>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Human beings inhabit an extraordinary multisensory world - one in which we constantly explore and interact with the 3D environment, collecting and analyzing a mélange of sensory data to accomplish various tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib56" title="">
      <span class="ltx_text" style="font-size:90%;">
       56
      </span>
     </a>
     ]
    </cite>
    .
Picture yourself situated within an embodied environment depicted as Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    . To reason about the question “is the donut ready for eating”, you begin by hearing the microwave beep. Subsequently, you decide to investigate whether the donut is inside the microwave. Once you locate the donut, you may touch it, sensing its hardness and coldness, leading you to the conclusion that the donut is not yet ready.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Existing multi-modal large language models (
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">
     e.g.,
    </span>
    LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib39" title="">
      <span class="ltx_text" style="font-size:90%;">
       39
      </span>
     </a>
     ]
    </cite>
    , Flamingo
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      <span class="ltx_text" style="font-size:90%;">
       1
      </span>
     </a>
     ]
    </cite>
    , BLIP-2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      <span class="ltx_text" style="font-size:90%;">
       37
      </span>
     </a>
     ]
    </cite>
    , PaLM-E
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      <span class="ltx_text" style="font-size:90%;">
       12
      </span>
     </a>
     ]
    </cite>
    ) excel at numerous vision-language tasks. However, they mainly focus on 2D scene understanding, struggling to reason about and interact with 3D environments. Recent works such as 3D-LLM
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib32" title="">
      <span class="ltx_text" style="font-size:90%;">
       32
      </span>
     </a>
     ]
    </cite>
    take preliminary steps to encode holistic 3D point clouds as inputs and show impressive results on 3D reasoning tasks, while suffering from expensive training and inefficient reasoning for objects.
More importantly, these models fall short of the ability to capture multisensory information that goes beyond vision and language.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Efforts have been made to bind representations from different modalities
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib28" title="">
      <span class="ltx_text" style="font-size:90%;">
       28
      </span>
     </a>
     ]
    </cite>
    , and adapt them to pre-trained LLMs
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib31" title="">
      <span class="ltx_text" style="font-size:90%;">
       31
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib40" title="">
      <span class="ltx_text" style="font-size:90%;">
       40
      </span>
     </a>
     ]
    </cite>
    . However, they often focus on a single object
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib30" title="">
      <span class="ltx_text" style="font-size:90%;">
       30
      </span>
     </a>
     ]
    </cite>
    or 2D image
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib28" title="">
      <span class="ltx_text" style="font-size:90%;">
       28
      </span>
     </a>
     ]
    </cite>
    , unable to encode a large 3D environment and
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">
     interact
    </span>
    with the 3D embodied environment. For example, to address a question illustrated in Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , a human would need to touch the donut to sense its softness and temperature, a capability well beyond the current scope of multi-modal LLMs.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Looking ahead, challenges inevitably exist for building embodied multisensory large language models. The first challenge resides in the paucity of multisensory interaction data for training such an LLM.
The next challenge lies in the appropriate representations of the 3D scenes and multisensory information of the objects.
Humans could hold a coarse impression of the scene by abstracting the scene as an object-centric representation and attending to the object details when further interacting with the objects. It’s essential for LLMs to also be able to flexibly switch between an abstracted object-centric representation and detailed multisensory information of the objects.
Lastly, existing LLMs are not tailored for instruction tuning with interaction data. They often take passive data as inputs and generate single-step outputs, incapable of connecting the words, actions, and percepts to engage with an embodied environment.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    To this end, we propose MultiPLY, a multisensory embodied LLM that could encode multisensory object-centric representations, including visual, audio, tactile, and thermal information, by deploying an LLM-powered agent to engage with the 3D environment. We first collect Multisensory Universe, a large-scale multisensory dataset comprising 500k data collected by an agent actively engaging with 3D embodied environments. We utilize the 3D environments from Habitat-Matterport 3D (HM3D) dataset
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib46" title="">
      <span class="ltx_text" style="font-size:90%;">
       46
      </span>
     </a>
     ]
    </cite>
    , and enrich the environments by adding interactive objects with rich sensory data from ObjectFolder
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib20" title="">
      <span class="ltx_text" style="font-size:90%;">
       20
      </span>
     </a>
     ]
    </cite>
    and Objaverse
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib11" title="">
      <span class="ltx_text" style="font-size:90%;">
       11
      </span>
     </a>
     ]
    </cite>
    . We prompt ChatGPT to create the input and output data of tasks ranging from multisensory captioning, question answering, dialogue, manipulation, task decomposition, and so on. An embodied agent explores the environment and interacts with the objects in the environment to get multisensory observations of these tasks.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    To perform instruction tuning on such generated data, we first encode the 3D scene as an abstracted object-centric representation, informing the LLM of what objects are in the scene. We further devise an additional set of action tokens such as
    <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.1">
     NAVIGATE
    </span>
    ,
    <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.2">
     OBSERVE
    </span>
    (for obtaining object point cloud),
    <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.3">
     TOUCH
    </span>
    (for tactile and thermal information),
    <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.4">
     HIT
    </span>
    (for getting the impact sound) to denote that the agent takes the actions to explore the environment and interacts with the objects. By interacting with the objects, more detailed multisensory information could be unveiled as outcomes of the actions and encoded via a set of state tokens.
All sensory observations are encoded by different sensor encoders and connected to the LLM using sensor-to-image adapters.
   </p>
  </div>
  <div class="ltx_para" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    In the inference time, MultiPLY could generate a series of action tokens through the LLM, instructing the agent to take the action and receive the outcome of the action as the next-state multisensory observation. The observation is then appended back to the LLM, enclosed by a set of state tokens, facilitating the next-step generation. Our MultiPLY, trained on Multisensory Universe, outperforms baseline models by a large margin on object retrieval, tool use, multi-modal captioning, and task decomposition.
   </p>
  </div>
  <div class="ltx_para" id="S1.p8">
   <p class="ltx_p" id="S1.p8.1">
    To sum up, the contributions of this paper are:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We propose Multisensory Universe, a large-scale multisensory dataset comprising 500k data collected by an agent engaging with the 3D embodied environment, covering a diverse set of tasks involving multisensory captioning, question answering, dialogue, manipulation, task decomposition, and so on.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We propose MultiPLY, a multisensory embodied LLM that could encode multisensory object-centric representations with a novel set of action tokens and state tokens for the end-to-end instruction tuning of a pre-trained LLM.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       Experimental results on object retrieval, tool use, multisensory captioning, and task decomposition show that MultiPLY outperforms baselines by a large margin.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Works
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     Multisensory Learning
    </span>
    Multisensory learning aims to learn from information from different sensors, including cameras, microphones, tactile sensors, etc.
For visual-audio learning, the datasets collecting visual-audio pairs in real-world
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      <span class="ltx_text" style="font-size:90%;">
       10
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      <span class="ltx_text" style="font-size:90%;">
       43
      </span>
     </a>
     ]
    </cite>
    or rendering sounds in simulators
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      <span class="ltx_text" style="font-size:90%;">
       6
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib8" title="">
      <span class="ltx_text" style="font-size:90%;">
       8
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib17" title="">
      <span class="ltx_text" style="font-size:90%;">
       17
      </span>
     </a>
     ]
    </cite>
    promote the development of this field of research.
Earlier works seek to combine audio and visuals information for audio-visual event localization
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib61" title="">
      <span class="ltx_text" style="font-size:90%;">
       61
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib60" title="">
      <span class="ltx_text" style="font-size:90%;">
       60
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib27" title="">
      <span class="ltx_text" style="font-size:90%;">
       27
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib57" title="">
      <span class="ltx_text" style="font-size:90%;">
       57
      </span>
     </a>
     ]
    </cite>
    , sound source localization in visual frame
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      <span class="ltx_text" style="font-size:90%;">
       14
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib16" title="">
      <span class="ltx_text" style="font-size:90%;">
       16
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib67" title="">
      <span class="ltx_text" style="font-size:90%;">
       67
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib66" title="">
      <span class="ltx_text" style="font-size:90%;">
       66
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib19" title="">
      <span class="ltx_text" style="font-size:90%;">
       19
      </span>
     </a>
     ]
    </cite>
    , visual-guided sound editing
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      <span class="ltx_text" style="font-size:90%;">
       7
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib25" title="">
      <span class="ltx_text" style="font-size:90%;">
       25
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      <span class="ltx_text" style="font-size:90%;">
       18
      </span>
     </a>
     ]
    </cite>
    , and visually-aligned sound generation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      <span class="ltx_text" style="font-size:90%;">
       15
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib9" title="">
      <span class="ltx_text" style="font-size:90%;">
       9
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib50" title="">
      <span class="ltx_text" style="font-size:90%;">
       50
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib44" title="">
      <span class="ltx_text" style="font-size:90%;">
       44
      </span>
     </a>
     ]
    </cite>
    .
As for visual-tactile learning, many works focus on building realistic tactile
simulation system
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib58" title="">
      <span class="ltx_text" style="font-size:90%;">
       58
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib41" title="">
      <span class="ltx_text" style="font-size:90%;">
       41
      </span>
     </a>
     ]
    </cite>
    or collecting tactile data of real objects
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib23" title="">
      <span class="ltx_text" style="font-size:90%;">
       23
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib24" title="">
      <span class="ltx_text" style="font-size:90%;">
       24
      </span>
     </a>
     ]
    </cite>
    . With these tactile data, researchers combine visual and tactile data for cross-modal retrieval
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      <span class="ltx_text" style="font-size:90%;">
       21
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      <span class="ltx_text" style="font-size:90%;">
       3
      </span>
     </a>
     ]
    </cite>
    , robotic manipulation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      <span class="ltx_text" style="font-size:90%;">
       5
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      <span class="ltx_text" style="font-size:90%;">
       4
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib36" title="">
      <span class="ltx_text" style="font-size:90%;">
       36
      </span>
     </a>
     ]
    </cite>
    , and 3D reconstruction
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib49" title="">
      <span class="ltx_text" style="font-size:90%;">
       49
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib48" title="">
      <span class="ltx_text" style="font-size:90%;">
       48
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib52" title="">
      <span class="ltx_text" style="font-size:90%;">
       52
      </span>
     </a>
     ]
    </cite>
    .
Different from the previous works, our MultiPLY aims to combine visual, audio, tactile, and thermal information in an interactive 3D environment for diverse embodied tasks.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    <span class="ltx_text ltx_font_bold" id="S2.p2.1.1">
     Multi-modal Large Language Models
    </span>
    LLMs
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib42" title="">
      <span class="ltx_text" style="font-size:90%;">
       42
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib53" title="">
      <span class="ltx_text" style="font-size:90%;">
       53
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib65" title="">
      <span class="ltx_text" style="font-size:90%;">
       65
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib55" title="">
      <span class="ltx_text" style="font-size:90%;">
       55
      </span>
     </a>
     ]
    </cite>
    demonstrate prowess across numerous domains. Recent works
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      <span class="ltx_text" style="font-size:90%;">
       1
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib37" title="">
      <span class="ltx_text" style="font-size:90%;">
       37
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib39" title="">
      <span class="ltx_text" style="font-size:90%;">
       39
      </span>
     </a>
     ]
    </cite>
    attempt to empower LLMs with visual understanding ability using large-scale image-text pair data and apply the trained models on downstream tasks like visual question-answering, image captioning, and multi-modal dialogue. Researchers
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib32" title="">
      <span class="ltx_text" style="font-size:90%;">
       32
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib62" title="">
      <span class="ltx_text" style="font-size:90%;">
       62
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib51" title="">
      <span class="ltx_text" style="font-size:90%;">
       51
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib64" title="">
      <span class="ltx_text" style="font-size:90%;">
       64
      </span>
     </a>
     ]
    </cite>
    also focus on incorporating 3D visual information into LLMs to empower spatial reasoning abilities. In addition to incorporating visual information into LLMs, recent works
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib31" title="">
      <span class="ltx_text" style="font-size:90%;">
       31
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib30" title="">
      <span class="ltx_text" style="font-size:90%;">
       30
      </span>
     </a>
     ]
    </cite>
    attempt to enable LLMs to understand multi-modal information. AnyMAL
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib40" title="">
      <span class="ltx_text" style="font-size:90%;">
       40
      </span>
     </a>
     ]
    </cite>
    presents a unified model that aligns multi-modal information including text, image, video, audio, and IMU motion reading.
However, these works process passive information rather than actively interact with the environment.
In contrast, our work focuses on an embodied large language model, which could actively interact with the multi-modal 3D world by navigating in the environment, touching objects to get tactile and thermal information, hitting objects to get impact sound, etc.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   The Multisensory-Universe Dataset
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    In this section, we illustrate the process of collecting the Multisensory-Universe dataset. As presented in Figure
    <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    , we begin by explaining how we input interactive objects into the scene to construct object-centric 3D scenes for our dataset in Section
    <a class="ltx_ref" href="#S3.SS1" title="3.1 Inputting Interactive Objects into 3D Scenes ‣ 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_tag">
      3.1
     </span>
    </a>
    . Subsequently, we outline the methodology for obtaining sensor data from these objects in Section
    <a class="ltx_ref" href="#S3.SS2" title="3.2 Object Sensor Data Acquisition ‣ 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_tag">
      3.2
     </span>
    </a>
    . Moving on to Section
    <a class="ltx_ref" href="#S3.SS3" title="3.3 Embodied Agents for Data Collection ‣ 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_tag">
      3.3
     </span>
    </a>
    , we describe the deployment of an embodied agent tasked with proposing tasks and exploring the environment to solve them. The resulting interaction data are collected as paired interaction-language data, which serves as training input for the LLM.
   </p>
  </div>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="470" id="S3.F2.g1" src="/html/2401.08577/assets/x2.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">
      Figure 2
     </span>
     :
    </span>
    <span class="ltx_text ltx_font_bold" id="S3.F2.4.2" style="font-size:90%;">
     Multisensory-Universe Generation Pipelines
     <span class="ltx_text ltx_font_medium" id="S3.F2.4.2.1">
      . We first add a set of new interactive objects in the embodied environments, then prompt ChatGPT to generate diverse tasks about the environment. An embodied agent interacts with the objects to retrieve the multisensory information and construct interaction data.
     </span>
    </span>
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Inputting Interactive Objects into 3D Scenes
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     We build our scenes on top of the Habitat-Matterport 3D (HM3D) semantics dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib46" title="">
       <span class="ltx_text" style="font-size:90%;">
        46
       </span>
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib63" title="">
       <span class="ltx_text" style="font-size:90%;">
        63
       </span>
      </a>
      ]
     </cite>
     , which has 216 3D spaces and 3,100 rooms within those spaces. However, the existing objects in HM3D scenes, with insufficient sensor data and limited diversity, are not interactive in Habitat-sim
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib47" title="">
       <span class="ltx_text" style="font-size:90%;">
        47
       </span>
      </a>
      ]
     </cite>
     . Thus, we propose to add new interactive objects to the scenes, allowing agents to interact with them using Habitat-sim.
The objects we add to the scenes are from two sources: 1) ObjectFolder
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib20" title="">
       <span class="ltx_text" style="font-size:90%;">
        20
       </span>
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       <span class="ltx_text" style="font-size:90%;">
        22
       </span>
      </a>
      ]
     </cite>
     , which contains 1k object meshes, with impact sounds of these objects stored in implicit neural fields, and annotated with object materials. 2) Objaverse
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib11" title="">
       <span class="ltx_text" style="font-size:90%;">
        11
       </span>
      </a>
      ]
     </cite>
     is a universe of 800K 3D objects spanning rich categories. We select the objects that could appear in indoor scenes.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     We ask ChatGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib42" title="">
       <span class="ltx_text" style="font-size:90%;">
        42
       </span>
      </a>
      ]
     </cite>
     to choose 1-10 new objects from ObjectFolder and Objaverse, and generate the proper bounding boxes for these newly-added objects. ChatGPT is also required to specify objects’ material categories (
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">
      e.g.,
     </span>
     ceramic, plastic, steel) and properties(
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">
      e.g.,
     </span>
     , deformation, elasticity hardness), as well as temperature labels (
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">
      e.g.,
     </span>
     whether the objects are hot, cold, or the same as room temperature).
Our prompt to GPT contains all existing objects in HM3D scenes and their bounding boxes, as well as several preferences: 1) Select some similar objects. For example, choose two bottles of similar appearances and specify one of them as plastic and the other one as steel. In this way, information from different sensors needs to be collected to resolve the ambiguity. 2) Select objects that are compatible with the environment and can be utilized together for interesting tasks. For instance, in a kitchen environment, we could put ingredients and tools for cooking. We also give some few-shot prompting examples to GPT.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Object Sensor Data Acquisition
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     We illustrate how we collect sensor data of added objects.
    </p>
    <ul class="ltx_itemize" id="S3.I1">
     <li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i1.p1">
       <p class="ltx_p" id="S3.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">
         Tactile
        </span>
        We use DiffTactile
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib2" title="">
          <span class="ltx_text" style="font-size:90%;">
           2
          </span>
         </a>
         ]
        </cite>
        which leverages MLS-MPM
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib33" title="">
          <span class="ltx_text" style="font-size:90%;">
           33
          </span>
         </a>
         ]
        </cite>
        to simulate rigid, elastic, elasto-plastic objects. We put meshes of added objects into DiffTactile, which uses the bubble gripper with several position markers to touch the objects at pre-defined positions. The tactile readings are the initial and final positions of the markers, which represent how much the bubble deforms.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i2.p1">
       <p class="ltx_p" id="S3.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">
         Ambient Sound
        </span>
        Each object could emit ambient sound to facilitate navigation or reasoning, or serve as cues for informing the agents what’s going on in the environment.
We prompt ChatGPT to match the sounds from AudioSet
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib26" title="">
          <span class="ltx_text" style="font-size:90%;">
           26
          </span>
         </a>
         ]
        </cite>
        with the semantic labels of the added objects. Given the Audioset description, ChatGPT needs to select objects in the candidate list that are possible to make this sound.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i3.p1">
       <p class="ltx_p" id="S3.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">
         Impact Sound
        </span>
        Impact sound represents the sound that we hear when we strike or hit an object, which is crucial for identifying the material of an object. We get the impact sounds of ObjectFolder objects by querying their implicit sound fields given a hitting position and a force.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i4.p1">
       <p class="ltx_p" id="S3.I1.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">
         Temperature
        </span>
        Given the temperature label of the object, we ask ChatGPT for a proper temperature of each object.
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Embodied Agents for Data Collection
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     Inspired by
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib59" title="">
       <span class="ltx_text" style="font-size:90%;">
        59
       </span>
      </a>
      ]
     </cite>
     , we utilize LLM-powered embodied agents to collect the data in the constructed scenes. We first prompt ChatGPT to propose tasks. Then we place an embodied agent to interact with the objects in 3D environments to perform the task and collect interaction data.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p2">
    <p class="ltx_p" id="S3.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">
      Generating Task Proposals
     </span>
     We follow the box-demonstration-instruction-based prompting method proposed by
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib32" title="">
       <span class="ltx_text" style="font-size:90%;">
        32
       </span>
      </a>
      ]
     </cite>
     , and prompt ChatGPT to generate tasks.
In addition to the ground-truth bounding boxes of objects, we also input the ground-truth materials, deformability, and hardness, as well as the ground-truth temperature labels of all objects. ChatGPT is provided with a list of actions to be performed in the environment. Then it generates specific tasks requiring interactions with objects, a sequence of words representing pseudo ground-truth actions, and language reasoning outputs which are deduced from the ground-truth feedback labels of the objects (note that ChatGPT has access to all material and temperature labels, so that it could generate a sentence like “it feels cold” after the “touch” action). We cover a diverse set of tasks including multisensory captioning, question answering, embodied dialogue, navigation, object manipulation, tool use, rearrangement, task decomposition, and so on. We append all prompts in Supplementary Material.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p3">
    <p class="ltx_p" id="S3.SS3.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">
      Interaction Data Collection
     </span>
     The embodied agent first randomly explores the environments to collect initial RGBD environment data. Given the actions, the agent executes the actions to interact with the objects in the environment and obtains the sensory feedback. For example, when the action is ”touching an object”, the agent returns the tactile and temperature readings of it. We store all the interaction results of the actions. From one interaction, we could incrementally construct several input-output data, denoting the interaction at different steps, as shown in Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   MultiPLY
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In this section, we introduce the MultiPLY framework. As in Figure
    <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4 MultiPLY ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    , we first encode the scene as an abstracted object-centric representation, while multisensory details of objects are unveiled only when the agent executes an action and interacts with them. We devise a set of action tokens denoting the actions of agents to interact with the environment. Interaction results are appended back to the LLM via state tokens to generate subsequent text or action tokens.
   </p>
  </div>
  <figure class="ltx_figure" id="S4.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="165" id="S4.F3.g1" src="/html/2401.08577/assets/x3.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S4.F3.3.1.1" style="font-size:90%;">
      Figure 3
     </span>
     :
    </span>
    <span class="ltx_text ltx_font_bold" id="S4.F3.4.2" style="font-size:90%;">
     Overview of our MultiPLY.
     <span class="ltx_text ltx_font_medium" id="S4.F3.4.2.1">
      We first encode the scene as an abstracted object-centric representation, while multisensory details of objects can only be unveiled when the agent executes an action and interacts with them. We devise a set of action tokens denoting the actions of agents to interact with the environment. The interaction results are appended back to the LLM via state tokens.
     </span>
    </span>
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Object-Centric Scene Representations
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.3">
     Our model first takes the features of the 3D environment explored by the agent as inputs to form an initial impression of what the scene looks like.
We follow 3D-LLM and utilize 2D features to construct 3D scene features, so that the visual features could be seamlessly fed into a pre-trained vision-language model without adaption. However, the point cloud encoding of 3D-LLMs makes it hard for LLMs to process thousands of points at a time. Alternatively, when humans explore a 3D environment, we abstract over the scene and roughly form an idea of objects and their locations without remembering all the details. Likewise, we propose to represent the environment as an abstracted object-centric representation. We use concept graphs
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib29" title="">
       <span class="ltx_text" style="font-size:90%;">
        29
       </span>
      </a>
      ]
     </cite>
     powered with a CLIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib45" title="">
       <span class="ltx_text" style="font-size:90%;">
        45
       </span>
      </a>
      ]
     </cite>
     encoder to first encode the objects in the observed images, and fuse the outputs in images to 3D by multi-view association. We also add position embeddings to the visual features of objects. We finally get
     <math alttext="\mathcal{O}\times 1024" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1">
      <semantics id="S4.SS1.p1.1.m1.1a">
       <mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">
        <mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">
         𝒪
        </mi>
        <mo id="S4.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.1.m1.1.1.1.cmml">
         ×
        </mo>
        <mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">
         1024
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b">
        <apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">
         <times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1">
         </times>
         <ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">
          𝒪
         </ci>
         <cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3">
          1024
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">
        \mathcal{O}\times 1024
       </annotation>
      </semantics>
     </math>
     features as an abstracted object-centric scene representation, where
     <math alttext="\mathcal{O}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1">
      <semantics id="S4.SS1.p1.2.m2.1a">
       <mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">
        𝒪
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b">
        <ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">
         𝒪
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">
        \mathcal{O}
       </annotation>
      </semantics>
     </math>
     is the number of objects. If there’s an ambient sound emitted by an object in the 3D environment, we encode the sound using the CLAP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       <span class="ltx_text" style="font-size:90%;">
        13
       </span>
      </a>
      ]
     </cite>
     audio encoder and get a
     <math alttext="1024" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1">
      <semantics id="S4.SS1.p1.3.m3.1a">
       <mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">
        1024
       </mn>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b">
        <cn id="S4.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p1.3.m3.1.1">
         1024
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">
        1024
       </annotation>
      </semantics>
     </math>
     -dim feature. The object-centric scene representation and ambient sound representation serve as the initial inputs to the LLM, enclosed by tokens as
     <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.3.1">
      &lt;SCENE&gt;
     </span>
     ,
     <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.3.2">
      &lt;/SCENE&gt;
     </span>
     and
     <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.3.3">
      &lt;AMBIENT_SOUND&gt;
     </span>
     ,
     <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.3.4">
      &lt;/AMBIENT_SOUND&gt;
     </span>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Action Tokens
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     We devise a set of action tokens that denote the agent’s interaction with the environment, which are listed below:
    </p>
    <ul class="ltx_itemize" id="S4.I1">
     <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i1.p1">
       <p class="ltx_p" id="S4.I1.i1.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i1.p1.1.1">
         &lt;SELECT&gt;
        </span>
        token selects an object to interact with. The object is chosen by the attention between the language features (
        <span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.2">
         i.e.
        </span>
        , the last hidden state of the LLM of the
        <span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.3">
         SELECT
        </span>
        token), and the CLIP visual features of the objects in the environment. It selects the object with the maximum attention score.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i2.p1">
       <p class="ltx_p" id="S4.I1.i2.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i2.p1.1.1">
         &lt;NAVIGATE&gt;
        </span>
        token asks an agent to navigate to the selected object. Note that the navigation action could be executed by any pre-defined pathfinder module and is not the research focus of this paper.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i3.p1">
       <p class="ltx_p" id="S4.I1.i3.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i3.p1.1.1">
         &lt;OBSERVE&gt;
        </span>
        token asks an agent to scrutinize an object that is chosen and get the object details (in the form of the detailed point cloud of the object).
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i4.p1">
       <p class="ltx_p" id="S4.I1.i4.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i4.p1.1.1">
         &lt;TOUCH&gt;
        </span>
        token allows the agent to touch the object that is chosen, to get the tactile and temperature information.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i5.p1">
       <p class="ltx_p" id="S4.I1.i5.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i5.p1.1.1">
         &lt;HIT&gt;
        </span>
        token allows the agent to hit the chosen object to get the impact sound.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i6.p1">
       <p class="ltx_p" id="S4.I1.i6.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i6.p1.1.1">
         &lt;PICK-UP&gt;
        </span>
        ,
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i6.p1.1.2">
         &lt;PUT-DOWN&gt;
        </span>
        tokens enable the agent to pick up or put down a chosen object.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i7.p1">
       <p class="ltx_p" id="S4.I1.i7.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i7.p1.1.1">
         &lt;LOOK-AROUND&gt;
        </span>
        token allows the agent to rotate its head and get nearby objects.
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    State Tokens
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     We devise another set of state tokens to feed the interaction results back to the LLM.
    </p>
    <ul class="ltx_itemize" id="S4.I2">
     <li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I2.i1.p1">
       <p class="ltx_p" id="S4.I2.i1.p1.2">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i1.p1.2.1">
         &lt;OBJECT&gt;
        </span>
        encodes the obtained object points when the agent
        <span class="ltx_text ltx_font_typewriter" id="S4.I2.i1.p1.2.2">
         &lt;OBSERVE&gt;
        </span>
        s an object. Specifically, we get the 3D features aggregated from 2D CLIP features
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib32" title="">
          <span class="ltx_text" style="font-size:90%;">
           32
          </span>
         </a>
         ]
        </cite>
        and add position embeddings to the 3D features. We build
        <math alttext="\mathcal{N}\times 1024" class="ltx_Math" display="inline" id="S4.I2.i1.p1.1.m1.1">
         <semantics id="S4.I2.i1.p1.1.m1.1a">
          <mrow id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml">
           <mi class="ltx_font_mathcaligraphic" id="S4.I2.i1.p1.1.m1.1.1.2" xref="S4.I2.i1.p1.1.m1.1.1.2.cmml">
            𝒩
           </mi>
           <mo id="S4.I2.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I2.i1.p1.1.m1.1.1.1.cmml">
            ×
           </mo>
           <mn id="S4.I2.i1.p1.1.m1.1.1.3" xref="S4.I2.i1.p1.1.m1.1.1.3.cmml">
            1024
           </mn>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b">
           <apply id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1">
            <times id="S4.I2.i1.p1.1.m1.1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1.1">
            </times>
            <ci id="S4.I2.i1.p1.1.m1.1.1.2.cmml" xref="S4.I2.i1.p1.1.m1.1.1.2">
             𝒩
            </ci>
            <cn id="S4.I2.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I2.i1.p1.1.m1.1.1.3">
             1024
            </cn>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">
           \mathcal{N}\times 1024
          </annotation>
         </semantics>
        </math>
        object point cloud features where
        <math alttext="\mathcal{N}" class="ltx_Math" display="inline" id="S4.I2.i1.p1.2.m2.1">
         <semantics id="S4.I2.i1.p1.2.m2.1a">
          <mi class="ltx_font_mathcaligraphic" id="S4.I2.i1.p1.2.m2.1.1" xref="S4.I2.i1.p1.2.m2.1.1.cmml">
           𝒩
          </mi>
          <annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.2.m2.1b">
           <ci id="S4.I2.i1.p1.2.m2.1.1.cmml" xref="S4.I2.i1.p1.2.m2.1.1">
            𝒩
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S4.I2.i1.p1.2.m2.1c">
           \mathcal{N}
          </annotation>
         </semantics>
        </math>
        is the number of points.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I2.i2.p1">
       <p class="ltx_p" id="S4.I2.i2.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i2.p1.1.1">
         &lt;IMPACT_SOUND&gt;
        </span>
        encodes the obtained impact sound when the agent
        <span class="ltx_text ltx_font_typewriter" id="S4.I2.i2.p1.1.2">
         &lt;HIT&gt;
        </span>
        s an object. We use CLAP audio encoder to encode the sound and get
        <math alttext="1024" class="ltx_Math" display="inline" id="S4.I2.i2.p1.1.m1.1">
         <semantics id="S4.I2.i2.p1.1.m1.1a">
          <mn id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">
           1024
          </mn>
          <annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b">
           <cn id="S4.I2.i2.p1.1.m1.1.1.cmml" type="integer" xref="S4.I2.i2.p1.1.m1.1.1">
            1024
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">
           1024
          </annotation>
         </semantics>
        </math>
        -dim impact sound representation. Since the CLAP features are not aligned with the LLM, we use a sound projector (one linear layer) to map to the feature space of the LLM.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I2.i3.p1">
       <p class="ltx_p" id="S4.I2.i3.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i3.p1.1.1">
         &lt;TACTILE&gt;
        </span>
        encodes the obtained tactile information when an object is being
        <span class="ltx_text ltx_font_typewriter" id="S4.I2.i3.p1.1.2">
         &lt;TOUCH&gt;
        </span>
        ed by an agent. We transform the tactile reading as a heatmap and use CLIP to encode the heatmap. We mean-pool over the patches and get
        <math alttext="1024" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1">
         <semantics id="S4.I2.i3.p1.1.m1.1a">
          <mn id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml">
           1024
          </mn>
          <annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b">
           <cn id="S4.I2.i3.p1.1.m1.1.1.cmml" type="integer" xref="S4.I2.i3.p1.1.m1.1.1">
            1024
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">
           1024
          </annotation>
         </semantics>
        </math>
        -dim temperature features. We use a tactile projector (one linear layer) to map to the feature space of the LLM.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I2.i4.p1">
       <p class="ltx_p" id="S4.I2.i4.p1.1">
        <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i4.p1.1.1">
         &lt;TEMPERATURE&gt;
        </span>
        encodes the obtained temperature. We transform the temperature reading as a heatmap and use CLIP to encode the heatmap. We mean-pool over the patches and get
        <math alttext="1024" class="ltx_Math" display="inline" id="S4.I2.i4.p1.1.m1.1">
         <semantics id="S4.I2.i4.p1.1.m1.1a">
          <mn id="S4.I2.i4.p1.1.m1.1.1" xref="S4.I2.i4.p1.1.m1.1.1.cmml">
           1024
          </mn>
          <annotation-xml encoding="MathML-Content" id="S4.I2.i4.p1.1.m1.1b">
           <cn id="S4.I2.i4.p1.1.m1.1.1.cmml" type="integer" xref="S4.I2.i4.p1.1.m1.1.1">
            1024
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S4.I2.i4.p1.1.m1.1c">
           1024
          </annotation>
         </semantics>
        </math>
        -dim temperature features. We use a temperature projector (one linear layer) to map to the feature space of the LLM.
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Training &amp; Inference
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">
      Model Architecture
     </span>
     We use LLaVA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib38" title="">
       <span class="ltx_text" style="font-size:90%;">
        38
       </span>
      </a>
      ]
     </cite>
     as our backbone multi-modal large language model. Since our visual features have been aligned to the same embedding space as LLaVA using ConceptGraphs
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib29" title="">
       <span class="ltx_text" style="font-size:90%;">
        29
       </span>
      </a>
      ]
     </cite>
     , we could directly use LLaVA’s vision-to-language projector without pretraining on vision-language data. For other sensor modalities, we leverage a lightweight adapter, which is a one-layer linear projector to project the sensor features into the text token embedding space of LLaVA.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p2">
    <p class="ltx_p" id="S4.SS4.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">
      Modality Alignment
     </span>
     As stated above, the tactile, sound, and temperature representations are not aligned with the language features. In the first stage, we train the sensor-to-language adapter for multisensory feature alignment. For audio-language alignment, we use AudioSet
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib26" title="">
       <span class="ltx_text" style="font-size:90%;">
        26
       </span>
      </a>
      ]
     </cite>
     and AudioCaps
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       <span class="ltx_text" style="font-size:90%;">
        34
       </span>
      </a>
      ]
     </cite>
     . For impact sound, tactile, and thermal data, we use ChatGPT to generate a one-sentence caption describing the material and the alignment between each sensor modality and language. We freeze the weight of the image encoder and the LLM for faster convergence and maintenance of language reasoning abilities.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p3">
    <p class="ltx_p" id="S4.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p3.1.1">
      Instruction tuning with Multisensory Universe
     </span>
     In the second stage, we tune LLaVA with our multisensory dataset. Our training loss consists of two parts. The first one is the LLM loss which is the same as the original LLaVA model. We add one more loss that forces the model to select the right object to attend to. Specifically, we calculate the attention between the last hidden state of the LLM of
the SELECT token, and each abstracted object feature. The feature goes through a Sigmoid layer, and is optimized with a binary cross entropy (BCE) loss. We unfreeze the whole model for the training of this stage. We use FSDP on 128 V100 GPUS for efficient training.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p4">
    <p class="ltx_p" id="S4.SS4.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">
      Inference
     </span>
     At the inference time, our MultiPLY first takes the task prompt and abstracted scene representation as inputs and generates subsequent tokens. Once an action token is generated, an embodied agent is instructed to take the action in Habitat-sim
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib47" title="">
       <span class="ltx_text" style="font-size:90%;">
        47
       </span>
      </a>
      ]
     </cite>
     and interact with the environment. The observation outcome of the agent is sent back to the LLM as inputs via state tokens. The LLM further generates next tokens based on the current state inputs.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Experiments
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    After training on our collected Multisensory Universe, we perform an evaluation in the simulator, where an agent could actually interact with the environment when the action tokens are generated by MultiPLY. Then, the LLM waits for the agent to complete the actions and send back the observations via state tokens to generate the next token.
We provide four experimental settings: object retrieval, tool use, multisensory captioning, and task decomposition, and provide detailed task descriptions, baselines, and analysis for each task. We ensure that no scenes and objects in the Multisensory Universe appear in the evaluation setup. Due to space limits, we attach more ablative studies in the Supplementary Material, where we experiment with each possible combination of sensory inputs from different modalities, with or without interaction with the environment.
   </p>
  </div>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Object Retrieval
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">
      Task Decription
     </span>
     We devise the object retrieval task where several similar objects are present in the 3D scene, and the agent needs to use multiple sensor data to retrieve the correct object. For example, the task input could be like ”retrieve the soft paper cup with hot water”, while there could be distracting objects like “hard paper cup with hot water”, “soft paper cup with hot water”, “soft plastic bowl with hot water” or “soft paper bowl with hot water”, etc. The scene setup is different from the Multisensory Universe as we place more distracting objects to retrieve from (while in Multisensory Universe most scenes have two similar objects), and we include different sensor attribute combinations from Multisensory Universe objects. For example, in the training set, we saw a ceramic cup and a paper bowl, and in the evaluation, we query about a paper cup.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">
      Baselines
     </span>
     We include a set of cross-modality retrieval models as our baselines, which return the similarity between aligned sensor embeddings. They can be categorized into 1) single-sensor language models, such as CLIP and CLAP. 2) 2D multisensory models, for which the embeddings of other modalities have been mapped to the same as 2D images like ImageBind
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       <span class="ltx_text" style="font-size:90%;">
        28
       </span>
      </a>
      ]
     </cite>
     . 3) 3D multisensory models, in which the embeddings of object point clouds are binded to other modalities, like PointBind
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib30" title="">
       <span class="ltx_text" style="font-size:90%;">
        30
       </span>
      </a>
      ]
     </cite>
     . We first explore the environment and use concept graphs to represent the scene as a set of object features like MultiPLY, where the object features are visual embeddings from these retrieval models. The select action could be achieved by calculating the similarity between the object embedding and the language embedding, and the object with the highest score will be retrieved. As these models cannot interact with the environment to get the tactile, impact sound, and temperature data, we refine three setups for the baselines: 1) No interaction, and retrieve the object with the highest retrieval score. (For CLAP we assume that we have impact sounds of all objects) 2) Interact with the environment using oracle interactive actions. That is, we first retrieve the objects of interest via visual-language similarity, then we manually control the agent to interact with the objects to get impact sound, tactile and temperature information. The embeddings of all sensors are averaged and calculate the similarities with the language query, and the object with the highest score is retrieved. Since the action tokens are pre-defined and not generated, this oracle setting makes it easier to compete with MultiPLY. 3) Finetuned with a modified version of our Multisensory Universe tailored for multi-modal alignment and retrieval. Specifically, we first align the sensor data of the objects in Multisensory Universe to visual modality (like in ImageBind and PointBind), then we further align them with the modified language data in Multisensory Universe.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p3">
    <p class="ltx_p" id="S5.SS1.p3.1">
     For LLM-based methods, we include Pointbind-LLM, which uses the pointbind representations and performs instruction tuning with LLaMA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib54" title="">
       <span class="ltx_text" style="font-size:90%;">
        54
       </span>
      </a>
      ]
     </cite>
     . We also experiment with MultiPLY-2D, a 2D variant of our model, where we replace 3D features with 2D single-view features.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T1">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.2">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S5.T1.2.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.2.1.1.1">
        <span class="ltx_text" id="S5.T1.2.1.1.1.1" style="font-size:90%;">
         Model
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.2.1.1.2">
        <span class="ltx_text" id="S5.T1.2.1.1.2.1" style="font-size:90%;">
         Retrieval Accuracy
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S5.T1.2.2.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.2.2.1.1">
        <span class="ltx_text" id="S5.T1.2.2.1.1.1" style="font-size:90%;">
         ConceptGraph+CLAP
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.1.2">
        <span class="ltx_text" id="S5.T1.2.2.1.2.1" style="font-size:90%;">
         14.5
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.3.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.3.2.1">
        <span class="ltx_text" id="S5.T1.2.3.2.1.1" style="font-size:90%;">
         ConceptGraph+CLIP
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T1.2.3.2.2">
        <span class="ltx_text" id="S5.T1.2.3.2.2.1" style="font-size:90%;">
         18.7
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.4.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.2.4.3.1">
        <span class="ltx_text" id="S5.T1.2.4.3.1.1" style="font-size:90%;">
         ConceptGraph+ImageBind
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.4.3.2">
        <span class="ltx_text" id="S5.T1.2.4.3.2.1" style="font-size:90%;">
         20.3
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.5.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.5.4.1">
        <span class="ltx_text" id="S5.T1.2.5.4.1.1" style="font-size:90%;">
         ConceptGraph+ImageBind-I
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T1.2.5.4.2">
        <span class="ltx_text" id="S5.T1.2.5.4.2.1" style="font-size:90%;">
         24.7
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.6.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.6.5.1">
        <span class="ltx_text" id="S5.T1.2.6.5.1.1" style="font-size:90%;">
         ConceptGraph+ImageBind-I (Finetuned)
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T1.2.6.5.2">
        <span class="ltx_text" id="S5.T1.2.6.5.2.1" style="font-size:90%;">
         36.7
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.7.6">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.7.6.1">
        <span class="ltx_text" id="S5.T1.2.7.6.1.1" style="font-size:90%;">
         MultiPLY-2D
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T1.2.7.6.2">
        <span class="ltx_text" id="S5.T1.2.7.6.2.1" style="font-size:90%;">
         44.6
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.8.7">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.2.8.7.1">
        <span class="ltx_text" id="S5.T1.2.8.7.1.1" style="font-size:90%;">
         ConceptGraph+PointBind
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.8.7.2">
        <span class="ltx_text" id="S5.T1.2.8.7.2.1" style="font-size:90%;">
         19.5
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.9.8">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.9.8.1">
        <span class="ltx_text" id="S5.T1.2.9.8.1.1" style="font-size:90%;">
         ConceptGraph+PointBind-I
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T1.2.9.8.2">
        <span class="ltx_text" id="S5.T1.2.9.8.2.1" style="font-size:90%;">
         22.7
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.10.9">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.10.9.1">
        <span class="ltx_text" id="S5.T1.2.10.9.1.1" style="font-size:90%;">
         ConceptGraph+PointBind-I (Finetuned)
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T1.2.10.9.2">
        <span class="ltx_text" id="S5.T1.2.10.9.2.1" style="font-size:90%;">
         40.4
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.11.10">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.2.11.10.1">
        <span class="ltx_text" id="S5.T1.2.11.10.1.1" style="font-size:90%;">
         PointBind-LLM (Finetuned)
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.11.10.2">
        <span class="ltx_text" id="S5.T1.2.11.10.2.1" style="font-size:90%;">
         48.9
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T1.2.12.11">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.2.12.11.1">
        <span class="ltx_text" id="S5.T1.2.12.11.1.1" style="font-size:90%;">
         MultiPLY
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.2.12.11.2">
        <span class="ltx_text ltx_font_bold" id="S5.T1.2.12.11.2.1" style="font-size:90%;">
         56.7
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     <span class="ltx_text ltx_font_bold" id="S5.T1.6.1">
      Experimental Results of Object Retrieval.
     </span>
     -I denotes the models utilize oracle action tokens to interact with the environment. (Finetuned) means finetuned on Multisensory Universe.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p4">
    <p class="ltx_p" id="S5.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">
      Analysis
     </span>
     Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‣ 5.1 Object Retrieval ‣ 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     shows the object retrieval results. We could come to several conclusions. First, models that take multiple sensory inputs outperform models that handle single modality inputs by a large margin. CLIP, CLAP, as well as
models that use the initial visual embeddings have a very low score in object retrieval, emphasizing the importance of integrating multisensory data for reasoning. Second, 3D-based models surpass 2D models, mainly because single-view images sometimes fail to provide enough information to reason about the objects due to view inconsistency and occlusion. Third, LLMs outperform similarity-based retrieval models. The reason could be that retrieval models fuse the multisensory embeddings into a whole, and do not disentangle the representation, or interact with the different sensors step by step. In general, our MultiPLY outperforms the baseline models a lot. That’s probably because one weakness of the binding-based methods is that they bind everything to the visual modality, while one visual attribute could be mapped to several attributes from another modality (
     <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.2">
      e.g.,
     </span>
     from the appearance of a cup, we could not tell whether it’s made of ceramic or plastic, unable to align to different impact sounds for alignment). Our MultiPLY resolves ambiguity by interacting with and reasoning about the different sensor data individually.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Tool Use
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">
      Task Description
     </span>
     In an embodied environment, multisensory data are crucial for finding an appropriate tool to solve a problem. One example is that when we are injured, we need to retrieve warm compresses or ice packs depending on the injured parts and how long we’ve been injured. We could also find substitute tools if the common ones are not present. For example, we could use a steel spoon to replace the can opener, but we can’t use a plastic spoon. Similar to the object retrieval task, we place some objects from different categories, and also objects from the same categories but with different materials/haptic/thermal information in the environment. We use one sentence to describe the current situation and the goal to be done, and ask the agent to retrieve the correct tool for dealing with the situation.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p2">
    <p class="ltx_p" id="S5.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">
      Baselines
     </span>
     We use the same baselines as the object retrieval experiment for tool retrieval. For LLM-based methods, we also need to give reasons when we select the tools.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p3">
    <p class="ltx_p" id="S5.SS2.p3.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">
      Analysis
     </span>
     Table
     <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2 Tool Use ‣ 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     shows the results of tool use. We could see that the binding-based methods have a very poor performance in tool use. It might be because that they treat the object sensory data as a whole, unable to disentangle the individual sensory information such as material from the representation, let alone reasoning about how this property could be utilized as a tool, and how to analyze and deduce the functionality of an object when the multisensory information is integrated.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T2">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.2">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S5.T2.2.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.2.1.1.1">
        <span class="ltx_text" id="S5.T2.2.1.1.1.1" style="font-size:90%;">
         Model
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.2.1.1.2">
        <span class="ltx_text" id="S5.T2.2.1.1.2.1" style="font-size:90%;">
         Accuracy
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S5.T2.2.2.1">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.2.2.1.1">
        <span class="ltx_text" id="S5.T2.2.2.1.1.1" style="font-size:90%;">
         ConceptGraph+CLIP
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.1.2">
        <span class="ltx_text" id="S5.T2.2.2.1.2.1" style="font-size:90%;">
         10.1
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.3.2">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.2.3.2.1">
        <span class="ltx_text" id="S5.T2.2.3.2.1.1" style="font-size:90%;">
         ConceptGraph+ImageBind
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.3.2.2">
        <span class="ltx_text" id="S5.T2.2.3.2.2.1" style="font-size:90%;">
         7.4
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.4.3">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.4.3.1">
        <span class="ltx_text" id="S5.T2.2.4.3.1.1" style="font-size:90%;">
         ConceptGraph+ImageBind-I
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T2.2.4.3.2">
        <span class="ltx_text" id="S5.T2.2.4.3.2.1" style="font-size:90%;">
         8.2
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.5.4">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.5.4.1">
        <span class="ltx_text" id="S5.T2.2.5.4.1.1" style="font-size:90%;">
         ConceptGraph+ImageBind-I (Finetuned)
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T2.2.5.4.2">
        <span class="ltx_text" id="S5.T2.2.5.4.2.1" style="font-size:90%;">
         16.4
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.6.5">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.6.5.1">
        <span class="ltx_text" id="S5.T2.2.6.5.1.1" style="font-size:90%;">
         MultiPLY-2D
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T2.2.6.5.2">
        <span class="ltx_text" id="S5.T2.2.6.5.2.1" style="font-size:90%;">
         36.3
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.7.6">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.2.7.6.1">
        <span class="ltx_text" id="S5.T2.2.7.6.1.1" style="font-size:90%;">
         ConceptGraph+PointBind
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.7.6.2">
        <span class="ltx_text" id="S5.T2.2.7.6.2.1" style="font-size:90%;">
         11.5
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.8.7">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.8.7.1">
        <span class="ltx_text" id="S5.T2.2.8.7.1.1" style="font-size:90%;">
         ConceptGraph+PointBind-I
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T2.2.8.7.2">
        <span class="ltx_text" id="S5.T2.2.8.7.2.1" style="font-size:90%;">
         13.2
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.9.8">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.9.8.1">
        <span class="ltx_text" id="S5.T2.2.9.8.1.1" style="font-size:90%;">
         ConceptGraph+PointBind-I (Finetuned)
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T2.2.9.8.2">
        <span class="ltx_text" id="S5.T2.2.9.8.2.1" style="font-size:90%;">
         18.7
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.10.9">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.2.10.9.1">
        <span class="ltx_text" id="S5.T2.2.10.9.1.1" style="font-size:90%;">
         PointBind-LLM (Finetuned)
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.10.9.2">
        <span class="ltx_text" id="S5.T2.2.10.9.2.1" style="font-size:90%;">
         32.1
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T2.2.11.10">
       <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.2.11.10.1">
        <span class="ltx_text" id="S5.T2.2.11.10.1.1" style="font-size:90%;">
         MultiPLY
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.2.11.10.2">
        <span class="ltx_text ltx_font_bold" id="S5.T2.2.11.10.2.1" style="font-size:90%;">
         41.6
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     <span class="ltx_text ltx_font_bold" id="S5.T2.5.1">
      Experimental Results of Tool Use.
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Multisensory Captioning
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">
      Task Description
     </span>
     Different from traditional single-modality captioning tasks, multisensory captioning requires the model to describe the object in all senses. By giving semantic information about an object or ambient sound emitted by the object, the agent must first navigate to the object to interact with it and describe it.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S5.F4.g1" src="/html/2401.08577/assets/x4.png" width="323"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S5.F4.3.1.1" style="font-size:90%;">
       Figure 4
      </span>
      :
     </span>
     <span class="ltx_text ltx_font_bold" id="S5.F4.4.2" style="font-size:90%;">
      Qualitative Examples of our MultiPLY
      <span class="ltx_text ltx_font_medium" id="S5.F4.4.2.1">
       . MultiPLY could interact with the objects in the embodied environments and gather multisensory information.
      </span>
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">
      Baselines
     </span>
     For baseline models, we include LLaVA, which takes a holistic scene image as input and generates a caption about the queried object. 3D-LLM takes the scene point cloud as inputs, and uses dense captioning to describe the object. Both methods only use visual information. PointBind-LLM first retrieves the objects by modality alignment, and then interacts with the objects and integrates multisensory information to describe the queried object.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T3">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.2">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S5.T3.2.1.1">
       <th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.2.1.1.1">
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.2">
        <span class="ltx_text" id="S5.T3.2.1.1.2.1" style="font-size:90%;">
         BLEU1
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.3">
        <span class="ltx_text" id="S5.T3.2.1.1.3.1" style="font-size:90%;">
         BLEU4
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.1.1.4">
        <span class="ltx_text" id="S5.T3.2.1.1.4.1" style="font-size:90%;">
         METEOR
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S5.T3.2.2.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.2.1.1">
        <span class="ltx_text" id="S5.T3.2.2.1.1.1" style="font-size:90%;">
         LLaVA
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.1.2">
        <span class="ltx_text" id="S5.T3.2.2.1.2.1" style="font-size:90%;">
         9.5
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.1.3">
        <span class="ltx_text" id="S5.T3.2.2.1.3.1" style="font-size:90%;">
         0.6
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.1.4">
        <span class="ltx_text" id="S5.T3.2.2.1.4.1" style="font-size:90%;">
         7.1
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.2.3.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.3.2.1">
        <span class="ltx_text" id="S5.T3.2.3.2.1.1" style="font-size:90%;">
         LLaVA (Finetuned)
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.3.2.2">
        <span class="ltx_text" id="S5.T3.2.3.2.2.1" style="font-size:90%;">
         28.6
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.3.2.3">
        <span class="ltx_text" id="S5.T3.2.3.2.3.1" style="font-size:90%;">
         10.1
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.3.2.4">
        <span class="ltx_text" id="S5.T3.2.3.2.4.1" style="font-size:90%;">
         10.4
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.2.4.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.4.3.1">
        <span class="ltx_text" id="S5.T3.2.4.3.1.1" style="font-size:90%;">
         3D-LLM
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.4.3.2">
        <span class="ltx_text" id="S5.T3.2.4.3.2.1" style="font-size:90%;">
         14.4
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.4.3.3">
        <span class="ltx_text" id="S5.T3.2.4.3.3.1" style="font-size:90%;">
         1.5
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.4.3.4">
        <span class="ltx_text" id="S5.T3.2.4.3.4.1" style="font-size:90%;">
         9.5
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.2.5.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.5.4.1">
        <span class="ltx_text" id="S5.T3.2.5.4.1.1" style="font-size:90%;">
         3D-LLM (Finetuned)
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.5.4.2">
        <span class="ltx_text" id="S5.T3.2.5.4.2.1" style="font-size:90%;">
         31.2
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.5.4.3">
        <span class="ltx_text" id="S5.T3.2.5.4.3.1" style="font-size:90%;">
         12.1
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.5.4.4">
        <span class="ltx_text" id="S5.T3.2.5.4.4.1" style="font-size:90%;">
         12.4
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.2.6.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.6.5.1">
        <span class="ltx_text" id="S5.T3.2.6.5.1.1" style="font-size:90%;">
         PointBind-LLM
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.6.5.2">
        <span class="ltx_text" id="S5.T3.2.6.5.2.1" style="font-size:90%;">
         16.5
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.6.5.3">
        <span class="ltx_text" id="S5.T3.2.6.5.3.1" style="font-size:90%;">
         2.3
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.6.5.4">
        <span class="ltx_text" id="S5.T3.2.6.5.4.1" style="font-size:90%;">
         7.7
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.2.7.6">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.2.7.6.1">
        <span class="ltx_text" id="S5.T3.2.7.6.1.1" style="font-size:90%;">
         PointBind-LLM (Finetuned)
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.7.6.2">
        <span class="ltx_text" id="S5.T3.2.7.6.2.1" style="font-size:90%;">
         36.7
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.7.6.3">
        <span class="ltx_text" id="S5.T3.2.7.6.3.1" style="font-size:90%;">
         14.5
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.2.7.6.4">
        <span class="ltx_text" id="S5.T3.2.7.6.4.1" style="font-size:90%;">
         15.1
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.2.8.7">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T3.2.8.7.1">
        <span class="ltx_text" id="S5.T3.2.8.7.1.1" style="font-size:90%;">
         MultiPLY
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.2.8.7.2">
        <span class="ltx_text ltx_font_bold" id="S5.T3.2.8.7.2.1" style="font-size:90%;">
         48.9
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.2.8.7.3">
        <span class="ltx_text ltx_font_bold" id="S5.T3.2.8.7.3.1" style="font-size:90%;">
         20.1
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.2.8.7.4">
        <span class="ltx_text ltx_font_bold" id="S5.T3.2.8.7.4.1" style="font-size:90%;">
         24.2
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     <span class="ltx_text ltx_font_bold" id="S5.T3.5.1">
      Experimental Results of Multisensory Captioning.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p3">
    <p class="ltx_p" id="S5.SS3.p3.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">
      Analysis
     </span>
     Table
     <a class="ltx_ref" href="#S5.T3" title="Table 3 ‣ 5.3 Multisensory Captioning ‣ 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     shows the result. From the table, we could see that 3D-based LLMs overall outshine 2D VLMs. LLaVA and 3D-LLM take the holistic representation as inputs, and thus fail to compete with models that could interact with the models to switch between representations. MultiPLY outshines Pointbind-LLM, probably because PointBind binds the representations of different modalities, making it difficult to disentangle the senses.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.4
    </span>
    Task Decomposition
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p1">
    <p class="ltx_p" id="S5.SS4.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p1.1.1">
      Task Definition
     </span>
     Task decomposition focuses on decomposing a high-level task into smaller actions. In our setting, we focus on retrieving different things to prepare for a task. For example, to prepare for dinner, we need to first detect available foods in the kitchen, and gauge its temperature. If it’s cold, we need to heat it in the microwave so we also need to retrieve a ceramic or glass container which is microwave-safe. We also need to prepare the utensils of the appropriate materials. In our setting, we place several possible choice combinations in the environment, we also place object combinations unseen from the Multisensory Universe. As long as the agent retrieves one of the correct combinations, the task is marked as success.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p2">
    <p class="ltx_p" id="S5.SS4.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">
      Baselines
     </span>
     We include LLaVA, a minimal 2D image version of our model. We output an image of the scene and ask the model to decompose the tasks into actions. We also utilize 3D-LLM since it’s capable of performing task decomposition. In the original paper, we take the whole point cloud as input and generate low-level actions. Note that there is a domain gap between the task decomposition data 3D-LLM was trained on and our setting, which yields almost zero success rates of 3D-LLM without finetuning. Therefore, we finetune all models as baselines. For each baseline we have two variants: 1) wo Interaction: generate all actions all at once, and execute the actions sequentially in the environment; 2) w Interaction: generate an action one at a time, take the action feedback and generate the next action.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T4">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.2">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S5.T4.2.1.1">
       <th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T4.2.1.1.1">
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.1.1.2">
        success rate
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S5.T4.2.2.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.2.2.1.1">
        LLaVA wo Interaction
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.2.1.2">
        4.0
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T4.2.3.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.2.3.2.1">
        LLaVA w Interaction
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T4.2.3.2.2">
        14.5
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T4.2.4.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.2.4.3.1">
        3D-LLM wo Interaction
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T4.2.4.3.2">
        8.7
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T4.2.5.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.2.5.4.1">
        3D-LLM w Interaction
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T4.2.5.4.2">
        22.4
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T4.2.6.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T4.2.6.5.1">
        MultiPLY
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.2.6.5.2">
        <span class="ltx_text ltx_font_bold" id="S5.T4.2.6.5.2.1">
         30.2
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text" id="S5.T4.4.1.1" style="font-size:90%;">
       Table 4
      </span>
      :
     </span>
     <span class="ltx_text ltx_font_bold" id="S5.T4.5.2" style="font-size:90%;">
      Experimental Results of Multisensory Captioning.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p3">
    <p class="ltx_p" id="S5.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">
      Analysis
     </span>
     Table
     <a class="ltx_ref" href="#S5.T4" title="Table 4 ‣ 5.4 Task Decomposition ‣ 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     shows the task decomposition results. From the table, we observe that models without interaction have very poor results, probably because vision-language models have hallucination to a great extent. For example, the models could generate “retrieve a bread” when there’s no bread in the scene. MultiPLY outperforms the baseline models by a large margin. One reason could be that MultiPLY leverages multisensory information while the other two leverage visual information. The other reason might be that baseline models take the whole scene as inputs, thus could not attend to the nuanced object in the scene.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.5
    </span>
    Qualitative Examples
   </h3>
   <div class="ltx_para" id="S5.SS5.p1">
    <p class="ltx_p" id="S5.SS5.p1.1">
     Qualitative Examples are shown in Figure
     <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ 5.3 Multisensory Captioning ‣ 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , demonstrating the power of MultiPLY to interact with objects in the embodied environments and gather multisensory information. More examples can be found in the
     <span class="ltx_text ltx_font_bold" id="S5.SS5.p1.1.1">
      supplementary materials
     </span>
     .
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this paper, we propose MultiPLY, a multisensory LLM that could incorporate multisensory interactive data into large language models. We introduce Multisensory Universe, a dataset comprising 500k multisensory data collected by an agent actively exploring and interacting with an environment. One limitation of our model is that currently MultiPLY does not involve detailed navigation and control policy, but utilizes pre-defined policies for carrying out the actions. We think that such aspects are orthogonal to our study, and could be explored and seamlessly integrated into our framework in the future.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">
      Alayrac et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
      Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">
      Flamingo: a visual language model for few-shot learning, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib2.4.4.1" style="font-size:90%;">
      Anonymous [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.6.1" style="font-size:90%;">
      Anonymous.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
      DIFFTACTILE: A physics-based differentiable tactile simulator for contact-rich robotic manipulation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.9.2" style="font-size:90%;">
      Submitted to The Twelfth International Conference on Learning Representations
     </em>
     <span class="ltx_text" id="bib.bib2.10.3" style="font-size:90%;">
      , 2023.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.11.1" style="font-size:90%;">
      under review.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">
      Aytar et al. [2017]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
      Yusuf Aytar, Carl Vondrick, and Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">
      See, hear, and read: Deep aligned representations.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.9.1" style="font-size:90%;">
      arXiv preprint arXiv:1706.00932
     </em>
     <span class="ltx_text" id="bib.bib3.10.2" style="font-size:90%;">
      , 2017.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">
      Calandra et al. [2017]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
      Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wenzhen Yuan, Justin Lin, Edward H Adelson, and Sergey Levine.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">
      The feeling of success: Does touch sensing help predict grasp outcomes?
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">
      arXiv preprint arXiv:1710.05512
     </em>
     <span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">
      , 2017.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">
      Calandra et al. [2018]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
      Roberto Calandra, Andrew Owens, Dinesh Jayaraman, Justin Lin, Wenzhen Yuan, Jitendra Malik, Edward H Adelson, and Sergey Levine.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">
      More than a feeling: Learning to grasp and regrasp using vision and touch.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.9.1" style="font-size:90%;">
      IEEE Robotics and Automation Letters
     </em>
     <span class="ltx_text" id="bib.bib5.10.2" style="font-size:90%;">
      , 3(4):3300–3307, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">
      Chen et al. [2020a]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
      Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, and Kristen Grauman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">
      Soundspaces: Audio-visual navigation in 3d environments.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">
      Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16
     </em>
     <span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">
      , pages 17–36. Springer, 2020a.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">
      Chen et al. [2022a]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
      Changan Chen, Ruohan Gao, Paul Calamia, and Kristen Grauman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">
      Visual acoustic matching.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.10.2" style="font-size:90%;">
      Conference on Computer Vision and Pattern Recognition (CVPR)
     </em>
     <span class="ltx_text" id="bib.bib7.11.3" style="font-size:90%;">
      , 2022a.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">
      Chen et al. [2022b]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
      Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip Robinson, and Kristen Grauman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">
      Soundspaces 2.0: A simulation platform for visual-acoustic learning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.9.1" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </em>
     <span class="ltx_text" id="bib.bib8.10.2" style="font-size:90%;">
      , 35:8896–8911, 2022b.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">
      Chen et al. [2020b]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
      Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang, and Chuang Gan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">
      Generating visually aligned sound from videos.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">
      IEEE Transactions on Image Processing
     </em>
     <span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">
      , 29:8292–8302, 2020b.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">
      Clarke et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
      Samuel Clarke, Ruohan Gao, Mason Wang, Mark Rau, Julia Xu, Mark Rau, Jui-Hsien Wang, Doug James, and Jiajun Wu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">
      Realimpact: A dataset of impact sound fields for real objects.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">
      Conference on Computer Vision and Pattern Recognition (CVPR)
     </em>
     <span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">
      Deitke et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
      Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">
      Objaverse: A universe of annotated 3d objects, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">
      Driess et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
      Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">
      Palm-e: An embodied multimodal language model, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">
      Elizalde et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
      Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">
      Clap: Learning audio concepts from natural language supervision, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">
      Gan et al. [2019]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
      Chuang Gan, Hang Zhao, Peiaho Chen, David Cox, and Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">
      Self-supervised moving vehicle tracking with stereo sound.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.10.2" style="font-size:90%;">
      International Conference on Computer Vision (ICCV)
     </em>
     <span class="ltx_text" id="bib.bib14.11.3" style="font-size:90%;">
      , 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">
      Gan et al. [2020a]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
      Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenenbaum, and Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">
      Foley music: Learning to generate music from videos.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">
      Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16
     </em>
     <span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">
      , pages 758–775. Springer, 2020a.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">
      Gan et al. [2020b]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
      Chuang Gan, Deng Huang, Hang Zhao, Joshua B Tenenbaum, and Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">
      Music gesture for visual sound separation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     <span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">
      , pages 10478–10487, 2020b.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">
      Gan et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
      Chuang Gan, Yi Gu, Siyuan Zhou, Jeremy Schwartz, Seth Alter, James Traer, Dan Gutfreund, Joshua B Tenenbaum, Josh H McDermott, and Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">
      Finding fallen objects via asynchronous audio-visual integration.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.10.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     <span class="ltx_text" id="bib.bib17.11.3" style="font-size:90%;">
      , pages 10523–10533, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib18.4.4.1" style="font-size:90%;">
      Gao and Grauman [2019]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.6.1" style="font-size:90%;">
      Ruohan Gao and Kristen Grauman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
      2.5d visual sound.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.9.2" style="font-size:90%;">
      Conference on Computer Vision and Pattern Recognition (CVPR)
     </em>
     <span class="ltx_text" id="bib.bib18.10.3" style="font-size:90%;">
      , 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">
      Gao et al. [2018]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
      Ruohan Gao, Rogerio Feris, and Kristen Grauman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">
      Learning to separate object sounds by watching unlabeled video.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.10.2" style="font-size:90%;">
      Proceedings of the European Conference on Computer Vision (ECCV)
     </em>
     <span class="ltx_text" id="bib.bib19.11.3" style="font-size:90%;">
      , pages 35–53, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">
      Gao et al. [2021a]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
      Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, and Jiajun Wu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">
      Objectfolder: A dataset of objects with implicit visual, auditory, and tactile representations.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.9.1" style="font-size:90%;">
      ArXiv
     </em>
     <span class="ltx_text" id="bib.bib20.10.2" style="font-size:90%;">
      , abs/2109.07991, 2021a.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">
      Gao et al. [2021b]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
      Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, and Jiajun Wu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">
      Objectfolder: A dataset of objects with implicit visual, auditory, and tactile representations.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2109.07991
     </em>
     <span class="ltx_text" id="bib.bib21.10.2" style="font-size:90%;">
      , 2021b.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">
      Gao et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
      Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">
      Objectfolder 2.0: A multisensory object dataset for sim2real transfer.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">
      2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
     </em>
     <span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">
      , pages 10588–10598, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">
      Gao* et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
      Ruohan Gao*, Zilin Si*, Yen-Yu Chang*, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">
      Objectfolder 2.0: A multisensory object dataset for sim2real transfer.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.10.2" style="font-size:90%;">
      Conference on Computer Vision and Pattern Recognition (CVPR)
     </em>
     <span class="ltx_text" id="bib.bib23.11.3" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">
      Gao et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
      Ruohan Gao, Yiming Dou, Hao Li, Tanmay Agarwal, Jeannette Bohg, Yunzhu Li, Li Fei-Fei, and Jiajun Wu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">
      The objectfolder benchmark: Multisensory learning with neural and real objects.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     <span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">
      , pages 17276–17286, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">
      Garg et al. [2021]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
      Rishabh Garg, Ruohan Gao, and Kristen Grauman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">
      Geometry-aware multi-task learning for binaural audio generation from video.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.10.2" style="font-size:90%;">
      British Machine Vision Conference (BMVC)
     </em>
     <span class="ltx_text" id="bib.bib25.11.3" style="font-size:90%;">
      , 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">
      Gemmeke et al. [2017]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
      Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">
      Audio set: An ontology and human-labeled dataset for audio events.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">
      2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
     </em>
     <span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">
      , pages 776–780, 2017.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">
      Geng et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
      Tiantian Geng, Teng Wang, Jinming Duan, Runmin Cong, and Feng Zheng.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">
      Dense-localizing audio-visual events in untrimmed videos: A large-scale benchmark and baseline.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.10.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     <span class="ltx_text" id="bib.bib27.11.3" style="font-size:90%;">
      , pages 22942–22951, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">
      Girdhar et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
      Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">
      Imagebind: One embedding space to bind them all, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">
      Gu et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
      Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum, Antonio Torralba, Florian Shkurti, and Liam Paull.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">
      Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">
      Guo et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
      Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, and Pheng-Ann Heng.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">
      Point-bind &amp; point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">
      Han et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
      Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, and Yu Qiao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">
      Imagebind-llm: Multi-modality instruction tuning, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">
      Hong et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
      Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">
      3d-llm: Injecting the 3d world into large language models, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">
      Hu et al. [2018]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
      Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">
      A moving least squares material point method with displacement discontinuity and two-way rigid body coupling.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.9.1" style="font-size:90%;">
      ACM Transactions on Graphics (TOG)
     </em>
     <span class="ltx_text" id="bib.bib33.10.2" style="font-size:90%;">
      , 37:1 – 14, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">
      Kim et al. [2019]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
      Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">
      Audiocaps: Generating captions for audios in the wild.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib34.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.10.2" style="font-size:90%;">
      North American Chapter of the Association for Computational Linguistics
     </em>
     <span class="ltx_text" id="bib.bib34.11.3" style="font-size:90%;">
      , 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">
      Kirillov et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
      Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">
      Segment anything.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.9.1" style="font-size:90%;">
      ArXiv
     </em>
     <span class="ltx_text" id="bib.bib35.10.2" style="font-size:90%;">
      , abs/2304.02643, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">
      Li et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
      Hao Li, Yizhi Zhang, Junzhe Zhu, Shaoxiong Wang, Michelle A Lee, Huazhe Xu, Edward Adelson, Li Fei-Fei, Ruohan Gao, and Jiajun Wu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">
      See, hear, and feel: Smart sensory fusion for robotic manipulation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2212.03858
     </em>
     <span class="ltx_text" id="bib.bib36.10.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">
      Li et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
      Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">
      Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">
      Liu et al. [2023a]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
      Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">
      Visual instruction tuning, 2023a.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">
      Liu et al. [2023b]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
      Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">
      Visual instruction tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.08485
     </em>
     <span class="ltx_text" id="bib.bib39.10.2" style="font-size:90%;">
      , 2023b.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">
      Moon et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
      Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, and Anuj Kumar.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">
      Anymal: An efficient and scalable any-modality augmented language model, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">
      Narang et al. [2021]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
      Yashraj Narang, Balakumar Sundaralingam, Miles Macklin, Arsalan Mousavian, and Dieter Fox.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">
      Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib41.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.10.2" style="font-size:90%;">
      2021 IEEE International Conference on Robotics and Automation (ICRA)
     </em>
     <span class="ltx_text" id="bib.bib41.11.3" style="font-size:90%;">
      , pages 6444–6451. IEEE, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib42.4.4.1" style="font-size:90%;">
      OpenAI [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib42.6.1" style="font-size:90%;">
      OpenAI.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
      GPT-4 technical report.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.8.1" style="font-size:90%;">
      ArXiv
     </em>
     <span class="ltx_text" id="bib.bib42.9.2" style="font-size:90%;">
      , abs/2303.08774, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">
      Owens et al. [2016]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
      Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H Adelson, and William T Freeman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">
      Visually indicated sounds.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">
      Proceedings of the IEEE conference on computer vision and pattern recognition
     </em>
     <span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">
      , pages 2405–2413, 2016.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">
      Qi et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
      Qiutang Qi, Haonan Cheng, Yang Wang, Long Ye, and Shaobin Li.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">
      Rd-fgfs: A rule-data hybrid framework for fine-grained footstep sound synthesis from visual guidance.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.10.2" style="font-size:90%;">
      Proceedings of the 31st ACM International Conference on Multimedia
     </em>
     <span class="ltx_text" id="bib.bib44.11.3" style="font-size:90%;">
      , pages 8525–8533, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">
      Radford et al. [2021]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
      Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">
      Learning transferable visual models from natural language supervision, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">
      Ramakrishnan et al. [2021]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
      Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">
      Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">
      Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)
     </em>
     <span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">
      , 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">
      Savva et al. [2019]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
      Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">
      Habitat: A Platform for Embodied AI Research.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib47.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.10.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)
     </em>
     <span class="ltx_text" id="bib.bib47.11.3" style="font-size:90%;">
      , 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">
      Smith et al. [2020]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
      Edward Smith, Roberto Calandra, Adriana Romero, Georgia Gkioxari, David Meger, Jitendra Malik, and Michal Drozdzal.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">
      3d shape reconstruction from vision and touch.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.9.1" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </em>
     <span class="ltx_text" id="bib.bib48.10.2" style="font-size:90%;">
      , 33:14193–14206, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">
      Smith et al. [2021]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
      Edward Smith, David Meger, Luis Pineda, Roberto Calandra, Jitendra Malik, Adriana Romero Soriano, and Michal Drozdzal.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">
      Active 3d shape reconstruction from vision and touch.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.9.1" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </em>
     <span class="ltx_text" id="bib.bib49.10.2" style="font-size:90%;">
      , 34:16064–16078, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">
      Su et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
      Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, and Chuang Gan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">
      Physics-driven diffusion models for impact sound synthesis from videos.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.10.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     <span class="ltx_text" id="bib.bib50.11.3" style="font-size:90%;">
      , pages 9749–9759, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">
      Sun et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
      Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">
      3d-gpt: Procedural 3d modeling with large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2310.12945
     </em>
     <span class="ltx_text" id="bib.bib51.10.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">
      Suresh et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
      Sudharshan Suresh, Zilin Si, Joshua G Mangelson, Wenzhen Yuan, and Michael Kaess.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">
      Shapemap 3-d: Efficient shape mapping through dense touch and vision.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib52.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.10.2" style="font-size:90%;">
      2022 International Conference on Robotics and Automation (ICRA)
     </em>
     <span class="ltx_text" id="bib.bib52.11.3" style="font-size:90%;">
      , pages 7073–7080. IEEE, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">
      Touvron et al. [2023a]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
      Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">
      Llama: Open and efficient foundation language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2302.13971
     </em>
     <span class="ltx_text" id="bib.bib53.10.2" style="font-size:90%;">
      , 2023a.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">
      Touvron et al. [2023b]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
      Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">
      Llama: Open and efficient foundation language models, 2023b.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">
      Touvron et al. [2023c]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
      Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">
      Llama 2: Open foundation and fine-tuned chat models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2307.09288
     </em>
     <span class="ltx_text" id="bib.bib55.10.2" style="font-size:90%;">
      , 2023c.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib56.4.4.1" style="font-size:90%;">
      Wallace [2004]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib56.6.1" style="font-size:90%;">
      Mark T. Wallace.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
      The development of multisensory processes.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.8.1" style="font-size:90%;">
      Cognitive Processing
     </em>
     <span class="ltx_text" id="bib.bib56.9.2" style="font-size:90%;">
      , 5:69–83, 2004.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">
      Wang et al. [2023a]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
      Hao Wang, Zheng-Jun Zha, Liang Li, Xuejin Chen, and Jiebo Luo.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">
      Context-aware proposal–boundary network with structural consistency for audiovisual event localization.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.9.1" style="font-size:90%;">
      IEEE Transactions on Neural Networks and Learning Systems
     </em>
     <span class="ltx_text" id="bib.bib57.10.2" style="font-size:90%;">
      , 2023a.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">
      Wang et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
      Shaoxiong Wang, Mike Lambeta, Po-Wei Chou, and Roberto Calandra.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">
      Tacto: A fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib58.9.1" style="font-size:90%;">
      IEEE Robotics and Automation Letters
     </em>
     <span class="ltx_text" id="bib.bib58.10.2" style="font-size:90%;">
      , 7(2):3930–3937, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib59">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib59.5.5.1" style="font-size:90%;">
      Wang et al. [2023b]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib59.7.1" style="font-size:90%;">
      Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Zackory Erickson, David Held, and Chuang Gan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">
      Robogen: Towards unleashing infinite data for automated robot learning via generative simulation, 2023b.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib60">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib60.4.4.1" style="font-size:90%;">
      Xia and Zhao [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib60.6.1" style="font-size:90%;">
      Yan Xia and Zhou Zhao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">
      Cross-modal background suppression for audio-visual event localization.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib60.8.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib60.9.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     <span class="ltx_text" id="bib.bib60.10.3" style="font-size:90%;">
      , pages 19989–19998, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib61">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">
      Xu et al. [2020]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
      Haoming Xu, Runhao Zeng, Qingyao Wu, Mingkui Tan, and Chuang Gan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">
      Cross-modal relation-aware networks for audio-visual event localization.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib61.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib61.10.2" style="font-size:90%;">
      Proceedings of the 28th ACM International Conference on Multimedia
     </em>
     <span class="ltx_text" id="bib.bib61.11.3" style="font-size:90%;">
      , pages 3893–3901, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib62">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib62.5.5.1" style="font-size:90%;">
      Xu et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">
      Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">
      Pointllm: Empowering large language models to understand point clouds.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib62.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2308.16911
     </em>
     <span class="ltx_text" id="bib.bib62.10.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib63">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">
      Yadav et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
      Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva, et al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">
      Habitat-matterport 3d semantics dataset.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib63.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2210.05633
     </em>
     <span class="ltx_text" id="bib.bib63.10.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib64">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">
      Yang et al. [2023]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
      Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F Fouhey, and Joyce Chai.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib64.8.1" style="font-size:90%;">
      Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib64.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2309.12311
     </em>
     <span class="ltx_text" id="bib.bib64.10.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib65">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">
      Zhang et al. [2022]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
      Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">
      Opt: Open pre-trained transformer language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib65.9.1" style="font-size:90%;">
      arXiv preprint arXiv:2205.01068
     </em>
     <span class="ltx_text" id="bib.bib65.10.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib66">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">
      Zhao et al. [2018]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
      Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">
      The sound of pixels.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib66.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib66.10.2" style="font-size:90%;">
      The European Conference on Computer Vision (ECCV)
     </em>
     <span class="ltx_text" id="bib.bib66.11.3" style="font-size:90%;">
      , 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib67">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib67.5.5.1" style="font-size:90%;">
      Zhao et al. [2019]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">
      Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">
      The sound of motions.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib67.9.1" style="font-size:90%;">
      In
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib67.10.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     <span class="ltx_text" id="bib.bib67.11.3" style="font-size:90%;">
      , pages 1735–1744, 2019.
     </span>
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <nav class="ltx_TOC ltx_list_toc ltx_toc_toc">
  <h6 class="ltx_title ltx_title_contents">
   Contents
  </h6>
  <ol class="ltx_toclist">
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S1" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       1
      </span>
      Introduction
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S2" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       2
      </span>
      Related Works
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S3" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       3
      </span>
      The Multisensory-Universe Dataset
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS1" title="In 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.1
        </span>
        Inputting Interactive Objects into 3D Scenes
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS2" title="In 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.2
        </span>
        Object Sensor Data Acquisition
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS3" title="In 3 The Multisensory-Universe Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.3
        </span>
        Embodied Agents for Data Collection
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S4" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       4
      </span>
      MultiPLY
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS1" title="In 4 MultiPLY ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.1
        </span>
        Object-Centric Scene Representations
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS2" title="In 4 MultiPLY ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.2
        </span>
        Action Tokens
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS3" title="In 4 MultiPLY ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.3
        </span>
        State Tokens
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS4" title="In 4 MultiPLY ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.4
        </span>
        Training &amp; Inference
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S5" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       5
      </span>
      Experiments
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS1" title="In 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.1
        </span>
        Object Retrieval
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS2" title="In 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.2
        </span>
        Tool Use
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS3" title="In 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.3
        </span>
        Multisensory Captioning
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS4" title="In 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.4
        </span>
        Task Decomposition
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS5" title="In 5 Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.5
        </span>
        Qualitative Examples
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S6" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       6
      </span>
      Conclusion
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A1" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       A
      </span>
      Dataset
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_appendix">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#A1.SS1" title="In Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         A.1
        </span>
        More details on Scene Construction
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#A1.SS2" title="In Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         A.2
        </span>
        More details on Sensor Data Acquisition
       </span>
      </a>
      <ol class="ltx_toclist ltx_toclist_subsection">
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#A1.SS2.SSS1" title="In A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           A.2.1
          </span>
          Tactile
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#A1.SS2.SSS2" title="In A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           A.2.2
          </span>
          Impact Sound
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#A1.SS2.SSS3" title="In A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           A.2.3
          </span>
          Ambient Sound
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#A1.SS2.SSS4" title="In A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           A.2.4
          </span>
          Temperature
         </span>
        </a>
       </li>
      </ol>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#A1.SS3" title="In Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         A.3
        </span>
        More details on Task Construction
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A2" title="In MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       B
      </span>
      Experiments
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_appendix">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#A2.SS1" title="In Appendix B Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         B.1
        </span>
        Experimental Details
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#A2.SS2" title="In Appendix B Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         B.2
        </span>
        Ablative Studies
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#A2.SS3" title="In Appendix B Experiments ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         B.3
        </span>
        More Qualitative Examples
       </span>
      </a>
     </li>
    </ol>
   </li>
  </ol>
 </nav>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Dataset
  </h2>
  <section class="ltx_subsection" id="A1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.1
    </span>
    More details on Scene Construction
   </h3>
   <div class="ltx_para" id="A1.SS1.p1">
    <p class="ltx_p" id="A1.SS1.p1.1">
     In figure
     <a class="ltx_ref" href="#A1.F5" title="Figure 5 ‣ A.1 More details on Scene Construction ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , we show how we add new objects to the HM3D scenes. Specifically, ChatGPT is asked to generate: 1) object bounding boxes; 2) object material and material properties; 3) temperatures.
    </p>
   </div>
   <figure class="ltx_figure" id="A1.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="A1.F5.g1" src="/html/2401.08577/assets/x5.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A1.F5.2.1.1" style="font-size:90%;">
       Figure 5
      </span>
      :
     </span>
     <span class="ltx_text" id="A1.F5.3.2" style="font-size:90%;">
      Prompts for adding objects to the scene
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="A1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.2
    </span>
    More details on Sensor Data Acquisition
   </h3>
   <div class="ltx_para" id="A1.SS2.p1">
    <p class="ltx_p" id="A1.SS2.p1.1">
     In this section, we elaborate on how we get the sensor data of the objects in details.
    </p>
   </div>
   <section class="ltx_subsubsection" id="A1.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      A.2.1
     </span>
     Tactile
    </h4>
    <div class="ltx_para" id="A1.SS2.SSS1.p1">
     <p class="ltx_p" id="A1.SS2.SSS1.p1.1">
      DiffTactile
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib2" title="">
        <span class="ltx_text" style="font-size:90%;">
         2
        </span>
       </a>
       ]
      </cite>
      requires us to provide a set of parameters for tactile simulation of different objects. In addition to telling to the model whether we are inputting a rigid, elastic, or elasto-plastic object, we also need to specify the parameters such as Young’s modulus, Poisson’s ratio, Yield Strength and so on.
     </p>
    </div>
    <div class="ltx_para" id="A1.SS2.SSS1.p2">
     <p class="ltx_p" id="A1.SS2.SSS1.p2.1">
      As in the main paper, when ChatGPT adds objects to the scene, it also specifies what kinds of objects (
      <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS1.p2.1.1">
       e.g
      </span>
      , rigid, elastic, plastic) and the softness / deformability (in the description of language) of each object. In order to get the parameters required by DiffTactile, we prompt ChatGPT with the type and the softness / deformability description, as well as detailed definition of each parameter, and the possible values of the parameters of several few-shot examples. ChatGPT is asked to return the detailed parameter combinations of the given objects. For example, a soft bread corresponds to a smaller young’s modulus, while a harder one corresponds to a larger young’s modulus. We add the prompt in getting the parameters in Figure
      <a class="ltx_ref" href="#A1.F6" title="Figure 6 ‣ A.2.1 Tactile ‣ A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="A1.F6">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="A1.F6.g1" src=""/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="A1.F6.2.1.1" style="font-size:90%;">
        Figure 6
       </span>
       :
      </span>
      <span class="ltx_text" id="A1.F6.3.2" style="font-size:90%;">
       Prompts for getting the material parameters for the objects
      </span>
     </figcaption>
    </figure>
    <div class="ltx_para" id="A1.SS2.SSS1.p3">
     <p class="ltx_p" id="A1.SS2.SSS1.p3.1">
      We input the object into DiffTactile, normalize the shape of the gripper according to the object. We record the 2D initial position and final position of the markers in the gripper. And we turn the tactile readings into a 2D image, by drawing an arrowed line from the initial position to the final position. We show some examples of tactile images in Figure
      <a class="ltx_ref" href="#A1.F7" title="Figure 7 ‣ A.2.1 Tactile ‣ A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_tag">
        7
       </span>
      </a>
      . We sample 16 touching positions of each object. In training and evaluation, we randomly return one image of the object.
     </p>
    </div>
    <figure class="ltx_figure" id="A1.F7">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="155" id="A1.F7.g1" src="/html/2401.08577/assets/figures/tactile1.jpg" width="206"/>
      </div>
      <div class="ltx_flex_break">
      </div>
      <div class="ltx_flex_cell ltx_flex_size_1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="155" id="A1.F7.g2" src="/html/2401.08577/assets/figures/tactile2.jpg" width="206"/>
      </div>
      <div class="ltx_flex_break">
      </div>
      <div class="ltx_flex_cell ltx_flex_size_1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="155" id="A1.F7.g3" src="/html/2401.08577/assets/figures/tactile3.jpg" width="206"/>
      </div>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="A1.F7.2.1.1" style="font-size:90%;">
        Figure 7
       </span>
       :
      </span>
      <span class="ltx_text" id="A1.F7.3.2" style="font-size:90%;">
       Examples of tactile images.
      </span>
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="A1.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      A.2.2
     </span>
     Impact Sound
    </h4>
    <div class="ltx_para" id="A1.SS2.SSS2.p1">
     <p class="ltx_p" id="A1.SS2.SSS2.p1.1">
      ObjectFolder
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib20" title="">
        <span class="ltx_text" style="font-size:90%;">
         20
        </span>
       </a>
       ]
      </cite>
      stores the multi-modal information all in implicit fields. That is, by inputting a striking location to the sound implicit field of an object, we could get the impact sound of striking the object at the specific location. For each object, we randomly sample 10 locations in the mesh points to get the impact sound. In training and evaluation, we randomly return one impact sound of the object.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="A1.SS2.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      A.2.3
     </span>
     Ambient Sound
    </h4>
    <div class="ltx_para" id="A1.SS2.SSS3.p1">
     <p class="ltx_p" id="A1.SS2.SSS3.p1.1">
      AudioSet is paired with objects to represent ambient sound. The AudioSet ontology is organized in a hierarchy structure. From the root node to the leaf node, the description granularity becomes finer (e.g., Music - Musical instrument - Keyboard - Piano - Electric piano). Each ontology entry is attached with a description (e.g., “Glass: sounds associated with the non-crystalline amorphous solid that is often transparent and has widespread practical, technological, and decorative uses”). Each audio is labeled with multiple ontology entries tracing from the child node to the root node (e.g., the sound of the piano will be labeled with “Piano”, “Keyboard”, “Musical Instrument”, and “Music”, but without “Electric piano” since this piano is not electric). We prompt ChatGPT to match each ontology entry with object categories (Figure
      <a class="ltx_ref" href="#A1.F8" title="Figure 8 ‣ A.2.3 Ambient Sound ‣ A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_tag">
        8
       </span>
      </a>
      ).
     </p>
    </div>
    <figure class="ltx_figure" id="A1.F8">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="A1.F8.g1" src="/html/2401.08577/assets/x7.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="A1.F8.2.1.1" style="font-size:90%;">
        Figure 8
       </span>
       :
      </span>
      <span class="ltx_text" id="A1.F8.3.2" style="font-size:90%;">
       Prompts to match AudioSet with Objects
      </span>
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="A1.SS2.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      A.2.4
     </span>
     Temperature
    </h4>
    <div class="ltx_para" id="A1.SS2.SSS4.p1">
     <p class="ltx_p" id="A1.SS2.SSS4.p1.1">
      We add the prompt in getting the temperature in Figure
      <a class="ltx_ref" href="#A1.F9" title="Figure 9 ‣ A.2.4 Temperature ‣ A.2 More details on Sensor Data Acquisition ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
       <span class="ltx_text ltx_ref_tag">
        9
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="A1.F9">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="121" id="A1.F9.g1" src="/html/2401.08577/assets/x8.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="A1.F9.2.1.1" style="font-size:90%;">
        Figure 9
       </span>
       :
      </span>
      <span class="ltx_text" id="A1.F9.3.2" style="font-size:90%;">
       Prompts on generating temperature for each object
      </span>
     </figcaption>
    </figure>
   </section>
  </section>
  <section class="ltx_subsection" id="A1.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.3
    </span>
    More details on Task Construction
   </h3>
   <div class="ltx_para" id="A1.SS3.p1">
    <p class="ltx_p" id="A1.SS3.p1.1">
     In Figure
     <a class="ltx_ref" href="#A1.F10" title="Figure 10 ‣ A.3 More details on Task Construction ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       10
      </span>
     </a>
     , we illustrate the prompts for generating the language task data for Multisensory-Universe. Specifically, the actions could return the expected observation in the form of language (
     <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.1">
      e.g.,
     </span>
     tactile map of and object when touching). We insert that into the state tokens for placeholder, and after the agent has executed the actions in the space and gets the observations, we append the observations back to the state tokens.
    </p>
   </div>
   <figure class="ltx_table" id="A1.T5">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.2">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="A1.T5.2.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T5.2.1.1.1">
        Ablative Model
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T5.2.1.1.2">
        Acc
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="A1.T5.2.2.1">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.2.2.1.1">
        MultiPLY Vision
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.2.2.1.2">
        21.0
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.3.2">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.3.2.1">
        MultiPLY Audio
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.3.2.2">
        13.2
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.4.3">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.4.3.1">
        MultiPLY Tactile
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.4.3.2">
        10.5
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.5.4">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.5.4.1">
        MultiPLY Temperature
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.5.4.2">
        11.2
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.6.5">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.2.6.5.1">
        MultiPLY Vision, Audio
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.2.6.5.2">
        31.8
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.7.6">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.7.6.1">
        MultiPLY Vision, Tactile
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.7.6.2">
        24.3
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.8.7">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.8.7.1">
        MultiPLY Vision, Temperature
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.8.7.2">
        25.7
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.9.8">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.9.8.1">
        MultiPLY Audio, Tactile
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.9.8.2">
        20.6
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.10.9">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.10.9.1">
        MultiPLY Audio, Temperature
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.10.9.2">
        23.4
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.11.10">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.11.10.1">
        MultiPLY Tactile, Temperature
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.11.10.2">
        18.9
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.12.11">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.2.12.11.1">
        MultiPLY Vision, Audio, Tactile
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.2.12.11.2">
        45.3
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.13.12">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.13.12.1">
        MultiPLY Vision, Tactile, Temperature
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.13.12.2">
        41.4
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.14.13">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.14.13.1">
        MultiPLY Vision, Audio, Temperature
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.14.13.2">
        45.3
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.15.14">
       <td class="ltx_td ltx_align_left ltx_border_r" id="A1.T5.2.15.14.1">
        MultiPLY Audio, Tactile, Temperature
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T5.2.15.14.2">
        37.7
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T5.2.16.15">
       <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="A1.T5.2.16.15.1">
        MultiPLY
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.2.16.15.2">
        56.7
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text" id="A1.T5.3.1.1" style="font-size:90%;">
       Table 5
      </span>
      :
     </span>
     <span class="ltx_text" id="A1.T5.4.2" style="font-size:90%;">
      Ablative Study of MultiPLY
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A1.F10">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="357" id="A1.F10.g1" src="/html/2401.08577/assets/x9.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A1.F10.2.1.1" style="font-size:90%;">
       Figure 10
      </span>
      :
     </span>
     <span class="ltx_text" id="A1.F10.3.2" style="font-size:90%;">
      Prompts for task construction
     </span>
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    Experimental Details
   </h3>
   <div class="ltx_para" id="A2.SS1.p1">
    <p class="ltx_p" id="A2.SS1.p1.1">
     We tune the model based on the llava-v1.5-7b checkpoint of the LLaVA model. We use Adam optimizer with learning rate of 1e-6. We train the model on 4*132 V100s. We use a batch size of 2112. The training of multi-modal adapters takes 2 hours, while the whole finetuning takes less day 1 day to complete.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p2">
    <p class="ltx_p" id="A2.SS1.p2.1">
     We use the mm projector of the original LLaVA for adapting scene representations and object point clouds to the LLM. The sound, tactile and temperature adapters are all one linear layer with input size 1024 and output size 1024.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p3">
    <p class="ltx_p" id="A2.SS1.p3.1">
     We use the default CLIP vision encoder of LLaVA to encode all objects, point clouds, tactile and temperature images. Specifically, for objects, we use segment anything
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib35" title="">
       <span class="ltx_text" style="font-size:90%;">
        35
       </span>
      </a>
      ]
     </cite>
     to get the objects out of 2D objects, mask out other objects and background, and crop the image to the size of the object, and use CLIP encoder to encode the object. We follow ConceptGraph
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib29" title="">
       <span class="ltx_text" style="font-size:90%;">
        29
       </span>
      </a>
      ]
     </cite>
     to merge the objects from 2D to 3D. For scene construction, each object has one CLIP feature. For object details (point cloud), we project the 2D pixels of the objects to 3D, and get the point clouds of the objects.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.2
    </span>
    Ablative Studies
   </h3>
   <div class="ltx_para" id="A2.SS2.p1">
    <p class="ltx_p" id="A2.SS2.p1.1">
     In Table
     <a class="ltx_ref" href="#A1.T5" title="Table 5 ‣ A.3 More details on Task Construction ‣ Appendix A Dataset ‣ MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , we show additional experimental results where we explore MultiPLY with single, double or triple modalities.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.3
    </span>
    More Qualitative Examples
   </h3>
   <figure class="ltx_figure" id="A2.F11">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="533" id="A2.F11.g1" src="/html/2401.08577/assets/x10.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A2.F11.2.1.1" style="font-size:90%;">
       Figure 11
      </span>
      :
     </span>
     <span class="ltx_text" id="A2.F11.3.2" style="font-size:90%;">
      More qualitative examples of MultiPLY
     </span>
    </figcaption>
   </figure>
  </section>
 </section>
</article>
