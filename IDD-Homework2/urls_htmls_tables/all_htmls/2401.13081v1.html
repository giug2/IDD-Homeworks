<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.13081] Free Form Medical Visual Question Answering in Radiology</title><meta property="og:description" content="Visual Question Answering (VQA) in the medical domain presents a unique, interdisciplinary challenge, combining fields such as Computer Vision, Natural Language Processing, and Knowledge Representation. Despite its imp…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Free Form Medical Visual Question Answering in Radiology">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Free Form Medical Visual Question Answering in Radiology">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.13081">

<!--Generated on Tue Feb 27 08:15:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Free Form Medical Visual Question Answering in Radiology</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abhishek Narayanan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rushabh Musthyala
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rahul Sankar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anirudh Prasad Nistala
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pranav Singh
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jacopo Cirrone
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering (VQA) in the medical domain presents a unique, interdisciplinary challenge, combining fields such as Computer Vision, Natural Language Processing, and Knowledge Representation. Despite its importance, research in medical VQA has been scant, only gaining momentum since 2018. Addressing this gap, our research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods. We innovatively augment the SLAKE dataset, enabling our model to respond to a more diverse array of questions, not limited to the immediate content of radiology or pathology images. Our model achieves a top-1 accuracy of 79.55% with a less complex architecture, demonstrating comparable performance to current state-of-the-art models. This research not only advances medical VQA but also opens avenues for practical applications in diagnostic settings.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
<p id="p2.4" class="ltx_p"><sup id="p2.4.1" class="ltx_sup"><span id="p2.4.1.1" class="ltx_text ltx_font_italic">1</span></sup>Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, New York, USA 
<br class="ltx_break"><sup id="p2.4.2" class="ltx_sup"><span id="p2.4.2.1" class="ltx_text ltx_font_italic">2</span></sup>Center for Data Science, New York University, New York, USA 
<br class="ltx_break"><sup id="p2.4.3" class="ltx_sup"><span id="p2.4.3.1" class="ltx_text ltx_font_italic">3</span></sup>Colton Center for Autoimmunity, NYU Grossman School of Medicine, New York, USA 
<br class="ltx_break"><sup id="p2.4.4" class="ltx_sup"><span id="p2.4.4.1" class="ltx_text ltx_font_italic">4</span></sup>Corresponding Author (E-mail: cirrone@courant.nyu.edu) 
<br class="ltx_break"></p>
</div>
<div id="p3" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advancements in Visual Question Answering (VQA) in the medical and healthcare sectors have garnered significant interest, building upon extensive research in general, free-form, and open-ended VQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>. Unlike traditional AI agents in medicine, often constrained to specific organs or diseases, a medical VQA system should adeptly handle natural language questions, comprehend medical imagery, and provide diagnostically accurate and reliable responses.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Nevertheless, medical VQA faces unique challenges compared to its generic counterpart. For example, while large-scale annotated datasets like VQA <cite class="ltx_cite ltx_citemacro_citep">(Al-Sadi et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> exist for general VQA, medical VQA datasets are smaller, requiring costly expert annotation and specialized medical knowledge. Synthetically generating question-image pairs is typically inappropriate due to the need for clinical relevance and domain-specific expertise. Additionally, generic VQA models struggle to adapt to medical images. These models require further specialization and the ability to focus on finer details, such as microscopic lesions, crucial for diagnosis. The unrestricted and frequently highly technical nature of the input questions, which may contain medical terminology not adequately represented by generic language models trained on expansive databases like Wikipedia, further increases the complexity of medical VQA. 
<br class="ltx_break"></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Medical VQA holds immense potential in healthcare, offering valuable support where clinician availability is limited. Given the vast number of queries and the operational scale, it is often challenging for clinicians to address each query promptly. This can lead to delays in addressing critical health inquiries, potentially slowing down the diagnosis of severe conditions with significant consequences. Furthermore, search engine responses, while abundant, tend to be generic, error-prone, irrelevant, and sometimes misleading. This underscores the necessity for an AI system capable of analyzing medical images and providing specific answers to related questions. Such a system could also assist clinicians by offering a secondary opinion on interpreting complex images.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In our work, we introduce an enhanced radiology dataset used for the pretraining of domain-specific visual encoders. Our experiments with various deep learning models focus on efficient image and text representation learning. We demonstrate that intra-domain transfer learning is more effective than inter-domain transfer learning for medical VQA tasks. Our proposed method not only matches benchmark accuracy but also has a simpler architectural design.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Survey</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Existing Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To evaluate the performance of VQA models in the medical field, various datasets have been created. In this study, we specifically focus on the radiology sector, thereby concentrating our review on datasets relevant to radiology imagery.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">A notable example for benchmarking medical VQA models is the VQA-Med dataset<cite class="ltx_cite ltx_citemacro_citep">(Ben Abacha et al., <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>. This dataset is particularly rich in content related to radiology images and reports. It comprises 4,500 radiology images paired with 4,500 question-and-answer combinations for training. Additionally, it includes sets of 500 images and 500 corresponding question-answer pairs each for both validation and testing purposes.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In addition to the VQA-Med dataset, there are other notable datasets in the medical VQA field. The VQA-RAD dataset<cite class="ltx_cite ltx_citemacro_citep">(Lau et al., <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite>, for instance, includes 315 radiology images accompanied by 3,515 question-answer pairs. Another significant resource is the ChestX-ray8 dataset<cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib54" title="" class="ltx_ref">2017b</a>)</cite>, which boasts over 100,000 chest X-ray images paired with associated textual reports. This dataset has been instrumental not only for VQA but also for various other medical image analysis tasks. Moreover, the SLAKE dataset<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib37" title="" class="ltx_ref">2021d</a>)</cite> contributes to the diversity of resources. It is a bilingual VQA dataset containing 642 radiology images from various body parts, along with more than 15,000 question-answer pairs.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Related Work</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In their survey, <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> analyzed 46 existing medical VQA works, 39 of which are variations of a common underlying structure, as shown in Fig <a href="#S2.F1" title="Figure 1 ‣ 2.2 Related Work ‣ 2 Literature Survey ‣ Free Form Medical Visual Question Answering in Radiology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This structure is known as the joint embedding framework, a baseline model frequently used for comparison. Based on general VQA, this framework has an image vectorizer, a question vectorizer, a fusion algorithm that combines features from both modes, and an answer generator that can be used as either a classifier or a generative model. The survey shows that a lot of different methods <cite class="ltx_cite ltx_citemacro_citep">(Gong et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>; Gupta et al., <a href="#bib.bib16" title="" class="ltx_ref">2021a</a>; Sharma et al., <a href="#bib.bib46" title="" class="ltx_ref">2021b</a>; Liu et al., <a href="#bib.bib35" title="" class="ltx_ref">2021b</a>)</cite> use CNN models trained on ImageNet data <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib13" title="" class="ltx_ref">2009</a>)</cite>, mainly ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite>, to do tasks using datasets like VQA-RAD and SLAKE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib37" title="" class="ltx_ref">2021d</a>; Lau et al., <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite> that are important to our study. These models use the pretrained weights for either initial weight setting or end-to-end fine-tuning. Despite its theoretical viability, using ImageNet, which has a data distribution vastly different from radiology, might not yield optimal results. Nevertheless, this practice is widespread, primarily due to the scarcity of large annotated medical datasets suitable for supervised pretraining of the image model.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2401.13081/assets/fig_1.png" id="S2.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="256" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Joint embedding framework for Medical VQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite></figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For the text encoder component, language models often employed include variations of recurrent neural networks, GloVe <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> , and other word embedding methodologies<cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Liu et al., <a href="#bib.bib36" title="" class="ltx_ref">2021c</a>)</cite>. While models pre-trained on general domain data exhibit reasonable performance, there has been limited advancement in enhancing the text encoding channel. However, recent approaches<cite class="ltx_cite ltx_citemacro_citep">(Jung et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>; Chen et al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> have begun integrating models like BioBERT<cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite>, which are pre-trained on medical datasets. These integrations have not only surpassed previous benchmarks but also highlighted the promising potential of research in this area.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In the realm of fusion approaches for combining image and text modalities, some studies have implemented straightforward techniques like element-wise product or feature concatenation <cite class="ltx_cite ltx_citemacro_citep">(Allaouzi &amp; Ahmed, <a href="#bib.bib6" title="" class="ltx_ref">2018</a>; Ambati &amp; Dudyala, <a href="#bib.bib8" title="" class="ltx_ref">2018</a>; Gupta et al., <a href="#bib.bib17" title="" class="ltx_ref">2021b</a>)</cite> , drawing inspiration from generic VQA. These methods are somewhat effective, but not entirely optimal, as they often fail to adequately capture the interaction between the two modalities, particularly in pinpointing the image regions targeted by the question. While multimodal pooling has shown effectiveness in enhancing accuracy in generic VQA, it does come with increased computational demands. Despite this, only a few existing approaches have adopted these pooling methods<cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib56" title="" class="ltx_ref">2017</a>, <a href="#bib.bib57" title="" class="ltx_ref">2018</a>)</cite> or have proposed unique fusion techniques<cite class="ltx_cite ltx_citemacro_citep">(Sharma et al., <a href="#bib.bib45" title="" class="ltx_ref">2021a</a>; Vu et al., <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>, indicating potential areas for further advancement. Attention-based methods, as cited in<cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib27" title="" class="ltx_ref">2018</a>; Lu et al., <a href="#bib.bib38" title="" class="ltx_ref">2016</a>; Vaswani et al., <a href="#bib.bib51" title="" class="ltx_ref">2017</a>)</cite>, have significantly improved upon baseline models in generic VQA. However, their adoption in medical VQA is limited, as these complex architectures often rely on the extensive data available in the general domain, a luxury not typically available in medical datasets.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Recently, a limited number of approaches have begun to investigate the pretraining of image models to create more effective image representations. This exploration utilizes supervised or semi-supervised learning on alternative medical data sources<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib34" title="" class="ltx_ref">2021a</a>)</cite>. Eslami et al. <cite class="ltx_cite ltx_citemacro_citep">(Eslami et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> delved into contrastive multi-modal learning, employing embeddings generated through contrastive language-image pretraining (CLiP) <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>. However, their methodology did not extend to the language model. Their proposed model is notably complex, incorporating an additional autoencoder (AE) alongside CLiP for image encoding. Despite the complexity, these recent developments in the field have shown encouraging results, which our proposed model seeks to build upon and refine.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Baseline models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">For our experimental comparison, we have chosen the previously mentioned joint embedding framework as our baseline model. This framework employs VGGNet <cite class="ltx_cite ltx_citemacro_citep">(Simonyan &amp; Zisserman, <a href="#bib.bib48" title="" class="ltx_ref">2014</a>)</cite> as the backbone for the image encoder and a bidirectional LSTM for the text encoder. The integration of these two modalities is achieved through an element-wise product prior to classification. In our approach, we conceptualize VQA as a classification task rather than a language generation problem. While this method is advantageous for generating short answers, it tends to be less effective for responses that require longer phrases or sentences. However, to maintain simplicity in model evaluation and ensure consistent comparisons against state-of-the-art models on the SLAKE benchmark dataset, we have structured the architecture as a classifier model. 
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The traditional transfer learning approach, as applied in the context described above, encounters specific challenges within the medical domain. In the baseline model, the image and text encoders are initialized using weights from models pre-trained on datasets like ImageNet. This strategy is based on the assumption that there could be underlying, shared knowledge beneficial to the target domain. This hypothesis persists despite the apparent disconnect in data distribution between general datasets like ImageNet and the more specialized fields of radiology and medicine.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">He et al. <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> highlighted that although an ImageNet pre-trained model can accelerate convergence, it may not enhance performance. Particularly in larger medical datasets, the advantage of an ImageNet pre-trained model over simpler models is negligible<cite class="ltx_cite ltx_citemacro_citep">(Raghu et al., <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>. Conversely, Zhang et al.<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite> demonstrated that an inappropriate selection of a foundational dataset or model for pre-training could lead to worse performance than foregoing transfer learning altogether. With these insights, our experiments aim to investigate the efficacy of transfer learning from models specifically trained on radiology image datasets, such as a DenseNet<cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> model pre-trained on the RadImageNet dataset<cite class="ltx_cite ltx_citemacro_citep">(Mei et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, and medical texts, like BioBERT<cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite>. This exploration will help us assess the relative benefits and drawbacks of cross-domain versus intra-domain transfer for our particular case. We are also looking into self-supervised learning with CASS<cite class="ltx_cite ltx_citemacro_citep">(Singh &amp; Cirrone, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>. In this case, we train the CNN-Visual Transformer (ViT) cross-architecture model on a larger set of x-ray images before making it work better for our task.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Augmentation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A significant challenge in medical VQA, as reported in numerous studies, is the limited size of available datasets. For perspective, the general VQA dataset<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib4" title="" class="ltx_ref">vis, </a>)</cite> contains over 200,000 images, whereas medical-specific datasets like VQA-RAD<cite class="ltx_cite ltx_citemacro_citep">(Lau et al., <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite> and SLAKE<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib37" title="" class="ltx_ref">2021d</a>)</cite> only have a few hundred images. Addressing this issue, Kovaleva et al.<cite class="ltx_cite ltx_citemacro_citep">(Kovaleva et al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite> utilized MIMIC-CXR reports and images to create conversational-style question-answer pairs, employing the Chexpert <cite class="ltx_cite ltx_citemacro_citep">(Irvin et al., <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> to generate QA pairs for each image. Building on this methodology, we aim to generate additional QA pairs from existing radiology datasets available online. We applied this strategy to datasets such as Chest X-rays (Indiana University) <cite class="ltx_cite ltx_citemacro_citep">(Raddar, <a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>, COVID CXR-2<cite class="ltx_cite ltx_citemacro_citep">(kag, <a href="#bib.bib1" title="" class="ltx_ref">a</a>)</cite>, RSNA Pneumonia Detection<cite class="ltx_cite ltx_citemacro_citep">(kag, <a href="#bib.bib2" title="" class="ltx_ref">b</a>)</cite>, and the NIH Chest X-rays dataset<cite class="ltx_cite ltx_citemacro_citep">(of Health Chest X-Ray Dataset, <a href="#bib.bib40" title="" class="ltx_ref">2018</a>)</cite>. This approach yielded a combined dataset of approximately 20,000 QA pairs, nearly triple the size of the SLAKE dataset. Our comprehensive dataset covers a broader spectrum of questions, having been trained on a diverse collection of images and reports. It encompasses over 12 diseases and includes various body parts, scan orientations, and comments on different images. We created this dataset with the intention of expanding the range of questions and answers that a trained model can effectively handle. Additionally, the increased size of our dataset mitigates the risk of overfitting.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Image Encoder Pre-training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">A notable concern highlighted in existing literature pertains to the quality of image encodings in multi-modal representations. General-purpose VQA methods often benefit from leveraging weights of image models pre-trained on the expansive ImageNet dataset<cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib13" title="" class="ltx_ref">2009</a>)</cite>, which houses over a million images. However, the type of images in ImageNet significantly differs from those in radiology, leading to less optimal pre-trained ImageNet weights for encoding radiology images. To address this, we opted to use an image encoder pre-trained specifically on radiology images. Theoretically, this approach should yield more accurate image encodings and, consequently, enhance overall performance. We utilized the DenseNet and ElasticNet models from TorchXRayVision <cite class="ltx_cite ltx_citemacro_citep">(Cohen et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> , a Python library featuring CNN models pre-trained on a blend of radiology datasets including RSNA Pneumonia Detection<cite class="ltx_cite ltx_citemacro_citep">(kag, <a href="#bib.bib2" title="" class="ltx_ref">b</a>)</cite>, NIH Chest X-rays dataset<cite class="ltx_cite ltx_citemacro_citep">(of Health Chest X-Ray Dataset, <a href="#bib.bib40" title="" class="ltx_ref">2018</a>)</cite>, PadChest<cite class="ltx_cite ltx_citemacro_citep">(Bustos et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, CheXpert<cite class="ltx_cite ltx_citemacro_citep">(Irvin et al., <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> , and MIMIC<cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>. Additionally, we experimented with pre-training the CASS architecture<cite class="ltx_cite ltx_citemacro_citep">(Singh &amp; Cirrone, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite> using images from our larger combined dataset in a self-supervised manner.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Joint learning of effective multi-modal representations with cross-modal supervision</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In the field of generic VQA, recent studies<cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite> have shown significant success using Contrastive Language-Image Pre-training (CLIP) for learning cross-modal supervision with extensive image-text pairs. However, the application of CLIP has been mainly focused on general-domain multi-modal challenges. Drawing inspiration from these developments, PubMedClip<cite class="ltx_cite ltx_citemacro_citep">(Eslami et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> utilized a variant of CLIP<cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>, fine-tuned with medical image-text pairs from the ROCO dataset<cite class="ltx_cite ltx_citemacro_citep">(Pelka et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>, achieving state-of-the-art results on the VQA-RAD<cite class="ltx_cite ltx_citemacro_citep">(Lau et al., <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite> and SLAKE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib37" title="" class="ltx_ref">2021d</a>)</cite> datasets. Nevertheless, PubMedClip has two main limitations: firstly, it functions solely as a pre-trained visual encoder, not exploiting CLIP’s full multi-modal potential for encoding both images and text. Secondly, it employs CLIP’s image and text encoders pre-trained on general-domain data, which are not specifically related to radiology. To address these issues, we propose the use of MedCLiP<cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite>, trained on the MedPix dataset<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">med, </a>)</cite>, which includes MRI, X-Ray, and CT Scan data. MedCLiP utilizes ClinicalBERT<cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> for text encoding and ResNet50 for image encoding. In our pipeline, MedCLiP is integrated into both the image and text embedding channels, initially freezing its weights for feature extraction. Subsequently, we fine-tune the derived embeddings for downstream VQA tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data And Experiment Setup</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In our research, we employ the SLAKE dataset<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib37" title="" class="ltx_ref">2021d</a>)</cite> for fine-tuning and comprehensive evaluation of our proposed models, benchmarking them against existing standards. SLAKE is a robust dataset featuring 642 radiology images sourced from three open-source datasets<cite class="ltx_cite ltx_citemacro_citep">(Simpson et al., <a href="#bib.bib49" title="" class="ltx_ref">2019</a>; Wang et al., <a href="#bib.bib53" title="" class="ltx_ref">2017a</a>; Kavur et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>, encompassing a range of modalities (CT, MRI, X-Ray) and body parts (head, neck, chest, abdomen, and pelvic cavity). It includes 14,028 question-answer pairs in English and Chinese, curated from experienced doctors who selected or modified pre-defined questions. These questions are categorized by type and balanced to mitigate statistical bias. The dataset is divided into training (70%), validation (15%), and test (15%) sets at the image level for each body part-modality category (e.g., headCT, chestXRay, etc.), yielding 450, 96, and 96 images for training, validation, and testing, respectively. Our study focuses exclusively on the English question-answer subset (7,000 pairs) to align with the benchmark. The experimental pipeline is outlined in Fig <a href="#S2.F1" title="Figure 1 ‣ 2.2 Related Work ‣ 2 Literature Survey ‣ Free Form Medical Visual Question Answering in Radiology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We benchmark against the accuracy of PubMedClip <cite class="ltx_cite ltx_citemacro_citep">(Eslami et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> on SLAKE. For training in all experiments, we use AdaDelta optimization. These experiments are conducted on 2 V100 GPUs over 150 epochs, with a batch size of 32.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We reproduced the results of <cite class="ltx_cite ltx_citemacro_citep">(Eslami et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> on the SLAKE dataset and obtained an accuracy (top-1) of 79.45%. We also implemented and tested the aforementioned baseline model with the VGGNet+LSTM backbone, which performs reasonably well, producing a test accuracy of 75% on the SLAKE dataset but leaving out scope for tremendous improvement in the state-of-the-art. By bridging the gaps we identified in existing work, we hypothesize that the proposed approach should perform equally well or outperform the state-of-the-art PubMedClip model on the aforementioned datasets. Fig. <a href="#S5.F2" title="Figure 2 ‣ 5 Results ‣ Free Form Medical Visual Question Answering in Radiology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. shows the output of various models tested as part of the PubMedClip paper implementation vs. our baseline.
<br class="ltx_break"></p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2401.13081/assets/fig_2.png" id="S5.F2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="294" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Predictions of proposed model vs that of PubMedClip <cite class="ltx_cite ltx_citemacro_citep">(Eslami et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite></figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5 Results ‣ Free Form Medical Visual Question Answering in Radiology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. portrays the accuracy metric for the various models tested by the authors of <cite class="ltx_cite ltx_citemacro_citep">(Eslami et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> vs. our initial baseline. The learning curves of the models that were trained have been portrayed in Fig. <a href="#S5.F3" title="Figure 3 ‣ 5 Results ‣ Free Form Medical Visual Question Answering in Radiology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2401.13081/assets/fig_3.png" id="S5.F3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="270" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Training and validation learning curves for our experiments</figcaption>
</figure>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
<span id="S5.T1.2.1" class="ltx_text ltx_font_bold">Comparison of our models with existing approaches wrt accuracy</span></figcaption>
<table id="S5.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.3.1.1" class="ltx_tr">
<th id="S5.T1.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.1.1.1.1.1" class="ltx_p" style="width:128.0pt;"><span id="S5.T1.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Image Encoder</span></span>
</span>
</th>
<th id="S5.T1.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T1.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.1.1.2.1.1" class="ltx_p" style="width:99.6pt;"><span id="S5.T1.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Language Encoder</span></span>
</span>
</th>
<th id="S5.T1.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T1.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.1.1.3.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T1.3.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Validation Accuracy</span></span>
</span>
</th>
<th id="S5.T1.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T1.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.1.1.4.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T1.3.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Test       Accuracy</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.3.2.1" class="ltx_tr">
<td id="S5.T1.3.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.2.1.1.1.1" class="ltx_p" style="width:128.0pt;">ElasticNet AE (Pretrained) <cite class="ltx_cite ltx_citemacro_citep">(Zou &amp; Hastie, <a href="#bib.bib59" title="" class="ltx_ref">2005</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.2.1.2.1.1" class="ltx_p" style="width:99.6pt;">BiLSTM</span>
</span>
</td>
<td id="S5.T1.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.2.1.3.1.1" class="ltx_p" style="width:51.2pt;">68.85%</span>
</span>
</td>
<td id="S5.T1.3.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.2.1.4.1.1" class="ltx_p" style="width:51.2pt;">64.43%</span>
</span>
</td>
</tr>
<tr id="S5.T1.3.3.2" class="ltx_tr">
<td id="S5.T1.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.3.2.1.1.1" class="ltx_p" style="width:128.0pt;">DenseNet (Pretrained) <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.3.2.2.1.1" class="ltx_p" style="width:99.6pt;">BiLSTM</span>
</span>
</td>
<td id="S5.T1.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.3.2.3.1.1" class="ltx_p" style="width:51.2pt;">81.29%</span>
</span>
</td>
<td id="S5.T1.3.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.3.2.4.1.1" class="ltx_p" style="width:51.2pt;">77.09%</span>
</span>
</td>
</tr>
<tr id="S5.T1.3.4.3" class="ltx_tr">
<td id="S5.T1.3.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.4.3.1.1.1" class="ltx_p" style="width:128.0pt;">VGG16 <cite class="ltx_cite ltx_citemacro_citep">(Simonyan &amp; Zisserman, <a href="#bib.bib48" title="" class="ltx_ref">2014</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.4.3.2.1.1" class="ltx_p" style="width:99.6pt;">BiLSTM</span>
</span>
</td>
<td id="S5.T1.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.4.3.3.1.1" class="ltx_p" style="width:51.2pt;">76.44%</span>
</span>
</td>
<td id="S5.T1.3.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.4.3.4.1.1" class="ltx_p" style="width:51.2pt;">75.0%</span>
</span>
</td>
</tr>
<tr id="S5.T1.3.5.4" class="ltx_tr">
<td id="S5.T1.3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.5.4.1.1.1" class="ltx_p" style="width:128.0pt;">MedCLIP <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.5.4.2.1.1" class="ltx_p" style="width:99.6pt;">BiLSTM</span>
</span>
</td>
<td id="S5.T1.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.5.4.3.1.1" class="ltx_p" style="width:51.2pt;">80.53%</span>
</span>
</td>
<td id="S5.T1.3.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.5.4.4.1.1" class="ltx_p" style="width:51.2pt;"><span id="S5.T1.3.5.4.4.1.1.1" class="ltx_text ltx_font_bold">79.55%</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.3.6.5" class="ltx_tr">
<td id="S5.T1.3.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.6.5.1.1.1" class="ltx_p" style="width:128.0pt;">MedCLIP <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.6.5.2.1.1" class="ltx_p" style="width:99.6pt;">MedCLIP <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.6.5.3.1.1" class="ltx_p" style="width:51.2pt;">61.25%</span>
</span>
</td>
<td id="S5.T1.3.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.6.5.4.1.1" class="ltx_p" style="width:51.2pt;">58.34%</span>
</span>
</td>
</tr>
<tr id="S5.T1.3.7.6" class="ltx_tr">
<td id="S5.T1.3.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.7.6.1.1.1" class="ltx_p" style="width:128.0pt;">CASS-ViT <cite class="ltx_cite ltx_citemacro_citep">(Singh &amp; Cirrone, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.7.6.2.1.1" class="ltx_p" style="width:99.6pt;">BioClinicalBERT <cite class="ltx_cite ltx_citemacro_citep">(Alsentzer et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.7.6.3.1.1" class="ltx_p" style="width:51.2pt;">56.41%</span>
</span>
</td>
<td id="S5.T1.3.7.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.7.6.4.1.1" class="ltx_p" style="width:51.2pt;">56.73%</span>
</span>
</td>
</tr>
<tr id="S5.T1.3.8.7" class="ltx_tr">
<td id="S5.T1.3.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.8.7.1.1.1" class="ltx_p" style="width:128.0pt;">PubMedCLIP ViT + AE <cite class="ltx_cite ltx_citemacro_citep">(Eslami et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.8.7.2.1.1" class="ltx_p" style="width:99.6pt;">GloVe + LSTM</span>
</span>
</td>
<td id="S5.T1.3.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.8.7.3.1.1" class="ltx_p" style="width:51.2pt;">N/A</span>
</span>
</td>
<td id="S5.T1.3.8.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.3.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.8.7.4.1.1" class="ltx_p" style="width:51.2pt;">80.1%</span>
</span>
</td>
</tr>
<tr id="S5.T1.3.9.8" class="ltx_tr">
<td id="S5.T1.3.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.3.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.9.8.1.1.1" class="ltx_p" style="width:128.0pt;">MedCLIP <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.3.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.9.8.2.1.1" class="ltx_p" style="width:99.6pt;">BioClinicalBERT <cite class="ltx_cite ltx_citemacro_citep">(Alsentzer et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S5.T1.3.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.3.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.9.8.3.1.1" class="ltx_p" style="width:51.2pt;">59.34%</span>
</span>
</td>
<td id="S5.T1.3.9.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.3.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.9.8.4.1.1" class="ltx_p" style="width:51.2pt;">59.29%</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">As expected, from Section 3.1, we see a boost in performance when using the radiology-pretrained DenseNet as opposed to a CNN model that was pretrained using ImageNet.
From Fig. <a href="#S5.F4" title="Figure 4 ‣ 5 Results ‣ Free Form Medical Visual Question Answering in Radiology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we can see that by virtue of making a bigger training dataset, we have enhanced the answering capability of our VQA model. Trained on only the SLAKE dataset, a VQA model would not be able to diagnose the patient on the right with calcified granuloma. However, since we have incorporated data from multiple sources into our training step, our model is now able to successfully make the diagnosis.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2401.13081/assets/fig_4.png" id="S5.F4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="354" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance on questions not in the SLAKE dataset</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this study, we introduce an augmented dataset specifically designed for pre-training visual encoders for medical images. Our experimentation involved pre-training various image models on an extensive radiology dataset while employing domain-specific language models for encoding questions. This approach highlights the superior efficacy of intra-domain transfer learning in medical visual question answering, in contrast to the inter-domain transfer learning prevalent in previous methodologies.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We also proposed the use of MedCLiP for the simultaneous development of effective multi-modal representations through cross-modal supervision. Our results on the SLAKE dataset show that this approach is on par with state-of-the-art models yet benefits from a simpler architecture and reduced model complexity. Our VQA model, incorporating MedCLiP as the image encoder, surpasses the baseline by 4.55%. However, the performance significantly diminishes when CLiP embeddings are employed for both image and question encoders. This drop is likely due to the disparity in distributions between the clinical captions used in training and the question-style text in our dataset.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitations</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">Despite these advancements, our approach does face limitations. One key issue is the lack of explainability and interpretability in the model, which are crucial for its practical deployment and ensuring the reliability of its predictions before making clinical decisions. Additionally, since the problem is framed as a classification task, the model’s ability to predict answers is confined to a fixed vocabulary, limiting its overall scope and applicability.</p>
</div>
</section>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We are grateful to New York University’s High Performance Computing team for providing us with the necessary computing support and resources to train our models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">kag (a)</span>
<span class="ltx_bibblock">
Covidx cxr-2, a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.kaggle.com/datasets/andyczhao/covidx-cxr2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/andyczhao/covidx-cxr2</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">kag (b)</span>
<span class="ltx_bibblock">
Rsna pneumonia detection challenge, b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/data</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Medpix.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://medpix.nlm.nih.gov/home" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://medpix.nlm.nih.gov/home</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
Vqa.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://visualqa.org/index.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/index.html</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Sadi et al. (2019)</span>
<span class="ltx_bibblock">
Al-Sadi, A., Talafha, B., Al-Ayyoub, M., Jararweh, Y., and Costen, F.

</span>
<span class="ltx_bibblock">Just at imageclef 2019 visual question answering in the medical domain.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CLEF (working notes)</em>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allaouzi &amp; Ahmed (2018)</span>
<span class="ltx_bibblock">
Allaouzi, I. and Ahmed, M. B.

</span>
<span class="ltx_bibblock">Deep neural networks and decision tree classifier for visual question answering in the medical domain.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CLEF (Working Notes)</em>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alsentzer et al. (2019)</span>
<span class="ltx_bibblock">
Alsentzer, E., Murphy, J. R., Boag, W., Weng, W., Jin, D., Naumann, T., and McDermott, M. B. A.

</span>
<span class="ltx_bibblock">Publicly available clinical BERT embeddings.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1904.03323, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1904.03323" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1904.03323</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ambati &amp; Dudyala (2018)</span>
<span class="ltx_bibblock">
Ambati, R. and Dudyala, C. R.

</span>
<span class="ltx_bibblock">A sequence-to-sequence model approach for imageclef 2018 medical domain visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2018 15th IEEE India Council International Conference (INDICON)</em>, pp.  1–6. IEEE, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Abacha et al. (2021)</span>
<span class="ltx_bibblock">
Ben Abacha, A., Sarrouti, M., Demner-Fushman, D., Hasan, S. A., and Müller, H.

</span>
<span class="ltx_bibblock">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</em>. 21-24 September 2021, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bustos et al. (2020)</span>
<span class="ltx_bibblock">
Bustos, A., Pertusa, A., Salinas, J.-M., and de la Iglesia-Vayá, M.

</span>
<span class="ltx_bibblock">PadChest: A large chest x-ray image dataset with multi-label annotated reports.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, 66:101797, dec 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.media.2020.101797</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1016%2Fj.media.2020.101797" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016%2Fj.media.2020.101797</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Chen, G., Gong, H., and Li, G.

</span>
<span class="ltx_bibblock">Hcp-mic at vqa-med 2020: Effective visual representation for medical visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">CLEF (Working Notes)</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al. (2022)</span>
<span class="ltx_bibblock">
Cohen, J. P., Viviano, J. D., Bertin, P., Morrison, P., Torabian, P., Guarrera, M., Lungren, M. P., Chaudhari, A., Brooks, R., Hashir, M., and Bertrand, H.

</span>
<span class="ltx_bibblock">TorchXRayVision: A library of chest X-ray datasets and models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Medical Imaging with Deep Learning</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/mlmed/torchxrayvision" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mlmed/torchxrayvision</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern recognition</em>, pp.  248–255. Ieee, 2009.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eslami et al. (2021)</span>
<span class="ltx_bibblock">
Eslami, S., de Melo, G., and Meinel, C.

</span>
<span class="ltx_bibblock">Does clip benefit visual question answering in the medical domain as much as it does in the general domain?, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2021)</span>
<span class="ltx_bibblock">
Gong, H., Chen, G., Liu, S., Yu, Y., and Li, G.

</span>
<span class="ltx_bibblock">Cross-modal self-attention with multi-task pre-training for medical visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 international conference on multimedia retrieval</em>, pp.  456–460, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2021a)</span>
<span class="ltx_bibblock">
Gupta, D., Suman, S., and Ekbal, A.

</span>
<span class="ltx_bibblock">Hierarchical deep multi-modal network for medical visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, 164:113993, 2021a.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2021b)</span>
<span class="ltx_bibblock">
Gupta, D., Suman, S., and Ekbal, A.

</span>
<span class="ltx_bibblock">Hierarchical deep multi-modal network for medical visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, 164:113993, 2021b.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  770–778, 2016.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2018)</span>
<span class="ltx_bibblock">
He, K., Girshick, R., and Dollár, P.

</span>
<span class="ltx_bibblock">Rethinking imagenet pre-training, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
He, X., Zhang, Y., Mou, L., Xing, E., and Xie, P.

</span>
<span class="ltx_bibblock">Pathvqa: 30000+ questions for medical visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.10286</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2018)</span>
<span class="ltx_bibblock">
Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2019)</span>
<span class="ltx_bibblock">
Huang, K., Altosaar, J., and Ranganath, R.

</span>
<span class="ltx_bibblock">Clinicalbert: Modeling clinical notes and predicting hospital readmission.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05342</em>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irvin et al. (2019)</span>
<span class="ltx_bibblock">
Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R. L., Shpanskaya, K. S., Seekins, J., Mong, D. A., Halabi, S. S., Sandberg, J. K., Jones, R., Larson, D. B., Langlotz, C. P., Patel, B. N., Lungren, M. P., and Ng, A. Y.

</span>
<span class="ltx_bibblock">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1901.07031, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1901.07031" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1901.07031</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2019)</span>
<span class="ltx_bibblock">
Johnson, A. E. W., Pollard, T. J., Greenbaum, N. R., Lungren, M. P., ying Deng, C., Peng, Y., Lu, Z., Mark, R. G., Berkowitz, S. J., and Horng, S.

</span>
<span class="ltx_bibblock">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et al. (2020)</span>
<span class="ltx_bibblock">
Jung, B., Gu, L., and Harada, T.

</span>
<span class="ltx_bibblock">bumjun_jung at vqa-med 2020: Vqa model based on feature extraction and multi-modal feature fusion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">CLEF (Working Notes)</em>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kavur et al. (2021)</span>
<span class="ltx_bibblock">
Kavur, A. E., Gezer, N. S., Barış, M., Aslan, S., Conze, P.-H., Groza, V., Pham, D. D., Chatterjee, S., Ernst, P., Özkan, S., Baydar, B., Lachinov, D., Han, S., Pauli, J., Isensee, F., Perkonigg, M., Sathish, R., Rajan, R., Sheet, D., Dovletov, G., Speck, O., Nürnberger, A., Maier-Hein, K. H., Akar, G. B., Ünal, G., Dicle, O., and Selver, M. A.

</span>
<span class="ltx_bibblock">CHAOS challenge - combined (CT-MR) healthy abdominal organ segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, 69:101950, apr 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.media.2020.101950</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1016%2Fj.media.2020.101950" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016%2Fj.media.2020.101950</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2018)</span>
<span class="ltx_bibblock">
Kim, J.-H., Jun, J., and Zhang, B.-T.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 31, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kovaleva et al. (2019)</span>
<span class="ltx_bibblock">
Kovaleva, O., Shivade, C. P., Kashyap, S., Kanjaria, K., Coy, A., Ballah, D., Guo, Y., Wu, J. T., Karargyris, A., Beymer, D. J., Rumshisky, A., and Mukherjee, V. V.

</span>
<span class="ltx_bibblock">Visual dialog for radiology: Data curation and firststeps.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ViGIL@NeurIPS</em>, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lau et al. (2018)</span>
<span class="ltx_bibblock">
Lau, J. J., Gayen, S., Ben Abacha, A., and Demner-Fushman, D.

</span>
<span class="ltx_bibblock">A dataset of clinically generated visual questions and answers about radiology images.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Scientific data</em>, 5(1):1–10, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2020)</span>
<span class="ltx_bibblock">
Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., and Kang, J.

</span>
<span class="ltx_bibblock">Biobert: a pre-trained biomedical language representation model for biomedical text mining.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, 36(4):1234–1240, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Li, Y., Yang, Z., and Hao, T.

</span>
<span class="ltx_bibblock">Tam at vqa-med 2021: A hybrid model with feature extraction and fusion for medical visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CLEF (Working Notes)</em>, pp.  1295–1304, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021)</span>
<span class="ltx_bibblock">
Lin, Z., Zhang, D., Tac, Q., Shi, D., Haffari, G., Wu, Q., He, M., and Ge, Z.

</span>
<span class="ltx_bibblock">Medical visual question answering: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.10056</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Lin, Z., Zhang, D., Tao, Q., Shi, D., Haffari, G., Wu, Q., He, M., and Ge, Z.

</span>
<span class="ltx_bibblock">Medical visual question answering: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Medicine</em>, pp.  102611, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021a)</span>
<span class="ltx_bibblock">
Liu, B., Zhan, L.-M., and Wu, X.-M.

</span>
<span class="ltx_bibblock">Contrastive pre-training and representation distillation for medical visual question answering based on radiology images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24</em>, pp.  210–220. Springer, 2021a.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021b)</span>
<span class="ltx_bibblock">
Liu, B., Zhan, L.-M., and Wu, X.-M.

</span>
<span class="ltx_bibblock">Contrastive pre-training and representation distillation for medical visual question answering based on radiology images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24</em>, pp.  210–220. Springer, 2021b.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021c)</span>
<span class="ltx_bibblock">
Liu, B., Zhan, L.-M., and Wu, X.-M.

</span>
<span class="ltx_bibblock">Contrastive pre-training and representation distillation for medical visual question answering based on radiology images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24</em>, pp.  210–220. Springer, 2021c.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021d)</span>
<span class="ltx_bibblock">
Liu, B., Zhan, L.-M., Xu, L., Ma, L., Yang, Y., and Wu, X.-M.

</span>
<span class="ltx_bibblock">Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</em>, pp.  1650–1654. IEEE, 2021d.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2016)</span>
<span class="ltx_bibblock">
Lu, J., Yang, J., Batra, D., and Parikh, D.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 29, 2016.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei et al. (2022)</span>
<span class="ltx_bibblock">
Mei, X., Liu, Z., Robson, P. M., Marinelli, B., Huang, M., Doshi, A., Jacobi, A., Cao, C., Link, K. E., Yang, T., Wang, Y., Greenspan, H., Deyer, T., Fayad, Z. A., and Yang, Y.

</span>
<span class="ltx_bibblock">Radimagenet: An open radiologic deep learning research dataset for effective transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Radiology: Artificial Intelligence</em>, 4(5):e210315, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1148/ryai.210315</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1148/ryai.210315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1148/ryai.210315</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">of Health Chest X-Ray Dataset (2018)</span>
<span class="ltx_bibblock">
of Health Chest X-Ray Dataset, N. I.

</span>
<span class="ltx_bibblock">Nih chest x-rays, Feb 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.kaggle.com/datasets/nih-chest-xrays/data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/nih-chest-xrays/data</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pelka et al. (2018)</span>
<span class="ltx_bibblock">
Pelka, O., Koitka, S., Rückert, J., Nensa, F., and Friedrich, C. M.

</span>
<span class="ltx_bibblock">Radiology objects in context (roco): A multimodal image dataset.

</span>
<span class="ltx_bibblock">In Stoyanov, D., Taylor, Z., Balocco, S., Sznitman, R., Martel, A., Maier-Hein, L., Duong, L., Zahnd, G., Demirci, S., Albarqouni, S., Lee, S.-L., Moriconi, S., Cheplygina, V., Mateus, D., Trucco, E., Granger, E., and Jannin, P. (eds.), <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</em>, pp.  180–189, Cham, 2018. Springer International Publishing.

</span>
<span class="ltx_bibblock">ISBN 978-3-030-01364-6.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raddar (2020)</span>
<span class="ltx_bibblock">
Raddar.

</span>
<span class="ltx_bibblock">Chest x-rays (indiana university), Feb 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raghu et al. (2019)</span>
<span class="ltx_bibblock">
Raghu, M., Zhang, C., Kleinberg, J., and Bengio, S.

</span>
<span class="ltx_bibblock">Transfusion: Understanding transfer learning for medical imaging, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2021a)</span>
<span class="ltx_bibblock">
Sharma, D., Purushotham, S., and Reddy, C. K.

</span>
<span class="ltx_bibblock">Medfusenet: An attention-based multimodal deep learning model for visual question answering in the medical domain.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Scientific Reports</em>, 11(1):19826, 2021a.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2021b)</span>
<span class="ltx_bibblock">
Sharma, H., Drukker, L., Papageorghiou, A. T., and Noble, J. A.

</span>
<span class="ltx_bibblock">Multi-modal learning from video, eye tracking, and pupillometry for operator skill characterization in clinical fetal ultrasound.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</em>, pp.  1646–1649. IEEE, 2021b.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
Shen, S., Li, L. H., Tan, H., Bansal, M., Rohrbach, A., Chang, K., Yao, Z., and Keutzer, K.

</span>
<span class="ltx_bibblock">How much can CLIP benefit vision-and-language tasks?

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2107.06383, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2107.06383" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2107.06383</a>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan &amp; Zisserman (2014)</span>
<span class="ltx_bibblock">
Simonyan, K. and Zisserman, A.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv 1409.1556</em>, 09 2014.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simpson et al. (2019)</span>
<span class="ltx_bibblock">
Simpson, A. L., Antonelli, M., Bakas, S., Bilello, M., Farahani, K., van Ginneken, B., Kopp-Schneider, A., Landman, B. A., Litjens, G., Menze, B., Ronneberger, O., Summers, R. M., Bilic, P., Christ, P. F., Do, R. K. G., Gollub, M., Golia-Pernicka, J., Heckers, S. H., Jarnagin, W. R., McHugo, M. K., Napel, S., Vorontsov, E., Maier-Hein, L., and Cardoso, M. J.

</span>
<span class="ltx_bibblock">A large annotated medical image dataset for the development and evaluation of segmentation algorithms, 2019.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh &amp; Cirrone (2023)</span>
<span class="ltx_bibblock">
Singh, P. and Cirrone, J.

</span>
<span class="ltx_bibblock">Efficient representation learning for healthcare with cross-architectural self-supervision.

</span>
<span class="ltx_bibblock">In Deshpande, K., Fiterau, M., Joshi, S., Lipton, Z., Ranganath, R., Urteaga, I., and Yeung, S. (eds.), <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th Machine Learning for Healthcare Conference</em>, volume 219 of <em id="bib.bib50.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pp.  691–711. PMLR, 11–12 Aug 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlr.press/v219/singh23a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v219/singh23a.html</a>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. (2020)</span>
<span class="ltx_bibblock">
Vu, M. H., Löfstedt, T., Nyholm, T., and Sznitman, R.

</span>
<span class="ltx_bibblock">A question-centric model for visual question answering in medical imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on medical imaging</em>, 39(9):2856–2868, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2017a)</span>
<span class="ltx_bibblock">
Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., and Summers, R. M.

</span>
<span class="ltx_bibblock">ChestX-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE, jul 2017a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/cvpr.2017.369</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109%2Fcvpr.2017.369" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109%2Fcvpr.2017.369</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2017b)</span>
<span class="ltx_bibblock">
Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., and Summers, R. M.

</span>
<span class="ltx_bibblock">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  2097–2106, 2017b.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Wang, Z., Wu, Z., Agarwal, D., and Sun, J.

</span>
<span class="ltx_bibblock">Medclip: Contrastive learning from unpaired medical images and text.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.10163</em>, 2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2017)</span>
<span class="ltx_bibblock">
Yu, Z., Yu, J., Fan, J., and Tao, D.

</span>
<span class="ltx_bibblock">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, pp.  1821–1830, 2017.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2018)</span>
<span class="ltx_bibblock">
Yu, Z., Yu, J., Xiang, C., Fan, J., and Tao, D.

</span>
<span class="ltx_bibblock">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>, 29(12):5947–5959, 2018.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Zhang, W., Deng, L., Zhang, L., and Wu, D.

</span>
<span class="ltx_bibblock">A survey on negative transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IEEE/CAA Journal of Automatica Sinica</em>, 10(2):305–329, feb 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/jas.2022.106004</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109%2Fjas.2022.106004" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109%2Fjas.2022.106004</a>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou &amp; Hastie (2005)</span>
<span class="ltx_bibblock">
Zou, H. and Hastie, T.

</span>
<span class="ltx_bibblock">Regularization and variable selection via the elastic net.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, 67(2):301–320, 2005.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.13078" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.13081" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.13081">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.13081" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.13082" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 08:15:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
