<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2208.11625] PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model</title><meta property="og:description" content="Quick global aggregation of effective distributed parameters is crucial to federated learning (FL), which requires adequate bandwidth for parameters communication and sufficient user data for local training. Otherwise,…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2208.11625">

<!--Generated on Wed Mar 13 18:53:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id1.id1" class="ltx_text ltx_font_smallcaps">PromptFL</span>: Let Federated Participants Cooperatively Learn Prompts Instead
<br class="ltx_break">of Models — Federated Learning in Age of Foundation Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tao Guo, Song Guo, Junxiao Wang, Wenchao Xu

</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Quick global aggregation of effective distributed parameters is crucial to federated learning (FL), which requires adequate bandwidth for parameters communication and sufficient user data for local training. Otherwise, FL may cost excessive training time for convergence and produce inaccurate models. In this paper, we propose a brand-new FL framework, <span id="id2.id1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span>, that replaces the federated model training with the federated prompt training, i.e., let federated participants train prompts instead of a shared model, to simultaneously achieve the efficient global aggregation and local training on insufficient data by exploiting the power of foundation models (FM) in a distributed way. <span id="id2.id1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> ships an off-the-shelf FM, i.e., CLIP, to distributed clients who would cooperatively train shared soft prompts based on very few local data. Since <span id="id2.id1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> only needs to update the prompts instead of the whole model, both the local training and the global aggregation can be significantly accelerated. And FM trained over large scale data can provide strong adaptation capability to distributed users tasks with the trained soft prompts. We empirically analyze the <span id="id2.id1.4" class="ltx_text ltx_font_smallcaps">PromptFL</span> via extensive experiments, and show its superiority in terms of system feasibility, user privacy, and performance.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The ever-growing edge devices, e.g., smart phones, autonomous vehicles, etc., are generating various types and rapidly growing big data. Artificial intelligence (AI) has shown its success to mine the big edge data and produce accurate models that can replace human decisions timely and properly. Traditional AI paradigms require to gather all raw data to a cloud center for centralized training, which can incur significant communication overhead and potential privacy leakage, and thus are not desirable for edge users.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Federated learning (FL) has emerged to conduct distributed machine learning that allows multiple edge users to jointly train a shared model without sharing their raw data, which has been demonstrated great success in many edge applications, e.g., input word prediction, voice assistant, etc. <cite class="ltx_cite ltx_citemacro_citep">(Hard et al. <a href="#bib.bib14" title="" class="ltx_ref">2018</a>; Liang et al. <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>, that can mine massive distributed data without exposing users’ privacy, and thus are widely applied in various edge scenarios. The FL training process comprises of two iterative phases, i.e., local training and global aggregation. Thus the learning performance is determined by both the effectiveness of the parameters from local training and smooth aggregation of them. However, these two requirements are not easy to satisfy in edge environment, i.e., edge users often have limited bandwidth and insufficient data, which can cause inefficient parameters aggregation, excessive training time and reduced model accuracy.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Existing research efforts have focused on improving the FL optimization process <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a href="#bib.bib21" title="" class="ltx_ref">2020</a>; Zhao et al. <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite> or refining model architectures <cite class="ltx_cite ltx_citemacro_citep">(Qu et al. <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, but this does not change that FL inherently entails a large number of communication rounds and a large amount of labeled data for training, which are often unavailable for edge users.
Such challenges are particularly salient under the combined effect of a long training process and unfavorable factors such as non-IID and unbalanced data, limited communication bandwidth, and unreliable and limited device availability.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">We revisits the question of how FL mines the distributed data in iterative training rounds, and exploit the emerging foundation model (FM) to optimize the FL training. FM refers to large neural model that trained on large scale data and has strong adaptation capability for various downstream tasks. We let federated participants cooperatively learn prompts instead of models to unleash the power of FM in a distributed way, whereby both the local training and global aggregation can be significantly accelerated.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">We investigate the behavior of the nascent model in a standard FL setting using popular off-the-shelf FMs, <em id="Sx1.p5.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, CLIP, and methods for FM adaptation.
We propose <span id="Sx1.p5.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span>, a framework that replaces existing federated model training with prompt training, i.e., FL clients train prompts instead of a model, which can simultaneously exploit the insufficient local data and reduce the aggregation overhead.
<span id="Sx1.p5.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> ships an off-the-shelf public CLIP to users and apply continuous prompts (<em id="Sx1.p5.1.4" class="ltx_emph ltx_font_italic">a.k.a.</em> soft prompts) for FM adaptation, which requires very few data samples from edge users.
The framework is technically very simple but effective.
The focus of our investigation is whether it meets the key principles:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p"><span id="Sx1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Feasibility.</span> What are the system costs? We examine the feasibility of <span id="Sx1.I1.i1.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> on modern hardware, focusing conservatively on personal cell phones. We demonstrate the feasibility of the system in terms of overhead in communication, training, and inference dimensions.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p"><span id="Sx1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Performance.</span> Are <span id="Sx1.I1.i2.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> competitive with FL? FL does not baseline against any such approach, so we implement a proof-of-concept in the framework, spanning a range of popular image classification tasks. We observe <span id="Sx1.I1.i2.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> competitive with strong FL baselines.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p"><span id="Sx1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Privacy.</span> Is <span id="Sx1.I1.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> privacy-preserving? We show that <span id="Sx1.I1.i3.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> keeps data on each device private, aiming to learn global prompts updated only by communicating gradients rather than the data itself, and thus not less private than FL.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Preliminaries</h2>

<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Foundation Model</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">AI is going through a paradigm shift with the rise of models (<em id="Sx2.SSx1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, BERT, GPT-3, CLIP, DALL-E<math id="Sx2.SSx1.p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="Sx2.SSx1.p1.1.m1.1a"><mo id="Sx2.SSx1.p1.1.m1.1.1" xref="Sx2.SSx1.p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.1.m1.1b"><ci id="Sx2.SSx1.p1.1.m1.1.1.cmml" xref="Sx2.SSx1.p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.1.m1.1c">\cdot</annotation></semantics></math>2) trained on broad data using self-supervision at scale that can be adapted to a wide range of downstream tasks.
Researchers call these models foundation models (FMs) to emphasize their key core.
From a technical standpoint, FMs are not new.
However, the sheer size and scope of FMs over the past few years has expanded our imagination of what is possible.
FMs are scientifically interesting for their impressive performance and capabilities, but what makes them critical to research is that they are rapidly being integrated into real-world deployments of AI systems, with profound implications for users.
</p>
</div>
<section id="Sx2.SSx1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CLIP</h4>

<div id="Sx2.SSx1.SSS0.Px1.p1" class="ltx_para">
<p id="Sx2.SSx1.SSS0.Px1.p1.1" class="ltx_p">Contrastive Language-Image Pre-Training (CLIP) is a neural network trained on hundreds of millions of (image, caption) pairs <cite class="ltx_cite ltx_citemacro_citep">(Radford et al. <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>.
CLIP encodes images and captions separately as vectors, enabling users with visual modality samples to retrieve, score, or classify samples from textual modalities.
Models are often very fragile and only know very specific things you trained them to do. CLIP extends the knowledge of classification models to a wider range of things by leveraging semantic information in text.
Standard classification models completely discard the semantic meaning of class labels and simply enumerate numeric classes behind the scenes; CLIP works by understanding the meaning of the classes.
ALIGN is another CLIP-like vision-language pre-training <cite class="ltx_cite ltx_citemacro_citep">(Jia et al. <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="Sx2.SSx1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image Classification with CLIP</h4>

<div id="Sx2.SSx1.SSS0.Px2.p1" class="ltx_para">
<p id="Sx2.SSx1.SSS0.Px2.p1.1" class="ltx_p">CLIP pre-trains an image encoder and a text encoder to predict which images are paired with which texts.
We can use this behavior to convert the CLIP to an image classifier.
We may convert all [class] to captions such as “picture of [class]” and predict the caption class CLIP estimates the best pairing with the given image.
In many previous works, this has involved prompt template engineering, in which human engineers or algorithms search for the best template for each class <cite class="ltx_cite ltx_citemacro_citep">(Fürst et al. <a href="#bib.bib9" title="" class="ltx_ref">2021</a>; Li et al. <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Singh et al. <a href="#bib.bib38" title="" class="ltx_ref">2022</a>; Yuan et al. <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Federated Learning</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">Recent neural models require large amounts of training data <cite class="ltx_cite ltx_citemacro_citep">(Dodge et al. <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, and users typically hold limited-scale labeled data.
To address the challenge of lack of sufficient data for individual users, federated learning of data across multiple privacy spheres (<em id="Sx2.SSx2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, users) has become a popular framework.</p>
</div>
<div id="Sx2.SSx2.p2" class="ltx_para">
<p id="Sx2.SSx2.p2.1" class="ltx_p">The term <em id="Sx2.SSx2.p2.1.1" class="ltx_emph ltx_font_italic">federated learning</em> was introduced by <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al. <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>.
In a centralized setting, the federated server initially sends global model parameters to each client.
After training with local data, the participants are only required to share gradients for model updates.
Then the server aggregates the gradients and transmits the updated model back to each client.
More specifically, federated learning is a machine learning setting where a set of <math id="Sx2.SSx2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="Sx2.SSx2.p2.1.m1.1a"><mi id="Sx2.SSx2.p2.1.m1.1.1" xref="Sx2.SSx2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.p2.1.m1.1b"><ci id="Sx2.SSx2.p2.1.m1.1.1.cmml" xref="Sx2.SSx2.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.p2.1.m1.1c">n</annotation></semantics></math> clients (<em id="Sx2.SSx2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> mobile devices) collaboratively train a model under the orchestration of a federated server (<em id="Sx2.SSx2.p2.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, service provider), while the training data of clients is stored locally and not exchanged <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al. <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.
The federated server orchestrates the collaborative training process, by repeating the following steps until training is converged:</p>
</div>
<section id="Sx2.SSx2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Client Selection</h4>

<div id="Sx2.SSx2.SSS0.Px1.p1" class="ltx_para">
<p id="Sx2.SSx2.SSS0.Px1.p1.3" class="ltx_p">Given the unstable client availability, for the round <math id="Sx2.SSx2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="Sx2.SSx2.SSS0.Px1.p1.1.m1.1a"><mi id="Sx2.SSx2.SSS0.Px1.p1.1.m1.1.1" xref="Sx2.SSx2.SSS0.Px1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px1.p1.1.m1.1b"><ci id="Sx2.SSx2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="Sx2.SSx2.SSS0.Px1.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px1.p1.1.m1.1c">t</annotation></semantics></math> of federated learning, the federated server samples a small subset of <math id="Sx2.SSx2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="Sx2.SSx2.SSS0.Px1.p1.2.m2.1a"><mi id="Sx2.SSx2.SSS0.Px1.p1.2.m2.1.1" xref="Sx2.SSx2.SSS0.Px1.p1.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px1.p1.2.m2.1b"><ci id="Sx2.SSx2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="Sx2.SSx2.SSS0.Px1.p1.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px1.p1.2.m2.1c">m</annotation></semantics></math> clients meeting eligibility requirements out of all <math id="Sx2.SSx2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="Sx2.SSx2.SSS0.Px1.p1.3.m3.1a"><mi id="Sx2.SSx2.SSS0.Px1.p1.3.m3.1.1" xref="Sx2.SSx2.SSS0.Px1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px1.p1.3.m3.1b"><ci id="Sx2.SSx2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="Sx2.SSx2.SSS0.Px1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px1.p1.3.m3.1c">n</annotation></semantics></math> clients to participate in the learning.</p>
</div>
</section>
<section id="Sx2.SSx2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Local Training</h4>

<div id="Sx2.SSx2.SSS0.Px2.p1" class="ltx_para">
<p id="Sx2.SSx2.SSS0.Px2.p1.7" class="ltx_p">Upon notification of being selected at the round <math id="Sx2.SSx2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p1.1.m1.1a"><mi id="Sx2.SSx2.SSS0.Px2.p1.1.m1.1.1" xref="Sx2.SSx2.SSS0.Px2.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p1.1.m1.1b"><ci id="Sx2.SSx2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p1.1.m1.1c">t</annotation></semantics></math>, each selected client downloads the current parameters <math id="Sx2.SSx2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p1.2.m2.1a"><mi id="Sx2.SSx2.SSS0.Px2.p1.2.m2.1.1" xref="Sx2.SSx2.SSS0.Px2.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p1.2.m2.1b"><ci id="Sx2.SSx2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p1.2.m2.1c">\theta</annotation></semantics></math> of global model and a training program from the federated server.
Each selected client locally computes an update to the global model on its local training data by executing the training program.
More specifically, the gradients updated at one client (denoted as <math id="Sx2.SSx2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="G" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p1.3.m3.1a"><mi id="Sx2.SSx2.SSS0.Px2.p1.3.m3.1.1" xref="Sx2.SSx2.SSS0.Px2.p1.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p1.3.m3.1b"><ci id="Sx2.SSx2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.3.m3.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p1.3.m3.1c">G</annotation></semantics></math>), are computed by <math id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3" class="ltx_Math" alttext="\frac{\partial\ell(X,y,\theta)}{\partial\theta}" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3a"><mfrac id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.cmml"><mrow id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.cmml"><mo rspace="0em" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.4" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.4.cmml">∂</mo><mrow id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.cmml"><mi mathvariant="normal" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.2" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.2.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.1" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.1.cmml">​</mo><mrow id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.2" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.1.cmml"><mo stretchy="false" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.2.1" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.1.cmml">(</mo><mi id="Sx2.SSx2.SSS0.Px2.p1.4.m4.1.1.1.1" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.1.1.1.1.cmml">X</mi><mo id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.2.2" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.1.cmml">,</mo><mi id="Sx2.SSx2.SSS0.Px2.p1.4.m4.2.2.2.2" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.2.2.2.2.cmml">y</mi><mo id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.2.3" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.1.cmml">,</mo><mi id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.3" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.3.cmml">θ</mi><mo stretchy="false" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.2.4" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.1.cmml">)</mo></mrow></mrow></mrow><mrow id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.cmml"><mo rspace="0em" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.1" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.1.cmml">∂</mo><mi id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.2" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.2.cmml">θ</mi></mrow></mfrac><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3b"><apply id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3"><divide id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.4.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3"></divide><apply id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3"><partialdiff id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.4.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.4"></partialdiff><apply id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5"><times id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.1"></times><ci id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.2.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.2">ℓ</ci><vector id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.5.3.2"><ci id="Sx2.SSx2.SSS0.Px2.p1.4.m4.1.1.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.1.1.1.1">𝑋</ci><ci id="Sx2.SSx2.SSS0.Px2.p1.4.m4.2.2.2.2.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.2.2.2.2">𝑦</ci><ci id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.3.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.3.3">𝜃</ci></vector></apply></apply><apply id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5"><partialdiff id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.1"></partialdiff><ci id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.2.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.4.m4.3.3.5.2">𝜃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p1.4.m4.3c">\frac{\partial\ell(X,y,\theta)}{\partial\theta}</annotation></semantics></math>, where <math id="Sx2.SSx2.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="X" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p1.5.m5.1a"><mi id="Sx2.SSx2.SSS0.Px2.p1.5.m5.1.1" xref="Sx2.SSx2.SSS0.Px2.p1.5.m5.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p1.5.m5.1b"><ci id="Sx2.SSx2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.5.m5.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p1.5.m5.1c">X</annotation></semantics></math>, <math id="Sx2.SSx2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p1.6.m6.1a"><mi id="Sx2.SSx2.SSS0.Px2.p1.6.m6.1.1" xref="Sx2.SSx2.SSS0.Px2.p1.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p1.6.m6.1b"><ci id="Sx2.SSx2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.6.m6.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p1.6.m6.1c">y</annotation></semantics></math> denote the batches of training data and corresponding labels, and <math id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="\ell(\cdot)" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1a"><mrow id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.cmml"><mi mathvariant="normal" id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.2" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.2.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.1" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.1.cmml">​</mo><mrow id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.3.2" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.cmml"><mo stretchy="false" id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.3.2.1" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.1" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.1.cmml">⋅</mo><mo stretchy="false" id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.3.2.2" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1b"><apply id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2"><times id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.1"></times><ci id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.2.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.2.2">ℓ</ci><ci id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p1.7.m7.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p1.7.m7.1c">\ell(\cdot)</annotation></semantics></math> refers to the loss function.</p>
</div>
<div id="Sx2.SSx2.SSS0.Px2.p2" class="ltx_para">
<p id="Sx2.SSx2.SSS0.Px2.p2.4" class="ltx_p">The gradients <math id="Sx2.SSx2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p2.1.m1.1a"><mi id="Sx2.SSx2.SSS0.Px2.p2.1.m1.1.1" xref="Sx2.SSx2.SSS0.Px2.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p2.1.m1.1b"><ci id="Sx2.SSx2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p2.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p2.1.m1.1c">G</annotation></semantics></math> in typical federated learning settings are the minimum that must be shared to the server, corresponding to FedSGD method.
In FedAVG <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al. <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>, models are consecutively updated on more batches of local data, which can be several epochs of training, and then shared.
We note that a common way is to share the updated model <math id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="\theta+G" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1a"><mrow id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.2" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.2.cmml">θ</mi><mo id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.1" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.1.cmml">+</mo><mi id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.3" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1b"><apply id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1"><plus id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.1"></plus><ci id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.2">𝜃</ci><ci id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="Sx2.SSx2.SSS0.Px2.p2.2.m2.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p2.2.m2.1c">\theta+G</annotation></semantics></math>, but this practically amounts to sharing <math id="Sx2.SSx2.SSS0.Px2.p2.3.m3.1" class="ltx_Math" alttext="G" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p2.3.m3.1a"><mi id="Sx2.SSx2.SSS0.Px2.p2.3.m3.1.1" xref="Sx2.SSx2.SSS0.Px2.p2.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p2.3.m3.1b"><ci id="Sx2.SSx2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p2.3.m3.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p2.3.m3.1c">G</annotation></semantics></math> since all participants know <math id="Sx2.SSx2.SSS0.Px2.p2.4.m4.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="Sx2.SSx2.SSS0.Px2.p2.4.m4.1a"><mi id="Sx2.SSx2.SSS0.Px2.p2.4.m4.1.1" xref="Sx2.SSx2.SSS0.Px2.p2.4.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px2.p2.4.m4.1b"><ci id="Sx2.SSx2.SSS0.Px2.p2.4.m4.1.1.cmml" xref="Sx2.SSx2.SSS0.Px2.p2.4.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px2.p2.4.m4.1c">\theta</annotation></semantics></math>.</p>
</div>
</section>
<section id="Sx2.SSx2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Global Aggregation</h4>

<div id="Sx2.SSx2.SSS0.Px3.p1" class="ltx_para">
<p id="Sx2.SSx2.SSS0.Px3.p1.1" class="ltx_p">Upon having received local updates from <math id="Sx2.SSx2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="Sx2.SSx2.SSS0.Px3.p1.1.m1.1a"><mi id="Sx2.SSx2.SSS0.Px3.p1.1.m1.1.1" xref="Sx2.SSx2.SSS0.Px3.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSS0.Px3.p1.1.m1.1b"><ci id="Sx2.SSx2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="Sx2.SSx2.SSS0.Px3.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSS0.Px3.p1.1.m1.1c">m</annotation></semantics></math> clients, the federated server aggregates these updates and update its global model, and initiates next round learning.
In addition to the federated learning framework that relies on the centralized server node, there are also some federated learning implementations based on the decentralized framework <cite class="ltx_cite ltx_citemacro_citep">(Roy et al. <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Lalitha et al. <a href="#bib.bib19" title="" class="ltx_ref">2018</a>; Hu, Jiang, and Wang <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>.
This means that the aggregation of gradients does not necessarily occur in a fixed federation server, but may also occur in some clients.</p>
</div>
<figure id="Sx2.F1" class="ltx_figure"><img src="/html/2208.11625/assets/x1.png" id="Sx2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="124" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Framework and workflow of <span id="Sx2.F1.2.1" class="ltx_text ltx_font_smallcaps">PromptFL</span>. Each client includes a prompt learner (with only a small amount of trainable parameters) and an out-of-the-box CLIP (with backbone frozen). The federated server aggregates only the parameter updates of prompt learners over multiple users, and transmit the updated parameters back to each user.</figcaption>
</figure>
</section>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Prompt-Based Federated Learning</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">We hypothesize that an off-the-shelf public CLIP-like model is shipped to the user device.
The CLIP-like model is a powerful image classifier that utilizes linguistic knowledge to classify images.
In other words, CLIP already knows a lot about the content of images.
But to unleash the power of CLIP in FL, we need to take advantage of something called prompt engineering that was mentioned in the preliminaries.</p>
</div>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Prompt Engineering</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.3" class="ltx_p">The prompting function <math id="Sx3.SSx1.p1.1.m1.1" class="ltx_Math" alttext="f_{\text{prompt}}(\cdot)" display="inline"><semantics id="Sx3.SSx1.p1.1.m1.1a"><mrow id="Sx3.SSx1.p1.1.m1.1.2" xref="Sx3.SSx1.p1.1.m1.1.2.cmml"><msub id="Sx3.SSx1.p1.1.m1.1.2.2" xref="Sx3.SSx1.p1.1.m1.1.2.2.cmml"><mi id="Sx3.SSx1.p1.1.m1.1.2.2.2" xref="Sx3.SSx1.p1.1.m1.1.2.2.2.cmml">f</mi><mtext id="Sx3.SSx1.p1.1.m1.1.2.2.3" xref="Sx3.SSx1.p1.1.m1.1.2.2.3a.cmml">prompt</mtext></msub><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p1.1.m1.1.2.1" xref="Sx3.SSx1.p1.1.m1.1.2.1.cmml">​</mo><mrow id="Sx3.SSx1.p1.1.m1.1.2.3.2" xref="Sx3.SSx1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="Sx3.SSx1.p1.1.m1.1.2.3.2.1" xref="Sx3.SSx1.p1.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p1.1.m1.1.1" xref="Sx3.SSx1.p1.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="Sx3.SSx1.p1.1.m1.1.2.3.2.2" xref="Sx3.SSx1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.1.m1.1b"><apply id="Sx3.SSx1.p1.1.m1.1.2.cmml" xref="Sx3.SSx1.p1.1.m1.1.2"><times id="Sx3.SSx1.p1.1.m1.1.2.1.cmml" xref="Sx3.SSx1.p1.1.m1.1.2.1"></times><apply id="Sx3.SSx1.p1.1.m1.1.2.2.cmml" xref="Sx3.SSx1.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.1.m1.1.2.2.1.cmml" xref="Sx3.SSx1.p1.1.m1.1.2.2">subscript</csymbol><ci id="Sx3.SSx1.p1.1.m1.1.2.2.2.cmml" xref="Sx3.SSx1.p1.1.m1.1.2.2.2">𝑓</ci><ci id="Sx3.SSx1.p1.1.m1.1.2.2.3a.cmml" xref="Sx3.SSx1.p1.1.m1.1.2.2.3"><mtext mathsize="70%" id="Sx3.SSx1.p1.1.m1.1.2.2.3.cmml" xref="Sx3.SSx1.p1.1.m1.1.2.2.3">prompt</mtext></ci></apply><ci id="Sx3.SSx1.p1.1.m1.1.1.cmml" xref="Sx3.SSx1.p1.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.1.m1.1c">f_{\text{prompt}}(\cdot)</annotation></semantics></math> is applied to modify the class label <span id="Sx3.SSx1.p1.3.1" class="ltx_text ltx_markedasmath ltx_font_bold">y</span> into a prompt <math id="Sx3.SSx1.p1.3.m3.1" class="ltx_Math" alttext="\textbf{y}^{\prime}=f_{\text{prompt}}(\textbf{y})" display="inline"><semantics id="Sx3.SSx1.p1.3.m3.1a"><mrow id="Sx3.SSx1.p1.3.m3.1.2" xref="Sx3.SSx1.p1.3.m3.1.2.cmml"><msup id="Sx3.SSx1.p1.3.m3.1.2.2" xref="Sx3.SSx1.p1.3.m3.1.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx1.p1.3.m3.1.2.2.2" xref="Sx3.SSx1.p1.3.m3.1.2.2.2a.cmml">y</mtext><mo id="Sx3.SSx1.p1.3.m3.1.2.2.3" xref="Sx3.SSx1.p1.3.m3.1.2.2.3.cmml">′</mo></msup><mo id="Sx3.SSx1.p1.3.m3.1.2.1" xref="Sx3.SSx1.p1.3.m3.1.2.1.cmml">=</mo><mrow id="Sx3.SSx1.p1.3.m3.1.2.3" xref="Sx3.SSx1.p1.3.m3.1.2.3.cmml"><msub id="Sx3.SSx1.p1.3.m3.1.2.3.2" xref="Sx3.SSx1.p1.3.m3.1.2.3.2.cmml"><mi id="Sx3.SSx1.p1.3.m3.1.2.3.2.2" xref="Sx3.SSx1.p1.3.m3.1.2.3.2.2.cmml">f</mi><mtext id="Sx3.SSx1.p1.3.m3.1.2.3.2.3" xref="Sx3.SSx1.p1.3.m3.1.2.3.2.3a.cmml">prompt</mtext></msub><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p1.3.m3.1.2.3.1" xref="Sx3.SSx1.p1.3.m3.1.2.3.1.cmml">​</mo><mrow id="Sx3.SSx1.p1.3.m3.1.2.3.3.2" xref="Sx3.SSx1.p1.3.m3.1.1a.cmml"><mo stretchy="false" id="Sx3.SSx1.p1.3.m3.1.2.3.3.2.1" xref="Sx3.SSx1.p1.3.m3.1.1a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.SSx1.p1.3.m3.1.1" xref="Sx3.SSx1.p1.3.m3.1.1.cmml">y</mtext><mo stretchy="false" id="Sx3.SSx1.p1.3.m3.1.2.3.3.2.2" xref="Sx3.SSx1.p1.3.m3.1.1a.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.3.m3.1b"><apply id="Sx3.SSx1.p1.3.m3.1.2.cmml" xref="Sx3.SSx1.p1.3.m3.1.2"><eq id="Sx3.SSx1.p1.3.m3.1.2.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.1"></eq><apply id="Sx3.SSx1.p1.3.m3.1.2.2.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.1.2.2.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.2">superscript</csymbol><ci id="Sx3.SSx1.p1.3.m3.1.2.2.2a.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.2.2"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx1.p1.3.m3.1.2.2.2.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.2.2">y</mtext></ci><ci id="Sx3.SSx1.p1.3.m3.1.2.2.3.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.2.3">′</ci></apply><apply id="Sx3.SSx1.p1.3.m3.1.2.3.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3"><times id="Sx3.SSx1.p1.3.m3.1.2.3.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3.1"></times><apply id="Sx3.SSx1.p1.3.m3.1.2.3.2.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3.2"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.1.2.3.2.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3.2">subscript</csymbol><ci id="Sx3.SSx1.p1.3.m3.1.2.3.2.2.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3.2.2">𝑓</ci><ci id="Sx3.SSx1.p1.3.m3.1.2.3.2.3a.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3.2.3"><mtext mathsize="70%" id="Sx3.SSx1.p1.3.m3.1.2.3.2.3.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3.2.3">prompt</mtext></ci></apply><ci id="Sx3.SSx1.p1.3.m3.1.1a.cmml" xref="Sx3.SSx1.p1.3.m3.1.2.3.3.2"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx1.p1.3.m3.1.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.1">y</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.3.m3.1c">\textbf{y}^{\prime}=f_{\text{prompt}}(\textbf{y})</annotation></semantics></math>.
The most natural form of implementing a prompting function is to manually create an intuitive template based on human introspection.
For example, as referred in <cite class="ltx_cite ltx_citemacro_citep">(Brown et al. <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> we may manually craft prefix prompts to handle an image classification task by using templates like “picture of [class]” or “a photo of a [class]”.
Based on that, many approaches have been proposed to automate the template design process.</p>
</div>
<div id="Sx3.SSx1.p2" class="ltx_para">
<p id="Sx3.SSx1.p2.1" class="ltx_p">Specifically, the automated prompting can be further separated into discrete prompts (<em id="Sx3.SSx1.p2.1.1" class="ltx_emph ltx_font_italic">a.k.a.</em> hard prompts), where the prompt is an actual text string, and continuous prompts (<em id="Sx3.SSx1.p2.1.2" class="ltx_emph ltx_font_italic">a.k.a.</em> soft prompts), where the prompt is performed directly in the embedding space of the model <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>.
Discrete prompts constraint that the embeddings of template words be the embeddings of natural language words <cite class="ltx_cite ltx_citemacro_citep">(Shin et al. <a href="#bib.bib37" title="" class="ltx_ref">2020</a>; Gao, Fisch, and Chen <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>.
Thus, discrete prompting is a clear way to visualize what “word” are learned for the vectors <cite class="ltx_cite ltx_citemacro_citep">(Deng et al. <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="Sx3.SSx1.p3" class="ltx_para">
<p id="Sx3.SSx1.p3.1" class="ltx_p">Our paper adopts continuous prompts instead of discrete prompts in FL for the reason that (1) Our purpose of prompt construction is to find a way to enable FL to efficiently perform the image classification tasks, not for human interpretation, there is no need to limit prompts to human-interpretable natural language.
(2) The templates have their own parameters that can be tuned based on training data from the user, which is a natural compatibility connecting FL and prompting.
More related topics of continuous prompts can refer to <cite class="ltx_cite ltx_citemacro_citep">(Li and Liang <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Lester, Al-Rfou, and Constant <a href="#bib.bib20" title="" class="ltx_ref">2021</a>; Tsimpoukelli et al. <a href="#bib.bib40" title="" class="ltx_ref">2021</a>; Hambardzumyan, Khachatrian, and May <a href="#bib.bib13" title="" class="ltx_ref">2021</a>; Zhou et al. <a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Framework to Learn Prompts in FL</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">The framework of <span id="Sx3.SSx2.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> is presented in Figure <a href="#Sx2.F1" title="Figure 1 ‣ Global Aggregation ‣ Federated Learning ‣ Preliminaries ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Each FL client consists of a prompt learner and an out-of-the-box CLIP model.
<span id="Sx3.SSx2.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> introduces only a small amount of trainable parameters in the prompt learner while keeping the CLIP backbone frozen.
In other words, during local training, only the parameters of the prompt learner are updated while the whole CLIP model turns off gradients in both the image and the text encoder.
The federated server is designed to aggregate only the parameter updates of prompt learners over multiple users, and transmit the updated parameters back to
each user.
Thus, <span id="Sx3.SSx2.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> evolves the goal of FL from model training to prompt learner training.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.p2.1" class="ltx_p">The CLIP backbone comprises two encoders, one for images and the other for texts.
The image encoder will map high-dimensional images into a low-dimensional embedding space.
The network of the image encoder can take the form of a CNN such as ResNet50 <cite class="ltx_cite ltx_citemacro_citep">(He et al. <a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> or Vision Transformer <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al. <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>.
The text encoder will generate text
representations from input.
The network of the text encoder is a Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al. <a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<figure id="Sx3.T1" class="ltx_table">
<table id="Sx3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="Sx3.T1.1.1" class="ltx_tr" style="background-color:#E6E6FF;">
<td id="Sx3.T1.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><svg version="1.1" height="19.07" width="145.22" overflow="visible"><g transform="translate(0,19.07) scale(1,-1)"><path d="M 0,19.07 145.22,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="69.15" height="9.46" overflow="visible">
<span id="Sx3.T1.1.1.1.pic1.1.1" class="ltx_inline-block" style="background-color:#FFE6E6;">
<span id="Sx3.T1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="Sx3.T1.1.1.1.pic1.1.1.1.1" class="ltx_p">Dimensions</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(72.61,9.46)"><g transform="translate(0,9.61) scale(1, -1)"><foreignObject width="72.61" height="9.61" overflow="visible">
<span id="Sx3.T1.1.1.1.pic1.2.1" class="ltx_inline-block" style="background-color:#FFE6E6;">
<span id="Sx3.T1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="Sx3.T1.1.1.1.pic1.2.1.1.1" class="ltx_p">Frameworks</span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="Sx3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="Sx3.T1.1.1.2.1" class="ltx_text"></span><span id="Sx3.T1.1.1.2.2" class="ltx_text" style="background-color:#FFE6E6;"> <span id="Sx3.T1.1.1.2.2.1" class="ltx_text">
<span id="Sx3.T1.1.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="Sx3.T1.1.1.2.2.1.1.1" class="ltx_tr">
<span id="Sx3.T1.1.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span></span></span>
<span id="Sx3.T1.1.1.2.2.1.1.2" class="ltx_tr">
<span id="Sx3.T1.1.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.2.2.1.1.2.1.1" class="ltx_text" style="background-color:#E6E6FF;">(150M parameter model)</span></span></span>
</span></span><span id="Sx3.T1.1.1.2.2.2" class="ltx_text"></span></span>
</td>
<td id="Sx3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="Sx3.T1.1.1.3.1" class="ltx_text"></span><span id="Sx3.T1.1.1.3.2" class="ltx_text" style="background-color:#FFE6E6;"> <span id="Sx3.T1.1.1.3.2.1" class="ltx_text">
<span id="Sx3.T1.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="Sx3.T1.1.1.3.2.1.1.1" class="ltx_tr">
<span id="Sx3.T1.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Federated Learning</span></span>
<span id="Sx3.T1.1.1.3.2.1.1.2" class="ltx_tr">
<span id="Sx3.T1.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.3.2.1.1.2.1.1" class="ltx_text" style="background-color:#E6E6FF;">(100M parameter model)</span></span></span>
</span></span><span id="Sx3.T1.1.1.3.2.2" class="ltx_text"></span></span>
</td>
<td id="Sx3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="Sx3.T1.1.1.4.1" class="ltx_text"></span><span id="Sx3.T1.1.1.4.2" class="ltx_text" style="background-color:#FFE6E6;"> <span id="Sx3.T1.1.1.4.2.1" class="ltx_text">
<span id="Sx3.T1.1.1.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="Sx3.T1.1.1.4.2.1.1.1" class="ltx_tr">
<span id="Sx3.T1.1.1.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Modern Mobile Phone Hardware</span></span>
<span id="Sx3.T1.1.1.4.2.1.1.2" class="ltx_tr">
<span id="Sx3.T1.1.1.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><cite class="ltx_cite ltx_citemacro_citep"><span id="Sx3.T1.1.1.4.2.1.1.2.1.1.1" class="ltx_text" style="background-color:#E6E6FF;">(</span>E. Freedman<span id="Sx3.T1.1.1.4.2.1.1.2.1.2.2.1.1" class="ltx_text" style="background-color:#E6E6FF;"> </span><a href="#bib.bib7" title="" class="ltx_ref">2021</a><span id="Sx3.T1.1.1.4.2.1.1.2.1.3.3" class="ltx_text" style="background-color:#E6E6FF;">)</span></cite></span></span>
</span></span><span id="Sx3.T1.1.1.4.2.2" class="ltx_text"></span></span>
</td>
</tr>
<tr id="Sx3.T1.1.2" class="ltx_tr" style="background-color:#E6E6FF;">
<td id="Sx3.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T1.1.2.1.1" class="ltx_text" style="background-color:#E6E6FF;">Communication</span></td>
<td id="Sx3.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="Sx3.T1.1.2.2.1" class="ltx_text"></span><span id="Sx3.T1.1.2.2.2" class="ltx_text" style="background-color:#E6E6FF;"> <span id="Sx3.T1.1.2.2.2.1" class="ltx_text">
<span id="Sx3.T1.1.2.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="Sx3.T1.1.2.2.2.1.1.1" class="ltx_tr">
<span id="Sx3.T1.1.2.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">600 MB File Download</span></span>
<span id="Sx3.T1.1.2.2.2.1.1.2" class="ltx_tr">
<span id="Sx3.T1.1.2.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">1.4 Minutes</span></span>
</span></span><span id="Sx3.T1.1.2.2.2.2" class="ltx_text"></span></span>
</td>
<td id="Sx3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="Sx3.T1.1.2.3.1" class="ltx_text"></span><span id="Sx3.T1.1.2.3.2" class="ltx_text" style="background-color:#E6E6FF;"> <span id="Sx3.T1.1.2.3.2.1" class="ltx_text">
<span id="Sx3.T1.1.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="Sx3.T1.1.2.3.2.1.1.1" class="ltx_tr">
<span id="Sx3.T1.1.2.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">40 GB File Download +</span></span>
<span id="Sx3.T1.1.2.3.2.1.1.2" class="ltx_tr">
<span id="Sx3.T1.1.2.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">40 GB File Upload</span></span>
<span id="Sx3.T1.1.2.3.2.1.1.3" class="ltx_tr">
<span id="Sx3.T1.1.2.3.2.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.2.3.2.1.1.3.1.1" class="ltx_text" style="background-color:#CCCCFF;">Totally 9 Hours</span></span></span>
</span></span><span id="Sx3.T1.1.2.3.2.2" class="ltx_text"></span></span>
</td>
<td id="Sx3.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="Sx3.T1.1.2.4.1" class="ltx_text"></span><span id="Sx3.T1.1.2.4.2" class="ltx_text" style="background-color:#E6E6FF;"> <span id="Sx3.T1.1.2.4.2.1" class="ltx_text">
<span id="Sx3.T1.1.2.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="Sx3.T1.1.2.4.2.1.1.1" class="ltx_tr">
<span id="Sx3.T1.1.2.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">54 Mbps Downstream RateLimit</span></span>
<span id="Sx3.T1.1.2.4.2.1.1.2" class="ltx_tr">
<span id="Sx3.T1.1.2.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">12 Mbps Upstream RateLimit</span></span>
<span id="Sx3.T1.1.2.4.2.1.1.3" class="ltx_tr">
<span id="Sx3.T1.1.2.4.2.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><cite class="ltx_cite ltx_citemacro_citep"><span id="Sx3.T1.1.2.4.2.1.1.3.1.1.1" class="ltx_text" style="background-color:#CCCCFF;">(</span>O’Dea<span id="Sx3.T1.1.2.4.2.1.1.3.1.2.2.1.1" class="ltx_text" style="background-color:#CCCCFF;"> </span><a href="#bib.bib29" title="" class="ltx_ref">2021</a><span id="Sx3.T1.1.2.4.2.1.1.3.1.3.3" class="ltx_text" style="background-color:#CCCCFF;">)</span></cite></span></span>
</span></span><span id="Sx3.T1.1.2.4.2.2" class="ltx_text"></span></span>
</td>
</tr>
<tr id="Sx3.T1.1.3" class="ltx_tr" style="background-color:#CCCCFF;">
<td id="Sx3.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T1.1.3.1.1" class="ltx_text" style="background-color:#CCCCFF;">Training</span></td>
<td id="Sx3.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T1.1.3.2.1" class="ltx_text" style="background-color:#CCCCFF;">Almost None</span></td>
<td id="Sx3.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T1.1.3.3.1" class="ltx_text" style="background-color:#CCCCFF;">4 TFLOPs</span></td>
<td id="Sx3.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T1.1.3.4.1" class="ltx_text" style="background-color:#CCCCFF;">1.5 TFLOPs, 8 GB RAM</span></td>
</tr>
<tr id="Sx3.T1.1.4" class="ltx_tr" style="background-color:#E6E6FF;">
<td id="Sx3.T1.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T1.1.4.1.1" class="ltx_text" style="background-color:#E6E6FF;">Inference</span></td>
<td id="Sx3.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T1.1.4.2.1" class="ltx_text" style="background-color:#E6E6FF;">60 GFLOPs</span></td>
<td id="Sx3.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T1.1.4.3.1" class="ltx_text" style="background-color:#E6E6FF;">40 GFLOPs</span></td>
<td id="Sx3.T1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T1.1.4.4.1" class="ltx_text" style="background-color:#E6E6FF;">1.5 TFLOPs, 8 GB RAM</span></td>
</tr>
<tr id="Sx3.T1.1.5" class="ltx_tr" style="background-color:#CCCCFF;">
<td id="Sx3.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T1.1.5.1.1" class="ltx_text" style="background-color:#CCCCFF;">Storage</span></td>
<td id="Sx3.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx3.T1.1.5.2.1" class="ltx_text" style="background-color:#CCCCFF;">600 MB on Disk</span></td>
<td id="Sx3.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx3.T1.1.5.3.1" class="ltx_text" style="background-color:#CCCCFF;">400 MB on Disk</span></td>
<td id="Sx3.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx3.T1.1.5.4.1" class="ltx_text" style="background-color:#CCCCFF;">1 TB on Disk</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>System cost comparison. Assumes 32 local training batch size, 1 local training epoch, 100 total communication rounds for FL. Assumes 196 input sequence length, full precision for <span id="Sx3.T1.3.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> and FL.</figcaption>
</figure>
<section id="Sx3.SSx2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Prompt Learner</h4>

<div id="Sx3.SSx2.SSS0.Px1.p1" class="ltx_para">
<p id="Sx3.SSx2.SSS0.Px1.p1.6" class="ltx_p">Given a pre-trained CLIP backbone, the input to the text encoder is designed in the form of [prompt vectors][class].
Inspired by the simple and straightforward prompt design in <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al. <a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>,
we introduce a set of <math id="Sx3.SSx2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p1.1.m1.1a"><mi id="Sx3.SSx2.SSS0.Px1.p1.1.m1.1.1" xref="Sx3.SSx2.SSS0.Px1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p1.1.m1.1b"><ci id="Sx3.SSx2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p1.1.m1.1c">p</annotation></semantics></math> continuous embeddings of dimension <math id="Sx3.SSx2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p1.2.m2.1a"><mi id="Sx3.SSx2.SSS0.Px1.p1.2.m2.1.1" xref="Sx3.SSx2.SSS0.Px1.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p1.2.m2.1b"><ci id="Sx3.SSx2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p1.2.m2.1c">d</annotation></semantics></math> in the [prompt vectors].
<math id="Sx3.SSx2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p1.3.m3.1a"><mi id="Sx3.SSx2.SSS0.Px1.p1.3.m3.1.1" xref="Sx3.SSx2.SSS0.Px1.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p1.3.m3.1b"><ci id="Sx3.SSx2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p1.3.m3.1c">d</annotation></semantics></math> is same as the dimension of word embeddings in the text encoder, thus 512 by default.
<math id="Sx3.SSx2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="p" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p1.4.m4.1a"><mi id="Sx3.SSx2.SSS0.Px1.p1.4.m4.1.1" xref="Sx3.SSx2.SSS0.Px1.p1.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p1.4.m4.1b"><ci id="Sx3.SSx2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p1.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p1.4.m4.1c">p</annotation></semantics></math> is a hyperparameter specifying the number of embeddings.
In a word, [prompt vectors] are <math id="Sx3.SSx2.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="p" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p1.5.m5.1a"><mi id="Sx3.SSx2.SSS0.Px1.p1.5.m5.1.1" xref="Sx3.SSx2.SSS0.Px1.p1.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p1.5.m5.1b"><ci id="Sx3.SSx2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p1.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p1.5.m5.1c">p</annotation></semantics></math> learnable <math id="Sx3.SSx2.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="d" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p1.6.m6.1a"><mi id="Sx3.SSx2.SSS0.Px1.p1.6.m6.1.1" xref="Sx3.SSx2.SSS0.Px1.p1.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p1.6.m6.1b"><ci id="Sx3.SSx2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p1.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p1.6.m6.1c">d</annotation></semantics></math>-dimensional vectors.</p>
</div>
<div id="Sx3.SSx2.SSS0.Px1.p2" class="ltx_para">
<p id="Sx3.SSx2.SSS0.Px1.p2.11" class="ltx_p">Given a batch of image-text pairs, CLIP will maximize the cosine similarity for matched pairs while minimize the cosine similarity for all other unmatched pairs.
Since CLIP is pre-trained to predict whether an image matches a textual description, it can compute the classification loss and logits by aligning the two embedding spaces for images and texts (<em id="Sx3.SSx2.SSS0.Px1.p2.11.1" class="ltx_emph ltx_font_italic">i.e.</em>, [prompt vectors][class]) respectively.
Formally, let <math id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="g(\cdot)" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1a"><mrow id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.cmml"><mi id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.1.cmml">​</mo><mrow id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.3.2" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.cmml"><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.3.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.3.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1b"><apply id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2"><times id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.1"></times><ci id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.2.2">𝑔</ci><ci id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.1.m1.1c">g(\cdot)</annotation></semantics></math> and <math id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="h(\cdot)" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1a"><mrow id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.cmml"><mi id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.1.cmml">​</mo><mrow id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.3.2" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.cmml"><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.3.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.3.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1b"><apply id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2"><times id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.1"></times><ci id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.2.2">ℎ</ci><ci id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.2.m2.1c">h(\cdot)</annotation></semantics></math> be the feature extraction function of the image and text encoder.
Let <math id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2" class="ltx_Math" alttext="w_{i}=h(\textbf{P},\textbf{K}_{i})" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2a"><mrow id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.cmml"><msub id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.cmml"><mi id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.2" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.2.cmml">w</mi><mi id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.3" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.3.cmml">i</mi></msub><mo id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.2.cmml">=</mo><mrow id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.cmml"><mi id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.3" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.2.cmml">​</mo><mrow id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.2.cmml"><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.2.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.1.1a.cmml">P</mtext><mo id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.3" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.2.cmml">,</mo><msub id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.2a.cmml">K</mtext><mi id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.3" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.4" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2b"><apply id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2"><eq id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.2"></eq><apply id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3"><csymbol cd="ambiguous" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3">subscript</csymbol><ci id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.2">𝑤</ci><ci id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.3.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.3.3">𝑖</ci></apply><apply id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1"><times id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.2"></times><ci id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.3.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.3">ℎ</ci><interval closure="open" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1"><ci id="Sx3.SSx2.SSS0.Px1.p2.3.m3.1.1a.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.1.1"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.1.1">P</mtext></ci><apply id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1">subscript</csymbol><ci id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.2a.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.2">K</mtext></ci><ci id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.3.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.3.m3.2.2.1.1.1.1.3">𝑖</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.3.m3.2c">w_{i}=h(\textbf{P},\textbf{K}_{i})</annotation></semantics></math> be the weight vector generated by the text encoder, where <math id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2" class="ltx_Math" alttext="i\in[1,k]" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2a"><mrow id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.cmml"><mi id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.2" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.2.cmml">i</mi><mo id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.1" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.1.cmml">∈</mo><mrow id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.2" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.1.cmml">[</mo><mn id="Sx3.SSx2.SSS0.Px1.p2.4.m4.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.1.1.cmml">1</mn><mo id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.1.cmml">,</mo><mi id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.2" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.2.cmml">k</mi><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.2.3" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2b"><apply id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3"><in id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.1"></in><ci id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.2">𝑖</ci><interval closure="closed" id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.3.3.2"><cn type="integer" id="Sx3.SSx2.SSS0.Px1.p2.4.m4.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.1.1">1</cn><ci id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.4.m4.2.2">𝑘</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.4.m4.2c">i\in[1,k]</annotation></semantics></math>.
<math id="Sx3.SSx2.SSS0.Px1.p2.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.5.m5.1a"><mi id="Sx3.SSx2.SSS0.Px1.p2.5.m5.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p2.5.m5.1b"><ci id="Sx3.SSx2.SSS0.Px1.p2.5.m5.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.5.m5.1c">k</annotation></semantics></math> denotes the number of classes and each <math id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2" class="ltx_Math" alttext="(\textbf{P},\textbf{K}_{i})" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2a"><mrow id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.2.cmml"><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.2.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.1.1a.cmml">P</mtext><mo id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.3" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.2.cmml">,</mo><msub id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.2a.cmml">K</mtext><mi id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.3" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.4" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2b"><interval closure="open" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1"><ci id="Sx3.SSx2.SSS0.Px1.p2.6.m6.1.1a.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.1.1"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.1.1">P</mtext></ci><apply id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1">subscript</csymbol><ci id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.2a.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.2">K</mtext></ci><ci id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.3.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.6.m6.2.2.1.1.3">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.6.m6.2c">(\textbf{P},\textbf{K}_{i})</annotation></semantics></math> is derived from the prompt in the form of [prompt vectors][class]<sub id="Sx3.SSx2.SSS0.Px1.p2.11.2" class="ltx_sub"><span id="Sx3.SSx2.SSS0.Px1.p2.11.2.1" class="ltx_text ltx_font_italic">i</span></sub>, where [class]<sub id="Sx3.SSx2.SSS0.Px1.p2.11.3" class="ltx_sub"><span id="Sx3.SSx2.SSS0.Px1.p2.11.3.1" class="ltx_text ltx_font_italic">i</span></sub> is replaced by the word embedding vector of specific class label name.
Let <math id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1" class="ltx_math_unparsed" alttext="\cos[\cdot|\cdot]" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1a"><mrow id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1b"><mi id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1.1">cos</mi><mrow id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1.2"><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1.2.1">[</mo><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1.2.2">⋅</mo><mo fence="false" stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1.2.3">|</mo><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1.2.4">⋅</mo><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1.2.5">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.9.m9.1c">\cos[\cdot|\cdot]</annotation></semantics></math> denote the cosine similarity used by CLIP.
By forwarding a <math id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2" class="ltx_Math" alttext="(\textbf{P},\textbf{K}_{i})" display="inline"><semantics id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2a"><mrow id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.2.cmml"><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.2.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.1.1a.cmml">P</mtext><mo id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.3" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.2.cmml">,</mo><msub id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.2" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.2a.cmml">K</mtext><mi id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.3" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.4" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2b"><interval closure="open" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1"><ci id="Sx3.SSx2.SSS0.Px1.p2.10.m10.1.1a.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.1.1"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.1.1">P</mtext></ci><apply id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1"><csymbol cd="ambiguous" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.1.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1">subscript</csymbol><ci id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.2a.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.2.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.2">K</mtext></ci><ci id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.3.cmml" xref="Sx3.SSx2.SSS0.Px1.p2.10.m10.2.2.1.1.3">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSS0.Px1.p2.10.m10.2c">(\textbf{P},\textbf{K}_{i})</annotation></semantics></math> and an image <span id="Sx3.SSx2.SSS0.Px1.p2.11.4" class="ltx_text ltx_markedasmath ltx_font_bold">x</span>, the classification prediction probability and logits are computed as</p>
<table id="Sx5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="Sx3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Sx3.E1.m1.9" class="ltx_Math" alttext="\displaystyle\text{p}(\textbf{y}=i|\textbf{x})=\frac{\exp\left(cos[g(\textbf{x})|h(\textbf{P},\textbf{K}_{i})]\right)}{\sum_{j=1}^{k}\exp\left(cos[g(\textbf{x})|h(\textbf{P},\textbf{K}_{j})]\right)}," display="inline"><semantics id="Sx3.E1.m1.9a"><mrow id="Sx3.E1.m1.9.9.1" xref="Sx3.E1.m1.9.9.1.1.cmml"><mrow id="Sx3.E1.m1.9.9.1.1" xref="Sx3.E1.m1.9.9.1.1.cmml"><mrow id="Sx3.E1.m1.9.9.1.1.1" xref="Sx3.E1.m1.9.9.1.1.1.cmml"><mtext id="Sx3.E1.m1.9.9.1.1.1.3" xref="Sx3.E1.m1.9.9.1.1.1.3a.cmml">p</mtext><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.9.9.1.1.1.2" xref="Sx3.E1.m1.9.9.1.1.1.2.cmml">​</mo><mrow id="Sx3.E1.m1.9.9.1.1.1.1.1" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.E1.m1.9.9.1.1.1.1.1.2" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx3.E1.m1.9.9.1.1.1.1.1.1" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.9.9.1.1.1.1.1.1.2" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.2a.cmml">y</mtext><mo id="Sx3.E1.m1.9.9.1.1.1.1.1.1.1" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.1.cmml">=</mo><mrow id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.cmml"><mi id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.2" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.2.cmml">i</mi><mo fence="false" id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.1" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.1.cmml">|</mo><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.3" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.3a.cmml">x</mtext></mrow></mrow><mo stretchy="false" id="Sx3.E1.m1.9.9.1.1.1.1.1.3" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Sx3.E1.m1.9.9.1.1.2" xref="Sx3.E1.m1.9.9.1.1.2.cmml">=</mo><mstyle displaystyle="true" id="Sx3.E1.m1.8.8" xref="Sx3.E1.m1.8.8.cmml"><mfrac id="Sx3.E1.m1.8.8a" xref="Sx3.E1.m1.8.8.cmml"><mrow id="Sx3.E1.m1.4.4.4.4" xref="Sx3.E1.m1.4.4.4.5.cmml"><mi id="Sx3.E1.m1.3.3.3.3" xref="Sx3.E1.m1.3.3.3.3.cmml">exp</mi><mo id="Sx3.E1.m1.4.4.4.4a" xref="Sx3.E1.m1.4.4.4.5.cmml">⁡</mo><mrow id="Sx3.E1.m1.4.4.4.4.1" xref="Sx3.E1.m1.4.4.4.5.cmml"><mo id="Sx3.E1.m1.4.4.4.4.1.2" xref="Sx3.E1.m1.4.4.4.5.cmml">(</mo><mrow id="Sx3.E1.m1.4.4.4.4.1.1" xref="Sx3.E1.m1.4.4.4.4.1.1.cmml"><mi id="Sx3.E1.m1.4.4.4.4.1.1.3" xref="Sx3.E1.m1.4.4.4.4.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.4.4.4.1.1.2" xref="Sx3.E1.m1.4.4.4.4.1.1.2.cmml">​</mo><mi id="Sx3.E1.m1.4.4.4.4.1.1.4" xref="Sx3.E1.m1.4.4.4.4.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.4.4.4.1.1.2a" xref="Sx3.E1.m1.4.4.4.4.1.1.2.cmml">​</mo><mi id="Sx3.E1.m1.4.4.4.4.1.1.5" xref="Sx3.E1.m1.4.4.4.4.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.4.4.4.1.1.2b" xref="Sx3.E1.m1.4.4.4.4.1.1.2.cmml">​</mo><mrow id="Sx3.E1.m1.4.4.4.4.1.1.1.1" xref="Sx3.E1.m1.4.4.4.4.1.1.1.2.cmml"><mo stretchy="false" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.2" xref="Sx3.E1.m1.4.4.4.4.1.1.1.2.1.cmml">[</mo><mrow id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.cmml"><mrow id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.cmml"><mi id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.2" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.1" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.1.cmml">​</mo><mrow id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.3.2" xref="Sx3.E1.m1.1.1.1.1a.cmml"><mo stretchy="false" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.3.2.1" xref="Sx3.E1.m1.1.1.1.1a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.1.1.1.1" xref="Sx3.E1.m1.1.1.1.1.cmml">x</mtext><mo stretchy="false" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.3.2.2" xref="Sx3.E1.m1.1.1.1.1a.cmml">)</mo></mrow></mrow><mo fence="false" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.2" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.2.cmml">|</mo><mrow id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.3" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.2" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.2.cmml">​</mo><mrow id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.2.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.2.2.2.2" xref="Sx3.E1.m1.2.2.2.2a.cmml">P</mtext><mo id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.2a.cmml">K</mtext><mi id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.4" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.3" xref="Sx3.E1.m1.4.4.4.4.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="Sx3.E1.m1.4.4.4.4.1.3" xref="Sx3.E1.m1.4.4.4.5.cmml">)</mo></mrow></mrow><mrow id="Sx3.E1.m1.8.8.8" xref="Sx3.E1.m1.8.8.8.cmml"><msubsup id="Sx3.E1.m1.8.8.8.5" xref="Sx3.E1.m1.8.8.8.5.cmml"><mo id="Sx3.E1.m1.8.8.8.5.2.2" xref="Sx3.E1.m1.8.8.8.5.2.2.cmml">∑</mo><mrow id="Sx3.E1.m1.8.8.8.5.2.3" xref="Sx3.E1.m1.8.8.8.5.2.3.cmml"><mi id="Sx3.E1.m1.8.8.8.5.2.3.2" xref="Sx3.E1.m1.8.8.8.5.2.3.2.cmml">j</mi><mo id="Sx3.E1.m1.8.8.8.5.2.3.1" xref="Sx3.E1.m1.8.8.8.5.2.3.1.cmml">=</mo><mn id="Sx3.E1.m1.8.8.8.5.2.3.3" xref="Sx3.E1.m1.8.8.8.5.2.3.3.cmml">1</mn></mrow><mi id="Sx3.E1.m1.8.8.8.5.3" xref="Sx3.E1.m1.8.8.8.5.3.cmml">k</mi></msubsup><mrow id="Sx3.E1.m1.8.8.8.4.1" xref="Sx3.E1.m1.8.8.8.4.2.cmml"><mi id="Sx3.E1.m1.7.7.7.3" xref="Sx3.E1.m1.7.7.7.3.cmml">exp</mi><mo id="Sx3.E1.m1.8.8.8.4.1a" xref="Sx3.E1.m1.8.8.8.4.2.cmml">⁡</mo><mrow id="Sx3.E1.m1.8.8.8.4.1.1" xref="Sx3.E1.m1.8.8.8.4.2.cmml"><mo id="Sx3.E1.m1.8.8.8.4.1.1.2" xref="Sx3.E1.m1.8.8.8.4.2.cmml">(</mo><mrow id="Sx3.E1.m1.8.8.8.4.1.1.1" xref="Sx3.E1.m1.8.8.8.4.1.1.1.cmml"><mi id="Sx3.E1.m1.8.8.8.4.1.1.1.3" xref="Sx3.E1.m1.8.8.8.4.1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.8.8.8.4.1.1.1.2" xref="Sx3.E1.m1.8.8.8.4.1.1.1.2.cmml">​</mo><mi id="Sx3.E1.m1.8.8.8.4.1.1.1.4" xref="Sx3.E1.m1.8.8.8.4.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.8.8.8.4.1.1.1.2a" xref="Sx3.E1.m1.8.8.8.4.1.1.1.2.cmml">​</mo><mi id="Sx3.E1.m1.8.8.8.4.1.1.1.5" xref="Sx3.E1.m1.8.8.8.4.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.8.8.8.4.1.1.1.2b" xref="Sx3.E1.m1.8.8.8.4.1.1.1.2.cmml">​</mo><mrow id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.2.cmml"><mo stretchy="false" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.2" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.2.1.cmml">[</mo><mrow id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.cmml"><mrow id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.cmml"><mi id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.2" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.1" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.3.2" xref="Sx3.E1.m1.5.5.5.1a.cmml"><mo stretchy="false" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.3.2.1" xref="Sx3.E1.m1.5.5.5.1a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.5.5.5.1" xref="Sx3.E1.m1.5.5.5.1.cmml">x</mtext><mo stretchy="false" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.3.2.2" xref="Sx3.E1.m1.5.5.5.1a.cmml">)</mo></mrow></mrow><mo fence="false" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.2" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.2.cmml">|</mo><mrow id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.2.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.6.6.6.2" xref="Sx3.E1.m1.6.6.6.2a.cmml">P</mtext><mo id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.2a.cmml">K</mtext><mi id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.4" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.3" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="Sx3.E1.m1.8.8.8.4.1.1.3" xref="Sx3.E1.m1.8.8.8.4.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo id="Sx3.E1.m1.9.9.1.2" xref="Sx3.E1.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E1.m1.9b"><apply id="Sx3.E1.m1.9.9.1.1.cmml" xref="Sx3.E1.m1.9.9.1"><eq id="Sx3.E1.m1.9.9.1.1.2.cmml" xref="Sx3.E1.m1.9.9.1.1.2"></eq><apply id="Sx3.E1.m1.9.9.1.1.1.cmml" xref="Sx3.E1.m1.9.9.1.1.1"><times id="Sx3.E1.m1.9.9.1.1.1.2.cmml" xref="Sx3.E1.m1.9.9.1.1.1.2"></times><ci id="Sx3.E1.m1.9.9.1.1.1.3a.cmml" xref="Sx3.E1.m1.9.9.1.1.1.3"><mtext id="Sx3.E1.m1.9.9.1.1.1.3.cmml" xref="Sx3.E1.m1.9.9.1.1.1.3">p</mtext></ci><apply id="Sx3.E1.m1.9.9.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1"><eq id="Sx3.E1.m1.9.9.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.1"></eq><ci id="Sx3.E1.m1.9.9.1.1.1.1.1.1.2a.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.9.9.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.2">y</mtext></ci><apply id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3"><csymbol cd="latexml" id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.1">conditional</csymbol><ci id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.2">𝑖</ci><ci id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.3a.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.3"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.3.cmml" xref="Sx3.E1.m1.9.9.1.1.1.1.1.1.3.3">x</mtext></ci></apply></apply></apply><apply id="Sx3.E1.m1.8.8.cmml" xref="Sx3.E1.m1.8.8"><divide id="Sx3.E1.m1.8.8.9.cmml" xref="Sx3.E1.m1.8.8"></divide><apply id="Sx3.E1.m1.4.4.4.5.cmml" xref="Sx3.E1.m1.4.4.4.4"><exp id="Sx3.E1.m1.3.3.3.3.cmml" xref="Sx3.E1.m1.3.3.3.3"></exp><apply id="Sx3.E1.m1.4.4.4.4.1.1.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1"><times id="Sx3.E1.m1.4.4.4.4.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.2"></times><ci id="Sx3.E1.m1.4.4.4.4.1.1.3.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.3">𝑐</ci><ci id="Sx3.E1.m1.4.4.4.4.1.1.4.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.4">𝑜</ci><ci id="Sx3.E1.m1.4.4.4.4.1.1.5.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.5">𝑠</ci><apply id="Sx3.E1.m1.4.4.4.4.1.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1"><csymbol cd="latexml" id="Sx3.E1.m1.4.4.4.4.1.1.1.2.1.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.2">delimited-[]</csymbol><apply id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1"><csymbol cd="latexml" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.2">conditional</csymbol><apply id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3"><times id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.1.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.1"></times><ci id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.2.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.2">𝑔</ci><ci id="Sx3.E1.m1.1.1.1.1a.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1.1">x</mtext></ci></apply><apply id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1"><times id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.2"></times><ci id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.3">ℎ</ci><interval closure="open" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1"><ci id="Sx3.E1.m1.2.2.2.2a.cmml" xref="Sx3.E1.m1.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.2.2.2.2.cmml" xref="Sx3.E1.m1.2.2.2.2">P</mtext></ci><apply id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.2a.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.2">K</mtext></ci><ci id="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></interval></apply></apply></apply></apply></apply><apply id="Sx3.E1.m1.8.8.8.cmml" xref="Sx3.E1.m1.8.8.8"><apply id="Sx3.E1.m1.8.8.8.5.cmml" xref="Sx3.E1.m1.8.8.8.5"><csymbol cd="ambiguous" id="Sx3.E1.m1.8.8.8.5.1.cmml" xref="Sx3.E1.m1.8.8.8.5">superscript</csymbol><apply id="Sx3.E1.m1.8.8.8.5.2.cmml" xref="Sx3.E1.m1.8.8.8.5"><csymbol cd="ambiguous" id="Sx3.E1.m1.8.8.8.5.2.1.cmml" xref="Sx3.E1.m1.8.8.8.5">subscript</csymbol><sum id="Sx3.E1.m1.8.8.8.5.2.2.cmml" xref="Sx3.E1.m1.8.8.8.5.2.2"></sum><apply id="Sx3.E1.m1.8.8.8.5.2.3.cmml" xref="Sx3.E1.m1.8.8.8.5.2.3"><eq id="Sx3.E1.m1.8.8.8.5.2.3.1.cmml" xref="Sx3.E1.m1.8.8.8.5.2.3.1"></eq><ci id="Sx3.E1.m1.8.8.8.5.2.3.2.cmml" xref="Sx3.E1.m1.8.8.8.5.2.3.2">𝑗</ci><cn type="integer" id="Sx3.E1.m1.8.8.8.5.2.3.3.cmml" xref="Sx3.E1.m1.8.8.8.5.2.3.3">1</cn></apply></apply><ci id="Sx3.E1.m1.8.8.8.5.3.cmml" xref="Sx3.E1.m1.8.8.8.5.3">𝑘</ci></apply><apply id="Sx3.E1.m1.8.8.8.4.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1"><exp id="Sx3.E1.m1.7.7.7.3.cmml" xref="Sx3.E1.m1.7.7.7.3"></exp><apply id="Sx3.E1.m1.8.8.8.4.1.1.1.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1"><times id="Sx3.E1.m1.8.8.8.4.1.1.1.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.2"></times><ci id="Sx3.E1.m1.8.8.8.4.1.1.1.3.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.3">𝑐</ci><ci id="Sx3.E1.m1.8.8.8.4.1.1.1.4.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.4">𝑜</ci><ci id="Sx3.E1.m1.8.8.8.4.1.1.1.5.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.5">𝑠</ci><apply id="Sx3.E1.m1.8.8.8.4.1.1.1.1.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1"><csymbol cd="latexml" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.2.1.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.2">delimited-[]</csymbol><apply id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.2">conditional</csymbol><apply id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3"><times id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.1.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.1"></times><ci id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.2">𝑔</ci><ci id="Sx3.E1.m1.5.5.5.1a.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.5.5.5.1.cmml" xref="Sx3.E1.m1.5.5.5.1">x</mtext></ci></apply><apply id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1"><times id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.2"></times><ci id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.3">ℎ</ci><interval closure="open" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1"><ci id="Sx3.E1.m1.6.6.6.2a.cmml" xref="Sx3.E1.m1.6.6.6.2"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.6.6.6.2.cmml" xref="Sx3.E1.m1.6.6.6.2">P</mtext></ci><apply id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.2a.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.2">K</mtext></ci><ci id="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.8.8.8.4.1.1.1.1.1.1.1.1.1.1.3">𝑗</ci></apply></interval></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E1.m1.9c">\displaystyle\text{p}(\textbf{y}=i|\textbf{x})=\frac{\exp\left(cos[g(\textbf{x})|h(\textbf{P},\textbf{K}_{i})]\right)}{\sum_{j=1}^{k}\exp\left(cos[g(\textbf{x})|h(\textbf{P},\textbf{K}_{j})]\right)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="Sx3.SSx2.SSS0.Px1.p2.12" class="ltx_p">where <span id="Sx3.SSx2.SSS0.Px1.p2.12.1" class="ltx_text ltx_markedasmath ltx_font_bold">P</span> is the only part that is updated in local back propagation and aggregated in the federated server.</p>
</div>
<div id="Sx3.SSx2.SSS0.Px1.p3" class="ltx_para">
<p id="Sx3.SSx2.SSS0.Px1.p3.1" class="ltx_p">Prompting are particularly useful in the FL case, as using prompts to push the model in the correct direction is particularly effective.
This feature enables prompting to converge quickly in FL, requires less data per user, and is less affected by adverse factors in the process, <em id="Sx3.SSx2.SSS0.Px1.p3.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, non-IID and unbalanced data, limited communication bandwidth, and unreliable and limited device availability.
In this paper, the prompt learner employed in <span id="Sx3.SSx2.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> though simple and straightforward as a bridge to our core idea is easy to follow.
We also envision that more complex and effective bridges would be there to replace the role and should be a valuable direction.</p>
</div>
</section>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">System Feasibility</h3>

<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.1" class="ltx_p">We examine the feasibility of <span id="Sx3.SSx3.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> on modern hardware, focusing conservatively on personal cell phones.
We notice that users can access GPUs from their mobile phones.
Enterprise users have more abundant resources.
Without loss of generality, we take a 100M parameter model for FL and 150M parameter CLIP backbone for image similarity-search of <span id="Sx3.SSx3.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span>.
The prompt learner introduces only a small number of parameters, that can be ignored.
We assume that the FL configures 32 local training batch size, 1 local training epoch, and 100 total communication rounds, which suggested in <cite class="ltx_cite ltx_citemacro_citep">(Qu et al. <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>.
We also assume that both FL and <span id="Sx3.SSx3.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> configure 196 input sequence length and the full precision.
The system cost comparison is summarized in Table <a href="#Sx3.T1" title="Table 1 ‣ Framework to Learn Prompts in FL ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> along the following dimensions:</p>
</div>
<section id="Sx3.SSx3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Communication</h4>

<div id="Sx3.SSx3.SSS0.Px1.p1" class="ltx_para">
<p id="Sx3.SSx3.SSS0.Px1.p1.1" class="ltx_p">The average download speed within the globe for mobile internet was 54 Mbps, and the average upload speed for mobile internet was 12 Mbps that reported by 2021 <cite class="ltx_cite ltx_citemacro_citep">(O’Dea <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>.
<span id="Sx3.SSx3.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> requires locally downloading while FL requires communicating the model repeatedly between users and the federated server.
Thus, the communication cost in terms of file transfer volume is that it takes only 1.4 minutes to transfer 600MB for <span id="Sx3.SSx3.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span>, and 9 hours for FL to transfer 40GB.</p>
</div>
<figure id="Sx3.T2" class="ltx_table">
<table id="Sx3.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="Sx3.T2.4.5" class="ltx_tr">
<td id="Sx3.T2.4.5.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="3">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="Sx3.T2.4.5.1.1" class="ltx_text ltx_font_smallcaps">Benchmark</span>
</td>
<td id="Sx3.T2.4.5.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="3"><span id="Sx3.T2.4.5.2.1" class="ltx_text ltx_font_smallcaps">Method</span></td>
<td id="Sx3.T2.4.5.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="4"><span id="Sx3.T2.4.5.3.1" class="ltx_text ltx_font_smallcaps">IID</span></td>
<td id="Sx3.T2.4.5.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="4"><span id="Sx3.T2.4.5.4.1" class="ltx_text ltx_font_smallcaps">Extreme Non-IID</span></td>
<td id="Sx3.T2.4.5.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2"><span id="Sx3.T2.4.5.5.1" class="ltx_text ltx_font_smallcaps">Learnable</span></td>
</tr>
<tr id="Sx3.T2.4.4" class="ltx_tr">
<td id="Sx3.T2.1.1.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2"><span id="Sx3.T2.1.1.1.1" class="ltx_text ltx_font_italic">Accuracy<math id="Sx3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="Sx3.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="Sx3.T2.1.1.1.1.m1.1.1" xref="Sx3.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Sx3.T2.1.1.1.1.m1.1b"><ci id="Sx3.T2.1.1.1.1.m1.1.1.cmml" xref="Sx3.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="Sx3.T2.2.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2"><span id="Sx3.T2.2.2.2.1" class="ltx_text ltx_font_italic">F-1<math id="Sx3.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="Sx3.T2.2.2.2.1.m1.1a"><mo stretchy="false" id="Sx3.T2.2.2.2.1.m1.1.1" xref="Sx3.T2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Sx3.T2.2.2.2.1.m1.1b"><ci id="Sx3.T2.2.2.2.1.m1.1.1.cmml" xref="Sx3.T2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="Sx3.T2.3.3.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2"><span id="Sx3.T2.3.3.3.1" class="ltx_text ltx_font_italic">Accuracy<math id="Sx3.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="Sx3.T2.3.3.3.1.m1.1a"><mo stretchy="false" id="Sx3.T2.3.3.3.1.m1.1.1" xref="Sx3.T2.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Sx3.T2.3.3.3.1.m1.1b"><ci id="Sx3.T2.3.3.3.1.m1.1.1.cmml" xref="Sx3.T2.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T2.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="Sx3.T2.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2"><span id="Sx3.T2.4.4.4.1" class="ltx_text ltx_font_italic">F-1<math id="Sx3.T2.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="Sx3.T2.4.4.4.1.m1.1a"><mo stretchy="false" id="Sx3.T2.4.4.4.1.m1.1.1" xref="Sx3.T2.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Sx3.T2.4.4.4.1.m1.1b"><ci id="Sx3.T2.4.4.4.1.m1.1.1.cmml" xref="Sx3.T2.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T2.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="Sx3.T2.4.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2"><span id="Sx3.T2.4.4.5.1" class="ltx_text ltx_font_smallcaps">Parameters</span></td>
</tr>
<tr id="Sx3.T2.4.6" class="ltx_tr">
<td id="Sx3.T2.4.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Rn50</td>
<td id="Sx3.T2.4.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vit</td>
<td id="Sx3.T2.4.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Rn50</td>
<td id="Sx3.T2.4.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vit</td>
<td id="Sx3.T2.4.6.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Rn50</td>
<td id="Sx3.T2.4.6.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vit</td>
<td id="Sx3.T2.4.6.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Rn50</td>
<td id="Sx3.T2.4.6.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vit</td>
<td id="Sx3.T2.4.6.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Rn50</td>
<td id="Sx3.T2.4.6.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vit</td>
</tr>
<tr id="Sx3.T2.4.7" class="ltx_tr">
<td id="Sx3.T2.4.7.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="3">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="Sx3.T2.4.7.1.1" class="ltx_text">Caltech101</span>
</td>
<td id="Sx3.T2.4.7.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.2.1" class="ltx_text ltx_font_smallcaps">PromptFL</span></td>
<td id="Sx3.T2.4.7.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.3.1" class="ltx_text ltx_font_bold">90.18</span></td>
<td id="Sx3.T2.4.7.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.4.1" class="ltx_text ltx_font_bold">94.65</span></td>
<td id="Sx3.T2.4.7.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.5.1" class="ltx_text ltx_font_bold">86.09</span></td>
<td id="Sx3.T2.4.7.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.6.1" class="ltx_text ltx_font_bold">91.76</span></td>
<td id="Sx3.T2.4.7.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.7.1" class="ltx_text ltx_font_bold">88.72</span></td>
<td id="Sx3.T2.4.7.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.8.1" class="ltx_text ltx_font_bold">94.12</span></td>
<td id="Sx3.T2.4.7.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.9.1" class="ltx_text ltx_font_bold">83.98</span></td>
<td id="Sx3.T2.4.7.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.7.10.1" class="ltx_text ltx_font_bold">90.48</span></td>
<td id="Sx3.T2.4.7.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</td>
<td id="Sx3.T2.4.7.12" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.01%</td>
</tr>
<tr id="Sx3.T2.4.8" class="ltx_tr">
<td id="Sx3.T2.4.8.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Finetuning FL</td>
<td id="Sx3.T2.4.8.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">90.02</td>
<td id="Sx3.T2.4.8.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">93.1</td>
<td id="Sx3.T2.4.8.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">84.72</td>
<td id="Sx3.T2.4.8.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">89.07</td>
<td id="Sx3.T2.4.8.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">29.78</td>
<td id="Sx3.T2.4.8.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">29.89</td>
<td id="Sx3.T2.4.8.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.2</td>
<td id="Sx3.T2.4.8.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.2</td>
<td id="Sx3.T2.4.8.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.8.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.9" class="ltx_tr">
<td id="Sx3.T2.4.9.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FL from scratch</td>
<td id="Sx3.T2.4.9.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">32.41</td>
<td id="Sx3.T2.4.9.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">32.49</td>
<td id="Sx3.T2.4.9.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.51</td>
<td id="Sx3.T2.4.9.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.89</td>
<td id="Sx3.T2.4.9.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.9.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.9.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.9.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.9.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.9.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.10" class="ltx_tr">
<td id="Sx3.T2.4.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="3"><span id="Sx3.T2.4.10.1.1" class="ltx_text">Flowers102</span></td>
<td id="Sx3.T2.4.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.10.2.1" class="ltx_text ltx_font_smallcaps">PromptFL</span></td>
<td id="Sx3.T2.4.10.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">88.14</td>
<td id="Sx3.T2.4.10.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">90.5</td>
<td id="Sx3.T2.4.10.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">87.62</td>
<td id="Sx3.T2.4.10.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">90.14</td>
<td id="Sx3.T2.4.10.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.10.7.1" class="ltx_text ltx_font_bold">66.3</span></td>
<td id="Sx3.T2.4.10.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.10.8.1" class="ltx_text ltx_font_bold">74.75</span></td>
<td id="Sx3.T2.4.10.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.10.9.1" class="ltx_text ltx_font_bold">60.14</span></td>
<td id="Sx3.T2.4.10.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.10.10.1" class="ltx_text ltx_font_bold">69.13</span></td>
<td id="Sx3.T2.4.10.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</td>
<td id="Sx3.T2.4.10.12" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.01%</td>
</tr>
<tr id="Sx3.T2.4.11" class="ltx_tr">
<td id="Sx3.T2.4.11.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Finetuning FL</td>
<td id="Sx3.T2.4.11.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.11.2.1" class="ltx_text ltx_font_bold">92.6</span></td>
<td id="Sx3.T2.4.11.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.11.3.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="Sx3.T2.4.11.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.11.4.1" class="ltx_text ltx_font_bold">91.56</span></td>
<td id="Sx3.T2.4.11.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.11.5.1" class="ltx_text ltx_font_bold">90.7</span></td>
<td id="Sx3.T2.4.11.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">24.4</td>
<td id="Sx3.T2.4.11.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">24.5</td>
<td id="Sx3.T2.4.11.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.68</td>
<td id="Sx3.T2.4.11.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.18</td>
<td id="Sx3.T2.4.11.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.11.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.12" class="ltx_tr">
<td id="Sx3.T2.4.12.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FL from scratch</td>
<td id="Sx3.T2.4.12.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">33.17</td>
<td id="Sx3.T2.4.12.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">38</td>
<td id="Sx3.T2.4.12.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">25.7</td>
<td id="Sx3.T2.4.12.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">32.5</td>
<td id="Sx3.T2.4.12.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.12.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.12.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.12.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.12.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.12.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.13" class="ltx_tr">
<td id="Sx3.T2.4.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="3"><span id="Sx3.T2.4.13.1.1" class="ltx_text">OxfordPets</span></td>
<td id="Sx3.T2.4.13.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.13.2.1" class="ltx_text ltx_font_smallcaps">PromptFL</span></td>
<td id="Sx3.T2.4.13.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">88.5</td>
<td id="Sx3.T2.4.13.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.13.4.1" class="ltx_text ltx_font_bold">92.89</span></td>
<td id="Sx3.T2.4.13.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">88.44</td>
<td id="Sx3.T2.4.13.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.13.6.1" class="ltx_text ltx_font_bold">92.8</span></td>
<td id="Sx3.T2.4.13.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.13.7.1" class="ltx_text ltx_font_bold">87.03</span></td>
<td id="Sx3.T2.4.13.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.13.8.1" class="ltx_text ltx_font_bold">89.51</span></td>
<td id="Sx3.T2.4.13.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.13.9.1" class="ltx_text ltx_font_bold">86.85</span></td>
<td id="Sx3.T2.4.13.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.13.10.1" class="ltx_text ltx_font_bold">88.45</span></td>
<td id="Sx3.T2.4.13.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</td>
<td id="Sx3.T2.4.13.12" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.01%</td>
</tr>
<tr id="Sx3.T2.4.14" class="ltx_tr">
<td id="Sx3.T2.4.14.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Finetuning FL</td>
<td id="Sx3.T2.4.14.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.14.2.1" class="ltx_text ltx_font_bold">90.38</span></td>
<td id="Sx3.T2.4.14.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">92.1</td>
<td id="Sx3.T2.4.14.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.14.4.1" class="ltx_text ltx_font_bold">90.06</span></td>
<td id="Sx3.T2.4.14.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">91.92</td>
<td id="Sx3.T2.4.14.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">24.83</td>
<td id="Sx3.T2.4.14.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">25.27</td>
<td id="Sx3.T2.4.14.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.3</td>
<td id="Sx3.T2.4.14.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.93</td>
<td id="Sx3.T2.4.14.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.14.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.15" class="ltx_tr">
<td id="Sx3.T2.4.15.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FL from scratch</td>
<td id="Sx3.T2.4.15.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.25</td>
<td id="Sx3.T2.4.15.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.722</td>
<td id="Sx3.T2.4.15.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.624</td>
<td id="Sx3.T2.4.15.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.318</td>
<td id="Sx3.T2.4.15.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.15.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.15.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.15.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.15.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.15.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.16" class="ltx_tr">
<td id="Sx3.T2.4.16.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="3"><span id="Sx3.T2.4.16.1.1" class="ltx_text">Food101</span></td>
<td id="Sx3.T2.4.16.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.2.1" class="ltx_text ltx_font_smallcaps">PromptFL</span></td>
<td id="Sx3.T2.4.16.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.3.1" class="ltx_text ltx_font_bold">78.0</span></td>
<td id="Sx3.T2.4.16.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.4.1" class="ltx_text ltx_font_bold">85.75</span></td>
<td id="Sx3.T2.4.16.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.5.1" class="ltx_text ltx_font_bold">77.9</span></td>
<td id="Sx3.T2.4.16.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.6.1" class="ltx_text ltx_font_bold">85.66</span></td>
<td id="Sx3.T2.4.16.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.7.1" class="ltx_text ltx_font_bold">78.1</span></td>
<td id="Sx3.T2.4.16.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.8.1" class="ltx_text ltx_font_bold">85.88</span></td>
<td id="Sx3.T2.4.16.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.9.1" class="ltx_text ltx_font_bold">78.03</span></td>
<td id="Sx3.T2.4.16.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx3.T2.4.16.10.1" class="ltx_text ltx_font_bold">85.8</span></td>
<td id="Sx3.T2.4.16.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</td>
<td id="Sx3.T2.4.16.12" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.01%</td>
</tr>
<tr id="Sx3.T2.4.17" class="ltx_tr">
<td id="Sx3.T2.4.17.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Finetuning FL</td>
<td id="Sx3.T2.4.17.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">69.28</td>
<td id="Sx3.T2.4.17.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">76.68</td>
<td id="Sx3.T2.4.17.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">69.08</td>
<td id="Sx3.T2.4.17.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">76.85</td>
<td id="Sx3.T2.4.17.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">22.92</td>
<td id="Sx3.T2.4.17.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">23.8</td>
<td id="Sx3.T2.4.17.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.19</td>
<td id="Sx3.T2.4.17.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.73</td>
<td id="Sx3.T2.4.17.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.17.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.18" class="ltx_tr">
<td id="Sx3.T2.4.18.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FL from scratch</td>
<td id="Sx3.T2.4.18.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">21.11</td>
<td id="Sx3.T2.4.18.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">21.03</td>
<td id="Sx3.T2.4.18.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">19.75</td>
<td id="Sx3.T2.4.18.5" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">19.92</td>
<td id="Sx3.T2.4.18.6" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.18.7" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.18.8" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.18.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="Sx3.T2.4.18.10" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="Sx3.T2.4.18.11" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
</tr>
<tr id="Sx3.T2.4.19" class="ltx_tr">
<td id="Sx3.T2.4.19.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="Sx3.T2.4.19.2" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.3" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.4" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.5" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.7" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.8" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.9" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.10" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.11" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="Sx3.T2.4.19.12" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="Sx3.T2.9.1" class="ltx_text ltx_font_bold">Performance of <span id="Sx3.T2.9.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> against existing FL framework on the four datasets</span>. The table report the accuracy and F-1 score according to the corresponding backbone and method. The best score of each group appears in bold. Compared with finetuning and training from the scratch, <span id="Sx3.T2.10.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> only update 0.01% <math id="Sx3.T2.6.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx3.T2.6.m1.1b"><mo id="Sx3.T2.6.m1.1.1" xref="Sx3.T2.6.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx3.T2.6.m1.1c"><csymbol cd="latexml" id="Sx3.T2.6.m1.1.1.cmml" xref="Sx3.T2.6.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T2.6.m1.1d">\sim</annotation></semantics></math> 0.1% parameters, however, still outperforms other methods across datasets. Given the poor result in training from the scratch even with iid mode, we assume that the performance from the non-iid setting can be even wore, so we omit the result in this row.</figcaption>
</figure>
</section>
<section id="Sx3.SSx3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training and Inference</h4>

<div id="Sx3.SSx3.SSS0.Px2.p1" class="ltx_para">
<p id="Sx3.SSx3.SSS0.Px2.p1.7" class="ltx_p">FL requires FLOPs computed by (2<math id="Sx3.SSx3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx3.SSS0.Px2.p1.1.m1.1a"><mo id="Sx3.SSx3.SSS0.Px2.p1.1.m1.1.1" xref="Sx3.SSx3.SSS0.Px2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.SSS0.Px2.p1.1.m1.1b"><times id="Sx3.SSx3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="Sx3.SSx3.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math>3<math id="Sx3.SSx3.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx3.SSS0.Px2.p1.2.m2.1a"><mo id="Sx3.SSx3.SSS0.Px2.p1.2.m2.1.1" xref="Sx3.SSx3.SSS0.Px2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.SSS0.Px2.p1.2.m2.1b"><times id="Sx3.SSx3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="Sx3.SSx3.SSS0.Px2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.SSS0.Px2.p1.2.m2.1c">\times</annotation></semantics></math>model parameters<math id="Sx3.SSx3.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx3.SSS0.Px2.p1.3.m3.1a"><mo id="Sx3.SSx3.SSS0.Px2.p1.3.m3.1.1" xref="Sx3.SSx3.SSS0.Px2.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.SSS0.Px2.p1.3.m3.1b"><times id="Sx3.SSx3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="Sx3.SSx3.SSS0.Px2.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.SSS0.Px2.p1.3.m3.1c">\times</annotation></semantics></math>local training epoch<math id="Sx3.SSx3.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx3.SSS0.Px2.p1.4.m4.1a"><mo id="Sx3.SSx3.SSS0.Px2.p1.4.m4.1.1" xref="Sx3.SSx3.SSS0.Px2.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.SSS0.Px2.p1.4.m4.1b"><times id="Sx3.SSx3.SSS0.Px2.p1.4.m4.1.1.cmml" xref="Sx3.SSx3.SSS0.Px2.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.SSS0.Px2.p1.4.m4.1c">\times</annotation></semantics></math>local training batch size<math id="Sx3.SSx3.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx3.SSS0.Px2.p1.5.m5.1a"><mo id="Sx3.SSx3.SSS0.Px2.p1.5.m5.1.1" xref="Sx3.SSx3.SSS0.Px2.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.SSS0.Px2.p1.5.m5.1b"><times id="Sx3.SSx3.SSS0.Px2.p1.5.m5.1.1.cmml" xref="Sx3.SSx3.SSS0.Px2.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.SSS0.Px2.p1.5.m5.1c">\times</annotation></semantics></math>input sequence length) for training, while the training FLOPs of <span id="Sx3.SSx3.SSS0.Px2.p1.7.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> is much smaller and negligible compared to FL.
For both <span id="Sx3.SSx3.SSS0.Px2.p1.7.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> and FL, inference requires FLOPs computed by (2<math id="Sx3.SSx3.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx3.SSS0.Px2.p1.6.m6.1a"><mo id="Sx3.SSx3.SSS0.Px2.p1.6.m6.1.1" xref="Sx3.SSx3.SSS0.Px2.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.SSS0.Px2.p1.6.m6.1b"><times id="Sx3.SSx3.SSS0.Px2.p1.6.m6.1.1.cmml" xref="Sx3.SSx3.SSS0.Px2.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.SSS0.Px2.p1.6.m6.1c">\times</annotation></semantics></math>model parameters<math id="Sx3.SSx3.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx3.SSS0.Px2.p1.7.m7.1a"><mo id="Sx3.SSx3.SSS0.Px2.p1.7.m7.1.1" xref="Sx3.SSx3.SSS0.Px2.p1.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.SSS0.Px2.p1.7.m7.1b"><times id="Sx3.SSx3.SSS0.Px2.p1.7.m7.1.1.cmml" xref="Sx3.SSx3.SSS0.Px2.p1.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.SSS0.Px2.p1.7.m7.1c">\times</annotation></semantics></math>input sequence length), in the setting where the key and value vectors for attention computation are cached.
Compared to the acceptable computational and storage costs, the RAM on the modern cell phones is a key bottleneck.
We believe that this bottleneck will no longer be a problem in the near future as the techniques evolve: (1) Out-of-the-box offloading inference <cite class="ltx_cite ltx_citemacro_citep">(Rajbhandari et al. <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.
(2) Trends for more RAM <cite class="ltx_cite ltx_citemacro_citep">(Patterson <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite> and tiny CLIPs <cite class="ltx_cite ltx_citemacro_citep">(Sisodia <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>.
(3) Inference with quantization methods <cite class="ltx_cite ltx_citemacro_citep">(Gholami et al. <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="Sx3.SSx3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Compatibility</h4>

<div id="Sx3.SSx3.SSS0.Px3.p1" class="ltx_para">
<p id="Sx3.SSx3.SSS0.Px3.p1.1" class="ltx_p">Apart from image classification, many different vision tasks are compatible with <span id="Sx3.SSx3.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span>, such as object detection <cite class="ltx_cite ltx_citemacro_citep">(Gu et al. <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, video understanding <cite class="ltx_cite ltx_citemacro_citep">(Xu et al. <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> and visual question answering <cite class="ltx_cite ltx_citemacro_citep">(Shen et al. <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>.
This means that the system cost of <span id="Sx3.SSx3.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> is shared by many tasks.
The prompt learner incurs these costs per personal task specific user subset requires.
<span id="Sx3.SSx3.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> is thus competitive in terms of economics.</p>
</div>
</section>
</section>
<section id="Sx3.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Privacy Concerns</h3>

<div id="Sx3.SSx4.p1" class="ltx_para">
<p id="Sx3.SSx4.p1.1" class="ltx_p">As we have outlined in the framework, <span id="Sx3.SSx4.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> achieves to train prompts in concert with the federated server.
Each participant user only needs to upload its local parameter update of the prompt learner rather than the raw data of images.
Such a method avoids leakage of raw images, thereby better adapting to the privacy-preserving settings of the FL.
On the other hand, the parameters of prompt learner only describes the correlation between classes and textual prompts, and do not directly contain any visual feature embeddings.
Also, the parameters of prompt learner are static (<em id="Sx3.SSx4.p1.1.2" class="ltx_emph ltx_font_italic">i.e.</em>, input-agnostic) across the training data.
This is useful when faced with a server that wants to recover the raw data from an update <cite class="ltx_cite ltx_citemacro_citep">(Zhu, Liu, and Han <a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<section id="Sx3.SSx4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Inference APIs</h4>

<div id="Sx3.SSx4.SSS0.Px1.p1" class="ltx_para">
<p id="Sx3.SSx4.SSS0.Px1.p1.1" class="ltx_p">While pre-trained CLIPs are available for download at the time of writing this paper, high-performance models in these domains are often costly to train.
For example, the CLIP model trained on 400 million labeled images.
The training process took 30 days across 592 V100 GPUs <cite class="ltx_cite ltx_citemacro_citep">(Radford et al. <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>.
This would have cost million dollars to train on AWS on-demand instances.
The value of these models and their exposure over publicly-accessible APIs make us rethink the framework of <span id="Sx3.SSx4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span>.
As illustrated in Figure <a href="#Sx2.F1" title="Figure 1 ‣ Global Aggregation ‣ Federated Learning ‣ Preliminaries ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we hypothesize that the model APIs typically return low-dimensional outputs like confidence scores or logits, so information leakage is significantly reduced <cite class="ltx_cite ltx_citemacro_citep">(Dziedzic et al. <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>.
In such a case, the prompt learner can still be trained normally, because the CLIP backbone is kept frozen during the training process.
The difference is that users need to make queries to the model APIs with their private images.
Some lightweight secure inference techniques like <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> can be used in the framework to protect privacy.</p>
</div>
<figure id="Sx3.F2" class="ltx_figure"><img src="/html/2208.11625/assets/x2.png" id="Sx3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="137" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="Sx3.F2.3.1" class="ltx_text ltx_font_bold">Performance of <span id="Sx3.F2.3.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> with different class distribution.</span> Bars represent accuracy and lines indicate F-1 score. We range the class distributed on each client from entirely disparate to 10%, 20% and 50% number of classes repeated on more than one client. Compared with the collapse of existing framework in  <a href="#Sx3.T2" title="Table 2 ‣ Communication ‣ System Feasibility ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the performance of <span id="Sx3.F2.4.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> remains stable and competitive. Further more, 50% overlapping of classes shows slightly improvement across majority of datasets and backbone.</figcaption>
</figure>
<figure id="Sx3.F3" class="ltx_figure"><img src="/html/2208.11625/assets/x3.png" id="Sx3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="391" height="353" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="Sx3.F3.3.1" class="ltx_text ltx_font_bold">Performance of <span id="Sx3.F3.3.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> with different clients and shots.</span> The overall performance enhanced as the number of shots increasing. However, as the classes on each client become sufficient, the performance of clients with different clients reach similar optimal results, which on the other hand reveals that clients number do not affect the performance of <span id="Sx3.F3.4.2" class="ltx_text ltx_font_smallcaps">PromptFL</span>.</figcaption>
</figure>
<figure id="Sx3.F4" class="ltx_figure"><img src="/html/2208.11625/assets/x4.png" id="Sx3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="Sx3.F4.8.1" class="ltx_text ltx_font_bold">Comparison of computation and communication cost of <span id="Sx3.F4.8.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> and Finetuning FL.</span> We measure the communication cost by the size of uploaded data per round, and observe that finetuning FL takes up to 110 times of cost more than <span id="Sx3.F4.9.2" class="ltx_text ltx_font_smallcaps">PromptFL</span>. Furthermore, finetuning and training from scratch take 2 to 3 times of round more than <span id="Sx3.F4.10.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> for training, which exacerbate the communication expenses. We also utilize GPU memory usage, training GPU time and training data usage to evaluate the computational cost. Training GPU time is calculated by the time of training 50 epoch and training data usage is reported by training food101, which we can observe that finetuning require 250<math id="Sx3.F4.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.F4.2.m1.1b"><mo id="Sx3.F4.2.m1.1.1" xref="Sx3.F4.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.F4.2.m1.1c"><times id="Sx3.F4.2.m1.1.1.cmml" xref="Sx3.F4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.F4.2.m1.1d">\times</annotation></semantics></math> more than <span id="Sx3.F4.11.4" class="ltx_text ltx_font_smallcaps">PromptFL</span>. We can see that <span id="Sx3.F4.12.5" class="ltx_text ltx_font_smallcaps">PromptFL</span> surpasses the existing framework in the entire aspects of communication and computation efficiency.</figcaption>
</figure>
</section>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiments</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">Our experiments aim to answer the following research questions that are important for the practical deployment of FL methods, while also contributing to our understanding of the <span id="Sx4.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> paradigm.</p>
<ul id="Sx4.I2" class="ltx_itemize">
<li id="Sx4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx4.I2.i1.p1" class="ltx_para">
<p id="Sx4.I2.i1.p1.1" class="ltx_p">Is <span id="Sx4.I2.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> able to train a competitive performance in FL as compared to which have been the de-facto method on image classification tasks?</p>
</div>
</li>
<li id="Sx4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx4.I2.i2.p1" class="ltx_para">
<p id="Sx4.I2.i2.p1.1" class="ltx_p">Is <span id="Sx4.I2.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> capable of handling heterogeneous data distributions (<em id="Sx4.I2.i2.p1.1.2" class="ltx_emph ltx_font_italic">a.k.a.</em> non-IID settings) across clients?</p>
</div>
</li>
<li id="Sx4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx4.I2.i3.p1" class="ltx_para">
<p id="Sx4.I2.i3.p1.1" class="ltx_p">Is <span id="Sx4.I2.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> competitive with the de-facto method in terms of computational communication overhead?</p>
</div>
</li>
<li id="Sx4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx4.I2.i4.p1" class="ltx_para">
<p id="Sx4.I2.i4.p1.1" class="ltx_p">What is the difference between <span id="Sx4.I2.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> and the fine-tuning of visual pre-trained models in FL?</p>
</div>
</li>
<li id="Sx4.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx4.I2.i5.p1" class="ltx_para">
<p id="Sx4.I2.i5.p1.1" class="ltx_p">What practical tips help the service provider and participants deploy <span id="Sx4.I2.i5.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> in FL?</p>
</div>
</li>
</ul>
</div>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Experimental Setup</h3>

<section id="Sx4.SSx1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets</h4>

<div id="Sx4.SSx1.SSS0.Px1.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px1.p1.1" class="ltx_p">We select a representative collection of recognition datasets used in CLIP as our benchmarks.
<span id="Sx4.SSx1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">General Objects:</span> Caltech101 <cite class="ltx_cite ltx_citemacro_citep">(Fei-Fei, Fergus, and Perona <a href="#bib.bib8" title="" class="ltx_ref">2004</a>)</cite> for general object detection. <span id="Sx4.SSx1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_bold">Fine-grained Categories:</span> Flowers102 <cite class="ltx_cite ltx_citemacro_citep">(Nilsback and Zisserman <a href="#bib.bib28" title="" class="ltx_ref">2008</a>)</cite>, OxfordPets <cite class="ltx_cite ltx_citemacro_citep">(Parkhi et al. <a href="#bib.bib30" title="" class="ltx_ref">2012</a>)</cite> and Food101 <cite class="ltx_cite ltx_citemacro_citep">(Bossard, Guillaumin, and Gool <a href="#bib.bib1" title="" class="ltx_ref">2014</a>)</cite> for fine-grained classification from diversified categories.</p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>

<div id="Sx4.SSx1.SSS0.Px2.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px2.p1.1" class="ltx_p">As compared to our proposed <span id="Sx4.SSx1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span>, we choose current representative framework in FL, FedAVG, by updating and averaging the model weights collaboratively among server and clients.
We compare both training from the scratch and fine-tuning with pretrained models as our baseline method.
We select the most prevailing models, Vit<span id="Sx4.SSx1.SSS0.Px2.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline"> </span>b16 and Retnet50, as our backbone in both our image encoder of <span id="Sx4.SSx1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> and the corresponding backbone in the baseline method.</p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fine-tuning <em id="Sx4.SSx1.SSS0.Px3.1.1" class="ltx_emph ltx_font_italic">vs.</em> Prompting</h4>

<div id="Sx4.SSx1.SSS0.Px3.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px3.p1.1" class="ltx_p">How does the prompting differ from the existing adaptation method in FL?
Currently in vision, the standard adaptation method is fine-tuning.
Therefore we consider fine-tuning as the de-facto way of adapting visual pre-trained models in FL.
Fine-tuning is highly flexible in its usage: it can adapt the pre-trained models to new input domains or new tasks with different output semantics.
Yet it also requires some level of access to the pre-trained models: often entire parameters.
Unlike fine-tuning, prompting adapts the inputs to a pre-trained model by modifying the model’s inputs.
This opens up unique applications: the input-space adaptation puts control in the hands of the FL user; FL users only need to find the prompts, they don’t need to control the pre-trained model itself while training and testing.
In this way, FL users can provide adapted images and prompts to an online API that can only operate on their inputs.
On the other hand, fine-tuning is typically conditioned on inputs. Its update also directly contains some embeddings of visual feature information.
In contrast, the prompts we explore in this paper are input-agnostic across the training data.
So the prompting can prevent leaking of user’s private information from FL update to a certain extent.</p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CLIP <span id="Sx4.SSx1.SSS0.Px4.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span>
</h4>

<div id="Sx4.SSx1.SSS0.Px4.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px4.p1.5" class="ltx_p">For CLIP, an image-language model, <span id="Sx4.SSx1.SSS0.Px4.p1.5.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> organizes users to collaboratively learn prompts as the CLIP’s output transformation function.
Given a frozen pre-trained CLIP <math id="Sx4.SSx1.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Sx4.SSx1.SSS0.Px4.p1.1.m1.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.1.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.1.m1.1b"><ci id="Sx4.SSx1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.1.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.1.m1.1c">\mathcal{F}</annotation></semantics></math> and a task dataset <math id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{D}\{(\textbf{x}_{m},\textbf{y}_{m})\}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1a"><mrow id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.cmml"><mi id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.3.cmml">𝐃</mi><mo lspace="0em" rspace="0em" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.2.cmml">​</mo><mrow id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.2.cmml">{</mo><mrow id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.3.cmml"><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.3" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.3.cmml">(</mo><msub id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.2a.cmml">x</mtext><mi id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.3.cmml">m</mi></msub><mo id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.4" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.3.cmml">,</mo><msub id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.2" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.2a.cmml">y</mtext><mi id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.3" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.3.cmml">m</mi></msub><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.5" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.3.cmml">)</mo></mrow><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1b"><apply id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1"><times id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.2"></times><ci id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.3">𝐃</ci><set id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1"><interval closure="open" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2"><apply id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.2a.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.2">x</mtext></ci><ci id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.1.1.3">𝑚</ci></apply><apply id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.2a.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.2">y</mtext></ci><ci id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.2.m2.1.1.1.1.1.2.2.3">𝑚</ci></apply></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.2.m2.1c">\mathbf{D}\{(\textbf{x}_{m},\textbf{y}_{m})\}</annotation></semantics></math> across clients, the target of <span id="Sx4.SSx1.SSS0.Px4.p1.5.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> is to learn a single, static, task-specific prompting <math id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="f_{\text{prompt}}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1a"><msub id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.cmml"><mi id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.2.cmml">f</mi><mtext id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.3a.cmml">prompt</mtext></msub><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1b"><apply id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.2">𝑓</ci><ci id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.3a.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.3"><mtext mathsize="70%" id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.3.m3.1.1.3">prompt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.3.m3.1c">f_{\text{prompt}}</annotation></semantics></math> on class space parameterized by [prompt vectors].
Image classes are represented by labels (<em id="Sx4.SSx1.SSS0.Px4.p1.5.3" class="ltx_emph ltx_font_italic">e.g.</em>, ‘panda’) which are then prompted (<em id="Sx4.SSx1.SSS0.Px4.p1.5.4" class="ltx_emph ltx_font_italic">i.e.</em>, ‘[prompt vectors][panda]’) to specify the context of the user’s task.
We follow CLIP’s protocol and compute the cosine similarity of the embeddings for each class, normalized to a probability distribution via softmax.
The class with the highest probability is selected as the model output.
The prompting is added to the class space to form a prompted output <math id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="\textbf{y}+v_{f}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1a"><mrow id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.2a.cmml">y</mtext><mo id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.1.cmml">+</mo><msub id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.cmml"><mi id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.2" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.2.cmml">v</mi><mi id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.3" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.3.cmml">f</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1b"><apply id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1"><plus id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.1"></plus><ci id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.2a.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.2">y</mtext></ci><apply id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.2">𝑣</ci><ci id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.4.m4.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.4.m4.1c">\textbf{y}+v_{f}</annotation></semantics></math>.
During training, <span id="Sx4.SSx1.SSS0.Px4.p1.5.5" class="ltx_text ltx_font_smallcaps">PromptFL</span> will maximize the likelihood of the correct label <span id="Sx4.SSx1.SSS0.Px4.p1.5.6" class="ltx_text ltx_markedasmath ltx_font_bold">y</span>,</p>
<table id="Sx5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="Sx4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Sx4.E2.m1.3" class="ltx_Math" alttext="\displaystyle\max_{f_{\text{prompt}}}\text{p}_{\mathcal{F};f_{\text{prompt}}}(\textbf{y}+v_{f}|\textbf{x})," display="inline"><semantics id="Sx4.E2.m1.3a"><mrow id="Sx4.E2.m1.3.3.1" xref="Sx4.E2.m1.3.3.1.1.cmml"><mrow id="Sx4.E2.m1.3.3.1.1" xref="Sx4.E2.m1.3.3.1.1.cmml"><mrow id="Sx4.E2.m1.3.3.1.1.3" xref="Sx4.E2.m1.3.3.1.1.3.cmml"><munder id="Sx4.E2.m1.3.3.1.1.3.1" xref="Sx4.E2.m1.3.3.1.1.3.1.cmml"><mi id="Sx4.E2.m1.3.3.1.1.3.1.2" xref="Sx4.E2.m1.3.3.1.1.3.1.2.cmml">max</mi><msub id="Sx4.E2.m1.3.3.1.1.3.1.3" xref="Sx4.E2.m1.3.3.1.1.3.1.3.cmml"><mi id="Sx4.E2.m1.3.3.1.1.3.1.3.2" xref="Sx4.E2.m1.3.3.1.1.3.1.3.2.cmml">f</mi><mtext id="Sx4.E2.m1.3.3.1.1.3.1.3.3" xref="Sx4.E2.m1.3.3.1.1.3.1.3.3a.cmml">prompt</mtext></msub></munder><mo lspace="0.167em" id="Sx4.E2.m1.3.3.1.1.3a" xref="Sx4.E2.m1.3.3.1.1.3.cmml">⁡</mo><msub id="Sx4.E2.m1.3.3.1.1.3.2" xref="Sx4.E2.m1.3.3.1.1.3.2.cmml"><mtext id="Sx4.E2.m1.3.3.1.1.3.2.2" xref="Sx4.E2.m1.3.3.1.1.3.2.2a.cmml">p</mtext><mrow id="Sx4.E2.m1.2.2.2.2" xref="Sx4.E2.m1.2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx4.E2.m1.1.1.1.1" xref="Sx4.E2.m1.1.1.1.1.cmml">ℱ</mi><mo id="Sx4.E2.m1.2.2.2.2.2" xref="Sx4.E2.m1.2.2.2.3.cmml">;</mo><msub id="Sx4.E2.m1.2.2.2.2.1" xref="Sx4.E2.m1.2.2.2.2.1.cmml"><mi id="Sx4.E2.m1.2.2.2.2.1.2" xref="Sx4.E2.m1.2.2.2.2.1.2.cmml">f</mi><mtext id="Sx4.E2.m1.2.2.2.2.1.3" xref="Sx4.E2.m1.2.2.2.2.1.3a.cmml">prompt</mtext></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="Sx4.E2.m1.3.3.1.1.2" xref="Sx4.E2.m1.3.3.1.1.2.cmml">​</mo><mrow id="Sx4.E2.m1.3.3.1.1.1.1" xref="Sx4.E2.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx4.E2.m1.3.3.1.1.1.1.2" xref="Sx4.E2.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="Sx4.E2.m1.3.3.1.1.1.1.1" xref="Sx4.E2.m1.3.3.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.E2.m1.3.3.1.1.1.1.1.2" xref="Sx4.E2.m1.3.3.1.1.1.1.1.2a.cmml">y</mtext><mo id="Sx4.E2.m1.3.3.1.1.1.1.1.1" xref="Sx4.E2.m1.3.3.1.1.1.1.1.1.cmml">+</mo><mrow id="Sx4.E2.m1.3.3.1.1.1.1.1.3" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.cmml"><msub id="Sx4.E2.m1.3.3.1.1.1.1.1.3.2" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.cmml"><mi id="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.2" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml">v</mi><mi id="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.3" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.3.cmml">f</mi></msub><mo fence="false" id="Sx4.E2.m1.3.3.1.1.1.1.1.3.1" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.1.cmml">|</mo><mtext class="ltx_mathvariant_bold" id="Sx4.E2.m1.3.3.1.1.1.1.1.3.3" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.3a.cmml">x</mtext></mrow></mrow><mo stretchy="false" id="Sx4.E2.m1.3.3.1.1.1.1.3" xref="Sx4.E2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Sx4.E2.m1.3.3.1.2" xref="Sx4.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.E2.m1.3b"><apply id="Sx4.E2.m1.3.3.1.1.cmml" xref="Sx4.E2.m1.3.3.1"><times id="Sx4.E2.m1.3.3.1.1.2.cmml" xref="Sx4.E2.m1.3.3.1.1.2"></times><apply id="Sx4.E2.m1.3.3.1.1.3.cmml" xref="Sx4.E2.m1.3.3.1.1.3"><apply id="Sx4.E2.m1.3.3.1.1.3.1.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="Sx4.E2.m1.3.3.1.1.3.1.1.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1">subscript</csymbol><max id="Sx4.E2.m1.3.3.1.1.3.1.2.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1.2"></max><apply id="Sx4.E2.m1.3.3.1.1.3.1.3.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1.3"><csymbol cd="ambiguous" id="Sx4.E2.m1.3.3.1.1.3.1.3.1.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1.3">subscript</csymbol><ci id="Sx4.E2.m1.3.3.1.1.3.1.3.2.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1.3.2">𝑓</ci><ci id="Sx4.E2.m1.3.3.1.1.3.1.3.3a.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1.3.3"><mtext mathsize="50%" id="Sx4.E2.m1.3.3.1.1.3.1.3.3.cmml" xref="Sx4.E2.m1.3.3.1.1.3.1.3.3">prompt</mtext></ci></apply></apply><apply id="Sx4.E2.m1.3.3.1.1.3.2.cmml" xref="Sx4.E2.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="Sx4.E2.m1.3.3.1.1.3.2.1.cmml" xref="Sx4.E2.m1.3.3.1.1.3.2">subscript</csymbol><ci id="Sx4.E2.m1.3.3.1.1.3.2.2a.cmml" xref="Sx4.E2.m1.3.3.1.1.3.2.2"><mtext id="Sx4.E2.m1.3.3.1.1.3.2.2.cmml" xref="Sx4.E2.m1.3.3.1.1.3.2.2">p</mtext></ci><list id="Sx4.E2.m1.2.2.2.3.cmml" xref="Sx4.E2.m1.2.2.2.2"><ci id="Sx4.E2.m1.1.1.1.1.cmml" xref="Sx4.E2.m1.1.1.1.1">ℱ</ci><apply id="Sx4.E2.m1.2.2.2.2.1.cmml" xref="Sx4.E2.m1.2.2.2.2.1"><csymbol cd="ambiguous" id="Sx4.E2.m1.2.2.2.2.1.1.cmml" xref="Sx4.E2.m1.2.2.2.2.1">subscript</csymbol><ci id="Sx4.E2.m1.2.2.2.2.1.2.cmml" xref="Sx4.E2.m1.2.2.2.2.1.2">𝑓</ci><ci id="Sx4.E2.m1.2.2.2.2.1.3a.cmml" xref="Sx4.E2.m1.2.2.2.2.1.3"><mtext mathsize="50%" id="Sx4.E2.m1.2.2.2.2.1.3.cmml" xref="Sx4.E2.m1.2.2.2.2.1.3">prompt</mtext></ci></apply></list></apply></apply><apply id="Sx4.E2.m1.3.3.1.1.1.1.1.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1"><plus id="Sx4.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.1"></plus><ci id="Sx4.E2.m1.3.3.1.1.1.1.1.2a.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx4.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.2">y</mtext></ci><apply id="Sx4.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3"><csymbol cd="latexml" id="Sx4.E2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.1">conditional</csymbol><apply id="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.2">subscript</csymbol><ci id="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.2">𝑣</ci><ci id="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.3.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.2.3">𝑓</ci></apply><ci id="Sx4.E2.m1.3.3.1.1.1.1.1.3.3a.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.3"><mtext class="ltx_mathvariant_bold" id="Sx4.E2.m1.3.3.1.1.1.1.1.3.3.cmml" xref="Sx4.E2.m1.3.3.1.1.1.1.1.3.3">x</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.E2.m1.3c">\displaystyle\max_{f_{\text{prompt}}}\text{p}_{\mathcal{F};f_{\text{prompt}}}(\textbf{y}+v_{f}|\textbf{x}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="Sx4.SSx1.SSS0.Px4.p1.9" class="ltx_p">while the gradient updates are applied only to the [prompt vectors] <math id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1" class="ltx_Math" alttext="v_{f}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1a"><msub id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.cmml"><mi id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.2.cmml">v</mi><mi id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1b"><apply id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.2">𝑣</ci><ci id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.6.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.6.m1.1c">v_{f}</annotation></semantics></math> and the CLIP parameters <math id="Sx4.SSx1.SSS0.Px4.p1.7.m2.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.7.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Sx4.SSx1.SSS0.Px4.p1.7.m2.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.7.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.7.m2.1b"><ci id="Sx4.SSx1.SSS0.Px4.p1.7.m2.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.7.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.7.m2.1c">\mathcal{F}</annotation></semantics></math> remain frozen.
During validation, the optimized prompt is added to all test-time classes, <math id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1" class="ltx_Math" alttext="\mathbf{D}_{\text{test}}\{(\textbf{x}_{m},\textbf{y}_{m}+v_{f})\}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1a"><mrow id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.cmml"><msub id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.cmml"><mi id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.2.cmml">𝐃</mi><mtext id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.3a.cmml">test</mtext></msub><mo lspace="0em" rspace="0em" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.2.cmml">​</mo><mrow id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.2.cmml"><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.2.cmml">{</mo><mrow id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.3.cmml"><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.3.cmml">(</mo><msub id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.2a.cmml">x</mtext><mi id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.3.cmml">m</mi></msub><mo id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.4" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.3.cmml">,</mo><mrow id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.cmml"><msub id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.2a.cmml">y</mtext><mi id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.3.cmml">m</mi></msub><mo id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.1" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.1.cmml">+</mo><msub id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.cmml"><mi id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.2" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.2.cmml">v</mi><mi id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.3.cmml">f</mi></msub></mrow><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.5" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.3.cmml">)</mo></mrow><mo stretchy="false" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.3" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1b"><apply id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1"><times id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.2"></times><apply id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.2">𝐃</ci><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.3a.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.3"><mtext mathsize="70%" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.3.3">test</mtext></ci></apply><set id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1"><interval closure="open" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2"><apply id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.2a.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.2">x</mtext></ci><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.1.1.3">𝑚</ci></apply><apply id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2"><plus id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.1"></plus><apply id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.2a.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.2">y</mtext></ci><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.2.3">𝑚</ci></apply><apply id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3">subscript</csymbol><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.2.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.2">𝑣</ci><ci id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.3.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.8.m3.1.1.1.1.1.2.2.3.3">𝑓</ci></apply></apply></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.8.m3.1c">\mathbf{D}_{\text{test}}\{(\textbf{x}_{m},\textbf{y}_{m}+v_{f})\}</annotation></semantics></math>, which will be then processed through the frozen <math id="Sx4.SSx1.SSS0.Px4.p1.9.m4.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="Sx4.SSx1.SSS0.Px4.p1.9.m4.1a"><mi class="ltx_font_mathcaligraphic" id="Sx4.SSx1.SSS0.Px4.p1.9.m4.1.1" xref="Sx4.SSx1.SSS0.Px4.p1.9.m4.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px4.p1.9.m4.1b"><ci id="Sx4.SSx1.SSS0.Px4.p1.9.m4.1.1.cmml" xref="Sx4.SSx1.SSS0.Px4.p1.9.m4.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px4.p1.9.m4.1c">\mathcal{F}</annotation></semantics></math>.</p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training Details</h4>

<div id="Sx4.SSx1.SSS0.Px5.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px5.p1.1" class="ltx_p">To validate the effectiveness of our method, we compare the performance of <span id="Sx4.SSx1.SSS0.Px5.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> with existing framework by 1)training the collaborative model from the scratch and 2)fine-tuning the full model with pretrained weights. We evaluate the performance across four representative dataset used in CLIP for both general objects and fine-grained classification. We report the performance with two representative and influential backbone, Resnet50(38.3M parameters) and Vit<span id="Sx4.SSx1.SSS0.Px5.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline"> </span>b16(86.6M parameters). For the evaluation metrics, we select three aspects to assess the performance of each method, 1)representative Top-1 accuracy on the test set, 2)F1 score to measure the weighted and unified average of precision and recall, which is more useful especially on unbalanced class distribution, 3)as well as the computational and communication cost reported in Fig. <a href="#Sx3.F4" title="Figure 4 ‣ Inference APIs ‣ Privacy Concerns ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We presuming that higher result on accuracy and F-1 score as well as lower result on computation latency will lead to better a framework, detailed comparison results show the superior if <span id="Sx4.SSx1.SSS0.Px5.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> in Tab. <a href="#Sx3.T2" title="Table 2 ‣ Communication ‣ System Feasibility ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="Sx4.SSx1.SSS0.Px5.p2" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px5.p2.1" class="ltx_p">All experiments are conducted with Pytorch on GeForce RTX 3090 GPU. Training is performed with SGD with 0.001 learning rate. Tab. <a href="#Sx3.T2" title="Table 2 ‣ Communication ‣ System Feasibility ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> measures the overall performance of <span id="Sx4.SSx1.SSS0.Px5.p2.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> against existing framework from the perspective of two data distribution settings. For the iid setting, each client shares the same classes, while for the extreme non-iid setting, each client owns the independent and non-overlapping classes. We can see that from Tab. <a href="#Sx3.T2" title="Table 2 ‣ Communication ‣ System Feasibility ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <span id="Sx4.SSx1.SSS0.Px5.p2.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> obtains superior results with similar or better accuracy and F1 value, but with only 0.01% <math id="Sx4.SSx1.SSS0.Px5.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx4.SSx1.SSS0.Px5.p2.1.m1.1a"><mo id="Sx4.SSx1.SSS0.Px5.p2.1.m1.1.1" xref="Sx4.SSx1.SSS0.Px5.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px5.p2.1.m1.1b"><csymbol cd="latexml" id="Sx4.SSx1.SSS0.Px5.p2.1.m1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px5.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px5.p2.1.m1.1c">\sim</annotation></semantics></math> 0.1% learnable parameters with the iid setting. Further more, with the non-iid setting <span id="Sx4.SSx1.SSS0.Px5.p2.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> achieves competitive performance on both accuracy and efficiency against existing framework. Superior outcome on both settings manifest the advantage of our proposed <span id="Sx4.SSx1.SSS0.Px5.p2.1.4" class="ltx_text ltx_font_smallcaps">PromptFL</span>.</p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Distribution Analysis</h4>

<div id="Sx4.SSx1.SSS0.Px6.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px6.p1.3" class="ltx_p">After obtaining the decent performance in both extreme iid and non-iid setting, we hope to further testify the stability of <span id="Sx4.SSx1.SSS0.Px6.p1.3.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> and figure out the impact of different data distribution on clients to the performance of <span id="Sx4.SSx1.SSS0.Px6.p1.3.2" class="ltx_text ltx_font_smallcaps">PromptFL</span>. To observe the intermediate status, we select <math id="Sx4.SSx1.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="Sx4.SSx1.SSS0.Px6.p1.1.m1.1a"><mi id="Sx4.SSx1.SSS0.Px6.p1.1.m1.1.1" xref="Sx4.SSx1.SSS0.Px6.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px6.p1.1.m1.1b"><ci id="Sx4.SSx1.SSS0.Px6.p1.1.m1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px6.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px6.p1.1.m1.1c">p</annotation></semantics></math>% overlapped ratio of classes from 0% to 10%, 20% and 50%, which means that <math id="Sx4.SSx1.SSS0.Px6.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="Sx4.SSx1.SSS0.Px6.p1.2.m2.1a"><mi id="Sx4.SSx1.SSS0.Px6.p1.2.m2.1.1" xref="Sx4.SSx1.SSS0.Px6.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px6.p1.2.m2.1b"><ci id="Sx4.SSx1.SSS0.Px6.p1.2.m2.1.1.cmml" xref="Sx4.SSx1.SSS0.Px6.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px6.p1.2.m2.1c">p</annotation></semantics></math>% of classes will appear on more than one client and the remaining <math id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1" class="ltx_Math" alttext="1-p" display="inline"><semantics id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1a"><mrow id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.cmml"><mn id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.2" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.2.cmml">1</mn><mo id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.1" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.1.cmml">−</mo><mi id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.3" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1b"><apply id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.cmml" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1"><minus id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.1"></minus><cn type="integer" id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.2.cmml" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.2">1</cn><ci id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.3.cmml" xref="Sx4.SSx1.SSS0.Px6.p1.3.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px6.p1.3.m3.1c">1-p</annotation></semantics></math>% classes only shows on single client. Fig.  <a href="#Sx3.F2" title="Figure 2 ‣ Inference APIs ‣ Privacy Concerns ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports the accuracy and F1 with corresponding distribution. From the result, we observe that given the circumstance that the class on each client is sufficient, the distribution of class has not much impact on the performance of <span id="Sx4.SSx1.SSS0.Px6.p1.3.3" class="ltx_text ltx_font_smallcaps">PromptFL</span>, only a tiny improvement when the overlapping of classes reaches 50%. On the contrary, existing framework shows miserable stability when encountering shifted class distribution other than unified mode by observing the Tab.  <a href="#Sx3.T2" title="Table 2 ‣ Communication ‣ System Feasibility ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Impact of number of shots</h4>

<div id="Sx4.SSx1.SSS0.Px7.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px7.p1.1" class="ltx_p">Following the few-shot evaluation setting adopted in CLIP, we use 2, 4, 8, 16 shots in training <span id="Sx4.SSx1.SSS0.Px7.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> and validate the performance with corresponding test sets. Unlike the circumstance in the centralized mode where data only from a single entrance, <span id="Sx4.SSx1.SSS0.Px7.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> involves several participants. Thus we redeclare that the number of shots for the FL mode implies the overall shot containing the entire participants, which is more practical and accord with the class unbalance scenarios in the real-world. From the result in  <a href="#Sx3.F3" title="Figure 3 ‣ Inference APIs ‣ Privacy Concerns ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that as the number of training examples per class increases, the performance of <span id="Sx4.SSx1.SSS0.Px7.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> enhanced. Furthermore, for the setting of adequate clients with sufficient class number on each client, the accuracy for each setting reveals rather steady.</p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px8" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison with different clients</h4>

<div id="Sx4.SSx1.SSS0.Px8.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px8.p1.1" class="ltx_p">Next, to eliminate the possible impact caused by different clients, we further study the performance of <span id="Sx4.SSx1.SSS0.Px8.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> with different clients from 16 to 32 to 64, with the iid mode that each client owns random set of classes. Also, to avoid that the collapse of performance due to deficiency of class on each client, especially for the case with large clients number, we also range the number of shots from 2 to 4 to 8 to 16. We observe that for different number of clients, performance will reach similar optimum for as the classes on each client is sufficient. For example, in caltech101, all settings achieve around 89% with 16 shots.</p>
</div>
</section>
<section id="Sx4.SSx1.SSS0.Px9" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Computation and Communication Cost Analysis</h4>

<div id="Sx4.SSx1.SSS0.Px9.p1" class="ltx_para">
<p id="Sx4.SSx1.SSS0.Px9.p1.2" class="ltx_p">We also analyse the efficiency of <span id="Sx4.SSx1.SSS0.Px9.p1.2.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> with regard to the computation and communication cost during training. We measure the communication cost by the size of uploaded data per round, and the total round to be transmitted. For the computation cost, we calculate the GPU memory utilization and training GPU time for given steps. Fig.  <a href="#Sx3.F4" title="Figure 4 ‣ Inference APIs ‣ Privacy Concerns ‣ Prompt-Based Federated Learning ‣ PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models — Federated Learning in Age of Foundation Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the comparison between existing finetuning framework and our proposed <span id="Sx4.SSx1.SSS0.Px9.p1.2.2" class="ltx_text ltx_font_smallcaps">PromptFL</span>. We observe that <span id="Sx4.SSx1.SSS0.Px9.p1.2.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> can save at most <math id="Sx4.SSx1.SSS0.Px9.p1.1.m1.1" class="ltx_Math" alttext="110" display="inline"><semantics id="Sx4.SSx1.SSS0.Px9.p1.1.m1.1a"><mn id="Sx4.SSx1.SSS0.Px9.p1.1.m1.1.1" xref="Sx4.SSx1.SSS0.Px9.p1.1.m1.1.1.cmml">110</mn><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px9.p1.1.m1.1b"><cn type="integer" id="Sx4.SSx1.SSS0.Px9.p1.1.m1.1.1.cmml" xref="Sx4.SSx1.SSS0.Px9.p1.1.m1.1.1">110</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px9.p1.1.m1.1c">110</annotation></semantics></math> times communication cost per round compared to existing prevailing method, let alone that <span id="Sx4.SSx1.SSS0.Px9.p1.2.4" class="ltx_text ltx_font_smallcaps">PromptFL</span> takes half of rounds to reach convergence, which makes a wider disparity in communication cost between them. As for the computation cost, we report the comparison of GPU time as in the same given steps, where <span id="Sx4.SSx1.SSS0.Px9.p1.2.5" class="ltx_text ltx_font_smallcaps">PromptFL</span> remains outperform existing framework around <math id="Sx4.SSx1.SSS0.Px9.p1.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="Sx4.SSx1.SSS0.Px9.p1.2.m2.1a"><mn id="Sx4.SSx1.SSS0.Px9.p1.2.m2.1.1" xref="Sx4.SSx1.SSS0.Px9.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSS0.Px9.p1.2.m2.1b"><cn type="integer" id="Sx4.SSx1.SSS0.Px9.p1.2.m2.1.1.cmml" xref="Sx4.SSx1.SSS0.Px9.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSS0.Px9.p1.2.m2.1c">3</annotation></semantics></math> times. Further more, there is huge advantage that <span id="Sx4.SSx1.SSS0.Px9.p1.2.6" class="ltx_text ltx_font_smallcaps">PromptFL</span> consumes far less GPU memory during training, which can alleviate the system burden in practical.</p>
</div>
</section>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">Overall, there are many unknowns about <span id="Sx5.p1.1.1" class="ltx_text ltx_font_smallcaps">PromptFL</span> and this paper sets out to investigate its feasibility.
In summary: (1) We demonstrate the system feasibility of <span id="Sx5.p1.1.2" class="ltx_text ltx_font_smallcaps">PromptFL</span> on modern hardware, in terms of overhead in communication, training, and inference.
(2) We show that <span id="Sx5.p1.1.3" class="ltx_text ltx_font_smallcaps">PromptFL</span> keeps data on each device private, aiming to learn global prompts updated only by communicating gradients rather than the data itself, and thus not less private than FL.
(3) We implement a proof-of-concept in the framework, spanning a range of popular image classification tasks. We find <span id="Sx5.p1.1.4" class="ltx_text ltx_font_smallcaps">PromptFL</span> to be competitive with strong FL baselines.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bossard, Guillaumin, and Gool (2014)</span>
<span class="ltx_bibblock">
Bossard, L.; Guillaumin, M.; and Gool, L. V. 2014.

</span>
<span class="ltx_bibblock">Food-101–mining discriminative components with random forests.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.;
Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>,
33: 1877–1901.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2022)</span>
<span class="ltx_bibblock">
Deng, M.; Wang, J.; Hsieh, C.-P.; Wang, Y.; Guo, H.; Shu, T.; Song, M.; Xing,
E. P.; and Hu, Z. 2022.

</span>
<span class="ltx_bibblock">RLPrompt: Optimizing Discrete Text Prompts With Reinforcement
Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12548</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et al. (2020)</span>
<span class="ltx_bibblock">
Dodge, J.; Ilharco, G.; Schwartz, R.; Farhadi, A.; Hajishirzi, H.; and Smith,
N. 2020.

</span>
<span class="ltx_bibblock">Fine-tuning pretrained language models: Weight initializations, data
orders, and early stopping.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.06305</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.;
Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al.
2021.

</span>
<span class="ltx_bibblock">An Image is Worth 16x16 Words: Transformers for Image Recognition at
Scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dziedzic et al. (2022)</span>
<span class="ltx_bibblock">
Dziedzic, A.; Dhawan, N.; Kaleem, M. A.; Guan, J.; and Papernot, N. 2022.

</span>
<span class="ltx_bibblock">On the Difficulty of Defending Self-Supervised Learning against Model
Extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine
Learning (ICML)</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">E. Freedman (2021)</span>
<span class="ltx_bibblock">
E. Freedman, A. 2021.

</span>
<span class="ltx_bibblock">Apple a15 bionic powers iphone 13 and ipad mini.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei-Fei, Fergus, and Perona (2004)</span>
<span class="ltx_bibblock">
Fei-Fei, L.; Fergus, R.; and Perona, P. 2004.

</span>
<span class="ltx_bibblock">Learning generative visual models from few training examples: An
incremental bayesian approach tested on 101 object categories.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW)</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fürst et al. (2021)</span>
<span class="ltx_bibblock">
Fürst, A.; Rumetshofer, E.; Tran, V.; Ramsauer, H.; Tang, F.; Lehner, J.;
Kreil, D.; Kopp, M.; Klambauer, G.; Bitto-Nemling, A.; et al. 2021.

</span>
<span class="ltx_bibblock">Cloob: Modern hopfield networks with infoloob outperform clip.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.11316</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao, Fisch, and Chen (2021)</span>
<span class="ltx_bibblock">
Gao, T.; Fisch, A.; and Chen, D. 2021.

</span>
<span class="ltx_bibblock">Making Pre-trained Language Models Better Few-shot Learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gholami et al. (2021)</span>
<span class="ltx_bibblock">
Gholami, A.; Kim, S.; Dong, Z.; Yao, Z.; Mahoney, M. W.; and Keutzer, K. 2021.

</span>
<span class="ltx_bibblock">A survey of quantization methods for efficient neural network
inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.13630</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2021)</span>
<span class="ltx_bibblock">
Gu, X.; Lin, T.-Y.; Kuo, W.; and Cui, Y. 2021.

</span>
<span class="ltx_bibblock">Open-vocabulary Object Detection via Vision and Language Knowledge
Distillation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hambardzumyan, Khachatrian, and May (2021)</span>
<span class="ltx_bibblock">
Hambardzumyan, K.; Khachatrian, H.; and May, J. 2021.

</span>
<span class="ltx_bibblock">WARP: Word-level Adversarial ReProgramming.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. (2018)</span>
<span class="ltx_bibblock">
Hard, A.; Rao, K.; Mathews, R.; Ramaswamy, S.; Beaufays, F.; Augenstein, S.;
Eichner, H.; Kiddon, C.; and Ramage, D. 2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu, Jiang, and Wang (2019)</span>
<span class="ltx_bibblock">
Hu, C.; Jiang, J.; and Wang, Z. 2019.

</span>
<span class="ltx_bibblock">Decentralized federated learning: A segmented gossip approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07782</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2021)</span>
<span class="ltx_bibblock">
Jia, C.; Yang, Y.; Xia, Y.; Chen, Y.-T.; Parekh, Z.; Pham, H.; Le, Q.; Sung,
Y.-H.; Li, Z.; and Duerig, T. 2021.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with
noisy text supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine
Learning (ICML)</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2021)</span>
<span class="ltx_bibblock">
Kairouz, P.; McMahan, H. B.; Avent, B.; Bellet, A.; Bennis, M.; Bhagoji, A. N.;
Bonawitz, K.; Charles, Z.; Cormode, G.; Cummings, R.; et al. 2021.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em>,
14(1–2): 1–210.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lalitha et al. (2018)</span>
<span class="ltx_bibblock">
Lalitha, A.; Shekhar, S.; Javidi, T.; and Koushanfar, F. 2018.

</span>
<span class="ltx_bibblock">Fully decentralized federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the NeurIPS Workshop on Bayesian Deep
Learning</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester, Al-Rfou, and Constant (2021)</span>
<span class="ltx_bibblock">
Lester, B.; Al-Rfou, R.; and Constant, N. 2021.

</span>
<span class="ltx_bibblock">The Power of Scale for Parameter-Efficient Prompt Tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Li, T.; Sahu, A. K.; Zaheer, M.; Sanjabi, M.; Talwalkar, A.; and Smith, V.
2020.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, 2: 429–450.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
Li, X. L.; and Liang, P. 2021.

</span>
<span class="ltx_bibblock">Prefix-Tuning: Optimizing Continuous Prompts for Generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Li, Y.; Liang, F.; Zhao, L.; Cui, Y.; Ouyang, W.; Shao, J.; Yu, F.; and Yan, J.
2021.

</span>
<span class="ltx_bibblock">Supervision Exists Everywhere: A Data Efficient Contrastive
Language-Image Pre-training Paradigm.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2020)</span>
<span class="ltx_bibblock">
Liang, P. P.; Liu, T.; Ziyin, L.; Allen, N. B.; Auerbach, R. P.; Brent, D.;
Salakhutdinov, R.; and Morency, L.-P. 2020.

</span>
<span class="ltx_bibblock">Think locally, act globally: Federated learning with local and global
representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.01523</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig, G. 2021.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting
methods in natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.13586</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Liu, Z.; Wu, Z.; Gan, C.; Zhu, L.; and Han, S. 2020.

</span>
<span class="ltx_bibblock">Datamix: Efficient privacy-preserving edge-cloud inference.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
McMahan, B.; Moore, E.; Ramage, D.; Hampson, S.; and y Arcas, B. A. 2017.

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks from Decentralized
Data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Artificial
Intelligence and Statistics (AISTATS)</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nilsback and Zisserman (2008)</span>
<span class="ltx_bibblock">
Nilsback, M.-E.; and Zisserman, A. 2008.

</span>
<span class="ltx_bibblock">Automated flower classification over a large number of classes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Indian Conference on Computer Vision,
Graphics and Image Processing (ICVGIP)</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Dea (2021)</span>
<span class="ltx_bibblock">
O’Dea, S. 2021.

</span>
<span class="ltx_bibblock">Average global mobile and fixed broadband download &amp; upload speed
worldwide.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parkhi et al. (2012)</span>
<span class="ltx_bibblock">
Parkhi, O. M.; Vedaldi, A.; Zisserman, A.; and Jawahar, C. 2012.

</span>
<span class="ltx_bibblock">Cats and dogs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson (2022)</span>
<span class="ltx_bibblock">
Patterson, B. 2022.

</span>
<span class="ltx_bibblock">Blake’s ios device specifications grid.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2022)</span>
<span class="ltx_bibblock">
Qu, L.; Zhou, Y.; Liang, P. P.; Xia, Y.; Wang, F.; Adeli, E.; Fei-Fei, L.; and
Rubin, D. 2022.

</span>
<span class="ltx_bibblock">Rethinking architecture design for tackling data heterogeneity in
federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry,
G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine
Learning (ICML)</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et al. (2021)</span>
<span class="ltx_bibblock">
Rajbhandari, S.; Ruwase, O.; Rasley, J.; Smith, S.; and He, Y. 2021.

</span>
<span class="ltx_bibblock">Zero-infinity: Breaking the gpu memory wall for extreme scale deep
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis (HPCA)</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy et al. (2019)</span>
<span class="ltx_bibblock">
Roy, A. G.; Siddiqui, S.; Pölsterl, S.; Navab, N.; and Wachinger, C. 2019.

</span>
<span class="ltx_bibblock">Braintorrent: A peer-to-peer environment for decentralized federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.06731</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
Shen, S.; Li, L. H.; Tan, H.; Bansal, M.; Rohrbach, A.; Chang, K.-W.; Yao, Z.;
and Keutzer, K. 2021.

</span>
<span class="ltx_bibblock">How Much Can CLIP Benefit Vision-and-Language Tasks?

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. (2020)</span>
<span class="ltx_bibblock">
Shin, T.; Razeghi, Y.; Logan IV, R. L.; Wallace, E.; and Singh, S. 2020.

</span>
<span class="ltx_bibblock">AutoPrompt: Eliciting Knowledge from Language Models with
Automatically Generated Prompts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2022)</span>
<span class="ltx_bibblock">
Singh, A.; Hu, R.; Goswami, V.; Couairon, G.; Galuba, W.; Rohrbach, M.; and
Kiela, D. 2022.

</span>
<span class="ltx_bibblock">Flava: A foundational language and vision alignment model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sisodia (2021)</span>
<span class="ltx_bibblock">
Sisodia, V. 2021.

</span>
<span class="ltx_bibblock">Distillation of clip model and other experiments.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsimpoukelli et al. (2021)</span>
<span class="ltx_bibblock">
Tsimpoukelli, M.; Menick, J. L.; Cabi, S.; Eslami, S.; Vinyals, O.; and Hill,
F. 2021.

</span>
<span class="ltx_bibblock">Multimodal few-shot learning with frozen language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>,
34: 200–212.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.;
Kaiser, Ł.; and Polosukhin, I. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>,
30.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2021)</span>
<span class="ltx_bibblock">
Xu, H.; Ghosh, G.; Huang, P.-Y.; Okhonko, D.; Aghajanyan, A.; Metze, F.;
Zettlemoyer, L.; and Feichtenhofer, C. 2021.

</span>
<span class="ltx_bibblock">VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text
Understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2021)</span>
<span class="ltx_bibblock">
Yuan, L.; Chen, D.; Chen, Y.-L.; Codella, N.; Dai, X.; Gao, J.; Hu, H.; Huang,
X.; Li, B.; Li, C.; et al. 2021.

</span>
<span class="ltx_bibblock">Florence: A new foundation model for computer vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.11432</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2018)</span>
<span class="ltx_bibblock">
Zhao, Y.; Li, M.; Lai, L.; Suda, N.; Civin, D.; and Chandra, V. 2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2021)</span>
<span class="ltx_bibblock">
Zhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2021.

</span>
<span class="ltx_bibblock">Learning to prompt for vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.01134</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu, Liu, and Han (2019)</span>
<span class="ltx_bibblock">
Zhu, L.; Liu, Z.; and Han, S. 2019.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>,
32.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2208.11624" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2208.11625" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.11625">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2208.11625" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2208.11626" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 18:53:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
