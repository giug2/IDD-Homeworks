<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.03022] Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes</title><meta property="og:description" content="We introduce Boundless, a photo-realistic synthetic data generation system for enabling highly accurate object detection in dense urban streetscapes. Boundless can replace massive real-world data collection and manual …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.03022">

<!--Generated on Sat Oct  5 21:51:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id1.1.id1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="id1.1.id1.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.1" class="ltx_td ltx_align_center" style="padding-bottom:8.61108pt;">
<span id="id1.1.id1.1.1.1.1" class="ltx_tabular ltx_align_top">
<span id="id1.1.id1.1.1.1.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.1.1.1.1" class="ltx_td ltx_align_center">Mehmet Kerem Turkcan</span></span>
<span id="id1.1.id1.1.1.1.1.2" class="ltx_tr">
<span id="id1.1.id1.1.1.1.1.2.1" class="ltx_td ltx_align_center">Columbia University</span></span>
<span id="id1.1.id1.1.1.1.1.3" class="ltx_tr">
<span id="id1.1.id1.1.1.1.1.3.1" class="ltx_td ltx_align_center">New York, NY</span></span>
<span id="id1.1.id1.1.1.1.1.4" class="ltx_tr">
<span id="id1.1.id1.1.1.1.1.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.1.1.1.1.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">mkt2126@columbia.edu</span></span></span>
</span></span>
<span id="id1.1.id1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-bottom:8.61108pt;">
<span id="id1.1.id1.1.1.2.1" class="ltx_tabular ltx_align_top">
<span id="id1.1.id1.1.1.2.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.2.1.1.1" class="ltx_td ltx_align_center">Yuyang Li</span></span>
<span id="id1.1.id1.1.1.2.1.2" class="ltx_tr">
<span id="id1.1.id1.1.1.2.1.2.1" class="ltx_td ltx_align_center">Columbia University</span></span>
<span id="id1.1.id1.1.1.2.1.3" class="ltx_tr">
<span id="id1.1.id1.1.1.2.1.3.1" class="ltx_td ltx_align_center">New York, NY</span></span>
<span id="id1.1.id1.1.1.2.1.4" class="ltx_tr">
<span id="id1.1.id1.1.1.2.1.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.1.1.2.1.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">yl5339@columbia.edu</span></span></span>
</span></span>
<span id="id1.1.id1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-bottom:8.61108pt;">
<span id="id1.1.id1.1.1.3.1" class="ltx_tabular ltx_align_top">
<span id="id1.1.id1.1.1.3.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.3.1.1.1" class="ltx_td ltx_align_center">Chengbo Zang</span></span>
<span id="id1.1.id1.1.1.3.1.2" class="ltx_tr">
<span id="id1.1.id1.1.1.3.1.2.1" class="ltx_td ltx_align_center">Columbia University</span></span>
<span id="id1.1.id1.1.1.3.1.3" class="ltx_tr">
<span id="id1.1.id1.1.1.3.1.3.1" class="ltx_td ltx_align_center">New York, NY</span></span>
<span id="id1.1.id1.1.1.3.1.4" class="ltx_tr">
<span id="id1.1.id1.1.1.3.1.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.1.1.3.1.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">cz2678@columbia.edu</span></span></span>
</span></span></span>
<span id="id1.1.id1.2.2" class="ltx_tr">
<span id="id1.1.id1.2.2.1" class="ltx_td ltx_align_center">
<span id="id1.1.id1.2.2.1.1" class="ltx_tabular ltx_align_top">
<span id="id1.1.id1.2.2.1.1.1" class="ltx_tr">
<span id="id1.1.id1.2.2.1.1.1.1" class="ltx_td ltx_align_center">Javad Ghaderi</span></span>
<span id="id1.1.id1.2.2.1.1.2" class="ltx_tr">
<span id="id1.1.id1.2.2.1.1.2.1" class="ltx_td ltx_align_center">Columbia University</span></span>
<span id="id1.1.id1.2.2.1.1.3" class="ltx_tr">
<span id="id1.1.id1.2.2.1.1.3.1" class="ltx_td ltx_align_center">New York, NY</span></span>
<span id="id1.1.id1.2.2.1.1.4" class="ltx_tr">
<span id="id1.1.id1.2.2.1.1.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.2.2.1.1.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">jg3465@columbia.edu</span></span></span>
</span></span>
<span id="id1.1.id1.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center">
<span id="id1.1.id1.2.2.2.1" class="ltx_tabular ltx_align_top">
<span id="id1.1.id1.2.2.2.1.1" class="ltx_tr">
<span id="id1.1.id1.2.2.2.1.1.1" class="ltx_td ltx_align_center">Gil Zussman</span></span>
<span id="id1.1.id1.2.2.2.1.2" class="ltx_tr">
<span id="id1.1.id1.2.2.2.1.2.1" class="ltx_td ltx_align_center">Columbia University</span></span>
<span id="id1.1.id1.2.2.2.1.3" class="ltx_tr">
<span id="id1.1.id1.2.2.2.1.3.1" class="ltx_td ltx_align_center">New York, NY</span></span>
<span id="id1.1.id1.2.2.2.1.4" class="ltx_tr">
<span id="id1.1.id1.2.2.2.1.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.2.2.2.1.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">gil.zussman@columbia.edu</span></span></span>
</span></span>
<span id="id1.1.id1.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">
<span id="id1.1.id1.2.2.3.1" class="ltx_tabular ltx_align_top">
<span id="id1.1.id1.2.2.3.1.1" class="ltx_tr">
<span id="id1.1.id1.2.2.3.1.1.1" class="ltx_td ltx_align_center">Zoran Kostic</span></span>
<span id="id1.1.id1.2.2.3.1.2" class="ltx_tr">
<span id="id1.1.id1.2.2.3.1.2.1" class="ltx_td ltx_align_center">Columbia University</span></span>
<span id="id1.1.id1.2.2.3.1.3" class="ltx_tr">
<span id="id1.1.id1.2.2.3.1.3.1" class="ltx_td ltx_align_center">New York, NY</span></span>
<span id="id1.1.id1.2.2.3.1.4" class="ltx_tr">
<span id="id1.1.id1.2.2.3.1.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.2.2.3.1.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">zk2172@columbia.edu</span></span></span>
</span></span></span>
</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">We introduce Boundless, a photo-realistic synthetic data generation system for enabling highly accurate object detection in dense urban streetscapes. Boundless can replace massive real-world data collection and manual ground-truth object annotation (labeling) with an automated and configurable process. Boundless is based on the Unreal Engine 5 (UE5) City Sample project with improvements enabling accurate collection of 3D bounding boxes across different lighting and scene variability conditions.</p>
<p id="id3.id2" class="ltx_p">We evaluate the performance of object detection models trained on the dataset generated by Boundless when used for inference on a real-world dataset acquired from medium-altitude cameras.
We compare the performance of the Boundless-trained model against the CARLA-trained model and observe an improvement of 7.8 mAP.
The results we achieved support the premise that synthetic data generation is a credible methodology for training/fine-tuning scalable object detection models for urban scenes.</p>
</div>
<figure id="S0.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S0.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.03022/assets/figures/fog_annotated.png" id="S0.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="340" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S0.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Fog</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S0.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.03022/assets/figures/snow_annotated.png" id="S0.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="340" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S0.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Snow</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S0.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.03022/assets/figures/rain_annotated.png" id="S0.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="340" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S0.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">Rain</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S0.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.03022/assets/figures/night_annotated.png" id="S0.F1.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="340" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S0.F1.sf4.3.2" class="ltx_text" style="font-size:90%;">Night</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">Boundless enables dynamic weather and environment changes for different situations, with changing time-of-day, weather conditions and background elements for every camera. (a-d) Different weather conditions.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Pedestrian safety and traffic management in bustling cities can be enhanced by using video-based AI systems for real-time monitoring and interaction with street objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>. Autonomous vehicles will face challenges due to irregular street layouts, numerous cohabitants, and unpredictable pedestrian behavior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. This calls for the use of infrastructure-mounted cameras and sensors to gather real-time video data at locations like traffic intersections, where object detection, tracking, trajectory prediction, and high-level reasoning can be performed on edge servers in real time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To scale up video monitoring systems in large cities, deep learning (DL) models need to be trained and fine-tuned for hundreds of intersections, each with multiple cameras at varying micro-locations.
Successful training of supervised DL models depends highly on the availability of ground-truth annotated (labeled) data. For traffic intersections, the annotation applies to vehicles, bicycles, pedestrians, and other moving objects, as well as immovable traffic furniture.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Real-world image collection is complicated by uncontrollable environmental conditions such as variations in lighting, weather, and the unpredictable behavior of transient objects like pedestrians and vehicles.
“Manual” ground-truth annotation of street objects from arbitrary camera angles requires a large time commitment and monetary resources.
Data collection in real-world scenarios additionally faces legal issues due to privacy violations. Consequently, existing urban datasets often comprise isolated scenes rather than exhaustive city maps, lacking in the ability to capture the multifaceted nature of cityscapes. These datasets offer limited perspectives, predominantly at eye-level street or high-altitude aerial views, which only partially represent the possible views that one might acquire in the urban deployment of cameras.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we investigate the use of synthetic data/image generators that can automatically create ground-truth annotations for training of object detection models, and therefore avoid the complexity and cost of manual ground-truth annotations.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We focus on the generation of synthetic image datasets using Unreal Engine 5<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.unrealengine.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com/</a></span></span></span> to address the shortcomings of existing object detection datasets in urban environments. We incorporate realistic variable lighting, apply post-processing effects to simulate weather conditions and improve render quality, and make adjustments for the level of detail of graphical assets and accurate capture of bounding boxes. The resulting simulator, which we call “Boundless”, can simulate diverse conditions, control environmental factors, and automatically generate high-quality ground-truth annotations. We investigate the suitability of medium-altitude data generated using Boundless for improving object detection performance in a setting for which available real-world training data is scarce.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Object Detection in Urban Environments.</span> A large number of datasets focus on low-altitude vehicle and pedestrian detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. Meanwhile, many other datasets focus on high-altitude aerial environments, where small object detection becomes an important challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. However, there remains a gap for public mixed-perspective datasets that can adapt to a multitude of different deployment conditions.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Real-Time Object Detection.</span> Many models have been proposed for object detection. For real-time applications, in recent years single-stage detectors like SSD and YOLO models or transformer-based DETR architecture variants have achieved significant results for real-time detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. In this work, we use the state-of-the-art YOLOv8x model for experiments. Rather than model development, our focus is the quality of data used for training.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Synthetic Data Generation.</span> Realistic 3D simulators have been used extensively for various urban computer vision problems. The SYNTHIA dataset provides a collection of images from a simulated city along with pixel-level semantic annotations, to support semantic segmentation and scene understanding tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. CARLA, developed in Unreal Engine 4, is an open-source autonomous driving simulator offering extensive resources, including environments and open digital assets explicitly designed for development, testing, and validating self-driving systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. CARLA-generated imagery has been used for object detection and segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. Extensive work has been conducted on GTA V, using frames collected from this video game for training autonomous driving agents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. In addition to urban traffic simulators like CARLA, Unreal Engine itself has been considered as a rendering engine for computer vision approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Recently, MatrixCity adapted the photo-realistic City Sample project as a benchmark for training neural rendering models by designing a plugin for saving frames <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, focusing on pedestrian- and vehicle-free environments and neural rendering applications. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> used the City Sample project to detect pedestrians and hand-annotated them for this purpose.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_italic">In contrast to previous work, here we focus on enhancing the available City Sample project to enable accurate and automated data collection for training performant models for urban deep learning applications</span>. We find that extensive customization is required to enable accurate bounding box annotation collection in Unreal Engine due to a variety of challenges. We further improve the project with live lighting changes and weather and release multiple datasets for medium-altitude object detection, a problem of interest in urban metropolises <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.03022/assets/figures/exs/566.jpg" id="S2.F2.sf1.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="299" height="299" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.03022/assets/figures/2933.jpg" id="S2.F2.sf1.g2" class="ltx_graphics ltx_figure_panel ltx_img_square" width="299" height="299" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">(left) Sample frame of a medium-altitude frame from Boundless; (right) sample frame from CARLA as a comparison against the visual quality of frames rendered using Boundless.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.03022/assets/figures/constel_a.jpg" id="S2.F2.sf2.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="299" height="299" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.03022/assets/figures/extra3_boundless_4820.jpg" id="S2.F2.sf2.g2" class="ltx_graphics ltx_figure_panel ltx_img_square" width="299" height="299" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">(left) An example frame from the real-world validation dataset; (right) sample frame from a digital twin designed to approximate the real-world validation data in Boundless for comparison.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Examples of frames used for the medium-altitude object detection benchmark.</span></figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Per-class average precision and mean average precision of YOLOv8x models trained on VisDrone, CARLA and Boundless datasets, evaluated on the real-world validation set.</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<th id="S2.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Training Dataset</span></th>
<th id="S2.T1.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">Pedestrian AP@0.5</span></th>
<th id="S2.T1.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.4.1.1.3.1" class="ltx_text ltx_font_bold">Vehicle AP@0.5</span></th>
<th id="S2.T1.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.4.1.1.4.1" class="ltx_text ltx_font_bold">mAP@0.5</span></th>
</tr>
<tr id="S2.T1.4.2.2" class="ltx_tr">
<th id="S2.T1.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">VisDrone</th>
<th id="S2.T1.4.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">24.8</th>
<th id="S2.T1.4.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.4.2.2.3.1" class="ltx_text ltx_font_bold">86.9</span></th>
<th id="S2.T1.4.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">55.8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.4.3.1" class="ltx_tr">
<td id="S2.T1.4.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">CARLA</td>
<td id="S2.T1.4.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">14.9</td>
<td id="S2.T1.4.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">75.4</td>
<td id="S2.T1.4.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">45.1</td>
</tr>
<tr id="S2.T1.4.4.2" class="ltx_tr">
<td id="S2.T1.4.4.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Boundless</td>
<td id="S2.T1.4.4.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">24.0</td>
<td id="S2.T1.4.4.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">81.8</td>
<td id="S2.T1.4.4.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">52.9</td>
</tr>
<tr id="S2.T1.4.5.3" class="ltx_tr">
<td id="S2.T1.4.5.3.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Boundless + Digital Twin</td>
<td id="S2.T1.4.5.3.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.4.5.3.2.1" class="ltx_text ltx_font_bold">47.5</span></td>
<td id="S2.T1.4.5.3.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">85.7</td>
<td id="S2.T1.4.5.3.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.4.5.3.4.1" class="ltx_text ltx_font_bold">66.6</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Boundless Simulator Design</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The freely available City Sample project provides an
environment for North American cityscapes, including vehicles, pedestrians, and traffic lights <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.unrealengine.com/marketplace/en-US/product/city-sample" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com/marketplace/en-US/product/city-sample</a></span></span></span>. To facilitate realistic data collection, we created Boundless by making technical changes to the City Sample project to enable realistic data collection for use in AI applications. We show examples of scenes created with Boundless under different weather conditions, including bounding boxes, in Figure <a href="#S0.F1" title="Figure 1 ‣ Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to show the capabilities of the simulator. We detail the technical improvements below.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Lighting.</span>
We allow lighting conditions to be changed dynamically before each new frame collection, thus allowing the scene to change substantially in a single capture session. We implement four new weather conditions corresponding to rain, snow, dust and heat waves. Implemented using decals projected onto the map, the rain and snow effects allow for a more realistic alternative to image-level augmentations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>. We add particle effects for all these weather conditions. We note that the City Sample comes with a default night-time implementation; however, the stylized and overly dark night weather does not correspond to real-world night-time conditions.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Anti-Aliasing.</span> The default temporal anti-aliasing approach in UE5 produces blurred frames. We replace the approach with MSAA and change the camera settings to output 3840x2160 resolution frames.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Level of Detail.</span> The City Sample project includes three levels of detail for pedestrian and vehicle actors. For the medium and low levels of detail, the substituted meshes have insufficient resolution and detail quality for facilitating real-world applications. We change the available levels of detail for each vehicle and pedestrian agent, enabling the simulator to capture distant object bounding boxes accurately from medium-altitude and street-level scenes.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Updated Bounding Boxes.</span> Due to the design of pedestrian and vehicle actors in the City Sample project which results in large or missing collision boundaries for different 3D meshes, significant changes are needed to correctly capture bounding boxes of objects. We re-compute the level of detail, visibility, and occlusion properties of individual objects in a scene before capturing a frame to make sure annotations of all visible objects are obtained.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Export Options.</span> Boundless exports 3D bounding boxes in the KITTI and 2D bounding boxes in the YOLO format.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Datasets</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We give an overview of the different datasets we introduce for our experiments in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Works ‣ Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We generated two synthetic datasets using Boundless:</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p2.1.m1.1a"><mo id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\bullet</annotation></semantics></math><span id="S4.p2.1.1" class="ltx_text ltx_font_bold"> Medium-Altitude City Sample Training Set.</span> The City Sample comes with default city maps. We use Boundless to collect an 8,000-frame dataset from a synthetic intersection from the City Sample project with a static camera angle. All bounding boxes are generated automatically by the simulator. Lighting conditions are changed throughout the training set in between every frame. A frame is saved from the simulation every 3 seconds in simulation time. For comparison purposes, we follow the same approach to create a corresponding dataset consisting of 22,000 frames using the CARLA simulator.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p3.1.m1.1a"><mo id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\bullet</annotation></semantics></math><span id="S4.p3.1.1" class="ltx_text ltx_font_bold"> Medium-Altitude Digital Twin Training Set.</span> We create a realistic 3D digital twin of a real-world intersection within Boundless. We collect 8,700 frames from this highly accurate scene, replicating the camera angles of the medium-altitude real-world validation dataset.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In addition to our synthetically generated datasets, we use the following two real-world image datasets:</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><math id="S4.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p5.1.m1.1a"><mo id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">\bullet</annotation></semantics></math><span id="S4.p5.1.1" class="ltx_text ltx_font_bold"> Medium-Altitude Real World Validation Set.</span> We create a real-world image dataset collected from a major North American metropolis for one intersection. This validation set consists of 3,084 frames collected on different days with a variety of weather and time-of-day conditions. The dataset contains 12,380 vehicles and 15,225 pedestrian bounding boxes.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p6.1.m1.1a"><mo id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><ci id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\bullet</annotation></semantics></math><span id="S4.p6.1.1" class="ltx_text ltx_font_bold"> VisDrone Dataset.</span> The VisDrone dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> contains 7,019 images captured from various perspectives, including top-down and ground-level views, with varying camera angles. Although the dataset originally contains multiple object classes, we adapt it to a two-class object detection problem.
We use the 561-image validation split for reporting our results.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Medium-Altitude Object Detection.</span> We used a medium-altitude object detection task to demonstrate the capabilities of Boundless, where the amount of data available is scarce compared to other types of urban data. In this task, we seek to detect pedestrians and vehicles from a static camera at <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><csymbol cd="latexml" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\sim</annotation></semantics></math>40m height. We wanted to explore how well a model performs on this task when trained on different datasets. To do so, we fine-tuned a COCO-pretrained YOLOv8x model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> on three different datasets: (i) VisDrone, (ii) CARLA, and (iii) Boundless. We evaluated the models on a custom dataset collected from a real-world traffic intersection. All models were trained with SGD for 10 epochs at a learning rate of 1e-3. We show the results of this comparison in Table <a href="#S2.T1" title="Table 1 ‣ 2 Related Works ‣ Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Use of Boundless-generated data using the City Sample map yields a model that greatly outperforms the CARLA-generated data on the real-world validation set, and further exceeds VisDrone performance in this dataset.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Motivated by the superior performance obtained using Boundless, we further implemented a digital twin of the real-world intersection from the real-world validation set in the form of a map in Unreal Engine. We collected more data using Boundless from this map to see how much the results could be improved. Using a 3D model as a map, we collect an additional 8,700 frames. Repeating the experiment by adding these additional frames further improves the mAP score significantly, yielding a higher mAP than the use of VisDrone.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Ethical Considerations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The real-world dataset collected is deliberately installed at an altitude that makes it impossible to discern the pedestrian faces or to read car license plates. The academic institution at whose facilities the camera is installed provided an IRB waiver for sharing this “high elevation data” with the general public since the data inherently preserves privacy.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We created and described the Boundless simulation platform, and benchmarked it in a real-world medium-altitude object detection task that demonstrates challenges with the deployment of object detection models to urban streetscapes. Our best-performing model, using Boundless and incorporating a digital twin of the real-world testbed, achieved an mAP of 66.6, demonstrating the ability of the simulator to create imagery with sufficient realism to be deployed in real-world scenarios.
With Boundless, we seek to support research on urban object detection for metropolises, where data collection, ground-truth annotation/labeling, and model training face technical and legal challenges.
By releasing the simulator along with collected datasets, we aim to facilitate future research and applications in urban computer vision problems.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Dataset and Code Availability</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Datasets and code for the experiments in this study are available at <a target="_blank" href="https://github.com/zk2172-columbia/boundless" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/zk2172-columbia/boundless</a><span id="Sx1.p1.1.1" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p"><span id="Sx2.p1.1.1" class="ltx_text" style="font-size:90%;">This work was supported in part by NSF grants CNS-1827923 and EEC-2133516, NSF grant CNS-2038984 and corresponding support from the Federal Highway Administration (FHA), NSF grant CNS-2148128 and by funds from federal agency and industry partners as specified in the Resilient &amp; Intelligent NextG Systems (RINGS) program, and ARO grant W911NF2210031.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Agarwal et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Dhruv Agarwal, Taci Kucukpinar, Joshua Fraser, Jeffrey Kerley, Andrew R Buck, Derek T Anderson, and Kannappan Palaniappan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Simulating city-scale aerial data collection using unreal engine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2023 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, pages 1–9. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Bochkovskiy et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">YOLOv4: Optimal speed and accuracy of object detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint:2004.10934</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Campbell et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Mark Campbell, Magnus Egerstedt, Jonathan P How, and Richard M Murray.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Autonomous driving in urban environments: approaches, lessons and challenges.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em><span id="bib.bib3.10.2" class="ltx_text" style="font-size:90%;">, 368(1928):4649–4672, 2010.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Carion et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European conference on computer vision</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 213–229. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Guang Chen, Haitao Wang, Kai Chen, Zhijun Li, Zida Song, Yinlong Liu, Wenkai Chen, and Alois Knoll.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">A survey of the four pillars for small object detection: Multiscale representation, contextual information, super-resolution, and region proposal.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on systems, man, and cybernetics: systems</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 52(2):936–953, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Damian et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Alexandru Damian, Claudiu Filip, Anamaria Nistor, Irina Petrariu, Cătălin Mariuc, and Valentin Stratan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Experimental results on synthetic data generation in unreal engine 5 for real-world object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2023 17th International Conference on Engineering of Modern Electric Systems (EMES)</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 1–4. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Dosovitskiy et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">CARLA: An open urban driving simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on robot learning</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, pages 1–16. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Duan et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Zhuoxu Duan, Zhengye Yang, Richard Samoilenko, Dwiref Snehal Oza, Ashvin Jagadeesan, Mingfei Sun, Hongzhe Ye, Zihao Xiong, Gil Zussman, and Zoran Kostic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Smart city traffic intersection: Impact of video quality and scene complexity on precision and inference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE Smart City’21</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Hu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Yue Hu, Gourav Datta, Kira Beerel, and Peter Beerel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Let’s roll: Synthetic dataset analysis for pedestrian detection across different shutter types.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.08136</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Jang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Jaesung Jang, Hyeongyu Lee, and Jong-Chan Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Carfree: Hassle-free object detection dataset generation using carla autonomous driving simulator.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Applied Sciences</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 12(1):281, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Jocher et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Glenn Jocher, Ayush Chaurasia, and Jing Qiu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Ultralytics YOLOv8, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Kostić et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Zoran Kostić, Alex Angus, Zhengye Yang, Zhuoxu Duan, Ivan Seskar, Gil Zussman, and Dipankar Raychaudhuri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Smart city intersections: Intelligence nodes for future metropolises.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer</em><span id="bib.bib12.10.2" class="ltx_text" style="font-size:90%;">, 55(12):74–85, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Kovvali et al. [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Vijay Gopal Kovvali, Vassili Alexiadis, and Lin Zhang PE.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Video-based vehicle trajectory data collection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">Technical report, 2007.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Krajewski et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Robert Krajewski, Julian Bock, Laurent Kloeker, and Lutz Eckstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 21st international conference on intelligent transportation systems (ITSC)</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, pages 2118–2125. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Krajewski et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Robert Krajewski, Tobias Moers, Julian Bock, Lennart Vater, and Lutz Eckstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">The round dataset: A drone dataset of road user trajectories at roundabouts in germany.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, pages 1–6. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Lai et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Huaqing Lai, Liangyan Chen, Weihua Liu, Zi Yan, and Sheng Ye.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">STC-YOLO: small object detection network for traffic signs in complex environments.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sensors</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, 23(11):5307, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Matrixcity: A large-scale city dataset for city-scale neural rendering and beyond.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 3205–3215, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 740–755. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">SSD: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ECCV</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Lv et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and Yi Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Detrs beat yolos on real-time object detection, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Lyssenko et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Maria Lyssenko, Christoph Gladisch, Christian Heinzemann, Matthias Woehrle, and Rudolph Triebel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Instance segmentation in carla: methodology and analysis for pedestrian-oriented synthetic data generation in crowded scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, pages 988–996, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Lyu et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Rtmdet: An empirical study of designing real-time object detectors.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.07784</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Niranjan et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
DR Niranjan, BC VinayKarthik, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Deep learning based object detection model for autonomous driving research using carla simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 2nd international conference on smart electronics and communication (ICOSEC)</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 1251–1258. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Oh et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee, Saurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">A large-scale benchmark dataset for event recognition in surveillance video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR 2011</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, pages 3153–3160. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Rasmussen et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Ingeborg Rasmussen, Sigurd Kvalsvik, Per-Arne Andersen, Teodor Nilsen Aune, and Daniel Hagen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Development of a novel object detection system based on synthetic data generated from unreal game engine.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Applied Sciences</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 12(17):8534, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.4.4.1" class="ltx_text" style="font-size:90%;">Redmon and Farhadi [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">YOLO9000: better, faster, stronger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib26.10.3" class="ltx_text" style="font-size:90%;">, pages 7263–7271, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.4.4.1" class="ltx_text" style="font-size:90%;">Redmon and Farhadi [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">YOLOv3: An incremental improvement.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint:1804.02767</em><span id="bib.bib27.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Redmon et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. CVPR</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Richter et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, pages 102–118. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Robicquet et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Learning social etiquette: Human trajectory prediction in crowded scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, page 5, 2016.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Ros et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib31.11.3" class="ltx_text" style="font-size:90%;">, pages 3234–3243, 2016.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.4.4.1" class="ltx_text" style="font-size:90%;">Saxena and Giriraj [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">
Ujjwal Saxena and Rohan Giriraj.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">Automold.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Road augmentation library at https://github.com/UjjwalSaxena/Automold–Road-Augmentation-Library</em><span id="bib.bib32.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.4.4.1" class="ltx_text" style="font-size:90%;">Shermeyer and Van Etten [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">
Jacob Shermeyer and Adam Van Etten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">The effects of super-resolution on object detection performance in satellite imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em><span id="bib.bib33.10.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Shuai et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Bing Shuai, Alessandro Bergamo, Uta Buechler, Andrew Berneshawi, Alyssa Boden, and Joe Tighe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Large scale real-world multi person tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Sun et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, pages 2446–2454, 2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.02696</em><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Jinwang Wang, Wen Yang, Haowen Guo, Ruixiang Zhang, and Gui-Song Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Tiny object detection in aerial images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 25th international conference on pattern recognition (ICPR)</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, pages 3791–3798. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Xia et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Dota: A large-scale dataset for object detection in aerial images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, pages 3974–3983, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Xu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Chang Xu, Jinwang Wang, Wen Yang, and Lei Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Dot distance for tiny object detection in aerial images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages 1192–1201, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Dongfang Yang, Linhui Li, Keith Redmill, and Ümit Özgüner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Top-view trajectories: A pedestrian dataset of vehicle-crowd interaction from controlled experiments and crowded campus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE Intelligent Vehicles Symposium (IV)</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, pages 899–904. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Yaqoob et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Ibrar Yaqoob, Latif U Khan, SM Ahsan Kazmi, Muhammad Imran, Nadra Guizani, and Choong Seon Hong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Autonomous driving cars in smart cities: Recent advances, requirements, and challenges.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Network</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, 34(1):174–181, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Qingyang Zhang, Hui Sun, Xiaopei Wu, and Hong Zhong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Edge video analytics for public safety: A review.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, 107(8):1675–1696, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Haibin Ling, Qinghua Hu, Qinqin Nie, Hao Cheng, Chenfeng Liu, Xiaoyu Liu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Visdrone-det2018: The vision meets drone object detection in image challenge results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ECCV Workshops</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.03021" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.03022" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.03022">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.03022" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.03023" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 21:51:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
