<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  GameGPT: Multi-agent Collaborative Framework for Game Development
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Dake Chen
    <br class="ltx_break"/>
    AutoGame Research
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="2.1.1">
     dk@autogame.ai
    </span>
    <br class="ltx_break"/>
    Hanbin Wang
    <br class="ltx_break"/>
    X-Institute
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="3.2.2">
     wanghanbin@mails.x-institute.edu.cn
     <br class="ltx_break"/>
    </span>
    Yunhao Huo
    <br class="ltx_break"/>
    University of Southern California
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="4.3.3">
     hhuo@usc.edu
    </span>
    <br class="ltx_break"/>
    Yuzhao Li
    <br class="ltx_break"/>
    AutoGame Research
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="5.4.4">
     ram@autogame.ai
    </span>
    <br class="ltx_break"/>
    Haoyang Zhang
    <br class="ltx_break"/>
    AutoGame Research
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="6.5.5">
     17@autogame.ai
    </span>
    <br class="ltx_break"/>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="1">
   The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes.
In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development.
While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns.
These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.
   <span class="ltx_figure" id="1.1">
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="562" id="1.1.g1" src="/html/2310.08067/assets/figures/Frame_3.png" width="598"/>
   </span>
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Artificial intelligence’s applications in game development can be traced back to classic games such as
    <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">
     Starcraft
    </span>
    and
    <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">
     Diablo
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      <span class="ltx_text" style="font-size:90%;">
       1
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib2" title="">
      <span class="ltx_text" style="font-size:90%;">
       2
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      <span class="ltx_text" style="font-size:90%;">
       3
      </span>
     </a>
     ]
    </cite>
    . Developers have consistently required AI systems for crafting interactive virtual worlds and characters. These systems have become standard in the development of such interactive platforms. Early game AI research emphasizes controlling non-player characters (NPCs) and pathfinding
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      <span class="ltx_text" style="font-size:90%;">
       4
      </span>
     </a>
     ]
    </cite>
    .
With the advancement of natural language processing (NLP), some pioneering works that focus on generating levels using the deep learning technique have emerged. A representative is MarioGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      <span class="ltx_text" style="font-size:90%;">
       5
      </span>
     </a>
     ]
    </cite>
    , which successfully generates levels in
    <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">
     Super Mario Bros
    </span>
    by fine-tuning GPT2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      <span class="ltx_text" style="font-size:90%;">
       6
      </span>
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Recently, transformer-based large language models (LLMs) have achieved substantial advancements, making notable strides in both natural language processing and computer vision
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      <span class="ltx_text" style="font-size:90%;">
       7
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib8" title="">
      <span class="ltx_text" style="font-size:90%;">
       8
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib9" title="">
      <span class="ltx_text" style="font-size:90%;">
       9
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib6" title="">
      <span class="ltx_text" style="font-size:90%;">
       6
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      <span class="ltx_text" style="font-size:90%;">
       10
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib11" title="">
      <span class="ltx_text" style="font-size:90%;">
       11
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib12" title="">
      <span class="ltx_text" style="font-size:90%;">
       12
      </span>
     </a>
     ]
    </cite>
    . The training of LLMs is a multi-phase process. The initial phase involves training these models on extensive corpora, fostering the acquisition of fundamental language capabilities. In the subsequent phase, which is of considerable significance, the models are fine-tuned via data for a diverse of NLP tasks that are delineated through instructions
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib13" title="">
      <span class="ltx_text" style="font-size:90%;">
       13
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib14" title="">
      <span class="ltx_text" style="font-size:90%;">
       14
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib15" title="">
      <span class="ltx_text" style="font-size:90%;">
       15
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib16" title="">
      <span class="ltx_text" style="font-size:90%;">
       16
      </span>
     </a>
     ]
    </cite>
    . This instruction tuning enhances the models’ ability to generalize across a wide range of applications, leading to noteworthy zero-shot performance on unseen tasks. Lastly, the reinforcement learning from human feedback (RLHF) phase guarantees the models’ structural integrity and reliability
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      <span class="ltx_text" style="font-size:90%;">
       17
      </span>
     </a>
     ]
    </cite>
    . More importantly, this phase also grants the model the capacity to generate content that emulates human style, thereby enhancing its versatility as an agent.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Moreover, the advancement of LLMs has catalyzed the utilization of agents in automating software development processes. Various studies have explored the deployment of a single LLM-based agent to perform diverse tasks. AutoGPT, for instance, employs an LLM agent to tackle real-world decision-making tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib18" title="">
      <span class="ltx_text" style="font-size:90%;">
       18
      </span>
     </a>
     ]
    </cite>
    , while HuggingGPT employs a single LLM as a controller to orchestrate the completion of complicated AI tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib19" title="">
      <span class="ltx_text" style="font-size:90%;">
       19
      </span>
     </a>
     ]
    </cite>
    . Despite these approaches relying on a sole LLM agent, they incorporate reviewer roles to refine decision-making. In AutoGPT, a secondary opinion is obtained from a supervised learner to augment performance, and HuggingGPT integrates GPT-4 as a critic to evaluate decision accuracy.
Furthermore, multiple works utilize multiple agents in their frameworks to make LLM competent for complex tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib20" title="">
      <span class="ltx_text" style="font-size:90%;">
       20
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib21" title="">
      <span class="ltx_text" style="font-size:90%;">
       21
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib22" title="">
      <span class="ltx_text" style="font-size:90%;">
       22
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib23" title="">
      <span class="ltx_text" style="font-size:90%;">
       23
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib24" title="">
      <span class="ltx_text" style="font-size:90%;">
       24
      </span>
     </a>
     ]
    </cite>
    . MetaGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      <span class="ltx_text" style="font-size:90%;">
       21
      </span>
     </a>
     ]
    </cite>
    introduces a multi-agent framework, which can be used for automating the development of various software. CHATDEV
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib20" title="">
      <span class="ltx_text" style="font-size:90%;">
       20
      </span>
     </a>
     ]
    </cite>
    presents a novel software development framework that harnesses agents to enhance collaboration among the various roles involved in the software development process.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    When employing LLM agents for automated software development, these studies encounter inherent limitations associated with LLMs, notably the issue of hallucination. This challenge manifests particularly during the planning and code generation phases
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib19" title="">
      <span class="ltx_text" style="font-size:90%;">
       19
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib20" title="">
      <span class="ltx_text" style="font-size:90%;">
       20
      </span>
     </a>
     ]
    </cite>
    . Distinct from generic software development, the game development industry operates under a stringent demand to keep up with the trends, necessitating heightened precision and conciseness throughout the development process for optimal efficiency. Moreover, tuning and employing one single LLM to serve the whole development cycle of game development without hallucination and high precision is impractical and costly. As a result, the framework requires multiple critic and reviewer roles to effectively mitigate the hallucinatory tendencies inherent in language models. Furthermore, in the context of game development, we identify an additional limitation of LLMs, that of redundancy. Particularly in the game development domain, LLMs can generate unnecessary and uninformative tasks or code snippets.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    To effectively address both hallucination and redundancy, the GameGPT framework strategically employs a combination of approaches, including dual collaboration, instruction tuning through in-house lexicons, and code decoupling. Notably, dual collaboration involves the interaction between LLMs and small expert deep learning models, alongside the collaborative engagement between execution roles and review roles. These synergistic have empirically demonstrated their effectiveness for mitigating both hallucination and redundancy within the framework.
In summary, our contributions are as follows:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We introduce an innovative multi-agent framework tailored to facilitate automated game development.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       Beyond hallucination, we identify the issue of redundancy inherent to LLM-based agents in the context of game development.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       To address the hallucination and redundancy concerns of LLMs within game development, several mitigations including dual collaboration and code decoupling are proposed.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i4.p1">
      <p class="ltx_p" id="S1.I1.i4.p1.1">
       Empirical results demonstrate the GameGPT’s capability in effective decision-making and decision-rectifying throughout the game development process.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   GameGPT
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Overview
   </h3>
   <figure class="ltx_figure" id="S2.F1">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="257" id="S2.F1.g1" src="/html/2310.08067/assets/figures/Frame_1.png" width="658"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 1:
     </span>
     Overview of the proposed framework
    </figcaption>
   </figure>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     The GameGPT framework is designed as a specialized multi-agent system for game development. To address the limitations of the LLM and the temporal constraints of game development, we integrate multiple agents with distinct roles into the framework. This integration aims to enhance precision and scalability. The scalability aspect of GameGPT offers the potential to create games of medium to large sizes. Moreover, GameGPT operates in a collaborative manner, exhibiting a dual collaboration approach. Firstly, it involves cooperation between the LLMs and smaller expert models dedicated to specific tasks, thereby enhancing the decision-making process. Secondly, collaboration occurs among agents assigned different roles, contributing to the decision-rectification and minimizing the hallucination of LLMs.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     Figure
     <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 Overview ‣ 2 GameGPT ‣ GameGPT: Multi-agent Collaborative Framework for Game Development">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     provides an overview of the proposed GameGPT framework, which operates through five distinct stages: game development planning, task classification, code generation, task execution, and result summarization. Upon receiving a client’s request, the game development manager initiates the game development planning stage, resulting in the creation of a task list. Subsequently, game development engineers utilize smaller expert models to accurately determine the task type and its associated parameters. Following this, game engine engineers proceed to generate code and scripts in alignment with the designated game engineer. Throughout the initial three stages, three critics are incorporated to mitigate concerns related to hallucination and redundancy. Concluding these stages, the game engine testing engineer undertakes the execution of tasks and subsequently produces a comprehensive result summary.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Multi-agent Framework
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.2">
     In GameGPT, each agent maintains a private memory system and can access the shared public discussion to acquire the necessary information for guiding their decision-making process. For agent
     <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1">
      <semantics id="S2.SS2.p1.1.m1.1a">
       <mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">
        i
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b">
        <ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">
         𝑖
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">
        i
       </annotation>
      </semantics>
     </math>
     at time step
     <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1">
      <semantics id="S2.SS2.p1.2.m2.1a">
       <mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">
        t
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b">
        <ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">
         𝑡
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">
        t
       </annotation>
      </semantics>
     </math>
     , this process can be formally represented as follows:
    </p>
    <table class="ltx_equation ltx_eqn_table" id="S2.E1">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="p_{\theta_{i}}(O_{it}|M_{it},P_{t})," class="ltx_Math" display="block" id="S2.E1.m1.1">
         <semantics id="S2.E1.m1.1a">
          <mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">
           <mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">
            <msub id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">
             <mi id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml">
              p
             </mi>
             <msub id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml">
              <mi id="S2.E1.m1.1.1.1.1.3.3.2" xref="S2.E1.m1.1.1.1.1.3.3.2.cmml">
               θ
              </mi>
              <mi id="S2.E1.m1.1.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.3.3.3.cmml">
               i
              </mi>
             </msub>
            </msub>
            <mo id="S2.E1.m1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S2.E1.m1.1.1.1.1.2.cmml">
             ​
            </mo>
            <mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">
             <mo id="S2.E1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">
              (
             </mo>
             <mrow id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">
              <msub id="S2.E1.m1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.1.1.1.4.cmml">
               <mi id="S2.E1.m1.1.1.1.1.1.1.1.4.2" xref="S2.E1.m1.1.1.1.1.1.1.1.4.2.cmml">
                O
               </mi>
               <mrow id="S2.E1.m1.1.1.1.1.1.1.1.4.3" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3.cmml">
                <mi id="S2.E1.m1.1.1.1.1.1.1.1.4.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3.2.cmml">
                 i
                </mi>
                <mo id="S2.E1.m1.1.1.1.1.1.1.1.4.3.1" lspace="0em" rspace="0em" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3.1.cmml">
                 ​
                </mo>
                <mi id="S2.E1.m1.1.1.1.1.1.1.1.4.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3.3.cmml">
                 t
                </mi>
               </mrow>
              </msub>
              <mo fence="false" id="S2.E1.m1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.3.cmml">
               |
              </mo>
              <mrow id="S2.E1.m1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.2.3.cmml">
               <msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">
                <mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">
                 M
                </mi>
                <mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                 <mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">
                  i
                 </mi>
                 <mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1" lspace="0em" rspace="0em" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">
                  ​
                 </mo>
                 <mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">
                  t
                 </mi>
                </mrow>
               </msub>
               <mo id="S2.E1.m1.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.2.3.cmml">
                ,
               </mo>
               <msub id="S2.E1.m1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml">
                <mi id="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml">
                 P
                </mi>
                <mi id="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml">
                 t
                </mi>
               </msub>
              </mrow>
             </mrow>
             <mo id="S2.E1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">
              )
             </mo>
            </mrow>
           </mrow>
           <mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">
            ,
           </mo>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b">
           <apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">
            <times id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2">
            </times>
            <apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3">
             <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3">
              subscript
             </csymbol>
             <ci id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2">
              𝑝
             </ci>
             <apply id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3">
              <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.3">
               subscript
              </csymbol>
              <ci id="S2.E1.m1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.3.2">
               𝜃
              </ci>
              <ci id="S2.E1.m1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3">
               𝑖
              </ci>
             </apply>
            </apply>
            <apply id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">
             <csymbol cd="latexml" id="S2.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.3">
              conditional
             </csymbol>
             <apply id="S2.E1.m1.1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.4">
              <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.4.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.4">
               subscript
              </csymbol>
              <ci id="S2.E1.m1.1.1.1.1.1.1.1.4.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.4.2">
               𝑂
              </ci>
              <apply id="S2.E1.m1.1.1.1.1.1.1.1.4.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3">
               <times id="S2.E1.m1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3.1">
               </times>
               <ci id="S2.E1.m1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3.2">
                𝑖
               </ci>
               <ci id="S2.E1.m1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.4.3.3">
                𝑡
               </ci>
              </apply>
             </apply>
             <list id="S2.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2">
              <apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1">
               <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1">
                subscript
               </csymbol>
               <ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.2">
                𝑀
               </ci>
               <apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3">
                <times id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1">
                </times>
                <ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">
                 𝑖
                </ci>
                <ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">
                 𝑡
                </ci>
               </apply>
              </apply>
              <apply id="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2.2">
               <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2.2">
                subscript
               </csymbol>
               <ci id="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.2">
                𝑃
               </ci>
               <ci id="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2.2.2.3">
                𝑡
               </ci>
              </apply>
             </list>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S2.E1.m1.1c">
           p_{\theta_{i}}(O_{it}|M_{it},P_{t}),
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (1)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <p class="ltx_p" id="S2.SS2.p1.10">
     where
     <math alttext="p_{\theta_{i}}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m1.1">
      <semantics id="S2.SS2.p1.3.m1.1a">
       <msub id="S2.SS2.p1.3.m1.1.1" xref="S2.SS2.p1.3.m1.1.1.cmml">
        <mi id="S2.SS2.p1.3.m1.1.1.2" xref="S2.SS2.p1.3.m1.1.1.2.cmml">
         p
        </mi>
        <msub id="S2.SS2.p1.3.m1.1.1.3" xref="S2.SS2.p1.3.m1.1.1.3.cmml">
         <mi id="S2.SS2.p1.3.m1.1.1.3.2" xref="S2.SS2.p1.3.m1.1.1.3.2.cmml">
          θ
         </mi>
         <mi id="S2.SS2.p1.3.m1.1.1.3.3" xref="S2.SS2.p1.3.m1.1.1.3.3.cmml">
          i
         </mi>
        </msub>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m1.1b">
        <apply id="S2.SS2.p1.3.m1.1.1.cmml" xref="S2.SS2.p1.3.m1.1.1">
         <csymbol cd="ambiguous" id="S2.SS2.p1.3.m1.1.1.1.cmml" xref="S2.SS2.p1.3.m1.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS2.p1.3.m1.1.1.2.cmml" xref="S2.SS2.p1.3.m1.1.1.2">
          𝑝
         </ci>
         <apply id="S2.SS2.p1.3.m1.1.1.3.cmml" xref="S2.SS2.p1.3.m1.1.1.3">
          <csymbol cd="ambiguous" id="S2.SS2.p1.3.m1.1.1.3.1.cmml" xref="S2.SS2.p1.3.m1.1.1.3">
           subscript
          </csymbol>
          <ci id="S2.SS2.p1.3.m1.1.1.3.2.cmml" xref="S2.SS2.p1.3.m1.1.1.3.2">
           𝜃
          </ci>
          <ci id="S2.SS2.p1.3.m1.1.1.3.3.cmml" xref="S2.SS2.p1.3.m1.1.1.3.3">
           𝑖
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.3.m1.1c">
        p_{\theta_{i}}
       </annotation>
      </semantics>
     </math>
     corresponds to the LLM or an expert model associated with agent
     <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m2.1">
      <semantics id="S2.SS2.p1.4.m2.1a">
       <mi id="S2.SS2.p1.4.m2.1.1" xref="S2.SS2.p1.4.m2.1.1.cmml">
        i
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m2.1b">
        <ci id="S2.SS2.p1.4.m2.1.1.cmml" xref="S2.SS2.p1.4.m2.1.1">
         𝑖
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.4.m2.1c">
        i
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="O_{it}" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m3.1">
      <semantics id="S2.SS2.p1.5.m3.1a">
       <msub id="S2.SS2.p1.5.m3.1.1" xref="S2.SS2.p1.5.m3.1.1.cmml">
        <mi id="S2.SS2.p1.5.m3.1.1.2" xref="S2.SS2.p1.5.m3.1.1.2.cmml">
         O
        </mi>
        <mrow id="S2.SS2.p1.5.m3.1.1.3" xref="S2.SS2.p1.5.m3.1.1.3.cmml">
         <mi id="S2.SS2.p1.5.m3.1.1.3.2" xref="S2.SS2.p1.5.m3.1.1.3.2.cmml">
          i
         </mi>
         <mo id="S2.SS2.p1.5.m3.1.1.3.1" lspace="0em" rspace="0em" xref="S2.SS2.p1.5.m3.1.1.3.1.cmml">
          ​
         </mo>
         <mi id="S2.SS2.p1.5.m3.1.1.3.3" xref="S2.SS2.p1.5.m3.1.1.3.3.cmml">
          t
         </mi>
        </mrow>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m3.1b">
        <apply id="S2.SS2.p1.5.m3.1.1.cmml" xref="S2.SS2.p1.5.m3.1.1">
         <csymbol cd="ambiguous" id="S2.SS2.p1.5.m3.1.1.1.cmml" xref="S2.SS2.p1.5.m3.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS2.p1.5.m3.1.1.2.cmml" xref="S2.SS2.p1.5.m3.1.1.2">
          𝑂
         </ci>
         <apply id="S2.SS2.p1.5.m3.1.1.3.cmml" xref="S2.SS2.p1.5.m3.1.1.3">
          <times id="S2.SS2.p1.5.m3.1.1.3.1.cmml" xref="S2.SS2.p1.5.m3.1.1.3.1">
          </times>
          <ci id="S2.SS2.p1.5.m3.1.1.3.2.cmml" xref="S2.SS2.p1.5.m3.1.1.3.2">
           𝑖
          </ci>
          <ci id="S2.SS2.p1.5.m3.1.1.3.3.cmml" xref="S2.SS2.p1.5.m3.1.1.3.3">
           𝑡
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.5.m3.1c">
        O_{it}
       </annotation>
      </semantics>
     </math>
     denotes the output or deliverable produced by the agent
     <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m4.1">
      <semantics id="S2.SS2.p1.6.m4.1a">
       <mi id="S2.SS2.p1.6.m4.1.1" xref="S2.SS2.p1.6.m4.1.1.cmml">
        i
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m4.1b">
        <ci id="S2.SS2.p1.6.m4.1.1.cmml" xref="S2.SS2.p1.6.m4.1.1">
         𝑖
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.6.m4.1c">
        i
       </annotation>
      </semantics>
     </math>
     at time step
     <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.p1.7.m5.1">
      <semantics id="S2.SS2.p1.7.m5.1a">
       <mi id="S2.SS2.p1.7.m5.1.1" xref="S2.SS2.p1.7.m5.1.1.cmml">
        t
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m5.1b">
        <ci id="S2.SS2.p1.7.m5.1.1.cmml" xref="S2.SS2.p1.7.m5.1.1">
         𝑡
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.7.m5.1c">
        t
       </annotation>
      </semantics>
     </math>
     , and
     <math alttext="M_{it}" class="ltx_Math" display="inline" id="S2.SS2.p1.8.m6.1">
      <semantics id="S2.SS2.p1.8.m6.1a">
       <msub id="S2.SS2.p1.8.m6.1.1" xref="S2.SS2.p1.8.m6.1.1.cmml">
        <mi id="S2.SS2.p1.8.m6.1.1.2" xref="S2.SS2.p1.8.m6.1.1.2.cmml">
         M
        </mi>
        <mrow id="S2.SS2.p1.8.m6.1.1.3" xref="S2.SS2.p1.8.m6.1.1.3.cmml">
         <mi id="S2.SS2.p1.8.m6.1.1.3.2" xref="S2.SS2.p1.8.m6.1.1.3.2.cmml">
          i
         </mi>
         <mo id="S2.SS2.p1.8.m6.1.1.3.1" lspace="0em" rspace="0em" xref="S2.SS2.p1.8.m6.1.1.3.1.cmml">
          ​
         </mo>
         <mi id="S2.SS2.p1.8.m6.1.1.3.3" xref="S2.SS2.p1.8.m6.1.1.3.3.cmml">
          t
         </mi>
        </mrow>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m6.1b">
        <apply id="S2.SS2.p1.8.m6.1.1.cmml" xref="S2.SS2.p1.8.m6.1.1">
         <csymbol cd="ambiguous" id="S2.SS2.p1.8.m6.1.1.1.cmml" xref="S2.SS2.p1.8.m6.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS2.p1.8.m6.1.1.2.cmml" xref="S2.SS2.p1.8.m6.1.1.2">
          𝑀
         </ci>
         <apply id="S2.SS2.p1.8.m6.1.1.3.cmml" xref="S2.SS2.p1.8.m6.1.1.3">
          <times id="S2.SS2.p1.8.m6.1.1.3.1.cmml" xref="S2.SS2.p1.8.m6.1.1.3.1">
          </times>
          <ci id="S2.SS2.p1.8.m6.1.1.3.2.cmml" xref="S2.SS2.p1.8.m6.1.1.3.2">
           𝑖
          </ci>
          <ci id="S2.SS2.p1.8.m6.1.1.3.3.cmml" xref="S2.SS2.p1.8.m6.1.1.3.3">
           𝑡
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.8.m6.1c">
        M_{it}
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="P_{t}" class="ltx_Math" display="inline" id="S2.SS2.p1.9.m7.1">
      <semantics id="S2.SS2.p1.9.m7.1a">
       <msub id="S2.SS2.p1.9.m7.1.1" xref="S2.SS2.p1.9.m7.1.1.cmml">
        <mi id="S2.SS2.p1.9.m7.1.1.2" xref="S2.SS2.p1.9.m7.1.1.2.cmml">
         P
        </mi>
        <mi id="S2.SS2.p1.9.m7.1.1.3" xref="S2.SS2.p1.9.m7.1.1.3.cmml">
         t
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m7.1b">
        <apply id="S2.SS2.p1.9.m7.1.1.cmml" xref="S2.SS2.p1.9.m7.1.1">
         <csymbol cd="ambiguous" id="S2.SS2.p1.9.m7.1.1.1.cmml" xref="S2.SS2.p1.9.m7.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS2.p1.9.m7.1.1.2.cmml" xref="S2.SS2.p1.9.m7.1.1.2">
          𝑃
         </ci>
         <ci id="S2.SS2.p1.9.m7.1.1.3.cmml" xref="S2.SS2.p1.9.m7.1.1.3">
          𝑡
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.9.m7.1c">
        P_{t}
       </annotation>
      </semantics>
     </math>
     refer to its private memory and the requisite public record up to time step
     <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.p1.10.m8.1">
      <semantics id="S2.SS2.p1.10.m8.1a">
       <mi id="S2.SS2.p1.10.m8.1.1" xref="S2.SS2.p1.10.m8.1.1.cmml">
        t
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m8.1b">
        <ci id="S2.SS2.p1.10.m8.1.1.cmml" xref="S2.SS2.p1.10.m8.1.1">
         𝑡
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p1.10.m8.1c">
        t
       </annotation>
      </semantics>
     </math>
     , respectively.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     The presence of multiple agents with distinct roles is crucial in GameGPT due to the unique characteristics of the game development industry and the limitations of LLMs. Given that game development cycles typically span several months, relying on a solitary agent with comprehensive memory and contextual information would render language models, including LLMs, inefficient. This approach could also result in scalability challenges as projects become more complicated over time. Furthermore, considering the limitations on the number of tokens processed by LLMs, employing a solitary agent with an all-encompassing memory for large-scale game development projects is not pragmatic. Additionally, the inherent issues of hallucination and redundancy observed in LLMs underscore the significance of collaboration among multiple agents, particularly those with critic roles. This collaboration is significant in mitigating the challenges posed by LLM hallucination and redundancy. GameGPT utilizes a diverse set of roles to facilitate its operations, including responsibilities across the game development cycle. These roles comprise the game content designer, game development manager, plan reviewer, game development engineer, task reviewer, game engine engineer, code reviewer, and game engine testing engineer. Each of these roles works on distinct missions throughout the game development process.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Game Development Planning
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     The initial phase of GameGPT, upon receiving a user request, involves generating a task plan. This planning phase stands as one of the pivotal steps, greatly influencing the seamless progression of the entire development process. Orchestrated by an LLM-based game development manager, this phase entails proposing an initial plan that is subsequently disassembled into a list of tasks. Notably, due to the limitations inherent in LLMs, this initial plan often exhibits instances of hallucinations, which give rise to unexpected tasks, and include redundant tasks that are uninformative or unnecessary. To address these challenges, we present four strategies designed to alleviate these concerns. All four strategies are orthogonal to each other and can be layered to achieve better effectiveness.
    </p>
   </div>
   <section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Game Genre Classification with Template
    </h4>
    <div class="ltx_para" id="S2.SS3.SSS0.Px1.p1">
     <p class="ltx_p" id="S2.SS3.SSS0.Px1.p1.1">
      The first strategy is to perform classification for the incoming request, aimed at discerning the genre intended for the game. Presently, the GameGPT framework accommodates development for five distinct game genres, namely, [e.g., action, strategy, role-playing, simulation, and adventure]. For each genre, we provide a standardized plan template, guiding the game development manager in completing the template with relevant information. By adopting this approach, the frequency of redundant tasks is notably reduced, while simultaneously mitigating the likelihood of hallucination occurrences.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS3.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Plan Review
    </h4>
    <div class="ltx_para" id="S2.SS3.SSS0.Px2.p1">
     <p class="ltx_p" id="S2.SS3.SSS0.Px2.p1.1">
      The second strategy involves the engagement of a plan reviewer which is another LLM-based agent. This plan reviewer operates through strategically crafted prompts, facilitating a comprehensive review of the task plan. Its primary objective is to minimize occurrences of hallucination and redundancy. The plan reviewer assesses the plan and furnishes feedback, aiming to refine and enhance its precision, efficiency, and conciseness. The insights provided by the plan reviewer serve as input to the game development manager, empowering the shaping of a task plan that is notably more accurate and refined.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS3.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction Tuning the Language Model for Planning
    </h4>
    <div class="ltx_para" id="S2.SS3.SSS0.Px3.p1">
     <p class="ltx_p" id="S2.SS3.SSS0.Px3.p1.1">
      The third approach aims to tune and tailor the LLM of the game development manager for game development planning through specialized instructions. This fine-tuning process endeavors to yield a plan that is both more accurate and concise. To facilitate this, we collect and consolidate an in-house dataset comprising numerous input-output pairs. While these pairs do not conform to a standardized format concerning length or structure, they uniformly revolve around requests for game development. The corresponding outputs are supplied by adept game development practitioners. By adopting this approach, we effectively bridge the gap between the LLM’s general linguistic capabilities and its aptitude for game development planning.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS3.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     User Presentation and Rectification
    </h4>
    <div class="ltx_para" id="S2.SS3.SSS0.Px4.p1">
     <p class="ltx_p" id="S2.SS3.SSS0.Px4.p1.1">
      The fourth and final strategy serves as a safety net within the planning phase. Throughout the planning process, the game development manager consistently shares interim outcomes with the users on the frontend interface, enabling them to remain informed about ongoing developments. To augment this, an interactive method is integrated, empowering users to actively review, rectify, and enhance the plan in accordance with their expectations. This approach safeguards alignment between the devised plan and the users’ desires.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.4
    </span>
    Game Development Task Classification
   </h3>
   <div class="ltx_para" id="S2.SS4.p1">
    <p class="ltx_p" id="S2.SS4.p1.1">
     The process of task Classification within GameGPT demands a high accuracy in identifying both the task type and its corresponding arguments. Consequently, to ensure the accuracy of this phase, the agent of the game development engineer role is allocated. This role is supported by the utilization of two expert models, which collaboratively engage in task classification. This collaborative approach enhances the accuracy and effectiveness of task and argument identification.
    </p>
   </div>
   <section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Task Classifier and Task Argument Identifier
    </h4>
    <div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
     <p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1">
      To circumvent the hallucination of language models and enhance the accuracy of the task classification, we provide a list of possible types of tasks in game development. In order to perform the classification, a BERT model is employed to effectively categorize each task. The BERT model has been trained with an in-house dataset. This dataset contains data entries uniquely tailored to the tasks of game development. The input is a task drawn from the predetermined list, while the output corresponds to the task’s designated category.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS4.SSS0.Px1.p2">
     <p class="ltx_p" id="S2.SS4.SSS0.Px1.p2.1">
      Identifying the argument involves another LLM. The agent provides a template that corresponds to the identified task type and subsequently prompts the LLM to populate this template. The incorporation of the template can elevate the accuracy of argument identification and significantly reduce the hallucination.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Task Type and Argument Review
    </h4>
    <div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
     <p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1">
      In this phase, a task reviewer reviewer agent is empolyed. It is prompted to double-confirm that the identified type and argument of each class are reasonable. The review process includes if the task type is in the predetermined range and is the best fit for the corresponding task. It also involves reviewing the argument list and see if it aligns with the task. In some scenarios where inferring the argument based on the contextual task information and the user’s request is unfeasible, GameGPT adopts a proactive approach. The task reviewer engages the user by initiating a prompt on the frontend interface and requests additional information necessary for the argument. This interactive method ensures the completeness of argument details even in instances where automated inference falls short.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS4.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Task Dependency and Execution Sequence
    </h4>
    <div class="ltx_para" id="S2.SS4.SSS0.Px3.p1">
     <p class="ltx_p" id="S2.SS4.SSS0.Px3.p1.1">
      The task reviewer agent is also responsible for discerning task dependencies and constructing a directed acyclic graph that encapsulates these relationships. Subsequent to the establishment of this graph, a breadth-first search algorithm is employed to traverse it. The traversal yields a determined sequence for task execution. This process ensures an orderly and systematic execution of tasks in accordance with their dependencies, resulting in a coherent and structured development progression.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.5
    </span>
    Code generation
   </h3>
   <div class="ltx_para" id="S2.SS5.p1">
    <p class="ltx_p" id="S2.SS5.p1.1">
     Generating lengthy code scripts with an LLM inherently carries a greater risk of encountering hallucinations and redundancy. In response, we introduce a novel approach to decoupling the code for game design, simplifying the inference process for the LLM and consequently mitigating both hallucination and redundancy.
    </p>
   </div>
   <section class="ltx_paragraph" id="S2.SS5.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Decouple the Script for Game Development
    </h4>
    <div class="ltx_para" id="S2.SS5.SSS0.Px1.p1">
     <p class="ltx_p" id="S2.SS5.SSS0.Px1.p1.1">
      The hallucination and redundancy tend to be more frequent when generating lengthy scripts. In response, our proposed framework introduces a novel decoupling approach specifically designed for game development, aimed at separating the Lua script. To achieve this, we strategically divide the expected script into numerous code snippets of manageable length for LLM processing.
This decoupling approach significantly eases the work of LLM and thereby mitigates the occurrence of hallucination and redundancy.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS5.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Post Decoupling In-context Inference
    </h4>
    <div class="ltx_para" id="S2.SS5.SSS0.Px2.p1">
     <p class="ltx_p" id="S2.SS5.SSS0.Px2.p1.1">
      In
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib10" title="">
        <span class="ltx_text" style="font-size:90%;">
         10
        </span>
       </a>
       ]
      </cite>
      , an effective inference method called in-context-learning to mitigate hallucination is proposed. In GameGPT, we adopt a similar post-training strategy built upon our decoupling method. As our decoupling approach breaks down task-related code into smaller code snippets, we no longer depend on lengthy example scripts for inference. Instead, we incorporate multiple example snippets into the prompt, effectively reducing both hallucination and redundancy.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS5.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Candidate selection
    </h4>
    <div class="ltx_para" id="S2.SS5.SSS0.Px3.p1">
     <p class="ltx_p" id="S2.SS5.SSS0.Px3.p1.1">
      Moreover, another technique integrated into GameGPT to counteract hallucinations involves generating a set of
      <math alttext="K" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.1.m1.1">
       <semantics id="S2.SS5.SSS0.Px3.p1.1.m1.1a">
        <mi id="S2.SS5.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS5.SSS0.Px3.p1.1.m1.1.1.cmml">
         K
        </mi>
        <annotation-xml encoding="MathML-Content" id="S2.SS5.SSS0.Px3.p1.1.m1.1b">
         <ci id="S2.SS5.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS5.SSS0.Px3.p1.1.m1.1.1">
          𝐾
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS5.SSS0.Px3.p1.1.m1.1c">
         K
        </annotation>
       </semantics>
      </math>
      code snippets for each task. These snippets are subsequently tested within a virtual environment and simultaneously presented to the user. Both the testing process and user feedback are leveraged to identify and eliminate problematic candidates, leaving only the most viable option for execution. This approach serves to further minimize the occurrence of hallucinations.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS5.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction Tuning
    </h4>
    <div class="ltx_para" id="S2.SS5.SSS0.Px4.p1">
     <p class="ltx_p" id="S2.SS5.SSS0.Px4.p1.1">
      Furthermore, we have collected an in-house lexicon comprising an extensive repository of code snippets designed for game development. Each of these snippets is annotated by labelers, providing clear instructions that specify their intended purpose. This high-quality lexicon serves as a valuable resource for fine-tuning our model.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS5.SSS0.Px5">
    <h4 class="ltx_title ltx_title_paragraph">
     Code Review and Enhancement
    </h4>
    <div class="ltx_para" id="S2.SS5.SSS0.Px5.p1">
     <p class="ltx_p" id="S2.SS5.SSS0.Px5.p1.1">
      Following the code generation by the game engine engineer, a code reviewer agent is engaged to thoroughly review and inspect the codebase. The code reviewer performs a comprehensive assessment, actively seeking out any instances of deviation from the original request or unintended hallucinations present within the code.
Upon thorough scrutiny, the code reviewer not only flags potential discrepancies but also furnishes recommendations for refining the code, ultimately yielding a more reasonable version.
Subsequent to the review process, the revised code, along with the code reviewer’s feedback, is shared with both the game engine engineer and the user through the frontend interface. If the user deems it necessary, they can provide suggestions for code revision directly via the frontend interface. These suggestions are relayed to the code reviewer, who subsequently assesses and incorporates them as appropriate, fostering a collaborative and iterative approach to code enhancement.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS6">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.6
    </span>
    Game Development Task Execution and Result Summary
   </h3>
   <div class="ltx_para" id="S2.SS6.p1">
    <p class="ltx_p" id="S2.SS6.p1.1">
     Once the code generation and enhancement are finished, the responsibility transitions to a game engine testing engineer, tasked with executing the generated tasks. During this phase, the testing engineer adheres to the execution sequence formulated in the preceding stage. The execution process involves sending the code of each individual task to the game engine. Subsequently, the testing engineer performs the execution and keeps track of the logs during the execution.
Upon the completion of all tasks specified in the execution sequence, the testing engineer consolidates all the logs generated throughout the execution process. This compilation results in the creation of a succinct and comprehensive summary, which is then presented to the user through the frontend interface.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS6.p2">
    <p class="ltx_p" id="S2.SS6.p2.1">
     Additionally, the testing engineer also identifies and reports any observed tracebacks during the execution. These tracebacks serve as critical indicators that may necessitate adjustments to the execution or code, enabling the refinement of the overall process and contributing to the generation of a polished end product.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib1.2.2.1" style="font-size:90%;">
      [1]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.4.1" style="font-size:90%;">
      J. Laird and M. VanLent, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib1.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib1.6.3" style="font-size:90%;">
      Human-level AI’s killer application:
Interactive computer games,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.7.4" style="font-size:90%;">
      AI magazine
     </em>
     <span class="ltx_text" id="bib.bib1.8.5" style="font-size:90%;">
      , vol. 22, no. 2, pp.
15–15, 2001.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib2.2.2.1" style="font-size:90%;">
      [2]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.4.1" style="font-size:90%;">
      D. Perez-Liebana, S. Samothrakis, J. Togelius, T. Schaul, and S. Lucas,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib2.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib2.6.3" style="font-size:90%;">
      General video game ai: Competition, challenges and
opportunities,” in
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.7.4" style="font-size:90%;">
      Proceedings of the AAAI Conference on Artificial
Intelligence
     </em>
     <span class="ltx_text" id="bib.bib2.8.5" style="font-size:90%;">
      , vol. 30, no. 1, 2016.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib3.2.2.1" style="font-size:90%;">
      [3]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.4.1" style="font-size:90%;">
      J. H. Kim and R. Wu, “Leveraging machine learning for game development,”
2021. [Online]. Available:
     </span>
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.research.google/2021/03/leveraging-machine-learning-for-game.html?m=1" style="font-size:90%;" target="_blank" title="">
      https://blog.research.google/2021/03/leveraging-machine-learning-for-game.html?m=1
     </a>
     <span class="ltx_text" id="bib.bib3.5.2" style="font-size:90%;">
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib4.2.2.1" style="font-size:90%;">
      [4]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.4.1" style="font-size:90%;">
      C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dębiak, C. Dennison,
D. Farhi, Q. Fischer, S. Hashme, C. Hesse
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib4.6.3" style="font-size:90%;">
      , “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib4.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib4.8.5" style="font-size:90%;">
      Dota 2
with large scale deep reinforcement learning,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.9.6" style="font-size:90%;">
      arXiv preprint
arXiv:1912.06680
     </em>
     <span class="ltx_text" id="bib.bib4.10.7" style="font-size:90%;">
      , 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib5.2.2.1" style="font-size:90%;">
      [5]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.4.1" style="font-size:90%;">
      S. Sudhakaran, M. González-Duque, C. Glanois, M. Freiberger, E. Najarro, and
S. Risi, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib5.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib5.6.3" style="font-size:90%;">
      MarioGPT: Open-Ended Text2Level Generation through Large
Language Models,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib6.2.2.1" style="font-size:90%;">
      [6]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.4.1" style="font-size:90%;">
      A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib6.6.3" style="font-size:90%;">
      ,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib6.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib6.8.5" style="font-size:90%;">
      Language models are unsupervised multitask learners,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.9.6" style="font-size:90%;">
      OpenAI blog
     </em>
     <span class="ltx_text" id="bib.bib6.10.7" style="font-size:90%;">
      , vol. 1, no. 8, p. 9, 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib7.2.2.1" style="font-size:90%;">
      [7]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.4.1" style="font-size:90%;">
      A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib7.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib7.6.3" style="font-size:90%;">
      Attention is all you need,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.7.4" style="font-size:90%;">
      Advances in neural information processing systems
     </em>
     <span class="ltx_text" id="bib.bib7.8.5" style="font-size:90%;">
      , vol. 30, 2017.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib8.2.2.1" style="font-size:90%;">
      [8]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.4.1" style="font-size:90%;">
      A. Radford, K. Narasimhan, T. Salimans, I. Sutskever
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib8.6.3" style="font-size:90%;">
      ,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib8.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib8.8.5" style="font-size:90%;">
      Improving language understanding by generative pre-training,”
2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib9.2.2.1" style="font-size:90%;">
      [9]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.4.1" style="font-size:90%;">
      J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib9.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib9.6.3" style="font-size:90%;">
      Bert:
Pre-training of deep bidirectional transformers for language
understanding,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.7.4" style="font-size:90%;">
      arXiv preprint arXiv:1810.04805
     </em>
     <span class="ltx_text" id="bib.bib9.8.5" style="font-size:90%;">
      , 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib10.2.2.1" style="font-size:90%;">
      [10]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.4.1" style="font-size:90%;">
      T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib10.6.3" style="font-size:90%;">
      ,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib10.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib10.8.5" style="font-size:90%;">
      Language models are few-shot learners,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.9.6" style="font-size:90%;">
      Advances in
neural information processing systems
     </em>
     <span class="ltx_text" id="bib.bib10.10.7" style="font-size:90%;">
      , vol. 33, pp. 1877–1901, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib11.2.2.1" style="font-size:90%;">
      [11]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.4.1" style="font-size:90%;">
      H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
B. Rozière, N. Goyal, E. Hambro, F. Azhar
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib11.6.3" style="font-size:90%;">
      ,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib11.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib11.8.5" style="font-size:90%;">
      Llama: Open and efficient foundation language models,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.9.6" style="font-size:90%;">
      arXiv preprint arXiv:2302.13971
     </em>
     <span class="ltx_text" id="bib.bib11.10.7" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib12.2.2.1" style="font-size:90%;">
      [12]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.4.1" style="font-size:90%;">
      W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,
Y. Zhuang, J. E. Gonzalez
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib12.6.3" style="font-size:90%;">
      , “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib12.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib12.8.5" style="font-size:90%;">
      Vicuna: An open-source
chatbot impressing gpt-4 with 90%* chatgpt quality,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.9.6" style="font-size:90%;">
      See
https://vicuna. lmsys. org (accessed 14 April 2023)
     </em>
     <span class="ltx_text" id="bib.bib12.10.7" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib13.2.2.1" style="font-size:90%;">
      [13]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.4.1" style="font-size:90%;">
      D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and
J. Steinhardt, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib13.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib13.6.3" style="font-size:90%;">
      Measuring massive multitask language
understanding,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.7.4" style="font-size:90%;">
      arXiv preprint arXiv:2009.03300
     </em>
     <span class="ltx_text" id="bib.bib13.8.5" style="font-size:90%;">
      , 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib14.2.2.1" style="font-size:90%;">
      [14]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.4.1" style="font-size:90%;">
      H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang,
M. Dehghani, S. Brahma
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib14.6.3" style="font-size:90%;">
      , “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib14.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib14.8.5" style="font-size:90%;">
      Scaling
instruction-finetuned language models,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.9.6" style="font-size:90%;">
      arXiv preprint
arXiv:2210.11416
     </em>
     <span class="ltx_text" id="bib.bib14.10.7" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib15.2.2.1" style="font-size:90%;">
      [15]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.4.1" style="font-size:90%;">
      Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and
H. Hajishirzi, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib15.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib15.6.3" style="font-size:90%;">
      Self-instruct: Aligning language model with self
generated instructions,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.7.4" style="font-size:90%;">
      arXiv preprint arXiv:2212.10560
     </em>
     <span class="ltx_text" id="bib.bib15.8.5" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib16.2.2.1" style="font-size:90%;">
      [16]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.4.1" style="font-size:90%;">
      C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib16.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib16.6.3" style="font-size:90%;">
      Wizardlm: Empowering large language models to follow complex
instructions,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.7.4" style="font-size:90%;">
      arXiv preprint arXiv:2304.12244
     </em>
     <span class="ltx_text" id="bib.bib16.8.5" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib17.2.2.1" style="font-size:90%;">
      [17]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.4.1" style="font-size:90%;">
      L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
S. Agarwal, K. Slama, A. Ray
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib17.6.3" style="font-size:90%;">
      , “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib17.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib17.8.5" style="font-size:90%;">
      Training language
models to follow instructions with human feedback,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.9.6" style="font-size:90%;">
      Advances in
Neural Information Processing Systems
     </em>
     <span class="ltx_text" id="bib.bib17.10.7" style="font-size:90%;">
      , vol. 35, pp. 27 730–27 744, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib18.2.2.1" style="font-size:90%;">
      [18]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.4.1" style="font-size:90%;">
      H. Yang, S. Yue, and Y. He, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib18.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib18.6.3" style="font-size:90%;">
      Auto-GPT for Online Decision Making:
Benchmarks and Additional Opinions,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.7.4" style="font-size:90%;">
      arXiv preprint
arXiv:2306.02224
     </em>
     <span class="ltx_text" id="bib.bib18.8.5" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib19.2.2.1" style="font-size:90%;">
      [19]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.4.1" style="font-size:90%;">
      Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib19.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib19.6.3" style="font-size:90%;">
      Hugginggpt:
Solving ai tasks with chatgpt and its friends in huggingface,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.7.4" style="font-size:90%;">
      arXiv
preprint arXiv:2303.17580
     </em>
     <span class="ltx_text" id="bib.bib19.8.5" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib20.2.2.1" style="font-size:90%;">
      [20]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib20.4.1" style="font-size:90%;">
      C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, and M. Sun,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib20.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib20.6.3" style="font-size:90%;">
      Communicative Agents for Software Development,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.7.4" style="font-size:90%;">
      arXiv
preprint arXiv:2307.07924
     </em>
     <span class="ltx_text" id="bib.bib20.8.5" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib21.2.2.1" style="font-size:90%;">
      [21]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.4.1" style="font-size:90%;">
      S. Hong, X. Zheng, J. Chen, Y. Cheng, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin,
L. Zhou, C. Ran
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib21.6.3" style="font-size:90%;">
      , “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib21.7.4">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib21.8.5" style="font-size:90%;">
      MetaGPT: Meta Programming for
Multi-Agent Collaborative Framework,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.9.6" style="font-size:90%;">
      arXiv preprint
arXiv:2308.00352
     </em>
     <span class="ltx_text" id="bib.bib21.10.7" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib22.2.2.1" style="font-size:90%;">
      [22]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.4.1" style="font-size:90%;">
      J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
Bernstein, “
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib22.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib22.6.3" style="font-size:90%;">
      Generative agents: Interactive simulacra of human
behavior,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.7.4" style="font-size:90%;">
      arXiv preprint arXiv:2304.03442
     </em>
     <span class="ltx_text" id="bib.bib22.8.5" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib23.2.2.1" style="font-size:90%;">
      [23]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.4.1" style="font-size:90%;">
      G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem,
“
     </span>
     <span class="ltx_ERROR undefined" id="bib.bib23.5.2">
      \titlecap
     </span>
     <span class="ltx_text" id="bib.bib23.6.3" style="font-size:90%;">
      CAMEL: Communicative Agents for "Mind" Exploration of Large Scale
Language Model Society,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     <span class="ltx_text" id="bib.bib24.2.2.1" style="font-size:90%;">
      [24]
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.4.1" style="font-size:90%;">
      R. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu,
D. Terzopoulos, L. Fei-Fei
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.5.2" style="font-size:90%;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib24.6.3" style="font-size:90%;">
      , “MindAgent: Emergent Gaming
Interaction,”
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.7.4" style="font-size:90%;">
      arXiv preprint arXiv:2309.09971
     </em>
     <span class="ltx_text" id="bib.bib24.8.5" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
  </ul>
 </section>
</article>
