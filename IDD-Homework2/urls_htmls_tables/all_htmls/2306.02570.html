<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.02570] When Decentralized Optimization Meets Federated Learning</title><meta property="og:description" content="Federated learning is a new learning paradigm for extracting knowledge from distributed data. Due to its favorable properties in preserving privacy and saving communication costs, it has been extensively studied and wi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="When Decentralized Optimization Meets Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="When Decentralized Optimization Meets Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.02570">

<!--Generated on Thu Feb 29 02:35:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content=" communication,  computation,  data distribution, 
decentralization,  federated learning,  optimization.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">When Decentralized Optimization Meets 
<br class="ltx_break">Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongchang Gao,
My T. Thai, 
and Jie Wu
</span><span class="ltx_author_notes">Hongchang Gao and Jie Wu are with Temple University; My T. Thai is with University of Florida. This submission was accepted to IEEE Network in January 2023.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning is a new learning paradigm for extracting knowledge from distributed data. Due to its favorable properties in preserving privacy and saving communication costs, it has been extensively studied and widely applied to numerous data analysis applications. However, most existing federated learning approaches concentrate on the centralized setting, which is vulnerable to a single-point failure.
An alternative strategy for addressing this issue is the decentralized communication topology. In this article, we systematically investigate the challenges and opportunities when renovating decentralized optimization for federated learning. In particular, we discussed them from the model, data, and communication sides, respectively, which can deepen our understanding about decentralized federated learning.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6> communication, computation, data distribution,
decentralization, federated learning, optimization.

</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">With the development of Internet-of-Things (IoT) devices and intelligent hardware, various data are generated on these devices every day. Extracting useful knowledge from these distributed data with machine learning (ML) models to benefit data owners becomes necessary and important.
Federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> provides a feasible way for this distributed ML task with a promise of protecting private information without consuming large communication costs.
Due to this favorable property, FL has been extensively studied and widely applied to many applications, such as virtual keyboard input suggestion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and smart healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, to name a few.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">In FL, a commonly used approach to coordinate the collaboration between all participants is federated averaging (FedAvg). In detail, the central server broadcasts the model parameter to all participants, i.e., data owners. Each participant updates the received model parameter for multiple iterations by the stochastic gradient computed with its local data, and then uploads the updated model parameter to the central server. After receiving the updated model parameters from all participants, the central server broadcasts the averaged model parameters to start the next round. With this learning paradigm, all participants can collaboratively learn an ML model without communicating their raw data. As such, the private information in raw data can be preserved to some extent. Meanwhile, since the model is shared and its size is much smaller than the raw data, the communication cost in FL is reduced significantly.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Along with such an extensive study of FL, federated optimization was born to further address the computation and communication challenges in FedAvg. Similar to the early phase of FL where focus is on the centralized setting, most of the work in this area concentrates on the parameter-server communication topology, where all participants communicate with the central server. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> studied the resource and performance optimization in centralized federated learning. This kind of centralized communication topology, unfortunately, may lead to a single-point failure. In particular, when the number of participants is large, communicating with the central server will cause the communication bottleneck on the central server.
With the advance of communication technology, such as 5G/6G, providing fast communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and cloud/edge computation through decentralized computation over IoT and edge devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, an alternative strategy is to employ the decentralized communication strategy where all participants perform the peer-to-peer (P2P) communication. As such, the communication bottleneck will be alleviated. Thus, the decentralized learning paradigm brings new opportunities to the FL development.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">In fact, decentralized optimization has been extensively studied in both ML and optimization communities for many years. Numerous decentralized optimization approaches have been developed for the conventional distributed ML model. However, FL brings new challenges to the conventional decentralized optimization. Just as shown in Figure <a href="#Sx1.F1" title="Figure 1 ‣ Introduction ‣ When Decentralized Optimization Meets Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, decentralized optimization serves as the bridge between distributed data and FL models. It should address the unique challenges in the model and data, as well as the issues in itself.
Even though some efforts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> have been devoted to facilitating decentralized optimization for FL in the past few years, numerous challenges are still untouched.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">To advance the decentralized FL, in this article, we will review the current development of decentralized federated optimization approaches and then discuss the new opportunities in decentralized FL. Specifically, this article will focus on the following aspects.</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">On the model side, how to improve the FL model’s generalization performance with decentralized optimization approaches was discussed, pointing out the directions for new algorithmic designs.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">On the communication side, various communication issues when applying decentralized optimization approaches to FL and potential techniques for addressing them were systematically discussed.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">On the data side, we discussed the current challenges and future directions when designing new decentralized optimization approaches for FL.</p>
</div>
</li>
</ul>
<p id="Sx1.p5.2" class="ltx_p">Following this, we introduce the background of federated learning and decentralized optimization. Then, we discuss the fundamental challenges and potential techniques in optimization algorithms for decentralized FL. In addition, we introduce challenging issues in communication of decentralized FL. Finally, we discuss how to handle different kinds of data in decentralized FL.</p>
</div>
<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2306.02570/assets/x1.png" id="Sx1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="333" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The illustration of decentralized optimization for federated learning. The decentralized optimization unifies FL models, distributed data, and communication together. In particular, the optimization algorithm on each device bridges the model and data by using the stochastic gradient, which is computed on local data and model, to update local model parameters. Meanwhile, the optimization algorithms across devices can unify all models and data in the entire system via the communication framework to learn a well-generalizing machine learning model. 
</figcaption>
</figure>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Background</h2>

<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Federated Optimization</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">Different from conventional distributed learning approaches under the data-centre setting, FL faces more computation and communication problems, such as unstable communication conditions, and highly heterogeneous data distributions, to name a few. A wide variety of federated optimization approaches have been proposed to address these challenging issues in FL. For instance, to improve the computation complexity, a line of research is to employ advanced gradient estimators, such as the momentum, the variance-reduced gradient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, to update the model parameter in each participant. As such, the convergence rate of the improved FedAvg is even able to close to the full gradient descent approach. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> can achieve the same-order sublinear convergence rate with the full-gradient descent approach for nonconvex problems. As for the communication complexity, a lot of efforts have been devoted to reducing the communication cost in each communication round and the total number of communication rounds. Moreover, other unique challenges in FL, such as model personalization, communication security, have also been extensively studied in the past few years. However, all these approaches just focus on the centralized setting, sharing the single-point failure issue.</p>
</div>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Decentralized Optimization</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">Before the era of FL, decentralized optimization has already been studied for several decades and has been applied to different domains, such as machine learning, automatic control, etc. Different from the aforementioned federated optimization approach where a central server coordinates all participants, the decentralized optimization approach does not have such a central server, where each participant directly communicates with its neighboring participants. Based on this communication paradigm, numerous decentralized optimization approaches have been proposed.</p>
</div>
<div id="Sx2.SSx2.p2" class="ltx_para">
<p id="Sx2.SSx2.p2.1" class="ltx_p">Typically, according to the specific communication strategy, decentralized optimization approaches can be categorized into two classes. The first category employs the <span id="Sx2.SSx2.p2.1.1" class="ltx_text ltx_font_italic">gossip</span> communication strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Specifically, each participant computes the gradient based on its local dataset, which is used to update its model parameter. Then, each participant communicates the updated model parameter with its neighboring participants. The second category employs the <span id="Sx2.SSx2.p2.1.2" class="ltx_text ltx_font_italic">gradient tracking</span> communication strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In particular, each participant introduces an additional variable to track the global gradient, which is employed to update the local model parameter. As such, in each iteration, the participant should communicate both the model parameter and the tracked gradient. Compared with the gossip-based approach, the tracked gradient is a better approximation for the global gradient. As such, the gradient tracking is preferable when the data distribution across participants is heterogeneous.</p>
</div>
<div id="Sx2.SSx2.p3" class="ltx_para">
<p id="Sx2.SSx2.p3.1" class="ltx_p">Based on the aforementioned two communication strategies, a wide variety of decentralized optimization approaches have been proposed. For instance, the most straightforward decentralized optimization approach employs the full gradient to update local model parameters and then conducts communication at every iteration. However, the full gradient descent approach suffers from large computational cost in each iteration when the number of samples is large. To handle the large-scale data, a line of research is to employ the stochastic gradient to update the model parameter in each participant. As such, the computational cost is reduced significantly in each iteration. In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> theoretically demonstrated that decentralized stochastic gradient descent (DSGD) algorithm has almost the same convergence rate with the centralized counterpart for nonconvex optimization problems and the decentralized communication topology only affects the high-order term of the convergence rate of DSGD. Such a favorable convergence rate of DSGD promotes the development of decentralized federated learning in the past few years. Nevertheless, the stochastic gradient introduces large variance so that the convergence rate is inferior to the full-gradient-based decentralized optimization approach. To address this drawback, multiple variance-reduced approaches have been developed to accelerate the convergence rate of decentralized stochastic gradient descent.</p>
</div>
</section>
<section id="Sx2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Integration of Decentralized Optimization and Federated Learning</h3>

<div id="Sx2.SSx3.p1" class="ltx_para">
<p id="Sx2.SSx3.p1.1" class="ltx_p">Most existing federated optimization approaches concentrate on the centralized setting, which suffers from the intrinsic problems of the centralized system. Thus, integrating decentralized optimization with FL becomes inevitable and promising. Formally, for decentralized FL, each participant conducts the following steps in each communication round:</p>
<ul id="Sx2.I2" class="ltx_itemize">
<li id="Sx2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I2.i1.p1" class="ltx_para">
<p id="Sx2.I2.i1.p1.2" class="ltx_p">It computes stochastic gradient based on its local dataset and leverages it to update its model parameter. This local updates is conducted for <math id="Sx2.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="Sx2.I2.i1.p1.1.m1.1a"><mi id="Sx2.I2.i1.p1.1.m1.1.1" xref="Sx2.I2.i1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx2.I2.i1.p1.1.m1.1b"><ci id="Sx2.I2.i1.p1.1.m1.1.1.cmml" xref="Sx2.I2.i1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I2.i1.p1.1.m1.1c">p</annotation></semantics></math> iterations, where <math id="Sx2.I2.i1.p1.2.m2.1" class="ltx_Math" alttext="p&gt;1" display="inline"><semantics id="Sx2.I2.i1.p1.2.m2.1a"><mrow id="Sx2.I2.i1.p1.2.m2.1.1" xref="Sx2.I2.i1.p1.2.m2.1.1.cmml"><mi id="Sx2.I2.i1.p1.2.m2.1.1.2" xref="Sx2.I2.i1.p1.2.m2.1.1.2.cmml">p</mi><mo id="Sx2.I2.i1.p1.2.m2.1.1.1" xref="Sx2.I2.i1.p1.2.m2.1.1.1.cmml">&gt;</mo><mn id="Sx2.I2.i1.p1.2.m2.1.1.3" xref="Sx2.I2.i1.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.I2.i1.p1.2.m2.1b"><apply id="Sx2.I2.i1.p1.2.m2.1.1.cmml" xref="Sx2.I2.i1.p1.2.m2.1.1"><gt id="Sx2.I2.i1.p1.2.m2.1.1.1.cmml" xref="Sx2.I2.i1.p1.2.m2.1.1.1"></gt><ci id="Sx2.I2.i1.p1.2.m2.1.1.2.cmml" xref="Sx2.I2.i1.p1.2.m2.1.1.2">𝑝</ci><cn type="integer" id="Sx2.I2.i1.p1.2.m2.1.1.3.cmml" xref="Sx2.I2.i1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I2.i1.p1.2.m2.1c">p&gt;1</annotation></semantics></math>.</p>
</div>
</li>
<li id="Sx2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I2.i2.p1" class="ltx_para">
<p id="Sx2.I2.i2.p1.1" class="ltx_p">When the local update is done, each participant communicates its local model parameter or tracked gradient with its neighboring participants according to the employed communication strategy.</p>
</div>
</li>
</ul>
<p id="Sx2.SSx3.p1.2" class="ltx_p">Obviously, this decentralized communication strategy avoids communicating with the central server so that there are no communication bottleneck and failure issues in the central server as the centralized federated learning. 
However, this integration introduces new challenges to decentralized optimization. In particular, as the bridge between the upper-level FL models and the lower-level distributed data, decentralized optimization faces with a wide variety of challenges. Just as shown in Figure <a href="#Sx1.F1" title="Figure 1 ‣ Introduction ‣ When Decentralized Optimization Meets Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, various FL models require to develop new decentralized optimization approaches to achieve good generalization performance. The complicated data distributions make conventional decentralized optimization approaches not work. The decentralized communication under the FL setting requires new algorithmic design to handle new communication challenges. In the following, we will systematically discuss these challenges and potential techniques to address them from the perspective of the model, communication, and data, which will help FL researchers and practitioners deepen their understanding of decentralized FL.</p>
</div>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Decentralized Optimization Meets Models</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The goal of FL is to learn a well-performing ML model for the real-world application. To deal with different kinds of applications, numerous FL models have been developed. How to ensure the decentralized optimization approach to learn a well-generalized ML model is important and challenging.
In what follows, we systematically discuss the fundamental challenges and potential techniques for addressing them.</p>
</div>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">How to Achieve Good Generalization Performance?</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.1" class="ltx_p">The ultimate goal of an ML model is to have good generalization performance. To improve the generalization performance, a lot of efforts have been devoted to the design of ML models. In recent years, the over-parameterized deep neural network has demonstrated superior generalization performance. As such, it has been applied to various FL applications. In turn, it also introduces new challenges to decentralized federated optimization. Specifically, the over-parameterized deep neural network has many local minima where different local minima have different generalization performance. Thus, it is of importance to find the local minima that have good generalization performance.</p>
</div>
<div id="Sx3.SSx1.p2" class="ltx_para">
<p id="Sx3.SSx1.p2.1" class="ltx_p">Existing federated optimization approaches, including both centralized and decentralized ones, mainly concentrate on the convergence performance. That is how fast an optimization algorithm converges to the local minima. In fact, other than the convergence speed, a decentralized optimization approach should also have the capability to find the local minima with good generalization performance. Thus, both convergence and generalization performance are of importance when designing decentralized optimization approaches for FL. In what follows, we list the essential aspects that need to investigate for the development of decentralized optimization approaches.</p>
</div>
<div id="Sx3.SSx1.p3" class="ltx_para">
<ul id="Sx3.I3" class="ltx_itemize">
<li id="Sx3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx3.I3.i1.p1" class="ltx_para">
<p id="Sx3.I3.i1.p1.1" class="ltx_p"><span id="Sx3.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Adaptation of existing approaches:</span> In recent years, a few new optimization approaches under the single-machine setting have been proposed to pursue the solution that has good generalization performance. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> developed a sharpness-aware optimization approach to find the flat minima, since <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> empirically demonstrated that the flat minima enjoy better generalization performance than sharp minima.
A straightforward strategy to empower decentralized optimization approaches with the capability of finding well-generalized solutions is to adapt existing single-machine approaches, e.g., the sharpness-aware approach, to the decentralized FL. However, this naive strategy may not work for decentralized FL. For instance, the sharpness-aware optimization approach has more computational cost due to the maximization and minimization steps in each iteration. Then, the limited computation capability of the participants, e.g., mobile devices, restricts its adaptation to decentralized FL. Moreover, the heterogeneous data distribution across participants also introduces new challenges when coordinating the maximization and minimization steps in each iteration.
Thus, adapting existing approaches to decentralized FL requires new efforts to address the computation and communication issues.
</p>
</div>
</li>
<li id="Sx3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx3.I3.i2.p1" class="ltx_para">
<p id="Sx3.I3.i2.p1.1" class="ltx_p"><span id="Sx3.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Co-design of FL models and decentralized optimization approaches:</span> Other than adapting existing approaches to decentralized FL, another promising strategy is to co-design the FL model and the decentralized optimization approach to pursue the well-generalized solution. On the one hand, when designing a FL model with good generalization performance, the model that is easy to be parallelized should be preferable. Especially, it should avoid employing the global information, e.g., the rank across all samples,
since it is difficult for decentralized optimization approaches to get the global information. On the other hand, developing new decentralized optimization approaches for optimizing the well-generalized FL models should be computation-efficient and communication-efficient. For instance, when the FL model requires the global information, it is necessary to employ some strategies to approximate it to avoid the frequent communication across participants.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">How to Handle Big Models?</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">To pursue the well-generalized ML model, a surge of interest focuses on developing big models. Specifically, the big model has a huge number of model parameters and it is trained with the huge volume of training data. For instance, GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> has 175 billion model parameters and it is trained with 45TB training data.
With such large model size and training data, big models enjoy superior generalization performance. For instance, GPT-3 can achieve great generalization performance for few-shot and zero-shot learning. Thus, adapting big models for FL can benefit a wide variety of real-world applications.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.p2.1" class="ltx_p">However, the big model incurs new challenges for decentralized FL due to its large model size and the huge volume of training data. Directly deploying big models to decentralized FL seems infeasible since the computation capability of the participants is limited. Moreover, training big models with decentralized FL requires a huge number of participants to get enough training data. Such kinds of large-scale distributed data is more likely to be heterogeneous. Without a central server, it is difficult for a decentralized FL system to get the global information to address the heterogeneous issue. Thus, it is difficult to train big models with decentralized FL.</p>
</div>
<div id="Sx3.SSx2.p3" class="ltx_para">
<p id="Sx3.SSx2.p3.1" class="ltx_p">How to apply the promising big model to decentralized FL requires new efforts in the design of learning paradigms and corresponding decentralized optimization approaches.
In what follows, we discuss several prominent aspects to address this unique challenge.</p>
<ul id="Sx3.I4" class="ltx_itemize">
<li id="Sx3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx3.I4.i1.p1" class="ltx_para">
<p id="Sx3.I4.i1.p1.1" class="ltx_p"><span id="Sx3.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Zeroth-order approaches:</span> Since it is infeasible to train a big model under the decentralized FL setting due to the huge model and data size, a potential strategy for leveraging big models is to employ the pre-trained big models as a service provider. In particular, rather than training a big model from scratch, we can directly utilize the pre-trained big model to benefit the small model training. For instance, as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the big language model GPT-3 can generate an augmented sample for the input sample, which can be utilized for prompt tuning. Therefore, we can put the big model on each participant. Then, the participant can leverage the model output from the local input data to optimize the parameter of the prompt learning part. Since the model parameters of big models are typically not accessible, we need to develop the zeroth-order decentralized optimization approach for this kind of task. Currently, there are very few works about zeroth-order decentralized optimization approaches for FL. Thus, the systematic investigation about the computation and communication complexities of zeroth-order approaches is of immense importance and necessity.</p>
</div>
</li>
<li id="Sx3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx3.I4.i2.p1" class="ltx_para">
<p id="Sx3.I4.i2.p1.1" class="ltx_p"><span id="Sx3.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">Low-dimensional approaches:</span> Since the big model has a large number of model parameters, a potential strategy to train or fine-tune this kind of big models is to optimize model parameters in the low-dimensional space. For instance, one can employ the sketching method to project model parameters in a low-dimensional subspace and then perform optimization in such a subspace. However, this strategy causes new challenges for decentralized FL.
For instance, how fast the low-dimensional approach will converge is not clear. Hence, new efforts should be devoted to the systematic investigation on the algorithmic design and theoretical analysis about the low-dimensional decentralized federated optimization approaches.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">How to Deal with Inductive Biases? </h3>

<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.1" class="ltx_p">Although the big model is promising in improving generalization performance, it requires powerful computation capability and a large volume of training data. For some real-world FL applications, it is difficult to obtain large-scale training data. For instance, the healthcare data is typically not large enough to train a big model. To address this issue, an alternative strategy is to incorporate the inductive bias to regularize the model to have the desired performance.
Specifically, an important kind of inductive bias is to make a FL model to capture the intrinsic structure in the data. For instance, the convolutional neural network should be invariant to the translation and rotation of input samples. A high-dimensional model should be aware of the low-dimensional subspace.
Moreover, another important inductive bias is to make a FL model to capture the domain knowledge in specific applications. For instance, the graph neural network for molecular graphs should be aware of the valid subgraph structure.</p>
</div>
<div id="Sx3.SSx3.p2" class="ltx_para">
<p id="Sx3.SSx3.p2.1" class="ltx_p">To incorporate inductive biases into FL models, some models use constraint to deal with them. For instance, the low-rank matrix completion model has a trace-norm constraint to pursue a low-rank solution. Most existing decentralized optimization approaches concentrate on the unconstrained problem. How to solve the constraint problem under the decentralized and periodical communication condition is still under explored, which requires systematic investigation as follows.</p>
<ul id="Sx3.I5" class="ltx_itemize">
<li id="Sx3.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx3.I5.i1.p1" class="ltx_para">
<p id="Sx3.I5.i1.p1.1" class="ltx_p"><span id="Sx3.I5.i1.p1.1.1" class="ltx_text ltx_font_bold">Convex constraint:</span> The convex constraint is widely used in ML models to deal with inductive biases, such as the low-rank constraint. To solve the FL model with convex constraint, a critical challenge is the computation complexity when dealing with the constraint. The possible strategy includes the projection gradient descent and conditional gradient descent. However, how these approaches converge under the decentralized FL setting is still unclear. Thus, it is necessary to adapt those algorithms to decentralized FL and investigate their computation and communication complexities.</p>
</div>
</li>
<li id="Sx3.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx3.I5.i2.p1" class="ltx_para">
<p id="Sx3.I5.i2.p1.1" class="ltx_p"><span id="Sx3.I5.i2.p1.1.1" class="ltx_text ltx_font_bold">Non-convex constraint:</span> Compared with the convex constraint, non-convex constraint is much more difficult to solve since the convex combination of the solutions may not satisfy the constraint. Thus, it requires new algorithmic design to deal with the non-convex constraint. Especially, it would be better if the new algorithm does not require the global information since it is difficult to get it under the decentralized FL setting. Moreover, more efforts should be devoted to the investigation of the computation and communication complexities of this kind of decentralized federated optimization approaches.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx3.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">More Challenges</h3>

<div id="Sx3.SSx4.p1" class="ltx_para">
<p id="Sx3.SSx4.p1.1" class="ltx_p">Other than the generalization issue, there are some other challenges in decentralized FL, e.g., the fairness issue. In particular, even though machine learning has achieved remarkable success in many real-world applications, it has been observed that the prediction result could have discrimination for minority groups. To address this issue, new decentralized optimization algorithms should be explored to learn a fair machine learning model. More specifically, some efforts have been made to developing new machine learning models, which are able to guarantee individual and group fairness. Those fair machine learning models cause new challenges for decentralized optimization. For instance, some of those new models belong to the min-max optimization problem, rather than the traditional minimization problem. How to facilitate them to decentralized FL is under-explored. Especially, how the communication period affects the convergence rate is still unclear. Therefore, more endeavor is needed to establish the foundations of decentralized federated optimization for these emerging machine learning models.</p>
</div>
<div id="Sx3.SSx4.p2" class="ltx_para">
<p id="Sx3.SSx4.p2.1" class="ltx_p">Moreover, in decentralized FL, each participant might optimize multiple tasks simultaneously, i.e., the multi-objective optimization problem. How to solve the multi-objective optimization problem under the decentralized FL setting is still unexplored. Especially, the intrinsic properties in decentralized FL bring unique challenges. For instance, different participants pay different attention to those objectives. How to differentiate the tasks should be considered when designing new decentralized optimization approaches for this kind of FL applications. Meanwhile, different tasks might have different inductive biases. How to deal with those inductive biases simultaneously should also be investigated under the decentralized FL setting.</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Decentralized Optimization Meets Communication</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">In FL, different participants have different communication conditions, such as limited communication budget, large communication latency, to name a few. Adapting decentralized optimization approaches to these complicated communication conditions is of importance and necessity.</p>
</div>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Limited Communication Budget</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">For decentralized optimization, each participant should communicate its local model parameters or gradients with its neighboring participants. When the size of FL models is large, the communication cost will be high, which can degenerate the empirical convergence speed. Thus, a core research question is to reduce the communication complexity. In fact, numerous efforts have been made to improve the communication complexity of the centralized FL. However, they are not applicable to the decentralized setting, especially how those techniques affect the convergence rate of decentralized optimization approaches is not clear. To address the communication complexity issues, the following aspects should be investigated.</p>
<ul id="Sx4.I6" class="ltx_itemize">
<li id="Sx4.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx4.I6.i1.p1" class="ltx_para">
<p id="Sx4.I6.i1.p1.1" class="ltx_p"><span id="Sx4.I6.i1.p1.1.1" class="ltx_text ltx_font_bold">Reducing communication rounds:</span> To improve the communication complexity, a promising strategy is to reduce the number of communication rounds. However, the periodic communication incurs new challenges for decentralized optimization with the gradient tracking technique. In particular, the tracked gradient in conventional decentralized optimization approaches is computed based on the local gradients in two consecutive iterations. With the periodic communication, it is unclear whether the gradients in two consecutive iterations or communication rounds should be used. Thus, it is necessary to investigate different algorithmic designs and how they affect the converge rate and communication complexity.</p>
</div>
</li>
<li id="Sx4.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx4.I6.i2.p1" class="ltx_para">
<p id="Sx4.I6.i2.p1.1" class="ltx_p"><span id="Sx4.I6.i2.p1.1.1" class="ltx_text ltx_font_bold">Reducing communication cost:</span> Another commonly employed strategy is to compress the communicated variables. As such, the communication cost in each communication round is reduced significantly. How to apply the compression techniques to the decentralized communication approach in the presence of periodic communication is still under-explored. Thus, it is promising to investigate how to combine the compression technique and periodic communication strategy to reduce the communication complexity of decentralized optimization approaches.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Large Communication Latency</h3>

<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">Since different participants possess different computation and communication capabilities, there usually exists large communication latency in a FL system, which can slow down the empirical convergence speed of decentralized optimization approaches. Even though some methods have been proposed for the centralized FL, they are not applicable to the decentralized FL due to the decentralized communication strategy. Thus, it is necessary to develop new decentralized optimization approaches to deal with the large latency issue in FL.</p>
</div>
<div id="Sx4.SSx2.p2" class="ltx_para">
<p id="Sx4.SSx2.p2.1" class="ltx_p">A promising direction to address this challenge is the asynchronous communication strategy, where each participant overlaps its computation and communication. As such, the empirical convergence speed will be improved. However, there exist new challenges when employing the asynchronous communication strategy for decentralized optimization. Especially when employing the gradient tracking technique, both model parameters and tracked gradients should be communicated. As such, there exists asynchrony between computation and communication, as well as two communication procedures for model parameters and gradients. Thus, it is challenging and important to investigate how the asynchronous decentralized optimization approaches for FL converge and how large communication latency it can admit.</p>
</div>
</section>
<section id="Sx4.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Hybrid Communication Topologies</h3>

<div id="Sx4.SSx3.p1" class="ltx_para">
<p id="Sx4.SSx3.p1.1" class="ltx_p">In some real-world FL applications, both the centralized and decentralized communication topologies are utilized to leverage their advantages. In particular, the decentralized communication in a P2P structure can alleviate the single-point failure issue in the centralized one. In turn, the centralized communication is able to benefit the convergence speed. Thus, it is necessary to develop new federated optimization approaches for the hybrid communication topology. Key issues in topology design are to decide the number of communicating neighbors and to choose these neighbors.</p>
</div>
<div id="Sx4.SSx3.p2" class="ltx_para">
<p id="Sx4.SSx3.p2.1" class="ltx_p">On the one hand, under the FL setting, the new decentralized optimization approach for improving the generalization performance should be developed, and its convergence rate requires to study. In particular, how the spectral gap affects the convergence rate needs to investigate. On the other hand, the communication-efficient decentralized optimization approach under the FL setting should be studied, and the convergence rate should be established.</p>
</div>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Decentralized Optimization Meets Data</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">In FL, the training data is much more complicated than the data-centre setting. For instance, the data might be highly heterogeneous across all participants.
Moreover, in some applications, such as autonomous driving, the data are sequentially generated. All these scenarios bring new challenges for decentralized federated optimization.</p>
</div>
<section id="Sx5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Heterogeneous Data</h3>

<div id="Sx5.SSx1.p1" class="ltx_para">
<p id="Sx5.SSx1.p1.1" class="ltx_p">When different participants have different data distributions, the stochastic gradient at each participant is significantly different from the global gradient. As such, the local model parameters at different participants will converge to different stationary points. Thus, it is of importance to alleviate the heterogeneous data distribution issue to guarantee convergence. However, there does not exist a central server to get the global information. Thus, alleviating the heterogeneous issue for decentralized FL requires new algorithmic designs.</p>
</div>
<div id="Sx5.SSx1.p2" class="ltx_para">
<p id="Sx5.SSx1.p2.1" class="ltx_p">In traditional decentralized optimization, a commonly used approach to address the aforementioned issue is the gradient tracking technique. In particular,
the gradient tracking technique requires to communicate both model parameters and gradients. As such, the gradient at each participant is able to track the global gradient. The effect from the heterogeneous data distribution can be alleviated to some extent. However, under the FL setting, the communication is performed periodically. Thus, whether the gradient tracking technique can effectively track the global gradient is unclear. Therefore, it is necessary to investigate how these two strategies affect the heterogeneity term in the convergence rate. Moreover, unlike the centralized FL where it is easy to obtain the global information to alleviate the heterogeneous issue, new strategies, such as combining centralized and decentralized communication, should be investigated to address this issue.</p>
</div>
</section>
<section id="Sx5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Sequential Data</h3>

<div id="Sx5.SSx2.p1" class="ltx_para">
<p id="Sx5.SSx2.p1.1" class="ltx_p">Most existing decentralized FL models concentrate on the independent data, where different samples are independent of each other. However, in some real-world applications, there exists dependence between different samples. For instance, in autonomous driving, the car interacts with the environment, and then the data is sequentially generated. As such, there exists dependence among this kind of sequential data. In fact, this kind of application belongs to multi-agent reinforcement learning when there are multiple self-driving cars.
Typically, since the self-driving car need to interact with its surrounding cars, it is appropriate to formulate this application as a decentralized FL task.
</p>
</div>
<div id="Sx5.SSx2.p2" class="ltx_para">
<p id="Sx5.SSx2.p2.1" class="ltx_p">Traditional decentralized optimization approaches for FL just focus on the standard gradient, ignoring the dependence in the data. Thus, it is necessary to develop new decentralized optimization approaches for the federated sequential decision task. In particular, a potential direction is to study the decentralized stochastic gradient descent (SGD) with periodic communication for Markov process. Specifically, in the sequential decision task, it is typically assumed that the decision procedure follows the Markov process. As such, we should investigate how the decentralized Markov SGD converges under the periodic communication strategy. Moreover, the communication complexity should also be investigated. In particular, how the communication period affects the convergence rate should be investigated to benefit the FL practitioners.</p>
</div>
</section>
<section id="Sx5.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Multi-modal Data</h3>

<div id="Sx5.SSx3.p1" class="ltx_para">
<p id="Sx5.SSx3.p1.1" class="ltx_p">The multi-modal data is very common in real-world FL applications. Different modalities might be distributed in different participants. For instance, the healthcare data could include different types of diagnosis records, and these records sometimes are distributed in different hospitals since a patient may take CT scan in one hospital and get diagnosis in another hospital. To make predictions for these kinds of patients, we need to unify the features from all hospitals. To address such kinds of multi-modal data, the centralized FL developed the vertical FL paradigm to coordinate the feature learning across all participants. However, under the decentralized FL setting, the data owners of different modalities might not be connected directly, and there is no central server to coordinate the collaboration among data owners. Thus, the existing vertical FL paradigm does not work for the decentralized setting. New learning paradigms should be investigated to address the dependence among different data owners when making predictions.  A potential solution to address this inter-device dependence issue is to employ the hybrid communication topology where the global communication is conducted but infrequently. As such, each local device could leverage the outdated multi-modal data to do prediction. Correspondingly, new decentralized optimization approaches for this kind of FL application should be developed to address the dependence among multi-modal data, e.g., how large the outdated period can be admitted without hampering the convergence rate.</p>
</div>
</section>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Conclusions</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">In this article, we provide a comprehensive discussion about decentralized optimization approaches for federated learning. In particular, the integration of decentralized optimization and federated learning brings new challenges and opportunities. The decentralized optimization is able to address the intrinsic problems of the conventional federated learning system. In turn, federated learning provides new opportunities to boost the development of decentralized optimization. We systematically investigate these challenges and opportunities from different perspectives, including the model, data, and communication, which points out the potential research directions and can help readers deepen their understanding about decentralized federated learning.</p>
</div>
</section>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgement</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">This research was supported in part by NSF grants CNS 2214940, CPS 2128378, CNS 2107014, CNS 2150152, CNS 1824440, CNS 1828363, CNS 1935923, CNS 2140477, and IIS 939725.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ashok Cutkosky and Francesco Orabona.

</span>
<span class="ltx_bibblock">Momentum-based variance reduction in non-convex sgd.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.

</span>
<span class="ltx_bibblock">Sharpness-aware minimization for efficiently improving
generalization.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.01412</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Hongchang Gao and Heng Huang.

</span>
<span class="ltx_bibblock">Periodic stochastic gradient descent with momentum for decentralized
training.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2008.10435</span>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
and Ping Tak Peter Tang.

</span>
<span class="ltx_bibblock">On large-batch training for deep learning: Generalization gap and
sharp minima.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1609.04836</span>, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Prashant Khanduri, Pranay Sharma, Haibo Yang, Mingyi Hong, Jia Liu, Ketan
Rajawat, and Pramod Varshney.

</span>
<span class="ltx_bibblock">Stem: A stochastic two-sided momentum algorithm achieving
near-optimal sample and communication complexities for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 34, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Khaled B Letaief, Yuanming Shi, Jianmin Lu, and Jianhua Lu.

</span>
<span class="ltx_bibblock">Edge artificial intelligence for 6g: Vision, enabling technologies,
and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Journal on Selected Areas in Communications</span>, 40(1):5–36,
2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.

</span>
<span class="ltx_bibblock">Can decentralized algorithms outperform centralized algorithms? a
case study for decentralized parallel stochastic gradient descent.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Li Lin, Xiaofei Liao, Hai Jin, and Peng Li.

</span>
<span class="ltx_bibblock">Computation offloading toward edge computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 107(8):1584–1607, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Shi Pu and Angelia Nedić.

</span>
<span class="ltx_bibblock">Distributed stochastic gradient tracking methods.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Mathematical Programming</span>, 187(1):409–457, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Black-box tuning for language-model-as-a-service.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.03514</span>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yuan Wu, Yuxiao Song, Tianshun Wang, Liping Qian, and Tony QS Quek.

</span>
<span class="ltx_bibblock">Non-orthogonal multiple access assisted federated learning via
wireless power transfer: A cost-efficient approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Communications</span>, 70(4):2853–2869, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei
Wang.

</span>
<span class="ltx_bibblock">Federated learning for healthcare informatics.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Journal of Healthcare Informatics Research</span>, 5(1):1–19, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.02903</span>, 2018.

</span>
</li>
</ul>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Biographies</h2>

<div id="Sx8.p1" class="ltx_para ltx_noindent">
<p id="Sx8.p1.1" class="ltx_p">Hongchang Gao (hongchang.gao@temple.edu) is an assistant professor in the department of computer and information sciences at Temple University, USA. He received his Ph.D. degree in electrical and computer engineering from University of Pittsburgh in 2020. His research interests include machine learning, deep learning, and optimization.</p>
</div>
<div id="Sx8.p2" class="ltx_para ltx_noindent">
<p id="Sx8.p2.1" class="ltx_p">My T. Thai [IEEE Fellow] (mythai@cise.ufl.edu)
is the UF Foundation Professor and Associate Director of Nelms Institute for the Connected World at the University of Florida. Her research interests include trustworthy AI, quantum computing, and optimization.</p>
</div>
<div id="Sx8.p3" class="ltx_para ltx_noindent">
<p id="Sx8.p3.1" class="ltx_p">Jie Wu [IEEE Fellow] (jiewu@temple.edu) is the Laura H. Carnell Professor and Director of Center for Networked Computing at Temple University. He is a member of the Academia Europaea. His research interests include mobile computing and wireless networks, network trust and security, applied machine learning, and cloud computing.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.02569" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.02570" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.02570">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.02570" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.02571" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 02:35:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
