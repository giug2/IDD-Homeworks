<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.13405] Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London.</title><meta property="og:description" content="Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and an…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.13405">

<!--Generated on Tue Feb 27 05:57:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
synthetic data generation,  instance segmentation,  pick-and-place operation
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK
<span id="id1.id1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(d.lee20, w.chen21, n.rojas)@imperial.ac.uk</span>
Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup id="id2.1.id1" class="ltx_sup">st</sup> Dongmyoung Lee
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.2.id1" class="ltx_text ltx_font_italic">Dyson School of Design Engineering</span>
<br class="ltx_break"><span id="id4.3.id2" class="ltx_text ltx_font_italic">Imperial College London
<br class="ltx_break"></span>London, United Kingdom 
<br class="ltx_break">d.lee20@imperial.ac.uk
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup id="id5.1.id1" class="ltx_sup">nd</sup> Wei Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.2.id1" class="ltx_text ltx_font_italic">Dyson School of Design Engineering</span>
<br class="ltx_break"><span id="id7.3.id2" class="ltx_text ltx_font_italic">Imperial College London
<br class="ltx_break"></span>London, United Kingdom 
<br class="ltx_break">w.chen21@imperial.ac.uk
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">3<sup id="id8.1.id1" class="ltx_sup">rd</sup> Nicolas Rojas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.2.id1" class="ltx_text ltx_font_italic">Dyson School of Design Engineering</span>
<br class="ltx_break"><span id="id10.3.id2" class="ltx_text ltx_font_italic">Imperial College London
<br class="ltx_break"></span>London, United Kingdom 
<br class="ltx_break">n.rojas@imperial.ac.uk
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and annotation.
In this work, we propose a synthetic data generation method that minimizes human intervention and makes downstream image segmentation algorithms more robust by combining a generated synthetic dataset with a smaller real-world dataset (hybrid dataset).
Annotation experiments show that the proposed synthetic scene generation can diminish labelling time dramatically.
RGB image segmentation is trained with hybrid dataset and combined with depth information to produce pixel-to-point correspondence of individual segmented objects. The object to grasp is then determined by the confidence score of the segmentation algorithm. Pick-and-place experiments demonstrate that segmentation trained on our hybrid dataset (98.9%, 70%) outperforms the real dataset and a publicly available dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping success rate, respectively.
Supplementary material is available at <a target="_blank" href="https://sites.google.com/view/synthetic-dataset-generation" title="" class="ltx_ref ltx_href">https://sites.google.com/view/synthetic-dataset-generation</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
synthetic data generation, instance segmentation, pick-and-place operation

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In order for robots to be utilized in many real-world settings, they must first be able to interact with their environment. While this is a relatively simple task for restricted industrial settings, many automation settings involve objects which exhibit large variation in shape and appearance. This dramatically increases the complexity of the task, not only in terms of object identification and localization, but also in terms of grasping strategy. Further still, difficulty is compounded when multiple objects are present, introducing occlusion and depth variation to the task.
Typical approaches to multi-object grasping have used computer vision for object identification and localization, with many works utilizing deep learning (DL) to estimate object pose from RGB images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, handle object occlusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and address generalized multi-object manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The vast majority of these have relied on prior knowledge of the object in the form of 3D CAD models or topological information, at least in their training stages. This allows the problem to be simplified to regressing and refining the pose of the known object in the scene, then a pre-defined grasping strategy can be utilized.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_1.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="367" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Robot learns to grasp multiple objects in clutter and sort them into target boxes with the proposed instance segmentation algorithm.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">For these works to be applied to objects exhibiting a high degree of variation, this variation must be captured in the training setup. Unfortunately, this prior knowledge may be infeasible to obtain since it is difficult to encode in a 3D CAD model, as is the case for many organic objects such as fruits or vegetables.
Without prior knowledge of the 3D object model, images of the scene must first be segmented in order to identify at a fine-level which pixels belong to each object. From this, suitable grasping points can be extracted via stereographic reconstruction or from the depth channel of an RGB-D camera.
Although training a deep learning model to segment images is considerably easier than capturing object variation in 3D models, it still necessitates a considerable size and diversity of training data. Preparing labelled datasets for image segmentation is demanding. First, it is difficult to gather raw images with a wide range of settings and circumstances. Furthermore, the procedure of obtaining pixel-level labels makes data preparation even more expensive. To alleviate these issues, some works have utilized synthetic data generation to automatically produce and annotate usable training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, however a sim-to-real gap limits performance. Therefore a hybrid approach, combining a mixture of synthetic and real training data, may be a fruitful avenue for research.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we consider the problem of fruit grasping (summarized in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). With high intra-class variation, fruits are difficult to represent as 3D models, and incur a limiting cost when producing a suitable training dataset due to their limited shelf-life. We propose a hybrid dataset generator which utilizes a generative adversarial network (GAN), trained using the publicly available Fruit-360 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, to produce self-annotated synthetic fruit images that can be superimposed on to images of real-world fruit grasping scenes for robust image segmentation performance.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We find that (1) GAN-based synthetic dataset improves segmentation performance, even when compared to an alternative hybrid approach that utilizes the Fruit-360 dataset directly, and (2) the proposed synthetic data generator can substantially reduce the labelling time than those of human-involved annotation methods.
Finally, we evaluate the effect of the proposed dataset generator on a real-world pick-and-place task, and discover that the proposed hybrid dataset dramatically improves labelling and grasping performances.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Object detection and segmentation algorithms</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Object detection and segmentation algorithms have been investigated for many robotic applications.
Some traditional techniques, such as using histogram thresholding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and watershed-based algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, were developed for image segmentation. However, these techniques often necessitate the manual extraction of diverse features meaning that it may be inadequate when dealing with a variety of surrounding environments.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To address these, Deep Learning (DL)-based methods have been applied successfully with the development of Convolutional Neural Networks (CNNs) architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, a large set of well-distributed input images is required to accurately train these algorithms. Even if these CNN-based algorithms have attained state-of-the-art results for object recognition tasks, skewed class distribution and a scarcity of training data remain a severe problem that can cause overfitting and a lack of generalization. Sufficient data acquisition and annotation are challenging since manual image annotation comes with enormous human annotation costs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. To tackle these issues, Segment Anything Model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has been developed to segment a wide variety of images with a high-quality object masks. However, it demands a substantial amount of computational resources due to its use of Transformer architecture with high-resolution input.
Therefore, we propose a data-efficient way to handle these issues with data augmentation techniques.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Traditional data augmentation algorithms, such as affine and color transformations, have been researched to change certain basic elements of input images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. While these methods have shown significant gains in simple applications, the probability distribution of augmented data input may still be uneven leading to overfitting issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
In recent years, generative adversarial networks (GANs) and diffusion models have been applied to augment the training dataset, which can generate photo-realistic synthetic images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Diffusion models, however, take longer to generate samples since they rely on a long Markov chain of diffusion steps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Therefore, in this work, synthetic objects are produced using GAN to boost the data generation.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_2.jpg" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="277" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overall procedure for fruit grasping in clutter.</figcaption>
</figure>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">In terms of GAN, it is challenging to train the GAN algorithm because of its instability and gradient vanishing issue. To address these issues, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> developed Wasserstein GAN (WGAN) and its advanced model (WGAN-GP) employing gradient penalty, respectively. The Wasserstein distance is a novel distance metric that indicates the minimum cost to transform the distribution of fake images into the distribution of data input. The Wasserstein distance can exactly estimate the distance gap avoiding the gradient vanishing problem. However, WGAN must use weight clipping to ensure that the weight parameters are 1-Lipschitz function. Furthermore, even if WGAN increases training stability, generated images are still of poor quality and the network sometimes fails to converge. As a result, WGAN-GP is developed to improve the convergence performance by employing gradient penalty (GP) instead of weight clipping. WGAN-GP algorithm is used in this work to produce photo-realistic fruit images that are suitable for this application.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Multi-object manipulation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In real-world applications, multi-object manipulation is required to grasp various instances of same/dissimilar object class in terms of size and shape. To account for shape inconsistency, an object pose estimation algorithm based on a shape-invariant feature representation and graph matching technique was proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
However, these approaches make reliable pose estimation challenging in the presence of significant shape and color discrepancy.
As an alternative, a 3D semantic keypoint detection algorithm has been used to identify the target object at the category level to accomplish certain tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
While this method has shown promising result for household items, it still requires manual human keypoint specification for the target item.
More recently, a novel canonical representation of intra-class instance candidates is suggested to estimate object pose and size for unseen objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The main ideas were (i) a synthetic scene generation method that incorporates real-world table-top scenes and synthetic objects placed on the table in the real image, and (ii) a hybrid dataset to train a model using both synthetic and real-world data, respectively. We construct synthetic scenes in a similar way and integrate them with a little quantity of real-world scenes to produce a hybrid dataset. Furthermore, in this paper, self-annotated synthetic scenes are generated, which can boost the labelled dataset preparation without using any CAD models.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The overall procedure for multi-fruit grasping, shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A Object detection and segmentation algorithms ‣ II Related Works ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, consists of a synthetic dataset generator algorithm to produce table-top synthetic dataset, an instance segmentation algorithm trained with this synthetic data and a limited quantity of real-world images, and real-world grasping demonstration. In this section, we discuss the design of a self-annotated synthetic dataset generator algorithm among these procedures.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Object-wise image generation method</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The Fruit-360 dataset is used to implement the GAN-based object-wise image generation. Fruit-360 is a publicly available dataset of fruits and vegetables. Apple, banana, strawberry, orange, peach, and plum are selected as target categories. The number of images and unique instances for the corresponding fruit categories are (6404, 940, 1230, 479, 1722, 1767) and (13, 3, 2, 1, 3, 3), respectively.
While there are enough images and unique instances of apples to train a deep learning model, others are insufficient to do so, especially in terms of the number of unique instances, which makes it difficult to learn intra-class shape and texture disparity. We, therefore, utilize the GAN algorithm to create not only a large amount of photo-realistic dataset but also a diverse shape/texture information.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_12.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The network architecture of WGAN-GP algorithm.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_3.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Sample output of generated fruits using WGAN-GP algorithm.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">WGAN-GP algorithm is trained for 5,000 epochs and extract 10,000 images for each fruit category. The network architecture of WGAN-GP is illustrated in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Object-wise image generation method ‣ III Methodology ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The entire training is carried out on a single GPU (GeForce RTX 3070, NVIDIA). The generator receives randomly distributed latent space (normal distribution) as an input and the number of latent variables is set to 100, which is one of the most commonly used but efficient value for the latent space dimension <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p">ReLU activation function and batch normalization are applied to all convolutional layers of the generator, and LeakyReLU activation function with a negative slope coefficient <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\alpha</annotation></semantics></math> = 0.2 is applied to all convolutional layers of the discriminator. The Adam optimizer is chosen to train the WGAN-GP algorithm (learning rate = 1e-4, <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">β</mi><mn id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝛽</ci><cn type="integer" id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\beta_{1}</annotation></semantics></math> = 0.5, <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">β</mi><mn id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝛽</ci><cn type="integer" id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\beta_{2}</annotation></semantics></math> = 0.9, and mini-batches of size 32). Table. <a href="#S3.T1" title="TABLE I ‣ III-A Object-wise image generation method ‣ III Methodology ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows the elapsed time (1) to train the WGAN-GP, and (2) to sample 10,000 images for each fruit category.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.4" class="ltx_p">Instead of using the actual image size of Fruit-360 dataset (<math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mn id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><cn type="integer" id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">100</annotation></semantics></math> x <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mn id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><cn type="integer" id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">100</annotation></semantics></math>), the input image size is set to <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mn id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><cn type="integer" id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">64</annotation></semantics></math> x <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mn id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><cn type="integer" id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">64</annotation></semantics></math>. More accurate image information is obtained via cropping since redundant background data is eliminated during a generative process. Fig. <a href="#S3.F4" title="Figure 4 ‣ III-A Object-wise image generation method ‣ III Methodology ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> depicts the example output images of generated fruits. This indicates that the generated object can not only be photo-realistic but also include a wide range of shape variations and intra-class information without relying on real-world data.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_4_new.jpg" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Synthetic scene is generated by randomly pasting object-wise images into the background scenes. <span id="S3.F5.3.1" class="ltx_text ltx_font_bold">(A)</span>: Generated fruit images and segmented pixels representing the target object. <span id="S3.F5.4.2" class="ltx_text ltx_font_bold">(B)</span>: Synthetic scenes with these instances.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Elapsed time of WGAN-GP algorithm training and sampling.</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:53.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.9pt,0.4pt) scale(0.983793642682253,0.983793642682253) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Type</th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Apple</th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Banana</th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Strawberry</th>
<th id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Orange</th>
<th id="S3.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Peach</th>
<th id="S3.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Plum</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Training (s)</td>
<td id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1404.35</td>
<td id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1395.20</td>
<td id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1411.18</td>
<td id="S3.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1397.56</td>
<td id="S3.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1389.97</td>
<td id="S3.T1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1433.73</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Sampling (s)</td>
<td id="S3.T1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">68.30</td>
<td id="S3.T1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">70.68</td>
<td id="S3.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">58.91</td>
<td id="S3.T1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">67.29</td>
<td id="S3.T1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">68.52</td>
<td id="S3.T1.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">67.92</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Self-annotated synthetic scene production algorithm</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Table-top input scenes, containing pixel-level annotations of various fruits and their corresponding background scenes, are necessary to train the instance segmentation algorithm. These scenes are generated using the proposed synthetic scene production algorithm.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Our synthetic scene production algorithm is based on copy and paste (CP) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. A fundamental idea of CP algorithm is to copy instances from one image and paste them randomly into another image using a large scale-jittering method. On the other hand, the proposed algorithm copies the annotated information of an instance from each object-wise image and randomly pastes the selected number of instances into background scenes.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To extract the object itself information from each WGAN-GP output image devoid of background elements, it is essential to perform pixel-wise labelling for each object-wise image. This can be achieved by excluding a range of white values since the background of WGAN-GP output images appears as white. The pixels extracted, which do not fall within this white range, are considered as the segmented components representing the object itself within each object-wise image, as demonstrated in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-A Object-wise image generation method ‣ III Methodology ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(A).
Subsequently, synthetic table-top scenes are generated by placing these annotated objects onto randomly cropped backgrounds, as depicted in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-A Object-wise image generation method ‣ III Methodology ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(B). These table-top synthetic scenes are employed to train the instance segmentation algorithm.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_5.jpg" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="534" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Elapsed time estimation of manual annotation, interactive segmentation, and the proposed synthetic scene generation method. <span id="S3.F6.4.1" class="ltx_text ltx_font_bold">(A)</span>: Self-annotation time per each instance. <span id="S3.F6.5.2" class="ltx_text ltx_font_bold">(B)</span>: Annotation time per each instance for 6-fruit scenes. <span id="S3.F6.6.3" class="ltx_text ltx_font_bold">(C)</span>: Elapsed time estimation by the number of 10-fruit scenes.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The advantage of self-annotated synthetic dataset generator can be proved by comparing (i) the elapsed time to label the training dataset, (ii) the performance of instance segmentation algorithm, and (iii) real-world pick-and-place performance between the proposed algorithm and baseline methods.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Annotation elapsed time</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In terms of baselines, human-involved annotation methods are considered: Manual human annotation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and Click-based interactive segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
The total annotation time of the proposed method is estimated based on the elapsed time of self-annotation of WGAN-GP output images and the subsequent task of copying and pasting these annotated objects onto their corresponding backgrounds. Fig. <a href="#S3.F6" title="Figure 6 ‣ III-B Self-annotated synthetic scene production algorithm ‣ III Methodology ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(A) shows the self-annotation time of WGAN-GP output for each fruit category and the average elapsed time is 19.64ms.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Fig. <a href="#S3.F6" title="Figure 6 ‣ III-B Self-annotated synthetic scene production algorithm ‣ III Methodology ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(B), on the other hand, presents the 1-fruit labelling time for 50 multiple fruit-included scenes, comparing human-involved annotation methods with the proposed algorithm.
Click-based interactive segmentation can substantially accelerate the annotation particularly for uncluttered objects than manual annotation method, however, labelling for highly cluttered objects is relatively decelerated since multiple same-type instances are sometimes recognized as a single instance. Based on these results, overall dataset preparation time are estimated by the number of 10-fruit scenes, which are used as an actual training dataset of instance segmentation algorithm. In case of the proposed method, elapsed time is not zero when the number of 10-fruit scene is zero since WGAN-GP training and image sampling are required as a prior condition. This result shows that the proposed synthetic dataset generator can significantly diminish not only the human-annotation time but also the absolute wall-clock dataset preparation time when a large size of training dataset becomes necessary.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Average precision and recall of instance segmentation algorithm with the Real-only, Syn-only, and CP-only datasets.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:296.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.5pt,31.8pt) scale(0.823448084644412,0.823448084644412) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Metric (%)</span></th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="8"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Number of images</span></th>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<th id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">50</th>
<th id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">100</th>
<th id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">150</th>
<th id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">200</th>
<th id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">300</th>
<th id="S4.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">400</th>
<th id="S4.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">500</th>
<th id="S4.T2.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">600</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.3.1" class="ltx_tr">
<th id="S4.T2.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S4.T2.1.1.3.1.1.1" class="ltx_text">Syn-only</span></th>
<th id="S4.T2.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">AP@0.5</th>
<td id="S4.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.2</td>
<td id="S4.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.3</td>
<td id="S4.T2.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.8</td>
<td id="S4.T2.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.7</td>
<td id="S4.T2.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.6</td>
<td id="S4.T2.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.1.8.1" class="ltx_text ltx_font_bold">80.6</span></td>
<td id="S4.T2.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.5</td>
<td id="S4.T2.1.1.3.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.0</td>
</tr>
<tr id="S4.T2.1.1.4.2" class="ltx_tr">
<th id="S4.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AP@0.75</th>
<td id="S4.T2.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">59.0</td>
<td id="S4.T2.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">67.3</td>
<td id="S4.T2.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">67.9</td>
<td id="S4.T2.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r">75.0</td>
<td id="S4.T2.1.1.4.2.6" class="ltx_td ltx_align_center ltx_border_r">77.5</td>
<td id="S4.T2.1.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.4.2.7.1" class="ltx_text ltx_font_bold">77.8</span></td>
<td id="S4.T2.1.1.4.2.8" class="ltx_td ltx_align_center ltx_border_r">74.0</td>
<td id="S4.T2.1.1.4.2.9" class="ltx_td ltx_align_center ltx_border_r">68.4</td>
</tr>
<tr id="S4.T2.1.1.5.3" class="ltx_tr">
<th id="S4.T2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AP@[0.5:0.95]</th>
<td id="S4.T2.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">50.4</td>
<td id="S4.T2.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">60.1</td>
<td id="S4.T2.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">60.7</td>
<td id="S4.T2.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r">68.1</td>
<td id="S4.T2.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_r">70.5</td>
<td id="S4.T2.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.5.3.7.1" class="ltx_text ltx_font_bold">72.5</span></td>
<td id="S4.T2.1.1.5.3.8" class="ltx_td ltx_align_center ltx_border_r">65.9</td>
<td id="S4.T2.1.1.5.3.9" class="ltx_td ltx_align_center ltx_border_r">61.7</td>
</tr>
<tr id="S4.T2.1.1.6.4" class="ltx_tr">
<th id="S4.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">AR@0.5</th>
<td id="S4.T2.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.8</td>
<td id="S4.T2.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.2</td>
<td id="S4.T2.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.4</td>
<td id="S4.T2.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.2</td>
<td id="S4.T2.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.6</td>
<td id="S4.T2.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.6.4.7.1" class="ltx_text ltx_font_bold">89.9</span></td>
<td id="S4.T2.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.8</td>
<td id="S4.T2.1.1.6.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.8</td>
</tr>
<tr id="S4.T2.1.1.7.5" class="ltx_tr">
<th id="S4.T2.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AR@0.75</th>
<td id="S4.T2.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r">78.4</td>
<td id="S4.T2.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r">78.7</td>
<td id="S4.T2.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">81.1</td>
<td id="S4.T2.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r">83.2</td>
<td id="S4.T2.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.7.5.6.1" class="ltx_text ltx_font_bold">84.9</span></td>
<td id="S4.T2.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_r">84.6</td>
<td id="S4.T2.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_r">80.7</td>
<td id="S4.T2.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_r">76.2</td>
</tr>
<tr id="S4.T2.1.1.8.6" class="ltx_tr">
<th id="S4.T2.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AR@[0.5:0.95]</th>
<td id="S4.T2.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r">65.1</td>
<td id="S4.T2.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r">67.7</td>
<td id="S4.T2.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">68.2</td>
<td id="S4.T2.1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_r">73.4</td>
<td id="S4.T2.1.1.8.6.6" class="ltx_td ltx_align_center ltx_border_r">74.9</td>
<td id="S4.T2.1.1.8.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.8.6.7.1" class="ltx_text ltx_font_bold">75.1</span></td>
<td id="S4.T2.1.1.8.6.8" class="ltx_td ltx_align_center ltx_border_r">73.1</td>
<td id="S4.T2.1.1.8.6.9" class="ltx_td ltx_align_center ltx_border_r">67.1</td>
</tr>
<tr id="S4.T2.1.1.9.7" class="ltx_tr">
<th id="S4.T2.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S4.T2.1.1.9.7.1.1" class="ltx_text">CP-only</span></th>
<th id="S4.T2.1.1.9.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">AP@0.5</th>
<td id="S4.T2.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.1</td>
<td id="S4.T2.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.9</td>
<td id="S4.T2.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.2</td>
<td id="S4.T2.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.4</td>
<td id="S4.T2.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.9</td>
<td id="S4.T2.1.1.9.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.9.7.8.1" class="ltx_text ltx_font_bold">72.5</span></td>
<td id="S4.T2.1.1.9.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.4</td>
<td id="S4.T2.1.1.9.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.7</td>
</tr>
<tr id="S4.T2.1.1.10.8" class="ltx_tr">
<th id="S4.T2.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AP@0.75</th>
<td id="S4.T2.1.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r">56.8</td>
<td id="S4.T2.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r">60.2</td>
<td id="S4.T2.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r">63.8</td>
<td id="S4.T2.1.1.10.8.5" class="ltx_td ltx_align_center ltx_border_r">66.8</td>
<td id="S4.T2.1.1.10.8.6" class="ltx_td ltx_align_center ltx_border_r">70.3</td>
<td id="S4.T2.1.1.10.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.10.8.7.1" class="ltx_text ltx_font_bold">70.8</span></td>
<td id="S4.T2.1.1.10.8.8" class="ltx_td ltx_align_center ltx_border_r">67.5</td>
<td id="S4.T2.1.1.10.8.9" class="ltx_td ltx_align_center ltx_border_r">66.0</td>
</tr>
<tr id="S4.T2.1.1.11.9" class="ltx_tr">
<th id="S4.T2.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AP@[0.5:0.95]</th>
<td id="S4.T2.1.1.11.9.2" class="ltx_td ltx_align_center ltx_border_r">49.8</td>
<td id="S4.T2.1.1.11.9.3" class="ltx_td ltx_align_center ltx_border_r">55.0</td>
<td id="S4.T2.1.1.11.9.4" class="ltx_td ltx_align_center ltx_border_r">58.6</td>
<td id="S4.T2.1.1.11.9.5" class="ltx_td ltx_align_center ltx_border_r">61.5</td>
<td id="S4.T2.1.1.11.9.6" class="ltx_td ltx_align_center ltx_border_r">65.0</td>
<td id="S4.T2.1.1.11.9.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.11.9.7.1" class="ltx_text ltx_font_bold">65.6</span></td>
<td id="S4.T2.1.1.11.9.8" class="ltx_td ltx_align_center ltx_border_r">62.9</td>
<td id="S4.T2.1.1.11.9.9" class="ltx_td ltx_align_center ltx_border_r">61.0</td>
</tr>
<tr id="S4.T2.1.1.12.10" class="ltx_tr">
<th id="S4.T2.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">AR@0.5</th>
<td id="S4.T2.1.1.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.3</td>
<td id="S4.T2.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.1</td>
<td id="S4.T2.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.5</td>
<td id="S4.T2.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.0</td>
<td id="S4.T2.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.12.10.6.1" class="ltx_text ltx_font_bold">86.0</span></td>
<td id="S4.T2.1.1.12.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.8</td>
<td id="S4.T2.1.1.12.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.2</td>
<td id="S4.T2.1.1.12.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.7</td>
</tr>
<tr id="S4.T2.1.1.13.11" class="ltx_tr">
<th id="S4.T2.1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AR@0.75</th>
<td id="S4.T2.1.1.13.11.2" class="ltx_td ltx_align_center ltx_border_r">76.0</td>
<td id="S4.T2.1.1.13.11.3" class="ltx_td ltx_align_center ltx_border_r">80.8</td>
<td id="S4.T2.1.1.13.11.4" class="ltx_td ltx_align_center ltx_border_r">80.5</td>
<td id="S4.T2.1.1.13.11.5" class="ltx_td ltx_align_center ltx_border_r">81.6</td>
<td id="S4.T2.1.1.13.11.6" class="ltx_td ltx_align_center ltx_border_r">82.5</td>
<td id="S4.T2.1.1.13.11.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.13.11.7.1" class="ltx_text ltx_font_bold">83.4</span></td>
<td id="S4.T2.1.1.13.11.8" class="ltx_td ltx_align_center ltx_border_r">79.8</td>
<td id="S4.T2.1.1.13.11.9" class="ltx_td ltx_align_center ltx_border_r">80.2</td>
</tr>
<tr id="S4.T2.1.1.14.12" class="ltx_tr">
<th id="S4.T2.1.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AR@[0.5:0.95]</th>
<td id="S4.T2.1.1.14.12.2" class="ltx_td ltx_align_center ltx_border_r">66.2</td>
<td id="S4.T2.1.1.14.12.3" class="ltx_td ltx_align_center ltx_border_r">69.8</td>
<td id="S4.T2.1.1.14.12.4" class="ltx_td ltx_align_center ltx_border_r">69.6</td>
<td id="S4.T2.1.1.14.12.5" class="ltx_td ltx_align_center ltx_border_r">71.4</td>
<td id="S4.T2.1.1.14.12.6" class="ltx_td ltx_align_center ltx_border_r">72.9</td>
<td id="S4.T2.1.1.14.12.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.14.12.7.1" class="ltx_text ltx_font_bold">73.4</span></td>
<td id="S4.T2.1.1.14.12.8" class="ltx_td ltx_align_center ltx_border_r">71.0</td>
<td id="S4.T2.1.1.14.12.9" class="ltx_td ltx_align_center ltx_border_r">70.5</td>
</tr>
<tr id="S4.T2.1.1.15.13" class="ltx_tr">
<th id="S4.T2.1.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S4.T2.1.1.15.13.1.1" class="ltx_text">Real-only</span></th>
<th id="S4.T2.1.1.15.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">AP@0.5</th>
<td id="S4.T2.1.1.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.0</td>
<td id="S4.T2.1.1.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.7</td>
<td id="S4.T2.1.1.15.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.0</td>
<td id="S4.T2.1.1.15.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.15.13.6.1" class="ltx_text ltx_font_bold">85.1</span></td>
<td id="S4.T2.1.1.15.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.15.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.15.13.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.15.13.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S4.T2.1.1.16.14" class="ltx_tr">
<th id="S4.T2.1.1.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AP@0.75</th>
<td id="S4.T2.1.1.16.14.2" class="ltx_td ltx_align_center ltx_border_r">81.5</td>
<td id="S4.T2.1.1.16.14.3" class="ltx_td ltx_align_center ltx_border_r">81.4</td>
<td id="S4.T2.1.1.16.14.4" class="ltx_td ltx_align_center ltx_border_r">82.5</td>
<td id="S4.T2.1.1.16.14.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.16.14.5.1" class="ltx_text ltx_font_bold">82.5</span></td>
<td id="S4.T2.1.1.16.14.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.16.14.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.16.14.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.16.14.9" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.1.1.17.15" class="ltx_tr">
<th id="S4.T2.1.1.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AP@[0.5:0.95]</th>
<td id="S4.T2.1.1.17.15.2" class="ltx_td ltx_align_center ltx_border_r">74.2</td>
<td id="S4.T2.1.1.17.15.3" class="ltx_td ltx_align_center ltx_border_r">75.7</td>
<td id="S4.T2.1.1.17.15.4" class="ltx_td ltx_align_center ltx_border_r">77.7</td>
<td id="S4.T2.1.1.17.15.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.17.15.5.1" class="ltx_text ltx_font_bold">78.4</span></td>
<td id="S4.T2.1.1.17.15.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.17.15.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.17.15.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.17.15.9" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.1.1.18.16" class="ltx_tr">
<th id="S4.T2.1.1.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">AR@0.5</th>
<td id="S4.T2.1.1.18.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.18.16.2.1" class="ltx_text ltx_font_bold">92.8</span></td>
<td id="S4.T2.1.1.18.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.1</td>
<td id="S4.T2.1.1.18.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.2</td>
<td id="S4.T2.1.1.18.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.6</td>
<td id="S4.T2.1.1.18.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.18.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.18.16.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.18.16.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S4.T2.1.1.19.17" class="ltx_tr">
<th id="S4.T2.1.1.19.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AR@0.75</th>
<td id="S4.T2.1.1.19.17.2" class="ltx_td ltx_align_center ltx_border_r">88.1</td>
<td id="S4.T2.1.1.19.17.3" class="ltx_td ltx_align_center ltx_border_r">87.6</td>
<td id="S4.T2.1.1.19.17.4" class="ltx_td ltx_align_center ltx_border_r">89.2</td>
<td id="S4.T2.1.1.19.17.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.19.17.5.1" class="ltx_text ltx_font_bold">89.4</span></td>
<td id="S4.T2.1.1.19.17.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.19.17.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.19.17.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.19.17.9" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.1.1.20.18" class="ltx_tr">
<th id="S4.T2.1.1.20.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">AR@[0.5:0.95]</th>
<td id="S4.T2.1.1.20.18.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">76.7</td>
<td id="S4.T2.1.1.20.18.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">79.7</td>
<td id="S4.T2.1.1.20.18.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">81.9</td>
<td id="S4.T2.1.1.20.18.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.1.20.18.5.1" class="ltx_text ltx_font_bold">82.9</span></td>
<td id="S4.T2.1.1.20.18.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
<td id="S4.T2.1.1.20.18.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
<td id="S4.T2.1.1.20.18.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
<td id="S4.T2.1.1.20.18.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Instance segmentation performance</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">PyTorch-based implementation for Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> is employed. ResNet-50-FPN backbone, which is pre-trained on MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, is applied to improve the performance of an instance segmentation technique.
Table-top synthetic scenes are refined using traditional data augmentation algorithms to mitigate overfitting by simulating realistic environmental conditions using the albumentation library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Input images are scaled to <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="1280" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">1280</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">1280</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">1280</annotation></semantics></math> x <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="720" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">720</annotation></semantics></math> with preserving aspect ratio using zero-padding if necessary.
The results of instance segmentation algorithms are presented using the standard evaluation metrics of average precision (AP) and recall (AR). Intersection over Union (IoU) is an important value to assess the AP and AR. It is calculated using overall regions and intersections of bounding boxes or masks. Given the wide range of cluster sizes and shapes, IoU values indicate a degree of coincidence in terms of pixels between actual and predicted target instances.
Fruit-360 dataset (CP-only), which only apply the proposed synthetic scene generation method without using GAN algorithm, is also compared with synthetic dataset (Syn-only) as a baseline like a real-world scene-based dataset (real-only). As a result, the network performances trained with real-only, CP-only, and Syn-only datasets are measured in terms of AP and AR over IoU thresholds. To evaluate the capacity to segment the region of each detected instance, AP at (1) 0.5 IoU threshold, (2) 0.75 IoU threshold, and (3) over IoU thresholds in the range from 0.5 to 0.95 with an interval of 0.05 are presented. To evaluate the performance of object detection, AR based on bounding boxes at the same IoU thresholds is reported.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_6_new.jpg" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="296" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>F1 score, which is a harmonic mean of average precision and recall, of the best real-only, CP-hybrid, and Gen-hybrid networks.</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_7.jpg" id="S4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="338" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The proposed hybrid network (Gen-hybrid) is compared with the baseline networks in terms of <span id="S4.F8.4.1" class="ltx_text ltx_font_bold">(A)</span>: Bounding box-based PR curves and <span id="S4.F8.5.2" class="ltx_text ltx_font_bold">(B)</span>: Mask-based PR curves.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">Table. <a href="#S4.T2" title="TABLE II ‣ IV-A Annotation elapsed time ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the evaluation results of segmentation algorithms trained with real-only, CP-only, and Syn-only datasets. The test set consists of 200 manually annotated real table-top scenes with different numbers and types of fruits. As for Syn-only and CP-only datasets, the performance of Syn-only dataset outperforms that of CP-only in general, especially in terms of AP. However, Syn-only network is not comparable to the real-only network performance. The main reason may be the self-annotating method. Self-annotated objects are not as accurate as human-annotated items since all the non-white pixels are segmented as an object according to the proposed self-annotating algorithm.
Furthermore, regardless of fruit category, all fruit images in the Fruit-360 dataset have a fixed resolution of <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">100</annotation></semantics></math> x <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">100</annotation></semantics></math>. Even if large scale-jittering is used, the image quality is not comparable to that of a real-world scene.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">To overcome the limitation of network training only with synthetic datasets, the hybrid dataset is proposed consisting of synthetic and real-world scenes.
We found the best hybrid networks, both for synthetic and Fruit-360 datasets, in terms of AP and AR for different numbers of real images (e.g. 50, 100, 150, and 200). The optimal network for each number of real images is found via comparing the performance of networks trained on hybrid data across a range of synthetic images, from 50 to 400 (e.g. 50, 100, 150, 200, 300, and 400). As illustrated in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the F1 score of network trained with synthetic and real-world data (Gen-hybrid) outperforms not only the real-only network but also the network trained on Fruit-360 and real-world input (CP-hybrid). The performance of Gen-hybrid network is significantly improved when the number of real image is limited (i.e., 50).</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">To evaluate the performance of real-only, Gen-hybrid, and CP-hybrid networks in detail, the precision-recall (PR) curve of these networks, when the number of real-world scenes is 200, are illustrated in Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. This result supports the fact that the performance of Gen-hybrid network substantially outperforms the baseline networks in general. While synthetic data slightly hinder the segmentation due to a lack of generated image quality in some recall ranges, synthetic data considerably aids the segmentation algorithm to gain the higher precision, especially in the range of recall from 0.0 to 0.6.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_8.jpg" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="268" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The performance comparison between real-only, CP-hybrid, and Gen-hybrid networks in terms of object category.</figcaption>
</figure>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Then, we compute F1 score (over IoU thresholds in the range from 0.5 to 0.95 with an interval of 0.05) of real-only, Gen-hybrid, and CP-hybrid networks in terms of fruit category, when the number of real-world scene is 200, as in Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The number of images and unique instances for each category in Fruit-360 dataset significantly affect the perception performance when comparing the gap between real-only and CP-hybrid. More interestingly, Gen-hybrid outperforms CP-hybrid in a similar way since GAN network is better trained as the number of images and unique instances increase. For instance, the case of apple exhibits the most considerable improvement since the amounts of images and unique instances are significantly larger than those of other fruits. It demonstrates that GAN-based synthetic dataset covers more diverse shape and texture information rather than Fruit-360 dataset itself.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Performance of point cloud segmentation methods.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:83.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(18.1pt,-5.8pt) scale(1.16214068885721,1.16214068885721) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">AP (%)</span></th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">AR (%)</span></th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">F1 score(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<td id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">K-means</td>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">57.8</td>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.2</td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.1</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<td id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DBSCAN</td>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.1</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.4</td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.4</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<td id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Gen-Hybrid</td>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">91.6</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">92.6</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">92.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_10.jpg" id="S4.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="296" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="S4.F10.4.1" class="ltx_text ltx_font_bold">(A)</span>: Target selection result, and <span id="S4.F10.5.2" class="ltx_text ltx_font_bold">(B)</span>: Grasping point selection method for the target instance.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Real-world pick-and-place operation</span>
</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Object localization</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">It is necessary to evaluate whether the proposed segmentation algorithm performs well or not in terms of point cloud to perform the real grasping experiment. Then, we compared the performance of the proposed algorithm with Euclidean clustering-based methods (K-means clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and DBSCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>) with 50 test sets. Euclidean clustering-based methods are chosen since other point cloud segmentation models require prior knowledge of CAD models or topological representations at least during the training stage. Due to the fact that these baseline methods can only get the cluster information, these methods are compared with the proposed algorithm in terms of overall AP and AR, meaning that 6 different types of fruits are all assigned as a single category for a fair comparison, as shown in Table. <a href="#S4.T3" title="TABLE III ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. In terms of AR, they are comparable with Gen-hybrid network since some pre-processing techniques are applied such as random sample consensus (RANSAC). However, despite the usage of these pre-processing functions, there is still 33.6% performance gap on average between Gen-hybrid network and baseline methods in terms of AP due to a large amount of false positive information, indicating that the segmented points of baseline methods are frequently not the actual target objects. The proposed algorithm has achieved a high F1 score of 92.1% in terms of point cloud segmentation.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Target selection</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In addition, the most graspable object should be chosen for the highly cluttered scenarios before finding the optimal grasping point. After applying the instance segmentation algorithm, graspable objects are detected based on Mask R-CNN confidence score, representing the likelihood that the prediction of instance segmentation algorithm is correct. The highest-scored instance is selected as the most graspable object as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(A).</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.7" class="ltx_p">In terms of grasping pose, it is not fully defined as we do not include any CAD models even for the training stage. Rather than directly associating the grasping pose with the overall 3D model of the instance, the desired grasping point can be defined, allowing the suction gripper to approach and grasp the target instance. Therefore, a geometry-based method is applied to acquire the optimal grasping point.
A perpendicular approach of an end-effector is required since other items can collide with the suction gripper during the grasping procedure.
Furthermore, a grasping point should be placed adjacent to the centroid of the target object. In this case, 2D <math id="S4.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS3.SSS2.p2.1.m1.1a"><mi id="S4.SS3.SSS2.p2.1.m1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.1.m1.1b"><ci id="S4.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.1.m1.1c">x</annotation></semantics></math>-<math id="S4.SS3.SSS2.p2.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS3.SSS2.p2.2.m2.1a"><mi id="S4.SS3.SSS2.p2.2.m2.1.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.2.m2.1b"><ci id="S4.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.2.m2.1c">y</annotation></semantics></math> centroid is considered since an end-effector approaches to the target instance vertically. As we proved the point cloud segmentation performance of the proposed method in Table. <a href="#S4.T3" title="TABLE III ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, 2D centroid of a target object can be predicted by the calculation of the average <math id="S4.SS3.SSS2.p2.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS3.SSS2.p2.3.m3.1a"><mi id="S4.SS3.SSS2.p2.3.m3.1.1" xref="S4.SS3.SSS2.p2.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.3.m3.1b"><ci id="S4.SS3.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS3.SSS2.p2.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.3.m3.1c">x</annotation></semantics></math>-<math id="S4.SS3.SSS2.p2.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS3.SSS2.p2.4.m4.1a"><mi id="S4.SS3.SSS2.p2.4.m4.1.1" xref="S4.SS3.SSS2.p2.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.4.m4.1b"><ci id="S4.SS3.SSS2.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS2.p2.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.4.m4.1c">y</annotation></semantics></math> position in the map of a point cloud. Then, a normal vector of each point within a certain region, which has a radius (<math id="S4.SS3.SSS2.p2.5.m5.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.SS3.SSS2.p2.5.m5.1a"><mi id="S4.SS3.SSS2.p2.5.m5.1.1" xref="S4.SS3.SSS2.p2.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.5.m5.1b"><ci id="S4.SS3.SSS2.p2.5.m5.1.1.cmml" xref="S4.SS3.SSS2.p2.5.m5.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.5.m5.1c">R</annotation></semantics></math> = 10mm) and its center is the average <math id="S4.SS3.SSS2.p2.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS3.SSS2.p2.6.m6.1a"><mi id="S4.SS3.SSS2.p2.6.m6.1.1" xref="S4.SS3.SSS2.p2.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.6.m6.1b"><ci id="S4.SS3.SSS2.p2.6.m6.1.1.cmml" xref="S4.SS3.SSS2.p2.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.6.m6.1c">x</annotation></semantics></math>-<math id="S4.SS3.SSS2.p2.7.m7.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS3.SSS2.p2.7.m7.1a"><mi id="S4.SS3.SSS2.p2.7.m7.1.1" xref="S4.SS3.SSS2.p2.7.m7.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.7.m7.1b"><ci id="S4.SS3.SSS2.p2.7.m7.1.1.cmml" xref="S4.SS3.SSS2.p2.7.m7.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.7.m7.1c">y</annotation></semantics></math> position of object points, is calculated. The point with the closest normal vector to the perpendicular unit vector in the world coordinate system, as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(B), is chosen as the optimal grasping point among the candidates near the 2D centroid.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2401.13405/assets/Figure/Figure_11.jpg" id="S4.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Real-world pick-and-place demonstration procedure.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS3.5.1.1" class="ltx_text">IV-C</span>3 </span>Real-world demonstration</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Pick-and-place experiments in cluttered settings are carried out to demonstrate the accuracy of (i) the extracted label of target object (Labelling) and (ii) segmented pixels of the target object (Grasping), as shown in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-C2 Target selection ‣ IV-C Real-world pick-and-place operation ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. The objective of the pick-and-place operation is to identify and pick up the most easily graspable object and correctly place it into the corresponding labelled box for object sorting. Labelling and grasping success rates are provided in Table. <a href="#S5.T4" title="TABLE IV ‣ V Conclusion ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. UR5 robotic arm and the customized suction gripper are used. To assess the robustness of the proposed algorithm, different instances of each fruit type are considered. In terms of the experiment setup, highly cluttered scenarios are considered with 12 stacked real-world fruits in a box. The condition of grasp failure is determined by the grasping and holding ability until the robotic arm gets to the target box. On the other hand, the state of labelling success is decided by the extracted class label of the most graspable object. Average labelling and grasping success rate of 98.9 and 70 percents are achieved, respectively.
In terms of grasping success rate, it generally follows the trend of an evaluation result in Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-B Instance segmentation performance ‣ IV Experiments ‣ Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter Dongmyoung Lee, Wei Chen, and Nicolas Rojas are with the REDS Lab, Dyson School of Design Engineering, Imperial College London, 25 Exhibition Road, London, SW7 2DB, UK (d.lee20, w.chen21, n.rojas)@imperial.ac.uk Wei Chen was supported in part by the China Scholarship Council and the Dyson School of Design Engineering, Imperial College London." class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The main reasons of grasping failure are following: (i) grasping point is occasionally imprecise in cases of occlusion since the experiment considers highly cluttered scenarios, (ii) even if the target is accurately defined, it is difficult to grasp it securely with the suction gripper when its surface is irregular or slippery, (iii) the target is sometimes soft making it vulnerable to be damaged, especially in case of strawberry, and (iv) the diameter of the suction cup may not fit to some small and light fruits. On the other hand, labelling failure is mainly caused by a similar appearance and occlusion. The proposed algorithm sometimes confuses between (i) red apple and peach or (ii) peach and plum. Other cases are primarily related to the occlusion indicating that the target is partially or fully occluded by a box or other fruits.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Robotic applications for multi-object grasping have been developed with CNN-based object localization algorithms. However, training data acquisition without access to CAD models are still challenging. We utilized WGAN-GP algorithm with self-annotation method to acquire a synthetic dataset to train instance segmentation algorithm.
The proposed synthetic scene production algorithm significantly boosts the training dataset preparation rather than human-involved annotation methods.
Our object recognition and localization methods can be applied without any 3D models even in highly cluttered scenarios. We show that synthetic scenes can be used as training input with few amount of real data to significantly improve the performance of instance segmentation.
Furthermore, while the proposed algorithm only considers RGB information, we demonstrate that it considerably outperforms traditional point cloud segmentation techniques. Real-world sorting experiments are successfully implemented to ensure the viability of the proposed network.
Further study could investigate deep learning-based 6D pose estimation without access to instance models for grasping with a robot hand, as well as more refined synthetic scene generation algorithms.
</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Sorting success rate for highly cluttered settings.</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:143.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.0pt,9.0pt) scale(0.888567535997534,0.888567535997534) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Items</span></th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Instance</span></th>
<th id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S5.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Real-only</span></th>
<th id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S5.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold">CP-Hybrid</span></th>
<th id="S5.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S5.T4.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Gen-Hybrid</span></th>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<th id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Label</th>
<th id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Grasp</th>
<th id="S5.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Label</th>
<th id="S5.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Grasp</th>
<th id="S5.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Label</th>
<th id="S5.T4.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Grasp</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.3.1" class="ltx_tr">
<th id="S5.T4.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Apple</th>
<th id="S5.T4.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">12</th>
<td id="S5.T4.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27/30</td>
<td id="S5.T4.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17/30</td>
<td id="S5.T4.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28/30</td>
<td id="S5.T4.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20/30</td>
<td id="S5.T4.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.3.1.7.1" class="ltx_text ltx_font_bold">29/30</span></td>
<td id="S5.T4.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.3.1.8.1" class="ltx_text ltx_font_bold">24/30</span></td>
</tr>
<tr id="S5.T4.1.1.4.2" class="ltx_tr">
<th id="S5.T4.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Banana</th>
<th id="S5.T4.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">6</th>
<td id="S5.T4.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">27/30</td>
<td id="S5.T4.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">17/30</td>
<td id="S5.T4.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r">29/30</td>
<td id="S5.T4.1.1.4.2.6" class="ltx_td ltx_align_center ltx_border_r">18/30</td>
<td id="S5.T4.1.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.4.2.7.1" class="ltx_text ltx_font_bold">30/30</span></td>
<td id="S5.T4.1.1.4.2.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.4.2.8.1" class="ltx_text ltx_font_bold">21/30</span></td>
</tr>
<tr id="S5.T4.1.1.5.3" class="ltx_tr">
<th id="S5.T4.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Strawberry</th>
<th id="S5.T4.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">12</th>
<td id="S5.T4.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">30/30</td>
<td id="S5.T4.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">12/30</td>
<td id="S5.T4.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r">30/30</td>
<td id="S5.T4.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_r">15/30</td>
<td id="S5.T4.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.5.3.7.1" class="ltx_text ltx_font_bold">30/30</span></td>
<td id="S5.T4.1.1.5.3.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.5.3.8.1" class="ltx_text ltx_font_bold">17/30</span></td>
</tr>
<tr id="S5.T4.1.1.6.4" class="ltx_tr">
<th id="S5.T4.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Orange</th>
<th id="S5.T4.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">6</th>
<td id="S5.T4.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">26/30</td>
<td id="S5.T4.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">15/30</td>
<td id="S5.T4.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r">28/30</td>
<td id="S5.T4.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_r">18/30</td>
<td id="S5.T4.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.6.4.7.1" class="ltx_text ltx_font_bold">30/30</span></td>
<td id="S5.T4.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.6.4.8.1" class="ltx_text ltx_font_bold">22/30</span></td>
</tr>
<tr id="S5.T4.1.1.7.5" class="ltx_tr">
<th id="S5.T4.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Peach</th>
<th id="S5.T4.1.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8</th>
<td id="S5.T4.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r">27/30</td>
<td id="S5.T4.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">15/30</td>
<td id="S5.T4.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r">28/30</td>
<td id="S5.T4.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_r">18/30</td>
<td id="S5.T4.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.7.5.7.1" class="ltx_text ltx_font_bold">29/30</span></td>
<td id="S5.T4.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.7.5.8.1" class="ltx_text ltx_font_bold">20/30</span></td>
</tr>
<tr id="S5.T4.1.1.8.6" class="ltx_tr">
<th id="S5.T4.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Plum</th>
<th id="S5.T4.1.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">12</th>
<td id="S5.T4.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r">29/30</td>
<td id="S5.T4.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">17/30</td>
<td id="S5.T4.1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_r">30/30</td>
<td id="S5.T4.1.1.8.6.6" class="ltx_td ltx_align_center ltx_border_r">19/30</td>
<td id="S5.T4.1.1.8.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.8.6.7.1" class="ltx_text ltx_font_bold">30/30</span></td>
<td id="S5.T4.1.1.8.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.8.6.8.1" class="ltx_text ltx_font_bold">22/30</span></td>
</tr>
<tr id="S5.T4.1.1.9.7" class="ltx_tr">
<th id="S5.T4.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Total</th>
<th id="S5.T4.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">56</th>
<td id="S5.T4.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">92.2%</td>
<td id="S5.T4.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">51.2%</td>
<td id="S5.T4.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">96.1%</td>
<td id="S5.T4.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">60%</td>
<td id="S5.T4.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T4.1.1.9.7.7.1" class="ltx_text ltx_font_bold">98.9%</span></td>
<td id="S5.T4.1.1.9.7.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T4.1.1.9.7.8.1" class="ltx_text ltx_font_bold">70%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox, “Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.00199</em>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Wang, D. Xu, Y. Zhu, R. Martín-Martín, C. Lu, L. Fei-Fei, and S. Savarese, “Densefusion: 6d object pose estimation by iterative dense fusion,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 3343–3352.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Luo, W. Xue, J. Chae, and G. Fu, “Skp: Semantic 3d keypoint detection for category-level robotic manipulation,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 7, no. 2, pp. 5437–5444, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Song, J. Song, and Q. Huang, “Hybridpose: 6d object pose estimation under hybrid representations,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 431–440.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P. D. Nguyen, T. Fischer, H. J. Chang, U. Pattacini, G. Metta, and Y. Demiris, “Transferring visuomotor learning from simulation to the real world for robotics manipulation tasks,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2018, pp. 6667–6674.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, “Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 12 627–12 637.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H. Mureşan and M. Oltean, “Fruit recognition from images using deep learning,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.00580</em>, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Thanammal, J. Jayasudha, R. Vijayalakshmi, and S. Arumugaperumal, “Effective histogram thresholding techniques for natural images using segmentation,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of Image and Graphics</em>, vol. 2, no. 2, pp. 113–116, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Kaur and E. G. Malik, “An image segmentation using improved fcm watershed algorithm and dbmf,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Journal of Image and Graphics</em>, vol. 2, no. 2, pp. 106–112, 2014.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. Richard, C. Fernandez Maloigne, C. Bonanomi, A. Rizzi <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Fuzzy color image segmentation using watershed transform,” <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Journal of Image and Graphics</em>, vol. 1, no. 3, pp. 157–160, 2013.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 25, pp. 1097–1105, 2012.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. Taylor and G. Nitschke, “Improving deep learning with generic data augmentation,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Symposium Series on Computational Intelligence (SSCI)</em>, 2018, pp. 1542–1547.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Segment anything,” <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.02643</em>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
E. J. Bjerrum, “Smiles enumeration as data augmentation for neural network modeling of molecules,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1703.07076</em>, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
L. Perez and J. Wang, “The effectiveness of data augmentation in image classification using deep learning,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.04621</em>, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 2223–2232.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. Song and S. Ermon, “Generative modeling by estimating gradients of the data distribution,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 32, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 8780–8794, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial networks,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2017, pp. 214–223.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville, “Improved training of wasserstein gans,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.00028</em>, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Sahin and T.-K. Kim, “Category-level 6d object pose recovery in depth images,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</em>, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Z. Qin, K. Fang, Y. Zhu, L. Fei-Fei, and S. Savarese, “Keto: Learning keypoint representations for tool manipulation,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020, pp. 7278–7285.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
H. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas, “Normalized object coordinate space for category-level 6d object pose and size estimation,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 2642–2651.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Fu and X. Wang, “Category-level 6d object pose estimation in the wild: A semi-supervised learning approach and a new dataset,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.15436</em>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
E. Agustsson, A. Sage, R. Timofte, and L. Van Gool, “Optimal transport maps for distribution preserving operations on latent spaces of generative models,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.01970</em>, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V. Le, and B. Zoph, “Simple copy-paste is a strong data augmentation method for instance segmentation,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 2918–2928.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Torralba, B. C. Russell, and J. Yuen, “Labelme: Online image annotation and applications,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 98, no. 8, pp. 1467–1484, 2010.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
K. Sofiiuk, I. A. Petrov, and A. Konushin, “Reviving iterative training with mask guidance for interactive segmentation,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Image Processing (ICIP)</em>.   IEEE, 2022, pp. 3141–3145.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
F. Massa and R. Girshick, “maskrcnn-benchmark: Fast, modular reference implementation of instance segmentation and object detection algorithms in pytorch,” <a target="_blank" href="https://github.com/facebookresearch/maskrcnn-benchmark" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/maskrcnn-benchmark</a>, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin, and A. A. Kalinin, “Albumentations: fast and flexible image augmentations,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Information</em>, vol. 11, no. 2, p. 125, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R. O. Duda, P. E. Hart <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">et al.</em>, <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">Pattern classification and scene analysis</em>.   Wiley New York, 1973, vol. 3.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M. Ester, H.-P. Kriegel, J. Sander, X. Xu <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A density-based algorithm for discovering clusters in large spatial databases with noise.” in <em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">kdd</em>, vol. 96, no. 34, 1996, pp. 226–231.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.13404" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.13405" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.13405">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.13405" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.13406" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 05:57:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
