<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Abstract</title>
<!--Generated on Thu Sep 12 15:52:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.08163v1/"/></head>
<body>
<nav class="ltx_page_navbar">
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div class="ltx_logical-block" id="id1">
<div class="ltx_para" id="id1.p1">
<br class="ltx_break ltx_align_left"/>
<p class="ltx_p ltx_align_left" id="id1.p1.1"><span class="ltx_text" id="id1.p1.1.1" style="font-size:144%;">Open Source Infrastructure for Automatic Cell Segmentation </span>
<br class="ltx_break"/></p>
<p class="ltx_p ltx_align_left" id="id1.p1.2">Aaron Rock Menezes<sup class="ltx_sup" id="id1.p1.2.1">1,2,*</sup>,
Bharath Ramsundar<sup class="ltx_sup" id="id1.p1.2.2">2,**</sup>,</p>
<br class="ltx_break ltx_align_left"/>
<p class="ltx_p ltx_align_left" id="id1.p1.3"><span class="ltx_text ltx_font_bold" id="id1.p1.3.1">1</span> Department of Biological Sciences, BITS Pilani, K.K. Birla Goa Campus</p>
<p class="ltx_p ltx_align_left" id="id1.p1.4"><span class="ltx_text ltx_font_bold" id="id1.p1.4.1">2</span> Deep Forest Sciences</p>
<br class="ltx_break ltx_align_left"/>
<p class="ltx_p ltx_align_left" id="id1.p1.5">* aaron.r.menezes@gmail.com</p>
<p class="ltx_p ltx_align_left" id="id1.p1.6">** bharath@deepforestsci.com</p>
</div>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Abstract</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Automated cell segmentation is crucial for various biological and medical applications, facilitating tasks like cell counting, morphology analysis, and drug discovery. However, manual segmentation is time-consuming and prone to subjectivity, necessitating robust automated methods. This paper presents open-source infrastructure, utilizing the UNet model, a deep-learning architecture noted for its effectiveness in image segmentation tasks. This implementation is integrated into the open-source DeepChem package, enhancing accessibility and usability for researchers and practitioners. The resulting tool offers a convenient and user-friendly interface, reducing the barrier to entry for cell segmentation while maintaining high accuracy. Additionally, we benchmark this model against various datasets, demonstrating its robustness and versatility across different imaging conditions and cell types.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">In biological and medical research, achieving precise cell segmentation is essential for tasks like cell counting, morphology analysis, and drug discovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib1" title="">1</a>]</cite>. However, manual segmentation methods are laborious, subjective, and prone to errors, especially when working with low-contrast microscopy techniques like bright field microscopy or with low-resolution imagery. These challenges prompt the need for robust automated solutions for cell segmentation. In recent years, the UNet model has emerged as a promising tool for image segmentation tasks, leveraging deep learning techniques to learn effective segmentations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib2" title="">2</a>]</cite>. Implementing and training such a model can be daunting for users who aren’t familiar with programming. Our work simplifies this process with a modular implementation that shields users from technical complexities, whilst enabling easy and convenient application of advanced image segmentation techniques. Our aim is to improve ease-of-use of automated cell segmentation, in order to help advance research across various scientific domains. Through evaluations on open-source microscopy datasets, we compare our UNet model on various open-source microscopy datasets and present a comparison with the current state-of-the-art results.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">DeepChem,<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib3" title="">3</a>]</cite>, is an open-source Python library aimed at scientific machine learning and deep learning, focusing on molecular and quantum datasets. DeepChem’s structure empowers the tackling of complex scientific challenges in areas such as drug discovery, bioinformatics, and physics. The organized framework of DeepChem has enabled it to be used for applications from molecular machine learning assessments using the MoleculeNet benchmark suite<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib4" title="">4</a>]</cite> to protein-ligand interaction modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib5" title="">5</a>]</cite>, and generative modeling of molecules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib6" title="">6</a>]</cite>, among others. DeepChem currently has limited support for image-based data. This work addresses this shortcoming by improving DeepChem’s image-handling functionality and leverages this improved functionality to build effective cell segmentation tools.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">In particular, this work integrates the UNet model in addition to tutorials for automated cell counting and segmentation to enable users to analyze image data, e.g. microscopy or biomedical imagery. In addition to the integration of the UNet model, we have also improved support for image-based datasets by improving the implementation of the <span class="ltx_text ltx_font_italic" id="Sx2.p3.1.1">ImageLoader</span>, <span class="ltx_text ltx_font_italic" id="Sx2.p3.1.2">ImageDataset</span> and <span class="ltx_text ltx_font_italic" id="Sx2.p3.1.3">ImageTransformer</span> classes in DeepChem to facilitate the loading and pre-processing of image data.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Implementation</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">We have built a full pipeline for analyzing microscopy images using DeepChem. This pipeline consists of the following parts: loading microscopy datasets using DeepChem’s <span class="ltx_text ltx_font_italic" id="Sx3.p1.1.1">ImageLoader</span> class, creating an <span class="ltx_text ltx_font_italic" id="Sx3.p1.1.2">ImageDataset</span> and pre-processing the data, and then using the UNet model to train a model on the dataset.</p>
</div>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Microscopy Datasets</h3>
<div class="ltx_para" id="Sx3.SSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.p1.1">There are many open-source microscopy datasets such as LIVECell<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib7" title="">7</a>]</cite>, the Broad Bioimage Benchmark Collection<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib8" title="">8</a>]</cite>, the ISBI Cell Tracking Challenge<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib9" title="">9</a>]</cite>. To evaluate the generalizability of our model, we utilize a diverse dataset collection encompassing various microscopy techniques and biological subjects. This includes the publicly available BBBC003v1 and BBBC039v1 datasets from the Broad Bioimage Benchmark Collection<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib8" title="">8</a>]</cite>, featuring DIC microscopy of Mouse embryos and fluorescence microscopy of the U2OS cell line of human osteosarcoma cells. We further leverage the open source datasets from the ISBI Cell Tracking Challenge<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib9" title="">9</a>]</cite>, incorporating a wider range of microscopy methods (phase contrast, fluorescence, and differential interference contrast microscopy) and cell types (mouse stem cells, HeLa cells, pancreatic stem cells, etc.). This selection allows us to test our model’s performance on distinct cell morphologies and imaging conditions, fostering a comprehensive assessment of its robustness.</p>
</div>
<figure class="ltx_table" id="Sx3.T1">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="Sx3.T1.1">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="Sx3.T1.2">-2.25in0in 

<span class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx3.T1.2.1">
<span class="ltx_thead">
<span class="ltx_tr" id="Sx3.T1.2.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T1.2.1.1.1.1.1">Dataset</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T1.2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.2.1.1.1.2.1">Cell Type</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T1.2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx3.T1.2.1.1.1.3.1">Microscopy Technique</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="Sx3.T1.2.1.2.1">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.2.1.1">BBBC003</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.2.1.2">Mouse embryos</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.2.1.3">Differential Interference Contrast</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.3.2">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.3.2.1">BBBC039</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.3.2.2">Huma osteosarcoma U2OS cells</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.3.2.3">Fluorescence</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.4.3">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.4.3.1">DIC-C2DH-HeLa</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.4.3.2">HeLa cells</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.4.3.3">Differential Interference Contrast</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.5.4">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.5.4.1">Fluo-C2DL-MSC</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.5.4.2">Rat mesenchymal stem cells</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.5.4.3">Fluorescence</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.6.5">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.6.5.1">Fluo-N2DH-GOWT1</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.6.5.2">GFP-GOWT1 Mouse stem cells</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.6.5.3">Fluorescence</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.7.6">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.7.6.1">Fluo-N2DH-HeLa</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.7.6.2">HeLa cells</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.7.6.3">Fluorescence</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.8.7">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.8.7.1">Fluo-N2DH-SIM</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.8.7.2">Human Leukemia HL60 cells</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.8.7.3">Fluorescence</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.9.8">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.9.8.1">PhC-C2DH-U373</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.9.8.2">Glioblastoma-astrocytoma U373 cells</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx3.T1.2.1.9.8.3">Phase Contrast</span></span>
<span class="ltx_tr" id="Sx3.T1.2.1.10.9">
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.2.1.10.9.1">PhC-C2DL-PSC</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T1.2.1.10.9.2">Pancreatic stem cells</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T1.2.1.10.9.3">Phase Contrast</span></span>
</span>
</span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="Sx3.T1.4.1">Summary of the datasets used in our benchmark experiments, with the types of cells imaged and microscopy techniques used to capture the images.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Image Loaders, Datasets, and Pre-Processing</h3>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">Image Loaders and Image Datasets are an integral part of the DeepChem package and allow scientists to easily access image datasets to visualize and process their data. The <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.p1.1.1">ImageLoader</span> class can be used to load data from folders and create <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.p1.1.2">ImageDatasets</span>, which can be used to access, analyze, and pre-process image datasets as well as for training and testing models. We improved the ImageDatasets to be able to work with images as both inputs and labels, a functionality that was missing prior to this work. This enables practitioners and researchers to conduct a wide array of computer vision based experiments.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p2">
<p class="ltx_p" id="Sx3.SSx2.p2.1">DeepChem also has several <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.p2.1.1">Transformers</span> which allow users to process data in various ways, such as using the <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.p2.1.2">NormalizationTransformer</span> for normalization of data or the <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.p2.1.3">ImageTransformer</span> for resizing of images.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx3">
<h3 class="ltx_title ltx_title_subsection">UNet Model</h3>
<div class="ltx_para" id="Sx3.SSx3.p1">
<p class="ltx_p" id="Sx3.SSx3.p1.1">The UNet model architecture is structured as a symmetric encoder-decoder convolutional neural network. At its core, it comprises a contracting path, where each layer progressively downsamples the input image’s spatial dimensions while increasing the number of feature channels. This contracting path consists of convolution layers followed by max-pooling operations, enabling the extraction of high-level features.</p>
</div>
<figure class="ltx_figure" id="Sx3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="348" id="Sx3.F1.g1" src="extracted/5848713/diagram.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig 1: </span>Block diagram of the UNet model architecture. The numbers show the number of channels in the image, we can see that our input and output are both 3-channel images. </figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx3.p2">
<p class="ltx_p" id="Sx3.SSx3.p2.1">To facilitate information flow between corresponding encoder and decoder layers, skip connections are employed, which concatenate feature maps from the contracting path with those from the expansive path, aiding in the preservation of spatial information and enabling precise segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib11" title="">11</a>]</cite>. Our UNet model, unlike the original implementation, pads all images to ensure spatial dimensions are consistent across all layers of the network to prevent any loss of information at the image borders. Our implementation allows users to select the number of input and output channels in addition to the optimizer, loss function, and the learning rate they wish to use.</p>
</div>
<div class="ltx_para" id="Sx3.SSx3.p3">
<p class="ltx_p" id="Sx3.SSx3.p3.1">Incorporating the UNet model into DeepChem involved utilizing PyTorch<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib12" title="">12</a>]</cite> as the backend framework. We integrated the UNet model into DeepChem, enabling researchers to access and utilize this tool for automated cell segmentation and other related tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx4">
<h3 class="ltx_title ltx_title_subsection">Cell Segmentation Pipeline</h3>
<div class="ltx_para" id="Sx3.SSx4.p1">
<p class="ltx_p" id="Sx3.SSx4.p1.1">Loading and pre-processing of data, training of the model and evaluation and inference of the model follows a simple pipeline, which can be achieved in a few lines of Python code. We describe the pipeline in Figure 1, which involves downloading and loading the data as an <span class="ltx_text ltx_font_italic" id="Sx3.SSx4.p1.1.1">ImageDataset</span> using an <span class="ltx_text ltx_font_italic" id="Sx3.SSx4.p1.1.2">ImageLoader</span>, preprocessing using the various <span class="ltx_text ltx_font_italic" id="Sx3.SSx4.p1.1.3">Transformers</span> followed by training and evaluating the UNet.</p>
</div>
<figure class="ltx_figure" id="Sx3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="111" id="Sx3.F2.g1" src="extracted/5848713/Image_Pipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig 2: </span>Overview of the cell segmentation pipeline using DeepChem. The pipeline includes data loading, pre-processing, model training, evaluation, and inference. </figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx4.p2">
<p class="ltx_p" id="Sx3.SSx4.p2.1">Users can download and process their data, and train models on it using just a few lines of code without having to refer to various packages or guides.</p>
</div>
<figure class="ltx_figure" id="Sx3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="Sx3.F3.g1" src="extracted/5848713/CodeSnippet.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig 3: </span>This is the DeepChem implementation of the Image Segmentation Pipeline as seen in Fig. 2.</figcaption>
</figure>
<figure class="ltx_table" id="Sx3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of the lines of code written to implement our pipeline using DeepChem and using only PyTorch<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib13" title="">13</a>]</cite>. </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx3.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.2">DeepChem</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.3">PyTorch</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx3.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.2.1.1">Lines of Code</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.2.1.2">9</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.2.1.3">250-300</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Results</h2>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Experimental Setup</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">We test the UNet model on various open-source datasets. All code was run using Google Colab Pro, on 1 NVIDIA A100 GPU. We report results averaged across 5 runs of the datasets mentioned above. For preprocessing the data, we normalize the pixel intensities of all the input images and ensure that the segmentation masks were binary in nature. All images are scaled down to a height of 256 and the width was scaled in proportion to the image’s original aspect ratio to the nearest multiple of 16. We use a batch size of 2 and train the models for a total of 100 epochs. All models are trained using Binary Cross Entropy Loss and the Adam optimizer with a learning rate of <math alttext="10^{-4}" class="ltx_Math" display="inline" id="Sx4.SSx1.p1.1.m1.1"><semantics id="Sx4.SSx1.p1.1.m1.1a"><msup id="Sx4.SSx1.p1.1.m1.1.1" xref="Sx4.SSx1.p1.1.m1.1.1.cmml"><mn id="Sx4.SSx1.p1.1.m1.1.1.2" xref="Sx4.SSx1.p1.1.m1.1.1.2.cmml">10</mn><mrow id="Sx4.SSx1.p1.1.m1.1.1.3" xref="Sx4.SSx1.p1.1.m1.1.1.3.cmml"><mo id="Sx4.SSx1.p1.1.m1.1.1.3a" xref="Sx4.SSx1.p1.1.m1.1.1.3.cmml">−</mo><mn id="Sx4.SSx1.p1.1.m1.1.1.3.2" xref="Sx4.SSx1.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.p1.1.m1.1b"><apply id="Sx4.SSx1.p1.1.m1.1.1.cmml" xref="Sx4.SSx1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.SSx1.p1.1.m1.1.1.1.cmml" xref="Sx4.SSx1.p1.1.m1.1.1">superscript</csymbol><cn id="Sx4.SSx1.p1.1.m1.1.1.2.cmml" type="integer" xref="Sx4.SSx1.p1.1.m1.1.1.2">10</cn><apply id="Sx4.SSx1.p1.1.m1.1.1.3.cmml" xref="Sx4.SSx1.p1.1.m1.1.1.3"><minus id="Sx4.SSx1.p1.1.m1.1.1.3.1.cmml" xref="Sx4.SSx1.p1.1.m1.1.1.3"></minus><cn id="Sx4.SSx1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="Sx4.SSx1.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.p1.1.m1.1c">10^{-4}</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx1.p1.1.m1.1d">10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>. The models are evaluated using Intersection over Union (IoU), the F1 Score and the Area under ROC (AuROC) as metrics.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Experimental Results</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">Models trained on the BBBC003 and BBBC0039 datasets from the Broad Bioimage Benchmark Collection show good performance using an 80-20 train-test split. These datasets were fairly small (16-200 images) compared to the Cell Tracking Challenge’s datasets.</p>
</div>
<figure class="ltx_table" id="Sx4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation performance of the DeepChem UNet Model on BBBC003 and BBBC039.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T3.1.1.1.1">Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx4.T3.1.1.1.2">Precision</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx4.T3.1.1.1.3">Recall</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx4.T3.1.1.1.4">F1 Score</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx4.T3.1.1.1.5">AuROC</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx4.T3.1.1.1.6">mIoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T3.1.2.1.1">BBBC003</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T3.1.2.1.2">0.7624</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T3.1.2.1.3">0.8263</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T3.1.2.1.4">0.7930</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T3.1.2.1.5">0.9888</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T3.1.2.1.6">0.6571</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T3.1.3.2.1">BBBC039</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T3.1.3.2.2">0.9086</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T3.1.3.2.3">0.9902</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T3.1.3.2.4">0.9477</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T3.1.3.2.5">0.9989</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T3.1.3.2.6">0.9006</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Sx4.SSx2.p2">
<p class="ltx_p" id="Sx4.SSx2.p2.1">We also trained the model on a subset of the Cell Tracking Challenge’s datasets. Each dataset contains 2 sets of images of cells taken over a duration of time at regular intervals. As these datasets contain images of cells over a fixed duration of time, we treated each frame as an independent image and randomly split the data for each sequence. We trained and tested the model on the 2 different sequences for each dataset due to the unavailability of the test set’s labels.</p>
</div>
<figure class="ltx_table" id="Sx4.T4">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="Sx4.T4.1">{adjustwidth}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="Sx4.T4.2">-2.25in0in


<span class="ltx_tabular ltx_centering ltx_align_middle" id="Sx4.T4.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="Sx4.T4.2.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.1.1.1">Dataset</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.1.1.2">Precision</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.1.1.3">Recall</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.1.1.4">F1 Score</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.1.1.5">AuROC</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.1.1.6">mIoU</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.1.1.7">SOTA mIoU<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib14" title="">14</a>]</cite></span></span>
<span class="ltx_tr" id="Sx4.T4.2.1.2.2">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.2.2.1">DIC-C2DH-HeLa</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.2.2.2">0.8977</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.2.2.3">0.7841</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.2.2.4">0.8371</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.2.2.5">0.9327</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.2.2.6">0.7198</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.2.2.7">0.877</span></span>
<span class="ltx_tr" id="Sx4.T4.2.1.3.3">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.3.3.1">Fluo-C2DL-MSC</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.3.3.2">0.8375</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.3.3.3">0.8059</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.3.3.4">0.8214</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.3.3.5">0.9698</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.3.3.6">0.6969</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.3.3.7">0.687</span></span>
<span class="ltx_tr" id="Sx4.T4.2.1.4.4">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.4.4.1">Fluo-N2DH-GOWT1</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.4.4.2">0.9742</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.4.4.3">0.8970</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.4.4.4">0.9340</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.4.4.5">0.9943</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.4.4.6">0.8762</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.4.4.7">0.938</span></span>
<span class="ltx_tr" id="Sx4.T4.2.1.5.5">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.5.5.1">Fluo-N2DH-HeLa</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.5.5.2">0.9476</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.5.5.3">0.9451</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.5.5.4">0.9463</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.5.5.5">0.9932</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.5.5.6">0.8982</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.5.5.7">0.923</span></span>
<span class="ltx_tr" id="Sx4.T4.2.1.6.6">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.6.6.1">Fluo-N2DH-SIM</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.6.6.2">0.8709</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.6.6.3">0.5915</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.6.6.4">0.7045</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.6.6.5">0.9395</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.6.6.6">0.5438</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.6.6.7">0.832</span></span>
<span class="ltx_tr" id="Sx4.T4.2.1.7.7">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.7.7.1">PhC-C2DH-U373</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.7.7.2">0.8854</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.7.7.3">0.9259</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.7.7.4">0.9051</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.7.7.5">0.9971</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.7.7.6">0.8267</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.2.1.7.7.7">0.931</span></span>
<span class="ltx_tr" id="Sx4.T4.2.1.8.8">
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.2.1.8.8.1">PhC-C2DL-PSC</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.2.1.8.8.2">0.9393</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.2.1.8.8.3">0.8468</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.2.1.8.8.4">0.8906</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.2.1.8.8.5">0.9960</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.2.1.8.8.6">0.8028</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.2.1.8.8.7">0.756</span></span>
</span>
</span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Evaluation performance of the DeepChem UNet Model on datasets from the Cell Tracking
Challenge 2020. Each Cell Tracking dataset has 2 captured sequences of images. We trained models
on sequence 1 and tested on sequence 2 for each dataset, as the test set is not public. SOTA numbers
are from the test set and are not directly comparable with our benchmarks but serve as a useful
comparison point.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Discussion</h2>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">The addition of the UNet model to DeepChem, in addition to the improvement in the <span class="ltx_text ltx_font_italic" id="Sx5.p1.1.1">ImageLoader</span>, <span class="ltx_text ltx_font_italic" id="Sx5.p1.1.2">ImageDataset</span> and the <span class="ltx_text ltx_font_italic" id="Sx5.p1.1.3">ImageTransformer</span>, come together to form the Cell segmentation pipeline, which is easy and intuitive to use. Our benchmarking experiments show that the model performs fairly well when compared to the best results on several datasets but still leaves room for improvement. Importantly, the model handled the diverse microscopy techniques and cell samples within the datasets effectively.</p>
</div>
<figure class="ltx_figure" id="Sx5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="Sx5.F4.g1" src="extracted/5848713/true_vs_preds.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig 4: </span>The above image compares the UNet model’s predictions with the true segmentation mask. We’ve compared 1 random sample from the Fluo-N2DH-GOWT1, Fluo-C2DL-MSC and PhC-C2DL-PSC datasets each.</figcaption>
</figure>
<section class="ltx_subsection" id="Sx5.SSx1">
<h3 class="ltx_title ltx_title_subsection">Performance and Accuracy</h3>
<div class="ltx_para" id="Sx5.SSx1.p1">
<p class="ltx_p" id="Sx5.SSx1.p1.1">The UNet model demonstrated robust performance across multiple open-source microscopy datasets. The high F1 Score and Jaccard Index (IoU) achieved in segmentation tasks affirm the model’s capability to handle diverse microscopy techniques and biological subjects. This versatility is critical, as it ensures the model’s applicability across different research domains and imaging modalities, from fluorescence microscopy to DIC microscopy.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Integration with DeepChem and Practical Implications</h3>
<div class="ltx_para" id="Sx5.SSx2.p1">
<p class="ltx_p" id="Sx5.SSx2.p1.1">Incorporating the UNet model into the DeepChem ecosystem expands the utility of this powerful open-source library. The tutorials and pre-configured pipelines provided within DeepChem facilitate ease of use, making advanced cell segmentation techniques accessible even to researchers with limited expertise in deep learning or image processing. The convenience and ease of use provided by the segmentation pipeline makes it relatively easy to be able to perform segmentation tasks when compared to other frameworks like PyTorch<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib12" title="">12</a>]</cite> or TensorFlow<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib15" title="">15</a>]</cite>, both of which require extensive knowledge of deep learning to be used effectively.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.p2">
<p class="ltx_p" id="Sx5.SSx2.p2.1">One of the primary advantages of our approach is the significant reduction in time and effort required for cell segmentation. Traditional manual segmentation is not only labor-intensive but also subject to human error and variability. By helping to automate this process, our pipeline not only accelerates the workflow but also enhances reproducibility and consistency in segmentation outcomes. This improvement is particularly beneficial for large-scale studies where manual segmentation would be impractical.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.p3">
<p class="ltx_p" id="Sx5.SSx2.p3.1">In drug discovery, precise cell segmentation can enhance the accuracy of cell counting and morphology analysis, leading to a better understanding of drug effects. In clinical settings, automated segmentation of medical images such as MRIs or X-rays can aid in diagnostics and treatment planning, potentially improving patient outcomes<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08163v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx5.SSx3">
<h3 class="ltx_title ltx_title_subsection">Limitations and Future Work</h3>
<div class="ltx_para" id="Sx5.SSx3.p1">
<p class="ltx_p" id="Sx5.SSx3.p1.1">There are several areas for future improvement. Our current implementation and evaluation are limited to a few datasets; expanding this to include a wider variety of datasets could further validate the model’s robustness. Training models on large datasets could help generalize predictions.</p>
</div>
<div class="ltx_para" id="Sx5.SSx3.p2">
<p class="ltx_p" id="Sx5.SSx3.p2.1">Future work could also explore the integration of additional machine learning models within DeepChem to handle other types of biological data, further enhancing its versatility and applicability. Moreover, continuous updates and improvements to the pre-processing and training pipelines could yield even better segmentation accuracy.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">In conclusion, our work demonstrates a significant step forward in the field of automated cell segmentation. By leveraging the power of the UNet model and integrating it into the user-friendly DeepChem framework, we provide a valuable tool for the scientific community. This advancement not only streamlines cell segmentation tasks but also opens up new possibilities for research and application in various biological and medical fields. The success of this approach highlights the potential of combining deep learning techniques with open-source scientific tools to drive innovation and efficiency in research workflows.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem"> 1.</span>
<span class="ltx_bibblock">
Bengtsson E, Wahlby C, Lindblad J.

</span>
<span class="ltx_bibblock">Robust cell image segmentation methods.

</span>
<span class="ltx_bibblock">Pattern Recognition and Image Analysis c/c of Raspoznavaniye Obrazov I Analiz Izobrazhenii. 2004;14(2):157–167.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem"> 2.</span>
<span class="ltx_bibblock">
Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation; 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem"> 3.</span>
<span class="ltx_bibblock">
Ramsundar B, Eastman P, Walters P, Pande V, Leswing K, Wu Z.

</span>
<span class="ltx_bibblock">Deep Learning for the Life Sciences.

</span>
<span class="ltx_bibblock">O’Reilly Media; 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem"> 4.</span>
<span class="ltx_bibblock">
Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, et al.. MoleculeNet: A Benchmark for Molecular Machine Learning; 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem"> 5.</span>
<span class="ltx_bibblock">
Gomes J, Ramsundar B, Feinberg EN, Pande VS. Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity; 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem"> 6.</span>
<span class="ltx_bibblock">
Frey NC, Gadepally V, Ramsundar B. FastFlows: Flow-Based Models for Molecular Graph Generation; 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem"> 7.</span>
<span class="ltx_bibblock">
Edlund C, Jackson TR, Khalid N, Bevan N, Dale T, Dengel A, et al.

</span>
<span class="ltx_bibblock">LIVECell—A large-scale dataset for label-free live cell segmentation.

</span>
<span class="ltx_bibblock">Nature methods. 2021;18(9):1038–1045.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem"> 8.</span>
<span class="ltx_bibblock">
Ljosa V, Sokolnicki KL, Carpenter AE.

</span>
<span class="ltx_bibblock">Annotated high-throughput microscopy image sets for validation.

</span>
<span class="ltx_bibblock">Nature methods. 2012;9(7):637–637.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem"> 9.</span>
<span class="ltx_bibblock">
Maška M, Ulman V, Delgado-Rodriguez P, Gómez-de Mariscal E, Nečasová T, Guerrero Peña FA, et al.

</span>
<span class="ltx_bibblock">The Cell Tracking Challenge: 10 years of objective benchmarking.

</span>
<span class="ltx_bibblock">Nature Methods. 2023;20(7):1010–1020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem"> 10.</span>
<span class="ltx_bibblock">
Long J, Shelhamer E, Darrell T.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic segmentation.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2015. p. 3431–3440.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem"> 11.</span>
<span class="ltx_bibblock">
Drozdzal M, Vorontsov E, Chartrand G, Kadoury S, Pal C.

</span>
<span class="ltx_bibblock">The importance of skip connections in biomedical image segmentation.

</span>
<span class="ltx_bibblock">In: International workshop on deep learning in medical image analysis, international workshop on large-scale annotation of biomedical data and expert label synthesis. Springer; 2016. p. 179–187.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem"> 12.</span>
<span class="ltx_bibblock">
Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems. 2019;32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem"> 13.</span>
<span class="ltx_bibblock">
Buda M, Saha A, Mazurowski MA.

</span>
<span class="ltx_bibblock">Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm.

</span>
<span class="ltx_bibblock">Computers in Biology and Medicine. 2019;109.

</span>
<span class="ltx_bibblock">doi:10.1016/j.compbiomed.2019.05.002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem"> 14.</span>
<span class="ltx_bibblock">
Cell Segmentation Benchmark;.

</span>
<span class="ltx_bibblock">Available from: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://celltrackingchallenge.net/latest-csb-results/" title="">https://celltrackingchallenge.net/latest-csb-results/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem"> 15.</span>
<span class="ltx_bibblock">
Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al.. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems; 2015.

</span>
<span class="ltx_bibblock">Available from: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tensorflow.org/" title="">https://www.tensorflow.org/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem"> 16.</span>
<span class="ltx_bibblock">
Starkuviene V, Pepperkok R.

</span>
<span class="ltx_bibblock">The potential of high-content high-throughput microscopy in drug discovery.

</span>
<span class="ltx_bibblock">British journal of pharmacology. 2007;152(1):62–71.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem"> 17.</span>
<span class="ltx_bibblock">
Wang J, Zhu H, Wang SH, Zhang YD.

</span>
<span class="ltx_bibblock">A review of deep learning on medical image analysis.

</span>
<span class="ltx_bibblock">Mobile Networks and Applications. 2021;26(1):351–380.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 15:52:05 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
