<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.08029] Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering</title><meta property="og:description" content="Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we pro…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.08029">

<!--Generated on Tue Mar 12 09:31:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ander Salaberria
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ander.salaberria@ehu.eus">ander.salaberria@ehu.eus</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gorka Azkune
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gorka.azcune@ehu.eus">gorka.azcune@ehu.eus</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Oier Lopez de Lacalle
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:oier.lopezdelacalle@ehu.eus">oier.lopezdelacalle@ehu.eus</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aitor Soroa
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:a.soroa@ehu.eus">a.soroa@ehu.eus</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eneko Agirre
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:e.agirre@ehu.eus">e.agirre@ehu.eus</a>
</span>
<span class="ltx_contact ltx_role_address">
HiTZ Basque Center for Language Technologies - Ixa NLP Group, University of the Basque Country (UPV/EHU), M. Lardizabal 1, Donostia 20018, Basque Country, Spain
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we propose to use a unimodal (text-only) train and inference procedure based on automatic off-the-shelf captioning of images and pretrained language models.
Our results on a visual question answering task which requires external knowledge (OK-VQA) show that our text-only model outperforms pretrained multimodal (image-text) models of comparable number of parameters. In contrast, our model is less effective in a standard VQA task (VQA 2.0) confirming that our text-only method is specially effective for tasks requiring external knowledge. In addition, we show that increasing the language model’s size improves notably its performance, yielding results comparable to the state-of-the-art with our largest model, significantly outperforming current multimodal systems, even though augmented with external knowledge.
Our qualitative analysis on OK-VQA reveals that automatic captions often fail to capture relevant information in the images, which seems to be balanced by the better inference ability of the text-only language models. Our work opens up possibilities to further improve inference in visio-linguistic tasks.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Visual Question Answering , Image Captioning , Language Models , Deep learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Most visio-linguistic tasks are framed in such a way that all the necessary information to solve them is in the images and texts provided in the dataset. That is the case of visual question-answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">1</span></a>]</cite> or visual entailment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2</span></a>]</cite>. In addition, some tasks require access to external knowledge in order to solve them. In this work we dive in <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">Outside Knowledge VQA</em> (OK-VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">3</span></a>]</cite>, where the image content is not sufficient to answer the questions. Contrary to self-contained VQA tasks, which can be solved grounding images and text alone, these tasks require methods that leverage external knowledge resources and are able to do inference on that knowledge.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2109.08029/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="313" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Given a question and image, we verbalize the contents of the image and apply a pretrained language model for inference. We show that current text-only models are better in generalization and inference than multimodal models for knowledge-based VQA. </span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">External knowledge useful for OK-VQA can be broadly classified into two categories, according to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a>]</cite>: (i) symbolic knowledge, which can be represented using graphs, for example ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">5</span></a>]</cite>, and (ii) implicit knowledge, which is encoded in the weights of neural networks trained in different datasets. Supporting the later case, transformer-based language models (LM) pretrained in large corpora like BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">6</span></a>]</cite> have been successfully used as implicit knowledge bases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">7</span></a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper we focus on the use of implicit knowledge in the form of pretrained LMs. While using LMs is relatively common in OK-VQA, they are usually integrated into multimodal transformers by diverse means, so as to integrate the visual and textual inputs of the task. Given that LMs were originally designed to process textual input and are extensively trained in textual corpora, we hypothesized that a system that relies exclusively on text will allow LMs to better leverage their implicit knowledge. Because OK-VQA is a visio-linguistic task, we propose to use automatic image captioning as a way to verbalize the information in the image, where the captions are descriptions of the images which are used as input to the LMs. Once the captions are generated, all the inference in our method is done using text-only models. We are aware that captions do not contain all the information in an image, and want to check whether the text-only models can compensate for that initial loss of information. The approach proposed in this paper, named Caption-based Model or CBM can be seen in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To validate our hypothesis, we present an extensive experimentation on the OK-VQA dataset, comparing our proposed caption-based model with the <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">de facto</span> standard of visio-linguistic tasks, i.e. multimodal transformers, which are widely used in VQA tasks to process the questions (text) and images. We also focus on language models of different sizes, to see the impact of model capacity on OK-VQA. As a result of our experiments, we find that:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Captions are more effective than images for OK-VQA when models of similar size are used as is, and achieve similar results when both are fine-tuned on additional VQA datasets.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Increasing the size and the capacity of language models allows to reach state-of-the-art results, outperforming by a large margin current multimodal transformers. Furthermore, we observe a trend of improvement that has not yet stabilized.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The complex use of in-context-learning as in PICa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">8</span></a>]</cite> does not beat fine-tuning our smaller model, that is, our system based on T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">9</span></a>]</cite> obtains results comparable to an ensemble of five GPT-3 runs which are 15-times larger in parameters.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">The larger contribution of captions on OK-VQA with respect to results on a regular VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">10</span></a>]</cite> show that text-only systems are specially effective when external knowledge is needed.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our code is available at <a target="_blank" href="https://github.com/salanueva/CBM" title="" class="ltx_ref ltx_href" style="color:#0000FF;">https://github.com/salanueva/CBM</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There are many <span id="S2.p1.1.1" class="ltx_text ltx_font_bold">visual question-answering datasets</span> in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">1</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">10</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">11</span></a>]</cite>, where given an image and a question about the contents of that image, a system has to provide a textual answer. Some VQA datasets also demand leveraging external knowledge to infer the answer and, thus, they are known as knowledge-based VQA tasks. Good examples are KB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">12</span></a>]</cite>, KVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">13</span></a>]</cite>, FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">14</span></a>]</cite> and OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">3</span></a>]</cite>. KVQA requires knowledge about named entities (e.g. Barack Obama, White House, United Nations) and that knowledge is already provided as a graph. FVQA annotates questions by selecting a fact from a fixed knowledge base but its size is relatively small. KB-VQA is even smaller, presenting template-based questions whose answers can be obtained reasoning over commonsense resources or Wikipedia. In contrast, OK-VQA requires knowledge from unspecified external resources and, although smaller than KVQA in terms of the number of images and question-answer pairs, it is considerably bigger than the other knowledge-based VQA datasets.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Currently, <span id="S2.p2.1.1" class="ltx_text ltx_font_bold">multimodal transformers</span> are the most successful systems for VQA and can be broadly classified into two types: single-stream and double-stream transformers. A good example of the former is VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">15</span></a>]</cite>, where the BERT architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">6</span></a>]</cite> is used, adding visual features obtained by an object detector as input and using visio-linguistic pretraining tasks, such as image-text matching. OSCAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">16</span></a>]</cite> also follows a very similar philosophy, adding object tags to the input and proposing different pretraining strategies. Among double-stream transformers, VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">17</span></a>]</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">18</span></a>]</cite> use a dedicated transformer for each modality (text and image) to fuse them with a cross-modal transformer. Their differences lie mainly on some architectural choices and pretraining task selection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">19</span></a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Regarding <span id="S2.p3.1.1" class="ltx_text ltx_font_bold">OK-VQA systems</span>, multimodal transformers have also been used to provide implicit knowledge from pretraining tasks. For example, VilBERT uses a pretrained BERT to encode the questions, so it uses the implicit knowledge that BERT acquired during its pretraining. Additionally, VilBERT is further trained on Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">20</span></a>]</cite>, a large image-caption dataset from where additional knowledge can be acquired. Those multimodal transformers are the backbone of most models used for OK-VQA, which also use symbolic knowledge to bring some extra performance.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">ConceptBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">21</span></a>]</cite> was the first system to use multimodal transformers and symbolic knowledge for OK-VQA. It is based on a combination of a pretrained BERT to encode questions, a graph convolutional neural network to encode triples extracted from the ConceptNet knowledge graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">5</span></a>]</cite> and a multimodal transformer (VilBERT) to jointly represent and reason over image features and encoded question tokens.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">A similar approach was followed by KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a>]</cite>, combining again a multimodal transformer with symbolic knowledge. In this case, the multimodal transformer, MM<sub id="S2.p5.1.1" class="ltx_sub">BERT</sub>, is based on VisualBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">15</span></a>]</cite> and initialized with the weights of a pretrained BERT. Additionally, authors built a knowledge graph fusing DBPedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">22</span></a>]</cite>, ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">5</span></a>]</cite>, VisualGenome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">23</span></a>]</cite> and hasPart KB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">24</span></a>]</cite>. They used different image feature encoders and the question tokens to obtain a subset of the full graph relevant to the target question and image. Finally, using a graph convolutional neural network, they combined the symbolic and implicit knowledge to predict the final answer.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Some recent approaches, named MAVEx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">25</span></a>]</cite> and RVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">26</span></a>]</cite> showed different ways to combine implicit and symbolic knowledge. MAVEx used a pretrained VilBERT to generate various candidate answers which were later validated using answer-specific knowledge retrieval. Authors used both textual and visual knowledge resources, including images searched using Google, sentences from Wikipedia articles, and concepts from ConceptNet. On the other hand, RVL trained the two-stream multimodal transformer LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">18</span></a>]</cite> with an auxiliary objective that aligned its representations with knowledge graph embeddings retrieved from ConceptNet and Wikidata.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Integrating annotated captions, or other types of text related to the image, in multimodal systems benefit several multimodal challenges. Examples range from fake news detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">27</span></a>]</cite> to image classification tasks such as flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">28</span></a>]</cite> and crisis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">29</span></a>]</cite> classification. However, regarding the use of automatically <span id="S2.p7.1.1" class="ltx_text ltx_font_bold">generated captions for VQA</span>, to the best of our knowledge, Mucko <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">30</span></a>]</cite> and PICa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">8</span></a>]</cite> are the only systems that explore this idea.
Mucko uses dense captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">31</span></a>]</cite> to query a knowledge graph to extract relevant information to answer the question. The reported results on OK-VQA are well below the state-of-the-art. Dense captions describe different regions of an image using short sentences. Our method differs in the use of a single caption which is the input to the LM, and does not require neither knowledge graphs nor the use of OCR systems that have recently been integrated in some other VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">32</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">33</span></a>]</cite>.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">On the other hand, PICa takes advantage of the implicit knowledge found in GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">34</span></a>]</cite> via prompt-engineering. Instead of supervised fine-tuning, PICa adapts to the task with a few in-context examples during inference time using both captions and object tags to describe the image, defining the current state-of-the-art with an ensemble of GPT-3’s and clever selection of those examples. However, GPT-3 is only accessible via OpenAI’s paid API and it has limited functionalities.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Implemented models</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we describe the implemented models. We use Pytorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">35</span></a>]</cite>, Pytorch Lightning and the Transformers library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">36</span></a>]</cite> for all the implementation work.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Caption-based model (CBM)</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our caption-based model, denoted by CBM, is divided in two steps: (i) a caption generation system that generates a short description of a given image and (ii) a language model that takes this caption and a question in order to answer it.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We use <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">OSCAR</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">16</span></a>]</cite> to generate captions from images, a transformer encoder that produces state-of-the-art results on several multimodal tasks including image captioning. As it is common in multimodal transformers, OSCAR uses a pretrained object detector called FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">37</span></a>]</cite> to obtain region features from images and their respective labels. Both features and labels alongside manually annotated captions are then fed to the transformer during pretraining, following the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">38</span></a>]</cite>. The performance on image-captioning of both base and large models is similar, so we use OSCAR-base as our image-captioning system for all of our experiments.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">During OSCAR’s fine-tuning step on image captioning, some of OK-VQA’s test split images and gold captions are used. In order to ensure fairness and avoid any contamination in our experiments, we fine-tune a pretrained OSCAR model on image-captioning removing these instances from its training process.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">For the second step, we explore two different language models: BERT, to perform comparative experiments with current multimodal transformers, and the T5 family, to explore the performance of LMs of increasing size.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>CBM<sub id="S3.SS1.SSS1.1.1" class="ltx_sub">BERT</sub>
</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.5" class="ltx_p">In this first approach, we use a pretrained <span id="S3.SS1.SSS1.p1.5.1" class="ltx_text ltx_font_bold">BERT-base</span> transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">6</span></a>]</cite> as our language model. We feed sequences of tokenized captions and questions <math id="S3.SS1.SSS1.p1.1.m1.6" class="ltx_Math" alttext="T^{(0)}=\{\mathbf{t}^{(0)}_{i}|i=1,\ldots,n_{t}\}" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.6a"><mrow id="S3.SS1.SSS1.p1.1.m1.6.6" xref="S3.SS1.SSS1.p1.1.m1.6.6.cmml"><msup id="S3.SS1.SSS1.p1.1.m1.6.6.4" xref="S3.SS1.SSS1.p1.1.m1.6.6.4.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.6.6.4.2" xref="S3.SS1.SSS1.p1.1.m1.6.6.4.2.cmml">T</mi><mrow id="S3.SS1.SSS1.p1.1.m1.1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.6.6.4.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.1.1.1.3.1" xref="S3.SS1.SSS1.p1.1.m1.6.6.4.cmml">(</mo><mn id="S3.SS1.SSS1.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.1.cmml">0</mn><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.1.1.1.3.2" xref="S3.SS1.SSS1.p1.1.m1.6.6.4.cmml">)</mo></mrow></msup><mo id="S3.SS1.SSS1.p1.1.m1.6.6.3" xref="S3.SS1.SSS1.p1.1.m1.6.6.3.cmml">=</mo><mrow id="S3.SS1.SSS1.p1.1.m1.6.6.2.2" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.3" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.3.1.cmml">{</mo><msubsup id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.2.2" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.2.2.cmml">𝐭</mi><mi id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.3.cmml">i</mi><mrow id="S3.SS1.SSS1.p1.1.m1.2.2.1.3" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.2.2.1.3.1" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.cmml">(</mo><mn id="S3.SS1.SSS1.p1.1.m1.2.2.1.1" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1.cmml">0</mn><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.2.2.1.3.2" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.cmml">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.4" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.3.1.cmml">|</mo><mrow id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.3" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.3.cmml">i</mi><mo id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.2" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.2.cmml">=</mo><mrow id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.2.cmml"><mn id="S3.SS1.SSS1.p1.1.m1.3.3" xref="S3.SS1.SSS1.p1.1.m1.3.3.cmml">1</mn><mo id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS1.p1.1.m1.4.4" xref="S3.SS1.SSS1.p1.1.m1.4.4.cmml">…</mi><mo id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.2.cmml">,</mo><msub id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.2.cmml">n</mi><mi id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.3.cmml">t</mi></msub></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.5" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.6b"><apply id="S3.SS1.SSS1.p1.1.m1.6.6.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6"><eq id="S3.SS1.SSS1.p1.1.m1.6.6.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.3"></eq><apply id="S3.SS1.SSS1.p1.1.m1.6.6.4.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.4"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.6.6.4.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.4">superscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.6.6.4.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.4.2">𝑇</ci><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.1">0</cn></apply><apply id="S3.SS1.SSS1.p1.1.m1.6.6.2.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2"><csymbol cd="latexml" id="S3.SS1.SSS1.p1.1.m1.6.6.2.3.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.3">conditional-set</csymbol><apply id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1">subscript</csymbol><apply id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.2.2">𝐭</ci><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.2.2.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2.1.1">0</cn></apply><ci id="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.5.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2"><eq id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.2"></eq><ci id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.3">𝑖</ci><list id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1"><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.3.3">1</cn><ci id="S3.SS1.SSS1.p1.1.m1.4.4.cmml" xref="S3.SS1.SSS1.p1.1.m1.4.4">…</ci><apply id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.2">𝑛</ci><ci id="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.6.6.2.2.2.1.1.1.3">𝑡</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.6c">T^{(0)}=\{\mathbf{t}^{(0)}_{i}|i=1,\ldots,n_{t}\}</annotation></semantics></math> to BERT, and take the output of the <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="[CLS]" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mrow id="S3.SS1.SSS1.p1.2.m2.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.2.m2.1.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.SSS1.p1.2.m2.1.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.1a" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.4" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.4.cmml">S</mi></mrow><mo stretchy="false" id="S3.SS1.SSS1.p1.2.m2.1.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1"><times id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.1"></times><ci id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.2">𝐶</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.3">𝐿</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.4.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">[CLS]</annotation></semantics></math> or first token of the sequence <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{t}^{(n_{l})}_{1}" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><msubsup id="S3.SS1.SSS1.p1.3.m3.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.2.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.2.2.2" xref="S3.SS1.SSS1.p1.3.m3.1.2.2.2.cmml">𝐭</mi><mn id="S3.SS1.SSS1.p1.3.m3.1.2.3" xref="S3.SS1.SSS1.p1.3.m3.1.2.3.cmml">1</mn><mrow id="S3.SS1.SSS1.p1.3.m3.1.1.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.2.cmml">n</mi><mi id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><apply id="S3.SS1.SSS1.p1.3.m3.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.2.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.2">subscript</csymbol><apply id="S3.SS1.SSS1.p1.3.m3.1.2.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.2.2.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.2">superscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m3.1.2.2.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.2.2.2">𝐭</ci><apply id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.1.1.3">𝑙</ci></apply></apply><cn type="integer" id="S3.SS1.SSS1.p1.3.m3.1.2.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.2.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">\mathbf{t}^{(n_{l})}_{1}</annotation></semantics></math>, where <math id="S3.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="n_{t}" display="inline"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><msub id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS1.p1.4.m4.1.1.2" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml">n</mi><mi id="S3.SS1.SSS1.p1.4.m4.1.1.3" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><apply id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.2">𝑛</ci><ci id="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">n_{t}</annotation></semantics></math> is the number of tokens in the sequence and <math id="S3.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="n_{l}" display="inline"><semantics id="S3.SS1.SSS1.p1.5.m5.1a"><msub id="S3.SS1.SSS1.p1.5.m5.1.1" xref="S3.SS1.SSS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS1.p1.5.m5.1.1.2" xref="S3.SS1.SSS1.p1.5.m5.1.1.2.cmml">n</mi><mi id="S3.SS1.SSS1.p1.5.m5.1.1.3" xref="S3.SS1.SSS1.p1.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.5.m5.1b"><apply id="S3.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1.2">𝑛</ci><ci id="S3.SS1.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m5.1c">n_{l}</annotation></semantics></math> is the number of transformer layers.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.2" class="ltx_p">In order to fine-tune the language model for VQA tasks, we add a <span id="S3.SS1.SSS1.p2.2.1" class="ltx_text ltx_font_bold">classification head</span> to the <math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="[CLS]" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mrow id="S3.SS1.SSS1.p2.1.m1.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p2.1.m1.1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.SSS1.p2.1.m1.1.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1a" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.4" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.4.cmml">S</mi></mrow><mo stretchy="false" id="S3.SS1.SSS1.p2.1.m1.1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1"><times id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1"></times><ci id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.2">𝐶</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.3">𝐿</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.4.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">[CLS]</annotation></semantics></math> embedding. Although VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">1</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">10</span></a>]</cite> and OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">3</span></a>]</cite> were defined with open-ended answers, recent models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">39</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a>]</cite> cast these tasks as classification problems, building a fixed vocabulary of answers from the training dataset. Following this trend, our classification head is a multilayer perceptron (MLP) with one hidden layer after <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{t}^{(n_{l})}_{1}" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><msubsup id="S3.SS1.SSS1.p2.2.m2.1.2" xref="S3.SS1.SSS1.p2.2.m2.1.2.cmml"><mi id="S3.SS1.SSS1.p2.2.m2.1.2.2.2" xref="S3.SS1.SSS1.p2.2.m2.1.2.2.2.cmml">𝐭</mi><mn id="S3.SS1.SSS1.p2.2.m2.1.2.3" xref="S3.SS1.SSS1.p2.2.m2.1.2.3.cmml">1</mn><mrow id="S3.SS1.SSS1.p2.2.m2.1.1.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.2" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.2" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.2.cmml">n</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.3" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><apply id="S3.SS1.SSS1.p2.2.m2.1.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2">subscript</csymbol><apply id="S3.SS1.SSS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.2.m2.1.2.2.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2">superscript</csymbol><ci id="S3.SS1.SSS1.p2.2.m2.1.2.2.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2.2.2">𝐭</ci><apply id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.1.1.1.3">𝑙</ci></apply></apply><cn type="integer" id="S3.SS1.SSS1.p2.2.m2.1.2.3.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">\mathbf{t}^{(n_{l})}_{1}</annotation></semantics></math>. We define our MLP in Eq. <a href="#S3.E1" title="In 3.1.1 CBMBERT ‣ 3.1 Caption-based model (CBM) ‣ 3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E1.m1.31" class="ltx_Math" alttext="\begin{split}\mathbf{h}&amp;=\mathrm{LayerNorm}(\mathrm{GELU}(\mathbf{W}_{h}\mathbf{t}^{(n_{l})}_{1}+\mathbf{b}_{h}))\\
\mathbf{\hat{y}}&amp;=\mathrm{Softmax}(\mathbf{W}_{\hat{y}}\mathbf{h}+\mathbf{b}_{\hat{y}})\end{split}" display="block"><semantics id="S3.E1.m1.31a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.31.31.4" xref="S3.E1.m1.29.29.2.cmml"><mtr id="S3.E1.m1.31.31.4a" xref="S3.E1.m1.29.29.2.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.31.31.4b" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">𝐡</mi></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.31.31.4c" xref="S3.E1.m1.29.29.2.cmml"><mrow id="S3.E1.m1.30.30.3.28.17.16" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.30.30.3.28.17.16.17" xref="S3.E1.m1.29.29.2a.cmml"></mi><mo id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml">=</mo><mrow id="S3.E1.m1.30.30.3.28.17.16.16" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.3.3.3.3.2.2" xref="S3.E1.m1.3.3.3.3.2.2.cmml">LayerNorm</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.30.30.3.28.17.16.16.2" xref="S3.E1.m1.29.29.2a.cmml">​</mo><mrow id="S3.E1.m1.30.30.3.28.17.16.16.1.1" xref="S3.E1.m1.29.29.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.4.4.3.3" xref="S3.E1.m1.29.29.2a.cmml">(</mo><mrow id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.5.5.5.5.4.4" xref="S3.E1.m1.5.5.5.5.4.4.cmml">GELU</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.2" xref="S3.E1.m1.29.29.2a.cmml">​</mo><mrow id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.1.1" xref="S3.E1.m1.29.29.2.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.6.6.5.5" xref="S3.E1.m1.29.29.2a.cmml">(</mo><mrow id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.1.1.1" xref="S3.E1.m1.29.29.2.cmml"><mrow id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.1.1.1.1" xref="S3.E1.m1.29.29.2.cmml"><msub id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.1.1.1.1.2" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.7.7.7.7.6.6" xref="S3.E1.m1.7.7.7.7.6.6.cmml">𝐖</mi><mi id="S3.E1.m1.8.8.8.8.7.7.1" xref="S3.E1.m1.8.8.8.8.7.7.1.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.1.1.1.1.1" xref="S3.E1.m1.29.29.2a.cmml">​</mo><msubsup id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.1.1.1.1.3" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.9.9.9.9.8.8" xref="S3.E1.m1.9.9.9.9.8.8.cmml">𝐭</mi><mn id="S3.E1.m1.11.11.11.11.10.10.1" xref="S3.E1.m1.11.11.11.11.10.10.1.cmml">1</mn><mrow id="S3.E1.m1.10.10.10.10.9.9.1.1" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.10.10.10.10.9.9.1.1.2" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.cmml">(</mo><msub id="S3.E1.m1.10.10.10.10.9.9.1.1.1" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.cmml"><mi id="S3.E1.m1.10.10.10.10.9.9.1.1.1.2" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.2.cmml">n</mi><mi id="S3.E1.m1.10.10.10.10.9.9.1.1.1.3" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S3.E1.m1.10.10.10.10.9.9.1.1.3" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.cmml">)</mo></mrow></msubsup></mrow><mo id="S3.E1.m1.12.12.12.12.11.11" xref="S3.E1.m1.12.12.12.12.11.11.cmml">+</mo><msub id="S3.E1.m1.30.30.3.28.17.16.16.1.1.1.1.1.1.2" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.13.13.13.13.12.12" xref="S3.E1.m1.13.13.13.13.12.12.cmml">𝐛</mi><mi id="S3.E1.m1.14.14.14.14.13.13.1" xref="S3.E1.m1.14.14.14.14.13.13.1.cmml">h</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.15.15.15.15.14.14" xref="S3.E1.m1.29.29.2a.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.16.16.16.16.15.15" xref="S3.E1.m1.29.29.2a.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.31.31.4d" xref="S3.E1.m1.29.29.2.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.31.31.4e" xref="S3.E1.m1.29.29.2.cmml"><mover accent="true" id="S3.E1.m1.17.17.17.1.1.1" xref="S3.E1.m1.17.17.17.1.1.1.cmml"><mi id="S3.E1.m1.17.17.17.1.1.1.3" xref="S3.E1.m1.17.17.17.1.1.1.3.cmml">𝐲</mi><mo id="S3.E1.m1.17.17.17.1.1.1.2" xref="S3.E1.m1.17.17.17.1.1.1.2.cmml">^</mo></mover></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.31.31.4f" xref="S3.E1.m1.29.29.2.cmml"><mrow id="S3.E1.m1.31.31.4.29.12.11" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.31.31.4.29.12.11.12" xref="S3.E1.m1.29.29.2a.cmml"></mi><mo id="S3.E1.m1.18.18.18.2.1.1" xref="S3.E1.m1.18.18.18.2.1.1.cmml">=</mo><mrow id="S3.E1.m1.31.31.4.29.12.11.11" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.19.19.19.3.2.2" xref="S3.E1.m1.19.19.19.3.2.2.cmml">Softmax</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.31.31.4.29.12.11.11.2" xref="S3.E1.m1.29.29.2a.cmml">​</mo><mrow id="S3.E1.m1.31.31.4.29.12.11.11.1.1" xref="S3.E1.m1.29.29.2.cmml"><mo stretchy="false" id="S3.E1.m1.20.20.20.4.3.3" xref="S3.E1.m1.29.29.2a.cmml">(</mo><mrow id="S3.E1.m1.31.31.4.29.12.11.11.1.1.1" xref="S3.E1.m1.29.29.2.cmml"><mrow id="S3.E1.m1.31.31.4.29.12.11.11.1.1.1.1" xref="S3.E1.m1.29.29.2.cmml"><msub id="S3.E1.m1.31.31.4.29.12.11.11.1.1.1.1.2" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.21.21.21.5.4.4" xref="S3.E1.m1.21.21.21.5.4.4.cmml">𝐖</mi><mover accent="true" id="S3.E1.m1.22.22.22.6.5.5.1" xref="S3.E1.m1.22.22.22.6.5.5.1.cmml"><mi id="S3.E1.m1.22.22.22.6.5.5.1.2" xref="S3.E1.m1.22.22.22.6.5.5.1.2.cmml">y</mi><mo id="S3.E1.m1.22.22.22.6.5.5.1.1" xref="S3.E1.m1.22.22.22.6.5.5.1.1.cmml">^</mo></mover></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.31.31.4.29.12.11.11.1.1.1.1.1" xref="S3.E1.m1.29.29.2a.cmml">​</mo><mi id="S3.E1.m1.23.23.23.7.6.6" xref="S3.E1.m1.23.23.23.7.6.6.cmml">𝐡</mi></mrow><mo id="S3.E1.m1.24.24.24.8.7.7" xref="S3.E1.m1.24.24.24.8.7.7.cmml">+</mo><msub id="S3.E1.m1.31.31.4.29.12.11.11.1.1.1.2" xref="S3.E1.m1.29.29.2.cmml"><mi id="S3.E1.m1.25.25.25.9.8.8" xref="S3.E1.m1.25.25.25.9.8.8.cmml">𝐛</mi><mover accent="true" id="S3.E1.m1.26.26.26.10.9.9.1" xref="S3.E1.m1.26.26.26.10.9.9.1.cmml"><mi id="S3.E1.m1.26.26.26.10.9.9.1.2" xref="S3.E1.m1.26.26.26.10.9.9.1.2.cmml">y</mi><mo id="S3.E1.m1.26.26.26.10.9.9.1.1" xref="S3.E1.m1.26.26.26.10.9.9.1.1.cmml">^</mo></mover></msub></mrow><mo stretchy="false" id="S3.E1.m1.27.27.27.11.10.10" xref="S3.E1.m1.29.29.2a.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.31b"><apply id="S3.E1.m1.29.29.2.cmml" xref="S3.E1.m1.31.31.4"><and id="S3.E1.m1.29.29.2a.cmml" xref="S3.E1.m1.30.30.3.28.17.16.17"></and><apply id="S3.E1.m1.29.29.2b.cmml" xref="S3.E1.m1.31.31.4"><eq id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"></eq><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝐡</ci><apply id="S3.E1.m1.28.28.1.1.cmml" xref="S3.E1.m1.31.31.4"><times id="S3.E1.m1.28.28.1.1.2.cmml" xref="S3.E1.m1.30.30.3.28.17.16.17"></times><ci id="S3.E1.m1.3.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.3.2.2">LayerNorm</ci><apply id="S3.E1.m1.28.28.1.1.1.1.1.cmml" xref="S3.E1.m1.31.31.4"><times id="S3.E1.m1.28.28.1.1.1.1.1.2.cmml" xref="S3.E1.m1.31.31.4"></times><ci id="S3.E1.m1.5.5.5.5.4.4.cmml" xref="S3.E1.m1.5.5.5.5.4.4">GELU</ci><apply id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.31.31.4"><plus id="S3.E1.m1.12.12.12.12.11.11.cmml" xref="S3.E1.m1.12.12.12.12.11.11"></plus><apply id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.31.31.4"><times id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.31.31.4"></times><apply id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.31.31.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.31.31.4">subscript</csymbol><ci id="S3.E1.m1.7.7.7.7.6.6.cmml" xref="S3.E1.m1.7.7.7.7.6.6">𝐖</ci><ci id="S3.E1.m1.8.8.8.8.7.7.1.cmml" xref="S3.E1.m1.8.8.8.8.7.7.1">ℎ</ci></apply><apply id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.31.31.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.31.31.4">subscript</csymbol><apply id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.31.31.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S3.E1.m1.31.31.4">superscript</csymbol><ci id="S3.E1.m1.9.9.9.9.8.8.cmml" xref="S3.E1.m1.9.9.9.9.8.8">𝐭</ci><apply id="S3.E1.m1.10.10.10.10.9.9.1.1.1.cmml" xref="S3.E1.m1.10.10.10.10.9.9.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.10.10.9.9.1.1.1.1.cmml" xref="S3.E1.m1.10.10.10.10.9.9.1.1">subscript</csymbol><ci id="S3.E1.m1.10.10.10.10.9.9.1.1.1.2.cmml" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.2">𝑛</ci><ci id="S3.E1.m1.10.10.10.10.9.9.1.1.1.3.cmml" xref="S3.E1.m1.10.10.10.10.9.9.1.1.1.3">𝑙</ci></apply></apply><cn type="integer" id="S3.E1.m1.11.11.11.11.10.10.1.cmml" xref="S3.E1.m1.11.11.11.11.10.10.1">1</cn></apply></apply><apply id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.31.31.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.31.31.4">subscript</csymbol><ci id="S3.E1.m1.13.13.13.13.12.12.cmml" xref="S3.E1.m1.13.13.13.13.12.12">𝐛</ci><ci id="S3.E1.m1.14.14.14.14.13.13.1.cmml" xref="S3.E1.m1.14.14.14.14.13.13.1">ℎ</ci></apply></apply></apply><apply id="S3.E1.m1.17.17.17.1.1.1.cmml" xref="S3.E1.m1.17.17.17.1.1.1"><ci id="S3.E1.m1.17.17.17.1.1.1.2.cmml" xref="S3.E1.m1.17.17.17.1.1.1.2">^</ci><ci id="S3.E1.m1.17.17.17.1.1.1.3.cmml" xref="S3.E1.m1.17.17.17.1.1.1.3">𝐲</ci></apply></apply></apply><apply id="S3.E1.m1.29.29.2c.cmml" xref="S3.E1.m1.31.31.4"><eq id="S3.E1.m1.18.18.18.2.1.1.cmml" xref="S3.E1.m1.18.18.18.2.1.1"></eq><share href="#S3.E1.m1.28.28.1.1.cmml" id="S3.E1.m1.29.29.2d.cmml" xref="S3.E1.m1.30.30.3.28.17.16.17"></share><apply id="S3.E1.m1.29.29.2.2.cmml" xref="S3.E1.m1.31.31.4"><times id="S3.E1.m1.29.29.2.2.2.cmml" xref="S3.E1.m1.30.30.3.28.17.16.17"></times><ci id="S3.E1.m1.19.19.19.3.2.2.cmml" xref="S3.E1.m1.19.19.19.3.2.2">Softmax</ci><apply id="S3.E1.m1.29.29.2.2.1.1.1.cmml" xref="S3.E1.m1.31.31.4"><plus id="S3.E1.m1.24.24.24.8.7.7.cmml" xref="S3.E1.m1.24.24.24.8.7.7"></plus><apply id="S3.E1.m1.29.29.2.2.1.1.1.2.cmml" xref="S3.E1.m1.31.31.4"><times id="S3.E1.m1.29.29.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.31.31.4"></times><apply id="S3.E1.m1.29.29.2.2.1.1.1.2.2.cmml" xref="S3.E1.m1.31.31.4"><csymbol cd="ambiguous" id="S3.E1.m1.29.29.2.2.1.1.1.2.2.1.cmml" xref="S3.E1.m1.31.31.4">subscript</csymbol><ci id="S3.E1.m1.21.21.21.5.4.4.cmml" xref="S3.E1.m1.21.21.21.5.4.4">𝐖</ci><apply id="S3.E1.m1.22.22.22.6.5.5.1.cmml" xref="S3.E1.m1.22.22.22.6.5.5.1"><ci id="S3.E1.m1.22.22.22.6.5.5.1.1.cmml" xref="S3.E1.m1.22.22.22.6.5.5.1.1">^</ci><ci id="S3.E1.m1.22.22.22.6.5.5.1.2.cmml" xref="S3.E1.m1.22.22.22.6.5.5.1.2">𝑦</ci></apply></apply><ci id="S3.E1.m1.23.23.23.7.6.6.cmml" xref="S3.E1.m1.23.23.23.7.6.6">𝐡</ci></apply><apply id="S3.E1.m1.29.29.2.2.1.1.1.3.cmml" xref="S3.E1.m1.31.31.4"><csymbol cd="ambiguous" id="S3.E1.m1.29.29.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.31.31.4">subscript</csymbol><ci id="S3.E1.m1.25.25.25.9.8.8.cmml" xref="S3.E1.m1.25.25.25.9.8.8">𝐛</ci><apply id="S3.E1.m1.26.26.26.10.9.9.1.cmml" xref="S3.E1.m1.26.26.26.10.9.9.1"><ci id="S3.E1.m1.26.26.26.10.9.9.1.1.cmml" xref="S3.E1.m1.26.26.26.10.9.9.1.1">^</ci><ci id="S3.E1.m1.26.26.26.10.9.9.1.2.cmml" xref="S3.E1.m1.26.26.26.10.9.9.1.2">𝑦</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.31c">\begin{split}\mathbf{h}&amp;=\mathrm{LayerNorm}(\mathrm{GELU}(\mathbf{W}_{h}\mathbf{t}^{(n_{l})}_{1}+\mathbf{b}_{h}))\\
\mathbf{\hat{y}}&amp;=\mathrm{Softmax}(\mathbf{W}_{\hat{y}}\mathbf{h}+\mathbf{b}_{\hat{y}})\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.6" class="ltx_p">We use a GELU activation function as well as layer normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">40</span></a>]</cite>. The trainable parameters are <math id="S3.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{W}_{h}\in\mathbb{R}^{d_{h}\times d_{h}}" display="inline"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><mrow id="S3.SS1.SSS1.p4.1.m1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.cmml"><msub id="S3.SS1.SSS1.p4.1.m1.1.1.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.2.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.2.cmml">𝐖</mi><mi id="S3.SS1.SSS1.p4.1.m1.1.1.2.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.3.cmml">h</mi></msub><mo id="S3.SS1.SSS1.p4.1.m1.1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p4.1.m1.1.1.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p4.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml"><msub id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.2.cmml">d</mi><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.3.cmml">h</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.3.cmml">h</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.1.m1.1b"><apply id="S3.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1"><in id="S3.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.1"></in><apply id="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.2">𝐖</ci><ci id="S3.SS1.SSS1.p4.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.3">ℎ</ci></apply><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3"><times id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1"></times><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.2">𝑑</ci><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.3">ℎ</ci></apply><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.3">ℎ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">\mathbf{W}_{h}\in\mathbb{R}^{d_{h}\times d_{h}}</annotation></semantics></math>, <math id="S3.SS1.SSS1.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{b}_{h}\in\mathbb{R}^{d_{h}}" display="inline"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mrow id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml"><msub id="S3.SS1.SSS1.p4.2.m2.1.1.2" xref="S3.SS1.SSS1.p4.2.m2.1.1.2.cmml"><mi id="S3.SS1.SSS1.p4.2.m2.1.1.2.2" xref="S3.SS1.SSS1.p4.2.m2.1.1.2.2.cmml">𝐛</mi><mi id="S3.SS1.SSS1.p4.2.m2.1.1.2.3" xref="S3.SS1.SSS1.p4.2.m2.1.1.2.3.cmml">h</mi></msub><mo id="S3.SS1.SSS1.p4.2.m2.1.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p4.2.m2.1.1.3" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.2.m2.1.1.3.2" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS1.SSS1.p4.2.m2.1.1.3.3" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3.cmml"><mi id="S3.SS1.SSS1.p4.2.m2.1.1.3.3.2" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3.2.cmml">d</mi><mi id="S3.SS1.SSS1.p4.2.m2.1.1.3.3.3" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3.3.cmml">h</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><apply id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1"><in id="S3.SS1.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.1"></in><apply id="S3.SS1.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS1.p4.2.m2.1.1.2.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.2.2">𝐛</ci><ci id="S3.SS1.SSS1.p4.2.m2.1.1.2.3.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.2.3">ℎ</ci></apply><apply id="S3.SS1.SSS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p4.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p4.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3.2">𝑑</ci><ci id="S3.SS1.SSS1.p4.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">\mathbf{b}_{h}\in\mathbb{R}^{d_{h}}</annotation></semantics></math>, <math id="S3.SS1.SSS1.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{W}_{\hat{y}}\in\mathbb{R}^{d_{h}\times n_{label}}" display="inline"><semantics id="S3.SS1.SSS1.p4.3.m3.1a"><mrow id="S3.SS1.SSS1.p4.3.m3.1.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.cmml"><msub id="S3.SS1.SSS1.p4.3.m3.1.1.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.cmml"><mi id="S3.SS1.SSS1.p4.3.m3.1.1.2.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.2.cmml">𝐖</mi><mover accent="true" id="S3.SS1.SSS1.p4.3.m3.1.1.2.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.3.cmml"><mi id="S3.SS1.SSS1.p4.3.m3.1.1.2.3.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.3.2.cmml">y</mi><mo id="S3.SS1.SSS1.p4.3.m3.1.1.2.3.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.3.1.cmml">^</mo></mover></msub><mo id="S3.SS1.SSS1.p4.3.m3.1.1.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p4.3.m3.1.1.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p4.3.m3.1.1.3.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.cmml"><msub id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.2.cmml">d</mi><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.3.cmml">h</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.2.cmml">n</mi><mrow id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.cmml"><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1a" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.4" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1b" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.5" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1c" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.6" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.6.cmml">l</mi></mrow></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.3.m3.1b"><apply id="S3.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1"><in id="S3.SS1.SSS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.1"></in><apply id="S3.SS1.SSS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS1.p4.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.2">𝐖</ci><apply id="S3.SS1.SSS1.p4.3.m3.1.1.2.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.3"><ci id="S3.SS1.SSS1.p4.3.m3.1.1.2.3.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.3.1">^</ci><ci id="S3.SS1.SSS1.p4.3.m3.1.1.2.3.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.3.2">𝑦</ci></apply></apply><apply id="S3.SS1.SSS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3"><times id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.1"></times><apply id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.2">𝑑</ci><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.2.3">ℎ</ci></apply><apply id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.2">𝑛</ci><apply id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3"><times id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.1"></times><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.2">𝑙</ci><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.3">𝑎</ci><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.4.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.4">𝑏</ci><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.5.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.5">𝑒</ci><ci id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.6.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.3.3.6">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.3.m3.1c">\mathbf{W}_{\hat{y}}\in\mathbb{R}^{d_{h}\times n_{label}}</annotation></semantics></math> and <math id="S3.SS1.SSS1.p4.4.m4.1" class="ltx_Math" alttext="\mathbf{b}_{\hat{y}}\in\mathbb{R}^{n_{label}}" display="inline"><semantics id="S3.SS1.SSS1.p4.4.m4.1a"><mrow id="S3.SS1.SSS1.p4.4.m4.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.cmml"><msub id="S3.SS1.SSS1.p4.4.m4.1.1.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.cmml"><mi id="S3.SS1.SSS1.p4.4.m4.1.1.2.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.2.cmml">𝐛</mi><mover accent="true" id="S3.SS1.SSS1.p4.4.m4.1.1.2.3" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.3.cmml"><mi id="S3.SS1.SSS1.p4.4.m4.1.1.2.3.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.3.2.cmml">y</mi><mo id="S3.SS1.SSS1.p4.4.m4.1.1.2.3.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.3.1.cmml">^</mo></mover></msub><mo id="S3.SS1.SSS1.p4.4.m4.1.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p4.4.m4.1.1.3" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.4.m4.1.1.3.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS1.SSS1.p4.4.m4.1.1.3.3" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.2.cmml">n</mi><mrow id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.3" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1a" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.4" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1b" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.5" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1c" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.6" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.6.cmml">l</mi></mrow></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.4.m4.1b"><apply id="S3.SS1.SSS1.p4.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1"><in id="S3.SS1.SSS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.1"></in><apply id="S3.SS1.SSS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.4.m4.1.1.2.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS1.p4.4.m4.1.1.2.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.2">𝐛</ci><apply id="S3.SS1.SSS1.p4.4.m4.1.1.2.3.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.3"><ci id="S3.SS1.SSS1.p4.4.m4.1.1.2.3.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.3.1">^</ci><ci id="S3.SS1.SSS1.p4.4.m4.1.1.2.3.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.3.2">𝑦</ci></apply></apply><apply id="S3.SS1.SSS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p4.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.2">𝑛</ci><apply id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3"><times id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.1"></times><ci id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.2">𝑙</ci><ci id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.3">𝑎</ci><ci id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.4.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.4">𝑏</ci><ci id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.5.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.5">𝑒</ci><ci id="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.6.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.3.3.3.6">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.4.m4.1c">\mathbf{b}_{\hat{y}}\in\mathbb{R}^{n_{label}}</annotation></semantics></math>, where <math id="S3.SS1.SSS1.p4.5.m5.1" class="ltx_Math" alttext="n_{label}" display="inline"><semantics id="S3.SS1.SSS1.p4.5.m5.1a"><msub id="S3.SS1.SSS1.p4.5.m5.1.1" xref="S3.SS1.SSS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.SSS1.p4.5.m5.1.1.2" xref="S3.SS1.SSS1.p4.5.m5.1.1.2.cmml">n</mi><mrow id="S3.SS1.SSS1.p4.5.m5.1.1.3" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.5.m5.1.1.3.2" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.5.m5.1.1.3.1" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.5.m5.1.1.3.3" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.5.m5.1.1.3.1a" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.5.m5.1.1.3.4" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.5.m5.1.1.3.1b" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.5.m5.1.1.3.5" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.5.m5.1.1.3.1c" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS1.p4.5.m5.1.1.3.6" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.5.m5.1b"><apply id="S3.SS1.SSS1.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.2">𝑛</ci><apply id="S3.SS1.SSS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3"><times id="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1"></times><ci id="S3.SS1.SSS1.p4.5.m5.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.2">𝑙</ci><ci id="S3.SS1.SSS1.p4.5.m5.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.3">𝑎</ci><ci id="S3.SS1.SSS1.p4.5.m5.1.1.3.4.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.4">𝑏</ci><ci id="S3.SS1.SSS1.p4.5.m5.1.1.3.5.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.5">𝑒</ci><ci id="S3.SS1.SSS1.p4.5.m5.1.1.3.6.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.5.m5.1c">n_{label}</annotation></semantics></math> equals to the number of labels on a given classification task and <math id="S3.SS1.SSS1.p4.6.m6.1" class="ltx_Math" alttext="d_{h}" display="inline"><semantics id="S3.SS1.SSS1.p4.6.m6.1a"><msub id="S3.SS1.SSS1.p4.6.m6.1.1" xref="S3.SS1.SSS1.p4.6.m6.1.1.cmml"><mi id="S3.SS1.SSS1.p4.6.m6.1.1.2" xref="S3.SS1.SSS1.p4.6.m6.1.1.2.cmml">d</mi><mi id="S3.SS1.SSS1.p4.6.m6.1.1.3" xref="S3.SS1.SSS1.p4.6.m6.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.6.m6.1b"><apply id="S3.SS1.SSS1.p4.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.6.m6.1.1.1.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p4.6.m6.1.1.2.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1.2">𝑑</ci><ci id="S3.SS1.SSS1.p4.6.m6.1.1.3.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.6.m6.1c">d_{h}</annotation></semantics></math> equals to 768.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>CBM<sub id="S3.SS1.SSS2.1.1" class="ltx_sub">T5</sub>
</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In our second approach, we use pretrained <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">T5</span> encoder-decoder transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">9</span></a>]</cite>, as they are the state-of-the-art models for text-only question-answering tasks and are available in different sizes, ranging from 60M parameters to 11B. Following CBM<sub id="S3.SS1.SSS2.p1.1.2" class="ltx_sub">BERT</sub>, we also feed sequences of tokenized captions and questions <math id="S3.SS1.SSS2.p1.1.m1.6" class="ltx_Math" alttext="T^{(0)}=\{\mathbf{t}^{(0)}_{i}|i=1,\ldots,n_{t}\}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.6a"><mrow id="S3.SS1.SSS2.p1.1.m1.6.6" xref="S3.SS1.SSS2.p1.1.m1.6.6.cmml"><msup id="S3.SS1.SSS2.p1.1.m1.6.6.4" xref="S3.SS1.SSS2.p1.1.m1.6.6.4.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.6.6.4.2" xref="S3.SS1.SSS2.p1.1.m1.6.6.4.2.cmml">T</mi><mrow id="S3.SS1.SSS2.p1.1.m1.1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.6.6.4.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.1.1.1.3.1" xref="S3.SS1.SSS2.p1.1.m1.6.6.4.cmml">(</mo><mn id="S3.SS1.SSS2.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.cmml">0</mn><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.1.1.1.3.2" xref="S3.SS1.SSS2.p1.1.m1.6.6.4.cmml">)</mo></mrow></msup><mo id="S3.SS1.SSS2.p1.1.m1.6.6.3" xref="S3.SS1.SSS2.p1.1.m1.6.6.3.cmml">=</mo><mrow id="S3.SS1.SSS2.p1.1.m1.6.6.2.2" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.3" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.3.1.cmml">{</mo><msubsup id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.2.2" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.2.2.cmml">𝐭</mi><mi id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.3.cmml">i</mi><mrow id="S3.SS1.SSS2.p1.1.m1.2.2.1.3" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.2.2.1.3.1" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.cmml">(</mo><mn id="S3.SS1.SSS2.p1.1.m1.2.2.1.1" xref="S3.SS1.SSS2.p1.1.m1.2.2.1.1.cmml">0</mn><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.2.2.1.3.2" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.cmml">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.4" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.3.1.cmml">|</mo><mrow id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.3" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.3.cmml">i</mi><mo id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.2.cmml">=</mo><mrow id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.2.cmml"><mn id="S3.SS1.SSS2.p1.1.m1.3.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.cmml">1</mn><mo id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS2.p1.1.m1.4.4" xref="S3.SS1.SSS2.p1.1.m1.4.4.cmml">…</mi><mo id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.2.cmml">,</mo><msub id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.2.cmml">n</mi><mi id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.3.cmml">t</mi></msub></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.5" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.6b"><apply id="S3.SS1.SSS2.p1.1.m1.6.6.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6"><eq id="S3.SS1.SSS2.p1.1.m1.6.6.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.3"></eq><apply id="S3.SS1.SSS2.p1.1.m1.6.6.4.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.4"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.6.6.4.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.4">superscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.6.6.4.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.4.2">𝑇</ci><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1">0</cn></apply><apply id="S3.SS1.SSS2.p1.1.m1.6.6.2.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2"><csymbol cd="latexml" id="S3.SS1.SSS2.p1.1.m1.6.6.2.3.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.3">conditional-set</csymbol><apply id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1">subscript</csymbol><apply id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.2.2">𝐭</ci><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.2.2.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.1.1">0</cn></apply><ci id="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.5.5.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2"><eq id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.2"></eq><ci id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.3">𝑖</ci><list id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1"><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3">1</cn><ci id="S3.SS1.SSS2.p1.1.m1.4.4.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4">…</ci><apply id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.2">𝑛</ci><ci id="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6.2.2.2.1.1.1.3">𝑡</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.6c">T^{(0)}=\{\mathbf{t}^{(0)}_{i}|i=1,\ldots,n_{t}\}</annotation></semantics></math> to the T5 model. Nevertheless, in this case we add text prefixes before each sentence, such as <span id="S3.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_italic">’caption:’</span> and <span id="S3.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_italic">’question:’</span>. This is mainly done to mimic the input prompts used during the pretraining process of the T5 model, helping the language model to better leverage what it has learnt before. Differently from BERT, T5 is a generative LM, so instead of classifying an answer, T5 produces it in an open-ended text generation manner. Thus we do not use any classifier head for this approach.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multimodal transformer (MM<sub id="S3.SS2.1.1" class="ltx_sub">BERT</sub>)</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We compare our CBM<sub id="S3.SS2.p1.1.1" class="ltx_sub">BERT</sub> model with the multimodal transformer-based MM<sub id="S3.SS2.p1.1.2" class="ltx_sub">BERT</sub> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a>]</cite>, a variant of BERT that uses the question text and image region features as input. While BERT is designed to only process textual inputs, MM<sub id="S3.SS2.p1.1.3" class="ltx_sub">BERT</sub> adapts its embedding layer in order to be able to process features from images.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.5" class="ltx_p">We use a FasterRCNN with a ResNeXt-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">41</span></a>]</cite> as its backbone to extract a total of <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="n_{v}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">n</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑛</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">n_{v}</annotation></semantics></math> region features <math id="S3.SS2.p2.2.m2.3" class="ltx_Math" alttext="\mathbf{V}=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n_{v}}\}" display="inline"><semantics id="S3.SS2.p2.2.m2.3a"><mrow id="S3.SS2.p2.2.m2.3.3" xref="S3.SS2.p2.2.m2.3.3.cmml"><mi id="S3.SS2.p2.2.m2.3.3.4" xref="S3.SS2.p2.2.m2.3.3.4.cmml">𝐕</mi><mo id="S3.SS2.p2.2.m2.3.3.3" xref="S3.SS2.p2.2.m2.3.3.3.cmml">=</mo><mrow id="S3.SS2.p2.2.m2.3.3.2.2" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.2.2.3" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">{</mo><msub id="S3.SS2.p2.2.m2.2.2.1.1.1" xref="S3.SS2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.2.2.1.1.1.2" xref="S3.SS2.p2.2.m2.2.2.1.1.1.2.cmml">𝐯</mi><mn id="S3.SS2.p2.2.m2.2.2.1.1.1.3" xref="S3.SS2.p2.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p2.2.m2.3.3.2.2.4" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">…</mi><mo id="S3.SS2.p2.2.m2.3.3.2.2.5" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p2.2.m2.3.3.2.2.2" xref="S3.SS2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.p2.2.m2.3.3.2.2.2.2" xref="S3.SS2.p2.2.m2.3.3.2.2.2.2.cmml">𝐯</mi><msub id="S3.SS2.p2.2.m2.3.3.2.2.2.3" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3.cmml"><mi id="S3.SS2.p2.2.m2.3.3.2.2.2.3.2" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3.2.cmml">n</mi><mi id="S3.SS2.p2.2.m2.3.3.2.2.2.3.3" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3.3.cmml">v</mi></msub></msub><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.2.2.6" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.3b"><apply id="S3.SS2.p2.2.m2.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3"><eq id="S3.SS2.p2.2.m2.3.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3.3"></eq><ci id="S3.SS2.p2.2.m2.3.3.4.cmml" xref="S3.SS2.p2.2.m2.3.3.4">𝐕</ci><set id="S3.SS2.p2.2.m2.3.3.2.3.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2"><apply id="S3.SS2.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1.2">𝐯</ci><cn type="integer" id="S3.SS2.p2.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">…</ci><apply id="S3.SS2.p2.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2.2">𝐯</ci><apply id="S3.SS2.p2.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.2.2.2.3.1.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3">subscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.2.2.2.3.2.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3.2">𝑛</ci><ci id="S3.SS2.p2.2.m2.3.3.2.2.2.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3.3">𝑣</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.3c">\mathbf{V}=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n_{v}}\}</annotation></semantics></math> per image. Each of these <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{v}_{i}\in\mathbb{R}^{d_{v}}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><msub id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">𝐯</mi><mi id="S3.SS2.p2.3.m3.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.3.2" xref="S3.SS2.p2.3.m3.1.1.3.3.2.cmml">d</mi><mi id="S3.SS2.p2.3.m3.1.1.3.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.3.cmml">v</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><in id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></in><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">𝐯</ci><ci id="S3.SS2.p2.3.m3.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.2">𝑑</ci><ci id="S3.SS2.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\mathbf{v}_{i}\in\mathbb{R}^{d_{v}}</annotation></semantics></math> features represents an object that appears in the image, where <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">d</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝑑</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">d_{v}</annotation></semantics></math> equals to 2048. <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathbf{V}</annotation></semantics></math> lacks the positional information between objects, which can be solved concatenating the corresponding bounding box coordinates to each feature. Upon some initial experiments, we concluded that this extra information does not improve performance in any of VQA 2.0 and OK-VQA.
We use MMF Multimodal Framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">42</span></a>]</cite> to extract the image region features that are fed into MM<sub id="S3.SS2.p2.5.1" class="ltx_sub">BERT</sub>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">In order to allow for easier comparison between our CBM and MM<sub id="S3.SS2.p3.1.1" class="ltx_sub">BERT</sub> we use the output representation for <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="[CLS]" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.1.m1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.1.cmml">[</mo><mrow id="S3.SS2.p3.1.m1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.p3.1.m1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1.1.1.1a" xref="S3.SS2.p3.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.p3.1.m1.1.1.1.1.4" xref="S3.SS2.p3.1.m1.1.1.1.1.4.cmml">S</mi></mrow><mo stretchy="false" id="S3.SS2.p3.1.m1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1"><times id="S3.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1"></times><ci id="S3.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.2">𝐶</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3">𝐿</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.4.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">[CLS]</annotation></semantics></math> to feed into the classification multilayer perceptron (see Section <a href="#S3.SS1" title="3.1 Caption-based model (CBM) ‣ 3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Note that this is slightly different from the original MM<sub id="S3.SS2.p3.1.2" class="ltx_sub">BERT</sub> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a>]</cite>, which uses the average of all token representations in the last transformer layer.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Question-only baseline (Q<sub id="S3.SS3.1.1" class="ltx_sub">BERT</sub>)</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to assess the contribution of captions, we also trained a model which only had the question in the input, without any information about the image or caption, denoted as Q<sub id="S3.SS3.p1.1.1" class="ltx_sub">BERT</sub>. This model can be seen as an ablation of CBM<sub id="S3.SS3.p1.1.2" class="ltx_sub">BERT</sub>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Loss function</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Contrary to previous works in VQA, we do not use binary cross-entropy loss for our classification models, as initial experiments showed that cross-entropy loss with soft labels (SCE) converges faster with similar results. SCE loss is defined in Eq. <a href="#S3.E2" title="In 3.4 Loss function ‣ 3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathbf{y}</annotation></semantics></math> is the ground truth vector with probabilities proportional to the VQA evaluation metric (Eq. <a href="#S4.E3" title="In 4.1 VQA 2.0 ‣ 4 Datasets ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) assigned to each class.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\mathcal{L}_{SCE}(\mathbf{y},\mathbf{\hat{y}})=-\mathbf{y}\cdot\log\mathbf{\hat{y}}" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.3" xref="S3.E2.m1.2.3.cmml"><mrow id="S3.E2.m1.2.3.2" xref="S3.E2.m1.2.3.2.cmml"><msub id="S3.E2.m1.2.3.2.2" xref="S3.E2.m1.2.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.3.2.2.2" xref="S3.E2.m1.2.3.2.2.2.cmml">ℒ</mi><mrow id="S3.E2.m1.2.3.2.2.3" xref="S3.E2.m1.2.3.2.2.3.cmml"><mi id="S3.E2.m1.2.3.2.2.3.2" xref="S3.E2.m1.2.3.2.2.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.2.2.3.1" xref="S3.E2.m1.2.3.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.2.2.3.3" xref="S3.E2.m1.2.3.2.2.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.2.2.3.1a" xref="S3.E2.m1.2.3.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.2.2.3.4" xref="S3.E2.m1.2.3.2.2.3.4.cmml">E</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.2.1" xref="S3.E2.m1.2.3.2.1.cmml">​</mo><mrow id="S3.E2.m1.2.3.2.3.2" xref="S3.E2.m1.2.3.2.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.3.2.3.2.1" xref="S3.E2.m1.2.3.2.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">𝐲</mi><mo id="S3.E2.m1.2.3.2.3.2.2" xref="S3.E2.m1.2.3.2.3.1.cmml">,</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mi id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">𝐲</mi><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E2.m1.2.3.2.3.2.3" xref="S3.E2.m1.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.3.1" xref="S3.E2.m1.2.3.1.cmml">=</mo><mrow id="S3.E2.m1.2.3.3" xref="S3.E2.m1.2.3.3.cmml"><mo id="S3.E2.m1.2.3.3a" xref="S3.E2.m1.2.3.3.cmml">−</mo><mrow id="S3.E2.m1.2.3.3.2" xref="S3.E2.m1.2.3.3.2.cmml"><mi id="S3.E2.m1.2.3.3.2.2" xref="S3.E2.m1.2.3.3.2.2.cmml">𝐲</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.2.3.3.2.1" xref="S3.E2.m1.2.3.3.2.1.cmml">⋅</mo><mrow id="S3.E2.m1.2.3.3.2.3" xref="S3.E2.m1.2.3.3.2.3.cmml"><mi id="S3.E2.m1.2.3.3.2.3.1" xref="S3.E2.m1.2.3.3.2.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.2.3.3.2.3a" xref="S3.E2.m1.2.3.3.2.3.cmml">⁡</mo><mover accent="true" id="S3.E2.m1.2.3.3.2.3.2" xref="S3.E2.m1.2.3.3.2.3.2.cmml"><mi id="S3.E2.m1.2.3.3.2.3.2.2" xref="S3.E2.m1.2.3.3.2.3.2.2.cmml">𝐲</mi><mo id="S3.E2.m1.2.3.3.2.3.2.1" xref="S3.E2.m1.2.3.3.2.3.2.1.cmml">^</mo></mover></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.3.cmml" xref="S3.E2.m1.2.3"><eq id="S3.E2.m1.2.3.1.cmml" xref="S3.E2.m1.2.3.1"></eq><apply id="S3.E2.m1.2.3.2.cmml" xref="S3.E2.m1.2.3.2"><times id="S3.E2.m1.2.3.2.1.cmml" xref="S3.E2.m1.2.3.2.1"></times><apply id="S3.E2.m1.2.3.2.2.cmml" xref="S3.E2.m1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.2.2.1.cmml" xref="S3.E2.m1.2.3.2.2">subscript</csymbol><ci id="S3.E2.m1.2.3.2.2.2.cmml" xref="S3.E2.m1.2.3.2.2.2">ℒ</ci><apply id="S3.E2.m1.2.3.2.2.3.cmml" xref="S3.E2.m1.2.3.2.2.3"><times id="S3.E2.m1.2.3.2.2.3.1.cmml" xref="S3.E2.m1.2.3.2.2.3.1"></times><ci id="S3.E2.m1.2.3.2.2.3.2.cmml" xref="S3.E2.m1.2.3.2.2.3.2">𝑆</ci><ci id="S3.E2.m1.2.3.2.2.3.3.cmml" xref="S3.E2.m1.2.3.2.2.3.3">𝐶</ci><ci id="S3.E2.m1.2.3.2.2.3.4.cmml" xref="S3.E2.m1.2.3.2.2.3.4">𝐸</ci></apply></apply><interval closure="open" id="S3.E2.m1.2.3.2.3.1.cmml" xref="S3.E2.m1.2.3.2.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐲</ci><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">^</ci><ci id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2">𝐲</ci></apply></interval></apply><apply id="S3.E2.m1.2.3.3.cmml" xref="S3.E2.m1.2.3.3"><minus id="S3.E2.m1.2.3.3.1.cmml" xref="S3.E2.m1.2.3.3"></minus><apply id="S3.E2.m1.2.3.3.2.cmml" xref="S3.E2.m1.2.3.3.2"><ci id="S3.E2.m1.2.3.3.2.1.cmml" xref="S3.E2.m1.2.3.3.2.1">⋅</ci><ci id="S3.E2.m1.2.3.3.2.2.cmml" xref="S3.E2.m1.2.3.3.2.2">𝐲</ci><apply id="S3.E2.m1.2.3.3.2.3.cmml" xref="S3.E2.m1.2.3.3.2.3"><log id="S3.E2.m1.2.3.3.2.3.1.cmml" xref="S3.E2.m1.2.3.3.2.3.1"></log><apply id="S3.E2.m1.2.3.3.2.3.2.cmml" xref="S3.E2.m1.2.3.3.2.3.2"><ci id="S3.E2.m1.2.3.3.2.3.2.1.cmml" xref="S3.E2.m1.2.3.3.2.3.2.1">^</ci><ci id="S3.E2.m1.2.3.3.2.3.2.2.cmml" xref="S3.E2.m1.2.3.3.2.3.2.2">𝐲</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathcal{L}_{SCE}(\mathbf{y},\mathbf{\hat{y}})=-\mathbf{y}\cdot\log\mathbf{\hat{y}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Regarding CBM<sub id="S3.SS4.p3.1.1" class="ltx_sub">T5</sub>, we fine-tune this generative model via teacher forcing using cross-entropy loss. Therefore, the model learns to map each input sequence with its respective target sequence. However, training the model using the teacher forcing paradigm causes a discrepancy with the human annotations, as each question in OK-VQA has multiple valid answers. We fix this by randomly choosing a target sequence on each epoch. Initial experiments also showed that randomly choosing an answer from all annotated answers is slightly detrimental, as some answers are not spelled correctly, are empty strings or do not make sense as an answer. Therefore, during training we exclude answers that do not obtain a full score in the VQA score, that is, we choose answers that are annotated by at least two annotators on a given question<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>If a question does not have answers that fulfill this rule, the question is discarded from training, which amounts to a total of 112 instances in the OK-VQA’s training split.</span></span></span>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2109.08029/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="123" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Some examples of VQA 2.0 and OK-VQA datasets for the same images. VQA questions are about image contents, while OK-VQA questions require outside knowledge. </span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Datasets</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The main dataset for our experiments is OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">3</span></a>]</cite>, since it allows us evaluating the usage of the implicit knowledge of LMs in a multimodal task. But we also run experiments on the VQA 2.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">10</span></a>]</cite> with a double motivation: (i) to use it as additional pretraining before applying the model to OK-VQA; (ii) to analyze the performance differences among models on a knowledge-based VQA dataset and a standard VQA dataset. Examples of both datasets can be found in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.4 Loss function ‣ 3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>VQA 2.0</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">This dataset contains open-ended questions about images where questions focus mainly on identifying objects in the image and their attributes, detecting relations between them, as well as counting those objects. The dataset is composed of 204K images taken from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">43</span></a>]</cite> and 1.1M questions, each question having 10 (possibly repeated) annotations as accepted answers. Following the classification setting of VQA tasks, which is currently the dominant paradigm, VQA 2.0 has 3129 different possible answers, extracted from the most frequent answers of the training split.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">VQA 2.0 is divided in three splits named train, dev and test. Some of the images from the development split of VQA 2.0 are reused in OK-VQA’s test split. So, in order to avoid any contamination, we do not use the VQA 2.0 dev set for any training or hyper-parameter tuning.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Antol et al, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">1</span></a>]</cite> proposed a standard evaluation metric for VQA tasks where a system answer is considered totally correct if it appears at least three times in the ten ground-truth annotations. Considering that a given answer appears <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">x</annotation></semantics></math> times in a question’s annotations, this accuracy metric is defined in Eq. <a href="#S4.E3" title="In 4.1 VQA 2.0 ‣ 4 Datasets ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E3.m1.3" class="ltx_Math" alttext="\mathrm{acc}=\min\left(\frac{x}{3},1\right)" display="block"><semantics id="S4.E3.m1.3a"><mrow id="S4.E3.m1.3.4" xref="S4.E3.m1.3.4.cmml"><mi id="S4.E3.m1.3.4.2" xref="S4.E3.m1.3.4.2.cmml">acc</mi><mo id="S4.E3.m1.3.4.1" xref="S4.E3.m1.3.4.1.cmml">=</mo><mrow id="S4.E3.m1.3.4.3.2" xref="S4.E3.m1.3.4.3.1.cmml"><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">min</mi><mo id="S4.E3.m1.3.4.3.2a" xref="S4.E3.m1.3.4.3.1.cmml">⁡</mo><mrow id="S4.E3.m1.3.4.3.2.1" xref="S4.E3.m1.3.4.3.1.cmml"><mo id="S4.E3.m1.3.4.3.2.1.1" xref="S4.E3.m1.3.4.3.1.cmml">(</mo><mfrac id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml"><mi id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.2.cmml">x</mi><mn id="S4.E3.m1.2.2.3" xref="S4.E3.m1.2.2.3.cmml">3</mn></mfrac><mo id="S4.E3.m1.3.4.3.2.1.2" xref="S4.E3.m1.3.4.3.1.cmml">,</mo><mn id="S4.E3.m1.3.3" xref="S4.E3.m1.3.3.cmml">1</mn><mo id="S4.E3.m1.3.4.3.2.1.3" xref="S4.E3.m1.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.3b"><apply id="S4.E3.m1.3.4.cmml" xref="S4.E3.m1.3.4"><eq id="S4.E3.m1.3.4.1.cmml" xref="S4.E3.m1.3.4.1"></eq><ci id="S4.E3.m1.3.4.2.cmml" xref="S4.E3.m1.3.4.2">acc</ci><apply id="S4.E3.m1.3.4.3.1.cmml" xref="S4.E3.m1.3.4.3.2"><min id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"></min><apply id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2"><divide id="S4.E3.m1.2.2.1.cmml" xref="S4.E3.m1.2.2"></divide><ci id="S4.E3.m1.2.2.2.cmml" xref="S4.E3.m1.2.2.2">𝑥</ci><cn type="integer" id="S4.E3.m1.2.2.3.cmml" xref="S4.E3.m1.2.2.3">3</cn></apply><cn type="integer" id="S4.E3.m1.3.3.cmml" xref="S4.E3.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.3c">\mathrm{acc}=\min\left(\frac{x}{3},1\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>OK-VQA</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The OK-VQA dataset is built upon 14,031 images from the COCO dataset and 14,055 crowd-sourced questions. Each question has ten annotated answers (possibly repeated), and the evaluation metric is the same as in VQA 2.0 (Eq. <a href="#S4.E3" title="In 4.1 VQA 2.0 ‣ 4 Datasets ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
As a knowledge-based VQA dataset, OK-VQA requires outside knowledge to answer the questions. However, this outside knowledge is neither provided nor identified, i.e. there is not a list of available knowledge sources for this task, making the task more challenging.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">There are two versions of this dataset, depending on how the stemming of the answers provided by the crowd-sourcers is handled. The stemming used in OK-VQA v1.0 results in some “non-word” answers (such as “poni tail” instead of “pony tail”). OK-VQA v1.1 applied a different stemming algorithm, resulting in a more coherent answer vocabulary. We use OK-VQA v1.1 through our experiments.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments and results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section provides results of the models defined in Section <a href="#S3" title="3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and compare them with the state-of-the-art.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental settings</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We use the same hyperparameters as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a>]</cite> for fine-tuning CBM<sub id="S5.SS1.p1.1.1" class="ltx_sub">BERT</sub>, MM<sub id="S5.SS1.p1.1.2" class="ltx_sub">BERT</sub> and Q<sub id="S5.SS1.p1.1.3" class="ltx_sub">BERT</sub> models both in VQA 2.0 and OK-VQA tasks. We train our models for 88K steps using AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">44</span></a>]</cite>. Our batch size is of 56 with a maximum learning rate of <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="5\cdot 10^{-5}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">⋅</mo><msup id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml"><mn id="S5.SS1.p1.1.m1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p1.1.m1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml"><mo id="S5.SS1.p1.1.m1.1.1.3.3a" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p1.1.m1.1.1.3.3.2" xref="S5.SS1.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><ci id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">5</cn><apply id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.3.2">10</cn><apply id="S5.SS1.p1.1.m1.1.1.3.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3"><minus id="S5.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">5\cdot 10^{-5}</annotation></semantics></math> following a cosine schedule with a linear warmup of 2K steps.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Regarding CBM<sub id="S5.SS1.p2.1.1" class="ltx_sub">T5</sub>, there are 5 different T5 models that vary on size. They range from 60M to 11B parameters and we show the performance of all five models on OK-VQA. To do so, we have chosen to keep the same hyperparameters as before with the following changes:</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.2" class="ltx_p">As models of different sizes need different amounts of training steps in order to converge, we propose the following methodology. We use <math id="S5.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S5.I1.i1.p1.1.m1.1a"><mrow id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml"><mn id="S5.I1.i1.p1.1.m1.1.1.2" xref="S5.I1.i1.p1.1.m1.1.1.2.cmml">20</mn><mo id="S5.I1.i1.p1.1.m1.1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><apply id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.I1.i1.p1.1.m1.1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.I1.i1.p1.1.m1.1.1.2.cmml" xref="S5.I1.i1.p1.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">20\%</annotation></semantics></math> of the training instances to define a validation split, train the models using the remaining <math id="S5.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S5.I1.i1.p1.2.m2.1a"><mrow id="S5.I1.i1.p1.2.m2.1.1" xref="S5.I1.i1.p1.2.m2.1.1.cmml"><mn id="S5.I1.i1.p1.2.m2.1.1.2" xref="S5.I1.i1.p1.2.m2.1.1.2.cmml">80</mn><mo id="S5.I1.i1.p1.2.m2.1.1.1" xref="S5.I1.i1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.2.m2.1b"><apply id="S5.I1.i1.p1.2.m2.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.I1.i1.p1.2.m2.1.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.I1.i1.p1.2.m2.1.1.2.cmml" xref="S5.I1.i1.p1.2.m2.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.2.m2.1c">80\%</annotation></semantics></math> for 20K steps and decide the final number of steps by taking the step with the best VQA score in the validation split. This process is done three times using the same validation split. After that, we compute the average number of steps of all three runs as our final number of training steps.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">As the number of training steps varies among different model sizes, we have decided to use a fixed learning rate of <math id="S5.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="5\cdot 10^{-5}" display="inline"><semantics id="S5.I1.i2.p1.1.m1.1a"><mrow id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml"><mn id="S5.I1.i2.p1.1.m1.1.1.2" xref="S5.I1.i2.p1.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S5.I1.i2.p1.1.m1.1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.1.cmml">⋅</mo><msup id="S5.I1.i2.p1.1.m1.1.1.3" xref="S5.I1.i2.p1.1.m1.1.1.3.cmml"><mn id="S5.I1.i2.p1.1.m1.1.1.3.2" xref="S5.I1.i2.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S5.I1.i2.p1.1.m1.1.1.3.3" xref="S5.I1.i2.p1.1.m1.1.1.3.3.cmml"><mo id="S5.I1.i2.p1.1.m1.1.1.3.3a" xref="S5.I1.i2.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S5.I1.i2.p1.1.m1.1.1.3.3.2" xref="S5.I1.i2.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.1b"><apply id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1"><ci id="S5.I1.i2.p1.1.m1.1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S5.I1.i2.p1.1.m1.1.1.2.cmml" xref="S5.I1.i2.p1.1.m1.1.1.2">5</cn><apply id="S5.I1.i2.p1.1.m1.1.1.3.cmml" xref="S5.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S5.I1.i2.p1.1.m1.1.1.3.2">10</cn><apply id="S5.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S5.I1.i2.p1.1.m1.1.1.3.3"><minus id="S5.I1.i2.p1.1.m1.1.1.3.3.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S5.I1.i2.p1.1.m1.1.1.3.3.2.cmml" xref="S5.I1.i2.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.1c">5\cdot 10^{-5}</annotation></semantics></math> during the fine-tuning process, without any learning-rate scheduler that depends on warmup steps or total number of training steps.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">All experiments regarding classification models have been run in a single GPU with 12GB of vRAM and their runtimes are at most of 12 hours. Regarding the much larger CBM<sub id="S5.SS1.p4.1.1" class="ltx_sub">T5</sub> we used up to 4 NVIDIA A100 GPUs (each with 80 GB of vRAM), changing both hardware and hyper-parameters to keep the same effective batch size across model sizes, and used DeepSpeed’s ZeRO Stage 2 optimization algorithm with CPU offload <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">45</span></a>]</cite> when fine-tuning the biggest model. However, their runtimes are at most of 4 hours, as less training steps are needed for CBM<sub id="S5.SS1.p4.1.2" class="ltx_sub">T5</sub>, compared to the rest of our models.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">In order to get consistent results we make each experiment three times and provide the mean VQA score and standard deviation in all of our results.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results for images vs. captions</h3>

<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.13" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{Model}" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mi id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">Model</mi><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">Model</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\mathrm{Model}</annotation></semantics></math></th>
<th id="S5.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Score</th>
<th id="S5.T1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">+ VQA pretraining</th>
<th id="S5.T1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.5.5" class="ltx_tr">
<td id="S5.T1.5.5.5" class="ltx_td ltx_align_left ltx_border_t">Q<sub id="S5.T1.5.5.5.1" class="ltx_sub">BERT</sub>
</td>
<td id="S5.T1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T1.2.2.1.m1.1" class="ltx_Math" alttext="21.2" display="inline"><semantics id="S5.T1.2.2.1.m1.1a"><mn id="S5.T1.2.2.1.m1.1.1" xref="S5.T1.2.2.1.m1.1.1.cmml">21.2</mn><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.m1.1b"><cn type="float" id="S5.T1.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.1.m1.1.1">21.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.1.m1.1c">21.2</annotation></semantics></math> <math id="S5.T1.3.3.2.m2.1" class="ltx_Math" alttext="\pm 0.2" display="inline"><semantics id="S5.T1.3.3.2.m2.1a"><mrow id="S5.T1.3.3.2.m2.1.1" xref="S5.T1.3.3.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T1.3.3.2.m2.1.1a" xref="S5.T1.3.3.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T1.3.3.2.m2.1.1.2" xref="S5.T1.3.3.2.m2.1.1.2.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.2.m2.1b"><apply id="S5.T1.3.3.2.m2.1.1.cmml" xref="S5.T1.3.3.2.m2.1.1"><csymbol cd="latexml" id="S5.T1.3.3.2.m2.1.1.1.cmml" xref="S5.T1.3.3.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.3.3.2.m2.1.1.2.cmml" xref="S5.T1.3.3.2.m2.1.1.2">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.2.m2.1c">\pm 0.2</annotation></semantics></math>
</td>
<td id="S5.T1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T1.4.4.3.m1.1" class="ltx_Math" alttext="23.0" display="inline"><semantics id="S5.T1.4.4.3.m1.1a"><mn id="S5.T1.4.4.3.m1.1.1" xref="S5.T1.4.4.3.m1.1.1.cmml">23.0</mn><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.3.m1.1b"><cn type="float" id="S5.T1.4.4.3.m1.1.1.cmml" xref="S5.T1.4.4.3.m1.1.1">23.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.3.m1.1c">23.0</annotation></semantics></math> <math id="S5.T1.5.5.4.m2.1" class="ltx_Math" alttext="\pm 0.2" display="inline"><semantics id="S5.T1.5.5.4.m2.1a"><mrow id="S5.T1.5.5.4.m2.1.1" xref="S5.T1.5.5.4.m2.1.1.cmml"><mo mathsize="90%" id="S5.T1.5.5.4.m2.1.1a" xref="S5.T1.5.5.4.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T1.5.5.4.m2.1.1.2" xref="S5.T1.5.5.4.m2.1.1.2.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.4.m2.1b"><apply id="S5.T1.5.5.4.m2.1.1.cmml" xref="S5.T1.5.5.4.m2.1.1"><csymbol cd="latexml" id="S5.T1.5.5.4.m2.1.1.1.cmml" xref="S5.T1.5.5.4.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.5.5.4.m2.1.1.2.cmml" xref="S5.T1.5.5.4.m2.1.1.2">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.4.m2.1c">\pm 0.2</annotation></semantics></math>
</td>
<td id="S5.T1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">112M</td>
</tr>
<tr id="S5.T1.9.9" class="ltx_tr">
<td id="S5.T1.9.9.5" class="ltx_td ltx_align_left">MM<sub id="S5.T1.9.9.5.1" class="ltx_sub">BERT</sub>
</td>
<td id="S5.T1.7.7.2" class="ltx_td ltx_align_center">
<math id="S5.T1.6.6.1.m1.1" class="ltx_Math" alttext="29.6" display="inline"><semantics id="S5.T1.6.6.1.m1.1a"><mn id="S5.T1.6.6.1.m1.1.1" xref="S5.T1.6.6.1.m1.1.1.cmml">29.6</mn><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.1.m1.1b"><cn type="float" id="S5.T1.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.1.m1.1.1">29.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.1.m1.1c">29.6</annotation></semantics></math> <math id="S5.T1.7.7.2.m2.1" class="ltx_Math" alttext="\pm 0.6" display="inline"><semantics id="S5.T1.7.7.2.m2.1a"><mrow id="S5.T1.7.7.2.m2.1.1" xref="S5.T1.7.7.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T1.7.7.2.m2.1.1a" xref="S5.T1.7.7.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T1.7.7.2.m2.1.1.2" xref="S5.T1.7.7.2.m2.1.1.2.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.2.m2.1b"><apply id="S5.T1.7.7.2.m2.1.1.cmml" xref="S5.T1.7.7.2.m2.1.1"><csymbol cd="latexml" id="S5.T1.7.7.2.m2.1.1.1.cmml" xref="S5.T1.7.7.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.7.7.2.m2.1.1.2.cmml" xref="S5.T1.7.7.2.m2.1.1.2">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.2.m2.1c">\pm 0.6</annotation></semantics></math>
</td>
<td id="S5.T1.9.9.4" class="ltx_td ltx_align_center">
<math id="S5.T1.8.8.3.m1.1" class="ltx_Math" alttext="35.7" display="inline"><semantics id="S5.T1.8.8.3.m1.1a"><mn id="S5.T1.8.8.3.m1.1.1" xref="S5.T1.8.8.3.m1.1.1.cmml">35.7</mn><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.3.m1.1b"><cn type="float" id="S5.T1.8.8.3.m1.1.1.cmml" xref="S5.T1.8.8.3.m1.1.1">35.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.3.m1.1c">35.7</annotation></semantics></math> <math id="S5.T1.9.9.4.m2.1" class="ltx_Math" alttext="\pm 0.3" display="inline"><semantics id="S5.T1.9.9.4.m2.1a"><mrow id="S5.T1.9.9.4.m2.1.1" xref="S5.T1.9.9.4.m2.1.1.cmml"><mo mathsize="90%" id="S5.T1.9.9.4.m2.1.1a" xref="S5.T1.9.9.4.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T1.9.9.4.m2.1.1.2" xref="S5.T1.9.9.4.m2.1.1.2.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.4.m2.1b"><apply id="S5.T1.9.9.4.m2.1.1.cmml" xref="S5.T1.9.9.4.m2.1.1"><csymbol cd="latexml" id="S5.T1.9.9.4.m2.1.1.1.cmml" xref="S5.T1.9.9.4.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.9.9.4.m2.1.1.2.cmml" xref="S5.T1.9.9.4.m2.1.1.2">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.4.m2.1c">\pm 0.3</annotation></semantics></math>
</td>
<td id="S5.T1.9.9.6" class="ltx_td ltx_align_center">114M</td>
</tr>
<tr id="S5.T1.13.13" class="ltx_tr">
<td id="S5.T1.13.13.5" class="ltx_td ltx_align_left ltx_border_bb">CBM<sub id="S5.T1.13.13.5.1" class="ltx_sub">BERT</sub> (ours)</td>
<td id="S5.T1.11.11.2" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S5.T1.10.10.1.m1.1" class="ltx_Math" alttext="\bf{32.5}" display="inline"><semantics id="S5.T1.10.10.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S5.T1.10.10.1.m1.1.1" xref="S5.T1.10.10.1.m1.1.1.cmml">32.5</mn><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.1.m1.1b"><cn type="float" id="S5.T1.10.10.1.m1.1.1.cmml" xref="S5.T1.10.10.1.m1.1.1">32.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.1.m1.1c">\bf{32.5}</annotation></semantics></math> <math id="S5.T1.11.11.2.m2.1" class="ltx_Math" alttext="\pm 0.4" display="inline"><semantics id="S5.T1.11.11.2.m2.1a"><mrow id="S5.T1.11.11.2.m2.1.1" xref="S5.T1.11.11.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T1.11.11.2.m2.1.1a" xref="S5.T1.11.11.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T1.11.11.2.m2.1.1.2" xref="S5.T1.11.11.2.m2.1.1.2.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.2.m2.1b"><apply id="S5.T1.11.11.2.m2.1.1.cmml" xref="S5.T1.11.11.2.m2.1.1"><csymbol cd="latexml" id="S5.T1.11.11.2.m2.1.1.1.cmml" xref="S5.T1.11.11.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.11.11.2.m2.1.1.2.cmml" xref="S5.T1.11.11.2.m2.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.2.m2.1c">\pm 0.4</annotation></semantics></math>
</td>
<td id="S5.T1.13.13.4" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S5.T1.12.12.3.m1.1" class="ltx_Math" alttext="\bf{36.0}" display="inline"><semantics id="S5.T1.12.12.3.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S5.T1.12.12.3.m1.1.1" xref="S5.T1.12.12.3.m1.1.1.cmml">36.0</mn><annotation-xml encoding="MathML-Content" id="S5.T1.12.12.3.m1.1b"><cn type="float" id="S5.T1.12.12.3.m1.1.1.cmml" xref="S5.T1.12.12.3.m1.1.1">36.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.12.3.m1.1c">\bf{36.0}</annotation></semantics></math> <math id="S5.T1.13.13.4.m2.1" class="ltx_Math" alttext="\pm 0.4" display="inline"><semantics id="S5.T1.13.13.4.m2.1a"><mrow id="S5.T1.13.13.4.m2.1.1" xref="S5.T1.13.13.4.m2.1.1.cmml"><mo mathsize="90%" id="S5.T1.13.13.4.m2.1.1a" xref="S5.T1.13.13.4.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T1.13.13.4.m2.1.1.2" xref="S5.T1.13.13.4.m2.1.1.2.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.13.13.4.m2.1b"><apply id="S5.T1.13.13.4.m2.1.1.cmml" xref="S5.T1.13.13.4.m2.1.1"><csymbol cd="latexml" id="S5.T1.13.13.4.m2.1.1.1.cmml" xref="S5.T1.13.13.4.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T1.13.13.4.m2.1.1.2.cmml" xref="S5.T1.13.13.4.m2.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.13.4.m2.1c">\pm 0.4</annotation></semantics></math>
</td>
<td id="S5.T1.13.13.6" class="ltx_td ltx_align_center ltx_border_bb">112M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.15.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.16.2" class="ltx_text" style="font-size:90%;">Performance on OK-VQA for three classification models (respectively, question only, image-based and caption-based) without and with additional pretraining on VQA 2.0. Mean VQA score and standard deviation across 3 different runs.</span></figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Results for images vs. captions ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results for the three models presented in Section <a href="#S3" title="3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which share the same architecture, size and initial parameters. We show the results for the models fine-tuned on OK-VQA, as well as the same models which have been previously fine-tuned on VQA 2.0.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We observe that the sole use of questions (Q<sub id="S5.SS2.p2.1.1" class="ltx_sub">BERT</sub>) offers poor performance compared to the other two systems, achieving up to 13 points less accuracy. This shows that having any representation of the image (captions or image region features) is key to answer questions correctly. This is further justified comparing the improvement that VQA pretraining entails, as Q<sub id="S5.SS2.p2.1.2" class="ltx_sub">BERT</sub> improves less than 2 points, whereas the other two improve their accuracy between 4-6 points.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Contribution of captions</h5>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">When we compare the performance of CBM<sub id="S5.SS2.SSS0.Px1.p1.1.1" class="ltx_sub">BERT</sub> and MM<sub id="S5.SS2.SSS0.Px1.p1.1.2" class="ltx_sub">BERT</sub>, we see that, when there is no visio-linguistic pretraining involved, CBM<sub id="S5.SS2.SSS0.Px1.p1.1.3" class="ltx_sub">BERT</sub> performs better in OK-VQA. However, when we pretrain these models in a similar multimodal task like VQA 2.0, their accuracy increases by 4-6 points and both obtain similar performance. As OK-VQA’s training is comparatively smaller (9K instances vs. VQA’s 410K instances), we hypothesize that training MM<sub id="S5.SS2.SSS0.Px1.p1.1.4" class="ltx_sub">BERT</sub> on OK-VQA is not enough to adapt the model to the new input modality. However, as CBM<sub id="S5.SS2.SSS0.Px1.p1.1.5" class="ltx_sub">BERT</sub> uses only text, the fine-tuning with such small training is more effective.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>T5 and larger models</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">As T5 ha been pre-trained on several question answering tasks, we directly fine-tune it on OK-VQA alone.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T2.10" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:208.1pt;">
<table id="S5.T2.10.10" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.10.10.11.1" class="ltx_tr">
<th id="S5.T2.10.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S5.T2.10.10.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Score</th>
<th id="S5.T2.10.10.11.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.2" class="ltx_tr">
<th id="S5.T2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CBM<sub id="S5.T2.2.2.2.3.1" class="ltx_sub">T5-Small</sub>
</th>
<td id="S5.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S5.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="29.2" display="inline"><semantics id="S5.T2.1.1.1.1.m1.1a"><mn id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">29.2</mn><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><cn type="float" id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">29.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">29.2</annotation></semantics></math> <math id="S5.T2.2.2.2.2.m2.1" class="ltx_Math" alttext="\pm 0.2" display="inline"><semantics id="S5.T2.2.2.2.2.m2.1a"><mrow id="S5.T2.2.2.2.2.m2.1.1" xref="S5.T2.2.2.2.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T2.2.2.2.2.m2.1.1a" xref="S5.T2.2.2.2.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T2.2.2.2.2.m2.1.1.2" xref="S5.T2.2.2.2.2.m2.1.1.2.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.m2.1b"><apply id="S5.T2.2.2.2.2.m2.1.1.cmml" xref="S5.T2.2.2.2.2.m2.1.1"><csymbol cd="latexml" id="S5.T2.2.2.2.2.m2.1.1.1.cmml" xref="S5.T2.2.2.2.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T2.2.2.2.2.m2.1.1.2.cmml" xref="S5.T2.2.2.2.2.m2.1.1.2">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.m2.1c">\pm 0.2</annotation></semantics></math>
</td>
<td id="S5.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">60M</td>
</tr>
<tr id="S5.T2.4.4.4" class="ltx_tr">
<th id="S5.T2.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">CBM<sub id="S5.T2.4.4.4.3.1" class="ltx_sub">T5-Base</sub>
</th>
<td id="S5.T2.4.4.4.2" class="ltx_td ltx_align_center">
<math id="S5.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="36.1" display="inline"><semantics id="S5.T2.3.3.3.1.m1.1a"><mn id="S5.T2.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.1.m1.1.1.cmml">36.1</mn><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.m1.1b"><cn type="float" id="S5.T2.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.m1.1.1">36.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.m1.1c">36.1</annotation></semantics></math> <math id="S5.T2.4.4.4.2.m2.1" class="ltx_Math" alttext="\pm 0.5" display="inline"><semantics id="S5.T2.4.4.4.2.m2.1a"><mrow id="S5.T2.4.4.4.2.m2.1.1" xref="S5.T2.4.4.4.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T2.4.4.4.2.m2.1.1a" xref="S5.T2.4.4.4.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T2.4.4.4.2.m2.1.1.2" xref="S5.T2.4.4.4.2.m2.1.1.2.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.2.m2.1b"><apply id="S5.T2.4.4.4.2.m2.1.1.cmml" xref="S5.T2.4.4.4.2.m2.1.1"><csymbol cd="latexml" id="S5.T2.4.4.4.2.m2.1.1.1.cmml" xref="S5.T2.4.4.4.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T2.4.4.4.2.m2.1.1.2.cmml" xref="S5.T2.4.4.4.2.m2.1.1.2">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.2.m2.1c">\pm 0.5</annotation></semantics></math>
</td>
<td id="S5.T2.4.4.4.4" class="ltx_td ltx_align_center">220M</td>
</tr>
<tr id="S5.T2.6.6.6" class="ltx_tr">
<th id="S5.T2.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">CBM<sub id="S5.T2.6.6.6.3.1" class="ltx_sub">T5-Large</sub>
</th>
<td id="S5.T2.6.6.6.2" class="ltx_td ltx_align_center">
<math id="S5.T2.5.5.5.1.m1.1" class="ltx_Math" alttext="40.8" display="inline"><semantics id="S5.T2.5.5.5.1.m1.1a"><mn id="S5.T2.5.5.5.1.m1.1.1" xref="S5.T2.5.5.5.1.m1.1.1.cmml">40.8</mn><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.1.m1.1b"><cn type="float" id="S5.T2.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.1.m1.1.1">40.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.1.m1.1c">40.8</annotation></semantics></math> <math id="S5.T2.6.6.6.2.m2.1" class="ltx_Math" alttext="\pm 0.4" display="inline"><semantics id="S5.T2.6.6.6.2.m2.1a"><mrow id="S5.T2.6.6.6.2.m2.1.1" xref="S5.T2.6.6.6.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T2.6.6.6.2.m2.1.1a" xref="S5.T2.6.6.6.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T2.6.6.6.2.m2.1.1.2" xref="S5.T2.6.6.6.2.m2.1.1.2.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.2.m2.1b"><apply id="S5.T2.6.6.6.2.m2.1.1.cmml" xref="S5.T2.6.6.6.2.m2.1.1"><csymbol cd="latexml" id="S5.T2.6.6.6.2.m2.1.1.1.cmml" xref="S5.T2.6.6.6.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T2.6.6.6.2.m2.1.1.2.cmml" xref="S5.T2.6.6.6.2.m2.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.2.m2.1c">\pm 0.4</annotation></semantics></math>
</td>
<td id="S5.T2.6.6.6.4" class="ltx_td ltx_align_center">770M</td>
</tr>
<tr id="S5.T2.8.8.8" class="ltx_tr">
<th id="S5.T2.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">CBM<sub id="S5.T2.8.8.8.3.1" class="ltx_sub">T5-3B</sub>
</th>
<td id="S5.T2.8.8.8.2" class="ltx_td ltx_align_center">
<math id="S5.T2.7.7.7.1.m1.1" class="ltx_Math" alttext="44.0" display="inline"><semantics id="S5.T2.7.7.7.1.m1.1a"><mn id="S5.T2.7.7.7.1.m1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.cmml">44.0</mn><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.1.m1.1b"><cn type="float" id="S5.T2.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1">44.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.1.m1.1c">44.0</annotation></semantics></math> <math id="S5.T2.8.8.8.2.m2.1" class="ltx_Math" alttext="\pm 0.7" display="inline"><semantics id="S5.T2.8.8.8.2.m2.1a"><mrow id="S5.T2.8.8.8.2.m2.1.1" xref="S5.T2.8.8.8.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T2.8.8.8.2.m2.1.1a" xref="S5.T2.8.8.8.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T2.8.8.8.2.m2.1.1.2" xref="S5.T2.8.8.8.2.m2.1.1.2.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.2.m2.1b"><apply id="S5.T2.8.8.8.2.m2.1.1.cmml" xref="S5.T2.8.8.8.2.m2.1.1"><csymbol cd="latexml" id="S5.T2.8.8.8.2.m2.1.1.1.cmml" xref="S5.T2.8.8.8.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T2.8.8.8.2.m2.1.1.2.cmml" xref="S5.T2.8.8.8.2.m2.1.1.2">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.2.m2.1c">\pm 0.7</annotation></semantics></math>
</td>
<td id="S5.T2.8.8.8.4" class="ltx_td ltx_align_center">3B</td>
</tr>
<tr id="S5.T2.10.10.10" class="ltx_tr">
<th id="S5.T2.10.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">CBM<sub id="S5.T2.10.10.10.3.1" class="ltx_sub">T5-11B</sub>
</th>
<td id="S5.T2.10.10.10.2" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S5.T2.9.9.9.1.m1.1" class="ltx_Math" alttext="\bf{47.9}" display="inline"><semantics id="S5.T2.9.9.9.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S5.T2.9.9.9.1.m1.1.1" xref="S5.T2.9.9.9.1.m1.1.1.cmml">47.9</mn><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.9.1.m1.1b"><cn type="float" id="S5.T2.9.9.9.1.m1.1.1.cmml" xref="S5.T2.9.9.9.1.m1.1.1">47.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.9.1.m1.1c">\bf{47.9}</annotation></semantics></math> <math id="S5.T2.10.10.10.2.m2.1" class="ltx_Math" alttext="\pm 0.2" display="inline"><semantics id="S5.T2.10.10.10.2.m2.1a"><mrow id="S5.T2.10.10.10.2.m2.1.1" xref="S5.T2.10.10.10.2.m2.1.1.cmml"><mo mathsize="90%" id="S5.T2.10.10.10.2.m2.1.1a" xref="S5.T2.10.10.10.2.m2.1.1.cmml">±</mo><mn mathsize="90%" id="S5.T2.10.10.10.2.m2.1.1.2" xref="S5.T2.10.10.10.2.m2.1.1.2.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.10.2.m2.1b"><apply id="S5.T2.10.10.10.2.m2.1.1.cmml" xref="S5.T2.10.10.10.2.m2.1.1"><csymbol cd="latexml" id="S5.T2.10.10.10.2.m2.1.1.1.cmml" xref="S5.T2.10.10.10.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T2.10.10.10.2.m2.1.1.2.cmml" xref="S5.T2.10.10.10.2.m2.1.1.2">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.10.2.m2.1c">\pm 0.2</annotation></semantics></math>
</td>
<td id="S5.T2.10.10.10.4" class="ltx_td ltx_align_center ltx_border_bb">11B</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.T2.10.12.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.10.13.2" class="ltx_text" style="font-size:90%;">Performance on OK-VQA of our generative CBM<sub id="S5.T2.10.13.2.1" class="ltx_sub">T5</sub> models.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:208.1pt;"><img src="/html/2109.08029/assets/x3.png" id="S5.T2.11.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.4.2" class="ltx_text" style="font-size:90%;">Correlation between the size of CBM<sub id="S5.F3.4.2.1" class="ltx_sub">T5</sub> models and their performance. The horizontal axis is in logarithmic scale.</span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">In Table <a href="#S5.T2" title="Table 2 ‣ 5.3 T5 and larger models ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we show the results of five differently sized CBM<sub id="S5.SS3.p2.1.1" class="ltx_sub">T5</sub> models on OK-VQA. Note that our T5-Base model obtains results comparable to our BERT-base model pre-trained on VQA 2.0. This was expected, as both models have been pre-trained with VQA datasets and both have comparable model sizes, T5-base being composed of two BERT-base encoder and decoder.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The results in Table <a href="#S5.T2" title="Table 2 ‣ 5.3 T5 and larger models ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are plotted in Figure <a href="#S5.F3" title="Figure 3 ‣ Table 2 ‣ 5.3 T5 and larger models ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, showing that the size of our models is logarithmically proportional to its score, which follows the scaling laws mentioned in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">46</span></a>]</cite>. This trend is followed even by our biggest model and does not seem to slow down yet.
These results show the importance of the model’s capacity in the results. All models have been pretrained with the same corpus and downstream tasks, but the difference in size helps bigger models to better leverage the information learnt from that corpus in order to incorporate the external knowledge needed to solve OK-VQA. Our largest model performs much better than the multimodal model.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">In fact, it is not clear whether larger multimodal models could match our largest text-only caption based model. We cannot currently test this hypothesis, as, to the best of our knowledge, there are no publicly available multimodal transformers with comparable numbers or parameters. Still, we hypothesize that in the case of knowledge-intensive datasets such as OK-VQA, current multimodal transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">17</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">15</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">18</span></a>]</cite> will underperform our system, as the only textual data fed to these models during their pretraining is mostly composed by captions or small descriptions attached to images. This means that these models only see a limited vocabulary from a limited corpus, compared to the rich, diverse and much larger corpora used to build models such as T5.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Comparison with the state of the art</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p"><span id="S5.SS4.p1.1.1" class="ltx_text" style="color:#000000;">In Table </span><a href="#S5.T3" title="Table 3 ‣ 5.4 Comparison with the state of the art ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS4.p1.1.2" class="ltx_text" style="color:#000000;">, we show the results of various state-of-the-art models in three groups: 1) classification models based on multimodal transformers, which additionally include the usage of symbolic knowledge; 2) GPT-3 based generative models that use in-context learning; 3) our caption-based models.</span></p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T3.2.1.1.1.1" class="ltx_text" style="color:#000000;">Model</span></th>
<td id="S5.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.2.1.1.2.1" class="ltx_text" style="color:#000000;">Score</span></td>
<td id="S5.T3.2.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S5.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.2.1.1.4.1" class="ltx_text" style="color:#000000;">Parameters</span></td>
</tr>
<tr id="S5.T3.2.2.2" class="ltx_tr">
<th id="S5.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T3.2.2.2.1.1" class="ltx_text" style="color:#000000;">ConceptBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.2.2.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">21</span></a><span id="S5.T3.2.2.2.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.T3.2.2.2.1.4" class="ltx_text" style="color:#000000;"> *</span>
</th>
<td id="S5.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.2.2.1" class="ltx_text" style="color:#000000;">31.4</span></td>
<td id="S5.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S5.T3.2.2.2.3.1" class="ltx_text" style="color:#000000;">(</span><span id="S5.T3.2.2.2.3.2" class="ltx_text" style="font-size:90%;color:#000000;">+sym.</span><span id="S5.T3.2.2.2.3.3" class="ltx_text" style="color:#000000;"> 33.7)</span>
</td>
<td id="S5.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.2.4.1" class="ltx_text" style="color:#000000;">348M</span></td>
</tr>
<tr id="S5.T3.2.3.3" class="ltx_tr">
<th id="S5.T3.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.2.3.3.1.1" class="ltx_text" style="color:#000000;">MAVEx </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.3.3.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">25</span></a><span id="S5.T3.2.3.3.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite>
</th>
<td id="S5.T3.2.3.3.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.3.2.1" class="ltx_text" style="color:#000000;">35.2</span></td>
<td id="S5.T3.2.3.3.3" class="ltx_td ltx_align_center">
<span id="S5.T3.2.3.3.3.1" class="ltx_text" style="color:#000000;">(</span><span id="S5.T3.2.3.3.3.2" class="ltx_text" style="font-size:90%;color:#000000;">+sym.</span><span id="S5.T3.2.3.3.3.3" class="ltx_text" style="color:#000000;"> 41.4)</span>
</td>
<td id="S5.T3.2.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.3.4.1" class="ltx_text" style="color:#000000;">353M</span></td>
</tr>
<tr id="S5.T3.2.4.4" class="ltx_tr">
<th id="S5.T3.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.2.4.4.1.1" class="ltx_text" style="color:#000000;">KRISP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.4.4.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a><span id="S5.T3.2.4.4.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite>
</th>
<td id="S5.T3.2.4.4.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.4.2.1" class="ltx_text" style="color:#000000;">37.1</span></td>
<td id="S5.T3.2.4.4.3" class="ltx_td ltx_align_center">
<span id="S5.T3.2.4.4.3.1" class="ltx_text" style="color:#000000;">(</span><span id="S5.T3.2.4.4.3.2" class="ltx_text" style="font-size:90%;color:#000000;">+sym.</span><span id="S5.T3.2.4.4.3.3" class="ltx_text" style="color:#000000;"> 38.9)</span>
</td>
<td id="S5.T3.2.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.4.4.1" class="ltx_text" style="color:#000000;">116M</span></td>
</tr>
<tr id="S5.T3.2.5.5" class="ltx_tr">
<th id="S5.T3.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.2.5.5.1.1" class="ltx_text" style="color:#000000;">RVL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.5.5.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">26</span></a><span id="S5.T3.2.5.5.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.T3.2.5.5.1.4" class="ltx_text" style="color:#000000;"> *†</span>
</th>
<td id="S5.T3.2.5.5.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.5.2.1" class="ltx_text" style="color:#000000;">37.3</span></td>
<td id="S5.T3.2.5.5.3" class="ltx_td ltx_align_center">
<span id="S5.T3.2.5.5.3.1" class="ltx_text" style="color:#000000;">(</span><span id="S5.T3.2.5.5.3.2" class="ltx_text" style="font-size:90%;color:#000000;">+sym.</span><span id="S5.T3.2.5.5.3.3" class="ltx_text" style="color:#000000;"> 39.0)</span>
</td>
<td id="S5.T3.2.5.5.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.5.4.1" class="ltx_text" style="color:#000000;">208M</span></td>
</tr>
<tr id="S5.T3.2.6.6" class="ltx_tr">
<th id="S5.T3.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T3.2.6.6.1.1" class="ltx_text" style="color:#000000;">PICa-Base </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.6.6.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">8</span></a><span id="S5.T3.2.6.6.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite>
</th>
<td id="S5.T3.2.6.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.6.2.1" class="ltx_text" style="color:#000000;">42.0</span></td>
<td id="S5.T3.2.6.6.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S5.T3.2.6.6.3.1" class="ltx_text" style="color:#000000;">(</span><span id="S5.T3.2.6.6.3.2" class="ltx_text" style="font-size:90%;color:#000000;">+tags</span><span id="S5.T3.2.6.6.3.3" class="ltx_text" style="color:#000000;">  43.3)</span>
</td>
<td id="S5.T3.2.6.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.6.4.1" class="ltx_text" style="color:#000000;">175B</span></td>
</tr>
<tr id="S5.T3.2.7.7" class="ltx_tr">
<th id="S5.T3.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.2.7.7.1.1" class="ltx_text" style="color:#000000;">PICa-Full </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.7.7.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">8</span></a><span id="S5.T3.2.7.7.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.T3.2.7.7.1.4" class="ltx_text" style="color:#000000;"> </span><span id="S5.T3.2.7.7.1.5" class="ltx_text" style="font-size:90%;color:#000000;">(Ensemble)</span>
</th>
<td id="S5.T3.2.7.7.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.7.2.1" class="ltx_text" style="color:#000000;">46.9</span></td>
<td id="S5.T3.2.7.7.3" class="ltx_td ltx_align_center">
<span id="S5.T3.2.7.7.3.1" class="ltx_text" style="color:#000000;">(</span><span id="S5.T3.2.7.7.3.2" class="ltx_text" style="font-size:90%;color:#000000;">+tags</span><span id="S5.T3.2.7.7.3.3" class="ltx_text" style="color:#000000;">  </span><span id="S5.T3.2.7.7.3.4" class="ltx_text ltx_font_bold" style="color:#000000;">48.0</span><span id="S5.T3.2.7.7.3.5" class="ltx_text" style="color:#000000;">)</span>
</td>
<td id="S5.T3.2.7.7.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.7.4.1" class="ltx_text" style="color:#000000;">175B</span></td>
</tr>
<tr id="S5.T3.2.8.8" class="ltx_tr">
<th id="S5.T3.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T3.2.8.8.1.1" class="ltx_text" style="color:#000000;">CBM</span><sub id="S5.T3.2.8.8.1.2" class="ltx_sub"><span id="S5.T3.2.8.8.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S5.T3.2.8.8.1.3" class="ltx_text" style="color:#000000;"> (ours)</span>
</th>
<td id="S5.T3.2.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.8.8.2.1" class="ltx_text" style="color:#000000;">36.0</span></td>
<td id="S5.T3.2.8.8.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.2.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.8.8.4.1" class="ltx_text" style="color:#000000;">112M</span></td>
</tr>
<tr id="S5.T3.2.9.9" class="ltx_tr">
<th id="S5.T3.2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S5.T3.2.9.9.1.1" class="ltx_text" style="color:#000000;">CBM</span><sub id="S5.T3.2.9.9.1.2" class="ltx_sub"><span id="S5.T3.2.9.9.1.2.1" class="ltx_text" style="color:#000000;">T5-11B</span></sub><span id="S5.T3.2.9.9.1.3" class="ltx_text" style="color:#000000;"> (ours)</span>
</th>
<td id="S5.T3.2.9.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.9.2.1" class="ltx_text ltx_font_bold" style="color:#000000;">47.9</span></td>
<td id="S5.T3.2.9.9.3" class="ltx_td ltx_border_bb"></td>
<td id="S5.T3.2.9.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.9.4.1" class="ltx_text" style="color:#000000;">11B</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.6.2" class="ltx_text" style="font-size:90%;">Comparison to the state-of-the-art on OK-VQA. +sym. stands for systems additionally using symbolic knowledge, and +tags for the additional use of object tags. Results of models marked with * are in OK-VQA v1.0 and † specifies contaminated results (see main text).</span></figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text" style="color:#000000;">The state-of-the-art classification models like KRISP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p2.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">4</span></a><span id="S5.SS4.p2.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.SS4.p2.1.4" class="ltx_text" style="color:#000000;">, MAVEx </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p2.1.5.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">25</span></a><span id="S5.SS4.p2.1.6.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.SS4.p2.1.7" class="ltx_text" style="color:#000000;"> and RVL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p2.1.8.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">26</span></a><span id="S5.SS4.p2.1.9.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.SS4.p2.1.10" class="ltx_text" style="color:#000000;"> show similar results on the implicit-only versions of their models, even though they are based on different multimodal transformers and pretraining tasks. Note that RVL has a contamination issue as images from OK-VQA’s test split were used to train their multimodal transformer. We also observe that using symbolic knowledge improves the results around 2 points, the exception being MAVEx which combines knowledge found in ConceptNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p2.1.11.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">5</span></a><span id="S5.SS4.p2.1.12.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.SS4.p2.1.13" class="ltx_text" style="color:#000000;">, Wikipedia and Google Images </span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>This result is obtained with an ensemble of 3 MAVEx models that share the same multimodal transformer. A unique MAVEx model achieves an accuracy of 40.3%.</span></span></span><span id="S5.SS4.p2.1.14" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text" style="color:#000000;">PICa </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p3.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">8</span></a><span id="S5.SS4.p3.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.SS4.p3.1.4" class="ltx_text" style="color:#000000;"> takes advantage of GPT-3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p3.1.5.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">34</span></a><span id="S5.SS4.p3.1.6.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S5.SS4.p3.1.7" class="ltx_text" style="color:#000000;"> to define a new state-of-the-art in a generative manner using in-context learning. Its base results (PICa-Base) already surpass the ones seen before without any need of symbolic knowledge. An ensemble of 5 GPT-3 models and a clever selection of annotated examples from the training data to build the input prompt further improves its results (PICa-Full). Table </span><a href="#S5.T3" title="Table 3 ‣ 5.4 Comparison with the state of the art ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS4.p3.1.8" class="ltx_text" style="color:#000000;"> reports two results for each PICa model: the results using automatically generated captions alone (like us), and the results when also using object tags automatically obtained from the image, which slightly improve the results.</span></p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p"><span id="S5.SS4.p4.1.1" class="ltx_text" style="color:#000000;">Our CBM</span><sub id="S5.SS4.p4.1.2" class="ltx_sub"><span id="S5.SS4.p4.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S5.SS4.p4.1.3" class="ltx_text" style="color:#000000;"> system performs on par with the multimodal transformers, which is remarkable, since we do not use directly any visual features in our models and only use the caption. Note that all those systems have models of comparable size. When scaling up our generative models, we see that CBM</span><sub id="S5.SS4.p4.1.4" class="ltx_sub"><span id="S5.SS4.p4.1.4.1" class="ltx_text" style="color:#000000;">T5-11B</span></sub><span id="S5.SS4.p4.1.5" class="ltx_text" style="color:#000000;"> outperforms current multimodal models by a large margin and is on par with the results obtained by PICa-Full. Indeed, CBM</span><sub id="S5.SS4.p4.1.6" class="ltx_sub"><span id="S5.SS4.p4.1.6.1" class="ltx_text" style="color:#000000;">T5-11B</span></sub><span id="S5.SS4.p4.1.7" class="ltx_text" style="color:#000000;"> achieves slightly better results than the PICa version which uses captions alone, even if our model is 15 times smaller.</span></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">6 </span>Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="color:#000000;">In this section we perform additional experiments. We first contrast the results on OK-VQA with those obtained in VQA 2.0, discussing the reasons for the different performance. We then combine our text only model with its counterpart multimodal model to analyze if they are complementary. Afterwards, we compare the performance of CBM</span><sub id="S6.p1.1.2" class="ltx_sub"><span id="S6.p1.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.p1.1.3" class="ltx_text" style="color:#000000;"> with manually annotated captions or the ones generated by OSCAR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.p1.1.4.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">16</span></a><span id="S6.p1.1.5.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S6.p1.1.6" class="ltx_text" style="color:#000000;">. Finally, we present some qualitative analysis.</span></p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Results on VQA 2.0</h3>

<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.2.1.1" class="ltx_tr">
<th id="S6.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S6.T4.2.1.1.1.1" class="ltx_text" style="color:#000000;">Model</span></th>
<td id="S6.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.2.1.1.2.1" class="ltx_text" style="color:#000000;">Score</span></td>
</tr>
<tr id="S6.T4.2.2.2" class="ltx_tr">
<th id="S6.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S6.T4.2.2.2.1.1" class="ltx_text" style="color:#000000;">MM</span><sub id="S6.T4.2.2.2.1.2" class="ltx_sub"><span id="S6.T4.2.2.2.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub>
</th>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T4.2.2.2.2.1" class="ltx_text ltx_font_bold" style="color:#000000;">65.8</span></td>
</tr>
<tr id="S6.T4.2.3.3" class="ltx_tr">
<th id="S6.T4.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T4.2.3.3.1.1" class="ltx_text" style="color:#000000;">PICa-Full</span></th>
<td id="S6.T4.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T4.2.3.3.2.1" class="ltx_text" style="color:#000000;">56.1</span></td>
</tr>
<tr id="S6.T4.2.4.4" class="ltx_tr">
<th id="S6.T4.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S6.T4.2.4.4.1.1" class="ltx_text" style="color:#000000;">CBM</span><sub id="S6.T4.2.4.4.1.2" class="ltx_sub"><span id="S6.T4.2.4.4.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.T4.2.4.4.1.3" class="ltx_text" style="color:#000000;"> (ours)</span>
</th>
<td id="S6.T4.2.4.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T4.2.4.4.2.1" class="ltx_text" style="color:#000000;">59.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S6.T4.9.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S6.T4.10.2" class="ltx_text" style="font-size:90%;">Performance on the dev split of VQA 2.0 of the multimodal model MM<sub id="S6.T4.10.2.1" class="ltx_sub">BERT</sub> and two text-only models: PICa-Full and CBM<sub id="S6.T4.10.2.2" class="ltx_sub">BERT</sub>.</span></figcaption>
</figure>
<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text" style="color:#000000;">Even though both unimodal and multimodal methods perform similarly in OK-VQA, we observed a different trend in VQA 2.0. Table </span><a href="#S6.T4" title="Table 4 ‣ 6.1 Results on VQA 2.0 ‣ 6 Analysis ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S6.SS1.p1.1.2" class="ltx_text" style="color:#000000;"> shows that CBM</span><sub id="S6.SS1.p1.1.3" class="ltx_sub"><span id="S6.SS1.p1.1.3.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS1.p1.1.4" class="ltx_text" style="color:#000000;"> obtains </span><math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="59.6" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mn mathcolor="#000000" id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">59.6</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><cn type="float" id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">59.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">59.6</annotation></semantics></math><span id="S6.SS1.p1.1.5" class="ltx_text" style="color:#000000;">, while MM</span><sub id="S6.SS1.p1.1.6" class="ltx_sub"><span id="S6.SS1.p1.1.6.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS1.p1.1.7" class="ltx_text" style="color:#000000;"> achieves 6 points more. We think this is due to the information loss when converting an image into a caption, as relevant information that is needed to answer the question can be lost. This is specially important for VQA 2.0, where the questions refer directly to image contents, spatial relations and object attributes (see Figure </span><a href="#S3.F2" title="Figure 2 ‣ 3.4 Loss function ‣ 3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S6.SS1.p1.1.8" class="ltx_text" style="color:#000000;">). A similar behavior can be observed for PICa </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS1.p1.1.9.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">8</span></a><span id="S6.SS1.p1.1.10.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S6.SS1.p1.1.11" class="ltx_text" style="color:#000000;">. Interestingly, PICa also uses object tags to minimize the information loss when verbalizing the image, but it does not perform as well as our system. Even with 1000 times less parameters, our CBM</span><sub id="S6.SS1.p1.1.12" class="ltx_sub"><span id="S6.SS1.p1.1.12.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS1.p1.1.13" class="ltx_text" style="color:#000000;"> outperforms PICa, showing the importance of fine-tuning in contrast to in-context-learning, specially when large training data is available, as in VQA 2.0.</span></p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text" style="color:#000000;">The difference between VQA and OK-VQA performances suggests that captions contain enough information to effectively use the implicit knowledge of language models for knowledge-intensive multimodal tasks like OK-VQA, but that in datasets where the answer can be found in the image, multimodal models preferable.</span></p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Combining visual information and captions</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Given the different nature of the inputs, we wanted to check whether CBM</span><sub id="S6.SS2.p1.1.2" class="ltx_sub"><span id="S6.SS2.p1.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p1.1.3" class="ltx_text" style="color:#000000;"> and MM</span><sub id="S6.SS2.p1.1.4" class="ltx_sub"><span id="S6.SS2.p1.1.4.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p1.1.5" class="ltx_text" style="color:#000000;"> are complementary. Our hypothesis is that the former can take advantage of the implicit knowledge acquired by the language model, whereas the latter has access to more fine-grained information found in image regions. Therefore, we define two different approaches to check how they complement each other.</span></p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Early fusion.</span><span id="S6.SS2.p2.1.2" class="ltx_text" style="color:#000000;"> For each question we feed both caption and image features alongside the question to the language model. This system can be seen as a MM</span><sub id="S6.SS2.p2.1.3" class="ltx_sub"><span id="S6.SS2.p2.1.3.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p2.1.4" class="ltx_text" style="color:#000000;"> which processes a multimodal input composed by a question (text), a caption (text) and image region features. We initialize the weights of this model with the weights of the base language model (BERT-base) and fine-tune it on the target train data.</span></p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Late fusion.</span><span id="S6.SS2.p3.1.2" class="ltx_text" style="color:#000000;"> We train CBM</span><sub id="S6.SS2.p3.1.3" class="ltx_sub"><span id="S6.SS2.p3.1.3.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p3.1.4" class="ltx_text" style="color:#000000;"> (Section </span><a href="#S3.SS1" title="3.1 Caption-based model (CBM) ‣ 3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3.1</span></a><span id="S6.SS2.p3.1.5" class="ltx_text" style="color:#000000;">) and MM</span><sub id="S6.SS2.p3.1.6" class="ltx_sub"><span id="S6.SS2.p3.1.6.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p3.1.7" class="ltx_text" style="color:#000000;"> (Section </span><a href="#S3.SS2" title="3.2 Multimodal transformer (MMBERT) ‣ 3 Implemented models ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3.2</span></a><span id="S6.SS2.p3.1.8" class="ltx_text" style="color:#000000;">) separately, each of them with their corresponding inputs, and combine their outputs in inference time to obtain the final answer. The combination is done by multiplying output probabilities of both models for each class and taking the answer with the highest value. We show their performance in Table </span><a href="#S6.T5" title="Table 5 ‣ 6.2 Combining visual information and captions ‣ 6 Analysis ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S6.SS2.p3.1.9" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p"><span id="S6.SS2.p4.1.1" class="ltx_text" style="color:#000000;">These fusion models improve the performance of both CBM</span><sub id="S6.SS2.p4.1.2" class="ltx_sub"><span id="S6.SS2.p4.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p4.1.3" class="ltx_text" style="color:#000000;"> and MM</span><sub id="S6.SS2.p4.1.4" class="ltx_sub"><span id="S6.SS2.p4.1.4.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p4.1.5" class="ltx_text" style="color:#000000;"> by 2-3 points in almost all cases. The only case where there is no improvement comparing to CBM</span><sub id="S6.SS2.p4.1.6" class="ltx_sub"><span id="S6.SS2.p4.1.6.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p4.1.7" class="ltx_text" style="color:#000000;"> is in the early fusion without VQA pretraining. This may be caused again by the small training split of OK-VQA, causing difficulties to learn how to ground textual and visual modalities. However, this is solved when VQA pretraining is added to the model, increasing vastly the amount of data seen by the models and showing similar performance on both early and late fusion models.</span></p>
</div>
<div id="S6.SS2.p5" class="ltx_para">
<p id="S6.SS2.p5.2" class="ltx_p"><span id="S6.SS2.p5.2.1" class="ltx_text" style="color:#000000;">Additionally, we also observed the complementarity of both modalities in the VQA dataset. Early fusion obtains </span><math id="S6.SS2.p5.1.m1.1" class="ltx_Math" alttext="67.8\%" display="inline"><semantics id="S6.SS2.p5.1.m1.1a"><mrow id="S6.SS2.p5.1.m1.1.1" xref="S6.SS2.p5.1.m1.1.1.cmml"><mn mathcolor="#000000" id="S6.SS2.p5.1.m1.1.1.2" xref="S6.SS2.p5.1.m1.1.1.2.cmml">67.8</mn><mo mathcolor="#000000" id="S6.SS2.p5.1.m1.1.1.1" xref="S6.SS2.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p5.1.m1.1b"><apply id="S6.SS2.p5.1.m1.1.1.cmml" xref="S6.SS2.p5.1.m1.1.1"><csymbol cd="latexml" id="S6.SS2.p5.1.m1.1.1.1.cmml" xref="S6.SS2.p5.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.SS2.p5.1.m1.1.1.2.cmml" xref="S6.SS2.p5.1.m1.1.1.2">67.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p5.1.m1.1c">67.8\%</annotation></semantics></math><span id="S6.SS2.p5.2.2" class="ltx_text" style="color:#000000;"> and late fusion </span><math id="S6.SS2.p5.2.m2.1" class="ltx_Math" alttext="67.7\%" display="inline"><semantics id="S6.SS2.p5.2.m2.1a"><mrow id="S6.SS2.p5.2.m2.1.1" xref="S6.SS2.p5.2.m2.1.1.cmml"><mn mathcolor="#000000" id="S6.SS2.p5.2.m2.1.1.2" xref="S6.SS2.p5.2.m2.1.1.2.cmml">67.7</mn><mo mathcolor="#000000" id="S6.SS2.p5.2.m2.1.1.1" xref="S6.SS2.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p5.2.m2.1b"><apply id="S6.SS2.p5.2.m2.1.1.cmml" xref="S6.SS2.p5.2.m2.1.1"><csymbol cd="latexml" id="S6.SS2.p5.2.m2.1.1.1.cmml" xref="S6.SS2.p5.2.m2.1.1.1">percent</csymbol><cn type="float" id="S6.SS2.p5.2.m2.1.1.2.cmml" xref="S6.SS2.p5.2.m2.1.1.2">67.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p5.2.m2.1c">67.7\%</annotation></semantics></math><span id="S6.SS2.p5.2.3" class="ltx_text" style="color:#000000;"> in the dev split of VQA 2.0, improving the performance of MM</span><sub id="S6.SS2.p5.2.4" class="ltx_sub"><span id="S6.SS2.p5.2.4.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS2.p5.2.5" class="ltx_text" style="color:#000000;"> by 2 points. The results validate our hypothesis, showing that image region features and captions are complementary in this setting.</span></p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><math id="S6.T5.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{Model}" display="inline"><semantics id="S6.T5.1.1.1.m1.1a"><mi mathcolor="#000000" id="S6.T5.1.1.1.m1.1.1" xref="S6.T5.1.1.1.m1.1.1.cmml">Model</mi><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.m1.1.1">Model</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.m1.1c">\mathrm{Model}</annotation></semantics></math></th>
<th id="S6.T5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.2.1" class="ltx_text" style="color:#000000;">Score</span></th>
<th id="S6.T5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.3.1" class="ltx_text" style="color:#000000;">+ VQA pretraining</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.5.5" class="ltx_tr">
<th id="S6.T5.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T5.5.5.5.1" class="ltx_text" style="color:#000000;">Early Fusion</span></th>
<td id="S6.T5.3.3.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S6.T5.2.2.1.m1.1" class="ltx_Math" alttext="32.5" display="inline"><semantics id="S6.T5.2.2.1.m1.1a"><mn mathcolor="#000000" id="S6.T5.2.2.1.m1.1.1" xref="S6.T5.2.2.1.m1.1.1.cmml">32.5</mn><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.1.m1.1b"><cn type="float" id="S6.T5.2.2.1.m1.1.1.cmml" xref="S6.T5.2.2.1.m1.1.1">32.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.1.m1.1c">32.5</annotation></semantics></math><span id="S6.T5.3.3.2.1" class="ltx_text" style="color:#000000;"> </span><math id="S6.T5.3.3.2.m2.1" class="ltx_Math" alttext="\pm 0.4" display="inline"><semantics id="S6.T5.3.3.2.m2.1a"><mrow id="S6.T5.3.3.2.m2.1.1" xref="S6.T5.3.3.2.m2.1.1.cmml"><mo mathcolor="#000000" mathsize="90%" id="S6.T5.3.3.2.m2.1.1a" xref="S6.T5.3.3.2.m2.1.1.cmml">±</mo><mn mathcolor="#000000" mathsize="90%" id="S6.T5.3.3.2.m2.1.1.2" xref="S6.T5.3.3.2.m2.1.1.2.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.3.3.2.m2.1b"><apply id="S6.T5.3.3.2.m2.1.1.cmml" xref="S6.T5.3.3.2.m2.1.1"><csymbol cd="latexml" id="S6.T5.3.3.2.m2.1.1.1.cmml" xref="S6.T5.3.3.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T5.3.3.2.m2.1.1.2.cmml" xref="S6.T5.3.3.2.m2.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.3.2.m2.1c">\pm 0.4</annotation></semantics></math>
</td>
<td id="S6.T5.5.5.4" class="ltx_td ltx_align_center ltx_border_t">
<math id="S6.T5.4.4.3.m1.1" class="ltx_Math" alttext="38.2" display="inline"><semantics id="S6.T5.4.4.3.m1.1a"><mn mathcolor="#000000" id="S6.T5.4.4.3.m1.1.1" xref="S6.T5.4.4.3.m1.1.1.cmml">38.2</mn><annotation-xml encoding="MathML-Content" id="S6.T5.4.4.3.m1.1b"><cn type="float" id="S6.T5.4.4.3.m1.1.1.cmml" xref="S6.T5.4.4.3.m1.1.1">38.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.4.4.3.m1.1c">38.2</annotation></semantics></math><span id="S6.T5.5.5.4.1" class="ltx_text" style="color:#000000;"> </span><math id="S6.T5.5.5.4.m2.1" class="ltx_Math" alttext="\pm 0.8" display="inline"><semantics id="S6.T5.5.5.4.m2.1a"><mrow id="S6.T5.5.5.4.m2.1.1" xref="S6.T5.5.5.4.m2.1.1.cmml"><mo mathcolor="#000000" mathsize="90%" id="S6.T5.5.5.4.m2.1.1a" xref="S6.T5.5.5.4.m2.1.1.cmml">±</mo><mn mathcolor="#000000" mathsize="90%" id="S6.T5.5.5.4.m2.1.1.2" xref="S6.T5.5.5.4.m2.1.1.2.cmml">0.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.5.5.4.m2.1b"><apply id="S6.T5.5.5.4.m2.1.1.cmml" xref="S6.T5.5.5.4.m2.1.1"><csymbol cd="latexml" id="S6.T5.5.5.4.m2.1.1.1.cmml" xref="S6.T5.5.5.4.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T5.5.5.4.m2.1.1.2.cmml" xref="S6.T5.5.5.4.m2.1.1.2">0.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.5.5.4.m2.1c">\pm 0.8</annotation></semantics></math>
</td>
</tr>
<tr id="S6.T5.9.9" class="ltx_tr">
<th id="S6.T5.9.9.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S6.T5.9.9.5.1" class="ltx_text" style="color:#000000;">Late Fusion</span></th>
<td id="S6.T5.7.7.2" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S6.T5.6.6.1.m1.1" class="ltx_Math" alttext="34.0" display="inline"><semantics id="S6.T5.6.6.1.m1.1a"><mn mathcolor="#000000" id="S6.T5.6.6.1.m1.1.1" xref="S6.T5.6.6.1.m1.1.1.cmml">34.0</mn><annotation-xml encoding="MathML-Content" id="S6.T5.6.6.1.m1.1b"><cn type="float" id="S6.T5.6.6.1.m1.1.1.cmml" xref="S6.T5.6.6.1.m1.1.1">34.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.6.6.1.m1.1c">34.0</annotation></semantics></math><span id="S6.T5.7.7.2.1" class="ltx_text" style="color:#000000;"> </span><math id="S6.T5.7.7.2.m2.1" class="ltx_Math" alttext="\pm 0.4" display="inline"><semantics id="S6.T5.7.7.2.m2.1a"><mrow id="S6.T5.7.7.2.m2.1.1" xref="S6.T5.7.7.2.m2.1.1.cmml"><mo mathcolor="#000000" mathsize="90%" id="S6.T5.7.7.2.m2.1.1a" xref="S6.T5.7.7.2.m2.1.1.cmml">±</mo><mn mathcolor="#000000" mathsize="90%" id="S6.T5.7.7.2.m2.1.1.2" xref="S6.T5.7.7.2.m2.1.1.2.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.7.7.2.m2.1b"><apply id="S6.T5.7.7.2.m2.1.1.cmml" xref="S6.T5.7.7.2.m2.1.1"><csymbol cd="latexml" id="S6.T5.7.7.2.m2.1.1.1.cmml" xref="S6.T5.7.7.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T5.7.7.2.m2.1.1.2.cmml" xref="S6.T5.7.7.2.m2.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.7.7.2.m2.1c">\pm 0.4</annotation></semantics></math>
</td>
<td id="S6.T5.9.9.4" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S6.T5.8.8.3.m1.1" class="ltx_Math" alttext="\bf{38.6}" display="inline"><semantics id="S6.T5.8.8.3.m1.1a"><mn class="ltx_mathvariant_bold" mathcolor="#000000" mathvariant="bold" id="S6.T5.8.8.3.m1.1.1" xref="S6.T5.8.8.3.m1.1.1.cmml">38.6</mn><annotation-xml encoding="MathML-Content" id="S6.T5.8.8.3.m1.1b"><cn type="float" id="S6.T5.8.8.3.m1.1.1.cmml" xref="S6.T5.8.8.3.m1.1.1">38.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.8.8.3.m1.1c">\bf{38.6}</annotation></semantics></math><span id="S6.T5.9.9.4.1" class="ltx_text" style="color:#000000;"> </span><math id="S6.T5.9.9.4.m2.1" class="ltx_Math" alttext="\pm 0.2" display="inline"><semantics id="S6.T5.9.9.4.m2.1a"><mrow id="S6.T5.9.9.4.m2.1.1" xref="S6.T5.9.9.4.m2.1.1.cmml"><mo mathcolor="#000000" mathsize="90%" id="S6.T5.9.9.4.m2.1.1a" xref="S6.T5.9.9.4.m2.1.1.cmml">±</mo><mn mathcolor="#000000" mathsize="90%" id="S6.T5.9.9.4.m2.1.1.2" xref="S6.T5.9.9.4.m2.1.1.2.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.9.9.4.m2.1b"><apply id="S6.T5.9.9.4.m2.1.1.cmml" xref="S6.T5.9.9.4.m2.1.1"><csymbol cd="latexml" id="S6.T5.9.9.4.m2.1.1.1.cmml" xref="S6.T5.9.9.4.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T5.9.9.4.m2.1.1.2.cmml" xref="S6.T5.9.9.4.m2.1.1.2">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.9.9.4.m2.1c">\pm 0.2</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S6.T5.13.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S6.T5.14.2" class="ltx_text" style="font-size:90%;">Performance on OK-VQA for early and late fusion models.</span></figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Ground truth captions</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p"><span id="S6.SS3.p1.1.1" class="ltx_text" style="color:#000000;">In order to measure the effects of the image captioning system to our proposed CBM model, we check the gap of performance between the use of generated captions and gold captions. As OK-VQA is built upon images from the COCO dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS3.p1.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">43</span></a><span id="S6.SS3.p1.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S6.SS3.p1.1.4" class="ltx_text" style="color:#000000;">, each image has five different annotated captions. We use these captions and fine-tune CBM</span><sub id="S6.SS3.p1.1.5" class="ltx_sub"><span id="S6.SS3.p1.1.5.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS3.p1.1.6" class="ltx_text" style="color:#000000;"> on OK-VQA without VQA pretraining following the same experimental settings.
We repeat this experiment three times, as it is done through the entire work. On each run we select a different set of captions, that is, for each image we just choose one gold caption randomly and use it during the entire training process. As we also have several captions in all of OK-VQA’s test split, we test each fine-tuned model three times following the same caption selection process.</span></p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.2" class="ltx_p"><span id="S6.SS3.p2.2.1" class="ltx_text" style="color:#000000;">Table </span><a href="#S5.T1" title="Table 1 ‣ 5.2 Results for images vs. captions ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S6.SS3.p2.2.2" class="ltx_text" style="color:#000000;"> already shows that we achieve an accuracy and standard deviation of </span><math id="S6.SS3.p2.1.m1.1" class="ltx_Math" alttext="32.5\pm 0.4" display="inline"><semantics id="S6.SS3.p2.1.m1.1a"><mrow id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml"><mn mathcolor="#000000" id="S6.SS3.p2.1.m1.1.1.2" xref="S6.SS3.p2.1.m1.1.1.2.cmml">32.5</mn><mo mathcolor="#000000" id="S6.SS3.p2.1.m1.1.1.1" xref="S6.SS3.p2.1.m1.1.1.1.cmml">±</mo><mn mathcolor="#000000" id="S6.SS3.p2.1.m1.1.1.3" xref="S6.SS3.p2.1.m1.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><apply id="S6.SS3.p2.1.m1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.SS3.p2.1.m1.1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.SS3.p2.1.m1.1.1.2.cmml" xref="S6.SS3.p2.1.m1.1.1.2">32.5</cn><cn type="float" id="S6.SS3.p2.1.m1.1.1.3.cmml" xref="S6.SS3.p2.1.m1.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">32.5\pm 0.4</annotation></semantics></math><span id="S6.SS3.p2.2.3" class="ltx_text" style="color:#000000;"> using generated captions on OK-VQA’s test split. However, when we use gold captions we get an average accuracy of </span><math id="S6.SS3.p2.2.m2.1" class="ltx_Math" alttext="32.3\pm 0.3" display="inline"><semantics id="S6.SS3.p2.2.m2.1a"><mrow id="S6.SS3.p2.2.m2.1.1" xref="S6.SS3.p2.2.m2.1.1.cmml"><mn mathcolor="#000000" id="S6.SS3.p2.2.m2.1.1.2" xref="S6.SS3.p2.2.m2.1.1.2.cmml">32.3</mn><mo mathcolor="#000000" id="S6.SS3.p2.2.m2.1.1.1" xref="S6.SS3.p2.2.m2.1.1.1.cmml">±</mo><mn mathcolor="#000000" id="S6.SS3.p2.2.m2.1.1.3" xref="S6.SS3.p2.2.m2.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.2.m2.1b"><apply id="S6.SS3.p2.2.m2.1.1.cmml" xref="S6.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S6.SS3.p2.2.m2.1.1.1.cmml" xref="S6.SS3.p2.2.m2.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.SS3.p2.2.m2.1.1.2.cmml" xref="S6.SS3.p2.2.m2.1.1.2">32.3</cn><cn type="float" id="S6.SS3.p2.2.m2.1.1.3.cmml" xref="S6.SS3.p2.2.m2.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m2.1c">32.3\pm 0.3</annotation></semantics></math><span id="S6.SS3.p2.2.4" class="ltx_text" style="color:#000000;"> in all of our runs. In both cases we obtain similar results, showing that captions generated by OSCAR contain enough information for CBM</span><sub id="S6.SS3.p2.2.5" class="ltx_sub"><span id="S6.SS3.p2.2.5.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS3.p2.2.6" class="ltx_text" style="color:#000000;"> to perform comparably on this specific task.</span></p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Qualitative Analysis on OK-VQA</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p"><span id="S6.SS4.p1.1.1" class="ltx_text" style="color:#000000;">Both CBM</span><sub id="S6.SS4.p1.1.2" class="ltx_sub"><span id="S6.SS4.p1.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p1.1.3" class="ltx_text" style="color:#000000;"> and its multimodal counterpart perform similarly (see Table </span><a href="#S5.T1" title="Table 1 ‣ 5.2 Results for images vs. captions ‣ 5 Experiments and results ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S6.SS4.p1.1.4" class="ltx_text" style="color:#000000;">), but in 38.7% of the test examples their output differs and only one of them is correct. Figure </span><a href="#S6.F4" title="Figure 4 ‣ 6.4 Qualitative Analysis on OK-VQA ‣ 6 Analysis ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S6.SS4.p1.1.5" class="ltx_text" style="color:#000000;"> shows some OK-VQA test examples together where the outputs of CBM</span><sub id="S6.SS4.p1.1.6" class="ltx_sub"><span id="S6.SS4.p1.1.6.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p1.1.7" class="ltx_text" style="color:#000000;"> and MM</span><sub id="S6.SS4.p1.1.8" class="ltx_sub"><span id="S6.SS4.p1.1.8.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p1.1.9" class="ltx_text" style="color:#000000;"> with VQA pretraining differ. We also add answers from CBM</span><sub id="S6.SS4.p1.1.10" class="ltx_sub"><span id="S6.SS4.p1.1.10.1" class="ltx_text" style="color:#000000;">T5-11B</span></sub><span id="S6.SS4.p1.1.11" class="ltx_text" style="color:#000000;"> for further comparisons.</span></p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2109.08029/assets/x4.png" id="S6.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="350" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S6.F4.8.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S6.F4.9.2" class="ltx_text" style="font-size:90%;">Examples of OK-VQA questions where only one between CBM<sub id="S6.F4.9.2.1" class="ltx_sub">BERT and MM<sub id="S6.F4.9.2.1.1" class="ltx_sub">BERT</sub></sub> answers correctly according to the ground truth (GT). We also show answers given by CBM<sub id="S6.F4.9.2.2" class="ltx_sub">T5-11B</sub> for further comparisons. C refers to captions generated by OSCAR. Correct answer in green, incorrect in red.</span></figcaption>
</figure>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p"><span id="S6.SS4.p2.1.1" class="ltx_text" style="color:#000000;">Starting with the top-left example, CBM</span><sub id="S6.SS4.p2.1.2" class="ltx_sub"><span id="S6.SS4.p2.1.2.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p2.1.3" class="ltx_text" style="color:#000000;"> can infer that elephants are native to Africa whereas MM</span><sub id="S6.SS4.p2.1.4" class="ltx_sub"><span id="S6.SS4.p2.1.4.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p2.1.5" class="ltx_text" style="color:#000000;"> does not. In fact, the generated caption includes the information that the animal found in the image is an elephant, performing the first step needed to answer the question. This way, the LM can focus on using its implicit knowledge in order to answer correctly. CBM</span><sub id="S6.SS4.p2.1.6" class="ltx_sub"><span id="S6.SS4.p2.1.6.1" class="ltx_text" style="color:#000000;">T5</span></sub><span id="S6.SS4.p2.1.7" class="ltx_text" style="color:#000000;"> generates ’forest’ as an answer. Although the answer may be considered as valid to us, the answer is not within the list of ground truth answers, making it incorrect.
The other two examples in the top row behave similarly. The caption facilitates the grounding between the question and the image. Whenever a question refers to the image (“this fruit” and “these items”), if the caption already mentions these objects (“bananas” and “traffic light”, respectively), the LM seems to better leverage its implicit knowledge and reasoning capabilities to answer the question. The top-right example is interesting in this regard. While the image shows red traffic lights, the question asks about the effects of green lights. This may trick MM</span><sub id="S6.SS4.p2.1.8" class="ltx_sub"><span id="S6.SS4.p2.1.8.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p2.1.9" class="ltx_text" style="color:#000000;"> into answering the effect that red lights produce, not the green ones.</span></p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p"><span id="S6.SS4.p3.1.1" class="ltx_text" style="color:#000000;">The bottom row of Figure </span><a href="#S6.F4" title="Figure 4 ‣ 6.4 Qualitative Analysis on OK-VQA ‣ 6 Analysis ‣ Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S6.SS4.p3.1.2" class="ltx_text" style="color:#000000;"> shows two examples where the caption does not give enough information to infer the answer for CBMs. In the first case CBMs cannot decide whether the meat is steamed, fried or grilled by only examining the caption, while MM</span><sub id="S6.SS4.p3.1.3" class="ltx_sub"><span id="S6.SS4.p3.1.3.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p3.1.4" class="ltx_text" style="color:#000000;"> does have access to visual cues of the image, where we can see that the meat is grilled. This also happens in the second example, as the caption does not specify any ingredient of the beverage while we can see fruits in the image. The rightmost example illustrates an example where the caption does support the inference, but where our BERT based CBM gets it wrong. With the given caption, “this game” refers to baseball, however, CBM</span><sub id="S6.SS4.p3.1.5" class="ltx_sub"><span id="S6.SS4.p3.1.5.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p3.1.6" class="ltx_text" style="color:#000000;"> is unable to infer that three strikes are enough for a strikeout whereas both CBM</span><sub id="S6.SS4.p3.1.7" class="ltx_sub"><span id="S6.SS4.p3.1.7.1" class="ltx_text" style="color:#000000;">T5-11B</span></sub><span id="S6.SS4.p3.1.8" class="ltx_text" style="color:#000000;"> and MM</span><sub id="S6.SS4.p3.1.9" class="ltx_sub"><span id="S6.SS4.p3.1.9.1" class="ltx_text" style="color:#000000;">BERT</span></sub><span id="S6.SS4.p3.1.10" class="ltx_text" style="color:#000000;"> manage to give the correct answer.</span></p>
</div>
<div id="S6.SS4.p4" class="ltx_para">
<p id="S6.SS4.p4.1" class="ltx_p"><span id="S6.SS4.p4.1.1" class="ltx_text" style="color:#000000;">All in all, these examples support our hypothesis that visual features and captions are complementary. They also show that our system has some advantages regarding the interpretability of the system, specially in the cases our method is wrong. In some cases like the two leftmost examples in the bottom row, the object or feature needed to answer the question is missing from the caption. In other cases, the required information is in the caption, but the inference is erroneous.</span></p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p"><span id="S7.p1.1.1" class="ltx_text" style="color:#000000;">In this paper we present a VQA system which describes images with a caption to then work only with textual data. We show that such a system performs surprisingly well in OK-VQA, where the questions cannot be answered with the image alone, requiring access to external knowledge. Our analysis indicates that the loss of information when summarizing the image into a caption is compensated by the better inference ability of text-only pretrained language models. We also show the importance of a language model’s capacity when leveraging the implicit knowledge found in it, achieving state-of-the-art results, outperforming current multimodal models by a large margin and matching a 15-times larger ensemble model. Compared to multimodal models, orders of magnitude bigger text-only LMs are available, which we show to be an advantage for knowledge-intensive tasks. In the future we would like to explore whether richer descriptions of images might improve results further, and whether large text-only language models can still benefit from incorporating symbolic knowledge graphs.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="color:#000000;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="color:#000000;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="color:#000000;">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, D. Parikh,
VQA: Visual Question Answering, in: International Conference on
Computer Vision (ICCV), 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="color:#000000;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="color:#000000;">
N. Xie, F. Lai, D. Doran, A. Kadav, Visual entailment: A novel task for
fine-grained image understanding, arXiv preprint arXiv:1901.06706.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="color:#000000;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="color:#000000;">
K. Marino, M. Rastegari, A. Farhadi, R. Mottaghi, Ok-vqa: A visual question
answering benchmark requiring external knowledge., in: CVPR, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="color:#000000;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="color:#000000;">
K. Marino, X. Chen, D. Parikh, A. Gupta, M. Rohrbach, Krisp: Integrating
implicit and symbolic knowledge for open-domain knowledge-based vqa, arXiv
e-prints (2020) arXiv–2012.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="color:#000000;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="color:#000000;">
R. Speer, J. Chin, C. Havasi, Conceptnet 5.5: An open multilingual graph of
general knowledge, in: Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 31, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="color:#000000;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="color:#000000;">
J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep
bidirectional transformers for language understanding, in: Proceedings of the
2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), Association for Computational Linguistics, Minneapolis,
Minnesota, 2019, pp. 4171–4186.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.18653/v1/N19-1423</span></a><span id="bib.bib6.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="color:#000000;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="color:#000000;">
F. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu,
A. Miller, Language models as knowledge bases?, in: Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
2019, pp. 2463–2473.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="color:#000000;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="color:#000000;">
Z. Yang, Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, L. Wang,
</span><a target="_blank" href="https://arxiv.org/abs/2109.05014" title="" class="ltx_ref ltx_href" style="color:#000000;">An empirical study of GPT-3 for
few-shot knowledge-based VQA</a><span id="bib.bib8.5.2" class="ltx_text" style="color:#000000;">, CoRR abs/2109.05014.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2109.05014" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2109.05014</span></a><span id="bib.bib8.6.1" class="ltx_text" style="color:#000000;">.
</span>
<br class="ltx_break"><span id="bib.bib8.7.2" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://arxiv.org/abs/2109.05014" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://arxiv.org/abs/2109.05014</a><span id="bib.bib8.8.3" class="ltx_text" style="color:#000000;">
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="color:#000000;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="color:#000000;">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
W. Li, P. J. Liu, Exploring the limits of transfer learning with a unified
text-to-text transformer, Journal of Machine Learning Research 21 (2020)
1–67.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="color:#000000;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="color:#000000;">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, D. Parikh, Making the V in
VQA matter: Elevating the role of image understanding in visual question
answering, in: 2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, IEEE
Computer Society, 2017, pp. 6325–6334.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/10.1109/CVPR.2017.670" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/CVPR.2017.670</span></a><span id="bib.bib10.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="color:#000000;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="color:#000000;">
J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick,
R. Girshick, Clevr: A diagnostic dataset for compositional language and
elementary visual reasoning, in: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2017, pp. 2901–2910.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="color:#000000;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="color:#000000;">
P. Wang, Q. Wu, C. Shen, A. R. Dick, A. van den Hengel, Explicit
knowledge-based reasoning for visual question answering, in: IJCAI, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="color:#000000;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="color:#000000;">
N. Y. Sanket Shah, Anand Mishra, P. P. Talukdar, Kvqa: Knowledge-aware visual
question answering, in: AAAI, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="color:#000000;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="color:#000000;">
P. Wang, Q. Wu, C. Shen, A. Dick, A. Van Den Hengel, Fvqa: Fact-based visual
question answering, IEEE transactions on pattern analysis and machine
intelligence 40 (10) (2017) 2413–2427.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="color:#000000;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="color:#000000;">
L. H. Li, M. Yatskar, D. Yin, C. Hsieh, K. Chang, Visualbert: A simple and
performant baseline for vision and language, CoRR abs/1908.03557.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1908.03557" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1908.03557</span></a><span id="bib.bib15.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="color:#000000;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="color:#000000;">
X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong,
F. Wei, et al., Oscar: Object-semantics aligned pre-training for
vision-language tasks, in: European Conference on Computer Vision, Springer,
2020, pp. 121–137.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="color:#000000;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="color:#000000;">
J. Lu, D. Batra, D. Parikh, S. Lee, Vilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks, in: H. M.
Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B. Fox,
R. Garnett (Eds.), Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 13–23.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="color:#000000;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="color:#000000;">
H. Tan, M. Bansal, LXMERT: learning cross-modality encoder representations
from transformers, in: K. Inui, J. Jiang, V. Ng, X. Wan (Eds.), Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Association for
Computational Linguistics, 2019, pp. 5099–5110.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/10.18653/v1/D19-1514" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.18653/v1/D19-1514</span></a><span id="bib.bib18.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="color:#000000;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="color:#000000;">
E. Bugliarello, R. Cotterell, N. Okazaki, D. Elliott, Multimodal pretraining
unmasked: A meta-analysis and a unified framework of vision-and-language
berts, arXiv preprint arXiv:2011.15124.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="color:#000000;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="color:#000000;">
P. Sharma, N. Ding, S. Goodman, R. Soricut, Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning, in:
Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 2018, pp. 2556–2565.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="color:#000000;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="color:#000000;">
F. Gardères, M. Ziaeefard, B. Abeloos, F. Lecue, ConceptBert:
Concept-aware representation for visual question answering, in: Findings of
the Association for Computational Linguistics: EMNLP 2020, Association for
Computational Linguistics, Online, 2020, pp. 489–498.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/10.18653/v1/2020.findings-emnlp.44" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.18653/v1/2020.findings-emnlp.44</span></a><span id="bib.bib21.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="color:#000000;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="color:#000000;">
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z. Ives, Dbpedia: A
nucleus for a web of open data, in: The semantic web, Springer, 2007, pp.
722–735.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="color:#000000;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="color:#000000;">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, et al., Visual genome: Connecting
language and vision using crowdsourced dense image annotations, International
journal of computer vision 123 (1) (2017) 32–73.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="color:#000000;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="color:#000000;">
S. Bhakthavatsalam, K. Richardson, N. Tandon, P. Clark, Do dogs have whiskers?
a new knowledge base of haspart relations, arXiv preprint arXiv:2006.07510.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="color:#000000;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="color:#000000;">
J. Wu, J. Lu, A. Sabharwal, R. Mottaghi, Multi-modal answer validation for
knowledge-based vqa, arXiv preprint arXiv:2103.12248.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="color:#000000;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="color:#000000;">
V. Shevchenko, D. Teney, A. Dick, A. van den Hengel, Reasoning over vision and
language: Exploring the benefits of supplemental knowledge, in: Proceedings
of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world
kNowledge (LANTERN), Association for Computational Linguistics, Kyiv,
Ukraine, 2021, pp. 1–18.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="color:#000000;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="color:#000000;">
R. Kumari, A. Ekbal,
</span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417421008320" title="" class="ltx_ref ltx_href" style="color:#000000;">Amfb:
Attention based multimodal factorized bilinear pooling for multimodal fake
news detection</a><span id="bib.bib27.5.2" class="ltx_text" style="color:#000000;">, Expert Systems with Applications 184 (2021) 115412.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.eswa.2021.115412" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:https://doi.org/10.1016/j.eswa.2021.115412</span></a><span id="bib.bib27.6.1" class="ltx_text" style="color:#000000;">.
</span>
<br class="ltx_break"><span id="bib.bib27.7.2" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417421008320" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://www.sciencedirect.com/science/article/pii/S0957417421008320</a><span id="bib.bib27.8.3" class="ltx_text" style="color:#000000;">
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="color:#000000;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="color:#000000;">
K. I. Bae, J. Park, J. Lee, Y. Lee, C. Lim,
</span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417420302797" title="" class="ltx_ref ltx_href" style="color:#000000;">Flower
classification with modified multimodal convolutional neural networks</a><span id="bib.bib28.5.2" class="ltx_text" style="color:#000000;">,
Expert Systems with Applications 159 (2020) 113455.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.eswa.2020.113455" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:https://doi.org/10.1016/j.eswa.2020.113455</span></a><span id="bib.bib28.6.1" class="ltx_text" style="color:#000000;">.
</span>
<br class="ltx_break"><span id="bib.bib28.7.2" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417420302797" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://www.sciencedirect.com/science/article/pii/S0957417420302797</a><span id="bib.bib28.8.3" class="ltx_text" style="color:#000000;">
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="color:#000000;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="color:#000000;">
Z. Ahmad, R. Jindal, M. N.S., A. Ekbal, P. Bhattachharyya,
</span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417422001166" title="" class="ltx_ref ltx_href" style="color:#000000;">Multi-modality
helps in crisis management: An attention-based deep learning approach of
leveraging text for image classification</a><span id="bib.bib29.5.2" class="ltx_text" style="color:#000000;">, Expert Systems with Applications
195 (2022) 116626.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.eswa.2022.116626" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:https://doi.org/10.1016/j.eswa.2022.116626</span></a><span id="bib.bib29.6.1" class="ltx_text" style="color:#000000;">.
</span>
<br class="ltx_break"><span id="bib.bib29.7.2" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417422001166" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://www.sciencedirect.com/science/article/pii/S0957417422001166</a><span id="bib.bib29.8.3" class="ltx_text" style="color:#000000;">
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="color:#000000;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="color:#000000;">
Z. Zhu, J. Yu, Y. Wang, Y. Sun, Y. Hu, Q. Wu, Mucko: Multi-layer cross-modal
knowledge reasoning for fact-based visual question answering, CoRR
abs/2006.09073.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2006.09073" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2006.09073</span></a><span id="bib.bib30.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="color:#000000;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="color:#000000;">
J. Johnson, A. Karpathy, L. Fei-Fei, Densecap: Fully convolutional localization
networks for dense captioning, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 4565–4574.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="color:#000000;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="color:#000000;">
A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh,
M. Rohrbach, Towards vqa models that can read, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.
8317–8326.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="color:#000000;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="color:#000000;">
H. Sharma, A. Singh Jalal,
</span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417421014822" title="" class="ltx_ref ltx_href" style="color:#000000;">A
framework for visual question answering with the integration of scene-text
using phocs and fisher vectors</a><span id="bib.bib33.5.2" class="ltx_text" style="color:#000000;">, Expert Systems with Applications 190 (2022)
116159.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.eswa.2021.116159" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:https://doi.org/10.1016/j.eswa.2021.116159</span></a><span id="bib.bib33.6.1" class="ltx_text" style="color:#000000;">.
</span>
<br class="ltx_break"><span id="bib.bib33.7.2" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0957417421014822" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://www.sciencedirect.com/science/article/pii/S0957417421014822</a><span id="bib.bib33.8.3" class="ltx_text" style="color:#000000;">
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="color:#000000;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="color:#000000;">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,
G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter,
C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei, Language
models are few-shot learners, in: H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, H. Lin (Eds.), Advances in Neural Information Processing
Systems, Vol. 33, Curran Associates, Inc., 2020, pp. 1877–1901.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="color:#000000;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="color:#000000;">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,
M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai,
S. Chintala, Pytorch: An imperative style, high-performance deep learning
library, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information
Processing Systems, Vol. 32, Curran Associates, Inc., 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="color:#000000;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="color:#000000;">
T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen,
C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest,
A. M. Rush, Transformers: State-of-the-art natural language processing, in:
Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations, Association for Computational Linguistics,
Online, 2020, pp. 38–45.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="color:#000000;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="color:#000000;">
S. Ren, K. He, R. B. Girshick, J. Sun, Faster R-CNN: towards real-time object
detection with region proposal networks, in: C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, R. Garnett (Eds.), Advances in Neural Information
Processing Systems 28: Annual Conference on Neural Information Processing
Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, 2015, pp.
91–99.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="color:#000000;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="color:#000000;">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, L. Zhang,
Bottom-up and top-down attention for image captioning and visual question
answering, in: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), IEEE Computer Society, 2018, pp. 6077–6086.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="color:#000000;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="color:#000000;">
P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, J. Gao, Vinvl:
Making visual representations matter in vision-language models, CoRR
abs/2101.00529.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2101.00529" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2101.00529</span></a><span id="bib.bib39.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="color:#000000;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="color:#000000;">
L. J. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, CoRR abs/1607.06450.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1607.06450" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1607.06450</span></a><span id="bib.bib40.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="color:#000000;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="color:#000000;">
S. Xie, R. Girshick, P. Dollár, Z. Tu, K. He, Aggregated residual
transformations for deep neural networks, arXiv preprint arXiv:1611.05431.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="color:#000000;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="color:#000000;">
A. Singh, V. Goswami, V. Natarajan, Y. Jiang, X. Chen, M. Shah, M. Rohrbach,
D. Batra, D. Parikh, Mmf: A multimodal framework for vision and language
research, </span><a target="_blank" href="https://github.com/facebookresearch/mmf" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://github.com/facebookresearch/mmf</a><span id="bib.bib42.5.2" class="ltx_text" style="color:#000000;"> (2020).
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="color:#000000;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="color:#000000;">
T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, C. L. Zitnick, Microsoft COCO: common objects in context,
in: D. J. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Computer Vision
- ECCV 2014 - 13th European Conference, Zurich, Switzerland, September
6-12, 2014, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer
Science, Springer, 2014, pp. 740–755.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/10.1007/978-3-319-10602-1_48" title="" class="ltx_ref ltx_href" style="color:#000000;"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1007/978-3-319-10602-1\_48</span></a><span id="bib.bib43.5.1" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="color:#000000;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="color:#000000;">
I. Loshchilov, F. Hutter, Decoupled weight decay regularization, in:
International Conference on Learning Representations, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="color:#000000;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="color:#000000;">
S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Memory optimizations toward
training trillion parameter models, in: SC20: International Conference for
High Performance Computing, Networking, Storage and Analysis, IEEE, 2020, pp.
1–16.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="color:#000000;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="color:#000000;">
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
S. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language
models, arXiv preprint arXiv:2001.08361.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.08028" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.08029" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.08029">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.08029" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.08031" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 12 09:31:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
