<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.03311] Federated Learning for Drowsiness Detection in Connected Vehicles</title><meta property="og:description" content="Ensuring driver readiness poses challenges, yet driver monitoring systems can assist in determining the driver’s state. By observing visual cues, such systems recognize various behaviors and associate them with specifi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning for Drowsiness Detection in Connected Vehicles">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning for Drowsiness Detection in Connected Vehicles">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.03311">

<!--Generated on Wed Jun  5 14:50:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning Driver Drowsiness Connected Vehicles">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>DENSO Automotive Deutschland GmbH, Freisinger Str. 21, 85386, Eching <span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>w.lindskog@eu.denso.com</span></span></span>
<br class="ltx_break"></span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Technical University of Munich, Germany
</span></span></span>
<h1 class="ltx_title ltx_title_document">Federated Learning for Drowsiness Detection in Connected Vehicles</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">William Lindskog
</span><span class="ltx_author_notes">11 2 2</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Valentin Spannagl
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christian Prehofer
</span><span class="ltx_author_notes">1122</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Ensuring driver readiness poses challenges, yet driver monitoring systems can assist in determining the driver’s state. By observing visual cues, such systems recognize various behaviors and associate them with specific conditions. For instance, yawning or eye blinking can indicate driver drowsiness. Consequently, an abundance of distributed data is generated for driver monitoring. Employing machine learning techniques, such as driver drowsiness detection, presents a potential solution. However, transmitting the data to a central machine for model training is impractical due to the large data size and privacy concerns. Conversely, training on a single vehicle would limit the available data and likely result in inferior performance. To address these issues, we propose a federated learning framework for drowsiness detection within a vehicular network, leveraging the YawDD dataset. Our approach achieves an accuracy of 99.2%, demonstrating its promise and comparability to conventional deep learning techniques. Lastly, we show how our model scales using various number of federated clients.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated Learning Driver Drowsiness Connected Vehicles
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Road accidents are predominantly caused by human errors, accounting for 90% of incidents in the United States in 2015 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. While drivers and passengers benefit from the safety features of vehicles, vulnerable road users such as cyclists and pedestrians remain at greater risk. Drowsy driving is a prevalent issue, with a significant number of accidents attributed to this cause.
The vision of fully automated vehicles offers potential solutions to address the problem of driver drowsiness. However, achieving fully automated driving remains a future aspiration, and the current trajectory suggests the adoption of shared driver-machine models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. For complex driving scenarios, the driver is required to assume control of the vehicle, necessitating a state of readiness.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Driver readiness is contingent upon various factors and influenced by the driver’s state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Automotive companies have developed different types of Driver Monitoring Systems (DMS) to enhance road safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. These systems analyze driver behavior and appearance to detect signs of hazardous conditions, such as distraction or drowsiness, indicating when the driver may not be prepared to assume control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. However, interpreting these signs can be challenging and subject to multiple interpretations. Consequently, Driver Monitoring encompasses several domains, with Driver Drowsiness Detection being one of them.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Various approaches have been employed to detect driver fatigue. Direct physiological measurements, involving the use of body sensors to track metrics like heart rate, have yielded promising results. Another approach utilizes vehicle sensors to monitor parameters like steering wheel angle, detecting anomalous patterns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. A third approach combines direct assessment of driver behavior with non-intrusive sensors. Optical algorithms, coupled with Deep Learning techniques, extract the driver’s state from camera recordings, demonstrating improved success rates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. However, in-vehicle Driver Monitoring presents challenges due to the large video data size and privacy constraints.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Federated Learning (FL), a Deep Learning methodology, offers a privacy-aware solution to train Machine Learning models on distributed data. By only transmitting the model parameters through the federated network, rather than raw data, FL reduces message sizes and minimizes the potential attack surface for adversarial attacks. FL has started to see real-world implementations, mainly in medicine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, but is being applied in other industries such as automotive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In order to tackle these challenges, our study introduces an FL framework tailored for drowsiness detection in a vehicular network, employing the YawDD dataset. Remarkably, our approach attains a remarkable accuracy rate of 99.2%, showcasing its potential and comparability to conventional deep learning methods.
Our main contributions are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Federated Learning framework for driver drowsiness detection using YawDD dataset for processing single frames and sequences.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">With our evaluation, we show how model performance scales when increasing the number of federated clients.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We achieve great results of 99.2% when classifying normal driving, talking and yawning driver.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Driver Drowsiness Detection is an actively researched area, and numerous studies have been conducted to address this critical issue. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> developed a benchmark implementation using the YawDD dataset, focusing on in-vehicle applications. However, the accuracy of their model is limited.
It is evident that there is room for improvement in achieving higher detection accuracies for driver drowsiness.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In recent works, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed a Convolutional Neural Network (CNN) architecture on the YawDD dataset, utilizing ensemble learning to achieve an impressive accuracy of approximately 99%. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> employed a simple CNN with dropout technique, obtaining an average accuracy of 96% on YawDD by focusing on the eyes and mouth and using labels of open or closed states for these facial features. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> introduced the PERCLOSE formulation, utilizing the eye area to determine drowsiness on the YawDD dataset. However, this approach encounters challenges when the driver’s face or eyes are not detectable. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> explored the application of Recurrent Neural Networks (RNNs) on the YawDD dataset, achieving an accuracy of 96%.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The aforementioned studies predominantly employ conventional deep learning techniques, such as ensemble learning, CNNs, and RNNs, to attain high accuracies in driver drowsiness detection. However, the integration of federated learning in the driver monitoring field remains underrepresented. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> presented one of the few works focusing on drowsiness detection using FL. They proposed a two-stage approach, utilizing the PERCLOSE method and another drowsiness metric called FOM, to identify fatigue. The application of the federated learning strategy called Dynamic Averaging yielded promising results, and the performance evaluation was conducted on the NTHU dataset. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> also used the NTHU dataset and the YawDD dataset. They present a privacy-preserving federated transfer learning method called PFTL-DDD for detecting driver drowsiness. The proposed method uses fine-tuning transfer learning on the FL system’s initial model and a CKKS-based security protocol to encrypt exchanged parameters, protecting driver privacy. The results show that the method is more accurate and efficient compared to conventional FL methods and reduces communication costs. Nevertheless, they do not include how many clients the evaluate their models with. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> also proposed a federated transfer learning model, but applied it to construction workers and fatigue monitoring. Lastly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposed an asynchronous federated scheme in internet of vehicles. They evaluate their model EHAFL on YawDD dataset and show how their model can reduce communication costs with 98% with a slight decrease in accuracy.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">While conventional deep learning techniques have demonstrated success in driver drowsiness detection, the potential of FL in this domain remains largely untapped. Moreover, the studies that have investigated FL for DMS do not show how performance scales when number of participating clients is increased.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>YawDD Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The YawDD dataset, as documented by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, comprises video recordings of drivers exhibiting various behaviors, with particular emphasis on yawning as a key indicator of drowsiness. To establish a baseline, normal driving videos were included to depict drivers in an alert state, free from drowsiness. Another category of videos captured drivers engaged in conversation. In a binary classification task involving yawning and normal driving, the act of mouth opening serves as an indication of yawning behavior. However, the introduction of talking data poses a challenge as this behavior can no longer be solely relied upon. The YawDD dataset consists of two subsets: one containing 322 videos and the other with 29 videos. The larger subset captures drivers from a rear mirror perspective, while the smaller subset features a camera placed in front of the driver on the dashboard, see Figure <a href="#S3.F1" title="Figure 1 ‣ 3 YawDD Dataset ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. A total of 107 drivers (57 male and 50 female) were recorded, with each driver providing at least three videos for each behavior category. Additionally, the YawDD dataset includes recordings with occlusions such as sunglasses or scarves.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.03311/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Samples from the YawDD dataset in two perspectives. Top: Rear mirror. Bottom: Dash <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Preprocessing</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Given that the provided labels in the YawDD dataset were different, encompassing a few seconds before and after the actual yawning event in a yawning video, we performed frame-level labeling of the entire dataset to ensure precise classification for this study. Analyzing the dataset is crucial to identify its properties. The dataset is divided into two subsets based on camera perspectives: rear mirror (320 items) and dash (29 items). Despite both perspectives being realistic, the larger rear mirror set is selected for further analysis. The file names reveal a potential issue: some videos have dual labels. For instance, a sample video includes footage of talking and yawning. Upon reviewing sample videos, labeling appears more problematic as a video labeled ”Yawning” contains a yawning event within normal driving. The drivers are throughout the videos driving normally, talking or yawning; thus we tackle a 3-class classification task.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2405.03311/assets/Figures/sample_category_issue.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="104" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample includes two categories. </figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Resizing is the subsequent step for data reduction while preserving crucial information. The camera used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> captures YawDD data at a resolution of 640x480 pixels, resulting in 307,200 pixel values per color layer. In the RGB format, this equates to 921,600 values per frame. Deep learning memory consumption scales with input data size, so minimizing input sample size is preferable. Lowering resolution causes image structures to blur. At 80x80 pixels, objects become indistinct, such as glasses merging with the eye structure. Resolutions above 160x160 pixels are recommended, with 320x320 already offering improvement compared to the original. The last step in frame pre-processing involves color representation. Since the driver drowsiness detection approach relies on behavioral measures, driver actions play a vital role. Yawning and eye blinking, identified as indicators, are not influenced by color representation. Grayscaling converts RGB to grayscale, significantly reducing data size.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method for Driver Drowsiness</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, deep learning for spatial-temporal data and the tools applied in our study.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Deep Learning and Spatial-Temporal Data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Sequential data as input for the neural network includes the concept of spatial-temporal data. Images can be considered spatial data, where pixels correspond to locations, areas, and distances. When single frames act as input, the neural network relies on spatial information, and the task is typically image classification using CNNs. However, in YawDD, the original data is in video form, introducing temporal information. The order of frames in a video is not arbitrary, and the relation between consecutive frames is temporal. With sequences of frames, we have spatial-temporal information, and the task becomes sequence or video classification. Utilizing spatial-temporal information is desirable as it preserves an additional dimension of information. For sequential data, RNNs are a specialized variation of deep learning. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> used an RNN extension of CNNs known as 3D CNNs, which extend CNNs to the third dimension, representing time. They achieved promising results, outperforming other methods like LSTMs. There is still nevertheless a discussion whether to use 2D or 3D CNNs for video processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Using single frames instead of sequences can help overcome hardware constraints and reduce the number of parameters to consider. Processing single frames could however result in a loss of information. Thus, we evaluate both 2D and 3D CNN on YawDD dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Federated Learning Tools</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our FL architecture was implemented using the Flower framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in conjunction with PyTorch. The Flower framework provides three key scripts: the <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">server</em> or <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">main</em> script, responsible for managing the federated training process; the <em id="S4.SS2.p1.1.3" class="ltx_emph ltx_font_italic">client</em> script, which handles local training and client-side evaluation; and the <em id="S4.SS2.p1.1.4" class="ltx_emph ltx_font_italic">utils</em> script, which incorporates essential functionalities such as data loading. Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Federated Learning Tools ‣ 4 Method for Driver Drowsiness ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts the Flower core framework architecture, how server and clients communicate and the possibility to simulate clients using a built-in virtual engine.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2405.03311/assets/Figures/flower_architecture.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="517" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Flower core framework architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">PyTorch offers a straightforward approach to load an image dataset from a folder. The <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">ImageFolder</em> function facilitates the loading and transformation of image samples along with their corresponding labels. Transformations are composed within the <em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">transform</em> object, which includes operations such as grayscale conversion, conversion to Tensor format, normalization, and standardization. The label information is inferred from the folder structure, as the dataset folder comprises three subfolders for each class.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">After loading the entire dataset into a variable, the next step involves splitting and assigning the subsets. We utilize the <em id="S4.SS2.p3.1.1" class="ltx_emph ltx_font_italic">random_split</em> function, which randomly permutes the dataset and splits it according to the specified length array. To ensure reproducibility, the random permutation is generated using a seeded number. Importantly, the randomization process respects the proportions of the label collections, guaranteeing that both splits maintain the same class distribution.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation and Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The experiments were conducted on the Ubuntu distribution, which is built on the Linux operating system. Specifically, Ubuntu version 22.04 was utilized for this study. The hardware setup consisted of an AMD seventh-generation processor and a NVIDIA GeForce RTX 3070 graphics card.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">At first, the loaded dataset size is controlled and the size 127.887 matches the number of files in the dataset folder. The 90:10 split is validated with a training set, containing 90% of the data, and a test set, which possesses 10% of the data. We also specify relevant hyperparameters before training. We search for optimal values for these and illustrate our results using the best choices found. The hyperparameter search space can be found in Table <a href="#S5.T1" title="Table 1 ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Moreover, we use FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as a federated strategy when aggregating the client updates at the server. Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is set as an optimizer.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Hyperparameter search space. <span id="S5.T1.10.1" class="ltx_text ltx_font_typewriter">sequence_length</span> and <span id="S5.T1.11.2" class="ltx_text ltx_font_typewriter">frame_skipping</span> parameters are only relevant for 3D-CNNs. </figcaption>
<table id="S5.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.7.8.1" class="ltx_tr">
<th id="S5.T1.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.7.8.1.1.1" class="ltx_text ltx_font_bold">Hyperparameter</span></th>
<th id="S5.T1.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Search Space Values</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.2.1" class="ltx_text ltx_font_typewriter">learning_rate</span></th>
<td id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T1.1.1.1.m1.6" class="ltx_Math" alttext="\{0.0001,0.001,0.002,0.005,0.01,0.1\}" display="inline"><semantics id="S5.T1.1.1.1.m1.6a"><mrow id="S5.T1.1.1.1.m1.6.7.2" xref="S5.T1.1.1.1.m1.6.7.1.cmml"><mo stretchy="false" id="S5.T1.1.1.1.m1.6.7.2.1" xref="S5.T1.1.1.1.m1.6.7.1.cmml">{</mo><mn id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">0.0001</mn><mo id="S5.T1.1.1.1.m1.6.7.2.2" xref="S5.T1.1.1.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.1.1.1.m1.2.2" xref="S5.T1.1.1.1.m1.2.2.cmml">0.001</mn><mo id="S5.T1.1.1.1.m1.6.7.2.3" xref="S5.T1.1.1.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.1.1.1.m1.3.3" xref="S5.T1.1.1.1.m1.3.3.cmml">0.002</mn><mo id="S5.T1.1.1.1.m1.6.7.2.4" xref="S5.T1.1.1.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.1.1.1.m1.4.4" xref="S5.T1.1.1.1.m1.4.4.cmml">0.005</mn><mo id="S5.T1.1.1.1.m1.6.7.2.5" xref="S5.T1.1.1.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.1.1.1.m1.5.5" xref="S5.T1.1.1.1.m1.5.5.cmml">0.01</mn><mo id="S5.T1.1.1.1.m1.6.7.2.6" xref="S5.T1.1.1.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.1.1.1.m1.6.6" xref="S5.T1.1.1.1.m1.6.6.cmml">0.1</mn><mo stretchy="false" id="S5.T1.1.1.1.m1.6.7.2.7" xref="S5.T1.1.1.1.m1.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.6b"><set id="S5.T1.1.1.1.m1.6.7.1.cmml" xref="S5.T1.1.1.1.m1.6.7.2"><cn type="float" id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">0.0001</cn><cn type="float" id="S5.T1.1.1.1.m1.2.2.cmml" xref="S5.T1.1.1.1.m1.2.2">0.001</cn><cn type="float" id="S5.T1.1.1.1.m1.3.3.cmml" xref="S5.T1.1.1.1.m1.3.3">0.002</cn><cn type="float" id="S5.T1.1.1.1.m1.4.4.cmml" xref="S5.T1.1.1.1.m1.4.4">0.005</cn><cn type="float" id="S5.T1.1.1.1.m1.5.5.cmml" xref="S5.T1.1.1.1.m1.5.5">0.01</cn><cn type="float" id="S5.T1.1.1.1.m1.6.6.cmml" xref="S5.T1.1.1.1.m1.6.6">0.1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.6c">\{0.0001,0.001,0.002,0.005,0.01,0.1\}</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.2.2" class="ltx_tr">
<th id="S5.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.2.2.2.1" class="ltx_text ltx_font_typewriter">momentum</span></th>
<td id="S5.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S5.T1.2.2.1.m1.7" class="ltx_Math" alttext="\{0.01,0.02,0.05,0.1,0.2,0.5,0.9\}" display="inline"><semantics id="S5.T1.2.2.1.m1.7a"><mrow id="S5.T1.2.2.1.m1.7.8.2" xref="S5.T1.2.2.1.m1.7.8.1.cmml"><mo stretchy="false" id="S5.T1.2.2.1.m1.7.8.2.1" xref="S5.T1.2.2.1.m1.7.8.1.cmml">{</mo><mn id="S5.T1.2.2.1.m1.1.1" xref="S5.T1.2.2.1.m1.1.1.cmml">0.01</mn><mo id="S5.T1.2.2.1.m1.7.8.2.2" xref="S5.T1.2.2.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.2.2.1.m1.2.2" xref="S5.T1.2.2.1.m1.2.2.cmml">0.02</mn><mo id="S5.T1.2.2.1.m1.7.8.2.3" xref="S5.T1.2.2.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.2.2.1.m1.3.3" xref="S5.T1.2.2.1.m1.3.3.cmml">0.05</mn><mo id="S5.T1.2.2.1.m1.7.8.2.4" xref="S5.T1.2.2.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.2.2.1.m1.4.4" xref="S5.T1.2.2.1.m1.4.4.cmml">0.1</mn><mo id="S5.T1.2.2.1.m1.7.8.2.5" xref="S5.T1.2.2.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.2.2.1.m1.5.5" xref="S5.T1.2.2.1.m1.5.5.cmml">0.2</mn><mo id="S5.T1.2.2.1.m1.7.8.2.6" xref="S5.T1.2.2.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.2.2.1.m1.6.6" xref="S5.T1.2.2.1.m1.6.6.cmml">0.5</mn><mo id="S5.T1.2.2.1.m1.7.8.2.7" xref="S5.T1.2.2.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.2.2.1.m1.7.7" xref="S5.T1.2.2.1.m1.7.7.cmml">0.9</mn><mo stretchy="false" id="S5.T1.2.2.1.m1.7.8.2.8" xref="S5.T1.2.2.1.m1.7.8.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.m1.7b"><set id="S5.T1.2.2.1.m1.7.8.1.cmml" xref="S5.T1.2.2.1.m1.7.8.2"><cn type="float" id="S5.T1.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.1.m1.1.1">0.01</cn><cn type="float" id="S5.T1.2.2.1.m1.2.2.cmml" xref="S5.T1.2.2.1.m1.2.2">0.02</cn><cn type="float" id="S5.T1.2.2.1.m1.3.3.cmml" xref="S5.T1.2.2.1.m1.3.3">0.05</cn><cn type="float" id="S5.T1.2.2.1.m1.4.4.cmml" xref="S5.T1.2.2.1.m1.4.4">0.1</cn><cn type="float" id="S5.T1.2.2.1.m1.5.5.cmml" xref="S5.T1.2.2.1.m1.5.5">0.2</cn><cn type="float" id="S5.T1.2.2.1.m1.6.6.cmml" xref="S5.T1.2.2.1.m1.6.6">0.5</cn><cn type="float" id="S5.T1.2.2.1.m1.7.7.cmml" xref="S5.T1.2.2.1.m1.7.7">0.9</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.1.m1.7c">\{0.01,0.02,0.05,0.1,0.2,0.5,0.9\}</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.3.3" class="ltx_tr">
<th id="S5.T1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.3.3.2.1" class="ltx_text ltx_font_typewriter">batch_size</span></th>
<td id="S5.T1.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S5.T1.3.3.1.m1.6" class="ltx_Math" alttext="\{2,8,16,32,64,128\}" display="inline"><semantics id="S5.T1.3.3.1.m1.6a"><mrow id="S5.T1.3.3.1.m1.6.7.2" xref="S5.T1.3.3.1.m1.6.7.1.cmml"><mo stretchy="false" id="S5.T1.3.3.1.m1.6.7.2.1" xref="S5.T1.3.3.1.m1.6.7.1.cmml">{</mo><mn id="S5.T1.3.3.1.m1.1.1" xref="S5.T1.3.3.1.m1.1.1.cmml">2</mn><mo id="S5.T1.3.3.1.m1.6.7.2.2" xref="S5.T1.3.3.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.3.3.1.m1.2.2" xref="S5.T1.3.3.1.m1.2.2.cmml">8</mn><mo id="S5.T1.3.3.1.m1.6.7.2.3" xref="S5.T1.3.3.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.3.3.1.m1.3.3" xref="S5.T1.3.3.1.m1.3.3.cmml">16</mn><mo id="S5.T1.3.3.1.m1.6.7.2.4" xref="S5.T1.3.3.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.3.3.1.m1.4.4" xref="S5.T1.3.3.1.m1.4.4.cmml">32</mn><mo id="S5.T1.3.3.1.m1.6.7.2.5" xref="S5.T1.3.3.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.3.3.1.m1.5.5" xref="S5.T1.3.3.1.m1.5.5.cmml">64</mn><mo id="S5.T1.3.3.1.m1.6.7.2.6" xref="S5.T1.3.3.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.3.3.1.m1.6.6" xref="S5.T1.3.3.1.m1.6.6.cmml">128</mn><mo stretchy="false" id="S5.T1.3.3.1.m1.6.7.2.7" xref="S5.T1.3.3.1.m1.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.1.m1.6b"><set id="S5.T1.3.3.1.m1.6.7.1.cmml" xref="S5.T1.3.3.1.m1.6.7.2"><cn type="integer" id="S5.T1.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.1.m1.1.1">2</cn><cn type="integer" id="S5.T1.3.3.1.m1.2.2.cmml" xref="S5.T1.3.3.1.m1.2.2">8</cn><cn type="integer" id="S5.T1.3.3.1.m1.3.3.cmml" xref="S5.T1.3.3.1.m1.3.3">16</cn><cn type="integer" id="S5.T1.3.3.1.m1.4.4.cmml" xref="S5.T1.3.3.1.m1.4.4">32</cn><cn type="integer" id="S5.T1.3.3.1.m1.5.5.cmml" xref="S5.T1.3.3.1.m1.5.5">64</cn><cn type="integer" id="S5.T1.3.3.1.m1.6.6.cmml" xref="S5.T1.3.3.1.m1.6.6">128</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.1.m1.6c">\{2,8,16,32,64,128\}</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.4.4" class="ltx_tr">
<th id="S5.T1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.4.4.2.1" class="ltx_text ltx_font_typewriter">weight_decay</span></th>
<td id="S5.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S5.T1.4.4.1.m1.6" class="ltx_Math" alttext="\{0.0001,0.001,0.002,0.005,0.01,0.1\}" display="inline"><semantics id="S5.T1.4.4.1.m1.6a"><mrow id="S5.T1.4.4.1.m1.6.7.2" xref="S5.T1.4.4.1.m1.6.7.1.cmml"><mo stretchy="false" id="S5.T1.4.4.1.m1.6.7.2.1" xref="S5.T1.4.4.1.m1.6.7.1.cmml">{</mo><mn id="S5.T1.4.4.1.m1.1.1" xref="S5.T1.4.4.1.m1.1.1.cmml">0.0001</mn><mo id="S5.T1.4.4.1.m1.6.7.2.2" xref="S5.T1.4.4.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.4.4.1.m1.2.2" xref="S5.T1.4.4.1.m1.2.2.cmml">0.001</mn><mo id="S5.T1.4.4.1.m1.6.7.2.3" xref="S5.T1.4.4.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.4.4.1.m1.3.3" xref="S5.T1.4.4.1.m1.3.3.cmml">0.002</mn><mo id="S5.T1.4.4.1.m1.6.7.2.4" xref="S5.T1.4.4.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.4.4.1.m1.4.4" xref="S5.T1.4.4.1.m1.4.4.cmml">0.005</mn><mo id="S5.T1.4.4.1.m1.6.7.2.5" xref="S5.T1.4.4.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.4.4.1.m1.5.5" xref="S5.T1.4.4.1.m1.5.5.cmml">0.01</mn><mo id="S5.T1.4.4.1.m1.6.7.2.6" xref="S5.T1.4.4.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.4.4.1.m1.6.6" xref="S5.T1.4.4.1.m1.6.6.cmml">0.1</mn><mo stretchy="false" id="S5.T1.4.4.1.m1.6.7.2.7" xref="S5.T1.4.4.1.m1.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.6b"><set id="S5.T1.4.4.1.m1.6.7.1.cmml" xref="S5.T1.4.4.1.m1.6.7.2"><cn type="float" id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1">0.0001</cn><cn type="float" id="S5.T1.4.4.1.m1.2.2.cmml" xref="S5.T1.4.4.1.m1.2.2">0.001</cn><cn type="float" id="S5.T1.4.4.1.m1.3.3.cmml" xref="S5.T1.4.4.1.m1.3.3">0.002</cn><cn type="float" id="S5.T1.4.4.1.m1.4.4.cmml" xref="S5.T1.4.4.1.m1.4.4">0.005</cn><cn type="float" id="S5.T1.4.4.1.m1.5.5.cmml" xref="S5.T1.4.4.1.m1.5.5">0.01</cn><cn type="float" id="S5.T1.4.4.1.m1.6.6.cmml" xref="S5.T1.4.4.1.m1.6.6">0.1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.6c">\{0.0001,0.001,0.002,0.005,0.01,0.1\}</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.5.5" class="ltx_tr">
<th id="S5.T1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.5.5.2.1" class="ltx_text ltx_font_typewriter">nbr_clients</span></th>
<td id="S5.T1.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S5.T1.5.5.1.m1.6" class="ltx_Math" alttext="\{2,4,8,16,20,40\}" display="inline"><semantics id="S5.T1.5.5.1.m1.6a"><mrow id="S5.T1.5.5.1.m1.6.7.2" xref="S5.T1.5.5.1.m1.6.7.1.cmml"><mo stretchy="false" id="S5.T1.5.5.1.m1.6.7.2.1" xref="S5.T1.5.5.1.m1.6.7.1.cmml">{</mo><mn id="S5.T1.5.5.1.m1.1.1" xref="S5.T1.5.5.1.m1.1.1.cmml">2</mn><mo id="S5.T1.5.5.1.m1.6.7.2.2" xref="S5.T1.5.5.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.5.5.1.m1.2.2" xref="S5.T1.5.5.1.m1.2.2.cmml">4</mn><mo id="S5.T1.5.5.1.m1.6.7.2.3" xref="S5.T1.5.5.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.5.5.1.m1.3.3" xref="S5.T1.5.5.1.m1.3.3.cmml">8</mn><mo id="S5.T1.5.5.1.m1.6.7.2.4" xref="S5.T1.5.5.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.5.5.1.m1.4.4" xref="S5.T1.5.5.1.m1.4.4.cmml">16</mn><mo id="S5.T1.5.5.1.m1.6.7.2.5" xref="S5.T1.5.5.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.5.5.1.m1.5.5" xref="S5.T1.5.5.1.m1.5.5.cmml">20</mn><mo id="S5.T1.5.5.1.m1.6.7.2.6" xref="S5.T1.5.5.1.m1.6.7.1.cmml">,</mo><mn id="S5.T1.5.5.1.m1.6.6" xref="S5.T1.5.5.1.m1.6.6.cmml">40</mn><mo stretchy="false" id="S5.T1.5.5.1.m1.6.7.2.7" xref="S5.T1.5.5.1.m1.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.6b"><set id="S5.T1.5.5.1.m1.6.7.1.cmml" xref="S5.T1.5.5.1.m1.6.7.2"><cn type="integer" id="S5.T1.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1">2</cn><cn type="integer" id="S5.T1.5.5.1.m1.2.2.cmml" xref="S5.T1.5.5.1.m1.2.2">4</cn><cn type="integer" id="S5.T1.5.5.1.m1.3.3.cmml" xref="S5.T1.5.5.1.m1.3.3">8</cn><cn type="integer" id="S5.T1.5.5.1.m1.4.4.cmml" xref="S5.T1.5.5.1.m1.4.4">16</cn><cn type="integer" id="S5.T1.5.5.1.m1.5.5.cmml" xref="S5.T1.5.5.1.m1.5.5">20</cn><cn type="integer" id="S5.T1.5.5.1.m1.6.6.cmml" xref="S5.T1.5.5.1.m1.6.6">40</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.6c">\{2,4,8,16,20,40\}</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.6.6" class="ltx_tr">
<th id="S5.T1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.6.6.2.1" class="ltx_text ltx_font_typewriter">sequence_length</span></th>
<td id="S5.T1.6.6.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S5.T1.6.6.1.m1.7" class="ltx_Math" alttext="\{8,10,12,14,26,18,20\}" display="inline"><semantics id="S5.T1.6.6.1.m1.7a"><mrow id="S5.T1.6.6.1.m1.7.8.2" xref="S5.T1.6.6.1.m1.7.8.1.cmml"><mo stretchy="false" id="S5.T1.6.6.1.m1.7.8.2.1" xref="S5.T1.6.6.1.m1.7.8.1.cmml">{</mo><mn id="S5.T1.6.6.1.m1.1.1" xref="S5.T1.6.6.1.m1.1.1.cmml">8</mn><mo id="S5.T1.6.6.1.m1.7.8.2.2" xref="S5.T1.6.6.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.6.6.1.m1.2.2" xref="S5.T1.6.6.1.m1.2.2.cmml">10</mn><mo id="S5.T1.6.6.1.m1.7.8.2.3" xref="S5.T1.6.6.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.6.6.1.m1.3.3" xref="S5.T1.6.6.1.m1.3.3.cmml">12</mn><mo id="S5.T1.6.6.1.m1.7.8.2.4" xref="S5.T1.6.6.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.6.6.1.m1.4.4" xref="S5.T1.6.6.1.m1.4.4.cmml">14</mn><mo id="S5.T1.6.6.1.m1.7.8.2.5" xref="S5.T1.6.6.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.6.6.1.m1.5.5" xref="S5.T1.6.6.1.m1.5.5.cmml">26</mn><mo id="S5.T1.6.6.1.m1.7.8.2.6" xref="S5.T1.6.6.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.6.6.1.m1.6.6" xref="S5.T1.6.6.1.m1.6.6.cmml">18</mn><mo id="S5.T1.6.6.1.m1.7.8.2.7" xref="S5.T1.6.6.1.m1.7.8.1.cmml">,</mo><mn id="S5.T1.6.6.1.m1.7.7" xref="S5.T1.6.6.1.m1.7.7.cmml">20</mn><mo stretchy="false" id="S5.T1.6.6.1.m1.7.8.2.8" xref="S5.T1.6.6.1.m1.7.8.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.1.m1.7b"><set id="S5.T1.6.6.1.m1.7.8.1.cmml" xref="S5.T1.6.6.1.m1.7.8.2"><cn type="integer" id="S5.T1.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.1.m1.1.1">8</cn><cn type="integer" id="S5.T1.6.6.1.m1.2.2.cmml" xref="S5.T1.6.6.1.m1.2.2">10</cn><cn type="integer" id="S5.T1.6.6.1.m1.3.3.cmml" xref="S5.T1.6.6.1.m1.3.3">12</cn><cn type="integer" id="S5.T1.6.6.1.m1.4.4.cmml" xref="S5.T1.6.6.1.m1.4.4">14</cn><cn type="integer" id="S5.T1.6.6.1.m1.5.5.cmml" xref="S5.T1.6.6.1.m1.5.5">26</cn><cn type="integer" id="S5.T1.6.6.1.m1.6.6.cmml" xref="S5.T1.6.6.1.m1.6.6">18</cn><cn type="integer" id="S5.T1.6.6.1.m1.7.7.cmml" xref="S5.T1.6.6.1.m1.7.7">20</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.1.m1.7c">\{8,10,12,14,26,18,20\}</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.7.7" class="ltx_tr">
<th id="S5.T1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S5.T1.7.7.2.1" class="ltx_text ltx_font_typewriter">frame_skipping</span></th>
<td id="S5.T1.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math id="S5.T1.7.7.1.m1.8" class="ltx_Math" alttext="\{2,3,4,5,6,8,10,12\}" display="inline"><semantics id="S5.T1.7.7.1.m1.8a"><mrow id="S5.T1.7.7.1.m1.8.9.2" xref="S5.T1.7.7.1.m1.8.9.1.cmml"><mo stretchy="false" id="S5.T1.7.7.1.m1.8.9.2.1" xref="S5.T1.7.7.1.m1.8.9.1.cmml">{</mo><mn id="S5.T1.7.7.1.m1.1.1" xref="S5.T1.7.7.1.m1.1.1.cmml">2</mn><mo id="S5.T1.7.7.1.m1.8.9.2.2" xref="S5.T1.7.7.1.m1.8.9.1.cmml">,</mo><mn id="S5.T1.7.7.1.m1.2.2" xref="S5.T1.7.7.1.m1.2.2.cmml">3</mn><mo id="S5.T1.7.7.1.m1.8.9.2.3" xref="S5.T1.7.7.1.m1.8.9.1.cmml">,</mo><mn id="S5.T1.7.7.1.m1.3.3" xref="S5.T1.7.7.1.m1.3.3.cmml">4</mn><mo id="S5.T1.7.7.1.m1.8.9.2.4" xref="S5.T1.7.7.1.m1.8.9.1.cmml">,</mo><mn id="S5.T1.7.7.1.m1.4.4" xref="S5.T1.7.7.1.m1.4.4.cmml">5</mn><mo id="S5.T1.7.7.1.m1.8.9.2.5" xref="S5.T1.7.7.1.m1.8.9.1.cmml">,</mo><mn id="S5.T1.7.7.1.m1.5.5" xref="S5.T1.7.7.1.m1.5.5.cmml">6</mn><mo id="S5.T1.7.7.1.m1.8.9.2.6" xref="S5.T1.7.7.1.m1.8.9.1.cmml">,</mo><mn id="S5.T1.7.7.1.m1.6.6" xref="S5.T1.7.7.1.m1.6.6.cmml">8</mn><mo id="S5.T1.7.7.1.m1.8.9.2.7" xref="S5.T1.7.7.1.m1.8.9.1.cmml">,</mo><mn id="S5.T1.7.7.1.m1.7.7" xref="S5.T1.7.7.1.m1.7.7.cmml">10</mn><mo id="S5.T1.7.7.1.m1.8.9.2.8" xref="S5.T1.7.7.1.m1.8.9.1.cmml">,</mo><mn id="S5.T1.7.7.1.m1.8.8" xref="S5.T1.7.7.1.m1.8.8.cmml">12</mn><mo stretchy="false" id="S5.T1.7.7.1.m1.8.9.2.9" xref="S5.T1.7.7.1.m1.8.9.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.m1.8b"><set id="S5.T1.7.7.1.m1.8.9.1.cmml" xref="S5.T1.7.7.1.m1.8.9.2"><cn type="integer" id="S5.T1.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1">2</cn><cn type="integer" id="S5.T1.7.7.1.m1.2.2.cmml" xref="S5.T1.7.7.1.m1.2.2">3</cn><cn type="integer" id="S5.T1.7.7.1.m1.3.3.cmml" xref="S5.T1.7.7.1.m1.3.3">4</cn><cn type="integer" id="S5.T1.7.7.1.m1.4.4.cmml" xref="S5.T1.7.7.1.m1.4.4">5</cn><cn type="integer" id="S5.T1.7.7.1.m1.5.5.cmml" xref="S5.T1.7.7.1.m1.5.5">6</cn><cn type="integer" id="S5.T1.7.7.1.m1.6.6.cmml" xref="S5.T1.7.7.1.m1.6.6">8</cn><cn type="integer" id="S5.T1.7.7.1.m1.7.7.cmml" xref="S5.T1.7.7.1.m1.7.7">10</cn><cn type="integer" id="S5.T1.7.7.1.m1.8.8.cmml" xref="S5.T1.7.7.1.m1.8.8">12</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.1.m1.8c">\{2,3,4,5,6,8,10,12\}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>3D-CNN and Video Sequence Processing</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Based on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> work, the hyperparameter values were adopted and a model was constructed as in Figure <a href="#S5.F4" title="Figure 4 ‣ 5.1 3D-CNN and Video Sequence Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2405.03311/assets/Figures/ed3dArchitecture.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>3D-CNN architecture as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The sequence length was found to influence the results and an initial sequence length of 16 was used, but other initializations did not improve performance. Frame skipping, introduced to enhance temporal information, showed no effect, with a commonly used frame skipping value of five. The batch size, limited by memory, was set to 2 due to the sequence length. The learning rate was the only parameter that showed some change, with smaller rates resulting in slower convergence but reduced fluctuations. The best values for frame skipping and sequence length were used after hyperparameter tuning. Highest accuracy achieved was 90.1% and processing was slow. To improve the approach and achieve better results, the process can be streamlined and simplified. Using single frames instead of sequences can help overcome hardware constraints and reduce the number of parameters to consider.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>2D-CNN and Image Processing</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The adopted model is a 2D-Convolutional Neural Network (CNN) for image classification, in which we process the videos frame-by-frame. We draw inspiration from the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> when constructing the 2D-CNN and the architecture can be found in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a></p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2405.03311/assets/x2.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>PyTorch code for convolutional neural network we use. The parameter values for the optimizer are initial values. </figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We thereafter evaluate our model using the mentioned pre-processing steps on a frame-by-frame basis. The task is a 3-class classification task in which we seek to correctly classify drivers driving normally with closed mouth, drivers talking, and drivers yawning. We illustrate our results using test accuracy and categorical cross-entropy loss. The values illustrated in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S5.F7" title="Figure 7 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> are averaged scores over 5 runs. We also show respective significance interval for both illustrations.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2405.03311/assets/Figures/clients_yawDD.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="515" height="386" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Test accuracy for various number of clients. Accuracy is shown with significant intervals, averaged over 5 runs. </figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2405.03311/assets/Figures/clients_yawDD_loss.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="382" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Test categorical cross-entropy loss for various number of clients. Accuracy is shown with significant intervals, averaged over 5 runs. </figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.3" class="ltx_p">The test accuracy in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows that highest accuracy is achieved using 2 clients. Maximum accuracy achieved is 99.2% and prediction outcomes can be seen in Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. We notice a slight decrease in performance when increasing the number of participating federated clients. This is clearly shown when increasing the number of clients from <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="16\rightarrow 20" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mn id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">16</mn><mo stretchy="false" id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">→</mo><mn id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><ci id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1">→</ci><cn type="integer" id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">16</cn><cn type="integer" id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">16\rightarrow 20</annotation></semantics></math> and <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="20\rightarrow 40" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mrow id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mn id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">20</mn><mo stretchy="false" id="S5.SS2.p3.2.m2.1.1.1" xref="S5.SS2.p3.2.m2.1.1.1.cmml">→</mo><mn id="S5.SS2.p3.2.m2.1.1.3" xref="S5.SS2.p3.2.m2.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><ci id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1.1">→</ci><cn type="integer" id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">20</cn><cn type="integer" id="S5.SS2.p3.2.m2.1.1.3.cmml" xref="S5.SS2.p3.2.m2.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">20\rightarrow 40</annotation></semantics></math>. However, the performance is fairly stable between <math id="S5.SS2.p3.3.m3.1" class="ltx_Math" alttext="2\rightarrow 16" display="inline"><semantics id="S5.SS2.p3.3.m3.1a"><mrow id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml"><mn id="S5.SS2.p3.3.m3.1.1.2" xref="S5.SS2.p3.3.m3.1.1.2.cmml">2</mn><mo stretchy="false" id="S5.SS2.p3.3.m3.1.1.1" xref="S5.SS2.p3.3.m3.1.1.1.cmml">→</mo><mn id="S5.SS2.p3.3.m3.1.1.3" xref="S5.SS2.p3.3.m3.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><apply id="S5.SS2.p3.3.m3.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1"><ci id="S5.SS2.p3.3.m3.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1.1">→</ci><cn type="integer" id="S5.SS2.p3.3.m3.1.1.2.cmml" xref="S5.SS2.p3.3.m3.1.1.2">2</cn><cn type="integer" id="S5.SS2.p3.3.m3.1.1.3.cmml" xref="S5.SS2.p3.3.m3.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">2\rightarrow 16</annotation></semantics></math> clients. The optimal choice of hyperparameters seems to be a low learning rate, weight decay, number of participating federated clients and batch size. A larger momentum gives better results.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we show the predictions of our best run for driver drowsiness using 2 clients. Similar pattern is seen when increasing the number of participating clients. From the results, we read that our model can easily distinguish the classes yawning and and normal driving and that talking is sometimes mistaken for yawning or normal driving, almost in same proportion.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2405.03311/assets/Figures/confusionMatrix_balanced.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="501" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Confusion matrix with predictions for driver drowsiness. </figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Firstly, we controlled the loaded dataset size, which contained 127,887 files, ensuring that the dataset folder was accurately represented. We split the dataset into a training set, comprising 90% of the data, and a test set, containing the remaining 10%. This splitting strategy is commonly employed in machine learning to evaluate the generalization performance of models. By using this approach, we can assess how well our model performs on unseen data. To ensure optimal performance, we conducted a hyperparameter search to identify the best values for the relevant parameters.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In our experiments, we employed the FedAvg FL strategy, as proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. This strategy enables efficient aggregation of client updates at the server while preserving data privacy. In other studies, researchers can come to evaluate other federated strategies such as FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> which could serve the area of personalized FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> better in case one seeks to split unique user data to individual clients.
We first evaluate our 3D-CNN on YawDD data and achieve 90.1% accuracy. This is not as good as results in other studies. As mentioned, using single frames for processing can help overcome hardware limitations and reduce the number of parameters considered.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">For evaluation purposes, we applied the specified pre-processing steps on a frame-by-frame basis. Our task involved a 3-class classification, where the objective was to accurately classify drivers engaged in normal driving with a closed mouth, drivers who were talking, and drivers who were yawning. To assess the performance of our model, we measured the test accuracy and the categorical cross-entropy loss. The results, shown in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, respectively, were averaged over 5 runs to account for potential variations. Analyzing the test accuracy results presented in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we observe that the highest accuracy of 99.2% was achieved when using 2 clients. This finding indicates that a smaller number of participating federated clients yielded superior performance. However, we notice a slight decrease in accuracy when the number of clients increased from 16 to 20 and from 20 to 40. This decline in performance suggests that as more clients participate, the aggregation process becomes more challenging, potentially due to increased heterogeneity or more likely, size of local datasets becoming too small. With smaller local datasets, the likelihood of it including sufficient and representative data decreases and thus we may experience an increase in heterogeneity. Heterogeneous datasets or non-identical and independent (non-IID) datasets are prevalent in FL and researchers have studied this extensively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Interestingly, the performance remained relatively stable when the number of clients ranged from 2 to 16. This observation implies that a moderate number of participating clients is optimal for the task at hand. To further improve the model’s performance, we identified several key hyperparameters that played a crucial role. These include a low learning rate, weight decay, number of participating federated clients, and batch size. Additionally, we found that a larger momentum value yielded better results, indicating the importance of effectively leveraging momentum during the optimization process.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">To gain more insights into the classification results, we analyzed the confusion matrix shown in Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 2D-CNN and Image Processing ‣ 5 Evaluation and Results ‣ Federated Learning for Drowsiness Detection in Connected Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. This matrix represents the predictions obtained from our best run using 2 clients. Notably, similar patterns were observed when increasing the number of participating clients. From the confusion matrix, we deduce that our model can effectively distinguish between yawning and normal driving classes. However, there is a notable confusion between the talking class and the yawning class, as well as between the talking class and the normal driving class. These misclassifications suggest that drivers who are talking exhibit certain facial movements or patterns that resemble both yawning and normal driving. Since we are operating on a frame-per-frame level, there will be certain cases where the decision boundary is ”blurry”, i.e., cases which look alike but belong to different classes. Further investigation into the distinguishing features between these classes could potentially lead to improvements in the model’s performance. To extend this study, future research could focus on exploring additional feature engineering techniques or investigating more advanced models to further enhance the classification accuracy. Additionally, collecting more diverse and extensive datasets could provide a more comprehensive evaluation of the model’s performance in real-world scenarios.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">One interesting area of research is the field of Personalized FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We see that researchers can apply learnings from this field onto the problem of accurate DMS. This includes investigating different model architectures and aggregating algorithms e.g. FL with personalization layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">While most of the vehicle control is handled by machines, drivers still need to be prepared to handle complex situations. Overcoming the challenges of ensuring driver readiness is crucial, and driver monitoring systems play a significant role in assessing the driver’s state. These systems utilize visual cues to recognize various behaviors and associate them with specific conditions, such as drowsiness indicated by yawning or eye blinking. Consequently, an abundance of distributed data is generated for driver monitoring.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">To address the task of driver drowsiness detection, machine learning techniques, such as the one employed in this study, offer a potential solution. However, transmitting the vast amount of data to a central machine for model training is impractical due to privacy concerns and the sheer size of the data. On the other hand, training the model solely on a single vehicle would limit the available data and likely result in inferior performance.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">To overcome these challenges, we propose an FL framework within a vehicular network for drowsiness detection, utilizing the YawDD dataset. Our approach demonstrates impressive accuracy, achieving a rate of 99.2%. This result highlights the promise and comparability of our method to conventional deep learning techniques. Our main contributions are:</p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">Federated Learning framework for driver drowsiness detection using YawDD dataset for processing single frames and sequences.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">With our evaluation, we show how model performance scales when increasing the number of federated clients.</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">We achieve great results of 99.2% when classifying normal driving, talking and yawning driver.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abtahi, S., Omidyeganeh, M., Shirmohammadi, S., Hariri, B.: Yawdd: A yawning
detection dataset. In: Proceedings of the 5th ACM multimedia systems
conference. pp. 24–28 (2014)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Al-sudani, A.R.: Yawn based driver fatigue level prediction. Proceedings of
35th International Confer <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">69</span>, 372–382 (2020)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Arivazhagan, M.G., Aggarwal, V., Singh, A.K., Choudhary, S.: Federated learning
with personalization layers. arXiv preprint arXiv:1912.00818 (2019)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Beutel, D.J., Topal, T., Mathur, A., Qiu, X., Parcollet, T., de Gusmão,
P.P., Lane, N.D.: Flower: A friendly federated learning research framework.
arXiv preprint arXiv:2007.14390 (2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cancello Tortora, G., Casini, M., Lagna, A., Marino, M., Vecchio, A.: Detection
of distracted driving: A smartphone-based approach. In: International
Conference on Intelligent Transport Systems. pp. 157–165. Springer (2022)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chen, C.F.R., Panda, R., Ramakrishnan, K., Feris, R., Cohn, J., Oliva, A., Fan,
Q.: Deep analysis of cnn-based spatio-temporal representations for action
recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 6165–6175 (2021)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dingus, T.A., Guo, F., Lee, S., Antin, J.F., Perez, M., Buchanan-King, M.,
Hankey, J.: Driver crash risk factors and prevalence evaluation using
naturalistic driving data. Proceedings of the National Academy of Sciences
<span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">113</span>(10), 2636–2641 (2016)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ed-Doughmi, Y., Idrissi, N., Hbali, Y.: Real-time system for driver fatigue
detection based on a recurrent neuronal network. Journal of imaging
<span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">6</span>(3),  8 (2020)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Fridman, L.: Human-centered autonomous vehicle systems: Principles of effective
shared autonomy. arXiv preprint arXiv:1810.01835 (2018)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Fridman, L., Langhans, P., Lee, J., Reimer, B.: Driver gaze region estimation
without use of eye movement. IEEE Intelligent Systems <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">31</span>(3),
49–56 (2016)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Halin, A., Verly, J.G., Van Droogenbroeck, M.: Survey and synthesis of state of
the art in driver monitoring. Sensors <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">21</span>(16),  5558 (2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Junaedi, S., Akbar, H.: Driver drowsiness detection based on face feature and
perclos. In: Journal of Physics: Conference Series. vol. 1090, p. 012037. IOP
Publishing (2018)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 (2014)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Konečnỳ, J., McMahan, H.B., Yu, F.X., Richtárik, P., Suresh,
A.T., Bacon, D.: Federated learning: Strategies for improving communication
efficiency. arXiv preprint arXiv:1610.05492 (2016)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V.:
Federated optimization in heterogeneous networks. Proceedings of Machine
learning and systems <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">2</span>, 429–450 (2020)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Li, X., Chi, H.l., Lu, W., Xue, F., Zeng, J., Li, C.Z.: Federated transfer
learning enabled smart work packaging for preserving personal image
information of construction worker. Automation in Construction <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">128</span>,
103738 (2021)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lindskog, W., Prehofer, C.: Federated learning for tabular data using tabnet: A
vehicular use-case. In: 2022 IEEE 18th International Conference on
Intelligent Computer Communication and Processing (ICCP). pp. 105–111. IEEE
(2022)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Mioch, T., Kroon, L., Neerincx, M.A.: Driver readiness model for regulating the
transfer from automation to human control. In: Proceedings of the 22nd
international conference on intelligent user interfaces. pp. 205–213 (2017)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Rajkar, A., Kulkarni, N., Raut, A.: Driver drowsiness detection using deep
learning. In: Applied Information Processing Systems, pp. 73–82. Springer
(2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Rieke, N., Hancox, J., Li, W., Milletari, F., Roth, H.R., Albarqouni, S.,
Bakas, S., Galtier, M.N., Landman, B.A., Maier-Hein, K., et al.: The future
of digital health with federated learning. NPJ digital medicine
<span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">3</span>(1),  119 (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sahayadhas, A., Sundaraj, K., Murugappan, M.: Detecting driver drowsiness based
on sensors: a review. Sensors <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">12</span>(12), 16937–16953 (2012)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Salman, R.M., Rashid, M., Roy, R., Ahsan, M.M., Siddique, Z.: Driver drowsiness
detection using ensemble convolutional neural networks on yawdd. arXiv
preprint arXiv:2112.10298 (2021)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Sikander, G., Anwar, S.: Driver fatigue detection systems: A review. IEEE
Transactions on Intelligent Transportation Systems <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">20</span>(6),
2339–2352 (2018)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Tan, A.Z., Yu, H., Cui, L., Yang, Q.: Towards personalized federated learning.
IEEE Transactions on Neural Networks and Learning Systems (2022)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yang, Z., Zhang, X., Wu, D., Wang, R., Zhang, P., Wu, Y.: Efficient
asynchronous federated learning research in the internet of vehicles. IEEE
Internet of Things Journal (2022)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zafar, A., Prehofer, C., Cheng, C.H.: Federated learning for driver status
monitoring. In: 2021 IEEE International Intelligent Transportation Systems
Conference (ITSC). pp. 1463–1469. IEEE (2021)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhang, L., Saito, H., Yang, L., Wu, J.: Privacy-preserving federated transfer
learning for driver drowsiness detection. IEEE Access <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">10</span>,
80565–80574 (2022)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning
with non-iid data. arXiv preprint arXiv:1806.00582 (2018)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.03310" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.03311" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.03311">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.03311" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.03312" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 14:50:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
