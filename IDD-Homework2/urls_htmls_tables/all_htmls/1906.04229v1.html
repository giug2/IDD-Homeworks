<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1906.04229] Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering</title><meta property="og:description" content="We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1906.04229">

<!--Generated on Fri Mar  1 22:43:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Psycholinguistics meets Continual Learning: 
<br class="ltx_break">Measuring Catastrophic Forgetting in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Claudio Greco<sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><span id="id10.10.id2" class="ltx_text ltx_font_typewriter">claudio.greco@unitn.it</span>
&amp;Barbara Plank<sup id="id11.11.id3" class="ltx_sup"><span id="id11.11.id3.1" class="ltx_text ltx_font_italic">2</span></sup>
<br class="ltx_break"><span id="id12.12.id4" class="ltx_text ltx_font_typewriter">bplank@itu.dk</span>
<span id="id13.13.id5" class="ltx_ERROR undefined">\AND</span>Raquel Fernández<sup id="id14.14.id6" class="ltx_sup"><span id="id14.14.id6.1" class="ltx_text ltx_font_italic">3</span></sup>
<br class="ltx_break"><span id="id15.15.id7" class="ltx_text ltx_font_typewriter">raquel.fernandez@uva.nl</span>
&amp;Raffaella Bernardi<sup id="id16.16.id8" class="ltx_sup"><span id="id16.16.id8.1" class="ltx_text ltx_font_italic">1,4</span></sup>
<br class="ltx_break"><span id="id17.17.id9" class="ltx_text ltx_font_typewriter">raffaella.bernardi@unitn.it</span>
<span id="id18.18.id10" class="ltx_ERROR undefined">\AND</span><sup id="id19.19.id11" class="ltx_sup"><span id="id19.19.id11.1" class="ltx_text ltx_font_italic">1</span></sup>CIMeC and <sup id="id20.20.id12" class="ltx_sup"><span id="id20.20.id12.1" class="ltx_text ltx_font_italic">4</span></sup>DISI 
<br class="ltx_break">University of Trento&amp;<sup id="id21.21.id13" class="ltx_sup"><span id="id21.21.id13.1" class="ltx_text ltx_font_italic">2</span></sup>Dept. of Computer Science
<br class="ltx_break">IT University of Copenhagen&amp;<sup id="id22.22.id14" class="ltx_sup"><span id="id22.22.id14.1" class="ltx_text ltx_font_italic">3</span></sup>ILLC
<br class="ltx_break">University of Amsterdam
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id23.id1" class="ltx_p">We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Supervised machine learning models are incapable of continuously learning new tasks,
as they forget how to perform the previously learned ones. This problem,
called <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">catastrophic forgetting</span>, is prominent in artificial
neural networks <cite class="ltx_cite ltx_citemacro_cite">McClelland et al. (<a href="#bib.bib10" title="" class="ltx_ref">1995</a>)</cite>. <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">Continual
Learning</span> (CL) addresses this problem by trying to equip models with
the capability to continuously learn new tasks over time <cite class="ltx_cite ltx_citemacro_cite">Ring (<a href="#bib.bib15" title="" class="ltx_ref">1997</a>)</cite>. Catastrophic
forgetting and CL have received considerable attention
in computer vision
<cite class="ltx_cite ltx_citemacro_cite">(e.g., Zenke et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Kirkpatrick et al., <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>, but far less attention within Natural Language Processing (NLP).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1906.04229/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="328" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our linguistically-informed CL setup for VQA.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We investigate catastrophic forgetting in the context of multimodal models for Visual Question Answering <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> motivated by evidence from psycholinguistics. VQA is
the task of answering natural language questions about an image.
Evidence from child language acquisition indicates that children
learn Wh-questions before polar (Yes/No) questions <cite class="ltx_cite ltx_citemacro_cite">Moradlou and Ginzburg (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>); Moradlou et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>.
Motivated by this finding, we design a set of linguistically-informed experiments: i) to investigate whether the order in which children acquire question types
facilitates continual learning for computational models and, accordingly, the impact of <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">task order</span> on catastrophic forgetting;
ii) to measure how far two well-known CL approaches help to overcome the problem <cite class="ltx_cite ltx_citemacro_cite">Robins (<a href="#bib.bib16" title="" class="ltx_ref">1995</a>); Kirkpatrick et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code and data are available at the link <a target="_blank" href="http://continual-vista.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://continual-vista.github.io/</a>.</span></span></span>.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Contributions:</h3>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">Our study contributes to the literature on CL in NLP. In particular:
i) we introduce a CL setup based on
linguistically-informed task pairs which differ with respect to question types and level of difficulty;
ii) we show the importance of task order, an often overlooked aspect, and observe asymmetric synergetic effects;
iii) our results show that our VQA model suffers from extreme forgetting; rehearsal gives better results than a regularization-based method. Our error analysis shows that the latter
approach encounters problems even in discerning Task A after having
been trained on Task B. Our study opens the door to deeper
investigations of CL on linguistic skills with
different levels of difficulty based of psycholinguistics findings.
</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Task Setup</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">As a first step towards understanding the connection between linguistic skills and the impact on CL, we design a set of experiments within VQA where tasks differ with respect to the <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">type of question</span> and the <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">level of difficulty</span> according to the psycholinguistics literature. The overall setup is illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and described next.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Dataset</h3>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">CLEVR <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib5" title="" class="ltx_ref">2017a</a>)</cite> allows to study the ability of VQA agents. It requires compositional language and basic spatial reasoning skills. Every question in CLEVR is derived by a Functional Program (FP) from a scene graph of the associated image.
The scene graph defines the objects and attributes in the image. The FP contains functions corresponding to skills, e.g., querying object attributes or comparing values (see Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, upper).
Questions are categorized by their type. CLEVR consists of five question
types whose answer labels range over 15 attributes, 10 numbers, and
“yes”/“no” (in total 27 labels).
</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Multimodal Tasks</h3>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">We select the CLEVR sub-tasks ‘query_attribute’ and ‘equal_attribute’ with attributes <span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">color, shape, material</span>, and <span id="S2.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">size</span>.
The two types of questions differ by answer type <math id="S2.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="y\in\mathcal{Y}" display="inline"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">y</mi><mo id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">𝒴</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1"><in id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.1"></in><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2">𝑦</ci><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3">𝒴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.1c">y\in\mathcal{Y}</annotation></semantics></math>:</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.6" class="ltx_p"><span id="S2.I1.i1.p1.6.1" class="ltx_text ltx_font_bold">Wh-questions</span> (Wh-q): Questions about the <span id="S2.I1.i1.p1.6.2" class="ltx_text ltx_font_italic">attribute</span> of an object, e.g., “What is the material of the large object…?”, where <math id="S2.I1.i1.p1.1.m1.5" class="ltx_Math" alttext="y\in\{blue,cube,small,\ldots,metal\}" display="inline"><semantics id="S2.I1.i1.p1.1.m1.5a"><mrow id="S2.I1.i1.p1.1.m1.5.5" xref="S2.I1.i1.p1.1.m1.5.5.cmml"><mi id="S2.I1.i1.p1.1.m1.5.5.6" xref="S2.I1.i1.p1.1.m1.5.5.6.cmml">y</mi><mo id="S2.I1.i1.p1.1.m1.5.5.5" xref="S2.I1.i1.p1.1.m1.5.5.5.cmml">∈</mo><mrow id="S2.I1.i1.p1.1.m1.5.5.4.4" xref="S2.I1.i1.p1.1.m1.5.5.4.5.cmml"><mo stretchy="false" id="S2.I1.i1.p1.1.m1.5.5.4.4.5" xref="S2.I1.i1.p1.1.m1.5.5.4.5.cmml">{</mo><mrow id="S2.I1.i1.p1.1.m1.2.2.1.1.1" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.I1.i1.p1.1.m1.2.2.1.1.1.2" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.2.2.1.1.1.1" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.2.2.1.1.1.3" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.2.2.1.1.1.1a" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.2.2.1.1.1.4" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.2.2.1.1.1.1b" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.2.2.1.1.1.5" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.5.cmml">e</mi></mrow><mo id="S2.I1.i1.p1.1.m1.5.5.4.4.6" xref="S2.I1.i1.p1.1.m1.5.5.4.5.cmml">,</mo><mrow id="S2.I1.i1.p1.1.m1.3.3.2.2.2" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.I1.i1.p1.1.m1.3.3.2.2.2.2" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.3.3.2.2.2.1" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.3.3.2.2.2.3" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.3.3.2.2.2.1a" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.3.3.2.2.2.4" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.3.3.2.2.2.1b" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.3.3.2.2.2.5" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.5.cmml">e</mi></mrow><mo id="S2.I1.i1.p1.1.m1.5.5.4.4.7" xref="S2.I1.i1.p1.1.m1.5.5.4.5.cmml">,</mo><mrow id="S2.I1.i1.p1.1.m1.4.4.3.3.3" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.I1.i1.p1.1.m1.4.4.3.3.3.2" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.4.4.3.3.3.1" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.4.4.3.3.3.3" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.4.4.3.3.3.1a" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.4.4.3.3.3.4" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.4.4.3.3.3.1b" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.4.4.3.3.3.5" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.4.4.3.3.3.1c" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.4.4.3.3.3.6" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.6.cmml">l</mi></mrow><mo id="S2.I1.i1.p1.1.m1.5.5.4.4.8" xref="S2.I1.i1.p1.1.m1.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml">…</mi><mo id="S2.I1.i1.p1.1.m1.5.5.4.4.9" xref="S2.I1.i1.p1.1.m1.5.5.4.5.cmml">,</mo><mrow id="S2.I1.i1.p1.1.m1.5.5.4.4.4" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.cmml"><mi id="S2.I1.i1.p1.1.m1.5.5.4.4.4.2" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.5.5.4.4.4.1" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.5.5.4.4.4.3" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.5.5.4.4.4.1a" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.5.5.4.4.4.4" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.5.5.4.4.4.1b" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.5.5.4.4.4.5" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.5.5.4.4.4.1c" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.1.cmml">​</mo><mi id="S2.I1.i1.p1.1.m1.5.5.4.4.4.6" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.6.cmml">l</mi></mrow><mo stretchy="false" id="S2.I1.i1.p1.1.m1.5.5.4.4.10" xref="S2.I1.i1.p1.1.m1.5.5.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.5b"><apply id="S2.I1.i1.p1.1.m1.5.5.cmml" xref="S2.I1.i1.p1.1.m1.5.5"><in id="S2.I1.i1.p1.1.m1.5.5.5.cmml" xref="S2.I1.i1.p1.1.m1.5.5.5"></in><ci id="S2.I1.i1.p1.1.m1.5.5.6.cmml" xref="S2.I1.i1.p1.1.m1.5.5.6">𝑦</ci><set id="S2.I1.i1.p1.1.m1.5.5.4.5.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4"><apply id="S2.I1.i1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1"><times id="S2.I1.i1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.1"></times><ci id="S2.I1.i1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.2">𝑏</ci><ci id="S2.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.3">𝑙</ci><ci id="S2.I1.i1.p1.1.m1.2.2.1.1.1.4.cmml" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.4">𝑢</ci><ci id="S2.I1.i1.p1.1.m1.2.2.1.1.1.5.cmml" xref="S2.I1.i1.p1.1.m1.2.2.1.1.1.5">𝑒</ci></apply><apply id="S2.I1.i1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2"><times id="S2.I1.i1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.1"></times><ci id="S2.I1.i1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.2">𝑐</ci><ci id="S2.I1.i1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.3">𝑢</ci><ci id="S2.I1.i1.p1.1.m1.3.3.2.2.2.4.cmml" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.4">𝑏</ci><ci id="S2.I1.i1.p1.1.m1.3.3.2.2.2.5.cmml" xref="S2.I1.i1.p1.1.m1.3.3.2.2.2.5">𝑒</ci></apply><apply id="S2.I1.i1.p1.1.m1.4.4.3.3.3.cmml" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3"><times id="S2.I1.i1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.1"></times><ci id="S2.I1.i1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.2">𝑠</ci><ci id="S2.I1.i1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.3">𝑚</ci><ci id="S2.I1.i1.p1.1.m1.4.4.3.3.3.4.cmml" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.4">𝑎</ci><ci id="S2.I1.i1.p1.1.m1.4.4.3.3.3.5.cmml" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.5">𝑙</ci><ci id="S2.I1.i1.p1.1.m1.4.4.3.3.3.6.cmml" xref="S2.I1.i1.p1.1.m1.4.4.3.3.3.6">𝑙</ci></apply><ci id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">…</ci><apply id="S2.I1.i1.p1.1.m1.5.5.4.4.4.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4"><times id="S2.I1.i1.p1.1.m1.5.5.4.4.4.1.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.1"></times><ci id="S2.I1.i1.p1.1.m1.5.5.4.4.4.2.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.2">𝑚</ci><ci id="S2.I1.i1.p1.1.m1.5.5.4.4.4.3.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.3">𝑒</ci><ci id="S2.I1.i1.p1.1.m1.5.5.4.4.4.4.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.4">𝑡</ci><ci id="S2.I1.i1.p1.1.m1.5.5.4.4.4.5.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.5">𝑎</ci><ci id="S2.I1.i1.p1.1.m1.5.5.4.4.4.6.cmml" xref="S2.I1.i1.p1.1.m1.5.5.4.4.4.6">𝑙</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.5c">y\in\{blue,cube,small,\ldots,metal\}</annotation></semantics></math> spans over <math id="S2.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="|color|=8" display="inline"><semantics id="S2.I1.i1.p1.2.m2.1a"><mrow id="S2.I1.i1.p1.2.m2.1.1" xref="S2.I1.i1.p1.2.m2.1.1.cmml"><mrow id="S2.I1.i1.p1.2.m2.1.1.1.1" xref="S2.I1.i1.p1.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S2.I1.i1.p1.2.m2.1.1.1.1.2" xref="S2.I1.i1.p1.2.m2.1.1.1.2.1.cmml">|</mo><mrow id="S2.I1.i1.p1.2.m2.1.1.1.1.1" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.2" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.3" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1a" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.4" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1b" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.5" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1c" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.6" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.6.cmml">r</mi></mrow><mo stretchy="false" id="S2.I1.i1.p1.2.m2.1.1.1.1.3" xref="S2.I1.i1.p1.2.m2.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.I1.i1.p1.2.m2.1.1.2" xref="S2.I1.i1.p1.2.m2.1.1.2.cmml">=</mo><mn id="S2.I1.i1.p1.2.m2.1.1.3" xref="S2.I1.i1.p1.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.1b"><apply id="S2.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1"><eq id="S2.I1.i1.p1.2.m2.1.1.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2"></eq><apply id="S2.I1.i1.p1.2.m2.1.1.1.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1"><abs id="S2.I1.i1.p1.2.m2.1.1.1.2.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.2"></abs><apply id="S2.I1.i1.p1.2.m2.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1"><times id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.1"></times><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.2">𝑐</ci><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.3">𝑜</ci><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.4.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.4">𝑙</ci><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.5.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.5">𝑜</ci><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.6.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.6">𝑟</ci></apply></apply><cn type="integer" id="S2.I1.i1.p1.2.m2.1.1.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.1c">|color|=8</annotation></semantics></math>, <math id="S2.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="|shape|=3" display="inline"><semantics id="S2.I1.i1.p1.3.m3.1a"><mrow id="S2.I1.i1.p1.3.m3.1.1" xref="S2.I1.i1.p1.3.m3.1.1.cmml"><mrow id="S2.I1.i1.p1.3.m3.1.1.1.1" xref="S2.I1.i1.p1.3.m3.1.1.1.2.cmml"><mo stretchy="false" id="S2.I1.i1.p1.3.m3.1.1.1.1.2" xref="S2.I1.i1.p1.3.m3.1.1.1.2.1.cmml">|</mo><mrow id="S2.I1.i1.p1.3.m3.1.1.1.1.1" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.cmml"><mi id="S2.I1.i1.p1.3.m3.1.1.1.1.1.2" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.3.m3.1.1.1.1.1.1" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.3.m3.1.1.1.1.1.3" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.3.m3.1.1.1.1.1.1a" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.3.m3.1.1.1.1.1.4" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.3.m3.1.1.1.1.1.1b" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.3.m3.1.1.1.1.1.5" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.3.m3.1.1.1.1.1.1c" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.3.m3.1.1.1.1.1.6" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.6.cmml">e</mi></mrow><mo stretchy="false" id="S2.I1.i1.p1.3.m3.1.1.1.1.3" xref="S2.I1.i1.p1.3.m3.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.I1.i1.p1.3.m3.1.1.2" xref="S2.I1.i1.p1.3.m3.1.1.2.cmml">=</mo><mn id="S2.I1.i1.p1.3.m3.1.1.3" xref="S2.I1.i1.p1.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.3.m3.1b"><apply id="S2.I1.i1.p1.3.m3.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1"><eq id="S2.I1.i1.p1.3.m3.1.1.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1.2"></eq><apply id="S2.I1.i1.p1.3.m3.1.1.1.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1"><abs id="S2.I1.i1.p1.3.m3.1.1.1.2.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.2"></abs><apply id="S2.I1.i1.p1.3.m3.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1"><times id="S2.I1.i1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.1"></times><ci id="S2.I1.i1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.2">𝑠</ci><ci id="S2.I1.i1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.3">ℎ</ci><ci id="S2.I1.i1.p1.3.m3.1.1.1.1.1.4.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.4">𝑎</ci><ci id="S2.I1.i1.p1.3.m3.1.1.1.1.1.5.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.5">𝑝</ci><ci id="S2.I1.i1.p1.3.m3.1.1.1.1.1.6.cmml" xref="S2.I1.i1.p1.3.m3.1.1.1.1.1.6">𝑒</ci></apply></apply><cn type="integer" id="S2.I1.i1.p1.3.m3.1.1.3.cmml" xref="S2.I1.i1.p1.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.3.m3.1c">|shape|=3</annotation></semantics></math>, <math id="S2.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="|size|=2" display="inline"><semantics id="S2.I1.i1.p1.4.m4.1a"><mrow id="S2.I1.i1.p1.4.m4.1.1" xref="S2.I1.i1.p1.4.m4.1.1.cmml"><mrow id="S2.I1.i1.p1.4.m4.1.1.1.1" xref="S2.I1.i1.p1.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="S2.I1.i1.p1.4.m4.1.1.1.1.2" xref="S2.I1.i1.p1.4.m4.1.1.1.2.1.cmml">|</mo><mrow id="S2.I1.i1.p1.4.m4.1.1.1.1.1" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.cmml"><mi id="S2.I1.i1.p1.4.m4.1.1.1.1.1.2" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.4.m4.1.1.1.1.1.1" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.4.m4.1.1.1.1.1.3" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.4.m4.1.1.1.1.1.1a" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.4.m4.1.1.1.1.1.4" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.4.cmml">z</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.4.m4.1.1.1.1.1.1b" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.4.m4.1.1.1.1.1.5" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.5.cmml">e</mi></mrow><mo stretchy="false" id="S2.I1.i1.p1.4.m4.1.1.1.1.3" xref="S2.I1.i1.p1.4.m4.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.I1.i1.p1.4.m4.1.1.2" xref="S2.I1.i1.p1.4.m4.1.1.2.cmml">=</mo><mn id="S2.I1.i1.p1.4.m4.1.1.3" xref="S2.I1.i1.p1.4.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.4.m4.1b"><apply id="S2.I1.i1.p1.4.m4.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1"><eq id="S2.I1.i1.p1.4.m4.1.1.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2"></eq><apply id="S2.I1.i1.p1.4.m4.1.1.1.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1"><abs id="S2.I1.i1.p1.4.m4.1.1.1.2.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1.2"></abs><apply id="S2.I1.i1.p1.4.m4.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1"><times id="S2.I1.i1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.1"></times><ci id="S2.I1.i1.p1.4.m4.1.1.1.1.1.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.2">𝑠</ci><ci id="S2.I1.i1.p1.4.m4.1.1.1.1.1.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.3">𝑖</ci><ci id="S2.I1.i1.p1.4.m4.1.1.1.1.1.4.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.4">𝑧</ci><ci id="S2.I1.i1.p1.4.m4.1.1.1.1.1.5.cmml" xref="S2.I1.i1.p1.4.m4.1.1.1.1.1.5">𝑒</ci></apply></apply><cn type="integer" id="S2.I1.i1.p1.4.m4.1.1.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.4.m4.1c">|size|=2</annotation></semantics></math> and <math id="S2.I1.i1.p1.5.m5.1" class="ltx_Math" alttext="|material|=2" display="inline"><semantics id="S2.I1.i1.p1.5.m5.1a"><mrow id="S2.I1.i1.p1.5.m5.1.1" xref="S2.I1.i1.p1.5.m5.1.1.cmml"><mrow id="S2.I1.i1.p1.5.m5.1.1.1.1" xref="S2.I1.i1.p1.5.m5.1.1.1.2.cmml"><mo stretchy="false" id="S2.I1.i1.p1.5.m5.1.1.1.1.2" xref="S2.I1.i1.p1.5.m5.1.1.1.2.1.cmml">|</mo><mrow id="S2.I1.i1.p1.5.m5.1.1.1.1.1" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.cmml"><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.2" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.3" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1a" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.4" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1b" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.5" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1c" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.6" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1d" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.7" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1e" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.8" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1f" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i1.p1.5.m5.1.1.1.1.1.9" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.9.cmml">l</mi></mrow><mo stretchy="false" id="S2.I1.i1.p1.5.m5.1.1.1.1.3" xref="S2.I1.i1.p1.5.m5.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.I1.i1.p1.5.m5.1.1.2" xref="S2.I1.i1.p1.5.m5.1.1.2.cmml">=</mo><mn id="S2.I1.i1.p1.5.m5.1.1.3" xref="S2.I1.i1.p1.5.m5.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.5.m5.1b"><apply id="S2.I1.i1.p1.5.m5.1.1.cmml" xref="S2.I1.i1.p1.5.m5.1.1"><eq id="S2.I1.i1.p1.5.m5.1.1.2.cmml" xref="S2.I1.i1.p1.5.m5.1.1.2"></eq><apply id="S2.I1.i1.p1.5.m5.1.1.1.2.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1"><abs id="S2.I1.i1.p1.5.m5.1.1.1.2.1.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.2"></abs><apply id="S2.I1.i1.p1.5.m5.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1"><times id="S2.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.1"></times><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.2">𝑚</ci><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.3">𝑎</ci><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.4.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.4">𝑡</ci><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.5.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.5">𝑒</ci><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.6.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.6">𝑟</ci><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.7.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.7">𝑖</ci><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.8.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.8">𝑎</ci><ci id="S2.I1.i1.p1.5.m5.1.1.1.1.1.9.cmml" xref="S2.I1.i1.p1.5.m5.1.1.1.1.1.9">𝑙</ci></apply></apply><cn type="integer" id="S2.I1.i1.p1.5.m5.1.1.3.cmml" xref="S2.I1.i1.p1.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.5.m5.1c">|material|=2</annotation></semantics></math> (in total <math id="S2.I1.i1.p1.6.m6.1" class="ltx_Math" alttext="|\mathcal{Y}|=15" display="inline"><semantics id="S2.I1.i1.p1.6.m6.1a"><mrow id="S2.I1.i1.p1.6.m6.1.2" xref="S2.I1.i1.p1.6.m6.1.2.cmml"><mrow id="S2.I1.i1.p1.6.m6.1.2.2.2" xref="S2.I1.i1.p1.6.m6.1.2.2.1.cmml"><mo stretchy="false" id="S2.I1.i1.p1.6.m6.1.2.2.2.1" xref="S2.I1.i1.p1.6.m6.1.2.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.I1.i1.p1.6.m6.1.1" xref="S2.I1.i1.p1.6.m6.1.1.cmml">𝒴</mi><mo stretchy="false" id="S2.I1.i1.p1.6.m6.1.2.2.2.2" xref="S2.I1.i1.p1.6.m6.1.2.2.1.1.cmml">|</mo></mrow><mo id="S2.I1.i1.p1.6.m6.1.2.1" xref="S2.I1.i1.p1.6.m6.1.2.1.cmml">=</mo><mn id="S2.I1.i1.p1.6.m6.1.2.3" xref="S2.I1.i1.p1.6.m6.1.2.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.6.m6.1b"><apply id="S2.I1.i1.p1.6.m6.1.2.cmml" xref="S2.I1.i1.p1.6.m6.1.2"><eq id="S2.I1.i1.p1.6.m6.1.2.1.cmml" xref="S2.I1.i1.p1.6.m6.1.2.1"></eq><apply id="S2.I1.i1.p1.6.m6.1.2.2.1.cmml" xref="S2.I1.i1.p1.6.m6.1.2.2.2"><abs id="S2.I1.i1.p1.6.m6.1.2.2.1.1.cmml" xref="S2.I1.i1.p1.6.m6.1.2.2.2.1"></abs><ci id="S2.I1.i1.p1.6.m6.1.1.cmml" xref="S2.I1.i1.p1.6.m6.1.1">𝒴</ci></apply><cn type="integer" id="S2.I1.i1.p1.6.m6.1.2.3.cmml" xref="S2.I1.i1.p1.6.m6.1.2.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.6.m6.1c">|\mathcal{Y}|=15</annotation></semantics></math>).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.2" class="ltx_p"><span id="S2.I1.i2.p1.2.1" class="ltx_text ltx_font_bold">Yes/No questions</span> (Y/N-q): Questions that <span id="S2.I1.i2.p1.2.2" class="ltx_text ltx_font_italic">compare</span> objects with respect to an
attribute, e.g., “Does the cyan ball have the same
material as …?”, with <math id="S2.I1.i2.p1.1.m1.2" class="ltx_Math" alttext="y\in\{yes,no\}" display="inline"><semantics id="S2.I1.i2.p1.1.m1.2a"><mrow id="S2.I1.i2.p1.1.m1.2.2" xref="S2.I1.i2.p1.1.m1.2.2.cmml"><mi id="S2.I1.i2.p1.1.m1.2.2.4" xref="S2.I1.i2.p1.1.m1.2.2.4.cmml">y</mi><mo id="S2.I1.i2.p1.1.m1.2.2.3" xref="S2.I1.i2.p1.1.m1.2.2.3.cmml">∈</mo><mrow id="S2.I1.i2.p1.1.m1.2.2.2.2" xref="S2.I1.i2.p1.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S2.I1.i2.p1.1.m1.2.2.2.2.3" xref="S2.I1.i2.p1.1.m1.2.2.2.3.cmml">{</mo><mrow id="S2.I1.i2.p1.1.m1.1.1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i2.p1.1.m1.1.1.1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1a" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.I1.i2.p1.1.m1.1.1.1.1.1.4" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.4.cmml">s</mi></mrow><mo id="S2.I1.i2.p1.1.m1.2.2.2.2.4" xref="S2.I1.i2.p1.1.m1.2.2.2.3.cmml">,</mo><mrow id="S2.I1.i2.p1.1.m1.2.2.2.2.2" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2.cmml"><mi id="S2.I1.i2.p1.1.m1.2.2.2.2.2.2" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.I1.i2.p1.1.m1.2.2.2.2.2.1" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2.1.cmml">​</mo><mi id="S2.I1.i2.p1.1.m1.2.2.2.2.2.3" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2.3.cmml">o</mi></mrow><mo stretchy="false" id="S2.I1.i2.p1.1.m1.2.2.2.2.5" xref="S2.I1.i2.p1.1.m1.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.2b"><apply id="S2.I1.i2.p1.1.m1.2.2.cmml" xref="S2.I1.i2.p1.1.m1.2.2"><in id="S2.I1.i2.p1.1.m1.2.2.3.cmml" xref="S2.I1.i2.p1.1.m1.2.2.3"></in><ci id="S2.I1.i2.p1.1.m1.2.2.4.cmml" xref="S2.I1.i2.p1.1.m1.2.2.4">𝑦</ci><set id="S2.I1.i2.p1.1.m1.2.2.2.3.cmml" xref="S2.I1.i2.p1.1.m1.2.2.2.2"><apply id="S2.I1.i2.p1.1.m1.1.1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1"><times id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1"></times><ci id="S2.I1.i2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.2">𝑦</ci><ci id="S2.I1.i2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.3">𝑒</ci><ci id="S2.I1.i2.p1.1.m1.1.1.1.1.1.4.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.4">𝑠</ci></apply><apply id="S2.I1.i2.p1.1.m1.2.2.2.2.2.cmml" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2"><times id="S2.I1.i2.p1.1.m1.2.2.2.2.2.1.cmml" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2.1"></times><ci id="S2.I1.i2.p1.1.m1.2.2.2.2.2.2.cmml" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2.2">𝑛</ci><ci id="S2.I1.i2.p1.1.m1.2.2.2.2.2.3.cmml" xref="S2.I1.i2.p1.1.m1.2.2.2.2.2.3">𝑜</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.2c">y\in\{yes,no\}</annotation></semantics></math> (in total <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="|\mathcal{Y}|=2" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><mrow id="S2.I1.i2.p1.2.m2.1.2" xref="S2.I1.i2.p1.2.m2.1.2.cmml"><mrow id="S2.I1.i2.p1.2.m2.1.2.2.2" xref="S2.I1.i2.p1.2.m2.1.2.2.1.cmml"><mo stretchy="false" id="S2.I1.i2.p1.2.m2.1.2.2.2.1" xref="S2.I1.i2.p1.2.m2.1.2.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml">𝒴</mi><mo stretchy="false" id="S2.I1.i2.p1.2.m2.1.2.2.2.2" xref="S2.I1.i2.p1.2.m2.1.2.2.1.1.cmml">|</mo></mrow><mo id="S2.I1.i2.p1.2.m2.1.2.1" xref="S2.I1.i2.p1.2.m2.1.2.1.cmml">=</mo><mn id="S2.I1.i2.p1.2.m2.1.2.3" xref="S2.I1.i2.p1.2.m2.1.2.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.2"><eq id="S2.I1.i2.p1.2.m2.1.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.2.1"></eq><apply id="S2.I1.i2.p1.2.m2.1.2.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.2.2.2"><abs id="S2.I1.i2.p1.2.m2.1.2.2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.2.2.2.1"></abs><ci id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">𝒴</ci></apply><cn type="integer" id="S2.I1.i2.p1.2.m2.1.2.3.cmml" xref="S2.I1.i2.p1.2.m2.1.2.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">|\mathcal{Y}|=2</annotation></semantics></math>).
</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Task Order</h3>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">We learn Task A followed by Task B (<span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">TaskA<math id="S2.SS0.SSS0.Px3.p1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S2.SS0.SSS0.Px3.p1.1.1.m1.1a"><mo stretchy="false" id="S2.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="S2.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.1.1.m1.1b"><ci id="S2.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.1.1.m1.1c">\rightarrow</annotation></semantics></math>TaskB</span>), but experiment with <span id="S2.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">both</span> directions, i.e.,
by first assigning Wh-q to Task A and Y/N-q to Task B, and vice versa.
We expect that the inherent difficulty of a task and the order in which tasks are learned have an impact on CL.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Single-head Evaluation</h3>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">CL methods can be tested in two ways. We opt for a <span id="S2.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_italic">single-head</span> evaluation setup (see Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, lower) with an output space over labels for all tasks (here: all CLEVR labels). In contrast, in a <span id="S2.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_italic">multi-head</span> setup predictions are restricted to task labels, as the task identifier is provided. Single-head is more difficult yet more realistic <cite class="ltx_cite ltx_citemacro_cite">Chaudhry et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Models and Experiments</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">VQA Model</h3>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">We take the model proposed
by <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> as a starting point, using the code released
by <cite class="ltx_cite ltx_citemacro_citet">Johnson et al. (<a href="#bib.bib6" title="" class="ltx_ref">2017b</a>)</cite> (LSTM+CNN+SA).
Questions are encoded with a recurrent
neural network with Long Short-Term Memory (LSTM) units. Images are encoded with a
ResNet-101 Convolutional Neural Network (CNN) pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib4" title="" class="ltx_ref">2016</a>)</cite>.
The two representations are combined using Spatial
Attention (SA) <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> to focus on the most salient objects and properties in the image and text. The final answer distribution
is predicted with a Multilayer Perceptron (MLP).</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Baselines</h3>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">In order to measure catastrophic forgetting,
we first consider per-task baselines: A random baseline (i.e., random stratified sample of the label distribution per task)
and the results of a model trained independently on each task
(i.e., over task-specific <math id="S3.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{Y}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">\mathcal{Y}</annotation></semantics></math>).
For CL, we report again a random baseline (this time a random stratified sample drawing predictions according to the answer distribution of both tasks), and
we consider the <span id="S3.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">Naive</span> and <span id="S3.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">Cumulative</span> baselines proposed by <cite class="ltx_cite ltx_citemacro_citet">Maltoni and Lomonaco (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>.
The <span id="S3.SS0.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">Naive</span> model is fine-tuned across tasks: It is first trained on Task
A and then on Task B starting from the
previously learned parameters.
The <span id="S3.SS0.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_italic">Cumulative</span> model is trained from scratch on the training sets of both Task A and Task B.
This is a kind of upper bound, or performance that a CL model should achieve.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Continual Learning Models</h3>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">In CL there are two broad families of methods: Those that assume memory and access to explicit previous knowledge (instances), and those that have only access to compressed knowledge, such as previously learned parameters. These two families correspond to rehearsal and regularization, respectively.
A widely-used regularization-based approach is <span id="S3.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">Elastic Weight
Consolidation</span> <cite class="ltx_cite ltx_citemacro_cite">(<em id="S3.SS0.SSS0.Px3.p1.1.2.1" class="ltx_emph ltx_font_italic">EWC</em>, Kirkpatrick et al., <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>. A
regularization term, parametrized by <math id="S3.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.1.m1.1c">\lambda</annotation></semantics></math>, is added to the loss function aiming the model
to converge to parameters where it has a low error for both
tasks.
In the <em id="S3.SS0.SSS0.Px3.p1.1.3" class="ltx_emph ltx_font_italic">Rehearsal</em> approach <cite class="ltx_cite ltx_citemacro_cite">Robins (<a href="#bib.bib16" title="" class="ltx_ref">1995</a>)</cite>, the
model is first trained on Task A, then the parameters are fine-tuned through
batches taken from a dataset containing a small number of examples of Task A and the training set of Task B.
The selection of training examples of Task A is done
through uniform sampling.
</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Data and Training Details</h3>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p1.1" class="ltx_p">Since CLEVR has no published ground-truth answers for the test set, we split the original validation set into a
validation and a test set. To avoid performance impact due to different training data sizes, we
downsample the training sets to the same size (Y/N-q data size), resulting in 125,654 training instances per task.
The validation and test sets contain, respectively, 26,960 and 26,774 data points for Wh-q and 13,417 and 13,681 data points for Y/N-q.</p>
</div>
<div id="S3.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p2.1" class="ltx_p">For the baselines, we select the model which reaches maximum
accuracy on the validation set of each task. For CL, we choose the
model with the highest CL score computed according to the validation
set of each task pair.
Details on hyper-parameters and evaluation metrics are provided in the supplementary material (SM).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The main results are provided in Table <a href="#S4.T1" title="Table 1 ‣ Catastrophic Forgetting ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. There are several take-aways.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Task Difficulty</h3>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">The results of the per-task models (cf. first two rows in Table <a href="#S4.T1" title="Table 1 ‣ Catastrophic Forgetting ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) show that there is
a large performance gap between the two tasks.
Wh-q is easier (.81) than Y/N-q (.52), regardless of the
fact that a priori the latter should be easier (as shown by the respective task-specific random baselines).
The Y/N-q task-specific model performs only slightly above chance (.52, in line with what
<cite class="ltx_cite ltx_citemacro_citet">Johnson et al. (<a href="#bib.bib5" title="" class="ltx_ref">2017a</a>)</cite> report for ‘equal_attribute’ questions).
This shows that despite
the limited output space of the Y/N-q task, such type of questions in CLEVR
are complex and require reasoning skills <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib5" title="" class="ltx_ref">2017a</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Catastrophic Forgetting</h3>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">We observe that extreme forgetting is at play.
<em id="S4.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">Naive</em> forgets the previously learned skill
completely: When tested on Task A after having been fine-tuned on Task
B, it achieves 0.0 accuracy on the first task <span id="S4.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">for both
directions</span> (<span id="S4.SS0.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_smallcaps">I</span> and <span id="S4.SS0.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_smallcaps">II</span>, cf. Table <a href="#S4.T1" title="Table 1 ‣ Catastrophic Forgetting ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lower). The <em id="S4.SS0.SSS0.Px2.p1.1.5" class="ltx_emph ltx_font_italic">Cumulative</em> model by nature cannot forget, since it is trained
on both tasks simultaneously, achieving
.81 and .74 on Wh-q and Y/N-q, respectively. Interestingly,
we observe an <span id="S4.SS0.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_italic">asymmetric synergetic effect</span>.
Being exposed to the Wh-q task helps the <em id="S4.SS0.SSS0.Px2.p1.1.7" class="ltx_emph ltx_font_italic">Cumulative</em> model improve on Y/N-q,
reaching results beyond the task-specific model (from .52 to .74).
The effect is not symmetric as the accuracy on Wh-q does not further increase.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.2" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:297.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(98.9pt,-67.9pt) scale(1.8388847259376,1.8388847259376) ;">
<table id="S4.T1.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.2.3.1" class="ltx_tr">
<th id="S4.T1.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Random (per-task)</th>
<th id="S4.T1.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T1.2.2.3.1.2.1" class="ltx_text ltx_font_smallcaps">Wh: 0.09</span></th>
<th id="S4.T1.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2"><span id="S4.T1.2.2.3.1.3.1" class="ltx_text ltx_font_smallcaps">Y/N: 0.50</span></th>
</tr>
<tr id="S4.T1.2.2.4.2" class="ltx_tr">
<th id="S4.T1.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">LSTM+CNN+SA</th>
<th id="S4.T1.2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2">
<span id="S4.T1.2.2.4.2.2.1" class="ltx_text ltx_font_smallcaps">Wh:</span> 0.81</th>
<th id="S4.T1.2.2.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l" colspan="2"><span id="S4.T1.2.2.4.2.3.1" class="ltx_text ltx_font_smallcaps">Y/N 0.52</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_t"><span id="S4.T1.2.2.2.3.1" class="ltx_text ltx_font_smallcaps">CL setups:</span></th>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" colspan="2"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">I)
Wh<math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math>Y/N</span></td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_tt ltx_border_t" colspan="2"><span id="S4.T1.2.2.2.2.1" class="ltx_text ltx_font_smallcaps">II)
Y/N<math id="S4.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.1.m1.1c">\rightarrow</annotation></semantics></math>Wh</span></td>
</tr>
<tr id="S4.T1.2.2.5.1" class="ltx_tr">
<th id="S4.T1.2.2.5.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.2.2.5.1.2" class="ltx_td ltx_align_center">Wh</td>
<td id="S4.T1.2.2.5.1.3" class="ltx_td ltx_align_right ltx_border_r">Y/N</td>
<td id="S4.T1.2.2.5.1.4" class="ltx_td ltx_align_center">Y/N</td>
<td id="S4.T1.2.2.5.1.5" class="ltx_td ltx_align_right">Wh</td>
</tr>
<tr id="S4.T1.2.2.6.2" class="ltx_tr">
<th id="S4.T1.2.2.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Random (both tasks)</th>
<td id="S4.T1.2.2.6.2.2" class="ltx_td ltx_align_center ltx_border_t">0.04</td>
<td id="S4.T1.2.2.6.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.25</td>
<td id="S4.T1.2.2.6.2.4" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S4.T1.2.2.6.2.5" class="ltx_td ltx_align_right ltx_border_t">0.04</td>
</tr>
<tr id="S4.T1.2.2.7.3" class="ltx_tr">
<th id="S4.T1.2.2.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Naive</th>
<td id="S4.T1.2.2.7.3.2" class="ltx_td ltx_align_center">0.00</td>
<td id="S4.T1.2.2.7.3.3" class="ltx_td ltx_align_right ltx_border_r">0.61</td>
<td id="S4.T1.2.2.7.3.4" class="ltx_td ltx_align_center">0.00</td>
<td id="S4.T1.2.2.7.3.5" class="ltx_td ltx_align_right">0.81</td>
</tr>
<tr id="S4.T1.2.2.8.4" class="ltx_tr">
<th id="S4.T1.2.2.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">EWC</th>
<td id="S4.T1.2.2.8.4.2" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S4.T1.2.2.8.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.51</td>
<td id="S4.T1.2.2.8.4.4" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
<td id="S4.T1.2.2.8.4.5" class="ltx_td ltx_align_right ltx_border_t">0.83</td>
</tr>
<tr id="S4.T1.2.2.9.5" class="ltx_tr">
<th id="S4.T1.2.2.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Rehearsal</th>
<td id="S4.T1.2.2.9.5.2" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T1.2.2.9.5.3" class="ltx_td ltx_align_right ltx_border_r">0.51</td>
<td id="S4.T1.2.2.9.5.4" class="ltx_td ltx_align_center">0.51</td>
<td id="S4.T1.2.2.9.5.5" class="ltx_td ltx_align_right">0.80</td>
</tr>
<tr id="S4.T1.2.2.10.6" class="ltx_tr">
<th id="S4.T1.2.2.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cumulative</th>
<td id="S4.T1.2.2.10.6.2" class="ltx_td ltx_align_center ltx_border_t">0.81</td>
<td id="S4.T1.2.2.10.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.74</td>
<td id="S4.T1.2.2.10.6.4" class="ltx_td ltx_align_center ltx_border_t">0.74</td>
<td id="S4.T1.2.2.10.6.5" class="ltx_td ltx_align_right ltx_border_t">0.81</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mean accuracy over 3 runs: Trained on each task independently
(first two rows; per-task label space <math id="S4.T1.5.m1.1" class="ltx_Math" alttext="\mathcal{Y}" display="inline"><semantics id="S4.T1.5.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S4.T1.5.m1.1.1" xref="S4.T1.5.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.T1.5.m1.1c"><ci id="S4.T1.5.m1.1.1.cmml" xref="S4.T1.5.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.m1.1d">\mathcal{Y}</annotation></semantics></math>) vs.  CL setups (single-head label space over all <math id="S4.T1.6.m2.1" class="ltx_Math" alttext="\mathcal{Y}" display="inline"><semantics id="S4.T1.6.m2.1b"><mi class="ltx_font_mathcaligraphic" id="S4.T1.6.m2.1.1" xref="S4.T1.6.m2.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.T1.6.m2.1c"><ci id="S4.T1.6.m2.1.1.cmml" xref="S4.T1.6.m2.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.m2.1d">\mathcal{Y}</annotation></semantics></math>).</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Does CL Help?</h3>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.3" class="ltx_p">Current CL methods show only limiting (or no) effect.
<em id="S4.SS0.SSS0.Px3.p1.3.4" class="ltx_emph ltx_font_italic">EWC</em> performs bad overall:
In the <span id="S4.SS0.SSS0.Px3.p1.3.5" class="ltx_text ltx_font_smallcaps">II)</span> setup (<span id="S4.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">y/n<math id="S4.SS0.SSS0.Px3.p1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.1.1.m1.1a"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1c">\rightarrow</annotation></semantics></math>wh</span>, harder task first), <em id="S4.SS0.SSS0.Px3.p1.3.6" class="ltx_emph ltx_font_italic">EWC</em> does not yield any improvement over the <span id="S4.SS0.SSS0.Px3.p1.3.7" class="ltx_text ltx_font_italic">Naive</span> model;
in the <span id="S4.SS0.SSS0.Px3.p1.2.2" class="ltx_text ltx_font_smallcaps">wh<math id="S4.SS0.SSS0.Px3.p1.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.2.2.m1.1a"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.2.2.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.2.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.2.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.2.m1.1c">\rightarrow</annotation></semantics></math>y/n</span> setup, the model’s result on Task A is above chance level (.25 vs. .04) but far off per-task performance (.81). The <span id="S4.SS0.SSS0.Px3.p1.3.8" class="ltx_text ltx_font_italic">Rehearsal</span> model forgets less than <em id="S4.SS0.SSS0.Px3.p1.3.9" class="ltx_emph ltx_font_italic">Naive</em> and <em id="S4.SS0.SSS0.Px3.p1.3.10" class="ltx_emph ltx_font_italic">EWC</em> in both setups:
In the <span id="S4.SS0.SSS0.Px3.p1.3.3" class="ltx_text ltx_font_smallcaps">y/n<math id="S4.SS0.SSS0.Px3.p1.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.3.3.m1.1a"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.3.3.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.3.3.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.3.3.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.3.3.m1.1c">\rightarrow</annotation></semantics></math>wh</span> setup,
it is above chance level (.51 vs. .25) reaching per-task random baseline results on <span id="S4.SS0.SSS0.Px3.p1.3.11" class="ltx_text ltx_font_smallcaps">Y/N</span> questions (i.e., the
model is able to identify Task A, despite the harder single-head setting, in contrast to the <em id="S4.SS0.SSS0.Px3.p1.3.12" class="ltx_emph ltx_font_italic">Naive</em> and <em id="S4.SS0.SSS0.Px3.p1.3.13" class="ltx_emph ltx_font_italic">EWC</em>
models).
There is no boost derived from being exposed to the Wh-q
task in any of the two setups.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Task Order</h3>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">The results in Table <a href="#S4.T1" title="Table 1 ‣ Catastrophic Forgetting ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> show that
<span id="S4.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_italic">the order of tasks plays an important role</span>: <span id="S4.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">wh<math id="S4.SS0.SSS0.Px4.p1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.1.1.m1.1a"><mo stretchy="false" id="S4.SS0.SSS0.Px4.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.1.1.m1.1b"><ci id="S4.SS0.SSS0.Px4.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.1.1.m1.1c">\rightarrow</annotation></semantics></math>y/n</span> facilitates CL
more than the opposite order: less forgetting is at place when <span id="S4.SS0.SSS0.Px4.p1.1.3" class="ltx_text ltx_font_smallcaps">wh</span> is learned first.
This confirms psycholinguistic evidence. Overall,
<span id="S4.SS0.SSS0.Px4.p1.1.4" class="ltx_text ltx_font_italic">Rehearsal</span> works better than <em id="S4.SS0.SSS0.Px4.p1.1.5" class="ltx_emph ltx_font_italic">EWC</em>, but mitigates forgetting only to a limiting degree.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/1906.04229/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Analysis of the neuron activations on the penultimate
hidden layer for the I) <span id="S4.F2.2.1" class="ltx_text ltx_font_smallcaps">Wh <math id="S4.F2.2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.F2.2.1.m1.1b"><mo stretchy="false" id="S4.F2.2.1.m1.1.1" xref="S4.F2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.2.1.m1.1c"><ci id="S4.F2.2.1.m1.1.1.cmml" xref="S4.F2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.1.m1.1d">\rightarrow</annotation></semantics></math> Y/N</span> setup. “equal_{shape,color,material,size}” refers to <span id="S4.F2.5.2" class="ltx_text ltx_font_smallcaps">Y/N</span>-q, “query_{..}” refers to <span id="S4.F2.6.3" class="ltx_text ltx_font_smallcaps">Wh</span>-questions.</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Analysis</h3>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p1.1" class="ltx_p">To get a deeper understanding of the models, we analyze the penultimate hidden layer on a sample of 512 questions from the test sets of both tasks (cf. Fig. <a href="#S4.F2" title="Figure 2 ‣ Task Order ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and relate the representations to confusion matrices of the whole test sets (provided in the SM) and test results (Table <a href="#S4.T1" title="Table 1 ‣ Catastrophic Forgetting ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S4.SS0.SSS0.Px5.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p2.1" class="ltx_p">First of all, the model trained on Wh-q discriminates Wh-questions about different attributes very well, reflected in overall high accuracy (.81). It
otherwise clusters all instances from the other task (Y/N-q, which it has not been trained on) around Wh-questions related to size.</p>
</div>
<div id="S4.SS0.SSS0.Px5.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p3.1" class="ltx_p">The <em id="S4.SS0.SSS0.Px5.p3.1.1" class="ltx_emph ltx_font_italic">Cumulative</em> model, in contrast, is able to further tease the different kinds of Y/N questions apart. Questions about different attributes become distinguishable in the plot, although overall Y/N questions remain closer together than the clusters for Wh-q. This is in line with the lower
performance of <em id="S4.SS0.SSS0.Px5.p3.1.2" class="ltx_emph ltx_font_italic">Cumulative</em> on Y/N-q. Our examination of the confusion matrices confirms that the two question types are never confused by the <em id="S4.SS0.SSS0.Px5.p3.1.3" class="ltx_emph ltx_font_italic">Cumulative</em> model.
In contrast, the <em id="S4.SS0.SSS0.Px5.p3.1.4" class="ltx_emph ltx_font_italic">Naive</em> model is very prone to this type of mistake (see plot in SM).</p>
</div>
<div id="S4.SS0.SSS0.Px5.p4" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p4.1" class="ltx_p">As for the CL models, Fig. <a href="#S4.F2" title="Figure 2 ‣ Task Order ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (two rightmost plots) shows that <em id="S4.SS0.SSS0.Px5.p4.1.1" class="ltx_emph ltx_font_italic">EWC</em> learns representations which are rather similar to those learned by the model trained
on Wh-q independently:
Y/N questions result in a big hard-to-distinguish “blob”, and are confused with
Wh-q about size, as visible in Fig. <a href="#S4.F2" title="Figure 2 ‣ Task Order ‣ 4 Results and Analysis ‣ Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and the confusion matrix analysis (in the SM).
In contrast, <em id="S4.SS0.SSS0.Px5.p4.1.2" class="ltx_emph ltx_font_italic">Rehearsal</em> remembers how to distinguish
among all kinds of Wh-q <span id="S4.SS0.SSS0.Px5.p4.1.3" class="ltx_text ltx_font_italic">and</span> between Wh-q and Y/N-q.
The error analysis confirms that the model hardly makes any mistakes
related to task confusion. However, despite the higher performance than <em id="S4.SS0.SSS0.Px5.p4.1.4" class="ltx_emph ltx_font_italic">EWC</em>,
<em id="S4.SS0.SSS0.Px5.p4.1.5" class="ltx_emph ltx_font_italic">Rehearsal</em> is still not able to discern well between different kinds of Y/N-q.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Early work on life-long learning <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib3" title="" class="ltx_ref">2015</a>); Mitchell et al. (<a href="#bib.bib11" title="" class="ltx_ref">2015</a>)</cite> is related to ours, but typically concerns a single task (e.g., relation extraction).
<cite class="ltx_cite ltx_citemacro_citet">Lee (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> aims to transfer conversational skills from a synthetic domain to a customer-specific application in dialogue agents, while
<cite class="ltx_cite ltx_citemacro_citet">Yogatama et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> show that current models for different NLP tasks are not able to properly reuse previously learned knowledge.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In general, continual learning has been mostly studied in computer vision.
To the best of our knowledge, little has been done on catastrophic
forgetting in VQA.
A study on forgetting in the context of VQA and closest to ours is <cite class="ltx_cite ltx_citemacro_citet">Perez et al. (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite>. They show that their model forgets after being fine-tuned
on data including images with objects of colors other than those
previously seen. We took this work as starting point and extended it to
consider different types of questions and to test different CL methods beyond fine-tuning.
</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We assessed to what extent a multimodal model suffers from
catastrophic forgetting in a VQA task. We built two tasks involving
different linguistic characteristics which are known to be learned
sequentially by children and on which multimodal models reach
different performance.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Our results show that dramatic forgetting is at
play in VQA, and for the tested task pairs we empirically found <span id="S6.p2.1.2" class="ltx_text ltx_font_italic">Rehearsal</span> to work better than a regularization-based method (<span id="S6.p2.1.3" class="ltx_text ltx_font_italic">EWC</span>).
More importantly, we show that the <span id="S6.p2.1.4" class="ltx_text ltx_font_italic">order</span> in which models learn
tasks is important, <span id="S6.p2.1.1" class="ltx_text ltx_font_smallcaps">wh<math id="S6.p2.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S6.p2.1.1.m1.1a"><mo stretchy="false" id="S6.p2.1.1.m1.1.1" xref="S6.p2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p2.1.1.m1.1b"><ci id="S6.p2.1.1.m1.1.1.cmml" xref="S6.p2.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.1.m1.1c">\rightarrow</annotation></semantics></math>y/n</span> facilitates continual learning
more than the opposite order, thereby confirming psycholinguistic evidence.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Our error analysis
highlights the importance of taking the kind of mistakes made by the models into account:
A model that does not detect Task A after having been
exposed to Task B should be penalized more than a model that answers
Task A with wrong task-related labels, but is still capable of identifying the task.
Most importantly, our study revealed that differences in the inherent
difficulty of the tasks at hand can have a strong impact on continual learning. Regularization-based
methods like <span id="S6.p3.1.1" class="ltx_text ltx_font_italic">EWC</span> appear to work less well when applied to tasks
with different levels of difficulty, as in our experiments.
We reserve a deeper investigation of this aspect to future research.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We kindly acknowledge the support of NVIDIA
Corporation with the donation of the GPUs used in our research to the University of Trento and IT University of Copenhagen.
R. Fernández was funded by the Netherlands Organisation for Scientific Research (NWO) under VIDI grant nr. 276-89-008, <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">Asymmetry in Conversation</span>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaudhry et al. (2018)</span>
<span class="ltx_bibblock">
Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip Torr.
2018.

</span>
<span class="ltx_bibblock">Riemannian walk for incremental learning: Understanding forgetting
and intransigence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2015)</span>
<span class="ltx_bibblock">
Zhiyuan Chen, Nianzu Ma, and Bing Liu. 2015.

</span>
<span class="ltx_bibblock">Lifelong learning for sentiment classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
<span class="ltx_bibblock">Short paper.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 770–778.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017a)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick. 2017a.

</span>
<span class="ltx_bibblock">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017b)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman,
Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. 2017b.

</span>
<span class="ltx_bibblock">Inferring and executing programs for visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick et al. (2017)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. 2017.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">PNAS</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee (2017)</span>
<span class="ltx_bibblock">
Sungjin Lee. 2017.

</span>
<span class="ltx_bibblock">Toward continual learning for conversational agents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maltoni and Lomonaco (2018)</span>
<span class="ltx_bibblock">
Davide Maltoni and Vincenzo Lomonaco. 2018.

</span>
<span class="ltx_bibblock">Continuous learning in single-incremental-task scenarios.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.08568</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McClelland et al. (1995)</span>
<span class="ltx_bibblock">
James L McClelland, Bruce L McNaughton, and Randall C O’reilly. 1995.

</span>
<span class="ltx_bibblock">Why there are complementary learning systems in the hippocampus and
neocortex: insights from the successes and failures of connectionist models
of learning and memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Psychol. Review</em>, 102(3).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell et al. (2015)</span>
<span class="ltx_bibblock">
T. Mitchell, W. Cohen, E. Hruscha, P. Talukdar, J. Betteridge, A. Carlson,
B. Dalvi, M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,
T. Mohammad, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles,
R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and
J. Welling. 2015.

</span>
<span class="ltx_bibblock">Never-ending learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moradlou and Ginzburg (2016)</span>
<span class="ltx_bibblock">
Sara Moradlou and Jonathan Ginzburg. 2016.

</span>
<span class="ltx_bibblock">Young children’s answers to questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Workshop on the Role of Pragmatic Factors on Child Language
Processing</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moradlou et al. (2018)</span>
<span class="ltx_bibblock">
Sara Moradlou, Xiaobei Zheng, Ye Tian, and Jonathan Ginzburg. 2018.

</span>
<span class="ltx_bibblock">Wh-questions are understood before polars.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of Architectures and Mechanisms for Language
Processing (AMLaP)</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et al. (2018)</span>
<span class="ltx_bibblock">
Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron
Courville. 2018.

</span>
<span class="ltx_bibblock">Film: Visual reasoning with a general conditioning layer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ring (1997)</span>
<span class="ltx_bibblock">
Mark Ring. 1997.

</span>
<span class="ltx_bibblock">CHILD: A first step towards continual learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Machine Learning</em>, 28(1).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robins (1995)</span>
<span class="ltx_bibblock">
Anthony Robins. 1995.

</span>
<span class="ltx_bibblock">Catastrophic forgetting, rehearsal and pseudorehearsal.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Connection Science</em>, 7(2):123–146.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yogatama et al. (2019)</span>
<span class="ltx_bibblock">
Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike
Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris
Dyer, et al. 2019.

</span>
<span class="ltx_bibblock">Learning and evaluating general linguistic intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.11373</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zenke et al. (2017)</span>
<span class="ltx_bibblock">
Friedemann Zenke, Ben Poole, and Surya Ganguli. 2017.

</span>
<span class="ltx_bibblock">Continual learning through synaptic intelligence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICML</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1906.04228" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1906.04229" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1906.04229">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1906.04229" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1906.04230" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 22:43:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
