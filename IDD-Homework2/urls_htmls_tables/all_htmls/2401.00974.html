<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.00974] Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models</title><meta property="og:description" content="Devising procedures for downstream task-oriented generative model selections is an unresolved problem of practical importance. Existing studies focused on the utility of a single family of generative models. They provi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.00974">

<!--Generated on Tue Feb 27 07:24:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">
Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yinan Cheng,
Chi-Hua Wang ,
<br class="ltx_break">Vamsi K. Potluru , Tucker Balch ,
Guang Cheng
</span><span class="ltx_author_notes">Dept of Statistics, UC Davis. Email: ynccheng@ucdavis.eduDepartment of Statistics and Data Science, Email: chihuawang@ucla.eduJ.P. Morgan AI ResearchJ.P. Morgan AI ResearchDepartment of Statistics and Data Science. Email: guangcheng@ucla.edu</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Devising procedures for downstream task-oriented generative model selections is an unresolved problem of practical importance. Existing studies focused on the utility of a single family of generative models. They provided limited insights on how synthetic data practitioners select the best family generative models for synthetic training tasks given a specific combination of machine learning model class and performance metric. In this paper, we approach the downstream task-oriented generative model selections problem in the case of training fraud detection models and investigate the best practice given different combinations of model interpretability and model performance constraints. Our investigation supports that, while both Neural Network(NN)-based and Bayesian Network(BN)-based generative models are both good to complete synthetic training task under loose model interpretability constrain, the BN-based generative models is better than NN-based when synthetic training fraud detection model under strict model interpretability constrain. Our results provides practical guidance for machine learning practitioner who is interested in replacing their training dataset from real to synthetic, and shed lights on more general downstream task-oriented generative model selection problems.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<br class="ltx_break">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold">Key Words:</span> Generative Model Selections, Synthetic Training Datasets, Fraud Detection, Accuracy-Interpretability Tradeoff, Data-centric Machine Learning, Train on Real Test on Synthetic.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Synthetic Data is receiving increasing attention from both academics and industry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Such attention is due to synthetic data’s potential to accelerate innovation and support decision-making without violating modern privacy regulations (e.g. GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> or CCPA). The potential for acceleration and support of the modern machine learning lifecycle is lucrative and hence invites both machine learning researchers and practitioners investigation on the benefits and limitations of using synthetic datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. In particular, <span id="S1.p1.1.1" class="ltx_text ltx_font_bold">what is the prize and price on <span id="S1.p1.1.1.1" class="ltx_text ltx_font_italic">machine learning model training process</span> if we replace from real training dataset with a synthetic training dataset?</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Indeed, the prize is individual-level privacy protection, and the price is performance degradation. Although the current community does not reach an agreement on how effective the synthetic data approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, existing studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> observe performance degradations from replacing real with the synthetic training datasets.
In practice, there are three family of generative models to synthesize training dataset: Machine Learning-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, Neural network-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and Bayesian network-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> generative models.
Due to the predicament of striking a balance between accuracy and privacy leakage for Machine Learning-based generative models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, in this paper, we focus on Neural network-based and Bayesian network-based generative models.
Given primitive privacy protection by replacing the real training dataset with the synthetic dataset, the following question guides our investigation:
<span id="S1.p2.1.1" class="ltx_text ltx_font_bold">Which family of synthetic data generative models sufferers least performance degradation?</span>
We define the above key research question as the <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">Generative Model Selection (GMS)</span> problems.
In particular, we investigate the generative model selection problem in the context of the fraud detection model (FDM) training procedure.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The fraud detection model is an integral part of the modern fraud management process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, where the utility-interpretability trade-off is of particular importance to stop fraudsters to limit fraud impact in financial service operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
The utility is the key performance metric to measure how effective the trained fraud detection model detecting potential fraudulent operations. However, FDM with high utility also incurs a high number of false alerts, leading to potential high human resource cost<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. In practice, those alerts reported by FDM are reviewed by fraud experts, demanding model interpretability to make decisions on free or suspend the potential fraudulent operation. Consequently, <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">insights on the impact of synthetic dataset trained FDM, especially around the utility-interpretability trade-off, are desired and of practical importance</span>.
In answering GMS problems in synthetic dataset trained FDM, this paper aims to solve the following two questions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Utility-oriented GMS</span>: <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">Given a cost-specific metric</span>, which family of generative models suffers the least degradation?</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Interpretability-oriented GMS</span>: <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">Given a specific interpretability constraint</span>, which family of generative models suffers the least degradation?</p>
</div>
</li>
</ul>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contributions</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">In this paper, we adopt a downstream task-oriented approach to the evaluation of generative models introduced in section <a href="#S2.SS1" title="2.1 Synthetic Data Generative Models ‣ 2 Relate Work ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>. Instead of assessing the generative data distribution by looking into the correlation or distance between real and synthetic datasets, we examine
the performance of fraud detection models trained on Neural network-based or Bayesian Network-based synthetic training data. Our evaluation is from 3 dimensions: data (Synthetic Data generative models), models (fraud detection models), and metrics (accuracy, AUROC, recall, precision). Our examination focus on two class of generative model selection problems: Utility-oriented generative model selections and Interpretability-oriented generative model selections.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">We provide 3 guidance for fraud data scientists and machine learning practitioners interested in the utility-interpretability behind using synthetic training data to train their fraud detection models</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We systematically compare different performance metrics for fraud detection models to give the best advice on different priorities to stop fraudsters.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We systematically compare different machine learning model candidates for fraud detection models from the layer of interpretability.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">We give guidance to select synthetic data generative models for different model-metric combinations.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p"><span id="S1.SS1.p3.1.1" class="ltx_text ltx_font_bold">Utility-oriented Generative Model Selections</span>
The key question of the Utility-oriented generative model selection problem is on which generative model should be used to generate synthetic training data to train fraud detection under <span id="S1.SS1.p3.1.2" class="ltx_text ltx_font_italic">given metric</span>. We found the best choice of the generative model family is <span id="S1.SS1.p3.1.3" class="ltx_text ltx_font_italic">metric-dependent</span>.
In short, we provide <span id="S1.SS1.p3.1.4" class="ltx_text ltx_font_italic">insights on Utility-oriented GMS.(Section <a href="#S4.SS2" title="4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>)</span></p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<ul id="S1.I3" class="ltx_itemize">
<li id="S1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i1.p1" class="ltx_para">
<p id="S1.I3.i1.p1.1" class="ltx_p">Accuracy did not show a preference between neural network-based (NN-based) and Bayesian network-based (BN-based) models.</p>
</div>
</li>
<li id="S1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i2.p1" class="ltx_para">
<p id="S1.I3.i2.p1.1" class="ltx_p">AUROC and Recall prefer NN-based generative models.</p>
</div>
</li>
<li id="S1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i3.p1" class="ltx_para">
<p id="S1.I3.i3.p1.1" class="ltx_p">F1 score and Precision prefers BN-based generative models.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p"><span id="S1.SS1.p5.1.1" class="ltx_text ltx_font_bold">Interpretability-oriented Generative Model Selections</span>
On the other hand, the interpretability-oriented generative model selection problem asks for the best family of generative models to generate training data <span id="S1.SS1.p5.1.2" class="ltx_text ltx_font_italic">given
fraud detection model class</span>. We found that the Bayesian Network-based method is better for an intrinsic interpretable model class, while both generative model families are good for the complex model classes. In short, we provide
<span id="S1.SS1.p5.1.3" class="ltx_text ltx_font_italic">insights on Interpretability-oriented GMS. (Section <a href="#S4.SS3" title="4.3 Results on Interpretability-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</span></p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<ul id="S1.I4" class="ltx_itemize">
<li id="S1.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I4.i1.p1" class="ltx_para">
<p id="S1.I4.i1.p1.1" class="ltx_p">Intrinsic interpretable model class and medium interpretable model class prefers BN-based generative models.</p>
</div>
</li>
<li id="S1.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I4.i2.p1" class="ltx_para">
<p id="S1.I4.i2.p1.1" class="ltx_p">Not-Easy interpretable model class shows no preference between NN-based and BN-based models.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Paper Organization</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">This paper is structured as follows.
Section <a href="#S2" title="2 Relate Work ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives a comprehensive reviews on related research communities across synthetic data generative models (Section <a href="#S2.SS1" title="2.1 Synthetic Data Generative Models ‣ 2 Relate Work ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>), fraud detection model interpretability (Section <a href="#S2.SS2" title="2.2 Fraud Detection Model Interpretability ‣ 2 Relate Work ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>) and fraud detection metric utility (Section <a href="#S2.SS3" title="2.3 Fraud Detection Metric Utility ‣ 2 Relate Work ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>). Section <a href="#S3" title="3 Experiment Setup ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> gives experiment details on how to synthesize training dataset (Section <a href="#S3.SS1" title="3.1 Training Data Synthesis ‣ 3 Experiment Setup ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), choices of fraud detection model class (Section <a href="#S3.SS2" title="3.2 Choice of Fraud Detection Model Class ‣ 3 Experiment Setup ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) and different fraud detection utility metrics (Section <a href="#S3.SS3" title="3.3 Choice of Fraud Detection Utility Metrics ‣ 3 Experiment Setup ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Section <a href="#S4" title="4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports result of our evaluation on how generative models has effect on the imbalance of real training dataset (Section <a href="#S4.SS1" title="4.1 Comparison of Original to Synthetic Data ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), and results on Utility-oriented GMS (Section <a href="#S4.SS2" title="4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) and Interpretability-oriented GMS (Section <a href="#S4.SS3" title="4.3 Results on Interpretability-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>). Section <a href="#S5" title="5 Conclusion ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> talks about our conclusion, new concerns and future direction.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Relate Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic Data Generative Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Neural Network-based generative models.</span>
<span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">Synthetic Data Vault</span> (SDV) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> provides three neural network-based generative models to synthesize data from a single table. Generative adversarial networks (GANs) are commonly used tools for fraud detection synthetic data due to their ability to address imbalanced datasets via data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. SDV provides a GAN-based generative model proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, namely conditional tabular GAN (CTGAN). CopulaGAN, another GAN-based model included in SDV, is a variation of the CTGAN Model which takes advantage of the cumulative distribution function (CDF) based transformation. Another type of neural network-based generative model is based on variational autoencoders (VAEs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. SDV also provides a VAE-based generative model, namely, TVAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, to synthesize tabular data. Besides neural network-based generative models, SDV offers GaussianCopula <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to model the covariances between features in addition to the distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Bayesian Network-based generative models.</span>
<span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">DataSynthesizer</span> (DS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> has three modes to invoke modules: random mode, independent attribute mode, and correlated attribute mode. The correlated attribute mode uses the GreedyBayes algorithm to construct Bayesian networks to model correlated attributes, which helps to retain the correlation among variables. Another important parameter in DS is epsilon which represents differential privacy to address data protection and privacy issues. When epsilon approaches 0, the presence or absence of a single case in the input will be undetectable in the output.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Fraud Detection Model Interpretability</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">(i) Intrinsic Interpretable Model class.</span>
Logistic Regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> provides predictions based on the estimated probability of an event occurring. A transaction will be predicted as a fraud if its estimated probability of being a fraud passes some threshold. Decision Tree <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is one of the non-parametric supervised learning models used for classification tasks. It predicts classes of transactions via decision rules for data features.
K-nearest Neighbors (KNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is another commonly used non-parametric supervised learning method. Under the assumption that similar points can be found near each other, it uses proximity to make predictions about the class of a transaction.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">(ii) Medium Interpretable Model class.</span>
Naïve Bayes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> is a probabilistic classifier based on Bayes’ Theorem with the assumption of conditional independence between each pair of data features. The predicted class of a transaction is with the maximum probability. Support Vector Machine (SVM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is designed to find the hyperplane in an n-dimensional space (n is the number of features) that distinctly classifies data points. Random forest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is one of the ensemble learning methods. It is composed of abundant decision trees and aggregates all predicted classes of decision trees to identify the most popular result as the prediction.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">(iii) Not-Easy Interpretable Model class.</span>
The generalized Additive Model (GAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is a generalized linear model in which the response is linearly dependent on smooth functions. The smooth relationships between the response and each feature can be estimated simultaneously, and the response in test data can be predicted by adding them up. For binary classification, a logit link is applied to data fitting. Xgboost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is a scalable, distributed gradient boosting system under the Gradient Boosting framework. It provides parallel tree boosting and improves computational efficiency and model performance. Neural Additive Model (NAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> focuses on a linear combination of deep neural networks that each has a single input feature. The model is fitted via training jointly these neural networks and learning complicated relationships between their inputs and outputs.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fraud Detection Metric Utility</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p"><span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_bold">Fraud Management Process.</span> Fraud management process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> has achieved great success in financial service industries due to its capability to catch fraudulent transactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. As the fraud detection models flag some transaction to be suspicious, it may cause interpretability and the fraud agent have difficulty telling whether the flag is a false alert or not. The fraud detection interpretability issues exist in all kinds of financial service applications including credit or debit cards, payments, and loan approval.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">One major reason behind this communication bottleneck in the fraud management process is the interpretability of detection. As the fraud data scientist wishes to catch as more suspicious operations as possible, the fraud detection model itself becomes more difficult to interpret by using a more complex model class, e.g. neural network-based model. Consequently, the alerted operations become difficult to interpret, and the fraud experts need to take more time to identify the fraud factor, leading to such a communication bottleneck. Such a late decision allows the fraudsters to maximize their fraud fain, which greatly impairs the effectiveness of the model and also results in enormous economical and opportunity loss. As such, the interpretability in fraud detection model machine learning has become a rising concern.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In order to trade off the model performance and model interpretability, existing works are all focused on a model-centric approach. The model-centric approach takes the mindset of ”accuracy first, interpretability later”. They first compare the performance of different classes of fraud detection models. Then adopts the model’s nature to gain the model interpretability. In addition to the intrinsic interpretable model class, there are various methods proposed to do post-doc methods to gain model-specific interpretability. The main drawback of the model-centric approach is the lack of a fair way to compare model interpretability in the same framework.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">Performance Metrics.</span> We evaluate synthetic data training of FDM with metrics for standard classifier and for imbalanced dataset trained classifier.
<span id="S2.SS3.p4.1.2" class="ltx_text ltx_font_bold">(i) Metrics for standard classifier.</span>
Accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is one of the commonly used metrics in machine learning tasks. It is useful when all classes are of equal importance, but it can be misleading for an imbalanced dataset. AUROC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> describes the model’s ability to discriminate between positive cases and negative cases. F1 score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> measures the performance of a model by computing the harmonic mean of the precision and recall of the model. <span id="S2.SS3.p4.1.3" class="ltx_text ltx_font_bold">(ii) Metrics for imbalanced dataset trained classifier.</span>
<span id="S2.SS3.p4.1.4" class="ltx_text ltx_font_italic">Recall</span> and <span id="S2.SS3.p4.1.5" class="ltx_text ltx_font_italic">precision</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> are two metrics which are more suitable than accuracy for imbalanced testing datasets. There is an inverse relationship between precision and recall usually. Precision-Recall curve <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> shows the tradeoff between precision and recall for different thresholds. Average precision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> summarizes such a tradeoff as the weighted mean of precisions achieved at each threshold, where the weight is the increase in recall from the prior threshold.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training Data Synthesis</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset-Credit Card Fraud Dataset.</span> We conduct the experiment on the Credit Card Fraud Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> which is highly imbalanced (0.1727% transactions are frauds). The dataset contains 31 variables. The feature ”Amount” is the transaction amount, and the feature ”Time” represents the seconds elapsed between each transaction and the first transaction in the dataset. ”Class” is the response variable and it takes 1 for fraudulent cases and 0 for other cases. Due to confidentiality, the remaining 28 features are principal components obtained by the means of PCA. In our experiment, we delete the feature ”Time” since it has nothing to do with ”Class”.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Generating Synthetic Training Dataset.</span>
SDV is applied with GaussianCopula, CTGAN, CopulaGAN and TVAE, to generate synthetic data. We employ the four generative models on the original dataset respectively and obtain 4 synthetic datasets. DS is utilized with the correlated attribute mode. Synthetic datasets are generated with two generative models. One is with epsilon = 0, and the other is with epsilon = 0.1.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Since the original dataset is highly imbalanced, some synthetic data generative models cannot yield transactions with class 1 (fraudulent cases). It is found that no fraudulent transactions are synthesized when using GaussianCopula and CopulaGAN. Therefore, we leave out the datasets generated by GaussianCopula and CopulaGAN in the following tasks.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Choice of Fraud Detection Model Class</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Data Training Procedure.</span>
After generating synthetic datasets of the same size as the original dataset, we randomly split the original dataset into training data (70% of the original data) and test data (30% of the original data), and randomly select 70% of each synthesized dataset as the synthesized training data. Then we fit fraud detection models to the training data and each synthesized training data and predict the results on the test data.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Model class by Interpretability.</span> We fit 9 fraud detection models to analyze utility-interpretability on FDM training tasks. The following list summarizes 9 models class from high to low interpretability:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Intrinsic interpretable models</span>: Logistic Regression (LR), Decision Tree (DT) and K-nearest Neighbors (KNN).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Medium interpretable models</span>: Naïve Bayes (NB), Support Vector Machines (SVM), Random Forest (RF).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Not-easy interpretable models</span>: Generalized Additive Model (GAM), Extreme Gradient Boosting (XGBoost), Neural Additive Model (NAM).</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Choice of Fraud Detection Utility Metrics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To evaluate the performance of synthesized data on fraud detection, we compute accuracy, AUROC, recall, precision, and F1 score, and generate the Precision-Recall curve and average precision (AP) for each machine learning task based on the predicted results.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Basic Performance Metrics for Classifier.</span></p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Accuracy</span> is the ratio of the number of correct predictions to the number of all predictions. It describes how the model performs across both classes.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">AUROC</span> measures the ability of a classifier to distinguish between the fraudulent class and the nonfraudulent class.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">F1 score</span> is the harmonic mean between precision and recall. It provides equal importance to precision and recall.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Performance Metrics for Classifier on Imbalanced Dataset.</span></p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<ul id="S3.I3" class="ltx_itemize">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p"><span id="S3.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Precision</span> is defined as the ratio of true positives to the sum of true positives and false positives. It is the number of correctly predicted fraudulent cases divided by the total number of predicted fraudulent cases for fraud detection.</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p"><span id="S3.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Recall</span> is defined as the ratio of true positives to the sum of true positives and false negatives. It is the number of correctly predicted fraudulent cases divided by the total number of actual fraudulent cases in the fraud detection scenario.</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p"><span id="S3.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Precision-Recall curve</span> shows the tradeoff between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision.</p>
</div>
</li>
<li id="S3.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i4.p1" class="ltx_para">
<p id="S3.I3.i4.p1.1" class="ltx_p"><span id="S3.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Average precision</span> is calculated as the weighted mean of precisions at each threshold, where the weight is the increase in recall from the prior threshold.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison of Original to Synthetic Data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">(1) Balance.</span>
Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Comparison of Original to Synthetic Data ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the degree of imbalance of synthetic training datasets.
Datasets generated by CTGAN and DS with epsilon = 0.1 are balanced (59.4919% and 45.4743% transactions are frauds respectively). TVAE synthesizes a more imbalanced dataset (0.0119% transactions are frauds) than the original dataset. When using DS with epsilon = 0, the synthesized dataset contains 0.1731% fraudulent transactions, which is similar to the original dataset.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Percentage of classes</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Approach</td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Class 1 (frauds)</td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Class 0</td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Original</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0.1727%</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">99.8273%</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_center">CTGAN</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_center">59.4919%</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_align_left">40.5081%</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_center">TVAE</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_center">0.0119%</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_left">99.9881%</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_center">DS 0</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_align_center">0.1731%</td>
<td id="S4.T1.1.5.3" class="ltx_td ltx_align_left">99.8269%</td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">DS 0.1</td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">45.4743%</td>
<td id="S4.T1.1.6.3" class="ltx_td ltx_align_left ltx_border_bb">54.5257%</td>
</tr>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">(2) Correlation.</span>
Dataset generated by DS with epsilon = 0 maintains the correlation of each pair of two features. However, datasets synthesized by other generative models do not retain the correlation between features.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results on Utility-oriented GMS</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In order to compare the performance of the original data and synthesized data in fraud detection tasks, we generate line charts to show the results of each metric, for data generated with different approaches as models’ interpretability increases.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">(1) Accuracy.</span>
Since the original dataset is highly imbalanced, the accuracy on test data is very close to 1 for each fraud detection model. In Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">training dataset generated by TVAE, DS with epsilon 0 and DS with epsilon 0.1 also yield high accuracy</span>. The accuracy of data synthesized by DS is even higher than the accuracy of the original data for Naïve Bayes. CTGAN has lower accuracy than other approaches for all fraud detection models.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">It is remarkable that
<span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">accuracy of Naïve Bayes is highly unstable across different synthetic training datasets</span>, while other fraud detection models are with stable accuracy except for CTGAN-based training data. Although DS with epsilon 0.1 has the highest accuracy for Naïve Bayes, its accuracy for Decision Tree is the second lowest. Compared with the original data, data synthesized by DS with epsilon 0 has almost the same or higher accuracy.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Hence, <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_italic">DS with epsilon 0 performs the best in accuracy overall for fraud detection tasks</span>. Bayesian network-based generative models are selected by accuracy metric due to their better performance in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where both Bayesian network-based generative models outstrip neural network-based generative models.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">(2) AUROC.</span>
For AUROC, the original data performs the best for GAM and Decision Tree. DS with epsilon 0.1 improves the performance for Naïve Bayes, CTGAN and TVAE improve the results for XGBoost, and only CTGAN increases the value of AUROC for the other 5 fraud detection models. DS with epsilon 0 produces smaller AUROC than the original data for all models. <span id="S4.SS2.p5.1.2" class="ltx_text ltx_font_bold">Naïve Bayes and Logistic Regression show robustness while the result of AUROC varies considerably among data generative approaches for KNN and Decision Tree</span>. Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> indicates that CTGAN is a suitable data generative technique to synthesize data considering the performance of AUROC, but for Naïve Bayes, DS with epsilon 0.1 is preferred.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">In the fraud detection scenario, <span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_italic">Naïve Bayes has better discrimination between fraudulent transactions and nonfraudulent transactions when the training data is generated by DS with epsilon 0.1</span>. When the training data is synthesized by CTGAN, Logistic Regression, KNN, SVM, Random Forest, XGBoost and NAM perform better at distinguishing between fraudulent cases and nonfraudulent cases. Thus, <span id="S4.SS2.p6.1.2" class="ltx_text ltx_font_bold">AUROC prefers neural-network based generative models, especially CTGAN which achieves comparable performance to the original data</span>.</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/accuracy.png" id="S4.F1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="126" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/auroc.png" id="S4.F1.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/f1.png" id="S4.F1.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/recall.png" id="S4.F1.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="132" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/precision.png" id="S4.F1.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="132" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Results to solve Utility-oriented GMS: Utility Metrics for Fraud Detection Classifiers</figcaption>
</figure>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">(3) F1 score.</span>
According to Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <span id="S4.SS2.p7.1.2" class="ltx_text ltx_font_italic">only DS with epsilon 0 has higher F1 score than the original for Naïve Bayes</span>. For other fraud detection models, the original data has the best performance, especially for SVM for which there is a noticeable difference in F1 score between the original data and synthesized data. In addition, it is found that <span id="S4.SS2.p7.1.3" class="ltx_text ltx_font_italic">F1 scores of CTGAN are almost 0 for all fraud detection models</span>. Even though F1 scores are relatively similar for Naïve Bayes, none of the models show robustness for all datasets. Data generated by DS with epsilon 0 yields the highest F1 score, but it seems doubtful that this approach is suitable for Naïve Bayes because all F1 scores, including the F1 score of DS with epsilon 0, are very low for Naïve Bayes.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">Combining recall and precision, DS with epsilon 0 improves the performance of Naïve Bayes for fraud detection. Other fraud detection models show better performance when they are trained by the original dataset. In conclusion, <span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_bold">F1 score selects Bayesian network-based generative models since overall DS with epsilon 0 has the best performance among all generative techniques</span>.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p"><span id="S4.SS2.p9.1.1" class="ltx_text ltx_font_bold">(4) Recall.</span>
<span id="S4.SS2.p9.1.2" class="ltx_text ltx_font_italic">CTGAN has considerably higher recall</span> than other approaches in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Compared with the original dataset, data generated by TVAE also increases recall scores for Naïve Bayes and Logistic Regression while DS with epsilon 0 and DS with epsilon 0.1 perform worse than the original for all fraud detection models. <span id="S4.SS2.p9.1.3" class="ltx_text ltx_font_italic">Only Naïve Bayes is robust for recall when leaving out DS with epsilon 0.1</span>. Because of the remarkable performance, <span id="S4.SS2.p9.1.4" class="ltx_text ltx_font_bold">CTGAN is the suitable technique to generate synthetic data when we focus on recall scores</span>.</p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p">All machine learning models yield more correctly predicted frauds over total actual frauds in the fraud detection tasks when the training data is synthesized by CTGAN. More fraudulent cases are recognized properly when using CTGAN to generate the training data. Therefore, <span id="S4.SS2.p10.1.1" class="ltx_text ltx_font_bold">neural network-based generative models are preferred by recall</span> because CTGAN surpasses all of the other methods and even distinctly exceeds the original data.</p>
</div>
<div id="S4.SS2.p11" class="ltx_para">
<p id="S4.SS2.p11.1" class="ltx_p"><span id="S4.SS2.p11.1.1" class="ltx_text ltx_font_bold">(5) Precision</span>
Compared with the original, TVAE increases the precision score a little for NAM, and DS with epsilon 0 improves the performance in precision for Naïve Bayes in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Data synthesized by DS with epsilon 0 produces the same precision score to the original data for SVM. We can see that DS with epsilon 0.1 has worse performance than the original for all fraud detection models, and all precision scores of CTGAN are close to 0. Except for Naïve Bayes, all models show differences in precision scores among approaches. For NAM, TVAE is a suitable method for data generation, and for SVM and Naïve Bayes, DS with epsilon 0 can synthesize data comparable to the original data when focusing on precision. There are no synthetic data generative models with comparable precision to the original dataset for other machine learning models.</p>
</div>
<div id="S4.SS2.p12" class="ltx_para">
<p id="S4.SS2.p12.1" class="ltx_p">For fraud detection, NAM can correctly predict more frauds among all predicted fraudulent cases when the training data is generated by TVAE. Naïve Bayes has more correct predictions for cases predicted to be fraudulent when the training data is synthesized by DS with epsilon 0. Other fraud detection models produce a higher ratio of correct predicted frauds to all fraudulent predictions when it is trained by the original data. Among all data generative tools, DS with epsilon 0 has the best performance overall and CTGAN performs the worst, so <span id="S4.SS2.p12.1.1" class="ltx_text ltx_font_bold">precision prefers Bayesian network-based generative models</span>.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/LR.png" id="S4.F2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DT.png" id="S4.F2.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/KNN.png" id="S4.F2.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/NB.png" id="S4.F2.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/SVM.png" id="S4.F2.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/RF.png" id="S4.F2.g6" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/GAM.png" id="S4.F2.g7" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/XGBoost.png" id="S4.F2.g8" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/NAM.png" id="S4.F2.g9" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="130" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Results to solve Interpretability-oriented GMS: Precision-Recall curve and Average Precision</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results on
Interpretability-oriented GMS</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The original dataset yields Precision-Recall curves in the top right corner and the highest average precision for all machine learning models except for Naïve Bayes according to Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. DS with epsilon 0.1 has the Precision-Recall curve in the top right corner and the highest average precision for Naïve Bayes, followed by DS with epsilon 0. For Logistic Regression, TVAE produces the second highest average precision, and the Precision-Recall curve is just lower than the original. CTGAN and TVAE have Precision-Recall curves just lower than the original and the same average precision for XGBoost. For the other 6 fraud detection models, DS with epsilon 0 yields the second highest average precision, and the Precision-Recall curve is just lower than the original.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">(i) Intrinsic Interpretable Model class.</span>
In the first row of Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, there is no noticeable difference between neural network-based models and Bayesian network-based models in Precision-Recall curves for Logistic Regression. So, Logistic Regression indicates that neural network-based and Bayesian network-based generative models have comparable performance. Decision Tree and KNN suggest Bayesian network-based generative models as better tools for synthetic data generation.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">(ii) Medium Interpretable Model class.</span>
In the second row of Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_bold">all medium interpretable models select Bayesian network-based generative models to synthesize data</span>. Bayesian network-based generative models even have a better performance than the original for Naïve Bayes. Although Bayesian network-based generative methods perform better for all medium interpretable machine learning models, the difference is that Naïve Bayes prefers DS with epsilon 0.1 while SVM and Random Forest prefer DS with epsilon 0.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">(iii) Not-Easy Interpretable Model class.</span>
The third row of Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Results on Utility-oriented GMS ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the parallel performance of Bayesian network-based and neural network-based generative models for not-easy interpretable fraud detection models. For GAM, TVAE and DS with epsilon 0 show comparable performance. For XGBoost and NAM, CTGAN, TVAE and DS with epsilon 0 have similar Precision-Recall curves. Curves for DS with epsilon 0.1 are located in the bottom left corner for all not-easy interpretable models.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results on Synthetic Augmented Training</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">By <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">synthetic augmented training</span>, we mean the training Dataset for ML classifier is a mixture of source real dataset and certain percentage of synthetic data. In particular, we consider five differen degree of such real-synthetic mixture: <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">syn0.1, syn0.2, syn0.3, syn0.4, syn0.5</span>. For example, <span id="S4.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">syn0.2</span> means the training dataset is the mixture of 100% source real dataset and 20% of synthetic dataset from the synthesizer.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The full empirical results on Synthetic Augmented Training from metric-oriented and synthesizer-oriented are given at section <a href="#A1" title="Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. We summarize key insights below. (1) <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">CTGAN</span>-augmented training datasets improves the AUROC and recall of synthetic trained ML classifier. However, <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_typewriter">CTGAN</span>-augmented training datasets damages accuracy, F1 score, precision and precision-recall curve across all synthetic trained ML classifier. (2) <span id="S4.SS4.p2.1.3" class="ltx_text ltx_font_typewriter">TVAE</span>-augmented training datasets in general do not improve or damage the utility of synthetic trained ML classifier across all 6 performance performance metrics and all 9 ML Classifier. (3) <span id="S4.SS4.p2.1.4" class="ltx_text ltx_font_typewriter">PrivBayes</span>-augmented training datasets improves the accuracy of synthetic trained ML classifier. However, <span id="S4.SS4.p2.1.5" class="ltx_text ltx_font_typewriter">PrivBayes</span>-augmented training datasets damages AUROC, F1 score, recall, precision across all synthetic trained ML classifier.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we provide a practical evaluation of generative model selections for synthetic training of fraud detection models. Our evaluation framework covers data, models, and metrics and provides results to answer utility-oriented generative model selections and interpretability-oriented generative model selections.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">One promising future work direction is to develop <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">generative model auditing process</span> for generative models and their synthetic datasets. Such model auditing process has been explored in the domain-agnostic and model-agnostic way <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, but more task-oriented studies on model generative model auditing process are in demand. Indeed, such a generative model auditing process will generate values for data scientists and machine learning practitioners in integrating synthetic datasets into their daily workflow and leading a more trustworthy machine learning lifecycle in the near future.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Disclaimer.</span> This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase &amp; Co and its affiliates (“J.P. Morgan”), and is not a product of the Research Department of J.P. Morgan. J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
2018 reform of eu data protection rules.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Aisha Abdallah, Mohd Aizaini Maarof, and Anazida Zainal.

</span>
<span class="ltx_bibblock">Fraud detection system: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Journal of Network and Computer Applications</span>, 68:90–113, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
John M Abowd and Lars Vilhuber.

</span>
<span class="ltx_bibblock">How protective are synthetic data?

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">International Conference on Privacy in Statistical Databases</span>, pages 239–246. Springer, 2008.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Rishabh Agarwal, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Neural additive models: Interpretable machine learning with neural nets.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.13912</span>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ahmed Alaa, Boris Van Breugel, Evgeny S Saveliev, and Mihaela van der Schaar.

</span>
<span class="ltx_bibblock">How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 290–306. PMLR, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Kent Allen, Madeline M Berry, Fred U Luehrs Jr, and James W Perry.

</span>
<span class="ltx_bibblock">Machine literature searching viii. operational criteria for designing information retrieval systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">American Documentation (pre-1986)</span>, 6(2):93, 1955.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Naomi S Altman.

</span>
<span class="ltx_bibblock">An introduction to kernel and nearest-neighbor nonparametric regression.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">The American Statistician</span>, 46(3):175–185, 1992.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Joseph Berkson.

</span>
<span class="ltx_bibblock">Application of the logistic function to bio-assay.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Journal of the American statistical association</span>, 39(227):357–365, 1944.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Classification and regression trees</span>.

</span>
<span class="ltx_bibblock">Routledge, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Gregory Caiola and Jerome P Reiter.

</span>
<span class="ltx_bibblock">Random forests for generating partially synthetic, categorical data.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Trans. Data Priv.</span>, 3(1):27–42, 2010.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Charitos Charitou, Simo Dragicevic, and Artur d’Avila Garcez.

</span>
<span class="ltx_bibblock">Synthetic data generation for fraud detection using gans.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.12546</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Tianqi Chen and Carlos Guestrin.

</span>
<span class="ltx_bibblock">Xgboost: A scalable tree boosting system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</span>, pages 785–794, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Corinna Cortes and Vladimir Naumovich Vapnik.

</span>
<span class="ltx_bibblock">Support-vector networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Machine Learning</span>, 20:273–297, 2004.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi.

</span>
<span class="ltx_bibblock">Calibrating probability with undersampling for unbalanced classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2015 IEEE symposium series on computational intelligence</span>, pages 159–166. IEEE, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jörg Drechsler.

</span>
<span class="ltx_bibblock">Using support vector machines for generating synthetic datasets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">International Conference on Privacy in Statistical Databases</span>, pages 148–161. Springer, 2010.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Khaled El Emam, Lucy Mosquera, and Richard Hoptroff.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Practical synthetic data generation: balancing privacy and the broad availability of data</span>.

</span>
<span class="ltx_bibblock">O’Reilly Media, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Josh Eno and Craig W Thompson.

</span>
<span class="ltx_bibblock">Generating synthetic data to match data mining patterns.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Internet Computing</span>, 12(3):78–82, 2008.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Evelyn Fix and Joseph Lawson Hodges.

</span>
<span class="ltx_bibblock">Discriminatory analysis. nonparametric discrimination: Consistency properties.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">International Statistical Review/Revue Internationale de Statistique</span>, 57(3):238–247, 1989.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Grigoriy Gogoshin, Sergio Branciamore, and Andrei S Rodin.

</span>
<span class="ltx_bibblock">Synthetic data generation with probabilistic bayesian networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Mathematical biosciences and engineering: MBE</span>, 18(6):8603, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
James A Hanley and Barbara J McNeil.

</span>
<span class="ltx_bibblock">The meaning and use of the area under a receiver operating characteristic (roc) curve.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Radiology</span>, 143(1):29–36, 1982.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Trevor Hastie and Robert Tibshirani.

</span>
<span class="ltx_bibblock">Generalized additive models: some applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Journal of the American Statistical Association</span>, 82(398):371–386, 1987.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Markus Hittmeir, Andreas Ekelhart, and Rudolf Mayer.

</span>
<span class="ltx_bibblock">On the utility of synthetic data: An empirical evaluation on machine learning tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the 14th International Conference on Availability, Reliability and Security</span>, pages 1–6, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Tin Kam Ho.

</span>
<span class="ltx_bibblock">Random decision forests.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of 3rd international conference on document analysis and recognition</span>, volume 1, pages 278–282. IEEE, 1995.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
James Jordon, Lukasz Szpruch, Florimond Houssiau, Mirko Bottarelli, Giovanni Cherubin, Carsten Maple, Samuel N Cohen, and Adrian Weller.

</span>
<span class="ltx_bibblock">Synthetic data–what, why and how?

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2205.03257</span>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Max Welling.

</span>
<span class="ltx_bibblock">Auto-encoding variational bayes.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1312.6114</span>, 2013.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A Langevin, T Cody, S Adams, and P Beling.

</span>
<span class="ltx_bibblock">Synthetic data augmentation of imbalanced datasets with generative adversarial networks under varying distributional assumptions: A case study in credit card fraud detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Journal of the Operational Research Society</span>, pages 1–28, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Alex Langevin, Tyler Cody, Stephen Adams, and Peter Beling.

</span>
<span class="ltx_bibblock">Generative adversarial networks for data augmentation and transfer in credit card fraud detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Journal of the Operational Research Society</span>, 73(1):153–180, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Majlinda Llugiqi and Rudolf Mayer.

</span>
<span class="ltx_bibblock">An empirical analysis of synthetic-data-based anomaly detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</span>, pages 306–327. Springer, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Guido Masarotto and Cristiano Varin.

</span>
<span class="ltx_bibblock">Gaussian copula marginal regression.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Electronic Journal of Statistics</span>, 6:1517–1549, 2012.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
W James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu.

</span>
<span class="ltx_bibblock">Definitions, methods, and applications in interpretable machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the National Academy of Sciences</span>, 116(44):22071–22080, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Anna Nesvijevskaia, Sophie Ouillade, Pauline Guilmin, and Jean-Daniel Zucker.

</span>
<span class="ltx_bibblock">The accuracy versus interpretability trade-off in fraud detection model.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Data &amp; Policy</span>, 3, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim.

</span>
<span class="ltx_bibblock">Data synthesis based on generative adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.03384</span>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Neha Patki, Roy Wedge, and Kalyan Veeramachaneni.

</span>
<span class="ltx_bibblock">The synthetic data vault.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</span>, pages 399–410. IEEE, 2016.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Haoyue Ping, Julia Stoyanovich, and Bill Howe.

</span>
<span class="ltx_bibblock">Datasynthesizer: Privacy-preserving synthetic datasets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the 29th International Conference on Scientific and Statistical Database Management</span>, pages 1–5, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
David MW Powers.

</span>
<span class="ltx_bibblock">Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.16061</span>, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Vijay Raghavan, Peter Bollmann, and Gwang S Jung.

</span>
<span class="ltx_bibblock">A critical investigation of recall and precision as measures of retrieval system performance.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Information Systems (TOIS)</span>, 7(3):205–229, 1989.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Nick F Ryman-Tubb, Paul Krause, and Wolfgang Garn.

</span>
<span class="ltx_bibblock">How artificial intelligence and machine learning research impacts payment card fraud detection: A survey and industry benchmark.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Engineering Applications of Artificial Intelligence</span>, 76:130–157, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yutaka Sasaki et al.

</span>
<span class="ltx_bibblock">The truth of the f-measure.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Teach tutor mater</span>, 1(5):1–5, 2007.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Hinrich Schütze, Christopher D Manning, and Prabhakar Raghavan.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Introduction to information retrieval</span>, volume 39.

</span>
<span class="ltx_bibblock">Cambridge University Press Cambridge, 2008.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Theresa Stadler, Bristena Oprisanu, and Carmela Troncoso.

</span>
<span class="ltx_bibblock">Synthetic data–anonymisation groundhog day.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">31st USENIX Security Symposium (USENIX Security 22)</span>, pages 1451–1468, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Giorgio Visani, Giacomo Graffi, Mattia Alfero, Enrico Bagli, Davide Capuzzo, and Federico Chesani.

</span>
<span class="ltx_bibblock">Enabling synthetic data adoption in regulated domains.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2204.06297</span>, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Wesley Kenneth Wilhelm.

</span>
<span class="ltx_bibblock">The fraud management lifecycle theory: A holistic approach to fraud management.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Journal of economic crime management</span>, 2(2):1–38, 2004.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni.

</span>
<span class="ltx_bibblock">Modeling tabular data using conditional gan.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 32, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Lei Xu and Kalyan Veeramachaneni.

</span>
<span class="ltx_bibblock">Synthesizing tabular data using generative adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.11264</span>, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Harry Zhang.

</span>
<span class="ltx_bibblock">The optimality of naive bayes.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Aa</span>, 1(2):3, 2004.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Jun Zhang, Graham Cormode, Cecilia M Procopiuc, Divesh Srivastava, and Xiaokui Xiao.

</span>
<span class="ltx_bibblock">Privbayes: Private data release via bayesian networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Database Systems (TODS)</span>, 42(4):1–41, 2017.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experiment Results on Synthetic Data Augmented Training</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">This section provides additional empirical results for Synthetic Augmented Training discussed at Section <a href="#S4.SS4" title="4.4 Results on Synthetic Augmented Training ‣ 4 Evaluation ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">Metric-Oriented results.</span></p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">Figure <a href="#A1.F3" title="Figure 3 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> gives Synthetic Augmented Training result under Accuracy performance metric.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">Figure <a href="#A1.F5" title="Figure 5 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> gives Synthetic Augmented Training result under AUROC performance metric.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">Figure <a href="#A1.F6" title="Figure 6 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> gives Synthetic Augmented Training result under F1 performance metric.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">Figure <a href="#A1.F4" title="Figure 4 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives Synthetic Augmented Training result under Recall performance metric.</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">Figure <a href="#A1.F7" title="Figure 7 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> gives Synthetic Augmented Training result under Precision performance metric.</p>
</div>
</li>
</ul>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p"><span id="A1.p3.1.1" class="ltx_text ltx_font_bold">Synthesizer-Oriented results.</span></p>
</div>
<div id="A1.p4" class="ltx_para">
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p">Figure <a href="#A1.F8" title="Figure 8 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> gives Synthetic Augmented Training result in Precision-Recall curve for CTGAN synthesizer.</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p">Figure <a href="#A1.F9" title="Figure 9 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> gives Synthetic Augmented Training result in Precision-Recall curve for TVAE synthesizer.</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p">Figure <a href="#A1.F10" title="Figure 10 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> gives Synthetic Augmented Training result in Precision-Recall curve for DS0 synthesizer.</p>
</div>
</li>
<li id="A1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i4.p1" class="ltx_para">
<p id="A1.I2.i4.p1.1" class="ltx_p">Figure <a href="#A1.F11" title="Figure 11 ‣ Appendix A Experiment Results on Synthetic Data Augmented Training ‣ Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> gives Synthetic Augmented Training result in Precision-Recall curve for DS1 synthesizer.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A1.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/accu1.png" id="A1.F3.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/accu2.png" id="A1.F3.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/accu3.png" id="A1.F3.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/accu4.png" id="A1.F3.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Accuracy. CTGAN-augmented training dataset damages synthetic trained classifier utility. PrivBayes-augmented training dataset improves synthetic trained classifier utility.</figcaption>
</figure>
<figure id="A1.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/rec1.png" id="A1.F4.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/rec2.png" id="A1.F4.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/rec3.png" id="A1.F4.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/rec4.png" id="A1.F4.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Recall. CTGAN-augmented training dataset improves synthetic trained classifier utility. PrivBayes-augmented training dataset damages synthetic trained classifier utility.</figcaption>
</figure>
<figure id="A1.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/roc1.png" id="A1.F5.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/roc2.png" id="A1.F5.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/roc3.png" id="A1.F5.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/roc4.png" id="A1.F5.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>AUROC. CTGAN-augmented training dataset improves synthetic trained classifier utility. PrivBayes-augmented training dataset damages synthetic trained classifier utility.</figcaption>
</figure>
<figure id="A1.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/f11.png" id="A1.F6.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/f12.png" id="A1.F6.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/f13.png" id="A1.F6.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/f14.png" id="A1.F6.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>F1. Both CTGAN-augmented and PrivBayes-augmented training dataset damages synthetic trained classifier utility.</figcaption>
</figure>
<figure id="A1.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/pre1.png" id="A1.F7.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/pre2.png" id="A1.F7.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/pre3.png" id="A1.F7.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.00974/assets/pre4.png" id="A1.F7.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="299" height="200" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Precision. Both CTGAN-augmented and PrivBayes-augmented training dataset damages synthetic trained classifier utility.</figcaption>
</figure>
<figure id="A1.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN0.png" id="A1.F8.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN1.png" id="A1.F8.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN2.png" id="A1.F8.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN3.png" id="A1.F8.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN4.png" id="A1.F8.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN5.png" id="A1.F8.g6" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN6.png" id="A1.F8.g7" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN7.png" id="A1.F8.g8" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/CTGAN8.png" id="A1.F8.g9" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Precision-Recall curve for CTGAN. CTGAN-augmented training datasets in general damages the Precision-Recall curve across all ML classifier.</figcaption>
</figure>
<figure id="A1.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE0.png" id="A1.F9.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE1.png" id="A1.F9.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE2.png" id="A1.F9.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE3.png" id="A1.F9.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE4.png" id="A1.F9.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE5.png" id="A1.F9.g6" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE6.png" id="A1.F9.g7" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE7.png" id="A1.F9.g8" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/TVAE8.png" id="A1.F9.g9" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Precision-Recall curve for TVAE. TVAE-augmented training datasets in general do not improve or damage the Precision-Recall curve across all ML classifier.</figcaption>
</figure>
<figure id="A1.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_00.png" id="A1.F10.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_01.png" id="A1.F10.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_02.png" id="A1.F10.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_03.png" id="A1.F10.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_04.png" id="A1.F10.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_05.png" id="A1.F10.g6" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_06.png" id="A1.F10.g7" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_07.png" id="A1.F10.g8" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_08.png" id="A1.F10.g9" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Precision-Recall curve for DS 0. DS 0-augmented training datasets in general do not improve or damage the Precision-Recall curve across all ML classifier.</figcaption>
</figure>
<figure id="A1.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_10.png" id="A1.F11.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_11.png" id="A1.F11.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_12.png" id="A1.F11.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_13.png" id="A1.F11.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_14.png" id="A1.F11.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_15.png" id="A1.F11.g6" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_16.png" id="A1.F11.g7" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_17.png" id="A1.F11.g8" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2401.00974/assets/DS_18.png" id="A1.F11.g9" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="192" height="128" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Precision-Recall curve for DS 0.1. DS 0.1-augmented training datasets in general damages the Precision-Recall curve across all ML classifier.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="A1.p5" class="ltx_para">
<p id="A1.p5.1" class="ltx_p"></p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.00973" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.00974" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.00974">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.00974" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.00976" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 07:24:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
