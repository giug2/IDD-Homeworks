<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.06082] xGQA: Cross-Lingual Visual Question Answering</title><meta property="og:description" content="Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we addr…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="xGQA: Cross-Lingual Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="xGQA: Cross-Lingual Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.06082">

<!--Generated on Sat Mar  2 03:52:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">xGQA: Cross-Lingual Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id11.11.11" class="ltx_text ltx_font_bold">Jonas Pfeiffer<sup id="id11.11.11.1" class="ltx_sup"><span id="id11.11.11.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Gregor Geigle<sup id="id11.11.11.2" class="ltx_sup"><span id="id11.11.11.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Aishwarya Kamath<sup id="id11.11.11.3" class="ltx_sup"><span id="id11.11.11.3.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>, Jan-Martin O. Steitz<sup id="id11.11.11.4" class="ltx_sup"><span id="id11.11.11.4.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup> 
<br class="ltx_break">Stefan Roth<sup id="id11.11.11.5" class="ltx_sup"><span id="id11.11.11.5.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>, Ivan Vulić<sup id="id11.11.11.6" class="ltx_sup"><span id="id11.11.11.6.1" class="ltx_text ltx_font_medium ltx_font_italic">4</span></sup>, Iryna Gurevych<sup id="id11.11.11.7" class="ltx_sup"><span id="id11.11.11.7.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>  
<br class="ltx_break"><sup id="id11.11.11.8" class="ltx_sup"><span id="id11.11.11.8.1" class="ltx_text ltx_font_medium">1</span></sup>Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt 
<br class="ltx_break"><sup id="id11.11.11.9" class="ltx_sup"><span id="id11.11.11.9.1" class="ltx_text ltx_font_medium">2</span></sup>Center for Data Science, New York University 
<br class="ltx_break"><sup id="id11.11.11.10" class="ltx_sup"><span id="id11.11.11.10.1" class="ltx_text ltx_font_medium">3</span></sup>Visual Inference Lab,
Technical University of Darmstadt 
<br class="ltx_break"><sup id="id11.11.11.11" class="ltx_sup"><span id="id11.11.11.11.1" class="ltx_text ltx_font_medium">4</span></sup>Language Technology Lab, University of Cambridge    
<br class="ltx_break"></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.1" class="ltx_p">Recent advances in multimodal <span id="id12.1.1" class="ltx_text ltx_font_italic">vision and language</span> modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> to 7 typologically diverse languages,
enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based
approaches
to adapt multimodal transformer-based models to become multilingual, and—vice versa—multilingual models to become multimodal. Our proposed
methods outperform current state-of-the-art multilingual multimodal models (e.g., M<sup id="id12.1.2" class="ltx_sup">3</sup>P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task.
Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The xGQA dataset is available online at:
<a target="_blank" href="https://github.com/Adapter-Hub/xGQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Adapter-Hub/xGQA</a>.</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Transformer-based architectures <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib62" title="" class="ltx_ref">2017</a>)</cite> have become ubiquitous in NLP <cite class="ltx_cite ltx_citemacro_cite">(Devlin et al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Liu et al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Conneau et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>, <span id="S1.p1.1.1.1" class="ltx_text ltx_font_italic">inter alia</span>)</cite> and
in computer vision
(CV)
<cite class="ltx_cite ltx_citemacro_cite">Carion et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Dosovitskiy et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>, offering unmatched task performance. Having a shared architecture for multiple modalities opened up possibilities for effective fusion of information,
yielding impressive performance gains across various multimodal tasks such as image captioning, phrase grounding, visual question answering, referring expression comprehension and image-text retrieval <cite class="ltx_cite ltx_citemacro_cite">(Lu et al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>; Tan and Bansal, <a href="#bib.bib60" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib31" title="" class="ltx_ref">2020b</a>; Zhang et al., <a href="#bib.bib68" title="" class="ltx_ref">2021</a>; Ni et al., <a href="#bib.bib38" title="" class="ltx_ref">2021</a>; Kamath et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Miech et al., <a href="#bib.bib37" title="" class="ltx_ref">2021</a>; Frank et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>; Bugliarello et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; Radford et al., <a href="#bib.bib47" title="" class="ltx_ref">2021</a>; Jia et al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Eichenberg et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Singh et al., <a href="#bib.bib56" title="" class="ltx_ref">2021</a>; Fu et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>; Yang et al., <a href="#bib.bib66" title="" class="ltx_ref">2021</a>; Yuan et al., <a href="#bib.bib67" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib63" title="" class="ltx_ref">2021a</a>; Li et al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>; Geigle et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>, <span id="S1.p1.1.2.1" class="ltx_text ltx_font_italic">inter alia</span>)</cite>. Yet, progress in this area has been limited mostly to the English language, as the main multimodal datasets consist only of English text. Due to the scarcity of multilingual evaluation benchmarks, there has been limited development of models that tackle this joint problem.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2109.06082/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="438" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example taken from the xGQA dataset with the same question uttered in 8 languages.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Aiming to address this gap, in this paper we propose <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">xGQA</span>, a multilingual evaluation benchmark for the visual question answering task, extending the monolingual English-only GQA dataset <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>. For xGQA we manually translate and adapt the balanced GQA test-dev set into 7 new languages from 7 language families, covering 5 distinct scripts; see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S3.T1" title="Table 1 ‣ 3 xGQA ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> later. In addition, we provide new fixed data splits to guide cross-lingual few-shot learning experiments, where only a small number of examples in the target language are utilized.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As pretraining is (i) notoriously computationally expensive for high-resource languages and (ii) only limited amounts of multilingual multimodal resources are available, we also propose computationally efficient adapter-based <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> approaches as additional baselines for constructing multilingual multimodal models. In a nutshell, we extend multimodal models pretrained only on English text <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2021</a>)</cite> to become multilingual and—vice versa—multilingual models <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> to become multimodal. To this end, we follow the approaches of <cite class="ltx_cite ltx_citemacro_citet">Artetxe et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020b</a>, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> and extend monolingual and multilingual models to new languages and scripts via learning new tokenizers and corresponding word-embedding matrices, as well as adapters for the target languages. To transfer the respective multilingual multimodal adapter-based models to the target task, we propose a novel <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">modality-specific split architecture</span>, which uses modality dependent adapter weights (see Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Baselines ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for an illustration of the architecture).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our results clearly indicate that the proposed adapter-based architecture outperforms the recent state-of-the-art pretrained multilingual multimodal M<sup id="S1.p4.1.1" class="ltx_sup">3</sup>P model <cite class="ltx_cite ltx_citemacro_cite">Ni et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> in zero-shot cross-lingual settings. However, the overall performance of zero-shot transfer remains low across the board, with an average drop of around 38 accuracy points across target languages. Using a small number of target language examples in a few-shot setup considerably improves performance for all approaches, but cross-lingual transfer performance still lags substantially behind source language performance. This demonstrates the inherent difficulty of the task, even though the corresponding questions are arguably simple as they are template based and only contain 8.5 words on average (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Contributions.</span> <span id="S1.p5.1.2" class="ltx_text ltx_font_bold">1)</span> We propose the first evaluation benchmark for cross-lingual visual question answering, covering 7 diverse target languages; <span id="S1.p5.1.3" class="ltx_text ltx_font_bold">2)</span> we propose novel adapter-based approaches for the creation of multilingual multimodal models; <span id="S1.p5.1.4" class="ltx_text ltx_font_bold">3)</span> we systematically benchmark state-of-the-art and new multilingual multimodal models in zero-shot and few-shot learning setups, demonstrating the difficulty of the proposed task and serving as strong reference points for future work; <span id="S1.p5.1.5" class="ltx_text ltx_font_bold">4)</span> we provide a thorough analysis of the different approaches, highlighting the aspects and question types that lead to the most common model failures, again motivating future work in this domain.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Multilingual Language Models.</span>
Pretrained multilingual transformer-based LMs such as mBERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> and XLM-R <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> adopt the same pretraining regime as their respective monolingual counterparts: BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> and RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>. They are pretrained via self-supervised masked language modelling objective (MLM) on concatenated text corpora of more than 100 languages, where text is tokenized using WordPiece, SentencePiece or BytePair encodings. These multilingual models have been shown to work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision <cite class="ltx_cite ltx_citemacro_cite">(e.g., parallel data, translation dictionaries; Pires et al., <a href="#bib.bib44" title="" class="ltx_ref">2019</a>; Wu and Dredze, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>; Artetxe et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Hu et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; K et al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>; Rust et al., <a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Vision and Language Models.</span>
Most transformer-based multimodal models <cite class="ltx_cite ltx_citemacro_cite">(Lu et al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>; Tan and Bansal, <a href="#bib.bib60" title="" class="ltx_ref">2019</a>; Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib29" title="" class="ltx_ref">2020a</a>; Gan et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib31" title="" class="ltx_ref">2020b</a>; Bugliarello et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; Ni et al., <a href="#bib.bib38" title="" class="ltx_ref">2021</a>, <span id="S2.p2.1.2.1" class="ltx_text ltx_font_italic">inter alia</span>)</cite> jointly encode text tokens and image region features by preprocessing images using object detection models—such as Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib49" title="" class="ltx_ref">2015</a>)</cite>—to extract features for regions of interest (RoI) <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>.
The image region features are passed through an affine layer, which learns to project the region features to the joint embedding space of the multimodal transformer.
The bounding box coordinates of the RoI act as positional embeddings for the visual features. As such, they undergo an affine transformation to the embedding space and are combined with their respective image region representation. The position-aware image region embeddings get passed into the transformer.
The multi-head attention then attends over all text and image inputs at every layer, learning a joint representation of both modalities. On the other hand, <cite class="ltx_cite ltx_citemacro_citet">Kamath et al. (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> avoid using object detectors as a black-box for pre-extracting these region features and instead make it a central part of the multimodal transformer architecture. Training the object detector end-to-end with the multimodal transformer
adds
flexibility and better representation capacity.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Similar to MLM,
multimodal transformer-based models are trained with self-supervised objectives such as masked feature regression, masked object detection, masked attribute detection, and contrastive losses such as cross-modality matching <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib60" title="" class="ltx_ref">2019</a>)</cite>.
Typically, image captioning datasets are used for pretraining such as COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite>, Flickr30k <cite class="ltx_cite ltx_citemacro_cite">Plummer et al. (<a href="#bib.bib45" title="" class="ltx_ref">2015</a>)</cite>, Conceptual Captions (CC) <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a href="#bib.bib54" title="" class="ltx_ref">2018</a>)</cite>, and SBU <cite class="ltx_cite ltx_citemacro_cite">Ordonez et al. (<a href="#bib.bib39" title="" class="ltx_ref">2011</a>)</cite>. Similar to unimodal language models, the [CLS] token is used as a contextual representation for classification tasks.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.3" class="ltx_p">Multilingual multimodal models have also been proposed recently: M<sup id="S2.p4.3.1" class="ltx_sup">3</sup>P <cite class="ltx_cite ltx_citemacro_cite">Ni et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> is trained on the Wikipedias of 50 different languages and the English multimodal CC dataset. In order to align tokens of languages other than English with image representations, M<sup id="S2.p4.3.2" class="ltx_sup">3</sup>P utilizes a code-switching mechanism, where words of the English CC examples are randomly replaced with words from corresponding bilingual dictionaries.
In UC<sup id="S2.p4.3.3" class="ltx_sup">2</sup>, <cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a href="#bib.bib69" title="" class="ltx_ref">2021</a>)</cite> augment English multimodal datasets with other languages via machine translation and propose masked region-to-token modeling and visual translation language modeling.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The model weights of UC<sup id="footnote2.1" class="ltx_sup">2</sup> were not released by the time of experimentation.</span></span></span></p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Adapters</span> <cite class="ltx_cite ltx_citemacro_cite">Rebuffi et al. (<a href="#bib.bib48" title="" class="ltx_ref">2017</a>); Houlsby et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> have been introduced as a more efficient fine-tuning strategy for transfer learning in NLP and CV. Instead of fine-tuning all the weights of a pretrained model on the target task, small feed-forward layers are introduced at each layer of the pretrained model. During task fine-tuning, only the adapter weights are updated, while the pretrained parameters remain fixed/frozen.
Adapters have been shown to be very training efficient <cite class="ltx_cite ltx_citemacro_cite">Rücklé et al. (<a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>, and among an increasing amount of applications they can be utilized to transfer between domains <cite class="ltx_cite ltx_citemacro_cite">Rücklé et al. (<a href="#bib.bib51" title="" class="ltx_ref">2020</a>)</cite> and tasks <cite class="ltx_cite ltx_citemacro_cite">Poth et al. (<a href="#bib.bib46" title="" class="ltx_ref">2021</a>)</cite>, and in machine translation <cite class="ltx_cite ltx_citemacro_citep">(Bapna and Firat, <a href="#bib.bib4" title="" class="ltx_ref">2019</a>; Philip et al., <a href="#bib.bib43" title="" class="ltx_ref">2020</a>; Le et al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> and cross-lingual transfer <cite class="ltx_cite ltx_citemacro_cite">(Pfeiffer et al., <a href="#bib.bib41" title="" class="ltx_ref">2020b</a>, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>; Üstün et al., <a href="#bib.bib61" title="" class="ltx_ref">2020</a>; Ansell et al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>, <span id="S2.p5.1.2.1" class="ltx_text ltx_font_italic">inter alia</span>)</cite> scenarios.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Datasets.</span>
Pretraining and fine-tuning data for multilingual multimodal models is typically based on (multimodal information from) Wikipedia
<cite class="ltx_cite ltx_citemacro_cite">(<span id="S2.p6.1.2.1" class="ltx_text ltx_font_bold">WikiCaps</span>, <span id="S2.p6.1.3.2" class="ltx_text ltx_font_bold">WIT</span>, Schamoni et al., <a href="#bib.bib53" title="" class="ltx_ref">2018</a>; Srinivasan et al., <a href="#bib.bib57" title="" class="ltx_ref">2021</a>)</cite>, or on available downstream task data.
<span id="S2.p6.1.4" class="ltx_text ltx_font_bold">Multi30k</span> <cite class="ltx_cite ltx_citemacro_cite">Elliott et al. (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite> is a multilingual image captioning dataset for retrieval-type questions, covering English, German, French, and Czech;
<span id="S2.p6.1.5" class="ltx_text ltx_font_bold">GEM</span> <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite> covers image and video retrieval tasks across 20 and 30 different languages, respectively; <span id="S2.p6.1.6" class="ltx_text ltx_font_bold">HowTo100M</span> <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> is a multilingual and multimodal pretraining dataset for image and video retrieval;
<span id="S2.p6.1.7" class="ltx_text ltx_font_bold">MultiSubs</span> <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib64" title="" class="ltx_ref">2021b</a>)</cite> focuses on fill-in-the-blank tasks and lexical translation, covering English, Spanish, German, Portuguese, and French.
<cite class="ltx_cite ltx_citemacro_citet">Gao et al. (<a href="#bib.bib17" title="" class="ltx_ref">2015</a>); Shimizu et al. (<a href="#bib.bib55" title="" class="ltx_ref">2018</a>)</cite> propose bilingual visual question answering datasets for English, and Chinese and Japanese respectively.
In contemporary work <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> propose <span id="S2.p6.1.8" class="ltx_text ltx_font_bold">MaRVL</span>, a binary multilingual question answering dataset similar to NLVR2 <cite class="ltx_cite ltx_citemacro_cite">Suhr et al. (<a href="#bib.bib59" title="" class="ltx_ref">2019</a>)</cite>, spanning 5 typologically diverse languages (Chinese, Tamil, Swahili, Indonesian, and Turkish).</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Previous datasets predominantly focus on (arguably simpler) retrieval-type tasks, only cover a small set of similar languages (e.g., Multi30k, MultiSubs), or only cover binary questions. In contrast, we propose the first multilingual visual question answering dataset, which covers a typologically more diverse set of languages.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Most recently, <span id="S2.p8.1.1" class="ltx_text ltx_font_bold">IGLUE</span> <cite class="ltx_cite ltx_citemacro_cite">Bugliarello et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>—a multilingual multimodal benchmark that integrates xGQA—was proposed: IGLUE brings together visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>xGQA</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The original English GQA dataset <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> was constructed by leveraging Visual Genome scene graphs <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>. An English question engine that utilizes <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">content</span> (i.e. information about objects, attributes, and relations provided) and <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">structure</span> (a linguistic grammar that couples hundreds of structural patterns and detailed lexical semantic resources) was used to generate over 22 million diverse questions, which are visually grounded in the image scene graphs. As the questions are automatically generated using templates, they do not necessarily reflect the wide spectrum of natural language, making any assumptions on the performance in the wild difficult.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Each question is associated with additional metadata such as <span id="S3.p2.1.1" class="ltx_text ltx_font_bold">structural types</span>:
(1) <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">verify</span> for yes/no questions (e.g. "Do you see any cats?"), (2) <span id="S3.p2.1.3" class="ltx_text ltx_font_italic">query</span> for all open questions (e.g. "Who is wearing jeans?"), (3) <span id="S3.p2.1.4" class="ltx_text ltx_font_italic">choose</span> for questions that present two alternatives
to choose from (e.g. “Is it red or blue?”), (4) <span id="S3.p2.1.5" class="ltx_text ltx_font_italic">logical</span> which
involve logical inference (e.g. "Is the field soft and snowy"), and (5) <span id="S3.p2.1.6" class="ltx_text ltx_font_italic">compare</span> for comparison
questions between two or more objects (e.g. "Are all the animals zebras?"). For further details regarding the metadata, we refer the reader to <cite class="ltx_cite ltx_citemacro_citet">Hudson and Manning (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Dataset Design.</span> The principal objective when devising xGQA was to create a genuinely typologically diverse multimodal and multilingual evaluation benchmark for visual question answering. We utilize the balanced<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>To reduce biases in the conditional answer distribution <cite class="ltx_cite ltx_citemacro_citet">Hudson and Manning (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> utilize the structural metadata to downsample and create balanced datasets that are more robust against shortcuts and
guesses.</span></span></span> test-dev set of GQA, which consists of 12,578 questions about 398 images.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We chose to translate the test-dev set of GQA, as the labels for test-std are not released. </span></span></span> Due to the defined structural patterns, the formulation of the questions is simple, with an average length of 8.5 words.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>For this reason, we chose to hire university students that are currently conducting their (Computer Science or Computational Linguistics) studies in English and are all fluent English speakers to translate the question into their native language. They were paid above the minimum hourly wage of the country of their respective university. After all questions have been translated, another, independent native speaker then verified the translations based on random spot checks.</span></span></span> The resulting xGQA dataset covers translations in 7 languages, each representing a distinct language family, and contains examples written in 5 different scripts (see Table <a href="#S3.T1" title="Table 1 ‣ 3 xGQA ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:274.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.9pt,-56.3pt) scale(1.69492938668992,1.69492938668992) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Language</th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">iso</th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Family</th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Script</th>
<th id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Speakers</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">English</td>
<td id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">en</td>
<td id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">IE:Germanic</td>
<td id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">Latin</td>
<td id="S3.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">400M</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.1.3.2.1" class="ltx_td ltx_align_left">German</td>
<td id="S3.T1.1.1.3.2.2" class="ltx_td ltx_align_left">de</td>
<td id="S3.T1.1.1.3.2.3" class="ltx_td ltx_align_left">IE:Germanic</td>
<td id="S3.T1.1.1.3.2.4" class="ltx_td ltx_align_left">Latin</td>
<td id="S3.T1.1.1.3.2.5" class="ltx_td ltx_align_center">95M</td>
</tr>
<tr id="S3.T1.1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.1.4.3.1" class="ltx_td ltx_align_left">Portuguese</td>
<td id="S3.T1.1.1.4.3.2" class="ltx_td ltx_align_left">pt</td>
<td id="S3.T1.1.1.4.3.3" class="ltx_td ltx_align_left">IE:Romance</td>
<td id="S3.T1.1.1.4.3.4" class="ltx_td ltx_align_left">Latin</td>
<td id="S3.T1.1.1.4.3.5" class="ltx_td ltx_align_center">250M</td>
</tr>
<tr id="S3.T1.1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.1.5.4.1" class="ltx_td ltx_align_left">Russian</td>
<td id="S3.T1.1.1.5.4.2" class="ltx_td ltx_align_left">ru</td>
<td id="S3.T1.1.1.5.4.3" class="ltx_td ltx_align_left">IE:Slavic</td>
<td id="S3.T1.1.1.5.4.4" class="ltx_td ltx_align_left">Cyrillic</td>
<td id="S3.T1.1.1.5.4.5" class="ltx_td ltx_align_center">150M</td>
</tr>
<tr id="S3.T1.1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.1.6.5.1" class="ltx_td ltx_align_left">Indonesian</td>
<td id="S3.T1.1.1.6.5.2" class="ltx_td ltx_align_left">id</td>
<td id="S3.T1.1.1.6.5.3" class="ltx_td ltx_align_left">Austronesian</td>
<td id="S3.T1.1.1.6.5.4" class="ltx_td ltx_align_left">Latin</td>
<td id="S3.T1.1.1.6.5.5" class="ltx_td ltx_align_center">43M</td>
</tr>
<tr id="S3.T1.1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.1.7.6.1" class="ltx_td ltx_align_left">Bengali</td>
<td id="S3.T1.1.1.7.6.2" class="ltx_td ltx_align_left">bn</td>
<td id="S3.T1.1.1.7.6.3" class="ltx_td ltx_align_left">IE:Iranian</td>
<td id="S3.T1.1.1.7.6.4" class="ltx_td ltx_align_left">Bengali</td>
<td id="S3.T1.1.1.7.6.5" class="ltx_td ltx_align_center">230M</td>
</tr>
<tr id="S3.T1.1.1.8.7" class="ltx_tr">
<td id="S3.T1.1.1.8.7.1" class="ltx_td ltx_align_left">Korean</td>
<td id="S3.T1.1.1.8.7.2" class="ltx_td ltx_align_left">ko</td>
<td id="S3.T1.1.1.8.7.3" class="ltx_td ltx_align_left">Koreanic</td>
<td id="S3.T1.1.1.8.7.4" class="ltx_td ltx_align_left">Korean</td>
<td id="S3.T1.1.1.8.7.5" class="ltx_td ltx_align_center">77M</td>
</tr>
<tr id="S3.T1.1.1.9.8" class="ltx_tr">
<td id="S3.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_bb">Chinese</td>
<td id="S3.T1.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_bb">zh</td>
<td id="S3.T1.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_bb">Sino-Tibetan</td>
<td id="S3.T1.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_bb">Chinese</td>
<td id="S3.T1.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb">1.2B</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Languages covered by xGQA. IE stands for Indo-European.</figcaption>
</figure>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Few-Shot Data Splits.</span> In order to conduct cross-lingual few-shot learning experiments, we provide new data splits of different sizes. We split on images and add all questions associated with the image to the respective set. The development and test sets consist of 50 and 300 images, respectively. The training splits consist of 1, 5, 10, 20, 25, and 48 images, see Table <a href="#S3.T2" title="Table 2 ‣ 3 xGQA ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We ensure that the distribution of structural types within each set is maintained.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">xGQA is the first truly typologically diverse multilingual multimodal benchmark, unlocking new experimentation and analysis opportunities in cross-lingual zero-shot and few-shot scenarios. While the questions in xGQA are intuitive and easy for humans to solve, we later show that current state-of-the-art models still have difficulty with transfer.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:83.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(76.1pt,-14.6pt) scale(1.54099659071887,1.54099659071887) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Set</th>
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Test</th>
<th id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Dev</th>
<th id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6">Train</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<th id="S3.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#Img</th>
<td id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">300</td>
<td id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">50</td>
<td id="S3.T2.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">1</td>
<td id="S3.T2.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">5</td>
<td id="S3.T2.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t">10</td>
<td id="S3.T2.1.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">20</td>
<td id="S3.T2.1.1.2.1.8" class="ltx_td ltx_align_right ltx_border_t">25</td>
<td id="S3.T2.1.1.2.1.9" class="ltx_td ltx_align_right ltx_border_t">48</td>
</tr>
<tr id="S3.T2.1.1.3.2" class="ltx_tr">
<th id="S3.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">#Ques</th>
<td id="S3.T2.1.1.3.2.2" class="ltx_td ltx_align_right ltx_border_bb">9666</td>
<td id="S3.T2.1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_bb">1422</td>
<td id="S3.T2.1.1.3.2.4" class="ltx_td ltx_align_right ltx_border_bb">27</td>
<td id="S3.T2.1.1.3.2.5" class="ltx_td ltx_align_right ltx_border_bb">155</td>
<td id="S3.T2.1.1.3.2.6" class="ltx_td ltx_align_right ltx_border_bb">317</td>
<td id="S3.T2.1.1.3.2.7" class="ltx_td ltx_align_right ltx_border_bb">594</td>
<td id="S3.T2.1.1.3.2.8" class="ltx_td ltx_align_right ltx_border_bb">704</td>
<td id="S3.T2.1.1.3.2.9" class="ltx_td ltx_align_right ltx_border_bb">1490</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Few-shot dataset sizes. The GQA test-dev set is split into new development, test sets, and training splits of different sizes. We maintain the distribution of structural types in each split.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Baselines</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To analyze the performance and current gaps on xGQA, we first evaluate the recently proposed M<sup id="S4.p1.1.1" class="ltx_sup">3</sup>P model, which has been pretrained on multilingual and multimodal data. However, pretraining is computationally expensive and only limited amounts of multilingual multimodal resources are available. Therefore, we further propose new and more efficient approaches that (1) extend state-of-the-art multilingual language models to the multimodal domain and (2) provide multilingual capabilities to state-of-the-art multimodal models.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Unless noted otherwise, we follow the predominant fine-tuning strategy for GQA; a prediction head is placed on top of the output of a pretrained transformer. All possible 1853 answers of the GQA task are mapped to a class label. The question associated with an image together with the position-aware region features are passed as input to the transformer, supervised using a cross-entropy loss.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>For instance, we use this strategy to fine-tune all parameters of M<sup id="footnote6.1" class="ltx_sup">3</sup>P on the GQA training data.</span></span></span></p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2109.06082/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="345" height="328" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of an adapter-based multilingual multimodal model. Text and image inputs share the weights of the multi-head attention (MHA) and feed-forward (FFN) layers, as well as the <span id="S4.F2.4.1" class="ltx_text ltx_font_italic">language</span> and <span id="S4.F2.5.2" class="ltx_text ltx_font_italic">multimodal align</span> adapters. Each modality is passed through a modality specific <span id="S4.F2.6.3" class="ltx_text ltx_font_italic">task</span> adapter, the outputs of which are concatenated. </figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Multimodal <math id="S4.SS1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.1.m1.1b"><mo stretchy="false" id="S4.SS1.1.m1.1.1" xref="S4.SS1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.1.m1.1c"><ci id="S4.SS1.1.m1.1.1.cmml" xref="S4.SS1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.1.m1.1d">\rightarrow</annotation></semantics></math> Multilingual</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">OSCAR+<sup id="S4.SS1.p1.1.1.1" class="ltx_sup"><span id="S4.SS1.p1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Emb</span></sup>.</span> To extend a monolingual transformer LM to a multilingual domain, <cite class="ltx_cite ltx_citemacro_citet">Artetxe et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> fine-tune a new word-embedding layer in the target language.
Inspired by this idea, we now describe how we extend the current state-of-the-art monolingual multimodal transformer model <span id="S4.SS1.p1.1.2" class="ltx_text">OSCAR+</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2021</a>)</cite> to learn new embeddings for the target languages.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In the <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">language-extension</span> phase, we replace the embedding matrix of OSCAR+ with a randomly initialized embedding matrix.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Following <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>, we copy the embeddings of lexically overlapping tokens (if such tokens exist) from the original embedding space to the new embedding space, as it typically works better than fully random initialization.</span></span></span> The transformer weights are frozen while only the newly introduced embeddings are fine-tuned on unlabeled text data of the target language with the MLM objective.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In the <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">target-task</span> phase, the original OSCAR+ model is fine-tuned on the English training data of GQA, where the transformer layers are fine-tuned, but the embedding layer is frozen. During inference, the embedding layer is replaced with the target language’s embedding layer.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">OSCAR+<sup id="S4.SS1.p4.1.1.1" class="ltx_sup"><span id="S4.SS1.p4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Ada</span></sup>.</span> We extend this by adding adapters.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">In the <span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_italic">language-extension</span> phase we follow <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> in order to extend the model to the target languages. Similar to OSCAR+<sup id="S4.SS1.p5.1.2" class="ltx_sup"><span id="S4.SS1.p5.1.2.1" class="ltx_text ltx_font_italic">Emb</span></sup>, we train a new embedding layer. We further add <span id="S4.SS1.p5.1.3" class="ltx_text ltx_font_italic">language</span> adapters at every transformer layer.
Given that OSCAR+ is trained on English text, we follow <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020b</a>)</cite> when training English <span id="S4.SS1.p5.1.4" class="ltx_text ltx_font_italic">language</span> adapter modules, without replacing the embedding matrix. The transformer weights are frozen while only the <span id="S4.SS1.p5.1.5" class="ltx_text ltx_font_italic">newly</span> introduced embeddings and <span id="S4.SS1.p5.1.6" class="ltx_text ltx_font_italic">language</span> adapter weights are fine-tuned on unlabeled text data of the language.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">For the <span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_italic">target-task</span> phase, we propose a novel modality-split architecture (see Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Baselines ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) inspired by the cross-lingual transfer method of <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020b</a>)</cite>. At each transformer layer, text and image representations are passed through the pretrained multi-head attention (MHA) and feed-forward (FFN) layers. Both image and text representations are also passed through the pre-trained <span id="S4.SS1.p6.1.2" class="ltx_text ltx_font_italic">language</span> adapters. Each modality is then passed through modality-specific <span id="S4.SS1.p6.1.3" class="ltx_text ltx_font_italic">text</span> and <span id="S4.SS1.p6.1.4" class="ltx_text ltx_font_italic">image</span> <span id="S4.SS1.p6.1.5" class="ltx_text ltx_font_italic">task</span> adapters and next through a shared <span id="S4.SS1.p6.1.6" class="ltx_text ltx_font_italic">multimodal alignment</span> adapter.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We have compared multiple different architectures as illustrated in Figure <a href="#A1.F6" title="Figure 6 ‣ Appendix A Appendix ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> in the Appendix, finding this setup to perform best. We present results of the alternative architectures also in the Appendix.</span></span></span>
We follow <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020b</a>)</cite>, freezing transformer, embedding and <span id="S4.SS1.p6.1.7" class="ltx_text ltx_font_italic">language</span> adapter weights during training, thus fine-tuning only the <span id="S4.SS1.p6.1.8" class="ltx_text ltx_font_italic">task</span> and <span id="S4.SS1.p6.1.9" class="ltx_text ltx_font_italic">multimodal aligner</span> adapter weights, together with the prediction head. At inference time, the embedding layer and the <span id="S4.SS1.p6.1.10" class="ltx_text ltx_font_italic">language</span> adapters are replaced with the target language weights.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multilingual <math id="S4.SS2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.1.m1.1b"><mo stretchy="false" id="S4.SS2.1.m1.1.1" xref="S4.SS2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.1.m1.1c"><ci id="S4.SS2.1.m1.1.1.cmml" xref="S4.SS2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.1.m1.1d">\rightarrow</annotation></semantics></math> Multimodal</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">mBERT<sup id="S4.SS2.p1.1.1.1" class="ltx_sup"><span id="S4.SS2.p1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Ada</span></sup>.</span>
For experiments where we extend a multilingual model to become multimodal, we utilize mBERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Given that mBERT is able to represent many different languages, it is not necessary to learn new embedding layers for the target languages in the <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">language-extension</span> phase. Instead, we utilize the mBERT-compatible <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">language</span> adapters available on <a target="_blank" href="https://AdapterHub.ml" title="" class="ltx_ref ltx_href">AdapterHub.ml</a> <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib40" title="" class="ltx_ref">2020a</a>)</cite>.<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>While all xGQA languages already have readily available language adapters on AdapterHub, any hypothetical extension of experiments to languages without such adapters would involve training their dedicated language adapters, e.g., following the procedure of <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020b</a>)</cite>.</span></span></span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">For the <span id="S4.SS2.p3.2.1" class="ltx_text ltx_font_italic">target-task</span> phase, we follow OSCAR+ for the image representation layer, where image features are combined with their respective positional information and passed through an affine transformation layer. We experiment with the same adapter architecture from Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Baselines ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, as described for OSCAR+<sup id="S4.SS2.p3.2.2" class="ltx_sup"><span id="S4.SS2.p3.2.2.1" class="ltx_text ltx_font_italic">Ada</span></sup>. We again freeze transformer, embedding and <span id="S4.SS2.p3.2.3" class="ltx_text ltx_font_italic">language</span> adapter weights during training. However, in contrast to OSCAR+<sup id="S4.SS2.p3.2.4" class="ltx_sup"><span id="S4.SS2.p3.2.4.1" class="ltx_text ltx_font_italic">∗</span></sup>, we randomly initialize and fine-tune the affine image transformation layer. We also fine-tune the <span id="S4.SS2.p3.2.5" class="ltx_text ltx_font_italic">task</span>, <span id="S4.SS2.p3.2.6" class="ltx_text ltx_font_italic">multimodal aligner</span> adapter weights, and prediction head, all on the GQA task. At inference time, the embedding layer and the <span id="S4.SS2.p3.2.7" class="ltx_text ltx_font_italic">language</span> adapters are replaced with the corresponding target language weights.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Language-Extension Phase</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.5" class="ltx_p">For OSCAR+<sup id="S5.SS1.p1.5.1" class="ltx_sup"><span id="S5.SS1.p1.5.1.1" class="ltx_text ltx_font_italic">Emb</span></sup> and OSCAR+<sup id="S5.SS1.p1.5.2" class="ltx_sup"><span id="S5.SS1.p1.5.2.1" class="ltx_text ltx_font_italic">Ada</span></sup>, we follow the general setups proposed by <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020b</a>, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>. We train a new word-piece tokenizer for each target language with a vocabulary size of 30k. We fine-tune the randomly initialized embedding layer, and (for OSCAR+<sup id="S5.SS1.p1.5.3" class="ltx_sup"><span id="S5.SS1.p1.5.3.1" class="ltx_text ltx_font_italic">Ada</span></sup>) adapter layers for 100k update steps with a batch size of 64 and a learning rate of <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="1\mathrm{e}{-4}" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><mrow id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mrow id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml"><mn id="S5.SS1.p1.4.m4.1.1.2.2" xref="S5.SS1.p1.4.m4.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p1.4.m4.1.1.2.1" xref="S5.SS1.p1.4.m4.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p1.4.m4.1.1.2.3" xref="S5.SS1.p1.4.m4.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS1.p1.4.m4.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.cmml">−</mo><mn id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><minus id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1"></minus><apply id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2"><times id="S5.SS1.p1.4.m4.1.1.2.1.cmml" xref="S5.SS1.p1.4.m4.1.1.2.1"></times><cn type="integer" id="S5.SS1.p1.4.m4.1.1.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2.2">1</cn><ci id="S5.SS1.p1.4.m4.1.1.2.3.cmml" xref="S5.SS1.p1.4.m4.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">1\mathrm{e}{-4}</annotation></semantics></math>. For mBERT<sup id="S5.SS1.p1.5.4" class="ltx_sup"><span id="S5.SS1.p1.5.4.1" class="ltx_text ltx_font_italic">Ada</span></sup>, we utilize the language adapters from <a target="_blank" href="https://AdapterHub.ml" title="" class="ltx_ref ltx_href">AdapterHub.ml</a>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Fine-tuning on GQA</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We follow the standard setup proposed by <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib31" title="" class="ltx_ref">2020b</a>)</cite>, passing the representation of the [CLS] token through a prediction head. We fine-tune the respective models using a cross-entropy loss with labels being all possible answers in the GQA dataset. Following prior work <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib31" title="" class="ltx_ref">2020b</a>)</cite>, we use a batch size of 192 and train for 5 epochs on the unbalanced GQA training portion.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.2" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">M<sup id="S5.SS2.p2.1.1.1" class="ltx_sup"><span id="S5.SS2.p2.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup>P.</span> We fine-tune all weights of the pretrained model with a learning rate of <math id="S5.SS2.p2.2.m1.1" class="ltx_Math" alttext="3\mathrm{e}{-5}" display="inline"><semantics id="S5.SS2.p2.2.m1.1a"><mrow id="S5.SS2.p2.2.m1.1.1" xref="S5.SS2.p2.2.m1.1.1.cmml"><mrow id="S5.SS2.p2.2.m1.1.1.2" xref="S5.SS2.p2.2.m1.1.1.2.cmml"><mn id="S5.SS2.p2.2.m1.1.1.2.2" xref="S5.SS2.p2.2.m1.1.1.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S5.SS2.p2.2.m1.1.1.2.1" xref="S5.SS2.p2.2.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS2.p2.2.m1.1.1.2.3" xref="S5.SS2.p2.2.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS2.p2.2.m1.1.1.1" xref="S5.SS2.p2.2.m1.1.1.1.cmml">−</mo><mn id="S5.SS2.p2.2.m1.1.1.3" xref="S5.SS2.p2.2.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m1.1b"><apply id="S5.SS2.p2.2.m1.1.1.cmml" xref="S5.SS2.p2.2.m1.1.1"><minus id="S5.SS2.p2.2.m1.1.1.1.cmml" xref="S5.SS2.p2.2.m1.1.1.1"></minus><apply id="S5.SS2.p2.2.m1.1.1.2.cmml" xref="S5.SS2.p2.2.m1.1.1.2"><times id="S5.SS2.p2.2.m1.1.1.2.1.cmml" xref="S5.SS2.p2.2.m1.1.1.2.1"></times><cn type="integer" id="S5.SS2.p2.2.m1.1.1.2.2.cmml" xref="S5.SS2.p2.2.m1.1.1.2.2">3</cn><ci id="S5.SS2.p2.2.m1.1.1.2.3.cmml" xref="S5.SS2.p2.2.m1.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS2.p2.2.m1.1.1.3.cmml" xref="S5.SS2.p2.2.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m1.1c">3\mathrm{e}{-5}</annotation></semantics></math>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.35" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:66pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.1pt,12.4pt) scale(0.725228838647294,0.725228838647294) ;">
<table id="S5.T3.35.35" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.35.35.36.1" class="ltx_tr">
<th id="S5.T3.35.35.36.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">model</th>
<th id="S5.T3.35.35.36.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">en</th>
<th id="S5.T3.35.35.36.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">de</th>
<th id="S5.T3.35.35.36.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">pt</th>
<th id="S5.T3.35.35.36.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">ru</th>
<th id="S5.T3.35.35.36.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">id</th>
<th id="S5.T3.35.35.36.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">bn</th>
<th id="S5.T3.35.35.36.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">ko</th>
<th id="S5.T3.35.35.36.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">zh</th>
<th id="S5.T3.35.35.36.1.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.8.8.8" class="ltx_tr">
<td id="S5.T3.8.8.8.9" class="ltx_td ltx_align_left ltx_border_t">M3P</td>
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t">58.43  <math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.1.1.1.1.1" class="ltx_text" style="color:#333333;">1.4</span>
</td>
<td id="S5.T3.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">23.93  <math id="S5.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.2.2.2.2.m1.1a"><mo mathcolor="#333333" id="S5.T3.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T3.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.2.2.2.2.1" class="ltx_text" style="color:#333333;">3.2</span>
</td>
<td id="S5.T3.3.3.3.3" class="ltx_td ltx_align_left ltx_border_t">24.37  <math id="S5.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.3.3.3.3.m1.1a"><mo mathcolor="#333333" id="S5.T3.3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T3.3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.3.3.3.3.1" class="ltx_text" style="color:#333333;">4.0</span>
</td>
<td id="S5.T3.4.4.4.4" class="ltx_td ltx_align_left ltx_border_t">20.37  <math id="S5.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.4.4.4.4.m1.1a"><mo mathcolor="#333333" id="S5.T3.4.4.4.4.m1.1.1" xref="S5.T3.4.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.m1.1b"><csymbol cd="latexml" id="S5.T3.4.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.4.4.4.4.1" class="ltx_text" style="color:#333333;">3.4</span>
</td>
<td id="S5.T3.5.5.5.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T3.5.5.5.5.1" class="ltx_text ltx_font_bold">22.57  <math id="S5.T3.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.5.5.5.5.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.5.5.5.5.1.m1.1.1" xref="S5.T3.5.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T3.5.5.5.5.1.m1.1.1.cmml" xref="S5.T3.5.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.5.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.5.5.5.5.1.1" class="ltx_text" style="color:#333333;">6.1</span></span></td>
<td id="S5.T3.6.6.6.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T3.6.6.6.6.1" class="ltx_text ltx_font_bold">15.83  <math id="S5.T3.6.6.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.6.6.6.6.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.6.6.6.6.1.m1.1.1" xref="S5.T3.6.6.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.6.1.m1.1b"><csymbol cd="latexml" id="S5.T3.6.6.6.6.1.m1.1.1.cmml" xref="S5.T3.6.6.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.6.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.6.6.6.6.1.1" class="ltx_text" style="color:#333333;">3.6</span></span></td>
<td id="S5.T3.7.7.7.7" class="ltx_td ltx_align_left ltx_border_t">16.90  <math id="S5.T3.7.7.7.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.7.7.7.7.m1.1a"><mo mathcolor="#333333" id="S5.T3.7.7.7.7.m1.1.1" xref="S5.T3.7.7.7.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.7.7.m1.1b"><csymbol cd="latexml" id="S5.T3.7.7.7.7.m1.1.1.cmml" xref="S5.T3.7.7.7.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.7.7.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.7.7.7.7.1" class="ltx_text" style="color:#333333;">3.8</span>
</td>
<td id="S5.T3.8.8.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">18.60  <math id="S5.T3.8.8.8.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.8.8.8.8.m1.1a"><mo mathcolor="#333333" id="S5.T3.8.8.8.8.m1.1.1" xref="S5.T3.8.8.8.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.8.m1.1b"><csymbol cd="latexml" id="S5.T3.8.8.8.8.m1.1.1.cmml" xref="S5.T3.8.8.8.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.8.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.8.8.8.8.1" class="ltx_text" style="color:#333333;">1.0</span>
</td>
<td id="S5.T3.8.8.8.10" class="ltx_td ltx_align_left ltx_border_t">20.37</td>
</tr>
<tr id="S5.T3.17.17.17" class="ltx_tr">
<td id="S5.T3.9.9.9.1" class="ltx_td ltx_align_left">OSCAR+<math id="S5.T3.9.9.9.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S5.T3.9.9.9.1.m1.1a"><msup id="S5.T3.9.9.9.1.m1.1.1" xref="S5.T3.9.9.9.1.m1.1.1.cmml"><mi id="S5.T3.9.9.9.1.m1.1.1a" xref="S5.T3.9.9.9.1.m1.1.1.cmml"></mi><mtext id="S5.T3.9.9.9.1.m1.1.1.1" xref="S5.T3.9.9.9.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.9.1.m1.1b"><apply id="S5.T3.9.9.9.1.m1.1.1.cmml" xref="S5.T3.9.9.9.1.m1.1.1"><ci id="S5.T3.9.9.9.1.m1.1.1.1a.cmml" xref="S5.T3.9.9.9.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.9.9.9.1.m1.1.1.1.cmml" xref="S5.T3.9.9.9.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.9.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</td>
<td id="S5.T3.10.10.10.2" class="ltx_td ltx_align_left"><span id="S5.T3.10.10.10.2.1" class="ltx_text ltx_font_bold">62.23  <math id="S5.T3.10.10.10.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.10.10.10.2.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.10.10.10.2.1.m1.1.1" xref="S5.T3.10.10.10.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.10.2.1.m1.1b"><csymbol cd="latexml" id="S5.T3.10.10.10.2.1.m1.1.1.cmml" xref="S5.T3.10.10.10.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.10.2.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.10.10.10.2.1.1" class="ltx_text" style="color:#333333;">0.3</span></span></td>
<td id="S5.T3.11.11.11.3" class="ltx_td ltx_align_left">17.35  <math id="S5.T3.11.11.11.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.11.11.11.3.m1.1a"><mo mathcolor="#333333" id="S5.T3.11.11.11.3.m1.1.1" xref="S5.T3.11.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.11.11.11.3.m1.1b"><csymbol cd="latexml" id="S5.T3.11.11.11.3.m1.1.1.cmml" xref="S5.T3.11.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.11.11.3.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.11.11.11.3.1" class="ltx_text" style="color:#333333;">1.0</span>
</td>
<td id="S5.T3.12.12.12.4" class="ltx_td ltx_align_left">19.25  <math id="S5.T3.12.12.12.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.12.12.12.4.m1.1a"><mo mathcolor="#333333" id="S5.T3.12.12.12.4.m1.1.1" xref="S5.T3.12.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.12.4.m1.1b"><csymbol cd="latexml" id="S5.T3.12.12.12.4.m1.1.1.cmml" xref="S5.T3.12.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.12.4.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.12.12.12.4.1" class="ltx_text" style="color:#333333;">0.4</span>
</td>
<td id="S5.T3.13.13.13.5" class="ltx_td ltx_align_left">10.52  <math id="S5.T3.13.13.13.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.13.13.13.5.m1.1a"><mo mathcolor="#333333" id="S5.T3.13.13.13.5.m1.1.1" xref="S5.T3.13.13.13.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.13.13.13.5.m1.1b"><csymbol cd="latexml" id="S5.T3.13.13.13.5.m1.1.1.cmml" xref="S5.T3.13.13.13.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.13.13.13.5.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.13.13.13.5.1" class="ltx_text" style="color:#333333;">4.0</span>
</td>
<td id="S5.T3.14.14.14.6" class="ltx_td ltx_align_left">18.26  <math id="S5.T3.14.14.14.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.14.14.14.6.m1.1a"><mo mathcolor="#333333" id="S5.T3.14.14.14.6.m1.1.1" xref="S5.T3.14.14.14.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.14.14.14.6.m1.1b"><csymbol cd="latexml" id="S5.T3.14.14.14.6.m1.1.1.cmml" xref="S5.T3.14.14.14.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.14.14.14.6.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.14.14.14.6.1" class="ltx_text" style="color:#333333;">0.4</span>
</td>
<td id="S5.T3.15.15.15.7" class="ltx_td ltx_align_left">14.93  <math id="S5.T3.15.15.15.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.15.15.15.7.m1.1a"><mo mathcolor="#333333" id="S5.T3.15.15.15.7.m1.1.1" xref="S5.T3.15.15.15.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.15.15.15.7.m1.1b"><csymbol cd="latexml" id="S5.T3.15.15.15.7.m1.1.1.cmml" xref="S5.T3.15.15.15.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.15.15.15.7.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.15.15.15.7.1" class="ltx_text" style="color:#333333;">2.0</span>
</td>
<td id="S5.T3.16.16.16.8" class="ltx_td ltx_align_left">17.10  <math id="S5.T3.16.16.16.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.16.16.16.8.m1.1a"><mo mathcolor="#333333" id="S5.T3.16.16.16.8.m1.1.1" xref="S5.T3.16.16.16.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.16.16.16.8.m1.1b"><csymbol cd="latexml" id="S5.T3.16.16.16.8.m1.1.1.cmml" xref="S5.T3.16.16.16.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.16.16.16.8.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.16.16.16.8.1" class="ltx_text" style="color:#333333;">1.8</span>
</td>
<td id="S5.T3.17.17.17.9" class="ltx_td ltx_align_left ltx_border_r">16.41  <math id="S5.T3.17.17.17.9.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.17.17.17.9.m1.1a"><mo mathcolor="#333333" id="S5.T3.17.17.17.9.m1.1.1" xref="S5.T3.17.17.17.9.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.17.17.17.9.m1.1b"><csymbol cd="latexml" id="S5.T3.17.17.17.9.m1.1.1.cmml" xref="S5.T3.17.17.17.9.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.17.17.17.9.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.17.17.17.9.1" class="ltx_text" style="color:#333333;">3.2</span>
</td>
<td id="S5.T3.17.17.17.10" class="ltx_td ltx_align_left">16.26</td>
</tr>
<tr id="S5.T3.26.26.26" class="ltx_tr">
<td id="S5.T3.18.18.18.1" class="ltx_td ltx_align_left">OSCAR+<math id="S5.T3.18.18.18.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S5.T3.18.18.18.1.m1.1a"><msup id="S5.T3.18.18.18.1.m1.1.1" xref="S5.T3.18.18.18.1.m1.1.1.cmml"><mi id="S5.T3.18.18.18.1.m1.1.1a" xref="S5.T3.18.18.18.1.m1.1.1.cmml"></mi><mtext id="S5.T3.18.18.18.1.m1.1.1.1" xref="S5.T3.18.18.18.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S5.T3.18.18.18.1.m1.1b"><apply id="S5.T3.18.18.18.1.m1.1.1.cmml" xref="S5.T3.18.18.18.1.m1.1.1"><ci id="S5.T3.18.18.18.1.m1.1.1.1a.cmml" xref="S5.T3.18.18.18.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.18.18.18.1.m1.1.1.1.cmml" xref="S5.T3.18.18.18.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.18.18.18.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</td>
<td id="S5.T3.19.19.19.2" class="ltx_td ltx_align_left">60.30  <math id="S5.T3.19.19.19.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.19.19.19.2.m1.1a"><mo mathcolor="#333333" id="S5.T3.19.19.19.2.m1.1.1" xref="S5.T3.19.19.19.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.19.19.19.2.m1.1b"><csymbol cd="latexml" id="S5.T3.19.19.19.2.m1.1.1.cmml" xref="S5.T3.19.19.19.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.19.19.19.2.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.19.19.19.2.1" class="ltx_text" style="color:#333333;">0.4</span>
</td>
<td id="S5.T3.20.20.20.3" class="ltx_td ltx_align_left">18.91  <math id="S5.T3.20.20.20.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.20.20.20.3.m1.1a"><mo mathcolor="#333333" id="S5.T3.20.20.20.3.m1.1.1" xref="S5.T3.20.20.20.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.20.20.20.3.m1.1b"><csymbol cd="latexml" id="S5.T3.20.20.20.3.m1.1.1.cmml" xref="S5.T3.20.20.20.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.20.20.20.3.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.20.20.20.3.1" class="ltx_text" style="color:#333333;">0.8</span>
</td>
<td id="S5.T3.21.21.21.4" class="ltx_td ltx_align_left">27.02  <math id="S5.T3.21.21.21.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.21.21.21.4.m1.1a"><mo mathcolor="#333333" id="S5.T3.21.21.21.4.m1.1.1" xref="S5.T3.21.21.21.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.21.21.21.4.m1.1b"><csymbol cd="latexml" id="S5.T3.21.21.21.4.m1.1.1.cmml" xref="S5.T3.21.21.21.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.21.21.21.4.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.21.21.21.4.1" class="ltx_text" style="color:#333333;">2.3</span>
</td>
<td id="S5.T3.22.22.22.5" class="ltx_td ltx_align_left">17.50  <math id="S5.T3.22.22.22.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.22.22.22.5.m1.1a"><mo mathcolor="#333333" id="S5.T3.22.22.22.5.m1.1.1" xref="S5.T3.22.22.22.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.22.22.22.5.m1.1b"><csymbol cd="latexml" id="S5.T3.22.22.22.5.m1.1.1.cmml" xref="S5.T3.22.22.22.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.22.22.22.5.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.22.22.22.5.1" class="ltx_text" style="color:#333333;">1.2</span>
</td>
<td id="S5.T3.23.23.23.6" class="ltx_td ltx_align_left">18.77  <math id="S5.T3.23.23.23.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.23.23.23.6.m1.1a"><mo mathcolor="#333333" id="S5.T3.23.23.23.6.m1.1.1" xref="S5.T3.23.23.23.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.23.23.23.6.m1.1b"><csymbol cd="latexml" id="S5.T3.23.23.23.6.m1.1.1.cmml" xref="S5.T3.23.23.23.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.23.23.23.6.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.23.23.23.6.1" class="ltx_text" style="color:#333333;">0.3</span>
</td>
<td id="S5.T3.24.24.24.7" class="ltx_td ltx_align_left">15.42  <math id="S5.T3.24.24.24.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.24.24.24.7.m1.1a"><mo mathcolor="#333333" id="S5.T3.24.24.24.7.m1.1.1" xref="S5.T3.24.24.24.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.24.24.24.7.m1.1b"><csymbol cd="latexml" id="S5.T3.24.24.24.7.m1.1.1.cmml" xref="S5.T3.24.24.24.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.24.24.24.7.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.24.24.24.7.1" class="ltx_text" style="color:#333333;">2.0</span>
</td>
<td id="S5.T3.25.25.25.8" class="ltx_td ltx_align_left">15.28  <math id="S5.T3.25.25.25.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.25.25.25.8.m1.1a"><mo mathcolor="#333333" id="S5.T3.25.25.25.8.m1.1.1" xref="S5.T3.25.25.25.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.25.25.25.8.m1.1b"><csymbol cd="latexml" id="S5.T3.25.25.25.8.m1.1.1.cmml" xref="S5.T3.25.25.25.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.25.25.25.8.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.25.25.25.8.1" class="ltx_text" style="color:#333333;">2.7</span>
</td>
<td id="S5.T3.26.26.26.9" class="ltx_td ltx_align_left ltx_border_r">14.96  <math id="S5.T3.26.26.26.9.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.26.26.26.9.m1.1a"><mo mathcolor="#333333" id="S5.T3.26.26.26.9.m1.1.1" xref="S5.T3.26.26.26.9.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.26.26.26.9.m1.1b"><csymbol cd="latexml" id="S5.T3.26.26.26.9.m1.1.1.cmml" xref="S5.T3.26.26.26.9.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.26.26.26.9.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.26.26.26.9.1" class="ltx_text" style="color:#333333;">2.1</span>
</td>
<td id="S5.T3.26.26.26.10" class="ltx_td ltx_align_left">18.27</td>
</tr>
<tr id="S5.T3.35.35.35" class="ltx_tr">
<td id="S5.T3.27.27.27.1" class="ltx_td ltx_align_left ltx_border_bb">mBERT<math id="S5.T3.27.27.27.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S5.T3.27.27.27.1.m1.1a"><msup id="S5.T3.27.27.27.1.m1.1.1" xref="S5.T3.27.27.27.1.m1.1.1.cmml"><mi id="S5.T3.27.27.27.1.m1.1.1a" xref="S5.T3.27.27.27.1.m1.1.1.cmml"></mi><mtext id="S5.T3.27.27.27.1.m1.1.1.1" xref="S5.T3.27.27.27.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S5.T3.27.27.27.1.m1.1b"><apply id="S5.T3.27.27.27.1.m1.1.1.cmml" xref="S5.T3.27.27.27.1.m1.1.1"><ci id="S5.T3.27.27.27.1.m1.1.1.1a.cmml" xref="S5.T3.27.27.27.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.27.27.27.1.m1.1.1.1.cmml" xref="S5.T3.27.27.27.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.27.27.27.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</td>
<td id="S5.T3.28.28.28.2" class="ltx_td ltx_align_left ltx_border_bb">56.25  <math id="S5.T3.28.28.28.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.28.28.28.2.m1.1a"><mo mathcolor="#333333" id="S5.T3.28.28.28.2.m1.1.1" xref="S5.T3.28.28.28.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.28.28.28.2.m1.1b"><csymbol cd="latexml" id="S5.T3.28.28.28.2.m1.1.1.cmml" xref="S5.T3.28.28.28.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.28.28.28.2.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.28.28.28.2.1" class="ltx_text" style="color:#333333;">0.5</span>
</td>
<td id="S5.T3.29.29.29.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T3.29.29.29.3.1" class="ltx_text ltx_font_bold">29.76  <math id="S5.T3.29.29.29.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.29.29.29.3.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.29.29.29.3.1.m1.1.1" xref="S5.T3.29.29.29.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.29.29.29.3.1.m1.1b"><csymbol cd="latexml" id="S5.T3.29.29.29.3.1.m1.1.1.cmml" xref="S5.T3.29.29.29.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.29.29.29.3.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.29.29.29.3.1.1" class="ltx_text" style="color:#333333;">2.3</span></span></td>
<td id="S5.T3.30.30.30.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T3.30.30.30.4.1" class="ltx_text ltx_font_bold">30.37  <math id="S5.T3.30.30.30.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.30.30.30.4.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.30.30.30.4.1.m1.1.1" xref="S5.T3.30.30.30.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.30.30.30.4.1.m1.1b"><csymbol cd="latexml" id="S5.T3.30.30.30.4.1.m1.1.1.cmml" xref="S5.T3.30.30.30.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.30.30.30.4.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.30.30.30.4.1.1" class="ltx_text" style="color:#333333;">1.8</span></span></td>
<td id="S5.T3.31.31.31.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T3.31.31.31.5.1" class="ltx_text ltx_font_bold">24.42  <math id="S5.T3.31.31.31.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.31.31.31.5.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.31.31.31.5.1.m1.1.1" xref="S5.T3.31.31.31.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.31.31.31.5.1.m1.1b"><csymbol cd="latexml" id="S5.T3.31.31.31.5.1.m1.1.1.cmml" xref="S5.T3.31.31.31.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.31.31.31.5.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.31.31.31.5.1.1" class="ltx_text" style="color:#333333;">1.1</span></span></td>
<td id="S5.T3.32.32.32.6" class="ltx_td ltx_align_left ltx_border_bb">19.15  <math id="S5.T3.32.32.32.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.32.32.32.6.m1.1a"><mo mathcolor="#333333" id="S5.T3.32.32.32.6.m1.1.1" xref="S5.T3.32.32.32.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.32.32.32.6.m1.1b"><csymbol cd="latexml" id="S5.T3.32.32.32.6.m1.1.1.cmml" xref="S5.T3.32.32.32.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.32.32.32.6.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.32.32.32.6.1" class="ltx_text" style="color:#333333;">2.8</span>
</td>
<td id="S5.T3.33.33.33.7" class="ltx_td ltx_align_left ltx_border_bb">15.12  <math id="S5.T3.33.33.33.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.33.33.33.7.m1.1a"><mo mathcolor="#333333" id="S5.T3.33.33.33.7.m1.1.1" xref="S5.T3.33.33.33.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.33.33.33.7.m1.1b"><csymbol cd="latexml" id="S5.T3.33.33.33.7.m1.1.1.cmml" xref="S5.T3.33.33.33.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.33.33.33.7.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.33.33.33.7.1" class="ltx_text" style="color:#333333;">1.9</span>
</td>
<td id="S5.T3.34.34.34.8" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T3.34.34.34.8.1" class="ltx_text ltx_font_bold">19.09  <math id="S5.T3.34.34.34.8.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.34.34.34.8.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.34.34.34.8.1.m1.1.1" xref="S5.T3.34.34.34.8.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.34.34.34.8.1.m1.1b"><csymbol cd="latexml" id="S5.T3.34.34.34.8.1.m1.1.1.cmml" xref="S5.T3.34.34.34.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.34.34.34.8.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.34.34.34.8.1.1" class="ltx_text" style="color:#333333;">0.9</span></span></td>
<td id="S5.T3.35.35.35.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S5.T3.35.35.35.9.1" class="ltx_text ltx_font_bold">24.86  <math id="S5.T3.35.35.35.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.35.35.35.9.1.m1.1a"><mo mathcolor="#333333" id="S5.T3.35.35.35.9.1.m1.1.1" xref="S5.T3.35.35.35.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.35.35.35.9.1.m1.1b"><csymbol cd="latexml" id="S5.T3.35.35.35.9.1.m1.1.1.cmml" xref="S5.T3.35.35.35.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.35.35.35.9.1.m1.1c">\pm</annotation></semantics></math><span id="S5.T3.35.35.35.9.1.1" class="ltx_text" style="color:#333333;">1.8</span></span></td>
<td id="S5.T3.35.35.35.10" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T3.35.35.35.10.1" class="ltx_text ltx_font_bold">23.25</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Zero-shot transfer results when transferring from English GQA. Average accuracy and standard deviation are reported. Best results are highlighted in <span id="S5.T3.38.1" class="ltx_text ltx_font_bold">bold</span>; <span id="S5.T3.39.2" class="ltx_text ltx_font_italic">mean</span> scores are not averaged over the source language (English).</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.3" class="ltx_p"><span id="S5.SS2.p3.3.3" class="ltx_text ltx_font_bold">OSCAR+<sup id="S5.SS2.p3.3.3.1" class="ltx_sup"><span id="S5.SS2.p3.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Emb</span></sup>, OSCAR+<sup id="S5.SS2.p3.3.3.2" class="ltx_sup"><span id="S5.SS2.p3.3.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Ada</span></sup>, and mBERT<sup id="S5.SS2.p3.3.3.3" class="ltx_sup"><span id="S5.SS2.p3.3.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Ada</span></sup>.</span> We use the pretrained weights and image region features provided by <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2021</a>)</cite>. However, we do not pass the object attribute labels as inputs to the model. The object attribute labels are in English and utilizing them in cross-lingual scenarios is non-trivial.<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>The replaced tokenizer and embedding representations of the target language potentially do not adequately represent English terms, resulting in a misalignment between the question (in the target language) and the object attributes (in English). </span></span></span> We leave this for future work.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.6" class="ltx_p">For the OSCAR+<sup id="S5.SS2.p4.6.1" class="ltx_sup"><span id="S5.SS2.p4.6.1.1" class="ltx_text ltx_font_italic">Emb</span></sup> setting, we fine-tune the transformer weights and the prediction head and freeze the embedding layer, using a learning rate of <math id="S5.SS2.p4.2.m2.1" class="ltx_Math" alttext="3\mathrm{e}{-5}" display="inline"><semantics id="S5.SS2.p4.2.m2.1a"><mrow id="S5.SS2.p4.2.m2.1.1" xref="S5.SS2.p4.2.m2.1.1.cmml"><mrow id="S5.SS2.p4.2.m2.1.1.2" xref="S5.SS2.p4.2.m2.1.1.2.cmml"><mn id="S5.SS2.p4.2.m2.1.1.2.2" xref="S5.SS2.p4.2.m2.1.1.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S5.SS2.p4.2.m2.1.1.2.1" xref="S5.SS2.p4.2.m2.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS2.p4.2.m2.1.1.2.3" xref="S5.SS2.p4.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS2.p4.2.m2.1.1.1" xref="S5.SS2.p4.2.m2.1.1.1.cmml">−</mo><mn id="S5.SS2.p4.2.m2.1.1.3" xref="S5.SS2.p4.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.2.m2.1b"><apply id="S5.SS2.p4.2.m2.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1"><minus id="S5.SS2.p4.2.m2.1.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1.1"></minus><apply id="S5.SS2.p4.2.m2.1.1.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2"><times id="S5.SS2.p4.2.m2.1.1.2.1.cmml" xref="S5.SS2.p4.2.m2.1.1.2.1"></times><cn type="integer" id="S5.SS2.p4.2.m2.1.1.2.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2.2">3</cn><ci id="S5.SS2.p4.2.m2.1.1.2.3.cmml" xref="S5.SS2.p4.2.m2.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS2.p4.2.m2.1.1.3.cmml" xref="S5.SS2.p4.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.2.m2.1c">3\mathrm{e}{-5}</annotation></semantics></math>.
For the OSCAR+<sup id="S5.SS2.p4.6.2" class="ltx_sup"><span id="S5.SS2.p4.6.2.1" class="ltx_text ltx_font_italic">Ada</span></sup> and mBERT<sup id="S5.SS2.p4.6.3" class="ltx_sup"><span id="S5.SS2.p4.6.3.1" class="ltx_text ltx_font_italic">Ada</span></sup> settings, we add adapter layers as described in §<a href="#S4.SS1" title="4.1 Multimodal → Multilingual ‣ 4 Baselines ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and illustrated in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Baselines ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We freeze all pretrained weights–including embeddings, transformer layers, and language adapters–and only fine-tune the newly introduced adapters and the prediction head. For mBERT<sup id="S5.SS2.p4.6.4" class="ltx_sup"><span id="S5.SS2.p4.6.4.1" class="ltx_text ltx_font_italic">Ada</span></sup>, we also add and train the affine image transformation layer. We fine-tune the adapter-based models with a learning rate of <math id="S5.SS2.p4.6.m6.1" class="ltx_Math" alttext="1\mathrm{e}{-4}" display="inline"><semantics id="S5.SS2.p4.6.m6.1a"><mrow id="S5.SS2.p4.6.m6.1.1" xref="S5.SS2.p4.6.m6.1.1.cmml"><mrow id="S5.SS2.p4.6.m6.1.1.2" xref="S5.SS2.p4.6.m6.1.1.2.cmml"><mn id="S5.SS2.p4.6.m6.1.1.2.2" xref="S5.SS2.p4.6.m6.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS2.p4.6.m6.1.1.2.1" xref="S5.SS2.p4.6.m6.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS2.p4.6.m6.1.1.2.3" xref="S5.SS2.p4.6.m6.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS2.p4.6.m6.1.1.1" xref="S5.SS2.p4.6.m6.1.1.1.cmml">−</mo><mn id="S5.SS2.p4.6.m6.1.1.3" xref="S5.SS2.p4.6.m6.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.6.m6.1b"><apply id="S5.SS2.p4.6.m6.1.1.cmml" xref="S5.SS2.p4.6.m6.1.1"><minus id="S5.SS2.p4.6.m6.1.1.1.cmml" xref="S5.SS2.p4.6.m6.1.1.1"></minus><apply id="S5.SS2.p4.6.m6.1.1.2.cmml" xref="S5.SS2.p4.6.m6.1.1.2"><times id="S5.SS2.p4.6.m6.1.1.2.1.cmml" xref="S5.SS2.p4.6.m6.1.1.2.1"></times><cn type="integer" id="S5.SS2.p4.6.m6.1.1.2.2.cmml" xref="S5.SS2.p4.6.m6.1.1.2.2">1</cn><ci id="S5.SS2.p4.6.m6.1.1.2.3.cmml" xref="S5.SS2.p4.6.m6.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS2.p4.6.m6.1.1.3.cmml" xref="S5.SS2.p4.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.6.m6.1c">1\mathrm{e}{-4}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Zero-Shot Cross-Lingual Transfer</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">For zero-shot cross-lingual evaluation, we utilize the model fine-tuned on the GQA training data and evaluate on the multilingual xGQA test data.
The model checkpoint that performed best on the English GQA validation data is selected for transfer.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">M<sup id="S5.SS3.p2.1.1.1" class="ltx_sup"><span id="S5.SS3.p2.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup>P.</span> As the model is pre-trained to cover, among others, xGQA languages, no additional steps are required for cross-lingual transfer.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">OSCAR+<sup id="S5.SS3.p3.1.1.1" class="ltx_sup"><span id="S5.SS3.p3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Emb</span></sup>.</span> We replace the English embedding layer with the target-language embedding layer.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.1" class="ltx_p"><span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_bold">OSCAR+<sup id="S5.SS3.p4.1.1.1" class="ltx_sup"><span id="S5.SS3.p4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Ada</span></sup>.</span> We replace the English embedding and language adapter layers with the embedding and adapters layers of the target language.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">mBERT<sup id="S5.SS3.p5.1.1.1" class="ltx_sup"><span id="S5.SS3.p5.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Ada</span></sup>.</span> We replace the language adapter layers with the adapters layers of the target language.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Few-Shot Cross-Lingual Transfer</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.8" class="ltx_p">For few-shot cross-lingual scenarios we follow <cite class="ltx_cite ltx_citemacro_citet">Lauscher et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> and start from the same fine-tuned model as for zero-shot transfer (see §<a href="#S5.SS3" title="5.3 Zero-Shot Cross-Lingual Transfer ‣ 5 Experimental Setup ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>). We then fine-tune the same parts of the model as when training on the English training data as in §<a href="#S5.SS2" title="5.2 Fine-tuning on GQA ‣ 5 Experimental Setup ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, but on the small portions of multimodal data available in the target language. We train on the different data splits, consisting of 1, 5, 10, 15, 20, 25, and 48 images (see Table <a href="#S3.T2" title="Table 2 ‣ 3 xGQA ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). We experiment with training for a different number of epochs (5, 10) using different learning rates (<math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="1\mathrm{e}{-5}" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mrow id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml"><mn id="S5.SS4.p1.1.m1.1.1.2.2" xref="S5.SS4.p1.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.1.m1.1.1.2.1" xref="S5.SS4.p1.1.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS4.p1.1.m1.1.1.2.3" xref="S5.SS4.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS4.p1.1.m1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.cmml">−</mo><mn id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><minus id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1"></minus><apply id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2"><times id="S5.SS4.p1.1.m1.1.1.2.1.cmml" xref="S5.SS4.p1.1.m1.1.1.2.1"></times><cn type="integer" id="S5.SS4.p1.1.m1.1.1.2.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2.2">1</cn><ci id="S5.SS4.p1.1.m1.1.1.2.3.cmml" xref="S5.SS4.p1.1.m1.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">1\mathrm{e}{-5}</annotation></semantics></math> and <math id="S5.SS4.p1.2.m2.1" class="ltx_Math" alttext="5\mathrm{e}{-5}" display="inline"><semantics id="S5.SS4.p1.2.m2.1a"><mrow id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mrow id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2.cmml"><mn id="S5.SS4.p1.2.m2.1.1.2.2" xref="S5.SS4.p1.2.m2.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.2.m2.1.1.2.1" xref="S5.SS4.p1.2.m2.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS4.p1.2.m2.1.1.2.3" xref="S5.SS4.p1.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS4.p1.2.m2.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1.cmml">−</mo><mn id="S5.SS4.p1.2.m2.1.1.3" xref="S5.SS4.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><minus id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1.1"></minus><apply id="S5.SS4.p1.2.m2.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2"><times id="S5.SS4.p1.2.m2.1.1.2.1.cmml" xref="S5.SS4.p1.2.m2.1.1.2.1"></times><cn type="integer" id="S5.SS4.p1.2.m2.1.1.2.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2.2">5</cn><ci id="S5.SS4.p1.2.m2.1.1.2.3.cmml" xref="S5.SS4.p1.2.m2.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS4.p1.2.m2.1.1.3.cmml" xref="S5.SS4.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">5\mathrm{e}{-5}</annotation></semantics></math> for M<sup id="S5.SS4.p1.8.1" class="ltx_sup">3</sup>P and OSCAR+<sup id="S5.SS4.p1.8.2" class="ltx_sup"><span id="S5.SS4.p1.8.2.1" class="ltx_text ltx_font_italic">Emb</span></sup>, and <math id="S5.SS4.p1.5.m5.1" class="ltx_Math" alttext="5\mathrm{e}{-5}" display="inline"><semantics id="S5.SS4.p1.5.m5.1a"><mrow id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml"><mrow id="S5.SS4.p1.5.m5.1.1.2" xref="S5.SS4.p1.5.m5.1.1.2.cmml"><mn id="S5.SS4.p1.5.m5.1.1.2.2" xref="S5.SS4.p1.5.m5.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.5.m5.1.1.2.1" xref="S5.SS4.p1.5.m5.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS4.p1.5.m5.1.1.2.3" xref="S5.SS4.p1.5.m5.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS4.p1.5.m5.1.1.1" xref="S5.SS4.p1.5.m5.1.1.1.cmml">−</mo><mn id="S5.SS4.p1.5.m5.1.1.3" xref="S5.SS4.p1.5.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><apply id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1"><minus id="S5.SS4.p1.5.m5.1.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1.1"></minus><apply id="S5.SS4.p1.5.m5.1.1.2.cmml" xref="S5.SS4.p1.5.m5.1.1.2"><times id="S5.SS4.p1.5.m5.1.1.2.1.cmml" xref="S5.SS4.p1.5.m5.1.1.2.1"></times><cn type="integer" id="S5.SS4.p1.5.m5.1.1.2.2.cmml" xref="S5.SS4.p1.5.m5.1.1.2.2">5</cn><ci id="S5.SS4.p1.5.m5.1.1.2.3.cmml" xref="S5.SS4.p1.5.m5.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS4.p1.5.m5.1.1.3.cmml" xref="S5.SS4.p1.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">5\mathrm{e}{-5}</annotation></semantics></math> and <math id="S5.SS4.p1.6.m6.1" class="ltx_Math" alttext="1\mathrm{e}{-4}" display="inline"><semantics id="S5.SS4.p1.6.m6.1a"><mrow id="S5.SS4.p1.6.m6.1.1" xref="S5.SS4.p1.6.m6.1.1.cmml"><mrow id="S5.SS4.p1.6.m6.1.1.2" xref="S5.SS4.p1.6.m6.1.1.2.cmml"><mn id="S5.SS4.p1.6.m6.1.1.2.2" xref="S5.SS4.p1.6.m6.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.6.m6.1.1.2.1" xref="S5.SS4.p1.6.m6.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS4.p1.6.m6.1.1.2.3" xref="S5.SS4.p1.6.m6.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS4.p1.6.m6.1.1.1" xref="S5.SS4.p1.6.m6.1.1.1.cmml">−</mo><mn id="S5.SS4.p1.6.m6.1.1.3" xref="S5.SS4.p1.6.m6.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.6.m6.1b"><apply id="S5.SS4.p1.6.m6.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1"><minus id="S5.SS4.p1.6.m6.1.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1.1"></minus><apply id="S5.SS4.p1.6.m6.1.1.2.cmml" xref="S5.SS4.p1.6.m6.1.1.2"><times id="S5.SS4.p1.6.m6.1.1.2.1.cmml" xref="S5.SS4.p1.6.m6.1.1.2.1"></times><cn type="integer" id="S5.SS4.p1.6.m6.1.1.2.2.cmml" xref="S5.SS4.p1.6.m6.1.1.2.2">1</cn><ci id="S5.SS4.p1.6.m6.1.1.2.3.cmml" xref="S5.SS4.p1.6.m6.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS4.p1.6.m6.1.1.3.cmml" xref="S5.SS4.p1.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.6.m6.1c">1\mathrm{e}{-4}</annotation></semantics></math> for OSCAR+<sup id="S5.SS4.p1.8.3" class="ltx_sup"><span id="S5.SS4.p1.8.3.1" class="ltx_text ltx_font_italic">Ada</span></sup> and mBERT<sup id="S5.SS4.p1.8.4" class="ltx_sup"><span id="S5.SS4.p1.8.4.1" class="ltx_text ltx_font_italic">Ada</span></sup>). We find that training for longer and with a larger learning rate performed best for all settings.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results and Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The main results are presented in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Fine-tuning on GQA ‣ 5 Experimental Setup ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (zero-shot experiments) and in Table <a href="#S6.T4" title="Table 4 ‣ 6.1 Zero-Shot Cross-Lingual Transfer ‣ 6 Results and Discussion ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (few-shot).</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Zero-Shot Cross-Lingual Transfer</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">One of our core findings is that multimodal zero-shot cross-lingual transfer is extremely difficult; we witness an average drop in accuracy of more than 38 points on the target languages of the xGQA dataset compared to English GQA scores (e.g., compare the results with M<sup id="S6.SS1.p1.1.1" class="ltx_sup">3</sup>P).</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">While, as expected, OSCAR+ achieves the best accuracy on the English test set, the massively multilingual models—M<sup id="S6.SS1.p2.1.1" class="ltx_sup">3</sup>P and mBERT—perform considerably better in cross-lingual transfer.<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>The superior accuracy of OSCAR+ on the English test set is expected as the model was pretrained on large English multimodal data.
We find that fine-tuning all transformer weights (OSCAR+<sup id="footnote11.1" class="ltx_sup"><span id="footnote11.1.1" class="ltx_text ltx_font_italic">Emb</span></sup>) achieves slightly better results than only training adapter weights (OSCAR+<sup id="footnote11.2" class="ltx_sup"><span id="footnote11.2.1" class="ltx_text ltx_font_italic">Ada</span></sup>). Our slightly lower scores compared to results by <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2021</a>)</cite> can be explained by us (1) not fine-tuning the embedding layer, and (2) not utilizing the attribute labels. Further, previous works that focus only on English add the official <span id="footnote11.3" class="ltx_text ltx_font_italic">validation</span> set to the <span id="footnote11.4" class="ltx_text ltx_font_italic">training</span> set, use the official <span id="footnote11.5" class="ltx_text ltx_font_italic">test-dev</span> set as their dev set, and report their test scores of the official GQA test benchmark <span id="footnote11.6" class="ltx_text ltx_font_italic">test-std</span> for which labels are not available. Our scores follow the training splits, where we use the official <span id="footnote11.7" class="ltx_text ltx_font_italic">test-dev</span> set as the final test set, as described before in §<a href="#S3" title="3 xGQA ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</span></span></span> This indicates, that joint multilingual pretraining is important and a simple multilingual adapter-based or embedding-based extension of monolingual models achieves inferior cross-lingual performance.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.3" class="ltx_p">While the pretraining method M<sup id="S6.SS1.p3.3.1" class="ltx_sup">3</sup>P achieves better accuracy on the English test set, the adapter-based multimodal extension of mBERT outperforms M<sup id="S6.SS1.p3.3.2" class="ltx_sup">3</sup>P in cross-lingual transfer. We hypothesize that, when fine-tuning all transformer weights on monolingual multimodal data, the cross-lingual alignment breaks within M<sup id="S6.SS1.p3.3.3" class="ltx_sup">3</sup>P. However, this does not happen in adapter-based settings, as the multilingual weights are frozen and thus remain intact.</p>
</div>
<figure id="S6.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x3.png" id="S6.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>M3P</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x4.png" id="S6.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>OSCAR+<math id="S6.F3.sf2.2.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.F3.sf2.2.m1.1b"><msup id="S6.F3.sf2.2.m1.1.1" xref="S6.F3.sf2.2.m1.1.1.cmml"><mi id="S6.F3.sf2.2.m1.1.1b" xref="S6.F3.sf2.2.m1.1.1.cmml"></mi><mtext id="S6.F3.sf2.2.m1.1.1.1" xref="S6.F3.sf2.2.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.F3.sf2.2.m1.1c"><apply id="S6.F3.sf2.2.m1.1.1.cmml" xref="S6.F3.sf2.2.m1.1.1"><ci id="S6.F3.sf2.2.m1.1.1.1a.cmml" xref="S6.F3.sf2.2.m1.1.1.1"><mtext mathsize="70%" id="S6.F3.sf2.2.m1.1.1.1.cmml" xref="S6.F3.sf2.2.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F3.sf2.2.m1.1d">{}^{\text{Ada}}</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x5.png" id="S6.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>mBERT<math id="S6.F3.sf3.2.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.F3.sf3.2.m1.1b"><msup id="S6.F3.sf3.2.m1.1.1" xref="S6.F3.sf3.2.m1.1.1.cmml"><mi id="S6.F3.sf3.2.m1.1.1b" xref="S6.F3.sf3.2.m1.1.1.cmml"></mi><mtext id="S6.F3.sf3.2.m1.1.1.1" xref="S6.F3.sf3.2.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.F3.sf3.2.m1.1c"><apply id="S6.F3.sf3.2.m1.1.1.cmml" xref="S6.F3.sf3.2.m1.1.1"><ci id="S6.F3.sf3.2.m1.1.1.1a.cmml" xref="S6.F3.sf3.2.m1.1.1.1"><mtext mathsize="70%" id="S6.F3.sf3.2.m1.1.1.1.cmml" xref="S6.F3.sf3.2.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F3.sf3.2.m1.1d">{}^{\text{Ada}}</annotation></semantics></math></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Zero-shot accuracy across different languages and structural question types from xGQA.</figcaption>
</figure>
<div id="S6.SS1.p4" class="ltx_para ltx_noindent">
<p id="S6.SS1.p4.1" class="ltx_p"><span id="S6.SS1.p4.1.1" class="ltx_text ltx_font_bold">Analysis of Structural Question Types.</span>
Figure <a href="#S6.F3" title="Figure 3 ‣ 6.1 Zero-Shot Cross-Lingual Transfer ‣ 6 Results and Discussion ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts our analysis of the structural question types in zero-shot experiments. We observe large drops in accuracy especially for <span id="S6.SS1.p4.1.2" class="ltx_text ltx_font_italic">query</span> and <span id="S6.SS1.p4.1.3" class="ltx_text ltx_font_italic">choose</span> type questions. <span id="S6.SS1.p4.1.4" class="ltx_text ltx_font_italic">Query</span> type questions are free-form and thus semantically the most difficult to answer, even in the source language (English). This explains the overall low accuracy across all approaches in zero-shot settings for this question type.</p>
</div>
<figure id="S6.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x6.png" id="S6.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>M3P</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x7.png" id="S6.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>OSCAR+<math id="S6.F4.sf2.2.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.F4.sf2.2.m1.1b"><msup id="S6.F4.sf2.2.m1.1.1" xref="S6.F4.sf2.2.m1.1.1.cmml"><mi id="S6.F4.sf2.2.m1.1.1b" xref="S6.F4.sf2.2.m1.1.1.cmml"></mi><mtext id="S6.F4.sf2.2.m1.1.1.1" xref="S6.F4.sf2.2.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.F4.sf2.2.m1.1c"><apply id="S6.F4.sf2.2.m1.1.1.cmml" xref="S6.F4.sf2.2.m1.1.1"><ci id="S6.F4.sf2.2.m1.1.1.1a.cmml" xref="S6.F4.sf2.2.m1.1.1.1"><mtext mathsize="70%" id="S6.F4.sf2.2.m1.1.1.1.cmml" xref="S6.F4.sf2.2.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.sf2.2.m1.1d">{}^{\text{Ada}}</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x8.png" id="S6.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>mBERT<math id="S6.F4.sf3.2.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.F4.sf3.2.m1.1b"><msup id="S6.F4.sf3.2.m1.1.1" xref="S6.F4.sf3.2.m1.1.1.cmml"><mi id="S6.F4.sf3.2.m1.1.1b" xref="S6.F4.sf3.2.m1.1.1.cmml"></mi><mtext id="S6.F4.sf3.2.m1.1.1.1" xref="S6.F4.sf3.2.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.F4.sf3.2.m1.1c"><apply id="S6.F4.sf3.2.m1.1.1.cmml" xref="S6.F4.sf3.2.m1.1.1"><ci id="S6.F4.sf3.2.m1.1.1.1a.cmml" xref="S6.F4.sf3.2.m1.1.1.1"><mtext mathsize="70%" id="S6.F4.sf3.2.m1.1.1.1.cmml" xref="S6.F4.sf3.2.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.sf3.2.m1.1d">{}^{\text{Ada}}</annotation></semantics></math></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Few-shot accuracy (with 48 images) across different languages and question types from xGQA.</figcaption>
</figure>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">This is in stark contrast with the <span id="S6.SS1.p5.1.1" class="ltx_text ltx_font_italic">choose</span>-type questions, which the models perform very well on in the source language. However, we report a substantial accuracy drop in zero-shot cross-lingual transfer. This decrease is most likely due to the nature of the question formulation and the modelling implementation. <span id="S6.SS1.p5.1.2" class="ltx_text ltx_font_italic">Choose</span>-type questions are formulated such that the answer to the question is a word or phrase which appears in the question, i.e. "Is it <span id="S6.SS1.p5.1.3" class="ltx_text ltx_framed ltx_framed_underline">red</span> or <span id="S6.SS1.p5.1.4" class="ltx_text ltx_framed ltx_framed_underline">blue</span>?". The label classes, and consequently the prediction head, are constructed as a set of all answers appearing in the dataset. This means that the model learns a distributed representation of each answer in its final layer. Consequently, in cross-lingual transfer,
the model is required to automatically align the question’s options "<span id="S6.SS1.p5.1.5" class="ltx_text ltx_framed ltx_framed_underline">red</span>" or "<span id="S6.SS1.p5.1.6" class="ltx_text ltx_framed ltx_framed_underline">blue</span>" (translated in their respective language), with their English latent representation of the model’s prediction head. The very low results in this category indicate that this cross-lingual word alignment breaks in zero-shot scenarios.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para">
<p id="S6.SS1.p6.2" class="ltx_p">Overall, zero-shot transfer with our proposed multimodal adapter-based extension of mBERT (mBERT<sup id="S6.SS1.p6.2.1" class="ltx_sup"><span id="S6.SS1.p6.2.1.1" class="ltx_text ltx_font_italic">Ada</span></sup>) achieves the best accuracy, with almost 3 points increase over M<sup id="S6.SS1.p6.2.2" class="ltx_sup">3</sup>P and almost 5 points increase over OSCAR+. However, the overall accuracy of all approaches remains low in comparison to the results in English. This indicates that zero-shot multimodal cross-lingual transfer is extremely difficult, most likely due to the misalignment issue between visual and cross-lingual internal representations. To investigate this conjecture further, we run similar tests in few-shot setups, which should potentially mitigate the misalignment issue observed in zero-shot setups.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.21" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:631.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(31.5pt,-46.0pt) scale(1.17022199187433,1.17022199187433) ;">
<table id="S6.T4.21.21" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.21.21.22.1" class="ltx_tr">
<th id="S6.T4.21.21.22.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="2"><span id="S6.T4.21.21.22.1.1.1" class="ltx_text ltx_font_bold">Lang</span></th>
<th id="S6.T4.21.21.22.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="2"><span id="S6.T4.21.21.22.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S6.T4.21.21.22.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" colspan="7"><span id="S6.T4.21.21.22.1.3.1" class="ltx_text ltx_font_bold"># Training Images</span></th>
</tr>
<tr id="S6.T4.21.21.23.2" class="ltx_tr">
<th id="S6.T4.21.21.23.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.23.2.1.1" class="ltx_text ltx_font_bold">0</span></th>
<th id="S6.T4.21.21.23.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.23.2.2.1" class="ltx_text ltx_font_bold">1</span></th>
<th id="S6.T4.21.21.23.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.23.2.3.1" class="ltx_text ltx_font_bold">5</span>
</th>
<th id="S6.T4.21.21.23.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.23.2.4.1" class="ltx_text ltx_font_bold">10</span>
</th>
<th id="S6.T4.21.21.23.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.23.2.5.1" class="ltx_text ltx_font_bold">20</span>
</th>
<th id="S6.T4.21.21.23.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.23.2.6.1" class="ltx_text ltx_font_bold">25</span>
</th>
<th id="S6.T4.21.21.23.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.23.2.7.1" class="ltx_text ltx_font_bold">48</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.21.21.24.1" class="ltx_tr">
<th id="S6.T4.21.21.24.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="4"><span id="S6.T4.21.21.24.1.1.1" class="ltx_text">de</span></th>
<th id="S6.T4.21.21.24.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">M3P</th>
<td id="S6.T4.21.21.24.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">24.78</td>
<td id="S6.T4.21.21.24.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.49</td>
<td id="S6.T4.21.21.24.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.24.1.5.1" class="ltx_text ltx_font_bold">39.31</span></td>
<td id="S6.T4.21.21.24.1.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.24.1.6.1" class="ltx_text ltx_font_bold">41.05</span></td>
<td id="S6.T4.21.21.24.1.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.24.1.7.1" class="ltx_text ltx_font_bold">42.22</span></td>
<td id="S6.T4.21.21.24.1.8" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.24.1.8.1" class="ltx_text ltx_font_bold">42.54</span></td>
<td id="S6.T4.21.21.24.1.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.24.1.9.1" class="ltx_text ltx_font_bold">43.16</span></td>
</tr>
<tr id="S6.T4.1.1.1" class="ltx_tr">
<th id="S6.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S6.T4.1.1.1.1.m1.1a"><msup id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml"><mi id="S6.T4.1.1.1.1.m1.1.1a" xref="S6.T4.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S6.T4.1.1.1.1.m1.1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><apply id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1"><ci id="S6.T4.1.1.1.1.m1.1.1.1a.cmml" xref="S6.T4.1.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.1.1.1.1.m1.1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</th>
<td id="S6.T4.1.1.1.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">17.49</td>
<td id="S6.T4.1.1.1.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">17.84</td>
<td id="S6.T4.1.1.1.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">29.09</td>
<td id="S6.T4.1.1.1.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.48</td>
<td id="S6.T4.1.1.1.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.35</td>
<td id="S6.T4.1.1.1.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">38.45</td>
<td id="S6.T4.1.1.1.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">41.08</td>
</tr>
<tr id="S6.T4.2.2.2" class="ltx_tr">
<th id="S6.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.2.2.2.1.m1.1a"><msup id="S6.T4.2.2.2.1.m1.1.1" xref="S6.T4.2.2.2.1.m1.1.1.cmml"><mi id="S6.T4.2.2.2.1.m1.1.1a" xref="S6.T4.2.2.2.1.m1.1.1.cmml"></mi><mtext id="S6.T4.2.2.2.1.m1.1.1.1" xref="S6.T4.2.2.2.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.1.m1.1b"><apply id="S6.T4.2.2.2.1.m1.1.1.cmml" xref="S6.T4.2.2.2.1.m1.1.1"><ci id="S6.T4.2.2.2.1.m1.1.1.1a.cmml" xref="S6.T4.2.2.2.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.2.2.2.1.m1.1.1.1.cmml" xref="S6.T4.2.2.2.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">17.84</td>
<td id="S6.T4.2.2.2.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">21.40</td>
<td id="S6.T4.2.2.2.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.26</td>
<td id="S6.T4.2.2.2.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">35.84</td>
<td id="S6.T4.2.2.2.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.92</td>
<td id="S6.T4.2.2.2.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">38.46</td>
<td id="S6.T4.2.2.2.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">40.58</td>
</tr>
<tr id="S6.T4.3.3.3" class="ltx_tr">
<th id="S6.T4.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">mBERT<math id="S6.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.3.3.3.1.m1.1a"><msup id="S6.T4.3.3.3.1.m1.1.1" xref="S6.T4.3.3.3.1.m1.1.1.cmml"><mi id="S6.T4.3.3.3.1.m1.1.1a" xref="S6.T4.3.3.3.1.m1.1.1.cmml"></mi><mtext id="S6.T4.3.3.3.1.m1.1.1.1" xref="S6.T4.3.3.3.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.1.m1.1b"><apply id="S6.T4.3.3.3.1.m1.1.1.cmml" xref="S6.T4.3.3.3.1.m1.1.1"><ci id="S6.T4.3.3.3.1.m1.1.1.1a.cmml" xref="S6.T4.3.3.3.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.3.3.3.1.m1.1.1.1.cmml" xref="S6.T4.3.3.3.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.3.3.3.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.3.3.3.2.1" class="ltx_text ltx_font_bold">32.41</span></td>
<td id="S6.T4.3.3.3.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.3.3.3.3.1" class="ltx_text ltx_font_bold">33.87</span></td>
<td id="S6.T4.3.3.3.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.44</td>
<td id="S6.T4.3.3.3.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">39.15</td>
<td id="S6.T4.3.3.3.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">40.65</td>
<td id="S6.T4.3.3.3.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">41.63</td>
<td id="S6.T4.3.3.3.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">42.71</td>
</tr>
<tr id="S6.T4.21.21.25.2" class="ltx_tr">
<th id="S6.T4.21.21.25.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="4"><span id="S6.T4.21.21.25.2.1.1" class="ltx_text">pt</span></th>
<th id="S6.T4.21.21.25.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">M3P</th>
<td id="S6.T4.21.21.25.2.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">26.73</td>
<td id="S6.T4.21.21.25.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.98</td>
<td id="S6.T4.21.21.25.2.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.23</td>
<td id="S6.T4.21.21.25.2.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.25.2.6.1" class="ltx_text ltx_font_bold">39.07</span></td>
<td id="S6.T4.21.21.25.2.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.25.2.7.1" class="ltx_text ltx_font_bold">40.92</span></td>
<td id="S6.T4.21.21.25.2.8" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.25.2.8.1" class="ltx_text ltx_font_bold">41.05</span></td>
<td id="S6.T4.21.21.25.2.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">43.06</td>
</tr>
<tr id="S6.T4.4.4.4" class="ltx_tr">
<th id="S6.T4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.4.4.4.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S6.T4.4.4.4.1.m1.1a"><msup id="S6.T4.4.4.4.1.m1.1.1" xref="S6.T4.4.4.4.1.m1.1.1.cmml"><mi id="S6.T4.4.4.4.1.m1.1.1a" xref="S6.T4.4.4.4.1.m1.1.1.cmml"></mi><mtext id="S6.T4.4.4.4.1.m1.1.1.1" xref="S6.T4.4.4.4.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.4.4.4.1.m1.1b"><apply id="S6.T4.4.4.4.1.m1.1.1.cmml" xref="S6.T4.4.4.4.1.m1.1.1"><ci id="S6.T4.4.4.4.1.m1.1.1.1a.cmml" xref="S6.T4.4.4.4.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.4.4.4.1.m1.1.1.1.cmml" xref="S6.T4.4.4.4.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.4.4.4.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</th>
<td id="S6.T4.4.4.4.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">19.36</td>
<td id="S6.T4.4.4.4.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">22.55</td>
<td id="S6.T4.4.4.4.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.42</td>
<td id="S6.T4.4.4.4.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">36.37</td>
<td id="S6.T4.4.4.4.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">39.01</td>
<td id="S6.T4.4.4.4.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">40.15</td>
<td id="S6.T4.4.4.4.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.4.4.4.8.1" class="ltx_text ltx_font_bold">43.27</span></td>
</tr>
<tr id="S6.T4.5.5.5" class="ltx_tr">
<th id="S6.T4.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.5.5.5.1.m1.1a"><msup id="S6.T4.5.5.5.1.m1.1.1" xref="S6.T4.5.5.5.1.m1.1.1.cmml"><mi id="S6.T4.5.5.5.1.m1.1.1a" xref="S6.T4.5.5.5.1.m1.1.1.cmml"></mi><mtext id="S6.T4.5.5.5.1.m1.1.1.1" xref="S6.T4.5.5.5.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.5.5.5.1.m1.1b"><apply id="S6.T4.5.5.5.1.m1.1.1.cmml" xref="S6.T4.5.5.5.1.m1.1.1"><ci id="S6.T4.5.5.5.1.m1.1.1.1a.cmml" xref="S6.T4.5.5.5.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.5.5.5.1.m1.1.1.1.cmml" xref="S6.T4.5.5.5.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.5.5.5.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.5.5.5.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">24.58</td>
<td id="S6.T4.5.5.5.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">29.61</td>
<td id="S6.T4.5.5.5.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.73</td>
<td id="S6.T4.5.5.5.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.46</td>
<td id="S6.T4.5.5.5.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">38.82</td>
<td id="S6.T4.5.5.5.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">39.70</td>
<td id="S6.T4.5.5.5.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">41.75</td>
</tr>
<tr id="S6.T4.6.6.6" class="ltx_tr">
<th id="S6.T4.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">mBERT<math id="S6.T4.6.6.6.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.6.6.6.1.m1.1a"><msup id="S6.T4.6.6.6.1.m1.1.1" xref="S6.T4.6.6.6.1.m1.1.1.cmml"><mi id="S6.T4.6.6.6.1.m1.1.1a" xref="S6.T4.6.6.6.1.m1.1.1.cmml"></mi><mtext id="S6.T4.6.6.6.1.m1.1.1.1" xref="S6.T4.6.6.6.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.6.6.6.1.m1.1b"><apply id="S6.T4.6.6.6.1.m1.1.1.cmml" xref="S6.T4.6.6.6.1.m1.1.1"><ci id="S6.T4.6.6.6.1.m1.1.1.1a.cmml" xref="S6.T4.6.6.6.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.6.6.6.1.m1.1.1.1.cmml" xref="S6.T4.6.6.6.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.6.6.6.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.6.6.6.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.6.6.6.2.1" class="ltx_text ltx_font_bold">31.45</span></td>
<td id="S6.T4.6.6.6.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.6.6.6.3.1" class="ltx_text ltx_font_bold">33.27</span></td>
<td id="S6.T4.6.6.6.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.6.6.6.4.1" class="ltx_text ltx_font_bold">37.31</span></td>
<td id="S6.T4.6.6.6.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">38.88</td>
<td id="S6.T4.6.6.6.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">40.51</td>
<td id="S6.T4.6.6.6.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">41.03</td>
<td id="S6.T4.6.6.6.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">42.62</td>
</tr>
<tr id="S6.T4.21.21.26.3" class="ltx_tr">
<th id="S6.T4.21.21.26.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="4"><span id="S6.T4.21.21.26.3.1.1" class="ltx_text">ru</span></th>
<th id="S6.T4.21.21.26.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">M3P</th>
<td id="S6.T4.21.21.26.3.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">24.29</td>
<td id="S6.T4.21.21.26.3.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.26.3.4.1" class="ltx_text ltx_font_bold">32.32</span></td>
<td id="S6.T4.21.21.26.3.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.26.3.5.1" class="ltx_text ltx_font_bold">36.71</span></td>
<td id="S6.T4.21.21.26.3.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.26.3.6.1" class="ltx_text ltx_font_bold">38.53</span></td>
<td id="S6.T4.21.21.26.3.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.26.3.7.1" class="ltx_text ltx_font_bold">39.94</span></td>
<td id="S6.T4.21.21.26.3.8" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.26.3.8.1" class="ltx_text ltx_font_bold">40.13</span></td>
<td id="S6.T4.21.21.26.3.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.26.3.9.1" class="ltx_text ltx_font_bold">41.85</span></td>
</tr>
<tr id="S6.T4.7.7.7" class="ltx_tr">
<th id="S6.T4.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.7.7.7.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S6.T4.7.7.7.1.m1.1a"><msup id="S6.T4.7.7.7.1.m1.1.1" xref="S6.T4.7.7.7.1.m1.1.1.cmml"><mi id="S6.T4.7.7.7.1.m1.1.1a" xref="S6.T4.7.7.7.1.m1.1.1.cmml"></mi><mtext id="S6.T4.7.7.7.1.m1.1.1.1" xref="S6.T4.7.7.7.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.7.7.7.1.m1.1b"><apply id="S6.T4.7.7.7.1.m1.1.1.cmml" xref="S6.T4.7.7.7.1.m1.1.1"><ci id="S6.T4.7.7.7.1.m1.1.1.1a.cmml" xref="S6.T4.7.7.7.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.7.7.7.1.m1.1.1.1.cmml" xref="S6.T4.7.7.7.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.7.7.7.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</th>
<td id="S6.T4.7.7.7.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">7.98</td>
<td id="S6.T4.7.7.7.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">17.32</td>
<td id="S6.T4.7.7.7.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">23.72</td>
<td id="S6.T4.7.7.7.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">28.21</td>
<td id="S6.T4.7.7.7.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.15</td>
<td id="S6.T4.7.7.7.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.87</td>
<td id="S6.T4.7.7.7.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">36.84</td>
</tr>
<tr id="S6.T4.8.8.8" class="ltx_tr">
<th id="S6.T4.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.8.8.8.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.8.8.8.1.m1.1a"><msup id="S6.T4.8.8.8.1.m1.1.1" xref="S6.T4.8.8.8.1.m1.1.1.cmml"><mi id="S6.T4.8.8.8.1.m1.1.1a" xref="S6.T4.8.8.8.1.m1.1.1.cmml"></mi><mtext id="S6.T4.8.8.8.1.m1.1.1.1" xref="S6.T4.8.8.8.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.8.8.8.1.m1.1b"><apply id="S6.T4.8.8.8.1.m1.1.1.cmml" xref="S6.T4.8.8.8.1.m1.1.1"><ci id="S6.T4.8.8.8.1.m1.1.1.1a.cmml" xref="S6.T4.8.8.8.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.8.8.8.1.m1.1.1.1.cmml" xref="S6.T4.8.8.8.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.8.8.8.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.8.8.8.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">16.38</td>
<td id="S6.T4.8.8.8.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">19.74</td>
<td id="S6.T4.8.8.8.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">27.42</td>
<td id="S6.T4.8.8.8.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">30.17</td>
<td id="S6.T4.8.8.8.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">33.22</td>
<td id="S6.T4.8.8.8.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.21</td>
<td id="S6.T4.8.8.8.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.28</td>
</tr>
<tr id="S6.T4.9.9.9" class="ltx_tr">
<th id="S6.T4.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">mBERT<math id="S6.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.9.9.9.1.m1.1a"><msup id="S6.T4.9.9.9.1.m1.1.1" xref="S6.T4.9.9.9.1.m1.1.1.cmml"><mi id="S6.T4.9.9.9.1.m1.1.1a" xref="S6.T4.9.9.9.1.m1.1.1.cmml"></mi><mtext id="S6.T4.9.9.9.1.m1.1.1.1" xref="S6.T4.9.9.9.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.9.9.9.1.m1.1b"><apply id="S6.T4.9.9.9.1.m1.1.1.cmml" xref="S6.T4.9.9.9.1.m1.1.1"><ci id="S6.T4.9.9.9.1.m1.1.1.1a.cmml" xref="S6.T4.9.9.9.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.9.9.9.1.m1.1.1.1.cmml" xref="S6.T4.9.9.9.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.9.9.9.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.9.9.9.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.9.9.9.2.1" class="ltx_text ltx_font_bold">25.51</span></td>
<td id="S6.T4.9.9.9.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">26.47</td>
<td id="S6.T4.9.9.9.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.69</td>
<td id="S6.T4.9.9.9.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.47</td>
<td id="S6.T4.9.9.9.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.93</td>
<td id="S6.T4.9.9.9.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">35.53</td>
<td id="S6.T4.9.9.9.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.42</td>
</tr>
<tr id="S6.T4.21.21.27.4" class="ltx_tr">
<th id="S6.T4.21.21.27.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="4"><span id="S6.T4.21.21.27.4.1.1" class="ltx_text">id</span></th>
<th id="S6.T4.21.21.27.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">M3P</th>
<td id="S6.T4.21.21.27.4.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">18.74</td>
<td id="S6.T4.21.21.27.4.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.37</td>
<td id="S6.T4.21.21.27.4.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.27.4.5.1" class="ltx_text ltx_font_bold">37.24</span></td>
<td id="S6.T4.21.21.27.4.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.27.4.6.1" class="ltx_text ltx_font_bold">38.65</span></td>
<td id="S6.T4.21.21.27.4.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.27.4.7.1" class="ltx_text ltx_font_bold">41.07</span></td>
<td id="S6.T4.21.21.27.4.8" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.27.4.8.1" class="ltx_text ltx_font_bold">42.00</span></td>
<td id="S6.T4.21.21.27.4.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.27.4.9.1" class="ltx_text ltx_font_bold">43.12</span></td>
</tr>
<tr id="S6.T4.10.10.10" class="ltx_tr">
<th id="S6.T4.10.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.10.10.10.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S6.T4.10.10.10.1.m1.1a"><msup id="S6.T4.10.10.10.1.m1.1.1" xref="S6.T4.10.10.10.1.m1.1.1.cmml"><mi id="S6.T4.10.10.10.1.m1.1.1a" xref="S6.T4.10.10.10.1.m1.1.1.cmml"></mi><mtext id="S6.T4.10.10.10.1.m1.1.1.1" xref="S6.T4.10.10.10.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.10.10.10.1.m1.1b"><apply id="S6.T4.10.10.10.1.m1.1.1.cmml" xref="S6.T4.10.10.10.1.m1.1.1"><ci id="S6.T4.10.10.10.1.m1.1.1.1a.cmml" xref="S6.T4.10.10.10.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.10.10.10.1.m1.1.1.1.cmml" xref="S6.T4.10.10.10.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.10.10.10.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</th>
<td id="S6.T4.10.10.10.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">17.89</td>
<td id="S6.T4.10.10.10.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">21.09</td>
<td id="S6.T4.10.10.10.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">29.76</td>
<td id="S6.T4.10.10.10.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">33.59</td>
<td id="S6.T4.10.10.10.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">36.69</td>
<td id="S6.T4.10.10.10.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.31</td>
<td id="S6.T4.10.10.10.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">40.51</td>
</tr>
<tr id="S6.T4.11.11.11" class="ltx_tr">
<th id="S6.T4.11.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.11.11.11.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.11.11.11.1.m1.1a"><msup id="S6.T4.11.11.11.1.m1.1.1" xref="S6.T4.11.11.11.1.m1.1.1.cmml"><mi id="S6.T4.11.11.11.1.m1.1.1a" xref="S6.T4.11.11.11.1.m1.1.1.cmml"></mi><mtext id="S6.T4.11.11.11.1.m1.1.1.1" xref="S6.T4.11.11.11.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.11.11.11.1.m1.1b"><apply id="S6.T4.11.11.11.1.m1.1.1.cmml" xref="S6.T4.11.11.11.1.m1.1.1"><ci id="S6.T4.11.11.11.1.m1.1.1.1a.cmml" xref="S6.T4.11.11.11.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.11.11.11.1.m1.1.1.1.cmml" xref="S6.T4.11.11.11.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.11.11.11.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.11.11.11.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">18.52</td>
<td id="S6.T4.11.11.11.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">23.94</td>
<td id="S6.T4.11.11.11.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.45</td>
<td id="S6.T4.11.11.11.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.60</td>
<td id="S6.T4.11.11.11.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.26</td>
<td id="S6.T4.11.11.11.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.97</td>
<td id="S6.T4.11.11.11.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">40.60</td>
</tr>
<tr id="S6.T4.12.12.12" class="ltx_tr">
<th id="S6.T4.12.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">mBERT<math id="S6.T4.12.12.12.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.12.12.12.1.m1.1a"><msup id="S6.T4.12.12.12.1.m1.1.1" xref="S6.T4.12.12.12.1.m1.1.1.cmml"><mi id="S6.T4.12.12.12.1.m1.1.1a" xref="S6.T4.12.12.12.1.m1.1.1.cmml"></mi><mtext id="S6.T4.12.12.12.1.m1.1.1.1" xref="S6.T4.12.12.12.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.12.12.12.1.m1.1b"><apply id="S6.T4.12.12.12.1.m1.1.1.cmml" xref="S6.T4.12.12.12.1.m1.1.1"><ci id="S6.T4.12.12.12.1.m1.1.1.1a.cmml" xref="S6.T4.12.12.12.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.12.12.12.1.m1.1.1.1.cmml" xref="S6.T4.12.12.12.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.12.12.12.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.12.12.12.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.12.12.12.2.1" class="ltx_text ltx_font_bold">19.77</span></td>
<td id="S6.T4.12.12.12.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.12.12.12.3.1" class="ltx_text ltx_font_bold">31.99</span></td>
<td id="S6.T4.12.12.12.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.49</td>
<td id="S6.T4.12.12.12.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">36.26</td>
<td id="S6.T4.12.12.12.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">39.15</td>
<td id="S6.T4.12.12.12.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">39.81</td>
<td id="S6.T4.12.12.12.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">40.88</td>
</tr>
<tr id="S6.T4.21.21.28.5" class="ltx_tr">
<th id="S6.T4.21.21.28.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="4"><span id="S6.T4.21.21.28.5.1.1" class="ltx_text">bn</span></th>
<th id="S6.T4.21.21.28.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">M3P</th>
<td id="S6.T4.21.21.28.5.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.28.5.3.1" class="ltx_text ltx_font_bold">17.59</span></td>
<td id="S6.T4.21.21.28.5.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">17.33</td>
<td id="S6.T4.21.21.28.5.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.28.5.5.1" class="ltx_text ltx_font_bold">26.94</span></td>
<td id="S6.T4.21.21.28.5.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.28.5.6.1" class="ltx_text ltx_font_bold">31.09</span></td>
<td id="S6.T4.21.21.28.5.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.28.5.7.1" class="ltx_text ltx_font_bold">34.58</span></td>
<td id="S6.T4.21.21.28.5.8" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.28.5.8.1" class="ltx_text ltx_font_bold">35.27</span></td>
<td id="S6.T4.21.21.28.5.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.28.5.9.1" class="ltx_text ltx_font_bold">37.96</span></td>
</tr>
<tr id="S6.T4.13.13.13" class="ltx_tr">
<th id="S6.T4.13.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.13.13.13.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S6.T4.13.13.13.1.m1.1a"><msup id="S6.T4.13.13.13.1.m1.1.1" xref="S6.T4.13.13.13.1.m1.1.1.cmml"><mi id="S6.T4.13.13.13.1.m1.1.1a" xref="S6.T4.13.13.13.1.m1.1.1.cmml"></mi><mtext id="S6.T4.13.13.13.1.m1.1.1.1" xref="S6.T4.13.13.13.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.13.13.13.1.m1.1b"><apply id="S6.T4.13.13.13.1.m1.1.1.cmml" xref="S6.T4.13.13.13.1.m1.1.1"><ci id="S6.T4.13.13.13.1.m1.1.1.1a.cmml" xref="S6.T4.13.13.13.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.13.13.13.1.m1.1.1.1.cmml" xref="S6.T4.13.13.13.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.13.13.13.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</th>
<td id="S6.T4.13.13.13.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">13.35</td>
<td id="S6.T4.13.13.13.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.13.13.13.3.1" class="ltx_text ltx_font_bold">17.40</span></td>
<td id="S6.T4.13.13.13.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">21.67</td>
<td id="S6.T4.13.13.13.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">26.61</td>
<td id="S6.T4.13.13.13.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.94</td>
<td id="S6.T4.13.13.13.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.78</td>
<td id="S6.T4.13.13.13.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">36.97</td>
</tr>
<tr id="S6.T4.14.14.14" class="ltx_tr">
<th id="S6.T4.14.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.14.14.14.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.14.14.14.1.m1.1a"><msup id="S6.T4.14.14.14.1.m1.1.1" xref="S6.T4.14.14.14.1.m1.1.1.cmml"><mi id="S6.T4.14.14.14.1.m1.1.1a" xref="S6.T4.14.14.14.1.m1.1.1.cmml"></mi><mtext id="S6.T4.14.14.14.1.m1.1.1.1" xref="S6.T4.14.14.14.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.14.14.14.1.m1.1b"><apply id="S6.T4.14.14.14.1.m1.1.1.cmml" xref="S6.T4.14.14.14.1.m1.1.1"><ci id="S6.T4.14.14.14.1.m1.1.1.1a.cmml" xref="S6.T4.14.14.14.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.14.14.14.1.m1.1.1.1.cmml" xref="S6.T4.14.14.14.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.14.14.14.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.14.14.14.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">13.96</td>
<td id="S6.T4.14.14.14.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">15.60</td>
<td id="S6.T4.14.14.14.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">22.35</td>
<td id="S6.T4.14.14.14.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">27.20</td>
<td id="S6.T4.14.14.14.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.25</td>
<td id="S6.T4.14.14.14.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.81</td>
<td id="S6.T4.14.14.14.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">35.45</td>
</tr>
<tr id="S6.T4.15.15.15" class="ltx_tr">
<th id="S6.T4.15.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">mBERT<math id="S6.T4.15.15.15.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.15.15.15.1.m1.1a"><msup id="S6.T4.15.15.15.1.m1.1.1" xref="S6.T4.15.15.15.1.m1.1.1.cmml"><mi id="S6.T4.15.15.15.1.m1.1.1a" xref="S6.T4.15.15.15.1.m1.1.1.cmml"></mi><mtext id="S6.T4.15.15.15.1.m1.1.1.1" xref="S6.T4.15.15.15.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.15.15.15.1.m1.1b"><apply id="S6.T4.15.15.15.1.m1.1.1.cmml" xref="S6.T4.15.15.15.1.m1.1.1"><ci id="S6.T4.15.15.15.1.m1.1.1.1a.cmml" xref="S6.T4.15.15.15.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.15.15.15.1.m1.1.1.1.cmml" xref="S6.T4.15.15.15.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.15.15.15.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.15.15.15.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">13.38</td>
<td id="S6.T4.15.15.15.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">11.33</td>
<td id="S6.T4.15.15.15.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">23.10</td>
<td id="S6.T4.15.15.15.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">26.55</td>
<td id="S6.T4.15.15.15.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.60</td>
<td id="S6.T4.15.15.15.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.26</td>
<td id="S6.T4.15.15.15.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.18</td>
</tr>
<tr id="S6.T4.21.21.29.6" class="ltx_tr">
<th id="S6.T4.21.21.29.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="4"><span id="S6.T4.21.21.29.6.1.1" class="ltx_text">ko</span></th>
<th id="S6.T4.21.21.29.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">M3P</th>
<td id="S6.T4.21.21.29.6.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">19.70</td>
<td id="S6.T4.21.21.29.6.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.29.6.4.1" class="ltx_text ltx_font_bold">22.94</span></td>
<td id="S6.T4.21.21.29.6.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.29.6.5.1" class="ltx_text ltx_font_bold">32.28</span></td>
<td id="S6.T4.21.21.29.6.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.29.6.6.1" class="ltx_text ltx_font_bold">35.50</span></td>
<td id="S6.T4.21.21.29.6.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.29.6.7.1" class="ltx_text ltx_font_bold">37.72</span></td>
<td id="S6.T4.21.21.29.6.8" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.29.6.8.1" class="ltx_text ltx_font_bold">37.84</span></td>
<td id="S6.T4.21.21.29.6.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.29.6.9.1" class="ltx_text ltx_font_bold">38.61</span></td>
</tr>
<tr id="S6.T4.16.16.16" class="ltx_tr">
<th id="S6.T4.16.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.16.16.16.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S6.T4.16.16.16.1.m1.1a"><msup id="S6.T4.16.16.16.1.m1.1.1" xref="S6.T4.16.16.16.1.m1.1.1.cmml"><mi id="S6.T4.16.16.16.1.m1.1.1a" xref="S6.T4.16.16.16.1.m1.1.1.cmml"></mi><mtext id="S6.T4.16.16.16.1.m1.1.1.1" xref="S6.T4.16.16.16.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.16.16.16.1.m1.1b"><apply id="S6.T4.16.16.16.1.m1.1.1.cmml" xref="S6.T4.16.16.16.1.m1.1.1"><ci id="S6.T4.16.16.16.1.m1.1.1.1a.cmml" xref="S6.T4.16.16.16.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.16.16.16.1.m1.1.1.1.cmml" xref="S6.T4.16.16.16.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.16.16.16.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</th>
<td id="S6.T4.16.16.16.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">15.11</td>
<td id="S6.T4.16.16.16.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">16.43</td>
<td id="S6.T4.16.16.16.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">19.99</td>
<td id="S6.T4.16.16.16.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">24.78</td>
<td id="S6.T4.16.16.16.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">29.48</td>
<td id="S6.T4.16.16.16.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">30.43</td>
<td id="S6.T4.16.16.16.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">35.59</td>
</tr>
<tr id="S6.T4.17.17.17" class="ltx_tr">
<th id="S6.T4.17.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.17.17.17.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.17.17.17.1.m1.1a"><msup id="S6.T4.17.17.17.1.m1.1.1" xref="S6.T4.17.17.17.1.m1.1.1.cmml"><mi id="S6.T4.17.17.17.1.m1.1.1a" xref="S6.T4.17.17.17.1.m1.1.1.cmml"></mi><mtext id="S6.T4.17.17.17.1.m1.1.1.1" xref="S6.T4.17.17.17.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.17.17.17.1.m1.1b"><apply id="S6.T4.17.17.17.1.m1.1.1.cmml" xref="S6.T4.17.17.17.1.m1.1.1"><ci id="S6.T4.17.17.17.1.m1.1.1.1a.cmml" xref="S6.T4.17.17.17.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.17.17.17.1.m1.1.1.1.cmml" xref="S6.T4.17.17.17.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.17.17.17.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.17.17.17.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">12.25</td>
<td id="S6.T4.17.17.17.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">15.48</td>
<td id="S6.T4.17.17.17.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">20.73</td>
<td id="S6.T4.17.17.17.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">25.97</td>
<td id="S6.T4.17.17.17.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.37</td>
<td id="S6.T4.17.17.17.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.20</td>
<td id="S6.T4.17.17.17.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">35.41</td>
</tr>
<tr id="S6.T4.18.18.18" class="ltx_tr">
<th id="S6.T4.18.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">mBERT<math id="S6.T4.18.18.18.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.18.18.18.1.m1.1a"><msup id="S6.T4.18.18.18.1.m1.1.1" xref="S6.T4.18.18.18.1.m1.1.1.cmml"><mi id="S6.T4.18.18.18.1.m1.1.1a" xref="S6.T4.18.18.18.1.m1.1.1.cmml"></mi><mtext id="S6.T4.18.18.18.1.m1.1.1.1" xref="S6.T4.18.18.18.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.18.18.18.1.m1.1b"><apply id="S6.T4.18.18.18.1.m1.1.1.cmml" xref="S6.T4.18.18.18.1.m1.1.1"><ci id="S6.T4.18.18.18.1.m1.1.1.1a.cmml" xref="S6.T4.18.18.18.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.18.18.18.1.m1.1.1.1.cmml" xref="S6.T4.18.18.18.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.18.18.18.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.18.18.18.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.18.18.18.2.1" class="ltx_text ltx_font_bold">19.92</span></td>
<td id="S6.T4.18.18.18.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">17.71</td>
<td id="S6.T4.18.18.18.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">27.83</td>
<td id="S6.T4.18.18.18.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.27</td>
<td id="S6.T4.18.18.18.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">34.44</td>
<td id="S6.T4.18.18.18.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">35.03</td>
<td id="S6.T4.18.18.18.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">36.51</td>
</tr>
<tr id="S6.T4.21.21.30.7" class="ltx_tr">
<th id="S6.T4.21.21.30.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;" rowspan="4"><span id="S6.T4.21.21.30.7.1.1" class="ltx_text">zh</span></th>
<th id="S6.T4.21.21.30.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">M3P</th>
<td id="S6.T4.21.21.30.7.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">19.66</td>
<td id="S6.T4.21.21.30.7.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.30.7.4.1" class="ltx_text ltx_font_bold">27.76</span></td>
<td id="S6.T4.21.21.30.7.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.30.7.5.1" class="ltx_text ltx_font_bold">36.15</span></td>
<td id="S6.T4.21.21.30.7.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.30.7.6.1" class="ltx_text ltx_font_bold">38.21</span></td>
<td id="S6.T4.21.21.30.7.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.30.7.7.1" class="ltx_text ltx_font_bold">40.48</span></td>
<td id="S6.T4.21.21.30.7.8" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.30.7.8.1" class="ltx_text ltx_font_bold">40.53</span></td>
<td id="S6.T4.21.21.30.7.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.30.7.9.1" class="ltx_text ltx_font_bold">42.55</span></td>
</tr>
<tr id="S6.T4.19.19.19" class="ltx_tr">
<th id="S6.T4.19.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.19.19.19.1.m1.1" class="ltx_Math" alttext="{}^{\text{Emb}}" display="inline"><semantics id="S6.T4.19.19.19.1.m1.1a"><msup id="S6.T4.19.19.19.1.m1.1.1" xref="S6.T4.19.19.19.1.m1.1.1.cmml"><mi id="S6.T4.19.19.19.1.m1.1.1a" xref="S6.T4.19.19.19.1.m1.1.1.cmml"></mi><mtext id="S6.T4.19.19.19.1.m1.1.1.1" xref="S6.T4.19.19.19.1.m1.1.1.1a.cmml">Emb</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.19.19.19.1.m1.1b"><apply id="S6.T4.19.19.19.1.m1.1.1.cmml" xref="S6.T4.19.19.19.1.m1.1.1"><ci id="S6.T4.19.19.19.1.m1.1.1.1a.cmml" xref="S6.T4.19.19.19.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.19.19.19.1.m1.1.1.1.cmml" xref="S6.T4.19.19.19.1.m1.1.1.1">Emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.19.19.19.1.m1.1c">{}^{\text{Emb}}</annotation></semantics></math>
</th>
<td id="S6.T4.19.19.19.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">12.66</td>
<td id="S6.T4.19.19.19.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">14.77</td>
<td id="S6.T4.19.19.19.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">19.17</td>
<td id="S6.T4.19.19.19.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">22.13</td>
<td id="S6.T4.19.19.19.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">27.97</td>
<td id="S6.T4.19.19.19.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">29.08</td>
<td id="S6.T4.19.19.19.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">33.24</td>
</tr>
<tr id="S6.T4.20.20.20" class="ltx_tr">
<th id="S6.T4.20.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">OSCAR+<math id="S6.T4.20.20.20.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.20.20.20.1.m1.1a"><msup id="S6.T4.20.20.20.1.m1.1.1" xref="S6.T4.20.20.20.1.m1.1.1.cmml"><mi id="S6.T4.20.20.20.1.m1.1.1a" xref="S6.T4.20.20.20.1.m1.1.1.cmml"></mi><mtext id="S6.T4.20.20.20.1.m1.1.1.1" xref="S6.T4.20.20.20.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.20.20.20.1.m1.1b"><apply id="S6.T4.20.20.20.1.m1.1.1.cmml" xref="S6.T4.20.20.20.1.m1.1.1"><ci id="S6.T4.20.20.20.1.m1.1.1.1a.cmml" xref="S6.T4.20.20.20.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.20.20.20.1.m1.1.1.1.cmml" xref="S6.T4.20.20.20.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.20.20.20.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.20.20.20.2" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">13.20</td>
<td id="S6.T4.20.20.20.3" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">15.12</td>
<td id="S6.T4.20.20.20.4" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">19.67</td>
<td id="S6.T4.20.20.20.5" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">22.74</td>
<td id="S6.T4.20.20.20.6" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">26.81</td>
<td id="S6.T4.20.20.20.7" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">28.19</td>
<td id="S6.T4.20.20.20.8" class="ltx_td ltx_align_right" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">31.69</td>
</tr>
<tr id="S6.T4.21.21.21" class="ltx_tr">
<th id="S6.T4.21.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">mBERT<math id="S6.T4.21.21.21.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.T4.21.21.21.1.m1.1a"><msup id="S6.T4.21.21.21.1.m1.1.1" xref="S6.T4.21.21.21.1.m1.1.1.cmml"><mi id="S6.T4.21.21.21.1.m1.1.1a" xref="S6.T4.21.21.21.1.m1.1.1.cmml"></mi><mtext id="S6.T4.21.21.21.1.m1.1.1.1" xref="S6.T4.21.21.21.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.T4.21.21.21.1.m1.1b"><apply id="S6.T4.21.21.21.1.m1.1.1.cmml" xref="S6.T4.21.21.21.1.m1.1.1"><ci id="S6.T4.21.21.21.1.m1.1.1.1a.cmml" xref="S6.T4.21.21.21.1.m1.1.1.1"><mtext mathsize="70%" id="S6.T4.21.21.21.1.m1.1.1.1.cmml" xref="S6.T4.21.21.21.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.21.21.21.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<td id="S6.T4.21.21.21.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;"><span id="S6.T4.21.21.21.2.1" class="ltx_text ltx_font_bold">26.16</span></td>
<td id="S6.T4.21.21.21.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">23.47</td>
<td id="S6.T4.21.21.21.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">32.93</td>
<td id="S6.T4.21.21.21.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">35.82</td>
<td id="S6.T4.21.21.21.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">38.22</td>
<td id="S6.T4.21.21.21.7" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">37.89</td>
<td id="S6.T4.21.21.21.8" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-0.35pt;padding-bottom:-0.35pt;">39.57</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Average accuracy of few-shot results, utilizing different amounts of training data. The <span id="S6.T4.24.1" class="ltx_text ltx_font_italic">0</span> column presents the best zero-shot results. These models are used as initialization for the subsequent few-shot experiments. <span id="S6.T4.25.2" class="ltx_text ltx_font_bold">Bold</span> numbers indicate the best scores.</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Few-Shot Cross-Lingual Transfer</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.2" class="ltx_p">The main results of few-shot experiments are provided in Table <a href="#S6.T4" title="Table 4 ‣ 6.1 Zero-Shot Cross-Lingual Transfer ‣ 6 Results and Discussion ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, while the plot illustrating the impact of different amounts of training data is shown in Figure <a href="#S6.F5" title="Figure 5 ‣ 6.3 Language Transfer ‣ 6 Results and Discussion ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
One crucial finding is that, as expected, utilizing an increasing amount of data instances in the target language consistently improves accuracy for all methods.
This culminates in an improvement of up to 20 accuracy points when specializing the model with only 48 images in the target language. This indicates that a small number of target-language examples supports the models in partially repairing its internal cross-lingual multimodal alignment.
Interestingly, we find that with as little as 5 images, and their corresponding questions, M<sup id="S6.SS2.p1.2.1" class="ltx_sup">3</sup>P begins to outperform mBERT<math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><msup id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml"><mi id="S6.SS2.p1.2.m2.1.1a" xref="S6.SS2.p1.2.m2.1.1.cmml"></mi><mtext id="S6.SS2.p1.2.m2.1.1.1" xref="S6.SS2.p1.2.m2.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><apply id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1"><ci id="S6.SS2.p1.2.m2.1.1.1a.cmml" xref="S6.SS2.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S6.SS2.p1.2.m2.1.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">{}^{\text{Ada}}</annotation></semantics></math>—the best performing zero-shot model.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">We again analyze the impact of few-shot learning on accuracy across different structural question types, with the results depicted in Figure <a href="#S6.F4" title="Figure 4 ‣ 6.1 Zero-Shot Cross-Lingual Transfer ‣ 6 Results and Discussion ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The overall accuracy increases across all types compared to zero-shot scenarios (cf., Figure <a href="#S6.F3" title="Figure 3 ‣ 6.1 Zero-Shot Cross-Lingual Transfer ‣ 6 Results and Discussion ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). However, the most pronounced gains are reported for <span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_italic">query</span> and <span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_italic">chose</span>-type questions, on which the model performed the worst in zero-shot setups. This implies the improved alignment between latent multimodal and multilingual representations, achieved via fine-tuning the model on a small amount of examples in the target language.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Language Transfer</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">We witness cross-lingual transfer capability patterns similar to those shown by previous work, where our models perform best on typologically close languages <cite class="ltx_cite ltx_citemacro_cite">Pires et al. (<a href="#bib.bib44" title="" class="ltx_ref">2019</a>); Lauscher et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>. Our models transfer best to German (de) and Portuguese (pt), both being part of the Indo-European (IE) language family and also sharing the same script (Latin) with the source language English (en). We see a small drop in accuracy for Russian (ru), Indonesian (id), and Chinese (zh) and a larger drop in accuracy for Bengali (bn) and Korean (ko). All of these languages are typologically different to the source language and in most cases do not share the same script. These differences
highlight the importance of language diversity in cross-lingual transfer. Our benchmark thus enables experimentation and evaluation of multilingual multimodal models on a representative set of truly typologically diverse languages.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2109.06082/assets/x9.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="235" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Few-shot accuracy with different training dataset sizes of the different approaches. Scores are averaged over all languages.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Contemporary Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">With the recent rise in interest in multilingual vision and language learning, contemporary work has already further analyzed and extended the proposed xGQA dataset. We provide a brief description and pointers to this work in what follows.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Further Analysis.</span>
<cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> provide an extensive analysis of multilingual and multimodal models trained on cross-lingual visual question answering, and propose several approaches to mitigate the multilingual misalignment problem discussed in §<a href="#S6.SS1" title="6.1 Zero-Shot Cross-Lingual Transfer ‣ 6 Results and Discussion ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>. Their results suggest that standard approaches
taken from text-only cross-lingual transfer scenarios <cite class="ltx_cite ltx_citemacro_cite">Pires et al. (<a href="#bib.bib44" title="" class="ltx_ref">2019</a>); Hu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> do not leverage the full multilingual capability of the pretrained models. Interestingly, they find that a deeper prediction head does not have any measurable impact on the model’s performance in the source language, while at the same time it considerably improves zero-shot transfer results across all target languages.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">Translated Test Data.</span>
<cite class="ltx_cite ltx_citemacro_citet">Bugliarello et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> propose the first benchmark for transfer learning
across modalities, tasks, and languages, covering visual question answering,
cross-modal retrieval, grounded reasoning, and
grounded entailment tasks across 20 diverse languages. They extend the xGQA dataset by providing machine translated test-set questions and evaluate state-of-the-art monolingual multimodal models in a translate-test setup. In this setting, they achieve slightly better results. However, the performance remains to fall behind source language performance. The translate-test data can be found at <a target="_blank" href="https://iglue-benchmark.github.io/" title="" class="ltx_ref ltx_href">iglue-benchmark.github.io</a>.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We have proposed xGQA, a first cross-lingual evaluation benchmark for the visual question answering task. xGQA extends the English GQA dataset with development and test data in 7 more typologically diverse languages, covering 5 different scripts. As additional baselines, we have further proposed new adapter-based methods to extend unimodal multilingual models to become multimodal and—vice-versa—monolingual multimodal models to become multilingual. Our results have indicated that 1) efficient adapter-based methods slightly outperform the
pretrained multilingual multimodal model M<sup id="S8.p1.1.1" class="ltx_sup">3</sup>P in zero-shot scenarios, but 2) the overall zero-shot cross-lingual transfer yields harsh accuracy drops compared to the English performance for all models in comparison. Further, accuracy can be partially recovered via few-shot learning, where small amounts of training data are available in the target language. However, the large gaps remain, suggesting the inherent complexity of the cross-lingual task despite it being extremely intuitive and easy to solve by (bilingual) humans.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">We hope that our dataset and error analysis will motivate future work on this task and, more broadly, in the exciting emerging domain of multilingual multimodal representation learning.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The Ubiquitous Knowledge Processing Lab acknowledges the financial support of the German Federal Ministry of Education and Research (BMBF) under the promotional reference 13N15897 (MISRIK), and the LOEWE initiative (Hesse, Germany) within the emergenCITY center.
Jan-Martin O. Steitz is supported by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. The work of Ivan Vulić is supported by a Huawei research donation and the ERC PoC Grant MultiConvAI: Enabling Multilingual Conversational AI (no. 957356). Stefan Roth is additionally supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 866008).</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">We thank Leonardo F. R. Ribeiro, Ji-Ung Lee, and Chen Liu for insightful feedback and suggestions on a draft of this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and
L. Zhang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00636" title="" class="ltx_ref ltx_href">Bottom-up and
top-down attention for image captioning and visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 6077–6086.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ansell et al. (2021)</span>
<span class="ltx_bibblock">
Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran
Glavaš, Ivan Vulić, and Anna Korhonen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-emnlp.410" title="" class="ltx_ref ltx_href">MAD-G: Multilingual adapter generation for efficient cross-lingual
transfer</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pages 4762–4781, Punta Cana, Dominican Republic. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et al. (2020)</span>
<span class="ltx_bibblock">
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.421" title="" class="ltx_ref ltx_href">On the
cross-lingual transferability of monolingual representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4623–4637, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna and Firat (2019)</span>
<span class="ltx_bibblock">
Ankur Bapna and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1165" title="" class="ltx_ref ltx_href">Simple, scalable
adaptation for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1538–1548, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bugliarello et al. (2021)</span>
<span class="ltx_bibblock">
Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, and Desmond Elliott.
2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00408" title="" class="ltx_ref ltx_href">Multimodal Pretraining
Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language
BERTs</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:978–994.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bugliarello et al. (2022)</span>
<span class="ltx_bibblock">
Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott,
Edoardo Maria Ponti, and Ivan Vulić. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2201.11732" title="" class="ltx_ref ltx_href">IGLUE: A benchmark for
transfer learning across modalities, tasks, and languages</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-58452-8_13" title="" class="ltx_ref ltx_href">End-to-end
object detection with transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference,
Glasgow, UK, August 23-28, 2020, Proceedings, Part I</em>, volume 12346 of
<em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 213–229. Springer.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-58577-8_7" title="" class="ltx_ref ltx_href">UNITER:
universal image-text representation learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference,
Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX</em>, volume 12375 of
<em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 104–120. Springer.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="" class="ltx_ref ltx_href">Unsupervised
cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages
8440–8451. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n19-1423" title="" class="ltx_ref ltx_href">BERT: pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
1 (Long and Short Papers)</em>, pages 4171–4186. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=YicbFdNTTy" title="" class="ltx_ref ltx_href">An image is worth
16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichenberg et al. (2021)</span>
<span class="ltx_bibblock">
Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and
Anette Frank. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2112.05253" title="" class="ltx_ref ltx_href">MAGMA - multimodal
augmentation of generative models through adapter-based finetuning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2016)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W16-3210" title="" class="ltx_ref ltx_href">Multi30K:
Multilingual English-German image descriptions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Workshop on Vision and Language</em>,
pages 70–74, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frank et al. (2021)</span>
<span class="ltx_bibblock">
Stella Frank, Emanuele Bugliarello, and Desmond Elliott. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.775" title="" class="ltx_ref ltx_href">Vision-and-language or vision-for-language? on cross-modal influence in
multimodal transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana,
Dominican Republic, 7-11 November, 2021</em>, pages 9847–9857. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2021)</span>
<span class="ltx_bibblock">
Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang,
and Zicheng Liu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2111.12681" title="" class="ltx_ref ltx_href">VIOLET : End-to-end
video-language transformers with masked visual-token modeling</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2020)</span>
<span class="ltx_bibblock">
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/49562478de4c54fafd4ec46fdb297de5-Abstract.html" title="" class="ltx_ref ltx_href">Large-scale adversarial training for vision-and-language representation
learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2015)</span>
<span class="ltx_bibblock">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://dl.acm.org/doi/10.5555/2969442.2969496" title="" class="ltx_ref ltx_href">Are you
talking to a machine? dataset and methods for multilingual image question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on Neural
Information Processing Systems - Volume 2</em>, NIPS’15, page 2296–2304,
Cambridge, MA, USA. MIT Press.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geigle et al. (2022)</span>
<span class="ltx_bibblock">
Gregor Geigle, Jonas Pfeiffer, Nils Reimers, Ivan Vulic, and Iryna Gurevych.
2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2103.11920" title="" class="ltx_ref ltx_href">Retrieve fast, rerank smart:
Cooperative and joint approaches for improved cross-modal retrieval</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v97/houlsby19a.html" title="" class="ltx_ref ltx_href">Parameter-efficient transfer learning for NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA</em>,
volume 97 of <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages
2790–2799. PMLR.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2020)</span>
<span class="ltx_bibblock">
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
Melvin Johnson. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v119/hu20b.html" title="" class="ltx_ref ltx_href">XTREME: A
massively multilingual multi-task benchmark for evaluating cross-lingual
generalisation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine
Learning</em>, volume 119 of <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
pages 4411–4421. PMLR.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2021)</span>
<span class="ltx_bibblock">
Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, and
Alexander Hauptmann. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.195" title="" class="ltx_ref ltx_href">Multilingual
multimodal pre-training for zero-shot cross-lingual transfer of
vision-language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 2443–2459, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00686" title="" class="ltx_ref ltx_href">GQA: A new
dataset for real-world visual reasoning and compositional question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
6700–6709. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2021)</span>
<span class="ltx_bibblock">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham,
Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v139/jia21b.html" title="" class="ltx_ref ltx_href">Scaling up
visual and vision-language representation learning with noisy text
supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of
<em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 4904–4916. PMLR.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">K et al. (2020)</span>
<span class="ltx_bibblock">
Karthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=HJeT3yrtDr" title="" class="ltx_ref ltx_href">Cross-lingual
ability of multilingual BERT: an empirical study</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International Conference on Learning
Representations (ICLR)</em>, Addis Ababa, Ethiopia. OpenReview.net.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath et al. (2021)</span>
<span class="ltx_bibblock">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and
Nicolas Carion. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2104.12763" title="" class="ltx_ref ltx_href">MDETR - modulated
detection for end-to-end multi-modal understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Computer Vision,
ICCV 2021, Online, October 10-17, 2021</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Li Fei-Fei. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s11263-016-0981-7" title="" class="ltx_ref ltx_href">Visual genome:
Connecting language and vision using crowdsourced dense image annotations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123(1):32–73.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lauscher et al. (2020)</span>
<span class="ltx_bibblock">
Anne Lauscher, Vinit Ravishankar, Ivan Vulić, and Goran Glavaš. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.363" title="" class="ltx_ref ltx_href">From zero to
hero: On the limitations of zero-shot language transfer with multilingual
Transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 4483–4499, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. (2021)</span>
<span class="ltx_bibblock">
Hang Le, Juan Miguel Pino, Changhan Wang, Jiatao Gu, Didier Schwab, and Laurent
Besacier. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-short.103" title="" class="ltx_ref ltx_href">Lightweight
adapter tuning for multilingual speech translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers),
Virtual Event, August 1-6, 2021</em>, pages 817–824. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aaai.org/ojs/index.php/AAAI/article/view/6795" title="" class="ltx_ref ltx_href">Unicoder-vl: A universal encoder for vision and language by cross-modal
pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of
Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium
on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020</em>, pages 11336–11344. AAAI Press.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang,
and Jianfeng Gao. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.microsoft.com/en-us/research/publication/grounded-language-image-pre-training/" title="" class="ltx_ref ltx_href">Grounded language-image pre-training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.
2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-58577-8_8" title="" class="ltx_ref ltx_href">Oscar:
Object-semantics aligned pre-training for vision-language tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference,
Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX</em>, volume 12375 of
<em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 121–137. Springer.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona,
Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-319-10602-1_48" title="" class="ltx_ref ltx_href">Microsoft
COCO: common objects in context</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2014 - 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V</em>, volume
8693 of <em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 740–755. Springer.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Chen Liu, Jonas Pfeiffer, Anna Korhonen, Ivan Vulić, and Iryna Gurevych. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2202.07630" title="" class="ltx_ref ltx_href">Delving deeper into
cross-lingual visual question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel
Collier, and Desmond Elliott. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2109.13238" title="" class="ltx_ref ltx_href">Visually grounded reasoning
across languages and cultures</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Online, November , 2021</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1907.11692" title="" class="ltx_ref ltx_href">Roberta: A robustly
optimized BERT pretraining approach</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html" title="" class="ltx_ref ltx_href">Vilbert: Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada</em>, pages 13–23.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miech et al. (2021)</span>
<span class="ltx_bibblock">
Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew
Zisserman. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Miech_Thinking_Fast_and_Slow_Efficient_Text-to-Visual_Retrieval_With_Transformers_CVPR_2021_paper.html" title="" class="ltx_ref ltx_href">Thinking fast and slow: Efficient text-to-visual retrieval with
transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2021, virtual, June 19-25, 2021</em>, pages 9826–9836.
Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al. (2021)</span>
<span class="ltx_bibblock">
Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang,
Dongdong Zhang, and Nan Duan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Ni_M3P_Learning_Universal_Representations_via_Multitask_Multilingual_Multimodal_Pre-Training_CVPR_2021_paper.html" title="" class="ltx_ref ltx_href">M3P: learning universal representations via multitask multilingual
multimodal pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2021, virtual, June 19-25, 2021</em>, pages 3977–3986.
Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordonez et al. (2011)</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html" title="" class="ltx_ref ltx_href">Im2text: Describing images using 1 million captioned photographs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 24: 25th
Annual Conference on Neural Information Processing Systems 2011. Proceedings
of a meeting held 12-14 December 2011, Granada, Spain</em>, pages 1143–1151.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2020a)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan
Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych.
2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-demos.7" title="" class="ltx_ref ltx_href">AdapterHub: A framework for adapting transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 46–54, Online.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2020b)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder.
2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.617" title="" class="ltx_ref ltx_href">MAD-X:
An Adapter-Based Framework for Multi-Task Cross-Lingual
Transfer</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 7654–7673, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2021)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.800" title="" class="ltx_ref ltx_href">UNKs
Everywhere: Adapting Multilingual Language Models to New Scripts</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana,
Dominican Republic, 7-11 November, 2021</em>, pages 10186–10203. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Philip et al. (2020)</span>
<span class="ltx_bibblock">
Jerin Philip, Alexandre Berard, Matthias Gallé, and Laurent Besacier. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.361" title="" class="ltx_ref ltx_href">Monolingual
adapters for zero-shot neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 4465–4470, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires et al. (2019)</span>
<span class="ltx_bibblock">
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1493" title="" class="ltx_ref ltx_href">How multilingual is
multilingual BERT?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4996–5001, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. (2015)</span>
<span class="ltx_bibblock">
Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia
Hockenmaier, and Svetlana Lazebnik. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2015.303" title="" class="ltx_ref ltx_href">Flickr30k entities:
Collecting region-to-phrase correspondences for richer image-to-sentence
models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015</em>, pages 2641–2649.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poth et al. (2021)</span>
<span class="ltx_bibblock">
Clifton Poth, Jonas Pfeiffer, Andreas Rücklé, and Iryna Gurevych.
2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.08247" title="" class="ltx_ref ltx_href">What to Pre-Train on?
Efficient Intermediate Task Selection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Online, November , 2021</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_href">Learning
transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of
<em id="bib.bib47.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rebuffi et al. (2017)</span>
<span class="ltx_bibblock">
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html" title="" class="ltx_ref ltx_href">Learning multiple visual domains with residual adapters</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA</em>, pages 506–516.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html" title="" class="ltx_ref ltx_href">Faster R-CNN: towards real-time object detection with region proposal
networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015, December 7-12,
2015, Montreal, Quebec, Canada</em>, pages 91–99.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rücklé et al. (2021)</span>
<span class="ltx_bibblock">
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas
Pfeiffer, Nils Reimers, and Iryna Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.626" title="" class="ltx_ref ltx_href">AdapterDrop: On the Efficiency of Adapters in Transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana,
Dominican Republic, 7-11 November, 2021</em>, pages 7930–7946. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rücklé et al. (2020)</span>
<span class="ltx_bibblock">
Andreas Rücklé, Jonas Pfeiffer, and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.194" title="" class="ltx_ref ltx_href">MultiCQA: Zero-shot transfer of self-supervised text matching models on
a massive scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2471–2486, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust et al. (2021)</span>
<span class="ltx_bibblock">
Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna
Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2012.15613" title="" class="ltx_ref ltx_href">How Good is Your
Tokenizer? On the Monolingual Performance of Multilingual Language Models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics, ACL 2021, Online, August 1-6, 2021</em>.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schamoni et al. (2018)</span>
<span class="ltx_bibblock">
Shigehiko Schamoni, Julian Hitschler, and Stefan Riezler. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W18-1814/" title="" class="ltx_ref ltx_href">A dataset and reranking
method for multimodal MT of user-generated image captions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th Conference of the Association for
Machine Translation in the Americas, AMTA 2018, Boston, MA, USA, March
17-21, 2018 - Volume 1: Research Papers</em>, pages 140–153. Association for
Machine Translation in the Americas.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1238" title="" class="ltx_ref ltx_href">Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2556–2565,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shimizu et al. (2018)</span>
<span class="ltx_bibblock">
Nobuyuki Shimizu, Na Rong, and Takashi Miyazaki. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/C18-1163/" title="" class="ltx_ref ltx_href">Visual question answering
dataset for bilingual image understanding: A study of cross-lingual
transfer using attention maps</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th International Conference on
Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August
20-26, 2018</em>, pages 1918–1928. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2021)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
Galuba, Marcus Rohrbach, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2112.04482" title="" class="ltx_ref ltx_href">FLAVA: A foundational
language and vision alignment model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivasan et al. (2021)</span>
<span class="ltx_bibblock">
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc
Najork. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3404835.3463257" title="" class="ltx_ref ltx_href">WIT:
wikipedia-based image text dataset for multimodal multilingual machine
learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">SIGIR ’21: The 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval, Virtual Event, Canada,
July 11-15, 2021</em>, pages 2443–2449. ACM.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2021)</span>
<span class="ltx_bibblock">
Lin Su, Nan Duan, Edward Cui, Lei Ji, Chenfei Wu, Huaishao Luo, Yongfei Liu,
Ming Zhong, Taroon Bharti, and Arun Sacheti. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-acl.229" title="" class="ltx_ref ltx_href">GEM: A
general evaluation benchmark for multimodal tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, pages 2594–2603.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr et al. (2019)</span>
<span class="ltx_bibblock">
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1644" title="" class="ltx_ref ltx_href">A corpus for reasoning
about natural language grounded in photographs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 6418–6428, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1514" title="" class="ltx_ref ltx_href">LXMERT: learning
cross-modality encoder representations from transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November
3-7, 2019</em>, pages 5099–5110. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Üstün et al. (2020)</span>
<span class="ltx_bibblock">
Ahmet Üstün, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.180" title="" class="ltx_ref ltx_href">UDapter:
Language adaptation for truly Universal Dependency parsing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2302–2315, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA</em>, pages 5998–6008.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021a)</span>
<span class="ltx_bibblock">
Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang, Xiyang Dai, Zicheng Liu,
Yumao Lu, and Lijuan Wang. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2111.10023" title="" class="ltx_ref ltx_href">UFO: A unified
transformer for vision-language representation learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021b)</span>
<span class="ltx_bibblock">
Josiah Wang, Pranava Madhyastha, Josiel Figueiredo, Chiraag Lala, and Lucia
Specia. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2103.01910" title="" class="ltx_ref ltx_href">Multisubs: A large-scale
multimodal and multilingual dataset</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Dredze (2019)</span>
<span class="ltx_bibblock">
Shijie Wu and Mark Dredze. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1077" title="" class="ltx_ref ltx_href">Beto, bentz, becas: The
surprising cross-lingual effectiveness of BERT</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 833–844, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu,
Yumao Lu, and Lijuan Wang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2111.12085" title="" class="ltx_ref ltx_href">Crossing the format boundary
of text and boxes: Towards unified vision-language modeling</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2021)</span>
<span class="ltx_bibblock">
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu,
Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen
Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2111.11432" title="" class="ltx_ref ltx_href">Florence: A new foundation
model for computer vision</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.pdf" title="" class="ltx_ref ltx_href">VinVL: Revisiting Visual Representations in Vision-Language Models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pages 5579–5588.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2021)</span>
<span class="ltx_bibblock">
Mingyang Zhou, Luowei Zhou, Shuohang Wang, Yu Cheng, Linjie Li, Zhou Yu, and
Jingjing Liu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.html" title="" class="ltx_ref ltx_href">UC2: universal cross-lingual cross-modal vision-and-language
pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2021, virtual, June 19-25, 2021</em>, pages 4155–4165.
Computer Vision Foundation / IEEE.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We experiment with different multimodal adapter architectures as illustrated in Figure <a href="#A1.F6" title="Figure 6 ‣ Appendix A Appendix ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. In initial experiments we find that splitting the modalities (settings 2-5) outperforms a joint adapter (setting 1). However, a joint "alignment" architectures (settings 4-5) outperform settings where we only use modality-specific adapters (settings 2-3). We more thoroughly investigate settings 4-5 and report scores in Table <a href="#A1.T5" title="Table 5 ‣ Appendix A Appendix ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Interestingly, we find that when only using the language adapter for the textual inputs, cross-lingual accuracy drops for both OSCAR+ and mBERT; The difference is more pronounced for OSCAR+. We speculate that this is due to a latent misalignment of the representation spaces, partly due to the residual connection. Due to the better performance of setting 5 on average, we have reported scores of this architecture in the main paper (as illustrated in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Baselines ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="A1.T5" class="ltx_table">
<div id="A1.T5.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:90.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(1.9pt,-0.4pt) scale(1.00900339712046,1.00900339712046) ;">
<table id="A1.T5.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.4.4.5.1" class="ltx_tr">
<th id="A1.T5.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">model</th>
<th id="A1.T5.4.4.5.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Setting</th>
<th id="A1.T5.4.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">en</th>
<th id="A1.T5.4.4.5.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">de</th>
<th id="A1.T5.4.4.5.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">pt</th>
<th id="A1.T5.4.4.5.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">ru</th>
<th id="A1.T5.4.4.5.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">id</th>
<th id="A1.T5.4.4.5.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">bn</th>
<th id="A1.T5.4.4.5.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">ko</th>
<th id="A1.T5.4.4.5.1.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">zh</th>
<th id="A1.T5.4.4.5.1.11" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">mean</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.1.1.1" class="ltx_tr">
<th id="A1.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">OSCAR+<math id="A1.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="A1.T5.1.1.1.1.m1.1a"><msup id="A1.T5.1.1.1.1.m1.1.1" xref="A1.T5.1.1.1.1.m1.1.1.cmml"><mi id="A1.T5.1.1.1.1.m1.1.1a" xref="A1.T5.1.1.1.1.m1.1.1.cmml"></mi><mtext id="A1.T5.1.1.1.1.m1.1.1.1" xref="A1.T5.1.1.1.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.m1.1b"><apply id="A1.T5.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1"><ci id="A1.T5.1.1.1.1.m1.1.1.1a.cmml" xref="A1.T5.1.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T5.1.1.1.1.m1.1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<th id="A1.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">4</th>
<th id="A1.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">60.21</th>
<td id="A1.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t">18.60</td>
<td id="A1.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t">25.48</td>
<td id="A1.T5.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t">8.22</td>
<td id="A1.T5.1.1.1.7" class="ltx_td ltx_align_left ltx_border_t">17.79</td>
<td id="A1.T5.1.1.1.8" class="ltx_td ltx_align_left ltx_border_t">10.47</td>
<td id="A1.T5.1.1.1.9" class="ltx_td ltx_align_left ltx_border_t">9.97</td>
<td id="A1.T5.1.1.1.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">12.54</td>
<td id="A1.T5.1.1.1.11" class="ltx_td ltx_align_left ltx_border_t">14.72</td>
</tr>
<tr id="A1.T5.2.2.2" class="ltx_tr">
<th id="A1.T5.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">OSCAR+<math id="A1.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="A1.T5.2.2.2.1.m1.1a"><msup id="A1.T5.2.2.2.1.m1.1.1" xref="A1.T5.2.2.2.1.m1.1.1.cmml"><mi id="A1.T5.2.2.2.1.m1.1.1a" xref="A1.T5.2.2.2.1.m1.1.1.cmml"></mi><mtext id="A1.T5.2.2.2.1.m1.1.1.1" xref="A1.T5.2.2.2.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.1.m1.1b"><apply id="A1.T5.2.2.2.1.m1.1.1.cmml" xref="A1.T5.2.2.2.1.m1.1.1"><ci id="A1.T5.2.2.2.1.m1.1.1.1a.cmml" xref="A1.T5.2.2.2.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T5.2.2.2.1.m1.1.1.1.cmml" xref="A1.T5.2.2.2.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<th id="A1.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<th id="A1.T5.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A1.T5.2.2.2.3.1" class="ltx_text ltx_font_bold">60.30</span></th>
<td id="A1.T5.2.2.2.4" class="ltx_td ltx_align_left"><span id="A1.T5.2.2.2.4.1" class="ltx_text ltx_font_bold">18.91</span></td>
<td id="A1.T5.2.2.2.5" class="ltx_td ltx_align_left"><span id="A1.T5.2.2.2.5.1" class="ltx_text ltx_font_bold">27.02</span></td>
<td id="A1.T5.2.2.2.6" class="ltx_td ltx_align_left"><span id="A1.T5.2.2.2.6.1" class="ltx_text ltx_font_bold">17.50</span></td>
<td id="A1.T5.2.2.2.7" class="ltx_td ltx_align_left"><span id="A1.T5.2.2.2.7.1" class="ltx_text ltx_font_bold">18.77</span></td>
<td id="A1.T5.2.2.2.8" class="ltx_td ltx_align_left"><span id="A1.T5.2.2.2.8.1" class="ltx_text ltx_font_bold">15.42</span></td>
<td id="A1.T5.2.2.2.9" class="ltx_td ltx_align_left"><span id="A1.T5.2.2.2.9.1" class="ltx_text ltx_font_bold">15.28</span></td>
<td id="A1.T5.2.2.2.10" class="ltx_td ltx_align_left ltx_border_r"><span id="A1.T5.2.2.2.10.1" class="ltx_text ltx_font_bold">14.96</span></td>
<td id="A1.T5.2.2.2.11" class="ltx_td ltx_align_left"><span id="A1.T5.2.2.2.11.1" class="ltx_text ltx_font_bold">18.27</span></td>
</tr>
<tr id="A1.T5.3.3.3" class="ltx_tr">
<th id="A1.T5.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="A1.T5.3.3.3.1.1" class="ltx_ERROR undefined">\hdashline</span>mBERT<math id="A1.T5.3.3.3.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="A1.T5.3.3.3.1.m1.1a"><msup id="A1.T5.3.3.3.1.m1.1.1" xref="A1.T5.3.3.3.1.m1.1.1.cmml"><mi id="A1.T5.3.3.3.1.m1.1.1a" xref="A1.T5.3.3.3.1.m1.1.1.cmml"></mi><mtext id="A1.T5.3.3.3.1.m1.1.1.1" xref="A1.T5.3.3.3.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.1.m1.1b"><apply id="A1.T5.3.3.3.1.m1.1.1.cmml" xref="A1.T5.3.3.3.1.m1.1.1"><ci id="A1.T5.3.3.3.1.m1.1.1.1a.cmml" xref="A1.T5.3.3.3.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T5.3.3.3.1.m1.1.1.1.cmml" xref="A1.T5.3.3.3.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<th id="A1.T5.3.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<th id="A1.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A1.T5.3.3.3.3.1" class="ltx_text ltx_font_bold">57.83</span></th>
<td id="A1.T5.3.3.3.4" class="ltx_td ltx_align_left">27.86</td>
<td id="A1.T5.3.3.3.5" class="ltx_td ltx_align_left">28.88</td>
<td id="A1.T5.3.3.3.6" class="ltx_td ltx_align_left">22.87</td>
<td id="A1.T5.3.3.3.7" class="ltx_td ltx_align_left"><span id="A1.T5.3.3.3.7.1" class="ltx_text ltx_font_bold">20.86</span></td>
<td id="A1.T5.3.3.3.8" class="ltx_td ltx_align_left">14.74</td>
<td id="A1.T5.3.3.3.9" class="ltx_td ltx_align_left">18.30</td>
<td id="A1.T5.3.3.3.10" class="ltx_td ltx_align_left ltx_border_r">24.39</td>
<td id="A1.T5.3.3.3.11" class="ltx_td ltx_align_left">22.56</td>
</tr>
<tr id="A1.T5.4.4.4" class="ltx_tr">
<th id="A1.T5.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">mBERT<math id="A1.T5.4.4.4.1.m1.1" class="ltx_Math" alttext="{}^{\text{Ada}}" display="inline"><semantics id="A1.T5.4.4.4.1.m1.1a"><msup id="A1.T5.4.4.4.1.m1.1.1" xref="A1.T5.4.4.4.1.m1.1.1.cmml"><mi id="A1.T5.4.4.4.1.m1.1.1a" xref="A1.T5.4.4.4.1.m1.1.1.cmml"></mi><mtext id="A1.T5.4.4.4.1.m1.1.1.1" xref="A1.T5.4.4.4.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.1.m1.1b"><apply id="A1.T5.4.4.4.1.m1.1.1.cmml" xref="A1.T5.4.4.4.1.m1.1.1"><ci id="A1.T5.4.4.4.1.m1.1.1.1a.cmml" xref="A1.T5.4.4.4.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T5.4.4.4.1.m1.1.1.1.cmml" xref="A1.T5.4.4.4.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.4.1.m1.1c">{}^{\text{Ada}}</annotation></semantics></math>
</th>
<th id="A1.T5.4.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">5</th>
<th id="A1.T5.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">56.25</th>
<td id="A1.T5.4.4.4.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="A1.T5.4.4.4.4.1" class="ltx_text ltx_font_bold">29.76</span></td>
<td id="A1.T5.4.4.4.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="A1.T5.4.4.4.5.1" class="ltx_text ltx_font_bold">30.37</span></td>
<td id="A1.T5.4.4.4.6" class="ltx_td ltx_align_left ltx_border_bb"><span id="A1.T5.4.4.4.6.1" class="ltx_text ltx_font_bold">24.42</span></td>
<td id="A1.T5.4.4.4.7" class="ltx_td ltx_align_left ltx_border_bb">19.15</td>
<td id="A1.T5.4.4.4.8" class="ltx_td ltx_align_left ltx_border_bb"><span id="A1.T5.4.4.4.8.1" class="ltx_text ltx_font_bold">15.12</span></td>
<td id="A1.T5.4.4.4.9" class="ltx_td ltx_align_left ltx_border_bb"><span id="A1.T5.4.4.4.9.1" class="ltx_text ltx_font_bold">19.09</span></td>
<td id="A1.T5.4.4.4.10" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="A1.T5.4.4.4.10.1" class="ltx_text ltx_font_bold">24.86</span></td>
<td id="A1.T5.4.4.4.11" class="ltx_td ltx_align_left ltx_border_bb"><span id="A1.T5.4.4.4.11.1" class="ltx_text ltx_font_bold">23.25</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Zero-shot transfer results on xGQA for the different adapter architecture settings (as illustrated in Figure <a href="#A1.F6" title="Figure 6 ‣ Appendix A Appendix ‣ xGQA: Cross-Lingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) when transferring from English GQA. Average accuracy is reported. Best results for each language and model type are highlighted in <span id="A1.T5.7.1" class="ltx_text ltx_font_bold">bold</span>; <span id="A1.T5.8.2" class="ltx_text ltx_font_italic">mean</span> scores are not averaged over the source language (English).</figcaption>
</figure>
<figure id="A1.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x10.png" id="A1.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Setting 1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x11.png" id="A1.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Setting 2</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x12.png" id="A1.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="335" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Setting 3</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x13.png" id="A1.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="414" height="395" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Setting 4</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F6.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.06082/assets/x14.png" id="A1.F6.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>Setting 5</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The different multimodal multilingual adapter architectures we experimented with. The best performing architecture was setting 5, for which we present results in the main paper. </figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.06081" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.06082" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.06082">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.06082" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.06083" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 03:52:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
