<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal</title>
<!--Generated on Thu Oct  3 09:40:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02337v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S1" title="In Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2" title="In Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Materials and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS1" title="In 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS1.SSS1" title="In 2.1 Data ‣ 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Data description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS1.SSS2" title="In 2.1 Data ‣ 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Image acquisition</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS2" title="In 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS3" title="In 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Deep learning networks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS4" title="In 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS5" title="In 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Breast boundaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.SS6" title="In 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Setup</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3" title="In Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.SS1" title="In 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.SS2" title="In 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Model performance and generalizability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.SS3" title="In 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Internal breast segmentation and boundary detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.SS4" title="In 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Training and inference time</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S4" title="In Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S5" title="In Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">[ 1,2,3]<span class="ltx_ERROR undefined" id="p1.1.1">\fnm</span>Sam <span class="ltx_ERROR undefined" id="p1.1.2">\sur</span>Narimani</p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">1]<span class="ltx_ERROR undefined" id="p2.1.1">\orgdiv</span><span class="ltx_text" id="p2.1.2" style="font-size:90%;"> Department of Physics, <span class="ltx_ERROR undefined" id="p2.1.2.1">\orgname</span>Norwegian University of Science and Technology, <span class="ltx_ERROR undefined" id="p2.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p2.1.2.3">\city</span>Trondheim, <span class="ltx_ERROR undefined" id="p2.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">2]<span class="ltx_ERROR undefined" id="p3.1.1">\orgdiv</span><span class="ltx_text" id="p3.1.2" style="font-size:90%;"> Research and Development Department, <span class="ltx_ERROR undefined" id="p3.1.2.1">\orgname</span>More og Romsdal Hospital Trust, <span class="ltx_ERROR undefined" id="p3.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p3.1.2.3">\city</span>Aalesund, <span class="ltx_ERROR undefined" id="p3.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">3]<span class="ltx_ERROR undefined" id="p4.1.1">\orgdiv</span><span class="ltx_text" id="p4.1.2" style="font-size:90%;"> Department of Radiology, <span class="ltx_ERROR undefined" id="p4.1.2.1">\orgname</span>More og Romsdal Hospital Trust, <span class="ltx_ERROR undefined" id="p4.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p4.1.2.3">\city</span>Aalesund, <span class="ltx_ERROR undefined" id="p4.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">4]<span class="ltx_ERROR undefined" id="p5.1.1">\orgdiv</span><span class="ltx_text" id="p5.1.2" style="font-size:90%;"> Department of Circulation and Medical Imaging, <span class="ltx_ERROR undefined" id="p5.1.2.1">\orgname</span>Norwegian University of Science and Technology, <span class="ltx_ERROR undefined" id="p5.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p5.1.2.3">\city</span>Trondheim, <span class="ltx_ERROR undefined" id="p5.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">5] <span class="ltx_ERROR undefined" id="p6.1.1">\orgdiv</span><span class="ltx_text" id="p6.1.2" style="font-size:90%;"> Department of Electrical Engineering and Computer Science, <span class="ltx_ERROR undefined" id="p6.1.2.1">\orgname</span>The University of Stavanger, <span class="ltx_ERROR undefined" id="p6.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p6.1.2.3">\city</span>Stavanger, <span class="ltx_ERROR undefined" id="p6.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p" id="p7.1">6] <span class="ltx_ERROR undefined" id="p7.1.1">\orgdiv</span><span class="ltx_text" id="p7.1.2" style="font-size:90%;"> Department of Radiology, <span class="ltx_ERROR undefined" id="p7.1.2.1">\orgname</span>Stavanger University Hospital, <span class="ltx_ERROR undefined" id="p7.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p7.1.2.3">\city</span>Stavanger, <span class="ltx_ERROR undefined" id="p7.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p8">
<p class="ltx_p" id="p8.1">7] <span class="ltx_ERROR undefined" id="p8.1.1">\orgdiv</span><span class="ltx_text" id="p8.1.2" style="font-size:90%;"> Department of Diagnostic Imaging, <span class="ltx_ERROR undefined" id="p8.1.2.1">\orgname</span>Akershus University Hospital, <span class="ltx_ERROR undefined" id="p8.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p8.1.2.3">\city</span>Lorenskog, <span class="ltx_ERROR undefined" id="p8.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p9">
<p class="ltx_p" id="p9.1">8] <span class="ltx_ERROR undefined" id="p9.1.1">\orgname</span><span class="ltx_text" id="p9.1.2" style="font-size:90%;"> NordicCAD AS, <span class="ltx_ERROR undefined" id="p9.1.2.1">\orgaddress</span> <span class="ltx_ERROR undefined" id="p9.1.2.2">\city</span>Aalesund, <span class="ltx_ERROR undefined" id="p9.1.2.3">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p10">
<p class="ltx_p" id="p10.1">9] <span class="ltx_ERROR undefined" id="p10.1.1">\orgdiv</span><span class="ltx_text" id="p10.1.2" style="font-size:90%;"> Institute of Clinical Medicine, <span class="ltx_ERROR undefined" id="p10.1.2.1">\orgname</span>University of Oslo, <span class="ltx_ERROR undefined" id="p10.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p10.1.2.3">\city</span>Lorenskog, <span class="ltx_ERROR undefined" id="p10.1.2.4">\country</span>Norway</span></p>
</div>
<div class="ltx_para" id="p11">
<p class="ltx_p" id="p11.1">10] <span class="ltx_ERROR undefined" id="p11.1.1">\orgdiv</span><span class="ltx_text" id="p11.1.2" style="font-size:90%;"> Department of Oncology, <span class="ltx_ERROR undefined" id="p11.1.2.1">\orgname</span>Akershus University Hospital, <span class="ltx_ERROR undefined" id="p11.1.2.2">\orgaddress</span> <span class="ltx_ERROR undefined" id="p11.1.2.3">\city</span>Lorenskog, <span class="ltx_ERROR undefined" id="p11.1.2.4">\country</span>Norway</span></p>
</div>
<h1 class="ltx_title ltx_font_bold ltx_title_document"> Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sam.narimania@gmail.com%20">sam.narimania@gmail.com </a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\fnm</span>Solveig <span class="ltx_ERROR undefined" id="id2.2.id2">\sur</span>Roth Hoff
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:solveig.roth.hoff@ntnu.no">solveig.roth.hoff@ntnu.no</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id3.1.id1">\fnm</span>Kathinka <span class="ltx_ERROR undefined" id="id4.2.id2">\sur</span>Dæhli Kurz
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kathinka.kurz@uis.no">kathinka.kurz@uis.no</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id5.1.id1">\fnm</span>Kjell-Inge <span class="ltx_ERROR undefined" id="id6.2.id2">\sur</span>Gjesdal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kjell.inge.gjesdal@nordiccad.com">kjell.inge.gjesdal@nordiccad.com</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id7.1.id1">\fnm</span>Jurgen <span class="ltx_ERROR undefined" id="id8.2.id2">\sur</span>Geisler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jurgen.geisler@medisin.uio.no">jurgen.geisler@medisin.uio.no</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id9.1.id1">\fnm</span>Endre <span class="ltx_ERROR undefined" id="id10.2.id2">\sur</span>Grovik
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:endre.grovik@gmail.com">endre.grovik@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1"><span class="ltx_text ltx_font_bold" id="id11.id1.1">Purpose:</span> Segmentation of the breast region in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is essential for the automatic measurement of breast density and the quantitative analysis of imaging findings. This study aims to compare various deep learning methods to enhance whole breast segmentation and reduce computational costs as well as environmental effect for future research.</p>
<p class="ltx_p" id="id12.id2"><span class="ltx_text ltx_font_bold" id="id12.id2.1">Methods:</span> We collected fifty-nine DCE-MRI scans from Stavanger University Hospital and, after preprocessing, analyzed fifty-eight scans. The preprocessing steps involved standardizing imaging protocols and resampling slices to ensure consistent volume across all patients. Using our novel approach, we defined new breast boundaries and generated corresponding segmentation masks. We evaluated seven deep learning models for segmentation namely UNet, UNet++, DenseNet, FCNResNet50, FCNResNet101, DeepLabv3ResNet50, and DeepLabv3ResNet101. To ensure robust model validation, we employed 10-fold cross-validation, dividing the dataset into ten subsets, training on nine, and validating on the remaining one, rotating this process to use all subsets for validation.</p>
<p class="ltx_p" id="id13.id3"><span class="ltx_text ltx_font_bold" id="id13.id3.1">Results:</span> The models demonstrated significant potential across multiple metrics. UNet++ achieved the highest performance in Dice score, while UNet excelled in validation and generalizability. FCNResNet50, notable for its lower carbon footprint and reasonable inference time, emerged as a robust model following UNet++. In boundary detection, both UNet and UNet++ outperformed other models, with DeepLabv3ResNet also delivering competitive results.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Breast Region, DCE-MRI, Deep Learning methods, Segmentation
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Whole breast segmentation is a pivotal step in assessing the risk of breast cancer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib1" title="">1</a>]</cite>. Accurate breast region segmentation can be beneficial not only for lowering computational cost for predicting breast cancer but also to focus on quantitative analysis of breast cancer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib3" title="">3</a>]</cite>. Among all medical imaging modalities, MRI plays a significant role in high quality visualization of the whole breast <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib4" title="">4</a>]</cite>. However, interpretation of breast MRI can be challenging due to noise and artifacts from surrounding anatomical structures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In recent years, deep learning (DL) techniques have emerged as powerful tools in medical image analysis, offering the potential to automate and improve various tasks such as segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib8" title="">8</a>]</cite>. Segmentation, the process of partitioning an image into multiple regions or segments, is a fundamental step in medical image analysis. In fact, it enables a fast and automatic delineation of Region of Interest (ROI), such as tumors or anatomical regions, with high precision and accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib9" title="">9</a>]</cite>. Segmentation of the breast region in MRI images makes it possible to create automatic models for breast density measurement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib10" title="">10</a>]</cite>. This process not only enhances the efficiency of data processing but also contributes to the rapid training and analysis of AI models, promoting more environmentally sustainable data processing. Furthermore, precise segmentation of the breast region facilitates the localization and characterization of abnormalities, thereby assisting radiologists in their diagnostic decision-making process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the last decade, significant advancements have been made in breast region segmentation due to the development of numerous AI architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib14" title="">14</a>]</cite>.
These advancements mark a notable shift in segmentation techniques, transitioning from traditional feature-based machine learning methods, such as clustering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib16" title="">16</a>]</cite>, to more advanced deep learning approaches, including UNet and its variants <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib17" title="">17</a>]</cite>. These efforts have led to improved diagnostics and more precise stratification of breast cancer tumors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib18" title="">18</a>]</cite>. Despite these advances, no comparative study has been conducted to evaluate the performance of well-known DL methods for breast region segmentation. Therefore, our study aims to fill this gap by comparing seven prominent DL architectures for segmenting breast regions in DCE-MRI. The goal is to identify the most competitive network that minimizes computational costs while effectively eliminating background noise.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Data description</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">The dataset utilized in this study consists of DCE-MRI scans obtained from 59 patients at Stavanger University Hospital in 2008. The DCE sequence comprises one pre- and five post-contrast image series with a temporal resolution of 63 seconds. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.T1" title="Table 1 ‣ 2.1.2 Image acquisition ‣ 2.1 Data ‣ 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">1</span></a> provides a detailed description of the dataset and screening parameters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Image acquisition</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">All DCE-MRI scans were acquired using a 1.5 Tesla MRI scanner, Philips Intera, with a dedicated breast coil equipped with SENSE technology for high-quality and high-resolution images.
Imaging parameters included T1 weighted fast spoiled gradient echo (FSPGR) sequence, with a scan resolution of 0.9659 x 0.9659 mm<sup class="ltx_sup" id="S2.SS1.SSS2.p1.1.1">2</sup> and dynamic acquisition time of 6 minutes and 20 seconds following contrast agent administration.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Detailed Specifications and Imaging Features of MRI Scans</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Category</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Attribute</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S2.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">Description</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2.2">
<td class="ltx_td ltx_border_t" id="S2.T1.1.2.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.2.2">Patient number</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S2.T1.1.2.2.3">59</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.3">
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.3.1.1">Study Information</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.3.2">Weight (kg)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.3.3.3">70.6 ± 8.4</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.4">
<td class="ltx_td" id="S2.T1.1.4.4.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.4.2">Patient Position</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.4.4.3">Head First Prone (HFP)</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.5">
<td class="ltx_td" id="S2.T1.1.5.5.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.5.2">Number of Images per Patient</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.5.5.3">6 (1 pre-contrast, 5 post-contrast)</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.6">
<td class="ltx_td ltx_border_t" id="S2.T1.1.6.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.6.6.2">Scanner Model</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S2.T1.1.6.6.3">Philips Intera MRI Scanner</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.7">
<td class="ltx_td ltx_align_left" id="S2.T1.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.7.7.1.1">Scanner Properties</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.7.7.2">Magnetic Field Strength (T)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.7.7.3">1.5</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.8">
<td class="ltx_td" id="S2.T1.1.8.8.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.8.8.2">Coil Technology</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.8.8.3">SENSE Technology</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.9">
<td class="ltx_td ltx_border_t" id="S2.T1.1.9.9.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.9.9.2">Image Dimensions</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S2.T1.1.9.9.3">(352,352,150), (352,352,140), (352,352,120)</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.10">
<td class="ltx_td ltx_align_left" id="S2.T1.1.10.10.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.10.10.1.1">Image characteristics</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.10.10.2">Pixel Spacing (mm)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.10.10.3">0.9659 x 0.9659</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.11">
<td class="ltx_td" id="S2.T1.1.11.11.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.11.11.2">Slice Thickness (mm)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.11.11.3">2</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.12">
<td class="ltx_td" id="S2.T1.1.12.12.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.12.12.2">Field of View (FOV) (mm)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.12.12.3">400</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.13.13">
<td class="ltx_td ltx_border_t" id="S2.T1.1.13.13.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.13.13.2">MRI Sequence</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S2.T1.1.13.13.3">T1 weighted fast spoiled gradient echo (FSPGR)</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.14.14">
<td class="ltx_td ltx_align_left" id="S2.T1.1.14.14.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.14.14.1.1">Imaging Features</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.14.14.2">Repetition Time (TR)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.14.14.3">6.91</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.15.15">
<td class="ltx_td" id="S2.T1.1.15.15.1"></td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.15.15.2">Echo Time (TE)</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.15.15.3">3.39</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.16.16">
<td class="ltx_td ltx_border_bb" id="S2.T1.1.16.16.1"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.16.16.2">Flip Angle</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S2.T1.1.16.16.3">12</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Preprocessing</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The initial dataset, comprised of imaging data in the DICOM standard format, underwent a meticulous cleansing process to ensure data integrity. Subsequently, both pre- and post-contrast images were automatically identified and converted to the NIFTI format, a prerequisite for our modeling endeavors. To ensure consistent data volume, a random oversampling method was applied to minority volumes. This approach simplifies data preparation, making it easier to process before feeding it into the model for further analysis. In addition, breast regions were annotated in detail, adhering to predefined boundary criteria outlined in breast boundary assumptions thereby providing insights for subsequent analyses.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Deep learning networks</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Over the past few decades, numerous segmentation models have been introduced by researchers. Among these, encoder-decoder based models with skip connections have garnered significant attention due to their effectiveness in retaining important features during training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib19" title="">19</a>]</cite>. In this study, we employed seven widely recognized segmentation architectures—UNet, UNet++, DenseNet, FCNResNet50, FCNResNet101, DeepLabv3ResNet50, and DeepLabv3ResNet101—to train on our dataset. These models were selected for their proven efficacy in medical image segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">UNet, introduced by Ronneberger et al. in 2015 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib19" title="">19</a>]</cite>, is one of the most popular segmentation methods. It consists of contraction and expansion pathways connected by skip connections. These skip connections help the model retain important features that might otherwise be forgotten during the training process.
UNet++ is an improved version of UNet, designed to achieve superior results. In UNet++, the skip connections were redesigned to reduce the loss of important features between the contraction and expansion pathways, enhancing the overall performance of the model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib20" title="">20</a>]</cite>.
DenseNet, another architecture utilized in this study, has demonstrated promise in propagating features throughout the model. In DenseNet, every layer is connected to other layers, thereby enhancing feature propagation across the entire network and improving the model’s ability to learn complex patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib21" title="">21</a>]</cite>. Given that DenseNet is primarily used for classification tasks, we employed its feature extraction part along with a decoder, excluding skip connections, to examine the impact of their absence in a deeper model.
Next network is FCNResNet comprising a ResNet as the feature extractor and an FCN header <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib22" title="">22</a>]</cite> for upsampling or decoding. ResNet’s structure, which includes residual blocks, has proven effective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib23" title="">23</a>]</cite>, while the FCN header connects to each feature level, serving as a skip connection.
Last architecture, DeepLabv3 is renowned for its Atrous Spatial Pyramid Pooling (ASPP) block <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib24" title="">24</a>]</cite>. Following the ResNet feature extractor, ASPP is applied and subsequently added to the decoder part of the architecture for upsampling.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.T2" title="Table 2 ‣ 2.3 Deep learning networks ‣ 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">2</span></a> provides practical information about the networks, including learning parameters, the number of layers, and their distinctive features. This comparative analysis offers valuable insights into the strengths and applications of each model in medical image segmentation tasks.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Model specification and features</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1">Architecture name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.2.1">Layers</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.3.1">Learning Parameters</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.4.1">Special Features</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T2.1.2.1.1">UNet</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.1.2">141</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.1.3">31,112,641</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S2.T2.1.2.1.4">Simple skip connection</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.3.2.1">UNet++</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.3.2.2">240</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.3.2.3">9,119,044</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.3.2.4">Dense skip connection</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.4.3.1">DenseNet</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.4.3.2">1216</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.4.3.3">70,536,843</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.4.3.4">Reusing Feature-maps in subsequent blocks</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.5.4.1">FCNResNet50</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.5.4.2">157</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.5.4.3">32,943,617</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.5.4.4">Strong feature extractor alongside FCN header</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.6.5.1">FCNResNet101</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.6.5.2">293</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.6.5.3">51,935,745</td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.1.6.5.4"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.7.6.1">DeepLabv3ResNet50</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.7.6.2">184</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.7.6.3">39,630,593</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T2.1.7.6.4">Atrous Spatial Pyramid Pooling (ASPP)</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S2.T2.1.8.7.1">DeepLabv3ResNet101</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.1.8.7.2">320</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.1.8.7.3">58,622,721</td>
<td class="ltx_td ltx_nopad_r ltx_border_b" id="S2.T2.1.8.7.4"></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Evaluation</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">In the evaluation section, we assess the performance of our models using the Dice loss function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib25" title="">25</a>]</cite> and k-fold cross-validation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib26" title="">26</a>]</cite>. The Dice loss function, a promising evaluator for measuring overlap in segmentation tasks, ensures precise model predictions. We employed 10-fold cross-validation meaning that partitioning the dataset into 10 equal parts to validate the model’s consistency and performance across different data subsets. The dice loss function was calculated as relation <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.E1" title="In 2.4 Evaluation ‣ 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">1</span></a>, where P and G refer to predicted and ground truth.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\text{Dice loss (P,G)}=1-2\cdot\frac{P\cap G}{P+G}" class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mtext id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2a.cmml">Dice loss (P,G)</mtext><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mn id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">1</mn><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">−</mo><mrow id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mn id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml">2</mn><mo id="S2.E1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.1.1.3.3.1.cmml">⋅</mo><mfrac id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml"><mrow id="S2.E1.m1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.2.cmml"><mi id="S2.E1.m1.1.1.3.3.3.2.2" xref="S2.E1.m1.1.1.3.3.3.2.2.cmml">P</mi><mo id="S2.E1.m1.1.1.3.3.3.2.1" xref="S2.E1.m1.1.1.3.3.3.2.1.cmml">∩</mo><mi id="S2.E1.m1.1.1.3.3.3.2.3" xref="S2.E1.m1.1.1.3.3.3.2.3.cmml">G</mi></mrow><mrow id="S2.E1.m1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.3.3.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.3.2.cmml">P</mi><mo id="S2.E1.m1.1.1.3.3.3.3.1" xref="S2.E1.m1.1.1.3.3.3.3.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.3.3.3.3" xref="S2.E1.m1.1.1.3.3.3.3.3.cmml">G</mi></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><ci id="S2.E1.m1.1.1.2a.cmml" xref="S2.E1.m1.1.1.2"><mtext id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">Dice loss (P,G)</mtext></ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><minus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></minus><cn id="S2.E1.m1.1.1.3.2.cmml" type="integer" xref="S2.E1.m1.1.1.3.2">1</cn><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><ci id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.1">⋅</ci><cn id="S2.E1.m1.1.1.3.3.2.cmml" type="integer" xref="S2.E1.m1.1.1.3.3.2">2</cn><apply id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"><divide id="S2.E1.m1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3"></divide><apply id="S2.E1.m1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2"><intersect id="S2.E1.m1.1.1.3.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.2.1"></intersect><ci id="S2.E1.m1.1.1.3.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2">𝑃</ci><ci id="S2.E1.m1.1.1.3.3.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.3.2.3">𝐺</ci></apply><apply id="S2.E1.m1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3"><plus id="S2.E1.m1.1.1.3.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3.3.1"></plus><ci id="S2.E1.m1.1.1.3.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.3.2">𝑃</ci><ci id="S2.E1.m1.1.1.3.3.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3.3">𝐺</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\text{Dice loss (P,G)}=1-2\cdot\frac{P\cap G}{P+G}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">Dice loss (P,G) = 1 - 2 ⋅ divide start_ARG italic_P ∩ italic_G end_ARG start_ARG italic_P + italic_G end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Breast boundaries</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Accurately annotating the boundaries of the breast region has been a persistent challenge, as highlighted in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib27" title="">27</a>]</cite>. This challenge arises from the similar intensities observed in imaging for the upper chest wall, fibroglandular tissue, and pectoral muscles, making differentiation difficult <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">To improve breast region segmentation, we propose a novel boundary framework designed to exclude high-intensity pixels, such as those from blood vessels near the heart, which are often misidentified by models. The anterior boundary is set at the skin line to remove low-intensity and noisy pixels anterior to it, while the posterior boundary is aligned with the lungs to eliminate low-intensity and noisy pixels dorsal to the chest wall. Additionally, our method incorporates the pectoralis and intercostal muscles, as well as the ribs, to ensure accurate staging of tumors that invade the chest wall.</p>
</div>
<div class="ltx_para" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.F1" title="Figure 1 ‣ 2.5 Breast boundaries ‣ 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the various regions of interest, highlighting low-intensity areas such as the background and lungs, and high-intensity areas like the heart and lesions, along with the delineated boundaries of the proposed breast region.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="538" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of distinct regions, highlighting the delineation of the proposed breast boundary.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="641" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic diagram of the study pipeline. Data part depicts data inputs (Pre-Post Contrast (PPC) and First Post Contrast (FPC)), annotator and mask file, and the Architecture block consisting of seven distinct models, each trained individually. Evaluation methods, including 10-fold cross-validation and the Dice loss function, are also illustrated. (Annotator components adapted from Vecteezy.com)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Setup</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.1">The simulations were conducted on a high-performance computing setup to ensure the efficient training and evaluation of the deep learning models. The hardware configuration utilized in this study includes an AMD Ryzen 9 7900X 12-Core Processor CPU, 32 GB of RAM, and an NVIDIA GeForce RTX 4090 GPU with 24 GB of GDDR6X memory. The power consumption of the GPU is reported to be 450 W by NVIDIA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib29" title="">29</a>]</cite>, and the entire system consumes roughly 1 kWh of energy during each simulation.</p>
</div>
<div class="ltx_para" id="S2.SS6.p2">
<p class="ltx_p" id="S2.SS6.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S2.F2" title="Figure 2 ‣ 2.5 Breast boundaries ‣ 2 Materials and Methods ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">2</span></a> depicts different pathways in data processing, the representation of various architectures, and the evaluation metrics and methods employed in our approach.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experiments</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The input data consisted of both pre- and first post-contrast images, with corresponding masks serving as the ground truth outputs. Training involved various deep learning models using 10-fold cross-validation by utilizing Dice loss function. RAdam optimizer with an initial learning rate of 0.0001 in conjunction with a ReduceLROnPlateau scheduler was utilized to enhance model convergence and performance. This scheduler dynamically adjusted the learning rate based on validation performance metrics, aiming not only to minimize the Dice loss function, but also to accelerate training performance.
Across all models, a consistent batch size of 8 was employed during training, with data shuffled to ensure robust model learning. Finally, a test subset comprising data from two patients was randomly partitioned to evaluate the model’s performance on previously unseen data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model performance and generalizability</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.T3" title="Table 3 ‣ 3.2 Model performance and generalizability ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">3</span></a> displays Dice training and validation losses across different deep learning architectures at their best epochs. UNet++ achieves the lowest training loss of 0.0112 ± 0.0022, while FCN with ResNet50 also performs well with a Dice training loss of 0.0126 ± 0.0028. On the other hand, UNet architecture stands out for its superior validation results, indicating strong generalization to unseen data essential for real-world applications with validation loss of 0.0448 ± 0.0077. Following closely, UNet++ demonstrates competitive validation performance with losses of 0.0466 ± 0.0167, emphasizing its balanced model performance and generalizability.
In contrast, DenseNet exhibits some of the poorest performance metrics, both in terms of training and validation loss, despite its deeper architecture. On the other hand, DeepLabv3 with a ResNet101 backbone achieves superior validation loss, second only to UNet.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Training and validation Dice loss for various DL models for <math alttext="k" class="ltx_centering" display="inline" id="S3.T3.2.m1.1"><semantics id="S3.T3.2.m1.1b"><mi id="S3.T3.2.m1.1.1" xref="S3.T3.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.T3.2.m1.1c"><ci id="S3.T3.2.m1.1.1.cmml" xref="S3.T3.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.m1.1e">italic_k</annotation></semantics></math>-fold cross-validation.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.16">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.16.15.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.16.15.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.16.15.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.16.15.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.16.15.1.2.1">Dice Training Loss</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.16.15.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.16.15.1.3.1">Dice Validation Loss</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.4.2.3">UNet</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.1.1">0.0146 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.3.1.1.m1.1"><semantics id="S3.T3.3.1.1.m1.1a"><mo id="S3.T3.3.1.1.m1.1.1" xref="S3.T3.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.1.1.m1.1b"><csymbol cd="latexml" id="S3.T3.3.1.1.m1.1.1.cmml" xref="S3.T3.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.1.1.m1.1d">±</annotation></semantics></math> 0.0024</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2">0.0448 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.4.2.2.m1.1"><semantics id="S3.T3.4.2.2.m1.1a"><mo id="S3.T3.4.2.2.m1.1.1" xref="S3.T3.4.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.2.2.m1.1b"><csymbol cd="latexml" id="S3.T3.4.2.2.m1.1.1.cmml" xref="S3.T3.4.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.2.2.m1.1d">±</annotation></semantics></math> 0.0077</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.6.4.3">UNet++</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.3.1">0.0112 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.5.3.1.m1.1"><semantics id="S3.T3.5.3.1.m1.1a"><mo id="S3.T3.5.3.1.m1.1.1" xref="S3.T3.5.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.3.1.m1.1b"><csymbol cd="latexml" id="S3.T3.5.3.1.m1.1.1.cmml" xref="S3.T3.5.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.3.1.m1.1d">±</annotation></semantics></math> 0.0022</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.4.2">0.0466 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.6.4.2.m1.1"><semantics id="S3.T3.6.4.2.m1.1a"><mo id="S3.T3.6.4.2.m1.1.1" xref="S3.T3.6.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.4.2.m1.1b"><csymbol cd="latexml" id="S3.T3.6.4.2.m1.1.1.cmml" xref="S3.T3.6.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.4.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.4.2.m1.1d">±</annotation></semantics></math> 0.0167</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.6.3">DenseNet</th>
<td class="ltx_td ltx_align_center" id="S3.T3.7.5.1">0.0163 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.7.5.1.m1.1"><semantics id="S3.T3.7.5.1.m1.1a"><mo id="S3.T3.7.5.1.m1.1.1" xref="S3.T3.7.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.5.1.m1.1b"><csymbol cd="latexml" id="S3.T3.7.5.1.m1.1.1.cmml" xref="S3.T3.7.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.5.1.m1.1d">±</annotation></semantics></math> 0.0038</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.6.2">0.0525 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.8.6.2.m1.1"><semantics id="S3.T3.8.6.2.m1.1a"><mo id="S3.T3.8.6.2.m1.1.1" xref="S3.T3.8.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.6.2.m1.1b"><csymbol cd="latexml" id="S3.T3.8.6.2.m1.1.1.cmml" xref="S3.T3.8.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.6.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.6.2.m1.1d">±</annotation></semantics></math> 0.0082</td>
</tr>
<tr class="ltx_tr" id="S3.T3.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.10.8.3">FCNResNet50</th>
<td class="ltx_td ltx_align_center" id="S3.T3.9.7.1">0.0126 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.9.7.1.m1.1"><semantics id="S3.T3.9.7.1.m1.1a"><mo id="S3.T3.9.7.1.m1.1.1" xref="S3.T3.9.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.9.7.1.m1.1b"><csymbol cd="latexml" id="S3.T3.9.7.1.m1.1.1.cmml" xref="S3.T3.9.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.9.7.1.m1.1d">±</annotation></semantics></math> 0.0028</td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.8.2">0.0474 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.10.8.2.m1.1"><semantics id="S3.T3.10.8.2.m1.1a"><mo id="S3.T3.10.8.2.m1.1.1" xref="S3.T3.10.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.10.8.2.m1.1b"><csymbol cd="latexml" id="S3.T3.10.8.2.m1.1.1.cmml" xref="S3.T3.10.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.10.8.2.m1.1d">±</annotation></semantics></math> 0.0100</td>
</tr>
<tr class="ltx_tr" id="S3.T3.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.12.10.3">FCNResNet101</th>
<td class="ltx_td ltx_align_center" id="S3.T3.11.9.1">0.0134 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.11.9.1.m1.1"><semantics id="S3.T3.11.9.1.m1.1a"><mo id="S3.T3.11.9.1.m1.1.1" xref="S3.T3.11.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.11.9.1.m1.1b"><csymbol cd="latexml" id="S3.T3.11.9.1.m1.1.1.cmml" xref="S3.T3.11.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.9.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.11.9.1.m1.1d">±</annotation></semantics></math> 0.0043</td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.10.2">0.0497 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.12.10.2.m1.1"><semantics id="S3.T3.12.10.2.m1.1a"><mo id="S3.T3.12.10.2.m1.1.1" xref="S3.T3.12.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.10.2.m1.1b"><csymbol cd="latexml" id="S3.T3.12.10.2.m1.1.1.cmml" xref="S3.T3.12.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.10.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.12.10.2.m1.1d">±</annotation></semantics></math> 0.0067</td>
</tr>
<tr class="ltx_tr" id="S3.T3.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.14.12.3">DeepLabv3ResNet50</th>
<td class="ltx_td ltx_align_center" id="S3.T3.13.11.1">0.0140 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.13.11.1.m1.1"><semantics id="S3.T3.13.11.1.m1.1a"><mo id="S3.T3.13.11.1.m1.1.1" xref="S3.T3.13.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.13.11.1.m1.1b"><csymbol cd="latexml" id="S3.T3.13.11.1.m1.1.1.cmml" xref="S3.T3.13.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.13.11.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.13.11.1.m1.1d">±</annotation></semantics></math> 0.0036</td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.12.2">0.0469 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.14.12.2.m1.1"><semantics id="S3.T3.14.12.2.m1.1a"><mo id="S3.T3.14.12.2.m1.1.1" xref="S3.T3.14.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.14.12.2.m1.1b"><csymbol cd="latexml" id="S3.T3.14.12.2.m1.1.1.cmml" xref="S3.T3.14.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.14.12.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.14.12.2.m1.1d">±</annotation></semantics></math> 0.0085</td>
</tr>
<tr class="ltx_tr" id="S3.T3.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.16.14.3">DeepLabv3ResNet101</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.15.13.1">0.0131 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.15.13.1.m1.1"><semantics id="S3.T3.15.13.1.m1.1a"><mo id="S3.T3.15.13.1.m1.1.1" xref="S3.T3.15.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.15.13.1.m1.1b"><csymbol cd="latexml" id="S3.T3.15.13.1.m1.1.1.cmml" xref="S3.T3.15.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.15.13.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.15.13.1.m1.1d">±</annotation></semantics></math> 0.0018</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.16.14.2">0.0462 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.16.14.2.m1.1"><semantics id="S3.T3.16.14.2.m1.1a"><mo id="S3.T3.16.14.2.m1.1.1" xref="S3.T3.16.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.16.14.2.m1.1b"><csymbol cd="latexml" id="S3.T3.16.14.2.m1.1.1.cmml" xref="S3.T3.16.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.16.14.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.16.14.2.m1.1d">±</annotation></semantics></math> 0.0034</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.F3" title="Figure 3 ‣ 3.2 Model performance and generalizability ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the segmentation results of different models across selected slices, specifically the first, 30th, middle, 120th, and last slices. These slices were chosen to represent the progression from the initial to the final slices of the volume, allowing for a comprehensive comparison of each model’s ability to maintain accuracy throughout the entire dataset.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="538" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Model segmentation results across selected slices (from top to down row: first, 30<sup class="ltx_sup ltx_centering" id="S3.F3.3.1">th</sup>, middle, 120<sup class="ltx_sup ltx_centering" id="S3.F3.4.2">th</sup> and last slices)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Internal breast segmentation and boundary detection</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.F4" title="Figure 4 ‣ 3.3 Internal breast segmentation and boundary detection ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">4</span></a> shows the distribution of Dice scores for each model on the test dataset, focusing on the median and range of performance. Notably, the UNet model has a median Dice score of 0.98, with a range from 0.91 to 0.995, highlighting its strong performance. Similarly, UNet++ achieves a median score of 0.98, with a range from 0.90 to 0.99. Close behind, DeepLabv3 with ResNet101 records a median of 0.975 with a slightly wider range from 0.88 to 0.99.
On the other hand, the FCN models with ResNet50 and ResNet101 backbones show median Dice scores of 0.970 and 0.972, respectively, with ranges approximately from 0.88 to 0.985 for both. In contrast, DenseNet, though having a similar median Dice score of 0.970, shows the widest range of 0.87 to 0.99, indicating more variability and less consistent segmentation accuracy.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="398" id="S3.F4.g1" src="x4.png" width="664"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Dice score for different DL models on test subset</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">As was evident in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.F3" title="Figure 3 ‣ 3.2 Model performance and generalizability ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">3</span></a>, the segmentation of the breast region was generally excellent across all models, but differences in boundary detection were observed. To further evaluate and compare the boundary detection capabilities of each model, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.F5" title="Figure 5 ‣ 3.3 Internal breast segmentation and boundary detection ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">5</span></a> presents the Hausdorff distance on the test subset. As shown in the figure, UNet and UNet++ exhibit the lowest median Hausdorff distances, indicating their superior ability to accurately capture boundary details with minimal deviation. In contrast, the FCN models, particularly with ResNet50 and ResNet101 backbones, display higher median Hausdorff distances and a broader spread, highlighting greater variability in boundary detection and less precise segmentation at the edges.
Similarly, DenseNet shows a wide range of Hausdorff distances, with a higher number of outliers, indicating that while it may perform adequately in some cases, it struggles with boundary accuracy in others. On the other hand, DeepLabv3 with ResNet50 and ResNet101 demonstrate relatively lower median Hausdorff distances compared to FCN models, but with a few outliers, suggesting these models are generally reliable but may still occasionally falter in capturing fine boundary details.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="398" id="S3.F5.g1" src="x5.png" width="664"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Hausdorff distance comparison across different models</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training and inference time</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Training and inference time are critical considerations in efficiency and cost of the modelling. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.T4" title="Table 4 ‣ 3.4 Training and inference time ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">4</span></a> Demonstrates training time results of 10-fold, with mean and standard deviation, alongside average inference time in the test set for diverse architectures. As presented in the table, FCN with ResNet50 shows the shortest training time at 87 ± 18 minutes, while DenseNet exhibits the longest at 185 ± 56 minutes. In terms of inference time per slice, UNet performs the best with 126 milliseconds, whereas DenseNet requires significantly more time at 696 milliseconds, highlighting varying computational efficiencies across these models. Despite possessing fewer trained parameters, UNet++ required more time, 199 ± 21, to train each fold. On the other hand, DeepLabv3ResNet50 demonstrated superior performance, followed by FCNResNet50, with training times of 104 ± 22 minutes per fold.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Training and inference times across different models.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T4.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.7.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T4.7.8.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.7.8.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.7.8.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.7.8.1.2.1">Training time per fold (min)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.7.8.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.7.8.1.3.1">Inference time per slice (msec)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.1.1.2">UNet</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1">136 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T4.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.m1.1d">±</annotation></semantics></math> 22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.3">126</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.2.2.2">UNet++</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.1">199 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T4.2.2.1.m1.1"><semantics id="S3.T4.2.2.1.m1.1a"><mo id="S3.T4.2.2.1.m1.1.1" xref="S3.T4.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.1.m1.1b"><csymbol cd="latexml" id="S3.T4.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.1.m1.1d">±</annotation></semantics></math> 21</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.3">152</td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.3.3.2">DenseNet</th>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.1">185 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T4.3.3.1.m1.1"><semantics id="S3.T4.3.3.1.m1.1a"><mo id="S3.T4.3.3.1.m1.1.1" xref="S3.T4.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.1.m1.1b"><csymbol cd="latexml" id="S3.T4.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.1.m1.1d">±</annotation></semantics></math> 56</td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3">696</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.4.4.2">FCNResNet50</th>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.1">87 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T4.4.4.1.m1.1"><semantics id="S3.T4.4.4.1.m1.1a"><mo id="S3.T4.4.4.1.m1.1.1" xref="S3.T4.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.1.m1.1b"><csymbol cd="latexml" id="S3.T4.4.4.1.m1.1.1.cmml" xref="S3.T4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T4.4.4.1.m1.1d">±</annotation></semantics></math> 18</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.3">140</td>
</tr>
<tr class="ltx_tr" id="S3.T4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.5.5.2">FCNResNet101</th>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.1">149 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T4.5.5.1.m1.1"><semantics id="S3.T4.5.5.1.m1.1a"><mo id="S3.T4.5.5.1.m1.1.1" xref="S3.T4.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.1.m1.1b"><csymbol cd="latexml" id="S3.T4.5.5.1.m1.1.1.cmml" xref="S3.T4.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T4.5.5.1.m1.1d">±</annotation></semantics></math> 35</td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.3">266</td>
</tr>
<tr class="ltx_tr" id="S3.T4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.6.6.2">DeepLabv3ResNet50</th>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.1">104 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T4.6.6.1.m1.1"><semantics id="S3.T4.6.6.1.m1.1a"><mo id="S3.T4.6.6.1.m1.1.1" xref="S3.T4.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.1.m1.1b"><csymbol cd="latexml" id="S3.T4.6.6.1.m1.1.1.cmml" xref="S3.T4.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T4.6.6.1.m1.1d">±</annotation></semantics></math> 22</td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.3">161</td>
</tr>
<tr class="ltx_tr" id="S3.T4.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T4.7.7.2">DeepLabv3ResNet101</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.7.7.1">178 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T4.7.7.1.m1.1"><semantics id="S3.T4.7.7.1.m1.1a"><mo id="S3.T4.7.7.1.m1.1.1" xref="S3.T4.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.1.m1.1b"><csymbol cd="latexml" id="S3.T4.7.7.1.m1.1.1.cmml" xref="S3.T4.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T4.7.7.1.m1.1d">±</annotation></semantics></math> 37</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.7.7.3">294</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Recent research underscores the growing importance of assessing the carbon footprint as a key factor in evaluating environmental sustainability across various models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib32" title="">32</a>]</cite>. Consequently, it is essential to investigate the carbon footprint associated with different network architectures during 10-fold cross-validation training. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.F6" title="Figure 6 ‣ 3.4 Training and inference time ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">6</span></a>, FCNResNet50 emerges as the most favorable and sustainable model, exhibiting a carbon footprint range of 0.45 to 0.85 kg CO2. This indicates that FCNResNet50 demonstrates the lowest environmental impact among the models analyzed. In contrast, DenseNet training is associated with higher energy consumption and, consequently, a larger carbon footprint. Following FCNResNet50, Deeplabv3ResNet50 displays the second-lowest carbon footprint, ranging from 0.55 to 1.15 kg CO2. Other models exhibit carbon footprints situated between these two extremes, reflecting a spectrum of environmental impacts.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="361" id="S3.F6.g1" src="x6.png" width="913"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Carbon footprint across folds for various architectures</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This study offers a comprehensive evaluation of various state-of-the-art segmentation architectures applied to whole breast segmentation. The models examined include UNet, UNet++, DenseNet, FCNResNet50, FCNResNet101, DeepLabv3ResNet50, and DeepLabv3ResNet101. The results indicate significant differences in performance and training efficiency across these architectures, providing valuable insights into MR breast image analysis.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">As outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.T3" title="Table 3 ‣ 3.2 Model performance and generalizability ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">3</span></a>, UNet exhibited a higher Dice loss function during training compared to UNet++. This may be attributed to UNet’s simpler architecture, which could enable more effective generalization during validation. Conversely, UNet++ introduces added complexity through its nested and dense skip connections, which may contribute to slower convergence, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.T4" title="Table 4 ‣ 3.4 Training and inference time ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">4</span></a>, or difficulties in optimizing all parameters effectively. This increased complexity may also render the model more susceptible to overfitting, as indicated by the higher variance in validation loss. The comparison between FCN with ResNet50 and ResNet101 underscores the impact of deeper networks. ResNet101, being a deeper model than ResNet50, generally allows for the learning of more complex features. However, the slightly higher training Dice loss in ResNet101 suggests that, while it has the potential to learn more detailed representations, the benefits may diminish, particularly if the dataset is insufficiently large to fully exploit the deeper network’s capacity. Similarly, DeepLabv3 with ResNet50 and ResNet101 shows relatively close performance. The architecture of DeepLabv3, which incorporates atrous convolutions and multi-scale context aggregation, is intended to enhance feature extraction for semantic segmentation. The minor variation in Dice loss suggests that, although ResNet101 offers more layers and potentially improved feature extraction, the advantages are not significantly superior to those of ResNet50. This may indicate that the additional layers in ResNet101 are not fully utilized, or that the model’s complexity poses challenges in training without overfitting. DenseNet demonstrates the highest Dice loss among the models. Although DenseNet’s architecture employs a dense connectivity pattern that encourages feature reuse and facilitates gradient flow during backpropagation, it lacks skip connections between the feature extractor and the expansion part of the network. Consequently, the poorest results can be attributed to this absence of skip connections.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Segmentation performance is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.F3" title="Figure 3 ‣ 3.2 Model performance and generalizability ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">3</span></a> for individual slices. In the first and last rows, corresponding to the first and last slices, significant differences are observed around the tails and the lower boundary, just above the heart and lungs. In contrast, the second and fourth rows, representing intermediate slices, exhibit improved boundary delineation across all models. The middle slice, depicted in the third row, demonstrates the best segmentation performance among all slices.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Segmentation efficiency can be evaluated based on two primary aspects: internal segmentation quality, measured by the Dice score, and accuracy in boundary delineation. For the assessment of boundary accuracy, the Hausdorff distance metric was employed to compare different models. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#S3.F5" title="Figure 5 ‣ 3.3 Internal breast segmentation and boundary detection ‣ 3 Results ‣ Comparative Analysis of Deep Learning Architectures for Breast Region Segmentation with a Novel Breast Boundary Proposal"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates that UNet, UNet++, and DeepLabv3 with ResNets demonstrate superior performance in boundary detection. UNet and UNet++ excel in boundary detection due to their architectures, which are optimized for fine-grained segmentation tasks. DeepLabv3 with ResNets achieves precise boundary detection capabilities by leveraging the ASPP block integrated within its architecture.
FCNResNet50 emerges as the model with the smallest carbon footprint, indicating its relative efficiency in terms of energy consumption during training. This efficiency may be attributed to the balance between model complexity and the depth of the ResNet50 backbone, which is adequate to perform well without excessive computational demand. Similarly, DeepLabv3 with ResNet50 also exhibits a relatively low carbon footprint, owing to its effective use of ASPP and multi-scale context aggregation. In contrast, UNet++ demonstrates an unexpectedly high carbon footprint despite having fewer parameters than other models like DenseNet. This can be largely attributed to its complex architecture, which includes nested and dense skip connections.
In comparison to the study conducted in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02337v1#bib.bib6" title="">6</a>]</cite>, all architectures used in this study demonstrate superior results. Although the datasets differ, the influence of hyperparameters and preprocessing steps cannot be overlooked. Additionally , the MRI dataset used in this study, collected in 2008, remains relevant for breast region segmentation due to its imaging quality and detailed annotations, which still provide critical insights for developing and assessing segmentation models. This study offers a thorough examination of deep learning architectures specifically for breast region segmentation in DCE-MRI images. We explore the latest advancements in deep learning techniques and assess their applicability and performance in this crucial task. Future work could further enhance preprocessing methods and investigate various loss functions to evaluate additional effective factors influencing breast region segmentation.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, a deep learning (DL) pipeline was developed to evaluate performance of seven DL architectures for the segmentation of breast regions in DCE-MRI. A boundary was proposed for delineating breast borders, and manual annotation was carried out to provide accurate ground truth data. The efficiency of each architecture was assessed in terms of model training loss, training time, inference time, Dice score, boundary detection accuracy, and carbon footprint. The results indicated that UNet++ exhibited the best overall model performance, demonstrating superior accuracy in terms of Dice score. However, UNet showed better prediction accuracy on the validation set. On the other hand, FCNResNet50 emerged as the most efficient model concerning training time and carbon footprint, while UNet achieved the best inference time, making it suitable for real-time applications. These findings underscore the trade-offs between different models, highlighting that each architecture has its strengths and weaknesses depending on the evaluation criteria. For instance, UNet++ provides high segmentation accuracy but requires longer training times, whereas FCNResNet50 is more environmentally sustainable with a lower carbon footprint and quicker training times. This study emphasizes that the choice of model should be based on specific requirements and constraints of the application at hand. Given the increasing importance of sustainability, carbon footprint has become a crucial factor in model selection. Consequently, FCNResNet50 is identified as the most competitive model, balancing excellent performance and efficiency with a moderate inference time.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Declarations</h2>
<section class="ltx_subsection" id="Sx1.SSx1">
<h3 class="ltx_title ltx_title_subsection">Ethics compliance</h3>
<div class="ltx_para" id="Sx1.SSx1.p1">
<p class="ltx_p" id="Sx1.SSx1.p1.1">All patients enrolled in the cohorts of the IMAGINE project received approval from the regional ethics committee.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SSx2">
<h3 class="ltx_title ltx_title_subsection">Data availability</h3>
<div class="ltx_para" id="Sx1.SSx2.p1">
<p class="ltx_p" id="Sx1.SSx2.p1.1">The Stavanger dataset analyzed in this study contains sensitive patient information and therefore not publicly available. This ensures compliance with patient confidentiality and privacy regulations.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SSx3">
<h3 class="ltx_title ltx_title_subsection">Code availability</h3>
<div class="ltx_para" id="Sx1.SSx3.p1">
<p class="ltx_p" id="Sx1.SSx3.p1.1">The code for data processing, analyzing and modelling is available on GitHub. To access the code repository, please follow the link on <a class="ltx_ref ltx_href" href="https://github.com/SamNarimani-lab/Breast.git" title="">GitHub.</a></p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SSx4">
<h3 class="ltx_title ltx_title_subsection">Acknowledgment</h3>
<div class="ltx_para" id="Sx1.SSx4.p1">
<p class="ltx_p" id="Sx1.SSx4.p1.1">We would like to extend our sincere thanks to More and Romsdal Hospital Trust for their invaluable support and Stavanger University Hospital for providing resources and data. Special thanks go to Kathrine Røe Redalen, Iman Esmaili, Taraneh Ghasemi and Hadi Akbari Zadeh for their assistance throughout the research process. This work was funded by the Central Norway Regional Health Authority under the name of the IMAGINE project.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx1.SSx5">
<h3 class="ltx_title ltx_title_subsection">Author contributions</h3>
<div class="ltx_para" id="Sx1.SSx5.p1">
<p class="ltx_p" id="Sx1.SSx5.p1.1"><span class="ltx_text ltx_font_bold" id="Sx1.SSx5.p1.1.1">Sam Narimani</span>: Drafted the introduction, methods and materials, proposed a new boundary for breast region segmentation, and authored the results, discussion, and conclusion sections. Additionally, he contributed programming code. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Sx1.SSx5.p1.1.2">Solveig Roth Hoff</span>: Contributed to the writing of the introduction, the breast boundary definition, Discussion section, providing major insights.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Sx1.SSx5.p1.1.3">Kathinka Dæhli Kurz</span>: Responsible for data acquisition and preparation, and contributed to the discussion section. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Sx1.SSx5.p1.1.4">Kjell-Inge Gjesdal</span>: Set up MRI protocols. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Sx1.SSx5.p1.1.5">Jurgen Geisler</span>: Provided revisions and improvements to the introduction.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Sx1.SSx5.p1.1.6">Endre Grovik</span>: Supervised the project, acquired funding, and contributed to the revision of the introduction, methods and materials, results and discussion sections.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.1.1">
<span class="ltx_bibblock"><span class="ltx_ERROR undefined" id="bib.1.1.1.1">\bibcommenthead</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acciavatti et al. [2023]</span>
<span class="ltx_bibblock">
Acciavatti, R.J.,
Lee, S.H.,
Reig, B.,
Moy, L.,
Conant, E.F.,
Kontos, D.,
Moon, W.K.:
Beyond breast density: risk measures for breast cancer in multiple imaging modalities.
Radiology
<span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">306</span>(3),
222575
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lew et al. [2024]</span>
<span class="ltx_bibblock">
Lew, C.O.,
Harouni, M.,
Kirksey, E.R.,
Kang, E.J.,
Dong, H.,
Gu, H.,
Grimm, L.J.,
Walsh, R.,
Lowell, D.A.,
Mazurowski, M.A.:
A publicly available deep learning model and dataset for segmentation of breast, fibroglandular tissue, and vessels in breast mri.
Scientific reports
<span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">14</span>(1),
5383
(2024)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ojala et al. [2001]</span>
<span class="ltx_bibblock">
Ojala, T.,
Näppi, J.,
Nevalainen, O.:
Accurate segmentation of the breast region from digitized mammograms.
Computerized medical imaging and graphics
<span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">25</span>(1),
47–59
(2001)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al. [2020]</span>
<span class="ltx_bibblock">
Jiao, H.,
Jiang, X.,
Pang, Z.,
Lin, X.,
Huang, Y.,
Li, L.:
Deep convolutional neural networks-based automatic breast segmentation and mass detection in dce-mri.
Computational and Mathematical Methods in Medicine
<span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">2020</span>(1),
2413706
(2020)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivanovska et al. [2019]</span>
<span class="ltx_bibblock">
Ivanovska, T.,
Jentschke, T.G.,
Daboul, A.,
Hegenscheid, K.,
Völzke, H.,
Wörgötter, F.:
A deep learning framework for efficient analysis of breast volume and fibroglandular tissue using mr data with strong artifacts.
International journal of computer assisted radiology and surgery
<span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">14</span>,
1627–1633
(2019)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huo et al. [2021]</span>
<span class="ltx_bibblock">
Huo, L.,
Hu, X.,
Xiao, Q.,
Gu, Y.,
Chu, X.,
Jiang, L.:
Segmentation of whole breast and fibroglandular tissue using nnu-net in dynamic contrast enhanced mr images.
Magnetic Resonance Imaging
<span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">82</span>,
31–41
(2021)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Velden et al. [2020]</span>
<span class="ltx_bibblock">
Velden, B.H.,
Janse, M.H.,
Ragusi, M.A.,
Loo, C.E.,
Gilhuijs, K.G.:
Volumetric breast density estimation on mri using explainable deep learning regression.
Scientific reports
<span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">10</span>(1),
18095
(2020)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al. [2022]</span>
<span class="ltx_bibblock">
Yue, W.,
Zhang, H.,
Zhou, J.,
Li, G.,
Tang, Z.,
Sun, Z.,
Cai, J.,
Tian, N.,
Gao, S.,
Dong, J., <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">et al.</span>:
Deep learning-based automatic segmentation for size and volumetric measurement of breast cancer on magnetic resonance imaging.
Frontiers in Oncology
<span class="ltx_text ltx_font_bold" id="bib.bib8.2.2">12</span>,
984626
(2022)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giannini et al. [2010]</span>
<span class="ltx_bibblock">
Giannini, V.,
Vignati, A.,
Morra, L.,
Persano, D.,
Brizzi, D.,
Carbonaro, L.,
Bert, A.,
Sardanelli, F.,
Regge, D.:
A fully automatic algorithm for segmentation of the breasts in dce-mr images.
In: 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology,
pp. 3146–3149
(2010).
IEEE


</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saffari et al. [2020]</span>
<span class="ltx_bibblock">
Saffari, N.,
Rashwan, H.A.,
Abdel-Nasser, M.,
Kumar Singh, V.,
Arenas, M.,
Mangina, E.,
Herrera, B.,
Puig, D.:
Fully automated breast density segmentation and classification using deep learning.
Diagnostics
<span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">10</span>(11),
988
(2020)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ertaş et al. [2008]</span>
<span class="ltx_bibblock">
Ertaş, G.,
Gülçür, H.Ö.,
Osman, O.,
Uçan, O.N.,
Tunacı, M.,
Dursun, M.:
Breast mr segmentation and lesion detection with cellular neural networks and 3d template matching.
Computers in biology and medicine
<span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">38</span>(1),
116–126
(2008)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2019]</span>
<span class="ltx_bibblock">
Zhang, Y.,
Chen, J.-H.,
Chang, K.-T.,
Park, V.Y.,
Kim, M.J.,
Chan, S.,
Chang, P.,
Chow, D.,
Luk, A.,
Kwong, T., <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">et al.</span>:
Automatic breast and fibroglandular tissue segmentation in breast mri using deep learning by a fully-convolutional residual neural network u-net.
Academic radiology
<span class="ltx_text ltx_font_bold" id="bib.bib12.2.2">26</span>(11),
1526–1535
(2019)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piantadosi et al. [2018]</span>
<span class="ltx_bibblock">
Piantadosi, G.,
Sansone, M.,
Sansone, C.:
Breast segmentation in mri via u-net deep convolutional neural networks.
In: 2018 24th International Conference on Pattern Recognition (ICPR),
pp. 3917–3922
(2018).
IEEE


</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2018]</span>
<span class="ltx_bibblock">
Xu, X.,
Fu, L.,
Chen, Y.,
Larsson, R.,
Zhang, D.,
Suo, S.,
Hua, J.,
Zhao, J.:
Breast region segmentation being convolutional neural network in dynamic contrast enhanced mri.
In: 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),
pp. 750–753
(2018).
IEEE


</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2009]</span>
<span class="ltx_bibblock">
Yao, J.,
Chen, J.,
Chow, C.:
Breast tumor analysis in dynamic contrast enhanced mri using texture features and wavelet transform.
IEEE Journal of selected topics in signal processing
<span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">3</span>(1),
94–100
(2009)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. [2008]</span>
<span class="ltx_bibblock">
Nie, K.,
Chen, J.-H.,
Chan, S.,
Chau, M.-K.I.,
Yu, H.J.,
Bahri, S.,
Tseng, T.,
Nalcioglu, O.,
Su, M.-Y.:
Development of a quantitative method for analysis of breast density based on three-dimensional breast mri.
Medical physics
<span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">35</span>(12),
5253–5262
(2008)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sui et al. [2021]</span>
<span class="ltx_bibblock">
Sui, D.,
Huang, Z.,
Song, X.,
Zhang, Y.,
Wang, Y.,
Zhang, L.:
Breast regions segmentation based on u-net++ from dce-mri image sequences.
In: Journal of Physics: Conference Series,
vol. 1748,
p. 042058
(2021).
IOP Publishing


</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radak et al. [2023]</span>
<span class="ltx_bibblock">
Radak, M.,
Lafta, H.Y.,
Fallahi, H.:
Machine learning and deep learning techniques for breast cancer diagnosis and classification: a comprehensive review of medical imaging studies.
Journal of Cancer Research and Clinical Oncology
<span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">149</span>(12),
10473–10491
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. [2015]</span>
<span class="ltx_bibblock">
Ronneberger, O.,
Fischer, P.,
Brox, T.:
U-net: Convolutional networks for biomedical image segmentation.
In: Medical Image Computing and Computer-assisted intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18,
pp. 234–241
(2015).
Springer


</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2018]</span>
<span class="ltx_bibblock">
Zhou, Z.,
Rahman Siddiquee, M.M.,
Tajbakhsh, N.,
Liang, J.:
Unet++: A nested u-net architecture for medical image segmentation.
In: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4,
pp. 3–11
(2018).
Springer


</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2017]</span>
<span class="ltx_bibblock">
Huang, G.,
Liu, Z.,
Van Der Maaten, L.,
Weinberger, K.Q.:
Densely connected convolutional networks.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4700–4708
(2017)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al. [2015]</span>
<span class="ltx_bibblock">
Long, J.,
Shelhamer, E.,
Darrell, T.:
Fully convolutional networks for semantic segmentation.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3431–3440
(2015)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2016]</span>
<span class="ltx_bibblock">
He, K.,
Zhang, X.,
Ren, S.,
Sun, J.:
Deep residual learning for image recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778
(2016)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen [2017]</span>
<span class="ltx_bibblock">
Chen, L.-C.:
Rethinking atrous convolution for semantic image segmentation.
arXiv preprint arXiv:1706.05587
(2017)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dice [1945]</span>
<span class="ltx_bibblock">
Dice, L.R.:
Measures of the amount of ecologic association between species.
Ecology
<span class="ltx_text ltx_font_bold" id="bib.bib25.1.1">26</span>(3),
297–302
(1945)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kohavi [1995]</span>
<span class="ltx_bibblock">
Kohavi, R.:
A study of cross-validation and bootstrap for accuracy estimation and model selection.
Morgan Kaufman Publishing
(1995)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosado-Toro et al. [2015]</span>
<span class="ltx_bibblock">
Rosado-Toro, J.A.,
Barr, T.,
Galons, J.-P.,
Marron, M.T.,
Stopeck, A.,
Thomson, C.,
Thompson, P.,
Carroll, D.,
Wolf, E.,
Altbach, M.I., <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">et al.</span>:
Automated breast segmentation of fat and water mr images using dynamic programming.
Academic radiology
<span class="ltx_text ltx_font_bold" id="bib.bib27.2.2">22</span>(2),
139–148
(2015)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fooladivanda et al. [2017]</span>
<span class="ltx_bibblock">
Fooladivanda, A.,
Shokouhi, S.B.,
Ahmadinejad, N.:
Breast-region segmentation in mri using chest region atlas and svm.
Turkish Journal of Electrical Engineering and Computer Sciences
<span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">25</span>(6),
4575–4592
(2017)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA Corporation [2024]</span>
<span class="ltx_bibblock">
NVIDIA Corporation:
GeForce RTX 4090 Graphics Card.
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nvidia.com/nb-no/geforce/graphics-cards/40-series/rtx-4090/" title="">https://www.nvidia.com/nb-no/geforce/graphics-cards/40-series/rtx-4090/</a>.
Accessed: 2024-09-02
(2024)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et al. [2020]</span>
<span class="ltx_bibblock">
Strubell, E.,
Ganesh, A.,
McCallum, A.:
Energy and policy considerations for modern deep learning research.
In: Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 34,
pp. 13693–13696
(2020)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. [2024]</span>
<span class="ltx_bibblock">
Zhong, J.,
Zhong, Y.,
Han, M.,
Yang, T.,
Zhang, Q.:
The impact of ai on carbon emissions: evidence from 66 countries.
Applied Economics
<span class="ltx_text ltx_font_bold" id="bib.bib31.1.1">56</span>(25),
2975–2989
(2024)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamburrini [2022]</span>
<span class="ltx_bibblock">
Tamburrini, G.:
The ai carbon footprint and responsibilities of ai scientists.
Philosophies
<span class="ltx_text ltx_font_bold" id="bib.bib32.1.1">7</span>(1),
4
(2022)


</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 09:40:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
