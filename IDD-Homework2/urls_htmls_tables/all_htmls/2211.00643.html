<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.00643] A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection</title><meta property="og:description" content="Autism Spectrum Disorder (ASD) is a neuro-developmental syndrome resulting from alterations in the embryological brain before birth. This disorder distinguishes its patients by special socially restricted and repetitivâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.00643">

<!--Generated on Thu Mar 14 06:05:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Machine Learning,  Autism-Spectrum Disorder,  Behavioral and Facial traits,  Privacy and Security
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hala Shamseddine <sup id="id11.8.id1" class="ltx_sup"><span id="id11.8.id1.1" class="ltx_text ltx_font_italic">â€ </span></sup>, Safa Otoum <sup id="id12.9.id2" class="ltx_sup"><span id="id12.9.id2.1" class="ltx_text ltx_font_italic">â€¡</span></sup>, Azzam Mourad <sup id="id13.10.id3" class="ltx_sup"><span id="id13.10.id3.1" class="ltx_text ltx_font_italic">â€ </span></sup><sup id="id14.11.id4" class="ltx_sup">â‹†</sup>
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id15.12.id1" class="ltx_sup"><span id="id15.12.id1.1" class="ltx_text ltx_font_italic">â€ </span></sup> Cyber Security Systems and Applied AI Research Center, Department of CSM, Lebanese American University, Lebanon
</span>
<span class="ltx_contact ltx_role_affiliation"><sup id="id16.13.id1" class="ltx_sup"><span id="id16.13.id1.1" class="ltx_text ltx_font_italic">â€¡</span></sup> College of Technological Innovation (CTI), Zayed University, United Arab Emirates
</span>
<span class="ltx_contact ltx_role_affiliation"><sup id="id17.14.id1" class="ltx_sup">â‹†</sup> Division of Science, New York University, Abu Dhabi, United Arab Emirates
</span>
<span class="ltx_contact ltx_role_affiliation"><a href="mailto:hala.shamseddine@lau.edu.lb" title="" class="ltx_ref ltx_href">hala.shamseddine</a>@lau.edu.lb,
<a href="mailto:safa.otoum@zu.ac.ae" title="" class="ltx_ref ltx_href">Safa.Otoum</a>@zu.ac.ae,
<a href="mailto:azzam.mourad@lau.edu.lb" title="" class="ltx_ref ltx_href">azzam.mourad</a>@lau.edu.lb


</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">Autism Spectrum Disorder (ASD) is a neuro-developmental syndrome resulting from alterations in the embryological brain before birth. This disorder distinguishes its patients by special socially restricted and repetitive behavior in addition to specific behavioral traits. Hence, this would possibly deteriorate their social behavior among other individuals, as well as their overall interaction within their community. Moreover, medical research has proved that ASD also affects the facial characteristics of its patients, making the syndrome recognizable from distinctive signs within an individualâ€™s face. Given that as a motivation behind our work, we propose a novel privacy-preserving federated learning scheme to predict ASD in a certain individual based on their behavioral and facial features, embedding a merging process of both data features through facial feature extraction while respecting patient data privacy. After training behavioral and facial image data on federated machine learning models, promising results are achieved, with 70% accuracy for the prediction of ASD according to behavioral traits in a federated learning environment, and a 62% accuracy is reached for the prediction of ASD given an image of the patientâ€™s face. Then, we test the behavior of regular as well as federated ML on our merged data, behavioral and facial, where a 65% accuracy is achieved with the regular logistic regression model and 63% accuracy with the federated learning model.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Machine Learning, Autism-Spectrum Disorder, Behavioral and Facial traits, Privacy and Security

</div>
<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_bold">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">To start with, Autism Spectrum disorder (ASD) could be defined as a complex neuro-developmental syndrome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> characterizing its patients by specific socially restricted and repetitive behavior. In other words, this disorder affects the social and behavioral traits of the patient, hence deteriorating their social behavior among other individuals, as well as their overall interaction within their community. According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, ASD could be a result of alterations in the embryological brain pre-birth, making it a brain-based disorder affecting patients.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Furthermore, ASD patients are distinguished from non-ASD persons not only by unique social and behavioral characteristics, but also by distinct facial features, as demonstrated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
According to the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, neuro-developmental disorders such as ASD can cause craniofacial abnormalities, or atypical facial features.
For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> prove that female and male individuals with higher levels of autistic traits are associated with less feminine and masculine facial structures, respectively. Furthermore, facial measurements taken in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> form a superb autism bio-marker. As a result, according to past medical studies, ASD sufferers have distinct facial traits that identify them from normal individuals.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Some previous studies have proposed predicting whether a patient has ASD using Machine learning (ML), a sub-field of Artificial Intelligence (AI). ML is based on giving a computer program the ability to learn from data so that new predictions are made <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This learning process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> could be divided into supervised learning, when data is labelled, or unsupervised learning, when the trainer learns patterns on unlabeled data. Machine learning applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> are majorly split into classification and regression, as the predicted value is either a discrete class/label or a continuous value, respectively. With the advancement of this science, a major application of ML has appeared in the healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and medical fields <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, specifically in disease prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Predicting ASD in children could help in the early diagnosis of this syndrome, which may be made possible using ML. However, data used for training such a machine learner is medical data, thus sensitive and classified as private to patients. Hence, to preserve the privacy of patients, we propose using a federated learning-based privacy-preserving framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> to predict whether a patient has ASD or not, based on behavioral as well as facial image data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Federated Learning is based on combining privacy with regular ML. Training on data by ML algorithms occurs on distributed sites, called clients, where raw data is not shared among clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, maintaining patientsâ€™ privacy. On the other hand, each client shares their local ML model weights, after learning its bulk of data, with a global aggregator who averages model parameters of all learned models by clients into a global model used for prediction. To the best of our knowledge, none of the previous works in the literature have used federated learning for predicting ASD using behavioral or facial image data. We, therefore, provide a novel privacy-preserving federated learning scheme for predicting ASD based on behavioral data and facial data separately, and then propose a merging method, through feature extraction, for both data sets to predict ASD according to a combination of both behavioral and facial traits.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contributions of this work are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Introducing a privacy-preserving federated learning model for predicting ASD based on behavioral and social characteristics of a patient.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Proposing a federated learning model for the diagnosis of ASD based on a clear image of the patientâ€™s face.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Through combining both behavioral and facial datasets via feature extraction methods, we devised a novel federated learning scheme for predicting ASD in patients from a behavioral and facial perspective.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The rest of the paper is organized as follows. In Section <a href="#S2" title="II Related Work â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we tackle some of the previous work in the literature. In Section <a href="#S3" title="III Proposed Scheme â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we illustrate the proposed FL scheme, discuss its components, and describe the model evaluation criteria. Next, in section <a href="#S4" title="IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we present the combined scheme of both behavioral and facial datasets under study and introduce the multi-aspect ASD prediction federated learning model. In Section <a href="#S5" title="V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we introduce the datasets used for training the machine learners, the FL Setup, and the experimental results. In Section <a href="#S6" title="VI Discussion â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, we discuss the results in comparison with previous methods and evaluate the performance of the proposed scheme. Finally, we conclude the paper in Section <a href="#S7" title="VII Conclusion â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_bold">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Several fields have proved the efficiency of applying regular as well as federated ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to predict certain real-world answers.
According to authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, a major application of ML would be in the field of medicine and healthcare, such as predicting certain human diseases, traits, or syndromes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Furthermore, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the authors tackle the issue of medical data scarcity and sensitivity, by proposing an additional level of privacy protection in medical imaging data through the use of federated privacy-preserving artificial intelligence. In federated machine learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, data is stored locally at each distributed client, training on it locally without moving the data outside the client devices, thus preserving the data privacy.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">For instance, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, authors proposed several machine learning as well as data mining methodologies for the sake of predicting heart diseases in patients, given their seriousness and the importance of early diagnosis of such diseases. Moreover, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> introduced machine learning models for predicting Parkinsonâ€™s disease based on biomedical voice measurements of patients. To add more, medical image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> plays a vital role in the clinical diagnosis of remarkable diseases and thus helps in the treatment process through early discovery of illnesses and disorders. Then, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> used deep learning methods to prove its effectiveness through disease prediction using neural network models.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Another effective main syndrome prediction using ML techniques would be Autism Spectrum Disorder (ASD), which is what we focus on in this paper.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, authors used ML classification models with 80% accuracy to predict whether a child is autistic or not based on behavioral data collected automatically from tablet games and eye-tracking data captured from face-to-face conversations with individuals, respectively. Also, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, eye-tracking information and image content was used to detect if an individual has ASD. Moreover, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposed a new ML technique called Rules-ML that reveals autistic traits of cases while offering knowledge rules, achieving an accuracy higher than other regular ML strategies. Furthermore, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> improved the measured accuracy in ASD classification, reaching a near 100% accuracy using the Multi-Layer Perceptron ML model on behavioral specific collected data.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, who used the same behavioral dataset as we aim at using in this paper, found that using random forests, decision trees, and support vector machine algorithms results in around 90% accuracy, which improves with using feature selection methods. A remarkable dataset used in the literature was the ABIDE dataset, which consisted of MRI neuro-images in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Deep learning models were proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to classify ASD vs. non-ASD patients, achieving a 70% accuracy. Using the same dataset of neuro-images, federated learning was used to associate an MRI image with an autistic individual or not <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, preserving the privacy of screened patients. However, none of the previous studies mentioned in the literature, to the best of our knowledge, has used federated learning to predict ASD based on behavioral data and facial features.</p>
</div>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_bold">Proposed Scheme</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present the problem definition and scheme overview, including the evaluation metrics.</p>
</div>
<section id="S3.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic"> </span><span id="S3.SS1.7.3" class="ltx_text ltx_font_bold">Problem Definition</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Notable applications for ML have been recently demonstrated in the fields of healthcare and medicine, specifically for predicting whether a certain patient carries a specific disease or not. In this context, we tackle the research problem of predicting Autism Spectrum Disorder (ASD) based on a set of input features or traits, based on two data formats separately. We then test the same problem within a combined approach, representing the different aspects of the disorder: behavioral and facial.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">First, given a set of behavioral features, including age and gender, in addition to behavior-specific traits, our federated learning model would predict whether the corresponding patient is autistic or not, having learned securely on the training data discussed in section <a href="#S5.SS1" title="V-A Datasets â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>. Moreover, the second part of our work, which includes another aspect of ASD for prediction, is based on the facial features of a patient. Given a clear image of their face, showing all of their facial sides and characteristics, our model would also return a classification of ASD for the respective patient, while preserving the privacy of data and images. Third, given a combination of these features, behavioral as well as facial landmarks, our model would be able to accurately classify whether the individual is autistic or not, by extracting facial distances from facial images and combining these features with behavioral ones.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_bold">Scheme Overview</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 â€£ III-B Scheme Overview â€£ III Proposed Scheme â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the architecture of our proposed scheme. As could be clearly noticed, we have two main entities: the ASD screening centers as well as the global aggregator.
First, given a set of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">n</annotation></semantics></math> ASD screening centers <math id="S3.SS2.p1.2.m2.2" class="ltx_Math" alttext="Sc_{1},Sc_{2}...Sc_{n}" display="inline"><semantics id="S3.SS2.p1.2.m2.2a"><mrow id="S3.SS2.p1.2.m2.2.2.2" xref="S3.SS2.p1.2.m2.2.2.3.cmml"><mrow id="S3.SS2.p1.2.m2.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p1.2.m2.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.1.1.3.2.cmml">c</mi><mn id="S3.SS2.p1.2.m2.1.1.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S3.SS2.p1.2.m2.2.2.2.3" xref="S3.SS2.p1.2.m2.2.2.3.cmml">,</mo><mrow id="S3.SS2.p1.2.m2.2.2.2.2" xref="S3.SS2.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS2.p1.2.m2.2.2.2.2.2" xref="S3.SS2.p1.2.m2.2.2.2.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.2.2.2.2.1" xref="S3.SS2.p1.2.m2.2.2.2.2.1.cmml">â€‹</mo><msub id="S3.SS2.p1.2.m2.2.2.2.2.3" xref="S3.SS2.p1.2.m2.2.2.2.2.3.cmml"><mi id="S3.SS2.p1.2.m2.2.2.2.2.3.2" xref="S3.SS2.p1.2.m2.2.2.2.2.3.2.cmml">c</mi><mn id="S3.SS2.p1.2.m2.2.2.2.2.3.3" xref="S3.SS2.p1.2.m2.2.2.2.2.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.2.2.2.2.1a" xref="S3.SS2.p1.2.m2.2.2.2.2.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p1.2.m2.2.2.2.2.4" xref="S3.SS2.p1.2.m2.2.2.2.2.4.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.2.2.2.2.1b" xref="S3.SS2.p1.2.m2.2.2.2.2.1.cmml">â€‹</mo><mi id="S3.SS2.p1.2.m2.2.2.2.2.5" xref="S3.SS2.p1.2.m2.2.2.2.2.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.2.2.2.2.1c" xref="S3.SS2.p1.2.m2.2.2.2.2.1.cmml">â€‹</mo><msub id="S3.SS2.p1.2.m2.2.2.2.2.6" xref="S3.SS2.p1.2.m2.2.2.2.2.6.cmml"><mi id="S3.SS2.p1.2.m2.2.2.2.2.6.2" xref="S3.SS2.p1.2.m2.2.2.2.2.6.2.cmml">c</mi><mi id="S3.SS2.p1.2.m2.2.2.2.2.6.3" xref="S3.SS2.p1.2.m2.2.2.2.2.6.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.2b"><list id="S3.SS2.p1.2.m2.2.2.3.cmml" xref="S3.SS2.p1.2.m2.2.2.2"><apply id="S3.SS2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.2">ğ‘†</ci><apply id="S3.SS2.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.3.2">ğ‘</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS2.p1.2.m2.2.2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2"><times id="S3.SS2.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.1"></times><ci id="S3.SS2.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.2">ğ‘†</ci><apply id="S3.SS2.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.2.2.2.2.3.1.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.2.2.2.2.3.2.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.3.2">ğ‘</ci><cn type="integer" id="S3.SS2.p1.2.m2.2.2.2.2.3.3.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.3.3">2</cn></apply><ci id="S3.SS2.p1.2.m2.2.2.2.2.4.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.4">â€¦</ci><ci id="S3.SS2.p1.2.m2.2.2.2.2.5.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.5">ğ‘†</ci><apply id="S3.SS2.p1.2.m2.2.2.2.2.6.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.6"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.2.2.2.2.6.1.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.6">subscript</csymbol><ci id="S3.SS2.p1.2.m2.2.2.2.2.6.2.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.6.2">ğ‘</ci><ci id="S3.SS2.p1.2.m2.2.2.2.2.6.3.cmml" xref="S3.SS2.p1.2.m2.2.2.2.2.6.3">ğ‘›</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.2c">Sc_{1},Sc_{2}...Sc_{n}</annotation></semantics></math>, each conducting certain surveys on screening ASD patients, screening images for the same patients, or collecting any data related to the latter. These results and data instances are stored locally within the local database of every center, without any sharing with external entities, given the confidentiality and sensitivity of data at hand, and respecting patientsâ€™ privacy. Therefore, every center has a local database <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="Ld_{i}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">d</mi><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><times id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></times><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">ğ¿</ci><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">Ld_{i}</annotation></semantics></math> that includes the data collected. In our case, data is split into two: the first local dataset contains behavioral data, and the second contains facial data.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.5" class="ltx_p">Then, for the sake of training, every entity would train on its own data stored in its local database, <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="Ld_{i}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">d</mi><mi id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ¿</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">Ld_{i}</annotation></semantics></math>. For example, <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="Sc_{1}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">c</mi><mn id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ‘†</ci><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">ğ‘</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">Sc_{1}</annotation></semantics></math> would train on <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="Ld_{1}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">d</mi><mn id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></times><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ğ¿</ci><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">Ld_{1}</annotation></semantics></math>, <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="Sc_{2}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.cmml">c</mi><mn id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ğ‘†</ci><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2">ğ‘</ci><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">Sc_{2}</annotation></semantics></math> on <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="Ld_{2}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">d</mi><mn id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><times id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></times><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">ğ¿</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">Ld_{2}</annotation></semantics></math>, etc. Hence, every training process would result in a local ML model trained on the corresponding data in its local database, represented as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.8" class="ltx_Math" alttext="M_{i}=\sum_{i,j=1}^{i,j=m}w_{i,j}.X_{i}" display="block"><semantics id="S3.E1.m1.8a"><mrow id="S3.E1.m1.8.8.2" xref="S3.E1.m1.8.8.3.cmml"><mrow id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml"><msub id="S3.E1.m1.7.7.1.1.2" xref="S3.E1.m1.7.7.1.1.2.cmml"><mi id="S3.E1.m1.7.7.1.1.2.2" xref="S3.E1.m1.7.7.1.1.2.2.cmml">M</mi><mi id="S3.E1.m1.7.7.1.1.2.3" xref="S3.E1.m1.7.7.1.1.2.3.cmml">i</mi></msub><mo rspace="0.111em" id="S3.E1.m1.7.7.1.1.1" xref="S3.E1.m1.7.7.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.7.7.1.1.3" xref="S3.E1.m1.7.7.1.1.3.cmml"><munderover id="S3.E1.m1.7.7.1.1.3.1" xref="S3.E1.m1.7.7.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.7.7.1.1.3.1.2.2" xref="S3.E1.m1.7.7.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.1.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.4.2.1" xref="S3.E1.m1.2.2.2.4.1.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">j</mi></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">=</mo><mn id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml">1</mn></mrow><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml"><mrow id="S3.E1.m1.4.4.2.4.2" xref="S3.E1.m1.4.4.2.4.1.cmml"><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">i</mi><mo id="S3.E1.m1.4.4.2.4.2.1" xref="S3.E1.m1.4.4.2.4.1.cmml">,</mo><mi id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">j</mi></mrow><mo id="S3.E1.m1.4.4.2.3" xref="S3.E1.m1.4.4.2.3.cmml">=</mo><mi id="S3.E1.m1.4.4.2.5" xref="S3.E1.m1.4.4.2.5.cmml">m</mi></mrow></munderover><msub id="S3.E1.m1.7.7.1.1.3.2" xref="S3.E1.m1.7.7.1.1.3.2.cmml"><mi id="S3.E1.m1.7.7.1.1.3.2.2" xref="S3.E1.m1.7.7.1.1.3.2.2.cmml">w</mi><mrow id="S3.E1.m1.6.6.2.4" xref="S3.E1.m1.6.6.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml">i</mi><mo id="S3.E1.m1.6.6.2.4.1" xref="S3.E1.m1.6.6.2.3.cmml">,</mo><mi id="S3.E1.m1.6.6.2.2" xref="S3.E1.m1.6.6.2.2.cmml">j</mi></mrow></msub></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S3.E1.m1.8.8.2.3" xref="S3.E1.m1.8.8.3a.cmml">.</mo><msub id="S3.E1.m1.8.8.2.2" xref="S3.E1.m1.8.8.2.2.cmml"><mi id="S3.E1.m1.8.8.2.2.2" xref="S3.E1.m1.8.8.2.2.2.cmml">X</mi><mi id="S3.E1.m1.8.8.2.2.3" xref="S3.E1.m1.8.8.2.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.8b"><apply id="S3.E1.m1.8.8.3.cmml" xref="S3.E1.m1.8.8.2"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.3a.cmml" xref="S3.E1.m1.8.8.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1.1"><eq id="S3.E1.m1.7.7.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1"></eq><apply id="S3.E1.m1.7.7.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.2">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2">ğ‘€</ci><ci id="S3.E1.m1.7.7.1.1.2.3.cmml" xref="S3.E1.m1.7.7.1.1.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.7.7.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3"><apply id="S3.E1.m1.7.7.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.1.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.3.1.2.cmml" xref="S3.E1.m1.7.7.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.7.7.1.1.3.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.1.2.2"></sum><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><eq id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></eq><list id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4.2"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘–</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘—</ci></list><cn type="integer" id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5">1</cn></apply></apply><apply id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"><eq id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.3"></eq><list id="S3.E1.m1.4.4.2.4.1.cmml" xref="S3.E1.m1.4.4.2.4.2"><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">ğ‘–</ci><ci id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2">ğ‘—</ci></list><ci id="S3.E1.m1.4.4.2.5.cmml" xref="S3.E1.m1.4.4.2.5">ğ‘š</ci></apply></apply><apply id="S3.E1.m1.7.7.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2">ğ‘¤</ci><list id="S3.E1.m1.6.6.2.3.cmml" xref="S3.E1.m1.6.6.2.4"><ci id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1.1">ğ‘–</ci><ci id="S3.E1.m1.6.6.2.2.cmml" xref="S3.E1.m1.6.6.2.2">ğ‘—</ci></list></apply></apply></apply><apply id="S3.E1.m1.8.8.2.2.cmml" xref="S3.E1.m1.8.8.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.2.2.1.cmml" xref="S3.E1.m1.8.8.2.2">subscript</csymbol><ci id="S3.E1.m1.8.8.2.2.2.cmml" xref="S3.E1.m1.8.8.2.2.2">ğ‘‹</ci><ci id="S3.E1.m1.8.8.2.2.3.cmml" xref="S3.E1.m1.8.8.2.2.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.8c">M_{i}=\sum_{i,j=1}^{i,j=m}w_{i,j}.X_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.9" class="ltx_p">where <math id="S3.SS2.p2.6.m1.9" class="ltx_Math" alttext="w_{1,1},w_{1,2},...w_{i,m}" display="inline"><semantics id="S3.SS2.p2.6.m1.9a"><mrow id="S3.SS2.p2.6.m1.9.9.3" xref="S3.SS2.p2.6.m1.9.9.4.cmml"><msub id="S3.SS2.p2.6.m1.7.7.1.1" xref="S3.SS2.p2.6.m1.7.7.1.1.cmml"><mi id="S3.SS2.p2.6.m1.7.7.1.1.2" xref="S3.SS2.p2.6.m1.7.7.1.1.2.cmml">w</mi><mrow id="S3.SS2.p2.6.m1.2.2.2.4" xref="S3.SS2.p2.6.m1.2.2.2.3.cmml"><mn id="S3.SS2.p2.6.m1.1.1.1.1" xref="S3.SS2.p2.6.m1.1.1.1.1.cmml">1</mn><mo id="S3.SS2.p2.6.m1.2.2.2.4.1" xref="S3.SS2.p2.6.m1.2.2.2.3.cmml">,</mo><mn id="S3.SS2.p2.6.m1.2.2.2.2" xref="S3.SS2.p2.6.m1.2.2.2.2.cmml">1</mn></mrow></msub><mo id="S3.SS2.p2.6.m1.9.9.3.4" xref="S3.SS2.p2.6.m1.9.9.4.cmml">,</mo><msub id="S3.SS2.p2.6.m1.8.8.2.2" xref="S3.SS2.p2.6.m1.8.8.2.2.cmml"><mi id="S3.SS2.p2.6.m1.8.8.2.2.2" xref="S3.SS2.p2.6.m1.8.8.2.2.2.cmml">w</mi><mrow id="S3.SS2.p2.6.m1.4.4.2.4" xref="S3.SS2.p2.6.m1.4.4.2.3.cmml"><mn id="S3.SS2.p2.6.m1.3.3.1.1" xref="S3.SS2.p2.6.m1.3.3.1.1.cmml">1</mn><mo id="S3.SS2.p2.6.m1.4.4.2.4.1" xref="S3.SS2.p2.6.m1.4.4.2.3.cmml">,</mo><mn id="S3.SS2.p2.6.m1.4.4.2.2" xref="S3.SS2.p2.6.m1.4.4.2.2.cmml">2</mn></mrow></msub><mo id="S3.SS2.p2.6.m1.9.9.3.5" xref="S3.SS2.p2.6.m1.9.9.4.cmml">,</mo><mrow id="S3.SS2.p2.6.m1.9.9.3.3" xref="S3.SS2.p2.6.m1.9.9.3.3.cmml"><mi mathvariant="normal" id="S3.SS2.p2.6.m1.9.9.3.3.2" xref="S3.SS2.p2.6.m1.9.9.3.3.2.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m1.9.9.3.3.1" xref="S3.SS2.p2.6.m1.9.9.3.3.1.cmml">â€‹</mo><msub id="S3.SS2.p2.6.m1.9.9.3.3.3" xref="S3.SS2.p2.6.m1.9.9.3.3.3.cmml"><mi id="S3.SS2.p2.6.m1.9.9.3.3.3.2" xref="S3.SS2.p2.6.m1.9.9.3.3.3.2.cmml">w</mi><mrow id="S3.SS2.p2.6.m1.6.6.2.4" xref="S3.SS2.p2.6.m1.6.6.2.3.cmml"><mi id="S3.SS2.p2.6.m1.5.5.1.1" xref="S3.SS2.p2.6.m1.5.5.1.1.cmml">i</mi><mo id="S3.SS2.p2.6.m1.6.6.2.4.1" xref="S3.SS2.p2.6.m1.6.6.2.3.cmml">,</mo><mi id="S3.SS2.p2.6.m1.6.6.2.2" xref="S3.SS2.p2.6.m1.6.6.2.2.cmml">m</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m1.9b"><list id="S3.SS2.p2.6.m1.9.9.4.cmml" xref="S3.SS2.p2.6.m1.9.9.3"><apply id="S3.SS2.p2.6.m1.7.7.1.1.cmml" xref="S3.SS2.p2.6.m1.7.7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m1.7.7.1.1.1.cmml" xref="S3.SS2.p2.6.m1.7.7.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m1.7.7.1.1.2.cmml" xref="S3.SS2.p2.6.m1.7.7.1.1.2">ğ‘¤</ci><list id="S3.SS2.p2.6.m1.2.2.2.3.cmml" xref="S3.SS2.p2.6.m1.2.2.2.4"><cn type="integer" id="S3.SS2.p2.6.m1.1.1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1.1.1">1</cn><cn type="integer" id="S3.SS2.p2.6.m1.2.2.2.2.cmml" xref="S3.SS2.p2.6.m1.2.2.2.2">1</cn></list></apply><apply id="S3.SS2.p2.6.m1.8.8.2.2.cmml" xref="S3.SS2.p2.6.m1.8.8.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m1.8.8.2.2.1.cmml" xref="S3.SS2.p2.6.m1.8.8.2.2">subscript</csymbol><ci id="S3.SS2.p2.6.m1.8.8.2.2.2.cmml" xref="S3.SS2.p2.6.m1.8.8.2.2.2">ğ‘¤</ci><list id="S3.SS2.p2.6.m1.4.4.2.3.cmml" xref="S3.SS2.p2.6.m1.4.4.2.4"><cn type="integer" id="S3.SS2.p2.6.m1.3.3.1.1.cmml" xref="S3.SS2.p2.6.m1.3.3.1.1">1</cn><cn type="integer" id="S3.SS2.p2.6.m1.4.4.2.2.cmml" xref="S3.SS2.p2.6.m1.4.4.2.2">2</cn></list></apply><apply id="S3.SS2.p2.6.m1.9.9.3.3.cmml" xref="S3.SS2.p2.6.m1.9.9.3.3"><times id="S3.SS2.p2.6.m1.9.9.3.3.1.cmml" xref="S3.SS2.p2.6.m1.9.9.3.3.1"></times><ci id="S3.SS2.p2.6.m1.9.9.3.3.2.cmml" xref="S3.SS2.p2.6.m1.9.9.3.3.2">â€¦</ci><apply id="S3.SS2.p2.6.m1.9.9.3.3.3.cmml" xref="S3.SS2.p2.6.m1.9.9.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m1.9.9.3.3.3.1.cmml" xref="S3.SS2.p2.6.m1.9.9.3.3.3">subscript</csymbol><ci id="S3.SS2.p2.6.m1.9.9.3.3.3.2.cmml" xref="S3.SS2.p2.6.m1.9.9.3.3.3.2">ğ‘¤</ci><list id="S3.SS2.p2.6.m1.6.6.2.3.cmml" xref="S3.SS2.p2.6.m1.6.6.2.4"><ci id="S3.SS2.p2.6.m1.5.5.1.1.cmml" xref="S3.SS2.p2.6.m1.5.5.1.1">ğ‘–</ci><ci id="S3.SS2.p2.6.m1.6.6.2.2.cmml" xref="S3.SS2.p2.6.m1.6.6.2.2">ğ‘š</ci></list></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m1.9c">w_{1,1},w_{1,2},...w_{i,m}</annotation></semantics></math> are the <math id="S3.SS2.p2.7.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.p2.7.m2.1a"><mi id="S3.SS2.p2.7.m2.1.1" xref="S3.SS2.p2.7.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m2.1b"><ci id="S3.SS2.p2.7.m2.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m2.1c">m</annotation></semantics></math> local model weights and <math id="S3.SS2.p2.8.m3.2" class="ltx_Math" alttext="X_{1},X_{2}...X_{m}" display="inline"><semantics id="S3.SS2.p2.8.m3.2a"><mrow id="S3.SS2.p2.8.m3.2.2.2" xref="S3.SS2.p2.8.m3.2.2.3.cmml"><msub id="S3.SS2.p2.8.m3.1.1.1.1" xref="S3.SS2.p2.8.m3.1.1.1.1.cmml"><mi id="S3.SS2.p2.8.m3.1.1.1.1.2" xref="S3.SS2.p2.8.m3.1.1.1.1.2.cmml">X</mi><mn id="S3.SS2.p2.8.m3.1.1.1.1.3" xref="S3.SS2.p2.8.m3.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p2.8.m3.2.2.2.3" xref="S3.SS2.p2.8.m3.2.2.3.cmml">,</mo><mrow id="S3.SS2.p2.8.m3.2.2.2.2" xref="S3.SS2.p2.8.m3.2.2.2.2.cmml"><msub id="S3.SS2.p2.8.m3.2.2.2.2.2" xref="S3.SS2.p2.8.m3.2.2.2.2.2.cmml"><mi id="S3.SS2.p2.8.m3.2.2.2.2.2.2" xref="S3.SS2.p2.8.m3.2.2.2.2.2.2.cmml">X</mi><mn id="S3.SS2.p2.8.m3.2.2.2.2.2.3" xref="S3.SS2.p2.8.m3.2.2.2.2.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m3.2.2.2.2.1" xref="S3.SS2.p2.8.m3.2.2.2.2.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p2.8.m3.2.2.2.2.3" xref="S3.SS2.p2.8.m3.2.2.2.2.3.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m3.2.2.2.2.1a" xref="S3.SS2.p2.8.m3.2.2.2.2.1.cmml">â€‹</mo><msub id="S3.SS2.p2.8.m3.2.2.2.2.4" xref="S3.SS2.p2.8.m3.2.2.2.2.4.cmml"><mi id="S3.SS2.p2.8.m3.2.2.2.2.4.2" xref="S3.SS2.p2.8.m3.2.2.2.2.4.2.cmml">X</mi><mi id="S3.SS2.p2.8.m3.2.2.2.2.4.3" xref="S3.SS2.p2.8.m3.2.2.2.2.4.3.cmml">m</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m3.2b"><list id="S3.SS2.p2.8.m3.2.2.3.cmml" xref="S3.SS2.p2.8.m3.2.2.2"><apply id="S3.SS2.p2.8.m3.1.1.1.1.cmml" xref="S3.SS2.p2.8.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m3.1.1.1.1.1.cmml" xref="S3.SS2.p2.8.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m3.1.1.1.1.2.cmml" xref="S3.SS2.p2.8.m3.1.1.1.1.2">ğ‘‹</ci><cn type="integer" id="S3.SS2.p2.8.m3.1.1.1.1.3.cmml" xref="S3.SS2.p2.8.m3.1.1.1.1.3">1</cn></apply><apply id="S3.SS2.p2.8.m3.2.2.2.2.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2"><times id="S3.SS2.p2.8.m3.2.2.2.2.1.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.1"></times><apply id="S3.SS2.p2.8.m3.2.2.2.2.2.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m3.2.2.2.2.2.1.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.8.m3.2.2.2.2.2.2.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.2.2">ğ‘‹</ci><cn type="integer" id="S3.SS2.p2.8.m3.2.2.2.2.2.3.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.2.3">2</cn></apply><ci id="S3.SS2.p2.8.m3.2.2.2.2.3.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.3">â€¦</ci><apply id="S3.SS2.p2.8.m3.2.2.2.2.4.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m3.2.2.2.2.4.1.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.4">subscript</csymbol><ci id="S3.SS2.p2.8.m3.2.2.2.2.4.2.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.4.2">ğ‘‹</ci><ci id="S3.SS2.p2.8.m3.2.2.2.2.4.3.cmml" xref="S3.SS2.p2.8.m3.2.2.2.2.4.3">ğ‘š</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m3.2c">X_{1},X_{2}...X_{m}</annotation></semantics></math> are <math id="S3.SS2.p2.9.m4.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.p2.9.m4.1a"><mi id="S3.SS2.p2.9.m4.1.1" xref="S3.SS2.p2.9.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m4.1b"><ci id="S3.SS2.p2.9.m4.1.1.cmml" xref="S3.SS2.p2.9.m4.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m4.1c">m</annotation></semantics></math> model features.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2211.00643/assets/images/newarch.drawio.drawio.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="680" height="882" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Federated Machine Learning Training for ASD Prediction</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.8" class="ltx_p">To clarify, the screening center <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="Sc_{1}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">c</mi><mn id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><times id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></times><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ğ‘†</ci><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">ğ‘</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">Sc_{1}</annotation></semantics></math>, having a local database <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="Ld_{1}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.3.2.cmml">d</mi><mn id="S3.SS2.p3.2.m2.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><times id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"></times><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ¿</ci><apply id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.3.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">Ld_{1}</annotation></semantics></math>, contains behavioral as well as facial data. First, local training on the behavioral data takes place, producing a local model <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="M_{1}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">M</mi><mn id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">M_{1}</annotation></semantics></math>. Then, the client trains locally on the facial dataset within the same local database <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="Ld_{1}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m4.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.p3.4.m4.1.1.3.2" xref="S3.SS2.p3.4.m4.1.1.3.2.cmml">d</mi><mn id="S3.SS2.p3.4.m4.1.1.3.3" xref="S3.SS2.p3.4.m4.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><times id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1"></times><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">ğ¿</ci><apply id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.p3.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.p3.4.m4.1.1.3.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">Ld_{1}</annotation></semantics></math>, resulting in a local model <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="M_{2}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">M</mi><mn id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">M_{2}</annotation></semantics></math>. Finally, facial feature extraction is done on the facial data in database <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="Ld_{1}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.1.1" xref="S3.SS2.p3.6.m6.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml"><mi id="S3.SS2.p3.6.m6.1.1.3.2" xref="S3.SS2.p3.6.m6.1.1.3.2.cmml">d</mi><mn id="S3.SS2.p3.6.m6.1.1.3.3" xref="S3.SS2.p3.6.m6.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><times id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1"></times><ci id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">ğ¿</ci><apply id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.3.1.cmml" xref="S3.SS2.p3.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.3.2.cmml" xref="S3.SS2.p3.6.m6.1.1.3.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.6.m6.1.1.3.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">Ld_{1}</annotation></semantics></math>, and extracted facial features are then merged with behavioral data in dataset <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="Ld_{1}" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mrow id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3.cmml"><mi id="S3.SS2.p3.7.m7.1.1.3.2" xref="S3.SS2.p3.7.m7.1.1.3.2.cmml">d</mi><mn id="S3.SS2.p3.7.m7.1.1.3.3" xref="S3.SS2.p3.7.m7.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><times id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1"></times><ci id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2">ğ¿</ci><apply id="S3.SS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.3.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.7.m7.1.1.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">Ld_{1}</annotation></semantics></math> of the same client (more details are provided in Section <a href="#S4" title="IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>). This process results in a third local dataset consisting of merged data containing behavioral as well as facial features, on which the client also trains locally, producing a third local model <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="M_{3}" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><msub id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">M</mi><mn id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">M_{3}</annotation></semantics></math>. These steps are repeated by all the clients, from which three local models are obtained for each one: the first one from training on behavioral data, the second on facial data, and the third on merged data.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.11" class="ltx_p">In order to achieve a global model capable of combining the ML results of all the centers into a single framework, a global aggregator (GA) is introduced. Once local training is done at each screening entity, and an <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">M</mi><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ğ‘€</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">M_{i}</annotation></semantics></math> local model is obtained at each <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="Sc_{i}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml"><mi id="S3.SS2.p4.2.m2.1.1.3.2" xref="S3.SS2.p4.2.m2.1.1.3.2.cmml">c</mi><mi id="S3.SS2.p4.2.m2.1.1.3.3" xref="S3.SS2.p4.2.m2.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><times id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></times><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ğ‘†</ci><apply id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p4.2.m2.1.1.3.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">Sc_{i}</annotation></semantics></math>, the ML model weights <math id="S3.SS2.p4.3.m3.9" class="ltx_Math" alttext="w_{1,1},w_{1,2},...w_{i,m}" display="inline"><semantics id="S3.SS2.p4.3.m3.9a"><mrow id="S3.SS2.p4.3.m3.9.9.3" xref="S3.SS2.p4.3.m3.9.9.4.cmml"><msub id="S3.SS2.p4.3.m3.7.7.1.1" xref="S3.SS2.p4.3.m3.7.7.1.1.cmml"><mi id="S3.SS2.p4.3.m3.7.7.1.1.2" xref="S3.SS2.p4.3.m3.7.7.1.1.2.cmml">w</mi><mrow id="S3.SS2.p4.3.m3.2.2.2.4" xref="S3.SS2.p4.3.m3.2.2.2.3.cmml"><mn id="S3.SS2.p4.3.m3.1.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.1.cmml">1</mn><mo id="S3.SS2.p4.3.m3.2.2.2.4.1" xref="S3.SS2.p4.3.m3.2.2.2.3.cmml">,</mo><mn id="S3.SS2.p4.3.m3.2.2.2.2" xref="S3.SS2.p4.3.m3.2.2.2.2.cmml">1</mn></mrow></msub><mo id="S3.SS2.p4.3.m3.9.9.3.4" xref="S3.SS2.p4.3.m3.9.9.4.cmml">,</mo><msub id="S3.SS2.p4.3.m3.8.8.2.2" xref="S3.SS2.p4.3.m3.8.8.2.2.cmml"><mi id="S3.SS2.p4.3.m3.8.8.2.2.2" xref="S3.SS2.p4.3.m3.8.8.2.2.2.cmml">w</mi><mrow id="S3.SS2.p4.3.m3.4.4.2.4" xref="S3.SS2.p4.3.m3.4.4.2.3.cmml"><mn id="S3.SS2.p4.3.m3.3.3.1.1" xref="S3.SS2.p4.3.m3.3.3.1.1.cmml">1</mn><mo id="S3.SS2.p4.3.m3.4.4.2.4.1" xref="S3.SS2.p4.3.m3.4.4.2.3.cmml">,</mo><mn id="S3.SS2.p4.3.m3.4.4.2.2" xref="S3.SS2.p4.3.m3.4.4.2.2.cmml">2</mn></mrow></msub><mo id="S3.SS2.p4.3.m3.9.9.3.5" xref="S3.SS2.p4.3.m3.9.9.4.cmml">,</mo><mrow id="S3.SS2.p4.3.m3.9.9.3.3" xref="S3.SS2.p4.3.m3.9.9.3.3.cmml"><mi mathvariant="normal" id="S3.SS2.p4.3.m3.9.9.3.3.2" xref="S3.SS2.p4.3.m3.9.9.3.3.2.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.3.m3.9.9.3.3.1" xref="S3.SS2.p4.3.m3.9.9.3.3.1.cmml">â€‹</mo><msub id="S3.SS2.p4.3.m3.9.9.3.3.3" xref="S3.SS2.p4.3.m3.9.9.3.3.3.cmml"><mi id="S3.SS2.p4.3.m3.9.9.3.3.3.2" xref="S3.SS2.p4.3.m3.9.9.3.3.3.2.cmml">w</mi><mrow id="S3.SS2.p4.3.m3.6.6.2.4" xref="S3.SS2.p4.3.m3.6.6.2.3.cmml"><mi id="S3.SS2.p4.3.m3.5.5.1.1" xref="S3.SS2.p4.3.m3.5.5.1.1.cmml">i</mi><mo id="S3.SS2.p4.3.m3.6.6.2.4.1" xref="S3.SS2.p4.3.m3.6.6.2.3.cmml">,</mo><mi id="S3.SS2.p4.3.m3.6.6.2.2" xref="S3.SS2.p4.3.m3.6.6.2.2.cmml">m</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.9b"><list id="S3.SS2.p4.3.m3.9.9.4.cmml" xref="S3.SS2.p4.3.m3.9.9.3"><apply id="S3.SS2.p4.3.m3.7.7.1.1.cmml" xref="S3.SS2.p4.3.m3.7.7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.7.7.1.1.1.cmml" xref="S3.SS2.p4.3.m3.7.7.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.7.7.1.1.2.cmml" xref="S3.SS2.p4.3.m3.7.7.1.1.2">ğ‘¤</ci><list id="S3.SS2.p4.3.m3.2.2.2.3.cmml" xref="S3.SS2.p4.3.m3.2.2.2.4"><cn type="integer" id="S3.SS2.p4.3.m3.1.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1.1">1</cn><cn type="integer" id="S3.SS2.p4.3.m3.2.2.2.2.cmml" xref="S3.SS2.p4.3.m3.2.2.2.2">1</cn></list></apply><apply id="S3.SS2.p4.3.m3.8.8.2.2.cmml" xref="S3.SS2.p4.3.m3.8.8.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.8.8.2.2.1.cmml" xref="S3.SS2.p4.3.m3.8.8.2.2">subscript</csymbol><ci id="S3.SS2.p4.3.m3.8.8.2.2.2.cmml" xref="S3.SS2.p4.3.m3.8.8.2.2.2">ğ‘¤</ci><list id="S3.SS2.p4.3.m3.4.4.2.3.cmml" xref="S3.SS2.p4.3.m3.4.4.2.4"><cn type="integer" id="S3.SS2.p4.3.m3.3.3.1.1.cmml" xref="S3.SS2.p4.3.m3.3.3.1.1">1</cn><cn type="integer" id="S3.SS2.p4.3.m3.4.4.2.2.cmml" xref="S3.SS2.p4.3.m3.4.4.2.2">2</cn></list></apply><apply id="S3.SS2.p4.3.m3.9.9.3.3.cmml" xref="S3.SS2.p4.3.m3.9.9.3.3"><times id="S3.SS2.p4.3.m3.9.9.3.3.1.cmml" xref="S3.SS2.p4.3.m3.9.9.3.3.1"></times><ci id="S3.SS2.p4.3.m3.9.9.3.3.2.cmml" xref="S3.SS2.p4.3.m3.9.9.3.3.2">â€¦</ci><apply id="S3.SS2.p4.3.m3.9.9.3.3.3.cmml" xref="S3.SS2.p4.3.m3.9.9.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.9.9.3.3.3.1.cmml" xref="S3.SS2.p4.3.m3.9.9.3.3.3">subscript</csymbol><ci id="S3.SS2.p4.3.m3.9.9.3.3.3.2.cmml" xref="S3.SS2.p4.3.m3.9.9.3.3.3.2">ğ‘¤</ci><list id="S3.SS2.p4.3.m3.6.6.2.3.cmml" xref="S3.SS2.p4.3.m3.6.6.2.4"><ci id="S3.SS2.p4.3.m3.5.5.1.1.cmml" xref="S3.SS2.p4.3.m3.5.5.1.1">ğ‘–</ci><ci id="S3.SS2.p4.3.m3.6.6.2.2.cmml" xref="S3.SS2.p4.3.m3.6.6.2.2">ğ‘š</ci></list></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.9c">w_{1,1},w_{1,2},...w_{i,m}</annotation></semantics></math> are forwarded by the clients into the GA, without sharing the original patient data. Therefore, for example, in the screening center, <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="Sc_{1}" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mrow id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m4.1.1.1" xref="S3.SS2.p4.4.m4.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml"><mi id="S3.SS2.p4.4.m4.1.1.3.2" xref="S3.SS2.p4.4.m4.1.1.3.2.cmml">c</mi><mn id="S3.SS2.p4.4.m4.1.1.3.3" xref="S3.SS2.p4.4.m4.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><times id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1.1"></times><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">ğ‘†</ci><apply id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.3.1.cmml" xref="S3.SS2.p4.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.4.m4.1.1.3.2.cmml" xref="S3.SS2.p4.4.m4.1.1.3.2">ğ‘</ci><cn type="integer" id="S3.SS2.p4.4.m4.1.1.3.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">Sc_{1}</annotation></semantics></math>, weights of local models <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="M_{1}" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><msub id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">M</mi><mn id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">M_{1}</annotation></semantics></math>, <math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="M_{2}" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><msub id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml"><mi id="S3.SS2.p4.6.m6.1.1.2" xref="S3.SS2.p4.6.m6.1.1.2.cmml">M</mi><mn id="S3.SS2.p4.6.m6.1.1.3" xref="S3.SS2.p4.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><apply id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m6.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m6.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p4.6.m6.1.1.3.cmml" xref="S3.SS2.p4.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">M_{2}</annotation></semantics></math>, and <math id="S3.SS2.p4.7.m7.1" class="ltx_Math" alttext="M_{3}" display="inline"><semantics id="S3.SS2.p4.7.m7.1a"><msub id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml"><mi id="S3.SS2.p4.7.m7.1.1.2" xref="S3.SS2.p4.7.m7.1.1.2.cmml">M</mi><mn id="S3.SS2.p4.7.m7.1.1.3" xref="S3.SS2.p4.7.m7.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m7.1.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p4.7.m7.1.1.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p4.7.m7.1.1.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">M_{3}</annotation></semantics></math> corresponding to each dataset (behavioral, facial, or merged), are shared securely with the GA. Then, <math id="S3.SS2.p4.8.m8.1" class="ltx_Math" alttext="Sc_{2}" display="inline"><semantics id="S3.SS2.p4.8.m8.1a"><mrow id="S3.SS2.p4.8.m8.1.1" xref="S3.SS2.p4.8.m8.1.1.cmml"><mi id="S3.SS2.p4.8.m8.1.1.2" xref="S3.SS2.p4.8.m8.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.8.m8.1.1.1" xref="S3.SS2.p4.8.m8.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p4.8.m8.1.1.3" xref="S3.SS2.p4.8.m8.1.1.3.cmml"><mi id="S3.SS2.p4.8.m8.1.1.3.2" xref="S3.SS2.p4.8.m8.1.1.3.2.cmml">c</mi><mn id="S3.SS2.p4.8.m8.1.1.3.3" xref="S3.SS2.p4.8.m8.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m8.1b"><apply id="S3.SS2.p4.8.m8.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1"><times id="S3.SS2.p4.8.m8.1.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1.1"></times><ci id="S3.SS2.p4.8.m8.1.1.2.cmml" xref="S3.SS2.p4.8.m8.1.1.2">ğ‘†</ci><apply id="S3.SS2.p4.8.m8.1.1.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m8.1.1.3.1.cmml" xref="S3.SS2.p4.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.8.m8.1.1.3.2.cmml" xref="S3.SS2.p4.8.m8.1.1.3.2">ğ‘</ci><cn type="integer" id="S3.SS2.p4.8.m8.1.1.3.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m8.1c">Sc_{2}</annotation></semantics></math> also shares the weights of its local models <math id="S3.SS2.p4.9.m9.1" class="ltx_Math" alttext="M_{1}" display="inline"><semantics id="S3.SS2.p4.9.m9.1a"><msub id="S3.SS2.p4.9.m9.1.1" xref="S3.SS2.p4.9.m9.1.1.cmml"><mi id="S3.SS2.p4.9.m9.1.1.2" xref="S3.SS2.p4.9.m9.1.1.2.cmml">M</mi><mn id="S3.SS2.p4.9.m9.1.1.3" xref="S3.SS2.p4.9.m9.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m9.1b"><apply id="S3.SS2.p4.9.m9.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m9.1.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p4.9.m9.1.1.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p4.9.m9.1.1.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m9.1c">M_{1}</annotation></semantics></math>, <math id="S3.SS2.p4.10.m10.1" class="ltx_Math" alttext="M_{2}" display="inline"><semantics id="S3.SS2.p4.10.m10.1a"><msub id="S3.SS2.p4.10.m10.1.1" xref="S3.SS2.p4.10.m10.1.1.cmml"><mi id="S3.SS2.p4.10.m10.1.1.2" xref="S3.SS2.p4.10.m10.1.1.2.cmml">M</mi><mn id="S3.SS2.p4.10.m10.1.1.3" xref="S3.SS2.p4.10.m10.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.10.m10.1b"><apply id="S3.SS2.p4.10.m10.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.10.m10.1.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p4.10.m10.1.1.2.cmml" xref="S3.SS2.p4.10.m10.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p4.10.m10.1.1.3.cmml" xref="S3.SS2.p4.10.m10.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.10.m10.1c">M_{2}</annotation></semantics></math>, and <math id="S3.SS2.p4.11.m11.1" class="ltx_Math" alttext="M_{3}" display="inline"><semantics id="S3.SS2.p4.11.m11.1a"><msub id="S3.SS2.p4.11.m11.1.1" xref="S3.SS2.p4.11.m11.1.1.cmml"><mi id="S3.SS2.p4.11.m11.1.1.2" xref="S3.SS2.p4.11.m11.1.1.2.cmml">M</mi><mn id="S3.SS2.p4.11.m11.1.1.3" xref="S3.SS2.p4.11.m11.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.11.m11.1b"><apply id="S3.SS2.p4.11.m11.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.11.m11.1.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p4.11.m11.1.1.2.cmml" xref="S3.SS2.p4.11.m11.1.1.2">ğ‘€</ci><cn type="integer" id="S3.SS2.p4.11.m11.1.1.3.cmml" xref="S3.SS2.p4.11.m11.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.11.m11.1c">M_{3}</annotation></semantics></math> with the GA, and similarly for the rest of the screening centers.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Finally, the GA averages the local weights for each local model separately, producing 3 global models <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="GM" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><times id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"></times><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">ğº</ci><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">ğ‘€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">GM</annotation></semantics></math>s: the first one for predicting ASD based on behavioral traits, the second based on facial traits, and the third based on a combination of behavioral and facial features.
In order to aggregate modelsâ€™ weights, averaging is done as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="GM=\sum_{i=1}^{i=m}h_{i}.X_{i}" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.3.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.2.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">M</mi></mrow><mo rspace="0.111em" id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><munderover id="S3.E2.m1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.1.1.1.1.3.1.2.2" xref="S3.E2.m1.1.1.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.1.1.1.1.3.1.2.3" xref="S3.E2.m1.1.1.1.1.3.1.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.1.2.3.2" xref="S3.E2.m1.1.1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.3.1.2.3.1" xref="S3.E2.m1.1.1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.3.1.2.3.3" xref="S3.E2.m1.1.1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.E2.m1.1.1.1.1.3.1.3" xref="S3.E2.m1.1.1.1.1.3.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.1.3.2" xref="S3.E2.m1.1.1.1.1.3.1.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.3.1.3.1" xref="S3.E2.m1.1.1.1.1.3.1.3.1.cmml">=</mo><mi id="S3.E2.m1.1.1.1.1.3.1.3.3" xref="S3.E2.m1.1.1.1.1.3.1.3.3.cmml">m</mi></mrow></munderover><msub id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.cmml">h</mi><mi id="S3.E2.m1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.3a.cmml">.</mo><msub id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.cmml">X</mi><mi id="S3.E2.m1.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.3a.cmml" xref="S3.E2.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"></eq><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1"></times><ci id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2">ğº</ci><ci id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3">ğ‘€</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><apply id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.3.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.3.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.3.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.2.3"><eq id="S3.E2.m1.1.1.1.1.3.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.3.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.3.1.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.3"><eq id="S3.E2.m1.1.1.1.1.3.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.3.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.3.2">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.3.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.3.3">ğ‘š</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2">â„</ci><ci id="S3.E2.m1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3">ğ‘–</ci></apply></apply></apply><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2">ğ‘‹</ci><ci id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">GM=\sum_{i=1}^{i=m}h_{i}.X_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.3" class="ltx_p">with <math id="S3.SS2.p5.2.m1.1" class="ltx_Math" alttext="h_{i}" display="inline"><semantics id="S3.SS2.p5.2.m1.1a"><msub id="S3.SS2.p5.2.m1.1.1" xref="S3.SS2.p5.2.m1.1.1.cmml"><mi id="S3.SS2.p5.2.m1.1.1.2" xref="S3.SS2.p5.2.m1.1.1.2.cmml">h</mi><mi id="S3.SS2.p5.2.m1.1.1.3" xref="S3.SS2.p5.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m1.1b"><apply id="S3.SS2.p5.2.m1.1.1.cmml" xref="S3.SS2.p5.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m1.1.1.1.cmml" xref="S3.SS2.p5.2.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m1.1.1.2.cmml" xref="S3.SS2.p5.2.m1.1.1.2">â„</ci><ci id="S3.SS2.p5.2.m1.1.1.3.cmml" xref="S3.SS2.p5.2.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m1.1c">h_{i}</annotation></semantics></math>, represented in <a href="#S3.E3" title="In III-B Scheme Overview â€£ III Proposed Scheme â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, being the averaged weight for each feature variable <math id="S3.SS2.p5.3.m2.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS2.p5.3.m2.1a"><msub id="S3.SS2.p5.3.m2.1.1" xref="S3.SS2.p5.3.m2.1.1.cmml"><mi id="S3.SS2.p5.3.m2.1.1.2" xref="S3.SS2.p5.3.m2.1.1.2.cmml">X</mi><mi id="S3.SS2.p5.3.m2.1.1.3" xref="S3.SS2.p5.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m2.1b"><apply id="S3.SS2.p5.3.m2.1.1.cmml" xref="S3.SS2.p5.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m2.1.1.1.cmml" xref="S3.SS2.p5.3.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m2.1.1.2.cmml" xref="S3.SS2.p5.3.m2.1.1.2">ğ‘‹</ci><ci id="S3.SS2.p5.3.m2.1.1.3.cmml" xref="S3.SS2.p5.3.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m2.1c">X_{i}</annotation></semantics></math>:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="h_{i}=\frac{\sum_{i=1}^{m}w_{i}}{n}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">h</mi><mi id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mfrac id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><msubsup id="S3.E3.m1.1.1.3.2.1" xref="S3.E3.m1.1.1.3.2.1.cmml"><mo id="S3.E3.m1.1.1.3.2.1.2.2" xref="S3.E3.m1.1.1.3.2.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.1.1.3.2.1.2.3" xref="S3.E3.m1.1.1.3.2.1.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.1.2.3.2" xref="S3.E3.m1.1.1.3.2.1.2.3.2.cmml">i</mi><mo id="S3.E3.m1.1.1.3.2.1.2.3.1" xref="S3.E3.m1.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.3.2.1.2.3.3" xref="S3.E3.m1.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.1.1.3.2.1.3" xref="S3.E3.m1.1.1.3.2.1.3.cmml">m</mi></msubsup><msub id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.3.2.2.2.cmml">w</mi><mi id="S3.E3.m1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.3.2.2.3.cmml">i</mi></msub></mrow><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">n</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">â„</ci><ci id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><divide id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3"></divide><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><apply id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.1.cmml" xref="S3.E3.m1.1.1.3.2.1">superscript</csymbol><apply id="S3.E3.m1.1.1.3.2.1.2.cmml" xref="S3.E3.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.2.1.cmml" xref="S3.E3.m1.1.1.3.2.1">subscript</csymbol><sum id="S3.E3.m1.1.1.3.2.1.2.2.cmml" xref="S3.E3.m1.1.1.3.2.1.2.2"></sum><apply id="S3.E3.m1.1.1.3.2.1.2.3.cmml" xref="S3.E3.m1.1.1.3.2.1.2.3"><eq id="S3.E3.m1.1.1.3.2.1.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.1.2.3.1"></eq><ci id="S3.E3.m1.1.1.3.2.1.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E3.m1.1.1.3.2.1.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.1.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.1.1.3.2.1.3.cmml" xref="S3.E3.m1.1.1.3.2.1.3">ğ‘š</ci></apply><apply id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2.2">ğ‘¤</ci><ci id="S3.E3.m1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3">ğ‘–</ci></apply></apply><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">h_{i}=\frac{\sum_{i=1}^{m}w_{i}}{n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.4" class="ltx_p">where n is the total number of clients.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic"> </span><span id="S3.SS3.7.3" class="ltx_text ltx_font_bold">Model Evaluation</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to evaluate each of the ML models used in our scheme, we measure each clientâ€™s accuracy value. This metric represents how far the predicted answer is from the actual classification, which is later aggregated into a global accuracy demonstrating the overall modelâ€™s performance. In the federated learning scheme, at each local training phase, data is split into 80% training data and the remaining 20% is kept for testing. Thus, once training is done, we measure the accuracy value by testing how many correct predictions are made in comparison with the real testing data having known labels or classes.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">A Binary Cross-Entropy loss is used for calculating this accuracy value at each client locally. Cross-entropy loss measures the performance of a classification model having an output with a probability value between 0 and 1. Its value increases while the predicted probability diverges from the actual label. As our classification is binary; we only have two classes, ASD and non-ASD. We choose this loss function as a suitable measure. Hence, the higher the number of correct predictions, the lower the cross-entropy loss, and the higher the obtained accuracy value <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="Acc_{i}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1a" xref="S3.SS3.p2.1.m1.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p2.1.m1.1.1.4" xref="S3.SS3.p2.1.m1.1.1.4.cmml"><mi id="S3.SS3.p2.1.m1.1.1.4.2" xref="S3.SS3.p2.1.m1.1.1.4.2.cmml">c</mi><mi id="S3.SS3.p2.1.m1.1.1.4.3" xref="S3.SS3.p2.1.m1.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ´</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">ğ‘</ci><apply id="S3.SS3.p2.1.m1.1.1.4.cmml" xref="S3.SS3.p2.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.4.1.cmml" xref="S3.SS3.p2.1.m1.1.1.4">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.4.2.cmml" xref="S3.SS3.p2.1.m1.1.1.4.2">ğ‘</ci><ci id="S3.SS3.p2.1.m1.1.1.4.3.cmml" xref="S3.SS3.p2.1.m1.1.1.4.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">Acc_{i}</annotation></semantics></math> is.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.2" class="ltx_p">The global accuracy <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="GAcc_{i}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.1a" xref="S3.SS3.p3.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.1.m1.1.1.4" xref="S3.SS3.p3.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.1b" xref="S3.SS3.p3.1.m1.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p3.1.m1.1.1.5" xref="S3.SS3.p3.1.m1.1.1.5.cmml"><mi id="S3.SS3.p3.1.m1.1.1.5.2" xref="S3.SS3.p3.1.m1.1.1.5.2.cmml">c</mi><mi id="S3.SS3.p3.1.m1.1.1.5.3" xref="S3.SS3.p3.1.m1.1.1.5.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ğº</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">ğ´</ci><ci id="S3.SS3.p3.1.m1.1.1.4.cmml" xref="S3.SS3.p3.1.m1.1.1.4">ğ‘</ci><apply id="S3.SS3.p3.1.m1.1.1.5.cmml" xref="S3.SS3.p3.1.m1.1.1.5"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.5.1.cmml" xref="S3.SS3.p3.1.m1.1.1.5">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.5.2.cmml" xref="S3.SS3.p3.1.m1.1.1.5.2">ğ‘</ci><ci id="S3.SS3.p3.1.m1.1.1.5.3.cmml" xref="S3.SS3.p3.1.m1.1.1.5.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">GAcc_{i}</annotation></semantics></math> is calculated by averaging the local accuracy values <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="Acc_{i}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.1a" xref="S3.SS3.p3.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p3.2.m2.1.1.4" xref="S3.SS3.p3.2.m2.1.1.4.cmml"><mi id="S3.SS3.p3.2.m2.1.1.4.2" xref="S3.SS3.p3.2.m2.1.1.4.2.cmml">c</mi><mi id="S3.SS3.p3.2.m2.1.1.4.3" xref="S3.SS3.p3.2.m2.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><times id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1"></times><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">ğ´</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">ğ‘</ci><apply id="S3.SS3.p3.2.m2.1.1.4.cmml" xref="S3.SS3.p3.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.4.1.cmml" xref="S3.SS3.p3.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.4.2.cmml" xref="S3.SS3.p3.2.m2.1.1.4.2">ğ‘</ci><ci id="S3.SS3.p3.2.m2.1.1.4.3.cmml" xref="S3.SS3.p3.2.m2.1.1.4.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">Acc_{i}</annotation></semantics></math> of all local models trained by clients throughout an experiment, similar to the averaging method of local model weights previously presented:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="GAcc_{i}=\frac{\sum_{i=1}^{i=n}Acc_{i}.Sd_{i}}{\sum_{i=1}^{i=n}Sd_{i}}" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.3" xref="S3.E4.m1.2.3.cmml"><mrow id="S3.E4.m1.2.3.2" xref="S3.E4.m1.2.3.2.cmml"><mi id="S3.E4.m1.2.3.2.2" xref="S3.E4.m1.2.3.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.1" xref="S3.E4.m1.2.3.2.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.2.3" xref="S3.E4.m1.2.3.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.1a" xref="S3.E4.m1.2.3.2.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.2.4" xref="S3.E4.m1.2.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.1b" xref="S3.E4.m1.2.3.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.2.3.2.5" xref="S3.E4.m1.2.3.2.5.cmml"><mi id="S3.E4.m1.2.3.2.5.2" xref="S3.E4.m1.2.3.2.5.2.cmml">c</mi><mi id="S3.E4.m1.2.3.2.5.3" xref="S3.E4.m1.2.3.2.5.3.cmml">i</mi></msub></mrow><mo id="S3.E4.m1.2.3.1" xref="S3.E4.m1.2.3.1.cmml">=</mo><mfrac id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><mrow id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.3.cmml"><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><msubsup id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.3.1.cmml">=</mo><mi id="S3.E4.m1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.cmml">n</mi></mrow></msubsup><mrow id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">â€‹</mo><mi id="S3.E4.m1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.2.1a" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.1.1.1.1.1.2.4" xref="S3.E4.m1.1.1.1.1.1.2.4.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2.4.2" xref="S3.E4.m1.1.1.1.1.1.2.4.2.cmml">c</mi><mi id="S3.E4.m1.1.1.1.1.1.2.4.3" xref="S3.E4.m1.1.1.1.1.1.2.4.3.cmml">i</mi></msub></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S3.E4.m1.2.2.2.2.3" xref="S3.E4.m1.2.2.2.3a.cmml">.</mo><mrow id="S3.E4.m1.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.2.3.cmml"><mi id="S3.E4.m1.2.2.2.2.2.3.2" xref="S3.E4.m1.2.2.2.2.2.3.2.cmml">d</mi><mi id="S3.E4.m1.2.2.2.2.2.3.3" xref="S3.E4.m1.2.2.2.2.2.3.3.cmml">i</mi></msub></mrow></mrow><mrow id="S3.E4.m1.2.2.4" xref="S3.E4.m1.2.2.4.cmml"><msubsup id="S3.E4.m1.2.2.4.1" xref="S3.E4.m1.2.2.4.1.cmml"><mo id="S3.E4.m1.2.2.4.1.2.2" xref="S3.E4.m1.2.2.4.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.2.2.4.1.2.3" xref="S3.E4.m1.2.2.4.1.2.3.cmml"><mi id="S3.E4.m1.2.2.4.1.2.3.2" xref="S3.E4.m1.2.2.4.1.2.3.2.cmml">i</mi><mo id="S3.E4.m1.2.2.4.1.2.3.1" xref="S3.E4.m1.2.2.4.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.2.2.4.1.2.3.3" xref="S3.E4.m1.2.2.4.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.E4.m1.2.2.4.1.3" xref="S3.E4.m1.2.2.4.1.3.cmml"><mi id="S3.E4.m1.2.2.4.1.3.2" xref="S3.E4.m1.2.2.4.1.3.2.cmml">i</mi><mo id="S3.E4.m1.2.2.4.1.3.1" xref="S3.E4.m1.2.2.4.1.3.1.cmml">=</mo><mi id="S3.E4.m1.2.2.4.1.3.3" xref="S3.E4.m1.2.2.4.1.3.3.cmml">n</mi></mrow></msubsup><mrow id="S3.E4.m1.2.2.4.2" xref="S3.E4.m1.2.2.4.2.cmml"><mi id="S3.E4.m1.2.2.4.2.2" xref="S3.E4.m1.2.2.4.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.4.2.1" xref="S3.E4.m1.2.2.4.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.2.2.4.2.3" xref="S3.E4.m1.2.2.4.2.3.cmml"><mi id="S3.E4.m1.2.2.4.2.3.2" xref="S3.E4.m1.2.2.4.2.3.2.cmml">d</mi><mi id="S3.E4.m1.2.2.4.2.3.3" xref="S3.E4.m1.2.2.4.2.3.3.cmml">i</mi></msub></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.3.cmml" xref="S3.E4.m1.2.3"><eq id="S3.E4.m1.2.3.1.cmml" xref="S3.E4.m1.2.3.1"></eq><apply id="S3.E4.m1.2.3.2.cmml" xref="S3.E4.m1.2.3.2"><times id="S3.E4.m1.2.3.2.1.cmml" xref="S3.E4.m1.2.3.2.1"></times><ci id="S3.E4.m1.2.3.2.2.cmml" xref="S3.E4.m1.2.3.2.2">ğº</ci><ci id="S3.E4.m1.2.3.2.3.cmml" xref="S3.E4.m1.2.3.2.3">ğ´</ci><ci id="S3.E4.m1.2.3.2.4.cmml" xref="S3.E4.m1.2.3.2.4">ğ‘</ci><apply id="S3.E4.m1.2.3.2.5.cmml" xref="S3.E4.m1.2.3.2.5"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.2.5.1.cmml" xref="S3.E4.m1.2.3.2.5">subscript</csymbol><ci id="S3.E4.m1.2.3.2.5.2.cmml" xref="S3.E4.m1.2.3.2.5.2">ğ‘</ci><ci id="S3.E4.m1.2.3.2.5.3.cmml" xref="S3.E4.m1.2.3.2.5.3">ğ‘–</ci></apply></apply><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><divide id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2"></divide><apply id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">subscript</csymbol><sum id="S3.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2"></sum><apply id="S3.E4.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3"><eq id="S3.E4.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.1"></eq><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3"><eq id="S3.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.1"></eq><ci id="S3.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2">ğ‘–</ci><ci id="S3.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3">ğ‘›</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2"><times id="S3.E4.m1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1"></times><ci id="S3.E4.m1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.2">ğ´</ci><ci id="S3.E4.m1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.2.3">ğ‘</ci><apply id="S3.E4.m1.1.1.1.1.1.2.4.cmml" xref="S3.E4.m1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.2.4.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.2.4.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.4.2">ğ‘</ci><ci id="S3.E4.m1.1.1.1.1.1.2.4.3.cmml" xref="S3.E4.m1.1.1.1.1.1.2.4.3">ğ‘–</ci></apply></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2"><times id="S3.E4.m1.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1"></times><ci id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2">ğ‘†</ci><apply id="S3.E4.m1.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.3.2">ğ‘‘</ci><ci id="S3.E4.m1.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.3.3">ğ‘–</ci></apply></apply></apply><apply id="S3.E4.m1.2.2.4.cmml" xref="S3.E4.m1.2.2.4"><apply id="S3.E4.m1.2.2.4.1.cmml" xref="S3.E4.m1.2.2.4.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.4.1.1.cmml" xref="S3.E4.m1.2.2.4.1">superscript</csymbol><apply id="S3.E4.m1.2.2.4.1.2.cmml" xref="S3.E4.m1.2.2.4.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.4.1.2.1.cmml" xref="S3.E4.m1.2.2.4.1">subscript</csymbol><sum id="S3.E4.m1.2.2.4.1.2.2.cmml" xref="S3.E4.m1.2.2.4.1.2.2"></sum><apply id="S3.E4.m1.2.2.4.1.2.3.cmml" xref="S3.E4.m1.2.2.4.1.2.3"><eq id="S3.E4.m1.2.2.4.1.2.3.1.cmml" xref="S3.E4.m1.2.2.4.1.2.3.1"></eq><ci id="S3.E4.m1.2.2.4.1.2.3.2.cmml" xref="S3.E4.m1.2.2.4.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E4.m1.2.2.4.1.2.3.3.cmml" xref="S3.E4.m1.2.2.4.1.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.2.2.4.1.3.cmml" xref="S3.E4.m1.2.2.4.1.3"><eq id="S3.E4.m1.2.2.4.1.3.1.cmml" xref="S3.E4.m1.2.2.4.1.3.1"></eq><ci id="S3.E4.m1.2.2.4.1.3.2.cmml" xref="S3.E4.m1.2.2.4.1.3.2">ğ‘–</ci><ci id="S3.E4.m1.2.2.4.1.3.3.cmml" xref="S3.E4.m1.2.2.4.1.3.3">ğ‘›</ci></apply></apply><apply id="S3.E4.m1.2.2.4.2.cmml" xref="S3.E4.m1.2.2.4.2"><times id="S3.E4.m1.2.2.4.2.1.cmml" xref="S3.E4.m1.2.2.4.2.1"></times><ci id="S3.E4.m1.2.2.4.2.2.cmml" xref="S3.E4.m1.2.2.4.2.2">ğ‘†</ci><apply id="S3.E4.m1.2.2.4.2.3.cmml" xref="S3.E4.m1.2.2.4.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.4.2.3.1.cmml" xref="S3.E4.m1.2.2.4.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.4.2.3.2.cmml" xref="S3.E4.m1.2.2.4.2.3.2">ğ‘‘</ci><ci id="S3.E4.m1.2.2.4.2.3.3.cmml" xref="S3.E4.m1.2.2.4.2.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">GAcc_{i}=\frac{\sum_{i=1}^{i=n}Acc_{i}.Sd_{i}}{\sum_{i=1}^{i=n}Sd_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.4" class="ltx_p">with n being the total number of clients, <math id="S3.SS3.p3.3.m1.1" class="ltx_Math" alttext="Sd_{i}" display="inline"><semantics id="S3.SS3.p3.3.m1.1a"><mrow id="S3.SS3.p3.3.m1.1.1" xref="S3.SS3.p3.3.m1.1.1.cmml"><mi id="S3.SS3.p3.3.m1.1.1.2" xref="S3.SS3.p3.3.m1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m1.1.1.1" xref="S3.SS3.p3.3.m1.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p3.3.m1.1.1.3" xref="S3.SS3.p3.3.m1.1.1.3.cmml"><mi id="S3.SS3.p3.3.m1.1.1.3.2" xref="S3.SS3.p3.3.m1.1.1.3.2.cmml">d</mi><mi id="S3.SS3.p3.3.m1.1.1.3.3" xref="S3.SS3.p3.3.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m1.1b"><apply id="S3.SS3.p3.3.m1.1.1.cmml" xref="S3.SS3.p3.3.m1.1.1"><times id="S3.SS3.p3.3.m1.1.1.1.cmml" xref="S3.SS3.p3.3.m1.1.1.1"></times><ci id="S3.SS3.p3.3.m1.1.1.2.cmml" xref="S3.SS3.p3.3.m1.1.1.2">ğ‘†</ci><apply id="S3.SS3.p3.3.m1.1.1.3.cmml" xref="S3.SS3.p3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m1.1.1.3.1.cmml" xref="S3.SS3.p3.3.m1.1.1.3">subscript</csymbol><ci id="S3.SS3.p3.3.m1.1.1.3.2.cmml" xref="S3.SS3.p3.3.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.SS3.p3.3.m1.1.1.3.3.cmml" xref="S3.SS3.p3.3.m1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m1.1c">Sd_{i}</annotation></semantics></math> being the sample data size at client i, and <math id="S3.SS3.p3.4.m2.1" class="ltx_Math" alttext="Acc_{i}" display="inline"><semantics id="S3.SS3.p3.4.m2.1a"><mrow id="S3.SS3.p3.4.m2.1.1" xref="S3.SS3.p3.4.m2.1.1.cmml"><mi id="S3.SS3.p3.4.m2.1.1.2" xref="S3.SS3.p3.4.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.4.m2.1.1.1" xref="S3.SS3.p3.4.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.4.m2.1.1.3" xref="S3.SS3.p3.4.m2.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.4.m2.1.1.1a" xref="S3.SS3.p3.4.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p3.4.m2.1.1.4" xref="S3.SS3.p3.4.m2.1.1.4.cmml"><mi id="S3.SS3.p3.4.m2.1.1.4.2" xref="S3.SS3.p3.4.m2.1.1.4.2.cmml">c</mi><mi id="S3.SS3.p3.4.m2.1.1.4.3" xref="S3.SS3.p3.4.m2.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m2.1b"><apply id="S3.SS3.p3.4.m2.1.1.cmml" xref="S3.SS3.p3.4.m2.1.1"><times id="S3.SS3.p3.4.m2.1.1.1.cmml" xref="S3.SS3.p3.4.m2.1.1.1"></times><ci id="S3.SS3.p3.4.m2.1.1.2.cmml" xref="S3.SS3.p3.4.m2.1.1.2">ğ´</ci><ci id="S3.SS3.p3.4.m2.1.1.3.cmml" xref="S3.SS3.p3.4.m2.1.1.3">ğ‘</ci><apply id="S3.SS3.p3.4.m2.1.1.4.cmml" xref="S3.SS3.p3.4.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m2.1.1.4.1.cmml" xref="S3.SS3.p3.4.m2.1.1.4">subscript</csymbol><ci id="S3.SS3.p3.4.m2.1.1.4.2.cmml" xref="S3.SS3.p3.4.m2.1.1.4.2">ğ‘</ci><ci id="S3.SS3.p3.4.m2.1.1.4.3.cmml" xref="S3.SS3.p3.4.m2.1.1.4.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m2.1c">Acc_{i}</annotation></semantics></math> being the local accuracy produced by each local model at a client i.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_bold">Multi-Aspect ASD Prediction Approach</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we tackle two significant aspects of ASD: a behavioral aspect, affecting the social behavior of an autistic patient within their community, and a facial aspect affecting the physical features of a patientâ€™s face. We have discussed in previous sections how each of these aspects of ASD is used by machine learners. However, in this section, we discuss the process of merging both datasets, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> behavioral and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> facial image, motivated by testing the behavior of machine learning for the prediction of ASD depending on the combination of multiple human aspects (e.g. behavioral and facial). Then, given a combination of behavioral as well as facial features of a certain individual, our federated machine learning model would predict whether this individual has ASD or not, rather than using each of these aspects separately.</p>
</div>
<div id="S4.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Data Generation</span> 
<br class="ltx_break">Two datasets are used in this study: a behavioral dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> consisting of 610 instances, and a facial image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> containing around 2900 images. Given that these two datasets do not have equal sizes, merging them is challenging. To tackle this issue, we built a random instance generator for behavioral data, producing instances with values in the ranges of those of real data. Then, we end up with around 2900 behavioral data instances, equally split among ASD and non-ASD classes, which is equal to the number of instances in the image dataset. Therefore, as the two datasets now have equal size, one behavioral data instance could be mapped to a facial image from the second dataset depending on their class.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Facial Feature Extraction</span> 
<br class="ltx_break">Another problem faced in this experiment would be merging datasets of different types: CSV dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. As we previously mentioned, medical research studies in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, have proved that ASD affects the facial characteristics of its patients, differentiating them from non-autistic individuals. To prove that, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> measure facial distances after examining the facial landmarks of the patientâ€™s face in comparison to healthy patients. Hence, we rely on this work in order to merge both datasets by extracting certain features from each of the images in the dataset and mapping these facial features into one behavioral instance rather than mapping the whole image.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2211.00643/assets/images/featureextraction.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="210" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Facial Feature Extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:86.7pt;"><img src="/html/2211.00643/assets/images/download1.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_square" width="598" height="708" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Before Facial Landmark Extraction</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:86.7pt;"><img src="/html/2211.00643/assets/images/download2.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_square" width="598" height="708" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>After Facial Landmark Extraction</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S4.I1.i2.p2" class="ltx_para">
<p id="S4.I1.i2.p2.4" class="ltx_p">For the sake of extracting features from the images of ASD and non-ASD individuals in the dataset used <a href="#S5.I1.i2" title="item 2 â€£ V-A Datasets â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, or in other words the facial landmarks, such as the eyes, nose, mouth, and brows, we use Pythonâ€™s open source library built for face recognition, Dlib, <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>http://dlib.net/</span></span></span>, proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. This library works by detecting a face within the input image. Then, within each face detected, it recognizes the facial landmarks and specifies their coordinates depending on a pre-defined facial template represented in Figure <a href="#S4.F2" title="Figure 2 â€£ item 2 â€£ IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Thus, each point in the face would belong to a part of the face and have a unique number between 0 and 67, as well as coordinates (x, y). In Figure <a href="#S4.F2" title="Figure 2 â€£ item 2 â€£ IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the facial template, points 0 through 16 represent the jaw points, the right and left brows are represented by points 17â€“21 and 22â€“26 respectively, the points 27â€“35 constitute the nose, 36â€“41 and 42â€“47 belong to the right and left eyes, 48â€“60 belong to the mouth points, and the points range 61â€“67 represent the lips. Therefore, to extract facial distances, representing facial features for each facial image in hand, we use Python Dlib to develop a program that takes as input the facial image dataset <a href="#S5.I1.i2" title="item 2 â€£ V-A Datasets â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Then, for each image of an individualâ€™s face in the dataset, such as the ones in Figure <a href="#S4.F4" title="Figure 4 â€£ item 2 â€£ IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the face is recognized, and then facial landmarks, including the brows, eyes, mouth, and nose, are distinguished through the plotted dots in Figure <a href="#S4.F4" title="Figure 4 â€£ item 2 â€£ IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Later, each plotted dot in the figure is characterized by a number in the range of 0-67, an abscissa â€x,â€ and ordinate â€yâ€ in the 2D plane. For the sake of calculating facial distances from the recognized dots of the face, we take the midpoints of each facial entity depending on the pre-defined Dlib template shown in Figure <a href="#S4.F2" title="Figure 2 â€£ item 2 â€£ IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and compute the euclidean distance between them. To clarify, we take the midpoint of the left brow and that of the right brow, which are points A(19) having coordinates (<math id="S4.I1.i2.p2.1.m1.1" class="ltx_Math" alttext="x_{1}" display="inline"><semantics id="S4.I1.i2.p2.1.m1.1a"><msub id="S4.I1.i2.p2.1.m1.1.1" xref="S4.I1.i2.p2.1.m1.1.1.cmml"><mi id="S4.I1.i2.p2.1.m1.1.1.2" xref="S4.I1.i2.p2.1.m1.1.1.2.cmml">x</mi><mn id="S4.I1.i2.p2.1.m1.1.1.3" xref="S4.I1.i2.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p2.1.m1.1b"><apply id="S4.I1.i2.p2.1.m1.1.1.cmml" xref="S4.I1.i2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p2.1.m1.1.1.1.cmml" xref="S4.I1.i2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.I1.i2.p2.1.m1.1.1.2.cmml" xref="S4.I1.i2.p2.1.m1.1.1.2">ğ‘¥</ci><cn type="integer" id="S4.I1.i2.p2.1.m1.1.1.3.cmml" xref="S4.I1.i2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p2.1.m1.1c">x_{1}</annotation></semantics></math>, <math id="S4.I1.i2.p2.2.m2.1" class="ltx_Math" alttext="y_{1}" display="inline"><semantics id="S4.I1.i2.p2.2.m2.1a"><msub id="S4.I1.i2.p2.2.m2.1.1" xref="S4.I1.i2.p2.2.m2.1.1.cmml"><mi id="S4.I1.i2.p2.2.m2.1.1.2" xref="S4.I1.i2.p2.2.m2.1.1.2.cmml">y</mi><mn id="S4.I1.i2.p2.2.m2.1.1.3" xref="S4.I1.i2.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p2.2.m2.1b"><apply id="S4.I1.i2.p2.2.m2.1.1.cmml" xref="S4.I1.i2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p2.2.m2.1.1.1.cmml" xref="S4.I1.i2.p2.2.m2.1.1">subscript</csymbol><ci id="S4.I1.i2.p2.2.m2.1.1.2.cmml" xref="S4.I1.i2.p2.2.m2.1.1.2">ğ‘¦</ci><cn type="integer" id="S4.I1.i2.p2.2.m2.1.1.3.cmml" xref="S4.I1.i2.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p2.2.m2.1c">y_{1}</annotation></semantics></math>) and B(24) having coordinates (<math id="S4.I1.i2.p2.3.m3.1" class="ltx_Math" alttext="x_{2}" display="inline"><semantics id="S4.I1.i2.p2.3.m3.1a"><msub id="S4.I1.i2.p2.3.m3.1.1" xref="S4.I1.i2.p2.3.m3.1.1.cmml"><mi id="S4.I1.i2.p2.3.m3.1.1.2" xref="S4.I1.i2.p2.3.m3.1.1.2.cmml">x</mi><mn id="S4.I1.i2.p2.3.m3.1.1.3" xref="S4.I1.i2.p2.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p2.3.m3.1b"><apply id="S4.I1.i2.p2.3.m3.1.1.cmml" xref="S4.I1.i2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p2.3.m3.1.1.1.cmml" xref="S4.I1.i2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.I1.i2.p2.3.m3.1.1.2.cmml" xref="S4.I1.i2.p2.3.m3.1.1.2">ğ‘¥</ci><cn type="integer" id="S4.I1.i2.p2.3.m3.1.1.3.cmml" xref="S4.I1.i2.p2.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p2.3.m3.1c">x_{2}</annotation></semantics></math>, <math id="S4.I1.i2.p2.4.m4.1" class="ltx_Math" alttext="y_{2}" display="inline"><semantics id="S4.I1.i2.p2.4.m4.1a"><msub id="S4.I1.i2.p2.4.m4.1.1" xref="S4.I1.i2.p2.4.m4.1.1.cmml"><mi id="S4.I1.i2.p2.4.m4.1.1.2" xref="S4.I1.i2.p2.4.m4.1.1.2.cmml">y</mi><mn id="S4.I1.i2.p2.4.m4.1.1.3" xref="S4.I1.i2.p2.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p2.4.m4.1b"><apply id="S4.I1.i2.p2.4.m4.1.1.cmml" xref="S4.I1.i2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p2.4.m4.1.1.1.cmml" xref="S4.I1.i2.p2.4.m4.1.1">subscript</csymbol><ci id="S4.I1.i2.p2.4.m4.1.1.2.cmml" xref="S4.I1.i2.p2.4.m4.1.1.2">ğ‘¦</ci><cn type="integer" id="S4.I1.i2.p2.4.m4.1.1.3.cmml" xref="S4.I1.i2.p2.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p2.4.m4.1c">y_{2}</annotation></semantics></math>) respectively. We then evaluate the euclidean distance between points A and B, depending on equation <a href="#S4.E5" title="In item 2 â€£ IV Multi-Aspect ASD Prediction Approach â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>:</p>
</div>
<div id="S4.I1.i2.p3" class="ltx_para">
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.2" class="ltx_Math" alttext="\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}" display="block"><semantics id="S4.E5.m1.2a"><msqrt id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml"><mrow id="S4.E5.m1.2.2.2" xref="S4.E5.m1.2.2.2.cmml"><msup id="S4.E5.m1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml"><msub id="S4.E5.m1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.2.2" xref="S4.E5.m1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mn id="S4.E5.m1.1.1.1.1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.1.1.1.1.2.3.cmml">2</mn></msub><mo id="S4.E5.m1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E5.m1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.3.2" xref="S4.E5.m1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mn id="S4.E5.m1.1.1.1.1.1.1.1.3.3" xref="S4.E5.m1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.E5.m1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.cmml">2</mn></msup><mo id="S4.E5.m1.2.2.2.3" xref="S4.E5.m1.2.2.2.3.cmml">+</mo><msup id="S4.E5.m1.2.2.2.2" xref="S4.E5.m1.2.2.2.2.cmml"><mrow id="S4.E5.m1.2.2.2.2.1.1" xref="S4.E5.m1.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.2.2.2.2.1.1.2" xref="S4.E5.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.2.2.2.2.1.1.1" xref="S4.E5.m1.2.2.2.2.1.1.1.cmml"><msub id="S4.E5.m1.2.2.2.2.1.1.1.2" xref="S4.E5.m1.2.2.2.2.1.1.1.2.cmml"><mi id="S4.E5.m1.2.2.2.2.1.1.1.2.2" xref="S4.E5.m1.2.2.2.2.1.1.1.2.2.cmml">y</mi><mn id="S4.E5.m1.2.2.2.2.1.1.1.2.3" xref="S4.E5.m1.2.2.2.2.1.1.1.2.3.cmml">2</mn></msub><mo id="S4.E5.m1.2.2.2.2.1.1.1.1" xref="S4.E5.m1.2.2.2.2.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E5.m1.2.2.2.2.1.1.1.3" xref="S4.E5.m1.2.2.2.2.1.1.1.3.cmml"><mi id="S4.E5.m1.2.2.2.2.1.1.1.3.2" xref="S4.E5.m1.2.2.2.2.1.1.1.3.2.cmml">y</mi><mn id="S4.E5.m1.2.2.2.2.1.1.1.3.3" xref="S4.E5.m1.2.2.2.2.1.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S4.E5.m1.2.2.2.2.1.1.3" xref="S4.E5.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mn id="S4.E5.m1.2.2.2.2.3" xref="S4.E5.m1.2.2.2.2.3.cmml">2</mn></msup></mrow></msqrt><annotation-xml encoding="MathML-Content" id="S4.E5.m1.2b"><apply id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2"><root id="S4.E5.m1.2.2a.cmml" xref="S4.E5.m1.2.2"></root><apply id="S4.E5.m1.2.2.2.cmml" xref="S4.E5.m1.2.2.2"><plus id="S4.E5.m1.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.3"></plus><apply id="S4.E5.m1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1">superscript</csymbol><apply id="S4.E5.m1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1"><minus id="S4.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E5.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.2.2">ğ‘¥</ci><cn type="integer" id="S4.E5.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.2.3">2</cn></apply><apply id="S4.E5.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.3.2">ğ‘¥</ci><cn type="integer" id="S4.E5.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><cn type="integer" id="S4.E5.m1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3">2</cn></apply><apply id="S4.E5.m1.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2">superscript</csymbol><apply id="S4.E5.m1.2.2.2.2.1.1.1.cmml" xref="S4.E5.m1.2.2.2.2.1.1"><minus id="S4.E5.m1.2.2.2.2.1.1.1.1.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.1"></minus><apply id="S4.E5.m1.2.2.2.2.1.1.1.2.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.2">subscript</csymbol><ci id="S4.E5.m1.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.2.2">ğ‘¦</ci><cn type="integer" id="S4.E5.m1.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.2.3">2</cn></apply><apply id="S4.E5.m1.2.2.2.2.1.1.1.3.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E5.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.3.2">ğ‘¦</ci><cn type="integer" id="S4.E5.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S4.E5.m1.2.2.2.2.1.1.1.3.3">1</cn></apply></apply><cn type="integer" id="S4.E5.m1.2.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.2c">\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.I1.i2.p4" class="ltx_para">
<p id="S4.I1.i2.p4.1" class="ltx_p">Finally, and similarly to the euclidean distance example calculated between points A and B, the euclidean distances are calculated between the right and left brows, the right and left eyes, and the nose and lips. Then, three facial distances are obtained for each of the patientsâ€™ faces in the facial image dataset in hand. This step enhances the ability to merge the image dataset by adding the three distances, as features, to the behavioral CSV data.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Combined Data</span> 
<br class="ltx_break">After running our program utilizing Python Dlib on the facial image dataset <a href="#S5.I1.i2" title="item 2 â€£ V-A Datasets â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, three facial distances are evaluated for each facial image in the dataset. Therefore, after evaluation, these distances are added as features on top of the behavioral data features constituting the initial behavioral dataset. Then, our merged data ended up containing 22 features, 19 for the behavioral data and the remaining 3 for the facial distances. For the sake of testing in this experiment, data instances (rows) are merged randomly. To the best of our knowledge, no dataset combining both behavioral and facial features of ASD and non-ASD individuals has been found in the literature. For example, the first behavioral instance belonging to an ASD patient is mapped to the first ASD facial distance instance. This process is repeated till all of the instances are in hand. Due to the blurriness of some facial images, the Dlib program was unable to recognize some facial landmarks, which is why we ended up with 2677 data instances rather than 2940, the original dataset size.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_bold">Experiments and Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we first introduce the datasets used for training our federated learning models. Second, we discuss each of the latter and give an overview of the federated learning framework used for accomplishing our experiments. Third, we illustrate the experimental results obtained.</p>
</div>
<section id="S5.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_bold">Datasets</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Two datasets were used in our experiments as our work is based on two aspects of Autism Spectrum Disorder (ASD). Each of these datasets is described in the sequel.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Autism Screening Behavioral Data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></span></p>
</div>
<div id="S5.I1.i1.p2" class="ltx_para">
<ul id="S5.I1.i1.I1" class="ltx_itemize">
<li id="S5.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Data Description</span></p>
</div>
<div id="S5.I1.i1.I1.i1.p2" class="ltx_para">
<p id="S5.I1.i1.I1.i1.p2.1" class="ltx_p">This dataset is a collection of questionnaire-response instances tackling the behavioral traits and aspects of ASD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. It is collected from responses to a survey on ASD. Patients or their parents answered certain behavior-related questions, in addition to some personal details, including age, gender, country, and ethnicity. These responses are recorded in a dataset of 21 columns, representing the features, and 705 rows, corresponding to the responses.</p>
</div>
</li>
<li id="S5.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Data Pre-Processing</span></p>
</div>
<div id="S5.I1.i1.I1.i2.p2" class="ltx_para">
<p id="S5.I1.i1.I1.i2.p2.1" class="ltx_p">In order to avoid bias in the training stage, the dataset is cleaned and preprocessed. First, rows containing any missing answers are dropped, through which we end up with 487 data instances. Then, all text answers are transformed into numbered categories so that each class of answers would correspond to a number rather than words, given that all of the features at hand are categorical rather than continuous values. For instance, in the â€Genderâ€ feature, a â€Maleâ€ answer is converted to 0 and a â€Femaleâ€ answer is converted to 1, and so on. This method has been proven to enhance the learning ability of the ML model. Moreover, the feature â€ASDâ€, indicating whether the responder is autistic or not, or the class feature, is also categorical, so a â€Yesâ€ answer is represented by 0 and â€Noâ€ by 1. It is worth mentioning that data is evenly split into both classes, to preclude any bias during training.
Finally, we save the pre-processed CSV data of responses into a python pickle file for faster and easier access later on in the training process in the FedML framework.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Autism Facial Image Data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span></p>
</div>
<div id="S5.I1.i2.p2" class="ltx_para">
<ul id="S5.I1.i2.I1" class="ltx_itemize">
<li id="S5.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i2.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Data Description</span></p>
</div>
<div id="S5.I1.i2.I1.i1.p2" class="ltx_para">
<p id="S5.I1.i2.I1.i1.p2.1" class="ltx_p">The second dataset used in this paper is facial images of autistic and non-autistic patients. It consists of 1470 images of ASD patients and another 1470 images of non-ASD individuals. Hence, the data is already unbiased and split in half between the two classes. All of the images are close to the faces of patients, showing the details of their facial features, which is what we target mainly in our study.</p>
</div>
</li>
<li id="S5.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Data Pre-Processing</span></p>
</div>
<div id="S5.I1.i2.I1.i2.p2" class="ltx_para">
<p id="S5.I1.i2.I1.i2.p2.1" class="ltx_p">To prepare the dataset for training, the data undergoes several preprocessing steps. First, we convert the images into grayscale using Python, for better learning of the facial features themselves. Plus, each of the images is then converted into a multi-dimensional array, which is fed to the federated learning model. The latter could then take as input an array corresponding to the image of a patient, and then return whether they have ASD or not based on the feature values, symmetric to their facial characteristics, within the input array. In the end, image array data is also saved into a python pickle file for the federated learning model to access and load faster and easier during the training process.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_bold">Setup</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we tackle the framework used for experimenting with ASD prediction using our federated learning scheme. We also describe the ML models used in the testing phase.</p>
<ol id="S5.I2" class="ltx_enumerate">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Federated Learning Framework</span>: 
<br class="ltx_break">To conduct our experiments, we use the LocalFed framework <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/arafeh94/localfed</span></span></span> built on top of FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for imitating the federated learning context. This is an easy-to-use and editable adaptive framework, built with all federated learning components. The major components of this system are summarized in Table <a href="#S5.T1" title="TABLE I â€£ item 1 â€£ V-B Setup â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S5.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Entity</span></span>
</span>
</th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.1.2.1.1" class="ltx_p" style="width:398.3pt;"><span id="S5.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.2.1.1.1.1" class="ltx_p" style="width:85.4pt;">Trainer Manager</span>
</span>
</td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.2.1.2.1.1" class="ltx_p" style="width:398.3pt;">An instance of this interface defines how trainers are running. Trainer Manager (TM) is followed by certain training parameters defined for each client, such as epochs and loss.</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.3.2.1.1.1" class="ltx_p" style="width:85.4pt;">Aggregator</span>
</span>
</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.3.2.2.1.1" class="ltx_p" style="width:398.3pt;">Defines how would the local models be merged into one global model, or in other words, aggregated.</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.4.3.1.1.1" class="ltx_p" style="width:85.4pt;">Client Selector</span>
</span>
</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:8.5359pt;">
<span id="S5.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.4.3.2.1.1" class="ltx_p" style="width:398.3pt;">Is an interface responsible of controlling the clients selected to train in each round.</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.5.4.1.1.1" class="ltx_p" style="width:85.4pt;">Metrics</span>
</span>
</td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.5.4.2.1.1" class="ltx_p" style="width:398.3pt;">Specify the accuracy metrics used to evaluate the model on test data at each round.</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<td id="S5.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.6.5.1.1.1" class="ltx_p" style="width:85.4pt;">Model</span>
</span>
</td>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.6.5.2.1.1" class="ltx_p" style="width:398.3pt;">Defines the ML model used for training, such as logistic regression.</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.7.6" class="ltx_tr">
<td id="S5.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.7.6.1.1.1" class="ltx_p" style="width:85.4pt;">Number of Rounds</span>
</span>
</td>
<td id="S5.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.7.6.2.1.1" class="ltx_p" style="width:398.3pt;">An indicator for how many rounds would the federated learning task run.</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.8.7" class="ltx_tr">
<td id="S5.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.8.7.1.1.1" class="ltx_p" style="width:85.4pt;">Data Loader</span>
</span>
</td>
<td id="S5.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.8.7.2.1.1" class="ltx_p" style="width:398.3pt;">An instance of this entity enables loading and distributing the data to a number of specified clients.</span>
</span>
</td>
</tr>
</tbody>
</table>
<br class="ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Experimental Simulation Settings</figcaption>
</figure>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Machine Learning Models</span>: 
<br class="ltx_break">To train the localFed framework, we must specify the ML models used for local training for each client.</p>
<ul id="S5.I2.i2.I1" class="ltx_itemize">
<li id="S5.I2.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i2.I1.i1.p1" class="ltx_para">
<p id="S5.I2.i2.I1.i1.p1.1" class="ltx_p"><span id="S5.I2.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Logistic Regression:</span> Previous work in literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, especially in binary classification, has proved the efficiency of this ML model. It is based on modelling the probability of a discrete outcome, in our case, either class ASD or non-ASD, given input variables or the features we have. This model is used for training the behavioral dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> given that it has multiple input features in addition to the merged data.</p>
</div>
</li>
<li id="S5.I2.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i2.I1.i2.p1" class="ltx_para">
<p id="S5.I2.i2.I1.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Neural Networks:</span> The neural networks model is used for classifying data in the ASD facial image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. This is a class of models commonly built with layers, inspired by the biological human brain and nervous system, which explains their name. A neural network model consists of an input layer, hidden layers, and output layer, where data is forwarded from one layer to the next one till reaching the desired accuracy. The main applications of this model, in which it has been proved highly efficient, are speech recognition, image recognition, and image classification, which is why we chose it to train on the facial image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
</li>
<li id="S5.I2.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i2.I1.i3.p1" class="ltx_para">
<p id="S5.I2.i2.I1.i3.p1.1" class="ltx_p"><span id="S5.I2.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Decision Trees:</span>
This is a supervised machine learning classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. A decision tree is directed and rooted, with two or more outgoing edges representing possible events, or if-then rules, that may occur depending on a certain discrete function. Leaf nodes represent the possible classes that may be predicted for each data instance, which are ASD or non-ASD in our case. For each new data instance, the class is predicted based on the pathway taken through the built decision tree. This ML model is used for experimenting with our merged ASD data.</p>
</div>
</li>
<li id="S5.I2.i2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i2.I1.i4.p1" class="ltx_para">
<p id="S5.I2.i2.I1.i4.p1.1" class="ltx_p"><span id="S5.I2.i2.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">K-Nearest Neighbors:</span>
The KNN classifier is a non-parametric machine learning model used in order to classify unlabeled instances by mapping them to the class of the most similar labelled instances within the dataset at hand <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. It thus classifies the data instance through predicting a class, in our case ASD or non-ASD, based on the classes of the K-nearest, neighbors or instances, to the instance under study. We utilize this model with k = 3 neighbors to test the merged data behavior in regular ML.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic"> </span><span id="S5.SS3.7.3" class="ltx_text ltx_font_bold">Results</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In this subsection, we tackle the results obtained from our federated learning scheme experiments on both datasets described earlier. We measure the accuracy as a function of the number of clients available and the maximum data size trained locally. All experiments displayed are tested on 20 epochs while varying the number of epochs does not play a role in changing the results.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<ol id="S5.I3" class="ltx_enumerate">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p"><span id="S5.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Our Approach with Autism Screening Behavioral Data</span></p>
</div>
<div id="S5.I3.i1.p2" class="ltx_para">
<p id="S5.I3.i1.p2.1" class="ltx_p">Experimental results obtained when ASD screening behavioral data is trained with FedML are represented in figure <a href="#S5.F5" title="Figure 5 â€£ item 1 â€£ V-C Results â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In the given plot, we represent the accuracy (right y-axis) by the line as a function of the number of clients in each experiment (x-axis) and the size of data trained locally by each client (left y-axis).
As could be seen clearly, the accuracy increases with increasing the size of data per client, achieved by decreasing the number of clients per experiment. For instance, we first test with a number of clients C = 50 clients. After the distribution of data by the localFed framework, every client receives around 20 data instances, hence a 0.48 accuracy is achieved. When the number of clients is decreased to C = 10, then the maximum data size per client is increased to around 45 data instances per client, and the accuracy increases to 0.51. The highest accuracy, equal to 0.7, is achieved with C = 3 clients and 250 data instances per client.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2211.00643/assets/images/behvfedmlresults2.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="315" height="249" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Results of FedML on Behavioral Dataset</figcaption>
</figure>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p"><span id="S5.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Our Approach with Facial Image Data</span></p>
</div>
<div id="S5.I3.i2.p2" class="ltx_para">
<p id="S5.I3.i2.p2.1" class="ltx_p">We plot, in figure <a href="#S5.F6" title="Figure 6 â€£ item 2 â€£ V-C Results â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the experimental results noted when ASD facial image data is trained with FedML. Similar to what we did previously with behavioral data, we represent the accuracy (right y-axis) by the line as a function of the number of clients in each experiment (x-axis), with the size of the data trained locally by each client (left y-axis).</p>
</div>
<div id="S5.I3.i2.p3" class="ltx_para">
<p id="S5.I3.i2.p3.1" class="ltx_p">By examining the given plot, we see that the global accuracy increases with a higher size of data per client and decreases the number of clients per experiment. First, we experiment with a number of clients, C = 50 clients. After the distribution of data by the localFed framework, every client receives around 58 data instances, representing facial images of autistic and non-autistic individuals. An accuracy of 0.5 is achieved. When we decrease the number of clients to C = 10, then the maximum data size per client is increased to around 290 data instances per client, and the accuracy increases to 0.53. Finally, as we decrease the number of clients to C = 3 clients and simultaneously increase the maximum data size to 250 data instances per client, the highest accuracy is achieved, equal to 0.6.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2211.00643/assets/images/imgresults.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="320" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Results of FedML on Facial Image Dataset</figcaption>
</figure>
</li>
<li id="S5.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I3.i3.p1" class="ltx_para">
<p id="S5.I3.i3.p1.1" class="ltx_p"><span id="S5.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Regular Machine Learning with Merged Data</span></p>
</div>
<div id="S5.I3.i3.p2" class="ltx_para">
<ol id="S5.I3.i3.I1" class="ltx_enumerate">
<li id="S5.I3.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.1</span> 
<div id="S5.I3.i3.I1.i1.p1" class="ltx_para">
<p id="S5.I3.i3.I1.i1.p1.1" class="ltx_p"><span id="S5.I3.i3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Train-Test Split</span></p>
</div>
<div id="S5.I3.i3.I1.i1.p2" class="ltx_para">
<p id="S5.I3.i3.I1.i1.p2.1" class="ltx_p">In Figure <a href="#S5.F7" title="Figure 7 â€£ item 3a â€£ item 3 â€£ V-C Results â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we plot the accuracy results obtained from testing several machine learning models on the merged data we have combined, as described in the previous section. The purpose of this experiment is to test the behavior of this data on regular machine learning models, to check which model provides the highest accuracy, before testing on our federated learning framework. For this machine learning training process, data is split regularly into 80% training set and 20% testing set, for the sake of measuring how far predictions are made later on from the test instances, or the prediction error <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> in other words. As we can visualize from the graph in Figure <a href="#S5.F7" title="Figure 7 â€£ item 3a â€£ item 3 â€£ V-C Results â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the highest accuracy of 0.65 is obtained with the logistic regression model. Lower accuracy values are obtained with decision trees, K-nearest neighbors with k = 3, and neural network models, which are equal to 0.62, 0.58, and 0.5 respectively.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2211.00643/assets/images/mergedregmlres.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="341" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Regular ML with train-test split on Merged Data</figcaption>
</figure>
</li>
<li id="S5.I3.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.2</span> 
<div id="S5.I3.i3.I1.i2.p1" class="ltx_para">
<p id="S5.I3.i3.I1.i2.p1.1" class="ltx_p"><span id="S5.I3.i3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">K-fold Cross-Validation</span></p>
</div>
<div id="S5.I3.i3.I1.i2.p2" class="ltx_para">
<p id="S5.I3.i3.I1.i2.p2.1" class="ltx_p">To experiment more with regular machine learning behavior on the merged data, we split the data using K-fold cross-validation (CV) instead of the usual train-test split. In K-fold cross-validation, as mentioned in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, data is partitioned into K separate equal-sized segments, called folds. Simultaneously, training occurs on K iterations, where at each iteration one fold is selected as a testing set and the remaining K-1 folds are used for training. We use cross-validation in order to test whether higher accuracy values would be achieved than with a train-test split for the different machine learning models used. We experiment with K = 8 folds while shuffling data instances during data segmentation. As plotted in Figure <a href="#S5.F8" title="Figure 8 â€£ item 3b â€£ item 3 â€£ V-C Results â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, the accuracy results are slightly higher than those noted with the train-test split in Figure <a href="#S5.F7" title="Figure 7 â€£ item 3a â€£ item 3 â€£ V-C Results â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The logistic regression model remains the most accurate at 0.6525, which is slightly higher than that with a regular train-test split. As for decision trees, K-nearest neighbors, and neural network models, the accuracy values noted are 0.636, 0.59, and 0.485, respectively, which are also somewhat higher than those obtained without cross-validation.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2211.00643/assets/images/regmlmergedkfold.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="276" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Regular ML on Merged Data with K-fold CV</figcaption>
</figure>
</li>
</ol>
</div>
</li>
<li id="S5.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I3.i4.p1" class="ltx_para">
<p id="S5.I3.i4.p1.1" class="ltx_p"><span id="S5.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Our Approach with Merged Data</span></p>
</div>
<div id="S5.I3.i4.p2" class="ltx_para">
<p id="S5.I3.i4.p2.1" class="ltx_p">After testing the merged data on regular machine learning, the best accuracy is obtained with a logistic regression model with an accuracy value of 65%. Thus, we apply this model in our federated learning framework described in Section <a href="#S5.SS2" title="V-B Setup â€£ V Experiments and Results â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>. The obtained results, constituting the global accuracy as a function of the number of clients and data size per client, are plotted in Figure <a href="#S6.F9" title="Figure 9 â€£ VI Discussion â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="S5.I3.i4.p3" class="ltx_para">
<p id="S5.I3.i4.p3.1" class="ltx_p">As presented in Figure <a href="#S6.F9" title="Figure 9 â€£ VI Discussion â€£ A Federated Learning Scheme for Neuro-developmental Disorders: Multi-Aspect ASD Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the global accuracy, computed in the global aggregator of the federated learning environment, is plotted while varying the number of clients or local trainers and the size of data per client. The first experiment was done with C = 50 clients, and the data size was 53 instances per client. The global accuracy is 0.5. As we decrease the number of clients to C = 10, the data size per client thus increases to 270 data instances per client, and the global accuracy also rises to 0.52. The latter keeps rising, while simultaneously increasing the number of data instances per client and decreasing the number of training clients. Then, the highest global accuracy of 0.63 is achieved with the lowest number of clients (C = 3) and the highest data size per client (1000 data instances). Therefore, the remarkable performance of our federated learning models on the different types of data would be that the number of clients is inversely proportional to the achieved global accuracy. Fewer clients lead to a higher data size per client, and then higher accuracy.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S6" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_bold">Discussion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Throughout this section, we discuss and analyze the results displayed in the previous section and give interpretations of how they were obtained. We also compare our results obtained for predicting Autism Spectrum Disorder (ASD) using our federated learning scheme to previous studies in the literature predicting the same disorder using regular ML models.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">According to what we noted in the results, similar behavior is marked with training both ASD behavioral screening data and ASD facial image data on our federated learning framework and then with the merged data as well. The most notable behavior of the plotted results would be that the global accuracy increases by decreasing the number of clients in the federated learning experiment. This note would be interpreted by the fact that when we have a higher number of clients, fewer data instances are available to each client for local training.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Therefore, as any ML training is based on the data fed to its model, having fewer clients will eventually lead to a higher number of data instances, given that data is equally divided among clients. Thus, this redirects the trainer to a higher chance of training on larger data, and then a higher accuracy of its model.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">In comparison to regular ML models, higher accuracy is achieved in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for ASD screening behavioral data and Kaggle <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.kaggle.com/code/cihan063/autism-cnn-vgg16</span></span></span> for ASD facial image data. Concerning the latter, a convolutional neural networks (CNN) model was used, which achieved a 0.82 test accuracy, higher than the best accuracy we obtained with our federated learning model, equal to 0.6 with 3 clients being available. As for the work done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for regular ML on ASD behavioral screening data, their accuracy noted was 0.9 with the artificial neural networks (ANN) model, which is also higher than the accuracy achieved in our experiments, which equals to 0.7 per 3 clients in the experiment.</p>
</div>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2211.00643/assets/images/mergedresultsfed.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="308" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Results of FedML on Merged Data</figcaption>
</figure>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Surely, higher accuracy would be observed with regular ML in comparison to federated models, due to the distribution of data among clients. However, a higher level of data security and privacy is achieved when it comes to federated learning, which lacks regular ML training. This is essential in the application we are tackling, ASD prediction, given that the data being used is medical and personal, thus confidential within the screening center, and sensitive to leakage.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Furthermore, when it comes to merged data, higher accuracy is achieved with regular ML in comparison to federated ML, due to the distribution of data instances among local trainers, as also observed with the other types of data. Moreover, the highest accuracy achieved with regular ML and federated learning on the merged data is around 65% and 63% respectively. Given that the merged data, through which we conducted these experiments, is randomly generated and combined, these accuracy values could be acceptable. On the other hand, higher accuracy values would be achieved if the data, combining both behavioral and facial characteristics of ASD as well as non-ASD individuals, was real rather than randomly generated and mapped. Due to the fact that we are merging two datasets, we are adding additional meaningful features to the data. This should enhance the performance of the machine learner as it would make the data/labels more detailed and specific, which improves the learning accuracy. Hence, having real ASD/Non-ASD data containing behavioral data as well as facial distances, representing real patients, rather than randomly generated and correlated, would enhance the global accuracy obtained. Moreover, the machine learnerâ€™s performance would be enhanced with multi-aspect data given the medical proof we base our work on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Then, measuring the facial distances, the process we did through feature extraction, would be a major signal for differentiating ASD from non-ASD individuals. Therefore, the facial distance features we have added to the behavioral data should positively impact the learning process of a machine learner, especially if the facial images were real and directly related to the behavioral data instances. We would thus suggest having a dataset of facial images, with each image having a brief description of the ASD patient or normal individual, which would help more in enhancing the merging criteria for both behavioral and facial datasets. Hence, in brief, having real ASD/Non-ASD data containing behavioral data as well as facial details that belong to the same individuals, representing real patients, rather than randomly generated and correlated, would enhance the global accuracy obtained.</p>
</div>
</section>
<section id="S7" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_bold">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we introduce a new federated learning scheme for predicting Autism Spectrum Disorder (ASD), characterized by special behavioral and social traits and medically proven to affect the facial traits of its patients. For this sake, we propose a prediction framework trained on two forms of data: the first consists of ASD screening behavioral data, made up of filled surveys on ASD patientsâ€™ social and personal traits, and the second consists of images corresponding to ASD as well as non-ASD individualsâ€™ faces. Thus, given one of these two input datasets, our federated learning scheme would predict whether the individual is autistic or not, in a private prediction framework. We obtain remarkable results in comparison to previous regular ML models. Within our private training environment, a 71% accuracy was obtained for ASD prediction given behavioral data, while facial image ASD data produced a 62% accuracy. However, a higher number of data instances for both datasets in the learning process would have been more promising. For the sake of testing ASD data containing both behavioral and facial features, we tested a merged dataset via facial feature extraction using Python Dlib. Then, we study the behavior of the merged data on regular as well as federated machine learning, achieving a 65% and 63% accuracy, respectively. Therefore, we propose, as a future enhancement, testing the different models on real-world data instead of experimenting with synthetic data instances. We would also test the feasibility of using Federated Transfer Learning for the prediction of ASD as well as other medical conditions according to special recognizable characteristics and features of individuals.</p>
</div>
</section>
<section id="Sx1" class="ltx_section ltx_indentfirst">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was jointly supported by the College of Technological Innovation (CTI), Zayed University (ZU), under grant number RIF-20130, and the Lebanese American University (LAU).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M.Â Emon, M.Â Keya, A.Â Sozib, and S.Â Islam, â€œA comparative analysis of autistic
spectrum disorder (asd) disease for children using ml approaches,â€

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K.Â Aldridge, I.Â D. George, K.Â K. Cole, J.Â R. Austin, T.Â N. Takahashi, Y.Â Duan,
and J.Â H. Miles, â€œFacial phenotypes in subgroups of prepubertal boys with
autism spectrum disorders are correlated with clinical phenotypes,â€ <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Molecular autism</span>, vol.Â 2, no.Â 1, pp.Â 1â€“12, 2011.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.Â J. McGonigle, J.Â M. Migyanka, S.Â J. Glor-Scheib, R.Â Cramer, J.Â J.
Fratangeli, G.Â G. Hegde, J.Â Shang, and A.Â Venkat, â€œDevelopment and
evaluation of educational materials for pre-hospital and emergency department
personnel on the care of patients with autism spectrum disorder,â€ <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Journal of Autism and Developmental Disorders</span>, vol.Â 44, no.Â 5,
pp.Â 1252â€“1259, 2014.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G.Â Tripi, S.Â Roux, D.Â Matranga, L.Â Maniscalco, P.Â Glorioso,
F.Â Bonnet-Brilhault, and M.Â Roccella, â€œCranio-facial characteristics in
children with autism spectrum disorders (asd),â€ <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Journal of Clinical
Medicine</span>, vol.Â 8, no.Â 5, p.Â 641, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D.Â W. Tan, M.Â T. Maybery, L.Â Ewing, J.-X. Tay, P.Â R. Eastwood, and A.Â J.
Whitehouse, â€œSex-specific variation in facial masculinity/femininity
associated with autistic traits in the general population,â€ <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">British
Journal of Psychology</span>, vol.Â 111, no.Â 4, pp.Â 723â€“741, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D.Â W. Tan, S.Â Z. Gilani, M.Â T. Maybery, A.Â Mian, A.Â Hunt, M.Â Walters, and A.Â J.
Whitehouse, â€œHypermasculinised facial morphology in boys and girls with
autism spectrum disorder and its association with symptomatology,â€ <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Scientific reports</span>, vol.Â 7, no.Â 1, pp.Â 1â€“11, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T.Â Obafemi-Ajayi, J.Â H. Miles, T.Â N. Takahashi, W.Â Qi, K.Â Aldridge, M.Â Zhang,
S.-Q. Xin, Y.Â He, and Y.Â Duan, â€œFacial structure analysis separates autism
spectrum disorders into meaningful clinical subgroups,â€ <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Journal of
autism and developmental disorders</span>, vol.Â 45, no.Â 5, pp.Â 1302â€“1317, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
I.Â ElÂ Naqa and M.Â J. Murphy, â€œWhat is machine learning?,â€ in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">machine
learning in radiation oncology</span>, pp.Â 3â€“11, Springer, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.Â B. Kotsiantis, I.Â Zaharakis, P.Â Pintelas, <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œSupervised machine
learning: A review of classification techniques,â€ <span id="bib.bib9.2.2" class="ltx_text ltx_font_italic">Emerging artificial
intelligence applications in computer engineering</span>, vol.Â 160, no.Â 1,
pp.Â 3â€“24, 2007.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
F.Â Osisanwo, J.Â Akinsola, O.Â Awodele, J.Â Hinmikaiye, O.Â Olakanmi, and
J.Â Akinjobi, â€œSupervised machine learning algorithms: classification and
comparison,â€ <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Trends and Technology
(IJCTT)</span>, vol.Â 48, no.Â 3, pp.Â 128â€“138, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C.-R. Shyu, K.Â T. Putra, H.-C. Chen, Y.-Y. Tsai, K.Â Hossain, W.Â Jiang, Z.-Y.
Shae, <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œA systematic review of federated learning in the
healthcare area: From the perspective of data properties and applications,â€
<span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Applied Sciences</span>, vol.Â 11, no.Â 23, p.Â 11191, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A.Â Rahman, M.Â Hossain, G.Â Muhammad, D.Â Kundu, T.Â Debnath, M.Â Rahman, M.Â Khan,
S.Â Islam, P.Â Tiwari, S.Â S. Band, <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œFederated learning-based ai
approaches in smart healthcare: concepts, taxonomies, challenges and open
issues,â€ <span id="bib.bib12.2.2" class="ltx_text ltx_font_italic">Cluster Computing</span>, pp.Â 1â€“41, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M.Â Usama, B.Â Ahmad, W.Â Xiao, M.Â S. Hossain, and G.Â Muhammad, â€œCorrigendum to
self-attention based recurrent convolutional neural network for disease
prediction using healthcare data,â€ <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Computer Methods and Programs in
Biomedicine</span>, vol.Â 220, p.Â 106710, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
O.Â A. Wahab, A.Â Mourad, H.Â Otrok, and T.Â Taleb, â€œFederated machine learning:
Survey, multi-level classification, desirable criteria and future directions
in communication and networking systems,â€ <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys
Tutorials</span>, vol.Â 23, no.Â 2, pp.Â 1342â€“1397, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S.Â Abdulrahman, H.Â Tout, H.Â Ould-Slimane, A.Â Mourad, C.Â Talhi, and M.Â Guizani,
â€œA survey on federated learning: The journey from centralized to distributed
on-site learning and beyond,â€ <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, vol.Â 8,
no.Â 7, pp.Â 5476â€“5497, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C.Â He, S.Â Li, J.Â So, X.Â Zeng, M.Â Zhang, H.Â Wang, X.Â Wang, P.Â Vepakomma,
A.Â Singh, H.Â Qiu, <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œFedml: A research library and benchmark for
federated machine learning,â€ <span id="bib.bib16.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.13518</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M.Â L. Cervantes-HenrÃ­quez, J.Â E. Acosta-LÃ³pez, A.Â F. Martinez,
M.Â Arcos-Burgos, P.Â J. Puentes-Rozo, and J.Â I. VÃ©lez, â€œMachine learning
prediction of adhd severity: association and linkage to adgrl3, drd4, and
snap25,â€ <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Journal of Attention Disorders</span>, p.Â 10870547211015426, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G.Â A. Kaissis, M.Â R. Makowski, D.Â RÃ¼ckert, and R.Â F. Braren, â€œSecure,
privacy-preserving and federated machine learning in medical imaging,â€ <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Nature Machine Intelligence</span>, vol.Â 2, no.Â 6, pp.Â 305â€“311, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S.Â K. Lo, Q.Â Lu, C.Â Wang, H.-Y. Paik, and L.Â Zhu, â€œA systematic literature
review on federated machine learning: From a software engineering
perspective,â€ <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, vol.Â 54, no.Â 5, pp.Â 1â€“39,
2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J.Â Patel, D.Â TejalUpadhyay, and S.Â Patel, â€œHeart disease prediction using
machine learning and data mining technique,â€ <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Heart Disease</span>, vol.Â 7,
no.Â 1, pp.Â 129â€“137, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S.Â Mohan, C.Â Thirumalai, and G.Â Srivastava, â€œEffective heart disease
prediction using hybrid machine learning techniques,â€ <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE access</span>,
vol.Â 7, pp.Â 81542â€“81554, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T.Â V. Sriram, M.Â V. Rao, G.Â Narayana, D.Â Kaladhar, and T.Â P.Â R. Vital,
â€œIntelligent parkinson disease prediction using machine learning
algorithms,â€ <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Int. J. Eng. Innov. Technol</span>, vol.Â 3, pp.Â 212â€“215, 2013.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.Â S. Yadav and S.Â M. Jadhav, â€œDeep convolutional neural network based medical
image classification for disease diagnosis,â€ <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Journal of Big Data</span>,
vol.Â 6, no.Â 1, pp.Â 1â€“18, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A.Â KoÅ‚akowska, A.Â Landowska, A.Â Anzulewicz, and K.Â Sobota, â€œAutomatic
recognition of therapy progress among children with autism,â€ <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Scientific
reports</span>, vol.Â 7, no.Â 1, pp.Â 1â€“14, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z.Â Zhao, H.Â Tang, X.Â Zhang, X.Â Qu, X.Â Hu, J.Â Lu, <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œClassification
of children with autism and typical development using eye-tracking data from
face-to-face conversations: Machine learning model development and
performance evaluation,â€ <span id="bib.bib25.2.2" class="ltx_text ltx_font_italic">Journal of Medical Internet Research</span>,
vol.Â 23, no.Â 8, p.Â e29328, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
P.Â Mazumdar, G.Â Arru, and F.Â Battisti, â€œEarly detection of children with
autism spectrum disorder based on visual exploration of images,â€ <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Signal
Processing: Image Communication</span>, vol.Â 94, p.Â 116184, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
F.Â Thabtah and D.Â Peebles, â€œA new machine learning model based on induction of
rules for autism detection,â€ <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Health informatics journal</span>, vol.Â 26,
no.Â 1, pp.Â 264â€“286, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C.Â J. Kumar and P.Â R. Das, â€œThe diagnosis of asd using multiple machine
learning techniques,â€ <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">International Journal of Developmental
Disabilities</span>, pp.Â 1â€“11, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A.Â S. Heinsfeld, A.Â R. Franco, R.Â C. Craddock, A.Â Buchweitz, and F.Â Meneguzzi,
â€œIdentification of autism spectrum disorder using deep learning and the
abide dataset,â€ <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">NeuroImage: Clinical</span>, vol.Â 17, pp.Â 16â€“23, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X.Â Li, Y.Â Gu, N.Â Dvornek, L.Â H. Staib, P.Â Ventola, and J.Â S. Duncan,
â€œMulti-site fmri analysis using privacy-preserving federated learning and
domain adaptation: Abide results,â€ <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Medical Image Analysis</span>, vol.Â 65,
p.Â 101765, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
â€œAutism Screening on Adults.â€
<a target="_blank" href="https://www.kaggle.com/datasets/andrewmvd/autism-screening-on-adults" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/andrewmvd/autism-screening-on-adults</a>,
2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
â€œAutism Image Data.â€
<a target="_blank" href="https://www.kaggle.com/datasets/cihan063/%5C%5C%0Aautism-image-data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/cihan063/
<br class="ltx_break">autism-image-data</a>, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M.Â Alsawwaf, Z.Â Chaczko, M.Â Kulbacki, and N.Â Sarathy, â€œIn your face: person
identification through ratios and distances between facial features,â€ <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Vietnam Journal of Computer Science</span>, vol.Â 9, no.Â 02, pp.Â 187â€“202, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
D.Â E. King, â€œDlib-ml: A machine learning toolkit,â€ <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">The Journal of
Machine Learning Research</span>, vol.Â 10, pp.Â 1755â€“1758, 2009.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
L.Â Rokach and O.Â Maimon, â€œDecision trees,â€ in <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Data mining and knowledge
discovery handbook</span>, pp.Â 165â€“192, Springer, 2005.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Z.Â Zhang, â€œIntroduction to machine learning: k-nearest neighbors,â€ <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Annals of translational medicine</span>, vol.Â 4, no.Â 11, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J.Â D. Rodriguez, A.Â Perez, and J.Â A. Lozano, â€œSensitivity analysis of k-fold
cross validation in prediction error estimation,â€ <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">IEEE transactions on
pattern analysis and machine intelligence</span>, vol.Â 32, no.Â 3, pp.Â 569â€“575,
2009.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
P.Â Refaeilzadeh, L.Â Tang, and H.Â Liu, â€œCross-validation.,â€ <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Encyclopedia
of database systems</span>, vol.Â 5, pp.Â 532â€“538, 2009.

</span>
</li>
</ul>
</section>
<figure id="id8" class="ltx_float biography">
<table id="id8.1" class="ltx_tabular">
<tr id="id8.1.1" class="ltx_tr">
<td id="id8.1.1.1" class="ltx_td"><img src="/html/2211.00643/assets/images/img.jpeg" id="id8.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="109" alt="[Uncaptioned image]"></td>
<td id="id8.1.1.2" class="ltx_td">
<span id="id8.1.1.2.1" class="ltx_inline-block">
<span id="id8.1.1.2.1.1" class="ltx_p"><span id="id8.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Hala Shamseddine</span> 
graduated with Bachelor of Science and Master of Science degrees in Computer Science from the Lebanese American University (LAU) based in Beirut, Lebanon, in June 2020 and August 2022 respectively. She is currently expanding her technical expertise within the tech field. Her research interests include fog federation formation, fog and cloud computing, federated machine learning, artificial intelligence, privacy and security.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id9" class="ltx_float biography">
<table id="id9.1" class="ltx_tabular">
<tr id="id9.1.1" class="ltx_tr">
<td id="id9.1.1.1" class="ltx_td"><img src="/html/2211.00643/assets/images/newsafapic.jpg" id="id9.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id9.1.1.2" class="ltx_td">
<span id="id9.1.1.2.1" class="ltx_inline-block">
<span id="id9.1.1.2.1.1" class="ltx_p"><span id="id9.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Safa Otoum</span> 
(Mâ€™19) is an assistant professor of computer engineering in the College of Technological Innovation (CTI), Zayed University, United Arab Emirates. She received her M.A.Sc. and Ph.D. degrees in computer engineering from the University of Ottawa, Canada, in 2015 and 2019, respectively. Her research interests include blockchain applications, applications of ML and AI, IoT, and intrusion detection and prevention systems. She is a registered Professional Engineer (P.Eng.) in Ontario.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id10" class="ltx_float biography">
<table id="id10.1" class="ltx_tabular">
<tr id="id10.1.1" class="ltx_tr">
<td id="id10.1.1.1" class="ltx_td"><img src="/html/2211.00643/assets/images/PublicationsNewPic2021.jpeg" id="id10.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="119" alt="[Uncaptioned image]"></td>
<td id="id10.1.1.2" class="ltx_td">
<span id="id10.1.1.2.1" class="ltx_inline-block">
<span id="id10.1.1.2.1.1" class="ltx_p"><span id="id10.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Azzam Mourad</span> 
received his M.Sc. in CS from Laval University, Canada (2003) and Ph.D. in ECE from Concordia University, Canada (2008). He is currently Professor of Computer Science and Founding Director of the Cyber Security Systems and Applied AI Research Center with the Lebanese American University, Visiting Professor of Computer Science with New York University Abu Dhabi and Affiliate Professor with the Software Engineering and IT Department, Ecole de Technologie Superieure (ETS), Montreal, Canada. His research interests include Cyber Security, Federated Machine Learning, Network and Service Optimization and Management targeting IoT and IoV, Cloud/Fog/Edge Computing, and Vehicular and Mobile Networks. He has served/serves as an associate editor for IEEE Transactions on Services Computing, IEEE Transactions on Network and Service Management, IEEE Network, IEEE Open Journal of the Communications Society, IET Quantum Communication, and IEEE Communications Letters, the General Chair of IWCMC2020, the General Co-Chair of WiMob2016, and the Track Chair, a TPC member, and a reviewer for several prestigious journals and conferences. He is an IEEE senior member.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.00642" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.00643" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.00643">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.00643" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.00646" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 06:05:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
