<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.15102] Fake it till you make it: face analysis in the wild using synthetic data alone</title><meta property="og:description" content="We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone.
The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fake it till you make it: face analysis in the wild using synthetic data alone">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fake it till you make it: face analysis in the wild using synthetic data alone">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.15102">

<!--Generated on Sat Mar  2 03:21:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Fake it till you make it: face analysis in the wild using synthetic data alone</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Erroll Wood  Tadas Baltrušaitis<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>  Charlie Hewitt  Sebastian Dziadzio 
<br class="ltx_break">Matthew Johnson  Virginia Estellers  Thomas J. Cashman  Jamie Shotton 
<br class="ltx_break">
Microsoft
</span><span class="ltx_author_notes">Denotes equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text">We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone.
The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces.
Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets.
We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity.
We train machine learning systems for face-related tasks such as landmark localization and face parsing,
showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">When faced with a machine learning problem, the hardest challenge often isn’t choosing the right machine learning model, it’s finding the right data.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark"></sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark"></sup><span class="ltx_tag ltx_tag_note"></span><a target="_blank" href="https://microsoft.github.io/FaceSynthetics" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://microsoft.github.io/FaceSynthetics</a></span></span></span>
This is especially difficult in the realm of human-related computer vision, where concerns about the fairness of models and the ethics of deployment are paramount <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>.
Instead of collecting and labelling real data, which is slow, expensive, and subject to bias, it can be preferable to <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">synthesize</em> training data using computer graphics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>.
With synthetic data, you can guarantee perfect labels without annotation noise, generate rich labels that are otherwise impossible to label by hand, and have full control over variation and diversity in a dataset.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_hero_render.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="200" height="290" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_hero_render_clay.jpg" id="S1.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="200" height="290" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_hero_render_gt_2.jpg" id="S1.F1.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="200" height="290" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_diversity_D1.jpg" id="S1.F1.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="120" height="174" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_diversity_D4.jpg" id="S1.F1.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="120" height="174" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_diversity_D5.jpg" id="S1.F1.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="120" height="174" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_diversity_D6.jpg" id="S1.F1.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="120" height="174" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/hero_figures/iccv21_diversity_D7.jpg" id="S1.F1.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="120" height="174" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We render training images of faces with unprecedented realism and diversity. The first example above is shown along with 3D geometry and accompanying labels for machine learning.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2109.15102/assets/figures/method_figures/iccv21_process_render_0.jpg" id="S1.F2.g1" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="83" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2109.15102/assets/figures/method_figures/iccv21_process_render_1.jpg" id="S1.F2.g2" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="83" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2109.15102/assets/figures/method_figures/iccv21_process_render_2.jpg" id="S1.F2.g3" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="83" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2109.15102/assets/figures/method_figures/iccv21_process_render_3.jpg" id="S1.F2.g4" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="83" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2109.15102/assets/figures/method_figures/iccv21_process_render_4.jpg" id="S1.F2.g5" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="83" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2109.15102/assets/figures/method_figures/iccv21_process_render_5.jpg" id="S1.F2.g6" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="83" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2109.15102/assets/figures/method_figures/iccv21_process_render_6.jpg" id="S1.F2.g7" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="83" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S1.F2.1" class="ltx_p ltx_figure_panel"><span id="S1.F2.1.1" class="ltx_text ltx_inline-block" style="font-size:90%;width:59.8pt;">Template face</span>
<span id="S1.F2.1.2" class="ltx_text ltx_inline-block" style="font-size:90%;width:59.8pt;">+ identity</span>
<span id="S1.F2.1.3" class="ltx_text ltx_inline-block" style="font-size:90%;width:59.8pt;">+ expression</span>
<span id="S1.F2.1.4" class="ltx_text ltx_inline-block" style="font-size:90%;width:59.8pt;">+ texture</span>
<span id="S1.F2.1.5" class="ltx_text ltx_inline-block" style="font-size:90%;width:59.8pt;">+ hair</span>
<span id="S1.F2.1.6" class="ltx_text ltx_inline-block" style="font-size:90%;width:59.8pt;">+ clothes</span>
<span id="S1.F2.1.7" class="ltx_text ltx_inline-block" style="font-size:90%;width:59.8pt;">+ environment</span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>We procedurally construct synthetic faces that are realistic and expressive. Starting with our template face, we randomize the identity, choose a random expression, apply a random texture, attach random hair and clothing, and render the face in a random environment.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Rendering convincing humans is one of the hardest problems in computer graphics.
Movies and video games have shown that realistic digital humans are possible, but with significant artist effort per individual <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>.
While it’s possible to generate endless novel face images with recent self-supervised approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, corresponding labels for supervised learning are not available.
As a result, previous work has resorted to synthesizing facial training data with simplifications, with results that are far from realistic.
We have seen progress in efforts that attempt to cross the domain gap using domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> by refining synthetic images to look more real, and domain-adversarial training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> where machine learning models are encouraged to ignore differences between the synthetic and real domains, but less work has attempted to improve the quality of synthetic data itself.
Synthesizing realistic face data has been considered so hard that we encounter the assumption that synthetic data cannot fully replace real data for problems in the wild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper we demonstrate that the opportunities for synthetic data are much wider than previously realised, and are achievable today.
We present a new method of acquiring training data for faces – rendering 3D face models with an unprecedented level of realism and diversity (see <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>).
With a sufficiently good synthetic framework, it is possible to create training data that can be used to solve real world problems in the wild, without using any real data at all.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">It requires considerable expertise and investment to develop a synthetics framework with minimal domain gap.
However, once implemented, it becomes possible to generate a wide variety of training data with minimal incremental effort.
Let’s consider some examples; say you have spent time labelling face images with landmarks.
However, you suddenly require additional landmarks in each image.
Relabelling and verifying will take a long time, but
with synthetics, you can regenerate clean and consistent labels at a moment’s notice.
Or, say you are developing computer vision algorithms for a new camera, e.g. an infrared face-recognition camera in a mobile phone.
Few, if any, hardware prototypes may exist, making it hard to collect a dataset.
Synthetics lets you render faces from a simulated device to develop algorithms and even guide hardware design itself.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We synthesize face images by procedurally combining a parametric face model with a large library of high-quality artist-created assets, including textures, hair, and clothing (see <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>).
With this data we train models for common face-related tasks: face parsing and landmark localization.
Our experiments show that models trained with a single generic synthetic dataset can be just as accurate as those trained with task-specific real datasets, achieving results in line with the state of the art.
This opens the door to other face-related tasks that can be confidently addressed with synthetic data instead of real.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions are as follows.
First, we describe how to synthesize realistic and diverse training data for face analysis in the wild, achieving results in line with the state of the art.
Second, we present ablation studies that validate the steps taken to achieve photorealism.
Third is the synthetic dataset itself, which is available from our project webpage: <a target="_blank" href="https://microsoft.github.io/FaceSynthetics" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://microsoft.github.io/FaceSynthetics</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Diverse face datasets are very difficult to collect and annotate.
Collection techniques such as web crawling pose significant privacy and copyright concerns.
Manual annotation is error-prone and can often result in inconsistent labels.
Hence, the research community is increasingly looking at augmenting or replacing real data with synthetic.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic face data</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The computer vision community has used synthetic data for many tasks, including
object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite>,
scene understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>,
eye tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>,
hand tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>, and
full-body analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>.
However, relatively little previous work has attempted to generate full-face synthetics using computer graphics, due to the complexity of modeling the human head.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">A common approach is to use a 3D Morphable Model (3DMM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, since these can provide consistent labels for different faces.
Previous work has focused on parts of the face such as the eye region <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite> or the <em id="S2.SS1.p2.1.1" class="ltx_emph ltx_font_italic">hockey mask</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Zeng et al.</span> [<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Richardson et al.</span> [<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Sela et al.</span> [<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> used 3DMMs to render training data for reconstructing detailed facial geometry.
Similarly, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wood et al.</span> [<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite> rendered an eye region 3DMM for gaze estimation. However, since these approaches only render part of the face, the resulting data has limited use for tasks that consider the whole face.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Building parametric models is challenging, so an alternative is to render 3D scans directly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>, <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Jeni et al.</span> [<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> rendered the BU-4DFE dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite> for dense 3D face alignment,
and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Kuhnke and Ostermann</span> [<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> rendered commercially-available 3D head scans for head pose estimation.
While often realistic, these approaches are limited by the diversity expressed in the scans themselves, and cannot provide rich semantic labels for machine learning.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Manipulating 2D images can be an alternative to using a 3D graphics pipeline.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Zhu et al.</span> [<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> fit a 3DMM to face images, and warped them to augment the head pose.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Nojavanasghari et al.</span> [<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> composited hand images onto faces to improve face detection.
These approaches can only make minor adjustments to existing images, limiting their use.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Training with synthetic data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Although it is common to rely on synthetic data alone for full-body tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, synthetic data is rarely used on its own for face-related machine learning.
Instead it is either first adapted to make it look more like some target domain, or used alongside real data for pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite> or regularizing models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>.
The reason for this is the <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">domain gap</em> – a difference in distributions between real and synthetic data which makes generalization difficult <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Learned domain adaptation modifies synthetic images to better match the appearance of real images.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Shrivastava et al.</span> [<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> use an adversarial refiner network to adapt synthetic eye images with regularization to preserve annotations.
Similarly, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bak et al.</span> [<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> adapt synthetic data using a CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite> with a regularization term for preserving identities.
A limitation of learned domain adaptation is the tendency for image semantics to change during adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, hence the need for regularization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>.
These techniques are therefore unsuitable for fine-grained annotations, such as per-pixel labels or precise landmark coordinates.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Instead of adapting data, it is possible to learn features that are resistant to the differences between domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wu et al.</span> [<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite> mix real and synthetic data through a domain classifier to learn domain-invariant features for text detection, and
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Saleh et al.</span> [<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite> exploit the observation that shape is less affected by the domain gap than appearance for scene semantic segmentation.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">In our work, we do not perform any of these techniques and instead minimize the domain gap at the source, by generating highly realistic synthetic data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthesizing face images</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The Visual Effects (VFX) industry has developed many techniques for convincing audiences that 3D faces are real, and we build upon these in our approach.
However, a key difference is scale: while VFX might be used for a handful of actors, we require diverse training data of thousands of synthetic individuals.
To address this, we use procedural generation to randomly create and render novel 3D faces without any manual intervention.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We start by sampling a generative 3D face model that captures the diversity of the human population.
We then randomly ‘dress up’ each face with samples from large collections of hair, clothing, and accessory assets.
All collections are sampled independently to create synthetic individuals who are as diverse as possible from one another.
This section describes the technical components we built in order to enable asset collections that can be mixed-and-matched atop 3D faces in a random, yet plausible manner.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>3D face model</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2109.15102/assets/figures/iccv21_identity_samples.jpg" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="286" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>3D faces sampled from our generative model, demonstrating how our model captures the diversity of the human population.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2109.15102/assets/x1.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="140" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Histograms of self-reported age, gender, and ethnicity in our scan collection, which was used to build our face model and texture library. Our collection covers a range of age and ethnicity.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">Our generative 3D face model captures how face shape varies across the human population, and changes during facial expressions.
It is a blendshape-based face rig similar to previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, and comprises a mesh of <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="N\!=\!7,667" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3" xref="S3.SS1.p1.1.m1.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.2.cmml">N</mi><mo lspace="0.108em" rspace="0.108em" id="S3.SS1.p1.1.m1.2.3.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.2.3.3.2" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">7</mn><mo id="S3.SS1.p1.1.m1.2.3.3.2.1" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">667</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.3"><eq id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.1"></eq><ci id="S3.SS1.p1.1.m1.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.3.2">𝑁</ci><list id="S3.SS1.p1.1.m1.2.3.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.3.2"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">7</cn><cn type="integer" id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">667</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">N\!=\!7,667</annotation></semantics></math> vertices and <math id="S3.SS1.p1.2.m2.2" class="ltx_Math" alttext="7,414" display="inline"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.3.2" xref="S3.SS1.p1.2.m2.2.3.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">7</mn><mo id="S3.SS1.p1.2.m2.2.3.2.1" xref="S3.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">414</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><list id="S3.SS1.p1.2.m2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.2"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">7</cn><cn type="integer" id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">414</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">7,414</annotation></semantics></math> polygons, and a minimal skeleton of <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="K\!=\!4" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">K</mi><mo lspace="0.108em" rspace="0.108em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></eq><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝐾</ci><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">K\!=\!4</annotation></semantics></math> joints: the head, neck, and two eyes.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.6" class="ltx_p">The face mesh vertex positions are defined by mesh generating function
<math id="S3.SS1.p2.1.m1.6" class="ltx_Math" alttext="\mathcal{M}(\vec{\beta},\vec{\psi},\vec{\theta})\!:\!\mathbb{R}^{|\vec{\beta}|\times|\vec{\psi}|\times|\vec{\theta}|}\!\to\!\mathbb{R}^{N\times 3}" display="inline"><semantics id="S3.SS1.p2.1.m1.6a"><mrow id="S3.SS1.p2.1.m1.6.7" xref="S3.SS1.p2.1.m1.6.7.cmml"><mrow id="S3.SS1.p2.1.m1.6.7.2" xref="S3.SS1.p2.1.m1.6.7.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.6.7.2.2" xref="S3.SS1.p2.1.m1.6.7.2.2.cmml">ℳ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.6.7.2.1" xref="S3.SS1.p2.1.m1.6.7.2.1.cmml">​</mo><mrow id="S3.SS1.p2.1.m1.6.7.2.3.2" xref="S3.SS1.p2.1.m1.6.7.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.6.7.2.3.2.1" xref="S3.SS1.p2.1.m1.6.7.2.3.1.cmml">(</mo><mover accent="true" id="S3.SS1.p2.1.m1.4.4" xref="S3.SS1.p2.1.m1.4.4.cmml"><mi id="S3.SS1.p2.1.m1.4.4.2" xref="S3.SS1.p2.1.m1.4.4.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.4.4.1" xref="S3.SS1.p2.1.m1.4.4.1.cmml">→</mo></mover><mo id="S3.SS1.p2.1.m1.6.7.2.3.2.2" xref="S3.SS1.p2.1.m1.6.7.2.3.1.cmml">,</mo><mover accent="true" id="S3.SS1.p2.1.m1.5.5" xref="S3.SS1.p2.1.m1.5.5.cmml"><mi id="S3.SS1.p2.1.m1.5.5.2" xref="S3.SS1.p2.1.m1.5.5.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.5.5.1" xref="S3.SS1.p2.1.m1.5.5.1.cmml">→</mo></mover><mo id="S3.SS1.p2.1.m1.6.7.2.3.2.3" xref="S3.SS1.p2.1.m1.6.7.2.3.1.cmml">,</mo><mover accent="true" id="S3.SS1.p2.1.m1.6.6" xref="S3.SS1.p2.1.m1.6.6.cmml"><mi id="S3.SS1.p2.1.m1.6.6.2" xref="S3.SS1.p2.1.m1.6.6.2.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.6.6.1" xref="S3.SS1.p2.1.m1.6.6.1.cmml">→</mo></mover><mo rspace="0.108em" stretchy="false" id="S3.SS1.p2.1.m1.6.7.2.3.2.4" xref="S3.SS1.p2.1.m1.6.7.2.3.1.cmml">)</mo></mrow></mrow><mo rspace="0.108em" id="S3.SS1.p2.1.m1.6.7.1" xref="S3.SS1.p2.1.m1.6.7.1.cmml">:</mo><mrow id="S3.SS1.p2.1.m1.6.7.3" xref="S3.SS1.p2.1.m1.6.7.3.cmml"><msup id="S3.SS1.p2.1.m1.6.7.3.2" xref="S3.SS1.p2.1.m1.6.7.3.2.cmml"><mi id="S3.SS1.p2.1.m1.6.7.3.2.2" xref="S3.SS1.p2.1.m1.6.7.3.2.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.1.m1.3.3.3" xref="S3.SS1.p2.1.m1.3.3.3.cmml"><mrow id="S3.SS1.p2.1.m1.3.3.3.5.2" xref="S3.SS1.p2.1.m1.3.3.3.5.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.3.3.3.5.2.1" xref="S3.SS1.p2.1.m1.3.3.3.5.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.1.m1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.cmml">→</mo></mover><mo rspace="0.055em" stretchy="false" id="S3.SS1.p2.1.m1.3.3.3.5.2.2" xref="S3.SS1.p2.1.m1.3.3.3.5.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.SS1.p2.1.m1.3.3.3.4" xref="S3.SS1.p2.1.m1.3.3.3.4.cmml">×</mo><mrow id="S3.SS1.p2.1.m1.3.3.3.6.2" xref="S3.SS1.p2.1.m1.3.3.3.6.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.3.3.3.6.2.1" xref="S3.SS1.p2.1.m1.3.3.3.6.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.1.m1.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.p2.1.m1.2.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.2.2.2.2.1" xref="S3.SS1.p2.1.m1.2.2.2.2.1.cmml">→</mo></mover><mo rspace="0.055em" stretchy="false" id="S3.SS1.p2.1.m1.3.3.3.6.2.2" xref="S3.SS1.p2.1.m1.3.3.3.6.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.SS1.p2.1.m1.3.3.3.4a" xref="S3.SS1.p2.1.m1.3.3.3.4.cmml">×</mo><mrow id="S3.SS1.p2.1.m1.3.3.3.7.2" xref="S3.SS1.p2.1.m1.3.3.3.7.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.3.3.3.7.2.1" xref="S3.SS1.p2.1.m1.3.3.3.7.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.1.m1.3.3.3.3" xref="S3.SS1.p2.1.m1.3.3.3.3.cmml"><mi id="S3.SS1.p2.1.m1.3.3.3.3.2" xref="S3.SS1.p2.1.m1.3.3.3.3.2.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.3.3.3.3.1" xref="S3.SS1.p2.1.m1.3.3.3.3.1.cmml">→</mo></mover><mo stretchy="false" id="S3.SS1.p2.1.m1.3.3.3.7.2.2" xref="S3.SS1.p2.1.m1.3.3.3.7.1.1.cmml">|</mo></mrow></mrow></msup><mo rspace="0.108em" stretchy="false" id="S3.SS1.p2.1.m1.6.7.3.1" xref="S3.SS1.p2.1.m1.6.7.3.1.cmml">→</mo><msup id="S3.SS1.p2.1.m1.6.7.3.3" xref="S3.SS1.p2.1.m1.6.7.3.3.cmml"><mi id="S3.SS1.p2.1.m1.6.7.3.3.2" xref="S3.SS1.p2.1.m1.6.7.3.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.1.m1.6.7.3.3.3" xref="S3.SS1.p2.1.m1.6.7.3.3.3.cmml"><mi id="S3.SS1.p2.1.m1.6.7.3.3.3.2" xref="S3.SS1.p2.1.m1.6.7.3.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.1.m1.6.7.3.3.3.1" xref="S3.SS1.p2.1.m1.6.7.3.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.1.m1.6.7.3.3.3.3" xref="S3.SS1.p2.1.m1.6.7.3.3.3.3.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.6b"><apply id="S3.SS1.p2.1.m1.6.7.cmml" xref="S3.SS1.p2.1.m1.6.7"><ci id="S3.SS1.p2.1.m1.6.7.1.cmml" xref="S3.SS1.p2.1.m1.6.7.1">:</ci><apply id="S3.SS1.p2.1.m1.6.7.2.cmml" xref="S3.SS1.p2.1.m1.6.7.2"><times id="S3.SS1.p2.1.m1.6.7.2.1.cmml" xref="S3.SS1.p2.1.m1.6.7.2.1"></times><ci id="S3.SS1.p2.1.m1.6.7.2.2.cmml" xref="S3.SS1.p2.1.m1.6.7.2.2">ℳ</ci><vector id="S3.SS1.p2.1.m1.6.7.2.3.1.cmml" xref="S3.SS1.p2.1.m1.6.7.2.3.2"><apply id="S3.SS1.p2.1.m1.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4"><ci id="S3.SS1.p2.1.m1.4.4.1.cmml" xref="S3.SS1.p2.1.m1.4.4.1">→</ci><ci id="S3.SS1.p2.1.m1.4.4.2.cmml" xref="S3.SS1.p2.1.m1.4.4.2">𝛽</ci></apply><apply id="S3.SS1.p2.1.m1.5.5.cmml" xref="S3.SS1.p2.1.m1.5.5"><ci id="S3.SS1.p2.1.m1.5.5.1.cmml" xref="S3.SS1.p2.1.m1.5.5.1">→</ci><ci id="S3.SS1.p2.1.m1.5.5.2.cmml" xref="S3.SS1.p2.1.m1.5.5.2">𝜓</ci></apply><apply id="S3.SS1.p2.1.m1.6.6.cmml" xref="S3.SS1.p2.1.m1.6.6"><ci id="S3.SS1.p2.1.m1.6.6.1.cmml" xref="S3.SS1.p2.1.m1.6.6.1">→</ci><ci id="S3.SS1.p2.1.m1.6.6.2.cmml" xref="S3.SS1.p2.1.m1.6.6.2">𝜃</ci></apply></vector></apply><apply id="S3.SS1.p2.1.m1.6.7.3.cmml" xref="S3.SS1.p2.1.m1.6.7.3"><ci id="S3.SS1.p2.1.m1.6.7.3.1.cmml" xref="S3.SS1.p2.1.m1.6.7.3.1">→</ci><apply id="S3.SS1.p2.1.m1.6.7.3.2.cmml" xref="S3.SS1.p2.1.m1.6.7.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.6.7.3.2.1.cmml" xref="S3.SS1.p2.1.m1.6.7.3.2">superscript</csymbol><ci id="S3.SS1.p2.1.m1.6.7.3.2.2.cmml" xref="S3.SS1.p2.1.m1.6.7.3.2.2">ℝ</ci><apply id="S3.SS1.p2.1.m1.3.3.3.cmml" xref="S3.SS1.p2.1.m1.3.3.3"><times id="S3.SS1.p2.1.m1.3.3.3.4.cmml" xref="S3.SS1.p2.1.m1.3.3.3.4"></times><apply id="S3.SS1.p2.1.m1.3.3.3.5.1.cmml" xref="S3.SS1.p2.1.m1.3.3.3.5.2"><abs id="S3.SS1.p2.1.m1.3.3.3.5.1.1.cmml" xref="S3.SS1.p2.1.m1.3.3.3.5.2.1"></abs><apply id="S3.SS1.p2.1.m1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1"><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1">→</ci><ci id="S3.SS1.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.2">𝛽</ci></apply></apply><apply id="S3.SS1.p2.1.m1.3.3.3.6.1.cmml" xref="S3.SS1.p2.1.m1.3.3.3.6.2"><abs id="S3.SS1.p2.1.m1.3.3.3.6.1.1.cmml" xref="S3.SS1.p2.1.m1.3.3.3.6.2.1"></abs><apply id="S3.SS1.p2.1.m1.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2"><ci id="S3.SS1.p2.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.1">→</ci><ci id="S3.SS1.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2">𝜓</ci></apply></apply><apply id="S3.SS1.p2.1.m1.3.3.3.7.1.cmml" xref="S3.SS1.p2.1.m1.3.3.3.7.2"><abs id="S3.SS1.p2.1.m1.3.3.3.7.1.1.cmml" xref="S3.SS1.p2.1.m1.3.3.3.7.2.1"></abs><apply id="S3.SS1.p2.1.m1.3.3.3.3.cmml" xref="S3.SS1.p2.1.m1.3.3.3.3"><ci id="S3.SS1.p2.1.m1.3.3.3.3.1.cmml" xref="S3.SS1.p2.1.m1.3.3.3.3.1">→</ci><ci id="S3.SS1.p2.1.m1.3.3.3.3.2.cmml" xref="S3.SS1.p2.1.m1.3.3.3.3.2">𝜃</ci></apply></apply></apply></apply><apply id="S3.SS1.p2.1.m1.6.7.3.3.cmml" xref="S3.SS1.p2.1.m1.6.7.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.6.7.3.3.1.cmml" xref="S3.SS1.p2.1.m1.6.7.3.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.6.7.3.3.2.cmml" xref="S3.SS1.p2.1.m1.6.7.3.3.2">ℝ</ci><apply id="S3.SS1.p2.1.m1.6.7.3.3.3.cmml" xref="S3.SS1.p2.1.m1.6.7.3.3.3"><times id="S3.SS1.p2.1.m1.6.7.3.3.3.1.cmml" xref="S3.SS1.p2.1.m1.6.7.3.3.3.1"></times><ci id="S3.SS1.p2.1.m1.6.7.3.3.3.2.cmml" xref="S3.SS1.p2.1.m1.6.7.3.3.3.2">𝑁</ci><cn type="integer" id="S3.SS1.p2.1.m1.6.7.3.3.3.3.cmml" xref="S3.SS1.p2.1.m1.6.7.3.3.3.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.6c">\mathcal{M}(\vec{\beta},\vec{\psi},\vec{\theta})\!:\!\mathbb{R}^{|\vec{\beta}|\times|\vec{\psi}|\times|\vec{\theta}|}\!\to\!\mathbb{R}^{N\times 3}</annotation></semantics></math> which takes parameters
<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\vec{\beta}\in\mathbb{R}^{|\vec{\beta}|}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.2" xref="S3.SS1.p2.2.m2.1.2.cmml"><mover accent="true" id="S3.SS1.p2.2.m2.1.2.2" xref="S3.SS1.p2.2.m2.1.2.2.cmml"><mi id="S3.SS1.p2.2.m2.1.2.2.2" xref="S3.SS1.p2.2.m2.1.2.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.2.m2.1.2.2.1" xref="S3.SS1.p2.2.m2.1.2.2.1.cmml">→</mo></mover><mo id="S3.SS1.p2.2.m2.1.2.1" xref="S3.SS1.p2.2.m2.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.2.m2.1.2.3" xref="S3.SS1.p2.2.m2.1.2.3.cmml"><mi id="S3.SS1.p2.2.m2.1.2.3.2" xref="S3.SS1.p2.2.m2.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.2.m2.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.3.1" xref="S3.SS1.p2.2.m2.1.1.1.2.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.2.m2.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml">→</mo></mover><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.1.2.1.cmml">|</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.2.cmml" xref="S3.SS1.p2.2.m2.1.2"><in id="S3.SS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.1"></in><apply id="S3.SS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2"><ci id="S3.SS1.p2.2.m2.1.2.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.2.1">→</ci><ci id="S3.SS1.p2.2.m2.1.2.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2">𝛽</ci></apply><apply id="S3.SS1.p2.2.m2.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.2.3.1.cmml" xref="S3.SS1.p2.2.m2.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.2.3.2.cmml" xref="S3.SS1.p2.2.m2.1.2.3.2">ℝ</ci><apply id="S3.SS1.p2.2.m2.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.3"><abs id="S3.SS1.p2.2.m2.1.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.3.1"></abs><apply id="S3.SS1.p2.2.m2.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1"><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1">→</ci><ci id="S3.SS1.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.2">𝛽</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\vec{\beta}\in\mathbb{R}^{|\vec{\beta}|}</annotation></semantics></math> for identity,
<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\vec{\psi}\in\mathbb{R}^{|\vec{\psi}|}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.2" xref="S3.SS1.p2.3.m3.1.2.cmml"><mover accent="true" id="S3.SS1.p2.3.m3.1.2.2" xref="S3.SS1.p2.3.m3.1.2.2.cmml"><mi id="S3.SS1.p2.3.m3.1.2.2.2" xref="S3.SS1.p2.3.m3.1.2.2.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS1.p2.3.m3.1.2.2.1" xref="S3.SS1.p2.3.m3.1.2.2.1.cmml">→</mo></mover><mo id="S3.SS1.p2.3.m3.1.2.1" xref="S3.SS1.p2.3.m3.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.3.m3.1.2.3" xref="S3.SS1.p2.3.m3.1.2.3.cmml"><mi id="S3.SS1.p2.3.m3.1.2.3.2" xref="S3.SS1.p2.3.m3.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.3.m3.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.3.1" xref="S3.SS1.p2.3.m3.1.1.1.2.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.3.m3.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml">→</mo></mover><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.1.2.1.cmml">|</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.2.cmml" xref="S3.SS1.p2.3.m3.1.2"><in id="S3.SS1.p2.3.m3.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.2.1"></in><apply id="S3.SS1.p2.3.m3.1.2.2.cmml" xref="S3.SS1.p2.3.m3.1.2.2"><ci id="S3.SS1.p2.3.m3.1.2.2.1.cmml" xref="S3.SS1.p2.3.m3.1.2.2.1">→</ci><ci id="S3.SS1.p2.3.m3.1.2.2.2.cmml" xref="S3.SS1.p2.3.m3.1.2.2.2">𝜓</ci></apply><apply id="S3.SS1.p2.3.m3.1.2.3.cmml" xref="S3.SS1.p2.3.m3.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.2.3.1.cmml" xref="S3.SS1.p2.3.m3.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.3.m3.1.2.3.2.cmml" xref="S3.SS1.p2.3.m3.1.2.3.2">ℝ</ci><apply id="S3.SS1.p2.3.m3.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.3"><abs id="S3.SS1.p2.3.m3.1.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.3.1"></abs><apply id="S3.SS1.p2.3.m3.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1"><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1">→</ci><ci id="S3.SS1.p2.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.2">𝜓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\vec{\psi}\in\mathbb{R}^{|\vec{\psi}|}</annotation></semantics></math> for expression, and
<math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\vec{\theta}\in\mathbb{R}^{K\times 3}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mover accent="true" id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2.2" xref="S3.SS1.p2.4.m4.1.1.2.2.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.4.m4.1.1.2.1" xref="S3.SS1.p2.4.m4.1.1.2.1.cmml">→</mo></mover><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><in id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></in><apply id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"><ci id="S3.SS1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.2.1">→</ci><ci id="S3.SS1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2">𝜃</ci></apply><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">𝐾</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\vec{\theta}\in\mathbb{R}^{K\times 3}</annotation></semantics></math> for skeletal pose.
The pose parameters <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\vec{\theta}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mover accent="true" id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.5.m5.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><ci id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1">→</ci><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\vec{\theta}</annotation></semantics></math> are per-joint local rotations represented as Euler angles. <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathcal{M}</annotation></semantics></math> is defined as</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.10" class="ltx_Math" alttext="\mathcal{M}(\vec{\beta},\vec{\psi},\vec{\theta})=\mathcal{L}(\mathcal{T}(\vec{\beta},\vec{\psi}),\vec{\theta},\mathcal{J}(\vec{\beta});\mathbf{W})" display="block"><semantics id="S3.Ex1.m1.10a"><mrow id="S3.Ex1.m1.10.10" xref="S3.Ex1.m1.10.10.cmml"><mrow id="S3.Ex1.m1.10.10.4" xref="S3.Ex1.m1.10.10.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.10.10.4.2" xref="S3.Ex1.m1.10.10.4.2.cmml">ℳ</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.10.10.4.1" xref="S3.Ex1.m1.10.10.4.1.cmml">​</mo><mrow id="S3.Ex1.m1.10.10.4.3.2" xref="S3.Ex1.m1.10.10.4.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.10.10.4.3.2.1" xref="S3.Ex1.m1.10.10.4.3.1.cmml">(</mo><mover accent="true" id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml">→</mo></mover><mo id="S3.Ex1.m1.10.10.4.3.2.2" xref="S3.Ex1.m1.10.10.4.3.1.cmml">,</mo><mover accent="true" id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2" xref="S3.Ex1.m1.2.2.2.cmml">ψ</mi><mo stretchy="false" id="S3.Ex1.m1.2.2.1" xref="S3.Ex1.m1.2.2.1.cmml">→</mo></mover><mo id="S3.Ex1.m1.10.10.4.3.2.3" xref="S3.Ex1.m1.10.10.4.3.1.cmml">,</mo><mover accent="true" id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml"><mi id="S3.Ex1.m1.3.3.2" xref="S3.Ex1.m1.3.3.2.cmml">θ</mi><mo stretchy="false" id="S3.Ex1.m1.3.3.1" xref="S3.Ex1.m1.3.3.1.cmml">→</mo></mover><mo stretchy="false" id="S3.Ex1.m1.10.10.4.3.2.4" xref="S3.Ex1.m1.10.10.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.10.10.3" xref="S3.Ex1.m1.10.10.3.cmml">=</mo><mrow id="S3.Ex1.m1.10.10.2" xref="S3.Ex1.m1.10.10.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.10.10.2.4" xref="S3.Ex1.m1.10.10.2.4.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.10.10.2.3" xref="S3.Ex1.m1.10.10.2.3.cmml">​</mo><mrow id="S3.Ex1.m1.10.10.2.2.2" xref="S3.Ex1.m1.10.10.2.2.3.cmml"><mo stretchy="false" id="S3.Ex1.m1.10.10.2.2.2.3" xref="S3.Ex1.m1.10.10.2.2.3.cmml">(</mo><mrow id="S3.Ex1.m1.9.9.1.1.1.1" xref="S3.Ex1.m1.9.9.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.9.9.1.1.1.1.2" xref="S3.Ex1.m1.9.9.1.1.1.1.2.cmml">𝒯</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.9.9.1.1.1.1.1" xref="S3.Ex1.m1.9.9.1.1.1.1.1.cmml">​</mo><mrow id="S3.Ex1.m1.9.9.1.1.1.1.3.2" xref="S3.Ex1.m1.9.9.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.9.9.1.1.1.1.3.2.1" xref="S3.Ex1.m1.9.9.1.1.1.1.3.1.cmml">(</mo><mover accent="true" id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml"><mi id="S3.Ex1.m1.4.4.2" xref="S3.Ex1.m1.4.4.2.cmml">β</mi><mo stretchy="false" id="S3.Ex1.m1.4.4.1" xref="S3.Ex1.m1.4.4.1.cmml">→</mo></mover><mo id="S3.Ex1.m1.9.9.1.1.1.1.3.2.2" xref="S3.Ex1.m1.9.9.1.1.1.1.3.1.cmml">,</mo><mover accent="true" id="S3.Ex1.m1.5.5" xref="S3.Ex1.m1.5.5.cmml"><mi id="S3.Ex1.m1.5.5.2" xref="S3.Ex1.m1.5.5.2.cmml">ψ</mi><mo stretchy="false" id="S3.Ex1.m1.5.5.1" xref="S3.Ex1.m1.5.5.1.cmml">→</mo></mover><mo stretchy="false" id="S3.Ex1.m1.9.9.1.1.1.1.3.2.3" xref="S3.Ex1.m1.9.9.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.10.10.2.2.2.4" xref="S3.Ex1.m1.10.10.2.2.3.cmml">,</mo><mover accent="true" id="S3.Ex1.m1.7.7" xref="S3.Ex1.m1.7.7.cmml"><mi id="S3.Ex1.m1.7.7.2" xref="S3.Ex1.m1.7.7.2.cmml">θ</mi><mo stretchy="false" id="S3.Ex1.m1.7.7.1" xref="S3.Ex1.m1.7.7.1.cmml">→</mo></mover><mo id="S3.Ex1.m1.10.10.2.2.2.5" xref="S3.Ex1.m1.10.10.2.2.3.cmml">,</mo><mrow id="S3.Ex1.m1.10.10.2.2.2.2" xref="S3.Ex1.m1.10.10.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.10.10.2.2.2.2.2" xref="S3.Ex1.m1.10.10.2.2.2.2.2.cmml">𝒥</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.10.10.2.2.2.2.1" xref="S3.Ex1.m1.10.10.2.2.2.2.1.cmml">​</mo><mrow id="S3.Ex1.m1.10.10.2.2.2.2.3.2" xref="S3.Ex1.m1.6.6.cmml"><mo stretchy="false" id="S3.Ex1.m1.10.10.2.2.2.2.3.2.1" xref="S3.Ex1.m1.6.6.cmml">(</mo><mover accent="true" id="S3.Ex1.m1.6.6" xref="S3.Ex1.m1.6.6.cmml"><mi id="S3.Ex1.m1.6.6.2" xref="S3.Ex1.m1.6.6.2.cmml">β</mi><mo stretchy="false" id="S3.Ex1.m1.6.6.1" xref="S3.Ex1.m1.6.6.1.cmml">→</mo></mover><mo stretchy="false" id="S3.Ex1.m1.10.10.2.2.2.2.3.2.2" xref="S3.Ex1.m1.6.6.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.10.10.2.2.2.6" xref="S3.Ex1.m1.10.10.2.2.3.cmml">;</mo><mi id="S3.Ex1.m1.8.8" xref="S3.Ex1.m1.8.8.cmml">𝐖</mi><mo stretchy="false" id="S3.Ex1.m1.10.10.2.2.2.7" xref="S3.Ex1.m1.10.10.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.10b"><apply id="S3.Ex1.m1.10.10.cmml" xref="S3.Ex1.m1.10.10"><eq id="S3.Ex1.m1.10.10.3.cmml" xref="S3.Ex1.m1.10.10.3"></eq><apply id="S3.Ex1.m1.10.10.4.cmml" xref="S3.Ex1.m1.10.10.4"><times id="S3.Ex1.m1.10.10.4.1.cmml" xref="S3.Ex1.m1.10.10.4.1"></times><ci id="S3.Ex1.m1.10.10.4.2.cmml" xref="S3.Ex1.m1.10.10.4.2">ℳ</ci><vector id="S3.Ex1.m1.10.10.4.3.1.cmml" xref="S3.Ex1.m1.10.10.4.3.2"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><ci id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1">→</ci><ci id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2">𝛽</ci></apply><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><ci id="S3.Ex1.m1.2.2.1.cmml" xref="S3.Ex1.m1.2.2.1">→</ci><ci id="S3.Ex1.m1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2">𝜓</ci></apply><apply id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3"><ci id="S3.Ex1.m1.3.3.1.cmml" xref="S3.Ex1.m1.3.3.1">→</ci><ci id="S3.Ex1.m1.3.3.2.cmml" xref="S3.Ex1.m1.3.3.2">𝜃</ci></apply></vector></apply><apply id="S3.Ex1.m1.10.10.2.cmml" xref="S3.Ex1.m1.10.10.2"><times id="S3.Ex1.m1.10.10.2.3.cmml" xref="S3.Ex1.m1.10.10.2.3"></times><ci id="S3.Ex1.m1.10.10.2.4.cmml" xref="S3.Ex1.m1.10.10.2.4">ℒ</ci><vector id="S3.Ex1.m1.10.10.2.2.3.cmml" xref="S3.Ex1.m1.10.10.2.2.2"><apply id="S3.Ex1.m1.9.9.1.1.1.1.cmml" xref="S3.Ex1.m1.9.9.1.1.1.1"><times id="S3.Ex1.m1.9.9.1.1.1.1.1.cmml" xref="S3.Ex1.m1.9.9.1.1.1.1.1"></times><ci id="S3.Ex1.m1.9.9.1.1.1.1.2.cmml" xref="S3.Ex1.m1.9.9.1.1.1.1.2">𝒯</ci><interval closure="open" id="S3.Ex1.m1.9.9.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.9.9.1.1.1.1.3.2"><apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4"><ci id="S3.Ex1.m1.4.4.1.cmml" xref="S3.Ex1.m1.4.4.1">→</ci><ci id="S3.Ex1.m1.4.4.2.cmml" xref="S3.Ex1.m1.4.4.2">𝛽</ci></apply><apply id="S3.Ex1.m1.5.5.cmml" xref="S3.Ex1.m1.5.5"><ci id="S3.Ex1.m1.5.5.1.cmml" xref="S3.Ex1.m1.5.5.1">→</ci><ci id="S3.Ex1.m1.5.5.2.cmml" xref="S3.Ex1.m1.5.5.2">𝜓</ci></apply></interval></apply><apply id="S3.Ex1.m1.7.7.cmml" xref="S3.Ex1.m1.7.7"><ci id="S3.Ex1.m1.7.7.1.cmml" xref="S3.Ex1.m1.7.7.1">→</ci><ci id="S3.Ex1.m1.7.7.2.cmml" xref="S3.Ex1.m1.7.7.2">𝜃</ci></apply><apply id="S3.Ex1.m1.10.10.2.2.2.2.cmml" xref="S3.Ex1.m1.10.10.2.2.2.2"><times id="S3.Ex1.m1.10.10.2.2.2.2.1.cmml" xref="S3.Ex1.m1.10.10.2.2.2.2.1"></times><ci id="S3.Ex1.m1.10.10.2.2.2.2.2.cmml" xref="S3.Ex1.m1.10.10.2.2.2.2.2">𝒥</ci><apply id="S3.Ex1.m1.6.6.cmml" xref="S3.Ex1.m1.10.10.2.2.2.2.3.2"><ci id="S3.Ex1.m1.6.6.1.cmml" xref="S3.Ex1.m1.6.6.1">→</ci><ci id="S3.Ex1.m1.6.6.2.cmml" xref="S3.Ex1.m1.6.6.2">𝛽</ci></apply></apply><ci id="S3.Ex1.m1.8.8.cmml" xref="S3.Ex1.m1.8.8">𝐖</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.10c">\mathcal{M}(\vec{\beta},\vec{\psi},\vec{\theta})=\mathcal{L}(\mathcal{T}(\vec{\beta},\vec{\psi}),\vec{\theta},\mathcal{J}(\vec{\beta});\mathbf{W})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.13" class="ltx_p">where <math id="S3.SS1.p2.7.m1.4" class="ltx_Math" alttext="\mathcal{L}(\mathbf{X},\vec{\theta},\mathbf{J};\mathbf{W})" display="inline"><semantics id="S3.SS1.p2.7.m1.4a"><mrow id="S3.SS1.p2.7.m1.4.5" xref="S3.SS1.p2.7.m1.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m1.4.5.2" xref="S3.SS1.p2.7.m1.4.5.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.7.m1.4.5.1" xref="S3.SS1.p2.7.m1.4.5.1.cmml">​</mo><mrow id="S3.SS1.p2.7.m1.4.5.3.2" xref="S3.SS1.p2.7.m1.4.5.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.7.m1.4.5.3.2.1" xref="S3.SS1.p2.7.m1.4.5.3.1.cmml">(</mo><mi id="S3.SS1.p2.7.m1.1.1" xref="S3.SS1.p2.7.m1.1.1.cmml">𝐗</mi><mo id="S3.SS1.p2.7.m1.4.5.3.2.2" xref="S3.SS1.p2.7.m1.4.5.3.1.cmml">,</mo><mover accent="true" id="S3.SS1.p2.7.m1.2.2" xref="S3.SS1.p2.7.m1.2.2.cmml"><mi id="S3.SS1.p2.7.m1.2.2.2" xref="S3.SS1.p2.7.m1.2.2.2.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.7.m1.2.2.1" xref="S3.SS1.p2.7.m1.2.2.1.cmml">→</mo></mover><mo id="S3.SS1.p2.7.m1.4.5.3.2.3" xref="S3.SS1.p2.7.m1.4.5.3.1.cmml">,</mo><mi id="S3.SS1.p2.7.m1.3.3" xref="S3.SS1.p2.7.m1.3.3.cmml">𝐉</mi><mo id="S3.SS1.p2.7.m1.4.5.3.2.4" xref="S3.SS1.p2.7.m1.4.5.3.1.cmml">;</mo><mi id="S3.SS1.p2.7.m1.4.4" xref="S3.SS1.p2.7.m1.4.4.cmml">𝐖</mi><mo stretchy="false" id="S3.SS1.p2.7.m1.4.5.3.2.5" xref="S3.SS1.p2.7.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m1.4b"><apply id="S3.SS1.p2.7.m1.4.5.cmml" xref="S3.SS1.p2.7.m1.4.5"><times id="S3.SS1.p2.7.m1.4.5.1.cmml" xref="S3.SS1.p2.7.m1.4.5.1"></times><ci id="S3.SS1.p2.7.m1.4.5.2.cmml" xref="S3.SS1.p2.7.m1.4.5.2">ℒ</ci><vector id="S3.SS1.p2.7.m1.4.5.3.1.cmml" xref="S3.SS1.p2.7.m1.4.5.3.2"><ci id="S3.SS1.p2.7.m1.1.1.cmml" xref="S3.SS1.p2.7.m1.1.1">𝐗</ci><apply id="S3.SS1.p2.7.m1.2.2.cmml" xref="S3.SS1.p2.7.m1.2.2"><ci id="S3.SS1.p2.7.m1.2.2.1.cmml" xref="S3.SS1.p2.7.m1.2.2.1">→</ci><ci id="S3.SS1.p2.7.m1.2.2.2.cmml" xref="S3.SS1.p2.7.m1.2.2.2">𝜃</ci></apply><ci id="S3.SS1.p2.7.m1.3.3.cmml" xref="S3.SS1.p2.7.m1.3.3">𝐉</ci><ci id="S3.SS1.p2.7.m1.4.4.cmml" xref="S3.SS1.p2.7.m1.4.4">𝐖</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m1.4c">\mathcal{L}(\mathbf{X},\vec{\theta},\mathbf{J};\mathbf{W})</annotation></semantics></math> is a standard linear blend skinning (LBS) function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> that rotates vertex positions <math id="S3.SS1.p2.8.m2.1" class="ltx_Math" alttext="\mathbf{X}\in\mathbb{R}^{N\times 3}" display="inline"><semantics id="S3.SS1.p2.8.m2.1a"><mrow id="S3.SS1.p2.8.m2.1.1" xref="S3.SS1.p2.8.m2.1.1.cmml"><mi id="S3.SS1.p2.8.m2.1.1.2" xref="S3.SS1.p2.8.m2.1.1.2.cmml">𝐗</mi><mo id="S3.SS1.p2.8.m2.1.1.1" xref="S3.SS1.p2.8.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.8.m2.1.1.3" xref="S3.SS1.p2.8.m2.1.1.3.cmml"><mi id="S3.SS1.p2.8.m2.1.1.3.2" xref="S3.SS1.p2.8.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.8.m2.1.1.3.3" xref="S3.SS1.p2.8.m2.1.1.3.3.cmml"><mi id="S3.SS1.p2.8.m2.1.1.3.3.2" xref="S3.SS1.p2.8.m2.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.8.m2.1.1.3.3.1" xref="S3.SS1.p2.8.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.8.m2.1.1.3.3.3" xref="S3.SS1.p2.8.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m2.1b"><apply id="S3.SS1.p2.8.m2.1.1.cmml" xref="S3.SS1.p2.8.m2.1.1"><in id="S3.SS1.p2.8.m2.1.1.1.cmml" xref="S3.SS1.p2.8.m2.1.1.1"></in><ci id="S3.SS1.p2.8.m2.1.1.2.cmml" xref="S3.SS1.p2.8.m2.1.1.2">𝐗</ci><apply id="S3.SS1.p2.8.m2.1.1.3.cmml" xref="S3.SS1.p2.8.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m2.1.1.3.1.cmml" xref="S3.SS1.p2.8.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.8.m2.1.1.3.2.cmml" xref="S3.SS1.p2.8.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.8.m2.1.1.3.3.cmml" xref="S3.SS1.p2.8.m2.1.1.3.3"><times id="S3.SS1.p2.8.m2.1.1.3.3.1.cmml" xref="S3.SS1.p2.8.m2.1.1.3.3.1"></times><ci id="S3.SS1.p2.8.m2.1.1.3.3.2.cmml" xref="S3.SS1.p2.8.m2.1.1.3.3.2">𝑁</ci><cn type="integer" id="S3.SS1.p2.8.m2.1.1.3.3.3.cmml" xref="S3.SS1.p2.8.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m2.1c">\mathbf{X}\in\mathbb{R}^{N\times 3}</annotation></semantics></math> about joint locations <math id="S3.SS1.p2.9.m3.1" class="ltx_Math" alttext="\mathbf{J}\in\mathbb{R}^{K\times 3}" display="inline"><semantics id="S3.SS1.p2.9.m3.1a"><mrow id="S3.SS1.p2.9.m3.1.1" xref="S3.SS1.p2.9.m3.1.1.cmml"><mi id="S3.SS1.p2.9.m3.1.1.2" xref="S3.SS1.p2.9.m3.1.1.2.cmml">𝐉</mi><mo id="S3.SS1.p2.9.m3.1.1.1" xref="S3.SS1.p2.9.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.9.m3.1.1.3" xref="S3.SS1.p2.9.m3.1.1.3.cmml"><mi id="S3.SS1.p2.9.m3.1.1.3.2" xref="S3.SS1.p2.9.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.9.m3.1.1.3.3" xref="S3.SS1.p2.9.m3.1.1.3.3.cmml"><mi id="S3.SS1.p2.9.m3.1.1.3.3.2" xref="S3.SS1.p2.9.m3.1.1.3.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.9.m3.1.1.3.3.1" xref="S3.SS1.p2.9.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.9.m3.1.1.3.3.3" xref="S3.SS1.p2.9.m3.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m3.1b"><apply id="S3.SS1.p2.9.m3.1.1.cmml" xref="S3.SS1.p2.9.m3.1.1"><in id="S3.SS1.p2.9.m3.1.1.1.cmml" xref="S3.SS1.p2.9.m3.1.1.1"></in><ci id="S3.SS1.p2.9.m3.1.1.2.cmml" xref="S3.SS1.p2.9.m3.1.1.2">𝐉</ci><apply id="S3.SS1.p2.9.m3.1.1.3.cmml" xref="S3.SS1.p2.9.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m3.1.1.3.1.cmml" xref="S3.SS1.p2.9.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.9.m3.1.1.3.2.cmml" xref="S3.SS1.p2.9.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.9.m3.1.1.3.3.cmml" xref="S3.SS1.p2.9.m3.1.1.3.3"><times id="S3.SS1.p2.9.m3.1.1.3.3.1.cmml" xref="S3.SS1.p2.9.m3.1.1.3.3.1"></times><ci id="S3.SS1.p2.9.m3.1.1.3.3.2.cmml" xref="S3.SS1.p2.9.m3.1.1.3.3.2">𝐾</ci><cn type="integer" id="S3.SS1.p2.9.m3.1.1.3.3.3.cmml" xref="S3.SS1.p2.9.m3.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m3.1c">\mathbf{J}\in\mathbb{R}^{K\times 3}</annotation></semantics></math> by local joint rotations <math id="S3.SS1.p2.10.m4.1" class="ltx_Math" alttext="\vec{\theta}" display="inline"><semantics id="S3.SS1.p2.10.m4.1a"><mover accent="true" id="S3.SS1.p2.10.m4.1.1" xref="S3.SS1.p2.10.m4.1.1.cmml"><mi id="S3.SS1.p2.10.m4.1.1.2" xref="S3.SS1.p2.10.m4.1.1.2.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.10.m4.1.1.1" xref="S3.SS1.p2.10.m4.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m4.1b"><apply id="S3.SS1.p2.10.m4.1.1.cmml" xref="S3.SS1.p2.10.m4.1.1"><ci id="S3.SS1.p2.10.m4.1.1.1.cmml" xref="S3.SS1.p2.10.m4.1.1.1">→</ci><ci id="S3.SS1.p2.10.m4.1.1.2.cmml" xref="S3.SS1.p2.10.m4.1.1.2">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m4.1c">\vec{\theta}</annotation></semantics></math>, with per-vertex weights <math id="S3.SS1.p2.11.m5.1" class="ltx_Math" alttext="\mathbf{W}\in\mathbb{R}^{K\times N}" display="inline"><semantics id="S3.SS1.p2.11.m5.1a"><mrow id="S3.SS1.p2.11.m5.1.1" xref="S3.SS1.p2.11.m5.1.1.cmml"><mi id="S3.SS1.p2.11.m5.1.1.2" xref="S3.SS1.p2.11.m5.1.1.2.cmml">𝐖</mi><mo id="S3.SS1.p2.11.m5.1.1.1" xref="S3.SS1.p2.11.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.11.m5.1.1.3" xref="S3.SS1.p2.11.m5.1.1.3.cmml"><mi id="S3.SS1.p2.11.m5.1.1.3.2" xref="S3.SS1.p2.11.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.11.m5.1.1.3.3" xref="S3.SS1.p2.11.m5.1.1.3.3.cmml"><mi id="S3.SS1.p2.11.m5.1.1.3.3.2" xref="S3.SS1.p2.11.m5.1.1.3.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.11.m5.1.1.3.3.1" xref="S3.SS1.p2.11.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.11.m5.1.1.3.3.3" xref="S3.SS1.p2.11.m5.1.1.3.3.3.cmml">N</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m5.1b"><apply id="S3.SS1.p2.11.m5.1.1.cmml" xref="S3.SS1.p2.11.m5.1.1"><in id="S3.SS1.p2.11.m5.1.1.1.cmml" xref="S3.SS1.p2.11.m5.1.1.1"></in><ci id="S3.SS1.p2.11.m5.1.1.2.cmml" xref="S3.SS1.p2.11.m5.1.1.2">𝐖</ci><apply id="S3.SS1.p2.11.m5.1.1.3.cmml" xref="S3.SS1.p2.11.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m5.1.1.3.1.cmml" xref="S3.SS1.p2.11.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.11.m5.1.1.3.2.cmml" xref="S3.SS1.p2.11.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.11.m5.1.1.3.3.cmml" xref="S3.SS1.p2.11.m5.1.1.3.3"><times id="S3.SS1.p2.11.m5.1.1.3.3.1.cmml" xref="S3.SS1.p2.11.m5.1.1.3.3.1"></times><ci id="S3.SS1.p2.11.m5.1.1.3.3.2.cmml" xref="S3.SS1.p2.11.m5.1.1.3.3.2">𝐾</ci><ci id="S3.SS1.p2.11.m5.1.1.3.3.3.cmml" xref="S3.SS1.p2.11.m5.1.1.3.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m5.1c">\mathbf{W}\in\mathbb{R}^{K\times N}</annotation></semantics></math> determining how rotations are interpolated across the mesh.
<math id="S3.SS1.p2.12.m6.4" class="ltx_Math" alttext="\mathcal{T}(\vec{\beta},\vec{\psi})\!:\!\mathbb{R}^{|\vec{\beta}|\times|\vec{\psi}|}\to\mathbb{R}^{N\times 3}" display="inline"><semantics id="S3.SS1.p2.12.m6.4a"><mrow id="S3.SS1.p2.12.m6.4.5" xref="S3.SS1.p2.12.m6.4.5.cmml"><mrow id="S3.SS1.p2.12.m6.4.5.2" xref="S3.SS1.p2.12.m6.4.5.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.12.m6.4.5.2.2" xref="S3.SS1.p2.12.m6.4.5.2.2.cmml">𝒯</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.12.m6.4.5.2.1" xref="S3.SS1.p2.12.m6.4.5.2.1.cmml">​</mo><mrow id="S3.SS1.p2.12.m6.4.5.2.3.2" xref="S3.SS1.p2.12.m6.4.5.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.12.m6.4.5.2.3.2.1" xref="S3.SS1.p2.12.m6.4.5.2.3.1.cmml">(</mo><mover accent="true" id="S3.SS1.p2.12.m6.3.3" xref="S3.SS1.p2.12.m6.3.3.cmml"><mi id="S3.SS1.p2.12.m6.3.3.2" xref="S3.SS1.p2.12.m6.3.3.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.12.m6.3.3.1" xref="S3.SS1.p2.12.m6.3.3.1.cmml">→</mo></mover><mo id="S3.SS1.p2.12.m6.4.5.2.3.2.2" xref="S3.SS1.p2.12.m6.4.5.2.3.1.cmml">,</mo><mover accent="true" id="S3.SS1.p2.12.m6.4.4" xref="S3.SS1.p2.12.m6.4.4.cmml"><mi id="S3.SS1.p2.12.m6.4.4.2" xref="S3.SS1.p2.12.m6.4.4.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS1.p2.12.m6.4.4.1" xref="S3.SS1.p2.12.m6.4.4.1.cmml">→</mo></mover><mo rspace="0.108em" stretchy="false" id="S3.SS1.p2.12.m6.4.5.2.3.2.3" xref="S3.SS1.p2.12.m6.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo rspace="0.108em" id="S3.SS1.p2.12.m6.4.5.1" xref="S3.SS1.p2.12.m6.4.5.1.cmml">:</mo><mrow id="S3.SS1.p2.12.m6.4.5.3" xref="S3.SS1.p2.12.m6.4.5.3.cmml"><msup id="S3.SS1.p2.12.m6.4.5.3.2" xref="S3.SS1.p2.12.m6.4.5.3.2.cmml"><mi id="S3.SS1.p2.12.m6.4.5.3.2.2" xref="S3.SS1.p2.12.m6.4.5.3.2.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.12.m6.2.2.2" xref="S3.SS1.p2.12.m6.2.2.2.cmml"><mrow id="S3.SS1.p2.12.m6.2.2.2.4.2" xref="S3.SS1.p2.12.m6.2.2.2.4.1.cmml"><mo stretchy="false" id="S3.SS1.p2.12.m6.2.2.2.4.2.1" xref="S3.SS1.p2.12.m6.2.2.2.4.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.12.m6.1.1.1.1" xref="S3.SS1.p2.12.m6.1.1.1.1.cmml"><mi id="S3.SS1.p2.12.m6.1.1.1.1.2" xref="S3.SS1.p2.12.m6.1.1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.12.m6.1.1.1.1.1" xref="S3.SS1.p2.12.m6.1.1.1.1.1.cmml">→</mo></mover><mo rspace="0.055em" stretchy="false" id="S3.SS1.p2.12.m6.2.2.2.4.2.2" xref="S3.SS1.p2.12.m6.2.2.2.4.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.SS1.p2.12.m6.2.2.2.3" xref="S3.SS1.p2.12.m6.2.2.2.3.cmml">×</mo><mrow id="S3.SS1.p2.12.m6.2.2.2.5.2" xref="S3.SS1.p2.12.m6.2.2.2.5.1.cmml"><mo stretchy="false" id="S3.SS1.p2.12.m6.2.2.2.5.2.1" xref="S3.SS1.p2.12.m6.2.2.2.5.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.12.m6.2.2.2.2" xref="S3.SS1.p2.12.m6.2.2.2.2.cmml"><mi id="S3.SS1.p2.12.m6.2.2.2.2.2" xref="S3.SS1.p2.12.m6.2.2.2.2.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS1.p2.12.m6.2.2.2.2.1" xref="S3.SS1.p2.12.m6.2.2.2.2.1.cmml">→</mo></mover><mo stretchy="false" id="S3.SS1.p2.12.m6.2.2.2.5.2.2" xref="S3.SS1.p2.12.m6.2.2.2.5.1.1.cmml">|</mo></mrow></mrow></msup><mo stretchy="false" id="S3.SS1.p2.12.m6.4.5.3.1" xref="S3.SS1.p2.12.m6.4.5.3.1.cmml">→</mo><msup id="S3.SS1.p2.12.m6.4.5.3.3" xref="S3.SS1.p2.12.m6.4.5.3.3.cmml"><mi id="S3.SS1.p2.12.m6.4.5.3.3.2" xref="S3.SS1.p2.12.m6.4.5.3.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.12.m6.4.5.3.3.3" xref="S3.SS1.p2.12.m6.4.5.3.3.3.cmml"><mi id="S3.SS1.p2.12.m6.4.5.3.3.3.2" xref="S3.SS1.p2.12.m6.4.5.3.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.12.m6.4.5.3.3.3.1" xref="S3.SS1.p2.12.m6.4.5.3.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.12.m6.4.5.3.3.3.3" xref="S3.SS1.p2.12.m6.4.5.3.3.3.3.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m6.4b"><apply id="S3.SS1.p2.12.m6.4.5.cmml" xref="S3.SS1.p2.12.m6.4.5"><ci id="S3.SS1.p2.12.m6.4.5.1.cmml" xref="S3.SS1.p2.12.m6.4.5.1">:</ci><apply id="S3.SS1.p2.12.m6.4.5.2.cmml" xref="S3.SS1.p2.12.m6.4.5.2"><times id="S3.SS1.p2.12.m6.4.5.2.1.cmml" xref="S3.SS1.p2.12.m6.4.5.2.1"></times><ci id="S3.SS1.p2.12.m6.4.5.2.2.cmml" xref="S3.SS1.p2.12.m6.4.5.2.2">𝒯</ci><interval closure="open" id="S3.SS1.p2.12.m6.4.5.2.3.1.cmml" xref="S3.SS1.p2.12.m6.4.5.2.3.2"><apply id="S3.SS1.p2.12.m6.3.3.cmml" xref="S3.SS1.p2.12.m6.3.3"><ci id="S3.SS1.p2.12.m6.3.3.1.cmml" xref="S3.SS1.p2.12.m6.3.3.1">→</ci><ci id="S3.SS1.p2.12.m6.3.3.2.cmml" xref="S3.SS1.p2.12.m6.3.3.2">𝛽</ci></apply><apply id="S3.SS1.p2.12.m6.4.4.cmml" xref="S3.SS1.p2.12.m6.4.4"><ci id="S3.SS1.p2.12.m6.4.4.1.cmml" xref="S3.SS1.p2.12.m6.4.4.1">→</ci><ci id="S3.SS1.p2.12.m6.4.4.2.cmml" xref="S3.SS1.p2.12.m6.4.4.2">𝜓</ci></apply></interval></apply><apply id="S3.SS1.p2.12.m6.4.5.3.cmml" xref="S3.SS1.p2.12.m6.4.5.3"><ci id="S3.SS1.p2.12.m6.4.5.3.1.cmml" xref="S3.SS1.p2.12.m6.4.5.3.1">→</ci><apply id="S3.SS1.p2.12.m6.4.5.3.2.cmml" xref="S3.SS1.p2.12.m6.4.5.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m6.4.5.3.2.1.cmml" xref="S3.SS1.p2.12.m6.4.5.3.2">superscript</csymbol><ci id="S3.SS1.p2.12.m6.4.5.3.2.2.cmml" xref="S3.SS1.p2.12.m6.4.5.3.2.2">ℝ</ci><apply id="S3.SS1.p2.12.m6.2.2.2.cmml" xref="S3.SS1.p2.12.m6.2.2.2"><times id="S3.SS1.p2.12.m6.2.2.2.3.cmml" xref="S3.SS1.p2.12.m6.2.2.2.3"></times><apply id="S3.SS1.p2.12.m6.2.2.2.4.1.cmml" xref="S3.SS1.p2.12.m6.2.2.2.4.2"><abs id="S3.SS1.p2.12.m6.2.2.2.4.1.1.cmml" xref="S3.SS1.p2.12.m6.2.2.2.4.2.1"></abs><apply id="S3.SS1.p2.12.m6.1.1.1.1.cmml" xref="S3.SS1.p2.12.m6.1.1.1.1"><ci id="S3.SS1.p2.12.m6.1.1.1.1.1.cmml" xref="S3.SS1.p2.12.m6.1.1.1.1.1">→</ci><ci id="S3.SS1.p2.12.m6.1.1.1.1.2.cmml" xref="S3.SS1.p2.12.m6.1.1.1.1.2">𝛽</ci></apply></apply><apply id="S3.SS1.p2.12.m6.2.2.2.5.1.cmml" xref="S3.SS1.p2.12.m6.2.2.2.5.2"><abs id="S3.SS1.p2.12.m6.2.2.2.5.1.1.cmml" xref="S3.SS1.p2.12.m6.2.2.2.5.2.1"></abs><apply id="S3.SS1.p2.12.m6.2.2.2.2.cmml" xref="S3.SS1.p2.12.m6.2.2.2.2"><ci id="S3.SS1.p2.12.m6.2.2.2.2.1.cmml" xref="S3.SS1.p2.12.m6.2.2.2.2.1">→</ci><ci id="S3.SS1.p2.12.m6.2.2.2.2.2.cmml" xref="S3.SS1.p2.12.m6.2.2.2.2.2">𝜓</ci></apply></apply></apply></apply><apply id="S3.SS1.p2.12.m6.4.5.3.3.cmml" xref="S3.SS1.p2.12.m6.4.5.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m6.4.5.3.3.1.cmml" xref="S3.SS1.p2.12.m6.4.5.3.3">superscript</csymbol><ci id="S3.SS1.p2.12.m6.4.5.3.3.2.cmml" xref="S3.SS1.p2.12.m6.4.5.3.3.2">ℝ</ci><apply id="S3.SS1.p2.12.m6.4.5.3.3.3.cmml" xref="S3.SS1.p2.12.m6.4.5.3.3.3"><times id="S3.SS1.p2.12.m6.4.5.3.3.3.1.cmml" xref="S3.SS1.p2.12.m6.4.5.3.3.3.1"></times><ci id="S3.SS1.p2.12.m6.4.5.3.3.3.2.cmml" xref="S3.SS1.p2.12.m6.4.5.3.3.3.2">𝑁</ci><cn type="integer" id="S3.SS1.p2.12.m6.4.5.3.3.3.3.cmml" xref="S3.SS1.p2.12.m6.4.5.3.3.3.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m6.4c">\mathcal{T}(\vec{\beta},\vec{\psi})\!:\!\mathbb{R}^{|\vec{\beta}|\times|\vec{\psi}|}\to\mathbb{R}^{N\times 3}</annotation></semantics></math>
constructs a face mesh in the bind pose by adding displacements to the template mesh <math id="S3.SS1.p2.13.m7.1" class="ltx_Math" alttext="\mathbf{\overline{T}}\!\in\!\mathbb{R}^{N\times 3}" display="inline"><semantics id="S3.SS1.p2.13.m7.1a"><mrow id="S3.SS1.p2.13.m7.1.1" xref="S3.SS1.p2.13.m7.1.1.cmml"><mover accent="true" id="S3.SS1.p2.13.m7.1.1.2" xref="S3.SS1.p2.13.m7.1.1.2.cmml"><mi id="S3.SS1.p2.13.m7.1.1.2.2" xref="S3.SS1.p2.13.m7.1.1.2.2.cmml">𝐓</mi><mo id="S3.SS1.p2.13.m7.1.1.2.1" xref="S3.SS1.p2.13.m7.1.1.2.1.cmml">¯</mo></mover><mo lspace="0.108em" rspace="0.108em" id="S3.SS1.p2.13.m7.1.1.1" xref="S3.SS1.p2.13.m7.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.13.m7.1.1.3" xref="S3.SS1.p2.13.m7.1.1.3.cmml"><mi id="S3.SS1.p2.13.m7.1.1.3.2" xref="S3.SS1.p2.13.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.13.m7.1.1.3.3" xref="S3.SS1.p2.13.m7.1.1.3.3.cmml"><mi id="S3.SS1.p2.13.m7.1.1.3.3.2" xref="S3.SS1.p2.13.m7.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.13.m7.1.1.3.3.1" xref="S3.SS1.p2.13.m7.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.13.m7.1.1.3.3.3" xref="S3.SS1.p2.13.m7.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m7.1b"><apply id="S3.SS1.p2.13.m7.1.1.cmml" xref="S3.SS1.p2.13.m7.1.1"><in id="S3.SS1.p2.13.m7.1.1.1.cmml" xref="S3.SS1.p2.13.m7.1.1.1"></in><apply id="S3.SS1.p2.13.m7.1.1.2.cmml" xref="S3.SS1.p2.13.m7.1.1.2"><ci id="S3.SS1.p2.13.m7.1.1.2.1.cmml" xref="S3.SS1.p2.13.m7.1.1.2.1">¯</ci><ci id="S3.SS1.p2.13.m7.1.1.2.2.cmml" xref="S3.SS1.p2.13.m7.1.1.2.2">𝐓</ci></apply><apply id="S3.SS1.p2.13.m7.1.1.3.cmml" xref="S3.SS1.p2.13.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m7.1.1.3.1.cmml" xref="S3.SS1.p2.13.m7.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.13.m7.1.1.3.2.cmml" xref="S3.SS1.p2.13.m7.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.13.m7.1.1.3.3.cmml" xref="S3.SS1.p2.13.m7.1.1.3.3"><times id="S3.SS1.p2.13.m7.1.1.3.3.1.cmml" xref="S3.SS1.p2.13.m7.1.1.3.3.1"></times><ci id="S3.SS1.p2.13.m7.1.1.3.3.2.cmml" xref="S3.SS1.p2.13.m7.1.1.3.3.2">𝑁</ci><cn type="integer" id="S3.SS1.p2.13.m7.1.1.3.3.3.cmml" xref="S3.SS1.p2.13.m7.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m7.1c">\mathbf{\overline{T}}\!\in\!\mathbb{R}^{N\times 3}</annotation></semantics></math>, which represents the average face with neutral expression:</p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.2" class="ltx_Math" alttext="\mathcal{T}(\vec{\beta},\vec{\psi})^{j}_{\&gt;k}=\overline{T}^{j}_{\&gt;k}+\beta_{i}S^{ij}_{\;\;k}+\psi_{i}E^{ij}_{\;\;k}" display="block"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.3" xref="S3.Ex2.m1.2.3.cmml"><mrow id="S3.Ex2.m1.2.3.2" xref="S3.Ex2.m1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.2.3.2.2" xref="S3.Ex2.m1.2.3.2.2.cmml">𝒯</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.3.2.1" xref="S3.Ex2.m1.2.3.2.1.cmml">​</mo><msubsup id="S3.Ex2.m1.2.3.2.3" xref="S3.Ex2.m1.2.3.2.3.cmml"><mrow id="S3.Ex2.m1.2.3.2.3.2.2.2" xref="S3.Ex2.m1.2.3.2.3.2.2.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.3.2.3.2.2.2.1" xref="S3.Ex2.m1.2.3.2.3.2.2.1.cmml">(</mo><mover accent="true" id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.2" xref="S3.Ex2.m1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.Ex2.m1.1.1.1" xref="S3.Ex2.m1.1.1.1.cmml">→</mo></mover><mo id="S3.Ex2.m1.2.3.2.3.2.2.2.2" xref="S3.Ex2.m1.2.3.2.3.2.2.1.cmml">,</mo><mover accent="true" id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml"><mi id="S3.Ex2.m1.2.2.2" xref="S3.Ex2.m1.2.2.2.cmml">ψ</mi><mo stretchy="false" id="S3.Ex2.m1.2.2.1" xref="S3.Ex2.m1.2.2.1.cmml">→</mo></mover><mo stretchy="false" id="S3.Ex2.m1.2.3.2.3.2.2.2.3" xref="S3.Ex2.m1.2.3.2.3.2.2.1.cmml">)</mo></mrow><mi id="S3.Ex2.m1.2.3.2.3.3" xref="S3.Ex2.m1.2.3.2.3.3.cmml">k</mi><mi id="S3.Ex2.m1.2.3.2.3.2.3" xref="S3.Ex2.m1.2.3.2.3.2.3.cmml">j</mi></msubsup></mrow><mo id="S3.Ex2.m1.2.3.1" xref="S3.Ex2.m1.2.3.1.cmml">=</mo><mrow id="S3.Ex2.m1.2.3.3" xref="S3.Ex2.m1.2.3.3.cmml"><msubsup id="S3.Ex2.m1.2.3.3.2" xref="S3.Ex2.m1.2.3.3.2.cmml"><mover accent="true" id="S3.Ex2.m1.2.3.3.2.2.2" xref="S3.Ex2.m1.2.3.3.2.2.2.cmml"><mi id="S3.Ex2.m1.2.3.3.2.2.2.2" xref="S3.Ex2.m1.2.3.3.2.2.2.2.cmml">T</mi><mo id="S3.Ex2.m1.2.3.3.2.2.2.1" xref="S3.Ex2.m1.2.3.3.2.2.2.1.cmml">¯</mo></mover><mi id="S3.Ex2.m1.2.3.3.2.3" xref="S3.Ex2.m1.2.3.3.2.3.cmml">k</mi><mi id="S3.Ex2.m1.2.3.3.2.2.3" xref="S3.Ex2.m1.2.3.3.2.2.3.cmml">j</mi></msubsup><mo id="S3.Ex2.m1.2.3.3.1" xref="S3.Ex2.m1.2.3.3.1.cmml">+</mo><mrow id="S3.Ex2.m1.2.3.3.3" xref="S3.Ex2.m1.2.3.3.3.cmml"><msub id="S3.Ex2.m1.2.3.3.3.2" xref="S3.Ex2.m1.2.3.3.3.2.cmml"><mi id="S3.Ex2.m1.2.3.3.3.2.2" xref="S3.Ex2.m1.2.3.3.3.2.2.cmml">β</mi><mi id="S3.Ex2.m1.2.3.3.3.2.3" xref="S3.Ex2.m1.2.3.3.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.3.3.3.1" xref="S3.Ex2.m1.2.3.3.3.1.cmml">​</mo><msubsup id="S3.Ex2.m1.2.3.3.3.3" xref="S3.Ex2.m1.2.3.3.3.3.cmml"><mi id="S3.Ex2.m1.2.3.3.3.3.2.2" xref="S3.Ex2.m1.2.3.3.3.3.2.2.cmml">S</mi><mi id="S3.Ex2.m1.2.3.3.3.3.3" xref="S3.Ex2.m1.2.3.3.3.3.3.cmml">k</mi><mrow id="S3.Ex2.m1.2.3.3.3.3.2.3" xref="S3.Ex2.m1.2.3.3.3.3.2.3.cmml"><mi id="S3.Ex2.m1.2.3.3.3.3.2.3.2" xref="S3.Ex2.m1.2.3.3.3.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.3.3.3.3.2.3.1" xref="S3.Ex2.m1.2.3.3.3.3.2.3.1.cmml">​</mo><mi id="S3.Ex2.m1.2.3.3.3.3.2.3.3" xref="S3.Ex2.m1.2.3.3.3.3.2.3.3.cmml">j</mi></mrow></msubsup></mrow><mo id="S3.Ex2.m1.2.3.3.1a" xref="S3.Ex2.m1.2.3.3.1.cmml">+</mo><mrow id="S3.Ex2.m1.2.3.3.4" xref="S3.Ex2.m1.2.3.3.4.cmml"><msub id="S3.Ex2.m1.2.3.3.4.2" xref="S3.Ex2.m1.2.3.3.4.2.cmml"><mi id="S3.Ex2.m1.2.3.3.4.2.2" xref="S3.Ex2.m1.2.3.3.4.2.2.cmml">ψ</mi><mi id="S3.Ex2.m1.2.3.3.4.2.3" xref="S3.Ex2.m1.2.3.3.4.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.3.3.4.1" xref="S3.Ex2.m1.2.3.3.4.1.cmml">​</mo><msubsup id="S3.Ex2.m1.2.3.3.4.3" xref="S3.Ex2.m1.2.3.3.4.3.cmml"><mi id="S3.Ex2.m1.2.3.3.4.3.2.2" xref="S3.Ex2.m1.2.3.3.4.3.2.2.cmml">E</mi><mi id="S3.Ex2.m1.2.3.3.4.3.3" xref="S3.Ex2.m1.2.3.3.4.3.3.cmml">k</mi><mrow id="S3.Ex2.m1.2.3.3.4.3.2.3" xref="S3.Ex2.m1.2.3.3.4.3.2.3.cmml"><mi id="S3.Ex2.m1.2.3.3.4.3.2.3.2" xref="S3.Ex2.m1.2.3.3.4.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.3.3.4.3.2.3.1" xref="S3.Ex2.m1.2.3.3.4.3.2.3.1.cmml">​</mo><mi id="S3.Ex2.m1.2.3.3.4.3.2.3.3" xref="S3.Ex2.m1.2.3.3.4.3.2.3.3.cmml">j</mi></mrow></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.3.cmml" xref="S3.Ex2.m1.2.3"><eq id="S3.Ex2.m1.2.3.1.cmml" xref="S3.Ex2.m1.2.3.1"></eq><apply id="S3.Ex2.m1.2.3.2.cmml" xref="S3.Ex2.m1.2.3.2"><times id="S3.Ex2.m1.2.3.2.1.cmml" xref="S3.Ex2.m1.2.3.2.1"></times><ci id="S3.Ex2.m1.2.3.2.2.cmml" xref="S3.Ex2.m1.2.3.2.2">𝒯</ci><apply id="S3.Ex2.m1.2.3.2.3.cmml" xref="S3.Ex2.m1.2.3.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.2.3.1.cmml" xref="S3.Ex2.m1.2.3.2.3">subscript</csymbol><apply id="S3.Ex2.m1.2.3.2.3.2.cmml" xref="S3.Ex2.m1.2.3.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.2.3.2.1.cmml" xref="S3.Ex2.m1.2.3.2.3">superscript</csymbol><interval closure="open" id="S3.Ex2.m1.2.3.2.3.2.2.1.cmml" xref="S3.Ex2.m1.2.3.2.3.2.2.2"><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><ci id="S3.Ex2.m1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1">→</ci><ci id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.2">𝛽</ci></apply><apply id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2"><ci id="S3.Ex2.m1.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1">→</ci><ci id="S3.Ex2.m1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2">𝜓</ci></apply></interval><ci id="S3.Ex2.m1.2.3.2.3.2.3.cmml" xref="S3.Ex2.m1.2.3.2.3.2.3">𝑗</ci></apply><ci id="S3.Ex2.m1.2.3.2.3.3.cmml" xref="S3.Ex2.m1.2.3.2.3.3">𝑘</ci></apply></apply><apply id="S3.Ex2.m1.2.3.3.cmml" xref="S3.Ex2.m1.2.3.3"><plus id="S3.Ex2.m1.2.3.3.1.cmml" xref="S3.Ex2.m1.2.3.3.1"></plus><apply id="S3.Ex2.m1.2.3.3.2.cmml" xref="S3.Ex2.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.2.1.cmml" xref="S3.Ex2.m1.2.3.3.2">subscript</csymbol><apply id="S3.Ex2.m1.2.3.3.2.2.cmml" xref="S3.Ex2.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.2.2.1.cmml" xref="S3.Ex2.m1.2.3.3.2">superscript</csymbol><apply id="S3.Ex2.m1.2.3.3.2.2.2.cmml" xref="S3.Ex2.m1.2.3.3.2.2.2"><ci id="S3.Ex2.m1.2.3.3.2.2.2.1.cmml" xref="S3.Ex2.m1.2.3.3.2.2.2.1">¯</ci><ci id="S3.Ex2.m1.2.3.3.2.2.2.2.cmml" xref="S3.Ex2.m1.2.3.3.2.2.2.2">𝑇</ci></apply><ci id="S3.Ex2.m1.2.3.3.2.2.3.cmml" xref="S3.Ex2.m1.2.3.3.2.2.3">𝑗</ci></apply><ci id="S3.Ex2.m1.2.3.3.2.3.cmml" xref="S3.Ex2.m1.2.3.3.2.3">𝑘</ci></apply><apply id="S3.Ex2.m1.2.3.3.3.cmml" xref="S3.Ex2.m1.2.3.3.3"><times id="S3.Ex2.m1.2.3.3.3.1.cmml" xref="S3.Ex2.m1.2.3.3.3.1"></times><apply id="S3.Ex2.m1.2.3.3.3.2.cmml" xref="S3.Ex2.m1.2.3.3.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.3.2.1.cmml" xref="S3.Ex2.m1.2.3.3.3.2">subscript</csymbol><ci id="S3.Ex2.m1.2.3.3.3.2.2.cmml" xref="S3.Ex2.m1.2.3.3.3.2.2">𝛽</ci><ci id="S3.Ex2.m1.2.3.3.3.2.3.cmml" xref="S3.Ex2.m1.2.3.3.3.2.3">𝑖</ci></apply><apply id="S3.Ex2.m1.2.3.3.3.3.cmml" xref="S3.Ex2.m1.2.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.3.3.1.cmml" xref="S3.Ex2.m1.2.3.3.3.3">subscript</csymbol><apply id="S3.Ex2.m1.2.3.3.3.3.2.cmml" xref="S3.Ex2.m1.2.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.3.3.2.1.cmml" xref="S3.Ex2.m1.2.3.3.3.3">superscript</csymbol><ci id="S3.Ex2.m1.2.3.3.3.3.2.2.cmml" xref="S3.Ex2.m1.2.3.3.3.3.2.2">𝑆</ci><apply id="S3.Ex2.m1.2.3.3.3.3.2.3.cmml" xref="S3.Ex2.m1.2.3.3.3.3.2.3"><times id="S3.Ex2.m1.2.3.3.3.3.2.3.1.cmml" xref="S3.Ex2.m1.2.3.3.3.3.2.3.1"></times><ci id="S3.Ex2.m1.2.3.3.3.3.2.3.2.cmml" xref="S3.Ex2.m1.2.3.3.3.3.2.3.2">𝑖</ci><ci id="S3.Ex2.m1.2.3.3.3.3.2.3.3.cmml" xref="S3.Ex2.m1.2.3.3.3.3.2.3.3">𝑗</ci></apply></apply><ci id="S3.Ex2.m1.2.3.3.3.3.3.cmml" xref="S3.Ex2.m1.2.3.3.3.3.3">𝑘</ci></apply></apply><apply id="S3.Ex2.m1.2.3.3.4.cmml" xref="S3.Ex2.m1.2.3.3.4"><times id="S3.Ex2.m1.2.3.3.4.1.cmml" xref="S3.Ex2.m1.2.3.3.4.1"></times><apply id="S3.Ex2.m1.2.3.3.4.2.cmml" xref="S3.Ex2.m1.2.3.3.4.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.4.2.1.cmml" xref="S3.Ex2.m1.2.3.3.4.2">subscript</csymbol><ci id="S3.Ex2.m1.2.3.3.4.2.2.cmml" xref="S3.Ex2.m1.2.3.3.4.2.2">𝜓</ci><ci id="S3.Ex2.m1.2.3.3.4.2.3.cmml" xref="S3.Ex2.m1.2.3.3.4.2.3">𝑖</ci></apply><apply id="S3.Ex2.m1.2.3.3.4.3.cmml" xref="S3.Ex2.m1.2.3.3.4.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.4.3.1.cmml" xref="S3.Ex2.m1.2.3.3.4.3">subscript</csymbol><apply id="S3.Ex2.m1.2.3.3.4.3.2.cmml" xref="S3.Ex2.m1.2.3.3.4.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.3.4.3.2.1.cmml" xref="S3.Ex2.m1.2.3.3.4.3">superscript</csymbol><ci id="S3.Ex2.m1.2.3.3.4.3.2.2.cmml" xref="S3.Ex2.m1.2.3.3.4.3.2.2">𝐸</ci><apply id="S3.Ex2.m1.2.3.3.4.3.2.3.cmml" xref="S3.Ex2.m1.2.3.3.4.3.2.3"><times id="S3.Ex2.m1.2.3.3.4.3.2.3.1.cmml" xref="S3.Ex2.m1.2.3.3.4.3.2.3.1"></times><ci id="S3.Ex2.m1.2.3.3.4.3.2.3.2.cmml" xref="S3.Ex2.m1.2.3.3.4.3.2.3.2">𝑖</ci><ci id="S3.Ex2.m1.2.3.3.4.3.2.3.3.cmml" xref="S3.Ex2.m1.2.3.3.4.3.2.3.3">𝑗</ci></apply></apply><ci id="S3.Ex2.m1.2.3.3.4.3.3.cmml" xref="S3.Ex2.m1.2.3.3.4.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">\mathcal{T}(\vec{\beta},\vec{\psi})^{j}_{\&gt;k}=\overline{T}^{j}_{\&gt;k}+\beta_{i}S^{ij}_{\;\;k}+\psi_{i}E^{ij}_{\;\;k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.17" class="ltx_p">given linear identity basis <math id="S3.SS1.p2.14.m1.1" class="ltx_Math" alttext="\mathbf{S}\!\in\!\mathbb{R}^{|\vec{\beta}|\times N\times 3}" display="inline"><semantics id="S3.SS1.p2.14.m1.1a"><mrow id="S3.SS1.p2.14.m1.1.2" xref="S3.SS1.p2.14.m1.1.2.cmml"><mi id="S3.SS1.p2.14.m1.1.2.2" xref="S3.SS1.p2.14.m1.1.2.2.cmml">𝐒</mi><mo lspace="0.108em" rspace="0.108em" id="S3.SS1.p2.14.m1.1.2.1" xref="S3.SS1.p2.14.m1.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.14.m1.1.2.3" xref="S3.SS1.p2.14.m1.1.2.3.cmml"><mi id="S3.SS1.p2.14.m1.1.2.3.2" xref="S3.SS1.p2.14.m1.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.14.m1.1.1.1" xref="S3.SS1.p2.14.m1.1.1.1.cmml"><mrow id="S3.SS1.p2.14.m1.1.1.1.3.2" xref="S3.SS1.p2.14.m1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.14.m1.1.1.1.3.2.1" xref="S3.SS1.p2.14.m1.1.1.1.3.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.14.m1.1.1.1.1" xref="S3.SS1.p2.14.m1.1.1.1.1.cmml"><mi id="S3.SS1.p2.14.m1.1.1.1.1.2" xref="S3.SS1.p2.14.m1.1.1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.14.m1.1.1.1.1.1" xref="S3.SS1.p2.14.m1.1.1.1.1.1.cmml">→</mo></mover><mo rspace="0.055em" stretchy="false" id="S3.SS1.p2.14.m1.1.1.1.3.2.2" xref="S3.SS1.p2.14.m1.1.1.1.3.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.SS1.p2.14.m1.1.1.1.2" xref="S3.SS1.p2.14.m1.1.1.1.2.cmml">×</mo><mi id="S3.SS1.p2.14.m1.1.1.1.4" xref="S3.SS1.p2.14.m1.1.1.1.4.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.14.m1.1.1.1.2a" xref="S3.SS1.p2.14.m1.1.1.1.2.cmml">×</mo><mn id="S3.SS1.p2.14.m1.1.1.1.5" xref="S3.SS1.p2.14.m1.1.1.1.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m1.1b"><apply id="S3.SS1.p2.14.m1.1.2.cmml" xref="S3.SS1.p2.14.m1.1.2"><in id="S3.SS1.p2.14.m1.1.2.1.cmml" xref="S3.SS1.p2.14.m1.1.2.1"></in><ci id="S3.SS1.p2.14.m1.1.2.2.cmml" xref="S3.SS1.p2.14.m1.1.2.2">𝐒</ci><apply id="S3.SS1.p2.14.m1.1.2.3.cmml" xref="S3.SS1.p2.14.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m1.1.2.3.1.cmml" xref="S3.SS1.p2.14.m1.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.14.m1.1.2.3.2.cmml" xref="S3.SS1.p2.14.m1.1.2.3.2">ℝ</ci><apply id="S3.SS1.p2.14.m1.1.1.1.cmml" xref="S3.SS1.p2.14.m1.1.1.1"><times id="S3.SS1.p2.14.m1.1.1.1.2.cmml" xref="S3.SS1.p2.14.m1.1.1.1.2"></times><apply id="S3.SS1.p2.14.m1.1.1.1.3.1.cmml" xref="S3.SS1.p2.14.m1.1.1.1.3.2"><abs id="S3.SS1.p2.14.m1.1.1.1.3.1.1.cmml" xref="S3.SS1.p2.14.m1.1.1.1.3.2.1"></abs><apply id="S3.SS1.p2.14.m1.1.1.1.1.cmml" xref="S3.SS1.p2.14.m1.1.1.1.1"><ci id="S3.SS1.p2.14.m1.1.1.1.1.1.cmml" xref="S3.SS1.p2.14.m1.1.1.1.1.1">→</ci><ci id="S3.SS1.p2.14.m1.1.1.1.1.2.cmml" xref="S3.SS1.p2.14.m1.1.1.1.1.2">𝛽</ci></apply></apply><ci id="S3.SS1.p2.14.m1.1.1.1.4.cmml" xref="S3.SS1.p2.14.m1.1.1.1.4">𝑁</ci><cn type="integer" id="S3.SS1.p2.14.m1.1.1.1.5.cmml" xref="S3.SS1.p2.14.m1.1.1.1.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m1.1c">\mathbf{S}\!\in\!\mathbb{R}^{|\vec{\beta}|\times N\times 3}</annotation></semantics></math> and
expression basis <math id="S3.SS1.p2.15.m2.1" class="ltx_Math" alttext="\mathbf{E}\!\in\!\mathbb{R}^{|\vec{\psi}|\times N\times 3}" display="inline"><semantics id="S3.SS1.p2.15.m2.1a"><mrow id="S3.SS1.p2.15.m2.1.2" xref="S3.SS1.p2.15.m2.1.2.cmml"><mi id="S3.SS1.p2.15.m2.1.2.2" xref="S3.SS1.p2.15.m2.1.2.2.cmml">𝐄</mi><mo lspace="0.108em" rspace="0.108em" id="S3.SS1.p2.15.m2.1.2.1" xref="S3.SS1.p2.15.m2.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.15.m2.1.2.3" xref="S3.SS1.p2.15.m2.1.2.3.cmml"><mi id="S3.SS1.p2.15.m2.1.2.3.2" xref="S3.SS1.p2.15.m2.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.15.m2.1.1.1" xref="S3.SS1.p2.15.m2.1.1.1.cmml"><mrow id="S3.SS1.p2.15.m2.1.1.1.3.2" xref="S3.SS1.p2.15.m2.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.15.m2.1.1.1.3.2.1" xref="S3.SS1.p2.15.m2.1.1.1.3.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.15.m2.1.1.1.1" xref="S3.SS1.p2.15.m2.1.1.1.1.cmml"><mi id="S3.SS1.p2.15.m2.1.1.1.1.2" xref="S3.SS1.p2.15.m2.1.1.1.1.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS1.p2.15.m2.1.1.1.1.1" xref="S3.SS1.p2.15.m2.1.1.1.1.1.cmml">→</mo></mover><mo rspace="0.055em" stretchy="false" id="S3.SS1.p2.15.m2.1.1.1.3.2.2" xref="S3.SS1.p2.15.m2.1.1.1.3.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.SS1.p2.15.m2.1.1.1.2" xref="S3.SS1.p2.15.m2.1.1.1.2.cmml">×</mo><mi id="S3.SS1.p2.15.m2.1.1.1.4" xref="S3.SS1.p2.15.m2.1.1.1.4.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.15.m2.1.1.1.2a" xref="S3.SS1.p2.15.m2.1.1.1.2.cmml">×</mo><mn id="S3.SS1.p2.15.m2.1.1.1.5" xref="S3.SS1.p2.15.m2.1.1.1.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m2.1b"><apply id="S3.SS1.p2.15.m2.1.2.cmml" xref="S3.SS1.p2.15.m2.1.2"><in id="S3.SS1.p2.15.m2.1.2.1.cmml" xref="S3.SS1.p2.15.m2.1.2.1"></in><ci id="S3.SS1.p2.15.m2.1.2.2.cmml" xref="S3.SS1.p2.15.m2.1.2.2">𝐄</ci><apply id="S3.SS1.p2.15.m2.1.2.3.cmml" xref="S3.SS1.p2.15.m2.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m2.1.2.3.1.cmml" xref="S3.SS1.p2.15.m2.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.15.m2.1.2.3.2.cmml" xref="S3.SS1.p2.15.m2.1.2.3.2">ℝ</ci><apply id="S3.SS1.p2.15.m2.1.1.1.cmml" xref="S3.SS1.p2.15.m2.1.1.1"><times id="S3.SS1.p2.15.m2.1.1.1.2.cmml" xref="S3.SS1.p2.15.m2.1.1.1.2"></times><apply id="S3.SS1.p2.15.m2.1.1.1.3.1.cmml" xref="S3.SS1.p2.15.m2.1.1.1.3.2"><abs id="S3.SS1.p2.15.m2.1.1.1.3.1.1.cmml" xref="S3.SS1.p2.15.m2.1.1.1.3.2.1"></abs><apply id="S3.SS1.p2.15.m2.1.1.1.1.cmml" xref="S3.SS1.p2.15.m2.1.1.1.1"><ci id="S3.SS1.p2.15.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.15.m2.1.1.1.1.1">→</ci><ci id="S3.SS1.p2.15.m2.1.1.1.1.2.cmml" xref="S3.SS1.p2.15.m2.1.1.1.1.2">𝜓</ci></apply></apply><ci id="S3.SS1.p2.15.m2.1.1.1.4.cmml" xref="S3.SS1.p2.15.m2.1.1.1.4">𝑁</ci><cn type="integer" id="S3.SS1.p2.15.m2.1.1.1.5.cmml" xref="S3.SS1.p2.15.m2.1.1.1.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m2.1c">\mathbf{E}\!\in\!\mathbb{R}^{|\vec{\psi}|\times N\times 3}</annotation></semantics></math>.
Note the use of Einstein summation notation in this definition and below.
Finally, <math id="S3.SS1.p2.16.m3.2" class="ltx_Math" alttext="\mathcal{J}(\vec{\beta})\!:\!\mathbb{R}^{|\vec{\beta}|}\to\mathbb{R}^{K\times 3}" display="inline"><semantics id="S3.SS1.p2.16.m3.2a"><mrow id="S3.SS1.p2.16.m3.2.3" xref="S3.SS1.p2.16.m3.2.3.cmml"><mrow id="S3.SS1.p2.16.m3.2.3.2" xref="S3.SS1.p2.16.m3.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.16.m3.2.3.2.2" xref="S3.SS1.p2.16.m3.2.3.2.2.cmml">𝒥</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.16.m3.2.3.2.1" xref="S3.SS1.p2.16.m3.2.3.2.1.cmml">​</mo><mrow id="S3.SS1.p2.16.m3.2.3.2.3.2" xref="S3.SS1.p2.16.m3.2.2.cmml"><mo stretchy="false" id="S3.SS1.p2.16.m3.2.3.2.3.2.1" xref="S3.SS1.p2.16.m3.2.2.cmml">(</mo><mover accent="true" id="S3.SS1.p2.16.m3.2.2" xref="S3.SS1.p2.16.m3.2.2.cmml"><mi id="S3.SS1.p2.16.m3.2.2.2" xref="S3.SS1.p2.16.m3.2.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.16.m3.2.2.1" xref="S3.SS1.p2.16.m3.2.2.1.cmml">→</mo></mover><mo rspace="0.108em" stretchy="false" id="S3.SS1.p2.16.m3.2.3.2.3.2.2" xref="S3.SS1.p2.16.m3.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.108em" id="S3.SS1.p2.16.m3.2.3.1" xref="S3.SS1.p2.16.m3.2.3.1.cmml">:</mo><mrow id="S3.SS1.p2.16.m3.2.3.3" xref="S3.SS1.p2.16.m3.2.3.3.cmml"><msup id="S3.SS1.p2.16.m3.2.3.3.2" xref="S3.SS1.p2.16.m3.2.3.3.2.cmml"><mi id="S3.SS1.p2.16.m3.2.3.3.2.2" xref="S3.SS1.p2.16.m3.2.3.3.2.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.16.m3.1.1.1.3" xref="S3.SS1.p2.16.m3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.16.m3.1.1.1.3.1" xref="S3.SS1.p2.16.m3.1.1.1.2.1.cmml">|</mo><mover accent="true" id="S3.SS1.p2.16.m3.1.1.1.1" xref="S3.SS1.p2.16.m3.1.1.1.1.cmml"><mi id="S3.SS1.p2.16.m3.1.1.1.1.2" xref="S3.SS1.p2.16.m3.1.1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p2.16.m3.1.1.1.1.1" xref="S3.SS1.p2.16.m3.1.1.1.1.1.cmml">→</mo></mover><mo stretchy="false" id="S3.SS1.p2.16.m3.1.1.1.3.2" xref="S3.SS1.p2.16.m3.1.1.1.2.1.cmml">|</mo></mrow></msup><mo stretchy="false" id="S3.SS1.p2.16.m3.2.3.3.1" xref="S3.SS1.p2.16.m3.2.3.3.1.cmml">→</mo><msup id="S3.SS1.p2.16.m3.2.3.3.3" xref="S3.SS1.p2.16.m3.2.3.3.3.cmml"><mi id="S3.SS1.p2.16.m3.2.3.3.3.2" xref="S3.SS1.p2.16.m3.2.3.3.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.16.m3.2.3.3.3.3" xref="S3.SS1.p2.16.m3.2.3.3.3.3.cmml"><mi id="S3.SS1.p2.16.m3.2.3.3.3.3.2" xref="S3.SS1.p2.16.m3.2.3.3.3.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.16.m3.2.3.3.3.3.1" xref="S3.SS1.p2.16.m3.2.3.3.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.16.m3.2.3.3.3.3.3" xref="S3.SS1.p2.16.m3.2.3.3.3.3.3.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.16.m3.2b"><apply id="S3.SS1.p2.16.m3.2.3.cmml" xref="S3.SS1.p2.16.m3.2.3"><ci id="S3.SS1.p2.16.m3.2.3.1.cmml" xref="S3.SS1.p2.16.m3.2.3.1">:</ci><apply id="S3.SS1.p2.16.m3.2.3.2.cmml" xref="S3.SS1.p2.16.m3.2.3.2"><times id="S3.SS1.p2.16.m3.2.3.2.1.cmml" xref="S3.SS1.p2.16.m3.2.3.2.1"></times><ci id="S3.SS1.p2.16.m3.2.3.2.2.cmml" xref="S3.SS1.p2.16.m3.2.3.2.2">𝒥</ci><apply id="S3.SS1.p2.16.m3.2.2.cmml" xref="S3.SS1.p2.16.m3.2.3.2.3.2"><ci id="S3.SS1.p2.16.m3.2.2.1.cmml" xref="S3.SS1.p2.16.m3.2.2.1">→</ci><ci id="S3.SS1.p2.16.m3.2.2.2.cmml" xref="S3.SS1.p2.16.m3.2.2.2">𝛽</ci></apply></apply><apply id="S3.SS1.p2.16.m3.2.3.3.cmml" xref="S3.SS1.p2.16.m3.2.3.3"><ci id="S3.SS1.p2.16.m3.2.3.3.1.cmml" xref="S3.SS1.p2.16.m3.2.3.3.1">→</ci><apply id="S3.SS1.p2.16.m3.2.3.3.2.cmml" xref="S3.SS1.p2.16.m3.2.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.16.m3.2.3.3.2.1.cmml" xref="S3.SS1.p2.16.m3.2.3.3.2">superscript</csymbol><ci id="S3.SS1.p2.16.m3.2.3.3.2.2.cmml" xref="S3.SS1.p2.16.m3.2.3.3.2.2">ℝ</ci><apply id="S3.SS1.p2.16.m3.1.1.1.2.cmml" xref="S3.SS1.p2.16.m3.1.1.1.3"><abs id="S3.SS1.p2.16.m3.1.1.1.2.1.cmml" xref="S3.SS1.p2.16.m3.1.1.1.3.1"></abs><apply id="S3.SS1.p2.16.m3.1.1.1.1.cmml" xref="S3.SS1.p2.16.m3.1.1.1.1"><ci id="S3.SS1.p2.16.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.16.m3.1.1.1.1.1">→</ci><ci id="S3.SS1.p2.16.m3.1.1.1.1.2.cmml" xref="S3.SS1.p2.16.m3.1.1.1.1.2">𝛽</ci></apply></apply></apply><apply id="S3.SS1.p2.16.m3.2.3.3.3.cmml" xref="S3.SS1.p2.16.m3.2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.16.m3.2.3.3.3.1.cmml" xref="S3.SS1.p2.16.m3.2.3.3.3">superscript</csymbol><ci id="S3.SS1.p2.16.m3.2.3.3.3.2.cmml" xref="S3.SS1.p2.16.m3.2.3.3.3.2">ℝ</ci><apply id="S3.SS1.p2.16.m3.2.3.3.3.3.cmml" xref="S3.SS1.p2.16.m3.2.3.3.3.3"><times id="S3.SS1.p2.16.m3.2.3.3.3.3.1.cmml" xref="S3.SS1.p2.16.m3.2.3.3.3.3.1"></times><ci id="S3.SS1.p2.16.m3.2.3.3.3.3.2.cmml" xref="S3.SS1.p2.16.m3.2.3.3.3.3.2">𝐾</ci><cn type="integer" id="S3.SS1.p2.16.m3.2.3.3.3.3.3.cmml" xref="S3.SS1.p2.16.m3.2.3.3.3.3.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.16.m3.2c">\mathcal{J}(\vec{\beta})\!:\!\mathbb{R}^{|\vec{\beta}|}\to\mathbb{R}^{K\times 3}</annotation></semantics></math> moves the template joint locations <math id="S3.SS1.p2.17.m4.1" class="ltx_Math" alttext="\mathbf{\overline{J}}\!\in\!\mathbb{R}^{K\times 3}" display="inline"><semantics id="S3.SS1.p2.17.m4.1a"><mrow id="S3.SS1.p2.17.m4.1.1" xref="S3.SS1.p2.17.m4.1.1.cmml"><mover accent="true" id="S3.SS1.p2.17.m4.1.1.2" xref="S3.SS1.p2.17.m4.1.1.2.cmml"><mi id="S3.SS1.p2.17.m4.1.1.2.2" xref="S3.SS1.p2.17.m4.1.1.2.2.cmml">𝐉</mi><mo id="S3.SS1.p2.17.m4.1.1.2.1" xref="S3.SS1.p2.17.m4.1.1.2.1.cmml">¯</mo></mover><mo lspace="0.108em" rspace="0.108em" id="S3.SS1.p2.17.m4.1.1.1" xref="S3.SS1.p2.17.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.17.m4.1.1.3" xref="S3.SS1.p2.17.m4.1.1.3.cmml"><mi id="S3.SS1.p2.17.m4.1.1.3.2" xref="S3.SS1.p2.17.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.17.m4.1.1.3.3" xref="S3.SS1.p2.17.m4.1.1.3.3.cmml"><mi id="S3.SS1.p2.17.m4.1.1.3.3.2" xref="S3.SS1.p2.17.m4.1.1.3.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.17.m4.1.1.3.3.1" xref="S3.SS1.p2.17.m4.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.17.m4.1.1.3.3.3" xref="S3.SS1.p2.17.m4.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.17.m4.1b"><apply id="S3.SS1.p2.17.m4.1.1.cmml" xref="S3.SS1.p2.17.m4.1.1"><in id="S3.SS1.p2.17.m4.1.1.1.cmml" xref="S3.SS1.p2.17.m4.1.1.1"></in><apply id="S3.SS1.p2.17.m4.1.1.2.cmml" xref="S3.SS1.p2.17.m4.1.1.2"><ci id="S3.SS1.p2.17.m4.1.1.2.1.cmml" xref="S3.SS1.p2.17.m4.1.1.2.1">¯</ci><ci id="S3.SS1.p2.17.m4.1.1.2.2.cmml" xref="S3.SS1.p2.17.m4.1.1.2.2">𝐉</ci></apply><apply id="S3.SS1.p2.17.m4.1.1.3.cmml" xref="S3.SS1.p2.17.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.17.m4.1.1.3.1.cmml" xref="S3.SS1.p2.17.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.17.m4.1.1.3.2.cmml" xref="S3.SS1.p2.17.m4.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.17.m4.1.1.3.3.cmml" xref="S3.SS1.p2.17.m4.1.1.3.3"><times id="S3.SS1.p2.17.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.17.m4.1.1.3.3.1"></times><ci id="S3.SS1.p2.17.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.17.m4.1.1.3.3.2">𝐾</ci><cn type="integer" id="S3.SS1.p2.17.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.17.m4.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.17.m4.1c">\mathbf{\overline{J}}\!\in\!\mathbb{R}^{K\times 3}</annotation></semantics></math> to account for changes in identity:</p>
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.2" class="ltx_Math" alttext="\mathcal{J}(\vec{\beta})^{j}_{\&gt;k}=\overline{J}^{j}_{\&gt;k}+W^{j}_{\;l}\beta_{i}S^{il}_{\;\;k}." display="block"><semantics id="S3.Ex3.m1.2a"><mrow id="S3.Ex3.m1.2.2.1" xref="S3.Ex3.m1.2.2.1.1.cmml"><mrow id="S3.Ex3.m1.2.2.1.1" xref="S3.Ex3.m1.2.2.1.1.cmml"><mrow id="S3.Ex3.m1.2.2.1.1.2" xref="S3.Ex3.m1.2.2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.2.2.1.1.2.2" xref="S3.Ex3.m1.2.2.1.1.2.2.cmml">𝒥</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.2.1" xref="S3.Ex3.m1.2.2.1.1.2.1.cmml">​</mo><msubsup id="S3.Ex3.m1.2.2.1.1.2.3" xref="S3.Ex3.m1.2.2.1.1.2.3.cmml"><mrow id="S3.Ex3.m1.2.2.1.1.2.3.2.2.2" xref="S3.Ex3.m1.1.1.cmml"><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.2.3.2.2.2.1" xref="S3.Ex3.m1.1.1.cmml">(</mo><mover accent="true" id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.2" xref="S3.Ex3.m1.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.Ex3.m1.1.1.1" xref="S3.Ex3.m1.1.1.1.cmml">→</mo></mover><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.2.3.2.2.2.2" xref="S3.Ex3.m1.1.1.cmml">)</mo></mrow><mi id="S3.Ex3.m1.2.2.1.1.2.3.3" xref="S3.Ex3.m1.2.2.1.1.2.3.3.cmml">k</mi><mi id="S3.Ex3.m1.2.2.1.1.2.3.2.3" xref="S3.Ex3.m1.2.2.1.1.2.3.2.3.cmml">j</mi></msubsup></mrow><mo id="S3.Ex3.m1.2.2.1.1.1" xref="S3.Ex3.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.2.2.1.1.3" xref="S3.Ex3.m1.2.2.1.1.3.cmml"><msubsup id="S3.Ex3.m1.2.2.1.1.3.2" xref="S3.Ex3.m1.2.2.1.1.3.2.cmml"><mover accent="true" id="S3.Ex3.m1.2.2.1.1.3.2.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.2.cmml">J</mi><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.cmml">¯</mo></mover><mi id="S3.Ex3.m1.2.2.1.1.3.2.3" xref="S3.Ex3.m1.2.2.1.1.3.2.3.cmml">k</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.3.cmml">j</mi></msubsup><mo id="S3.Ex3.m1.2.2.1.1.3.1" xref="S3.Ex3.m1.2.2.1.1.3.1.cmml">+</mo><mrow id="S3.Ex3.m1.2.2.1.1.3.3" xref="S3.Ex3.m1.2.2.1.1.3.3.cmml"><msubsup id="S3.Ex3.m1.2.2.1.1.3.3.2" xref="S3.Ex3.m1.2.2.1.1.3.3.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.3.2.2.2" xref="S3.Ex3.m1.2.2.1.1.3.3.2.2.2.cmml">W</mi><mi id="S3.Ex3.m1.2.2.1.1.3.3.2.3" xref="S3.Ex3.m1.2.2.1.1.3.3.2.3.cmml">l</mi><mi id="S3.Ex3.m1.2.2.1.1.3.3.2.2.3" xref="S3.Ex3.m1.2.2.1.1.3.3.2.2.3.cmml">j</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.3.3.1" xref="S3.Ex3.m1.2.2.1.1.3.3.1.cmml">​</mo><msub id="S3.Ex3.m1.2.2.1.1.3.3.3" xref="S3.Ex3.m1.2.2.1.1.3.3.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.3.3.2" xref="S3.Ex3.m1.2.2.1.1.3.3.3.2.cmml">β</mi><mi id="S3.Ex3.m1.2.2.1.1.3.3.3.3" xref="S3.Ex3.m1.2.2.1.1.3.3.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.3.3.1a" xref="S3.Ex3.m1.2.2.1.1.3.3.1.cmml">​</mo><msubsup id="S3.Ex3.m1.2.2.1.1.3.3.4" xref="S3.Ex3.m1.2.2.1.1.3.3.4.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.3.4.2.2" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.2.cmml">S</mi><mi id="S3.Ex3.m1.2.2.1.1.3.3.4.3" xref="S3.Ex3.m1.2.2.1.1.3.3.4.3.cmml">k</mi><mrow id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.2" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.1" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.1.cmml">​</mo><mi id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.3" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.3.cmml">l</mi></mrow></msubsup></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex3.m1.2.2.1.2" xref="S3.Ex3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.2b"><apply id="S3.Ex3.m1.2.2.1.1.cmml" xref="S3.Ex3.m1.2.2.1"><eq id="S3.Ex3.m1.2.2.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1"></eq><apply id="S3.Ex3.m1.2.2.1.1.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2"><times id="S3.Ex3.m1.2.2.1.1.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1"></times><ci id="S3.Ex3.m1.2.2.1.1.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2.2">𝒥</ci><apply id="S3.Ex3.m1.2.2.1.1.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.2.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.3">subscript</csymbol><apply id="S3.Ex3.m1.2.2.1.1.2.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.2.3.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.3">superscript</csymbol><apply id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.3.2.2.2"><ci id="S3.Ex3.m1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1">→</ci><ci id="S3.Ex3.m1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.2">𝛽</ci></apply><ci id="S3.Ex3.m1.2.2.1.1.2.3.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.2.3.2.3">𝑗</ci></apply><ci id="S3.Ex3.m1.2.2.1.1.2.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.2.3.3">𝑘</ci></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3"><plus id="S3.Ex3.m1.2.2.1.1.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.1"></plus><apply id="S3.Ex3.m1.2.2.1.1.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2">subscript</csymbol><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2">superscript</csymbol><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2"><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1">¯</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.2">𝐽</ci></apply><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.3">𝑗</ci></apply><ci id="S3.Ex3.m1.2.2.1.1.3.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.3">𝑘</ci></apply><apply id="S3.Ex3.m1.2.2.1.1.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3"><times id="S3.Ex3.m1.2.2.1.1.3.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.1"></times><apply id="S3.Ex3.m1.2.2.1.1.3.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.2">subscript</csymbol><apply id="S3.Ex3.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.3.2.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.2">superscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.3.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.2.2.2">𝑊</ci><ci id="S3.Ex3.m1.2.2.1.1.3.3.2.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.2.2.3">𝑗</ci></apply><ci id="S3.Ex3.m1.2.2.1.1.3.3.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.2.3">𝑙</ci></apply><apply id="S3.Ex3.m1.2.2.1.1.3.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.3.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.3.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.3.2">𝛽</ci><ci id="S3.Ex3.m1.2.2.1.1.3.3.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.3.3">𝑖</ci></apply><apply id="S3.Ex3.m1.2.2.1.1.3.3.4.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.3.4.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4">subscript</csymbol><apply id="S3.Ex3.m1.2.2.1.1.3.3.4.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.3.4.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4">superscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.3.4.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.2">𝑆</ci><apply id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3"><times id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.1"></times><ci id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.2">𝑖</ci><ci id="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4.2.3.3">𝑙</ci></apply></apply><ci id="S3.Ex3.m1.2.2.1.1.3.3.4.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.3.4.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.2c">\mathcal{J}(\vec{\beta})^{j}_{\&gt;k}=\overline{J}^{j}_{\&gt;k}+W^{j}_{\;l}\beta_{i}S^{il}_{\;\;k}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F5.7" class="ltx_ERROR ltx_figure_panel undefined">\stackinset</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F5.6" class="ltx_p ltx_figure_panel">l2ptb2.5pt<span id="S3.F5.1.1" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Raw<img src="/html/2109.15102/assets/figures/raw_vs_clean_scans/iccv21_raw_scan_2.jpg" id="S3.F5.1.1.g1" class="ltx_graphics ltx_img_portrait" width="99" height="131" alt="Refer to caption"><span id="S3.F5.1.1.1" class="ltx_ERROR undefined">\stackinset</span></span>l2ptb2.5pt<span id="S3.F5.2.2" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Clean<img src="/html/2109.15102/assets/figures/raw_vs_clean_scans/iccv21_clean_scan_2.jpg" id="S3.F5.2.2.g1" class="ltx_graphics ltx_img_portrait" width="99" height="131" alt="Refer to caption"></span> <img src="/html/2109.15102/assets/figures/raw_vs_clean_scans/iccv21_raw_scan_1.jpg" id="S3.F5.3.g1" class="ltx_graphics ltx_img_portrait" width="99" height="131" alt="Refer to caption"><img src="/html/2109.15102/assets/figures/raw_vs_clean_scans/iccv21_clean_scan_1.jpg" id="S3.F5.4.g2" class="ltx_graphics ltx_img_portrait" width="99" height="131" alt="Refer to caption"> <img src="/html/2109.15102/assets/figures/raw_vs_clean_scans/iccv21_raw_scan_3.jpg" id="S3.F5.5.g3" class="ltx_graphics ltx_img_portrait" width="99" height="131" alt="Refer to caption"><img src="/html/2109.15102/assets/figures/raw_vs_clean_scans/iccv21_clean_scan_3.jpg" id="S3.F5.6.g4" class="ltx_graphics ltx_img_portrait" width="99" height="131" alt="Refer to caption"></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>We manually “clean” raw high-resolution 3D head scans to remove noise and hair. We use the resulting clean scans to build our generative geometry model and texture library.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.10" class="ltx_p">We learn the identity basis <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathbf{S}</annotation></semantics></math> from high quality 3D scans of <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="M\!=\!511" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">M</mi><mo lspace="0.108em" rspace="0.108em" id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">511</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><eq id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></eq><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝑀</ci><cn type="integer" id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">511</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">M\!=\!511</annotation></semantics></math> individuals with neutral expression.
Each scan was cleaned (see <a href="#S3.F5" title="Figure 5 ‣ 3.1 3D face model ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>), and registered to the topology of <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{\overline{T}}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mover accent="true" id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">𝐓</mi><mo id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><ci id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1">¯</ci><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝐓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathbf{\overline{T}}</annotation></semantics></math> using commercial software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>, resulting in training dataset <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{V}\in\mathbb{R}^{M\times 3N}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mrow id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">𝐕</mi><mo id="S3.SS1.p3.4.m4.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.2" xref="S3.SS1.p3.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.4.m4.1.1.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.cmml"><mrow id="S3.SS1.p3.4.m4.1.1.3.3.2" xref="S3.SS1.p3.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.3.2.2" xref="S3.SS1.p3.4.m4.1.1.3.3.2.2.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.4.m4.1.1.3.3.2.1" xref="S3.SS1.p3.4.m4.1.1.3.3.2.1.cmml">×</mo><mn id="S3.SS1.p3.4.m4.1.1.3.3.2.3" xref="S3.SS1.p3.4.m4.1.1.3.3.2.3.cmml">3</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.1.3.3.1" xref="S3.SS1.p3.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p3.4.m4.1.1.3.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.3.cmml">N</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><in id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1"></in><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">𝐕</ci><apply id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3"><times id="S3.SS1.p3.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.1"></times><apply id="S3.SS1.p3.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2"><times id="S3.SS1.p3.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2.1"></times><ci id="S3.SS1.p3.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2.2">𝑀</ci><cn type="integer" id="S3.SS1.p3.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2.3">3</cn></apply><ci id="S3.SS1.p3.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\mathbf{V}\in\mathbb{R}^{M\times 3N}</annotation></semantics></math>.
We then jointly fit identity basis <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\mathbf{S}</annotation></semantics></math> and parameters <math id="S3.SS1.p3.6.m6.3" class="ltx_Math" alttext="[\vec{\beta}_{1},\ldots,\vec{\beta}_{M}]" display="inline"><semantics id="S3.SS1.p3.6.m6.3a"><mrow id="S3.SS1.p3.6.m6.3.3.2" xref="S3.SS1.p3.6.m6.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.p3.6.m6.3.3.2.3" xref="S3.SS1.p3.6.m6.3.3.3.cmml">[</mo><msub id="S3.SS1.p3.6.m6.2.2.1.1" xref="S3.SS1.p3.6.m6.2.2.1.1.cmml"><mover accent="true" id="S3.SS1.p3.6.m6.2.2.1.1.2" xref="S3.SS1.p3.6.m6.2.2.1.1.2.cmml"><mi id="S3.SS1.p3.6.m6.2.2.1.1.2.2" xref="S3.SS1.p3.6.m6.2.2.1.1.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p3.6.m6.2.2.1.1.2.1" xref="S3.SS1.p3.6.m6.2.2.1.1.2.1.cmml">→</mo></mover><mn id="S3.SS1.p3.6.m6.2.2.1.1.3" xref="S3.SS1.p3.6.m6.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p3.6.m6.3.3.2.4" xref="S3.SS1.p3.6.m6.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">…</mi><mo id="S3.SS1.p3.6.m6.3.3.2.5" xref="S3.SS1.p3.6.m6.3.3.3.cmml">,</mo><msub id="S3.SS1.p3.6.m6.3.3.2.2" xref="S3.SS1.p3.6.m6.3.3.2.2.cmml"><mover accent="true" id="S3.SS1.p3.6.m6.3.3.2.2.2" xref="S3.SS1.p3.6.m6.3.3.2.2.2.cmml"><mi id="S3.SS1.p3.6.m6.3.3.2.2.2.2" xref="S3.SS1.p3.6.m6.3.3.2.2.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS1.p3.6.m6.3.3.2.2.2.1" xref="S3.SS1.p3.6.m6.3.3.2.2.2.1.cmml">→</mo></mover><mi id="S3.SS1.p3.6.m6.3.3.2.2.3" xref="S3.SS1.p3.6.m6.3.3.2.2.3.cmml">M</mi></msub><mo stretchy="false" id="S3.SS1.p3.6.m6.3.3.2.6" xref="S3.SS1.p3.6.m6.3.3.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.3b"><list id="S3.SS1.p3.6.m6.3.3.3.cmml" xref="S3.SS1.p3.6.m6.3.3.2"><apply id="S3.SS1.p3.6.m6.2.2.1.1.cmml" xref="S3.SS1.p3.6.m6.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.p3.6.m6.2.2.1.1">subscript</csymbol><apply id="S3.SS1.p3.6.m6.2.2.1.1.2.cmml" xref="S3.SS1.p3.6.m6.2.2.1.1.2"><ci id="S3.SS1.p3.6.m6.2.2.1.1.2.1.cmml" xref="S3.SS1.p3.6.m6.2.2.1.1.2.1">→</ci><ci id="S3.SS1.p3.6.m6.2.2.1.1.2.2.cmml" xref="S3.SS1.p3.6.m6.2.2.1.1.2.2">𝛽</ci></apply><cn type="integer" id="S3.SS1.p3.6.m6.2.2.1.1.3.cmml" xref="S3.SS1.p3.6.m6.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">…</ci><apply id="S3.SS1.p3.6.m6.3.3.2.2.cmml" xref="S3.SS1.p3.6.m6.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.3.3.2.2.1.cmml" xref="S3.SS1.p3.6.m6.3.3.2.2">subscript</csymbol><apply id="S3.SS1.p3.6.m6.3.3.2.2.2.cmml" xref="S3.SS1.p3.6.m6.3.3.2.2.2"><ci id="S3.SS1.p3.6.m6.3.3.2.2.2.1.cmml" xref="S3.SS1.p3.6.m6.3.3.2.2.2.1">→</ci><ci id="S3.SS1.p3.6.m6.3.3.2.2.2.2.cmml" xref="S3.SS1.p3.6.m6.3.3.2.2.2.2">𝛽</ci></apply><ci id="S3.SS1.p3.6.m6.3.3.2.2.3.cmml" xref="S3.SS1.p3.6.m6.3.3.2.2.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.3c">[\vec{\beta}_{1},\ldots,\vec{\beta}_{M}]</annotation></semantics></math> to <math id="S3.SS1.p3.7.m7.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS1.p3.7.m7.1a"><mi id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><ci id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">\mathbf{V}</annotation></semantics></math>.
In order to generate novel face shapes, we fit a multivariate normal distribution to the fitted identity parameters, and sample from it (see <a href="#S3.F3" title="Figure 3 ‣ 3.1 3D face model ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>).
As is common in computer animation, both expression basis <math id="S3.SS1.p3.8.m8.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S3.SS1.p3.8.m8.1a"><mi id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml">𝐄</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.1b"><ci id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1">𝐄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">\mathbf{E}</annotation></semantics></math> and skinning weights <math id="S3.SS1.p3.9.m9.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S3.SS1.p3.9.m9.1a"><mi id="S3.SS1.p3.9.m9.1.1" xref="S3.SS1.p3.9.m9.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m9.1b"><ci id="S3.SS1.p3.9.m9.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m9.1c">\mathbf{W}</annotation></semantics></math> were authored by an artist, and are kept fixed while learning <math id="S3.SS1.p3.10.m10.1" class="ltx_Math" alttext="\mathbf{S}" display="inline"><semantics id="S3.SS1.p3.10.m10.1a"><mi id="S3.SS1.p3.10.m10.1.1" xref="S3.SS1.p3.10.m10.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.m10.1b"><ci id="S3.SS1.p3.10.m10.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.m10.1c">\mathbf{S}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Expression</h3>

<figure id="S3.F6" class="ltx_figure"><img src="/html/2109.15102/assets/figures/iccv21_expression_samples.jpg" id="S3.F6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="289" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Examples from our data-driven expression library and manually animated sequence, visualized on our template face.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We apply random expressions to each face so that our downstream machine learning models are robust to facial motion.
We use two sources of facial expression.
Our primary source is a library of 27,000 expression parameters <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\{\vec{\psi}_{i}\}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">{</mo><msub id="S3.SS2.p1.1.m1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.p1.1.m1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.1.1.2.2.cmml">ψ</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.1.1.1.1.2.1" xref="S3.SS2.p1.1.m1.1.1.1.1.2.1.cmml">→</mo></mover><mi id="S3.SS2.p1.1.m1.1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p1.1.m1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><set id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1"><apply id="S3.SS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.2"><ci id="S3.SS2.p1.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.2.1">→</ci><ci id="S3.SS2.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.2.2">𝜓</ci></apply><ci id="S3.SS2.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\{\vec{\psi}_{i}\}</annotation></semantics></math> built by fitting a 3D face model to a corpus of 2D images with annotated face landmarks.
However, since the annotated landmarks are sparse, it is not possible to recover all types of expression from these landmarks alone, e.g. cheek puffs.
Therefore, we additionally sample expressions from a manually animated sequence that was designed to fill the gaps in our expression library by exercising the face in realistic, but extreme ways.
<a href="#S3.F6" title="Figure 6 ‣ 3.2 Expression ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> shows samples from our expression collection. In addition to facial expression, we layer random eye gaze directions on top of sampled expressions, and use procedural logic to pose the eyelids accordingly.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Texture</h3>

<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F7.5" class="ltx_ERROR ltx_figure_panel undefined">\stackinset</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F7.4" class="ltx_p ltx_figure_panel">l2ptb2.5pt<span id="S3.F7.1.1" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Face model<span id="S3.F7.1.1.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span><img src="/html/2109.15102/assets/figures/displacement_maps/iccv21_disp_maps_1.jpg" id="S3.F7.1.1.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption"></span> <span id="S3.F7.4.4" class="ltx_ERROR undefined">\stackinset</span>l2ptb2.5pt<span id="S3.F7.2.2" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">+coarse disp.<img src="/html/2109.15102/assets/figures/displacement_maps/iccv21_disp_maps_2.jpg" id="S3.F7.2.2.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption"></span> <span id="S3.F7.4.5" class="ltx_ERROR undefined">\stackinset</span>l2ptb2.5pt<span id="S3.F7.3.3" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">+meso disp.<img src="/html/2109.15102/assets/figures/displacement_maps/iccv21_disp_maps_3.jpg" id="S3.F7.3.3.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption"></span> <img src="/html/2109.15102/assets/figures/displacement_maps/iccv21_disp_maps_4.jpg" id="S3.F7.4.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption"></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>We apply coarse and meso-displacement to our 3D face model to ensure faces look realistic even when viewed close-up.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Synthetic faces should look realistic even when viewed at extremely close range, for example by an eye-tracking camera in a head-mounted device.
To achieve this, we collected <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><cn type="integer" id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">200</annotation></semantics></math> sets of high resolution (8192<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\!\times\!" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mo id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><times id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\!\times\!</annotation></semantics></math>8192 px) textures from our cleaned face scans.
For each scan, we extract one albedo texture for skin color, and two displacement maps (see <a href="#S3.F7" title="Figure 7 ‣ 3.3 Texture ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>).
The coarse displacement map encodes scan geometry that is not captured by the sparse nature of our vertex-level identity model.
The meso-displacement map approximates skin-pore level detail and is built by high-pass filtering the albedo texture, assuming that dark pixels correspond to slightly recessed parts of the skin.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Unlike previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, we do not build a generative model of texture, as such models struggle to faithfully produce high-frequency details like wrinkles and pores.
Instead, we simply pick a corresponding set of albedo and displacement textures from each scan.
The textures are combined in a physically-based skin material featuring subsurface scattering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. Finally, we optionally apply makeup effects to simulate eyeshadow, eyeliner and mascara.
</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Hair</h3>

<figure id="S3.F8" class="ltx_figure">
<table id="S3.F8.10" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F8.5.5" class="ltx_tr">
<td id="S3.F8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H1.jpg" id="S3.F8.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H2.jpg" id="S3.F8.2.2.2.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H3.jpg" id="S3.F8.3.3.3.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H4.jpg" id="S3.F8.4.4.4.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H5.jpg" id="S3.F8.5.5.5.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
</tr>
<tr id="S3.F8.10.10" class="ltx_tr">
<td id="S3.F8.6.6.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H6.jpg" id="S3.F8.6.6.1.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.7.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H7.jpg" id="S3.F8.7.7.2.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.8.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H8.jpg" id="S3.F8.8.8.3.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.9.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H9.jpg" id="S3.F8.9.9.4.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
<td id="S3.F8.10.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/hair_library/iccv21_hair_H10.jpg" id="S3.F8.10.10.5.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Our hair library contains a diverse range of scalp hair, eyebrows, and beards. When assembling a 3D face, we choose hair style and appearance at random.</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In contrast to other work which approximates hair with textures or coarse geometry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, we represent hair as individual 3D strands, with a full head of hair comprising over 100,000 strands.
Modelling hair at the strand level allows us to capture realistic multi-path illumination effects.
Shown in <a href="#S3.F8" title="Figure 8 ‣ 3.4 Hair ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>, our hair library includes 512 scalp hair styles, 162 eyebrows, 142 beards, and 42 sets of eyelashes.
Each asset was authored by a groom artist who specializes in creating digital hair.
At render time, we randomly combine scalp, eyebrow, beard, and eyelash grooms.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">We use a physically-based procedural hair shader to accurately model the complex material properties of hair <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>.
This shader allows us to control the color of the hair with parameters for melanin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> and grayness, and even lets us dye or bleach the hair for less common hair styles.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Clothing</h3>

<figure id="S3.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.F9.15" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F9.5.5" class="ltx_tr">
<td id="S3.F9.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_000.jpg" id="S3.F9.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_002.jpg" id="S3.F9.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_017.jpg" id="S3.F9.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_003.jpg" id="S3.F9.4.4.4.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_001.jpg" id="S3.F9.5.5.5.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
</tr>
<tr id="S3.F9.10.10" class="ltx_tr">
<td id="S3.F9.6.6.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_005.jpg" id="S3.F9.6.6.1.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.7.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_006.jpg" id="S3.F9.7.7.2.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.8.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_007.jpg" id="S3.F9.8.8.3.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.9.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_009.jpg" id="S3.F9.9.9.4.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.10.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_016.jpg" id="S3.F9.10.10.5.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
</tr>
<tr id="S3.F9.15.15" class="ltx_tr">
<td id="S3.F9.11.11.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_010.jpg" id="S3.F9.11.11.1.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.12.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_011.jpg" id="S3.F9.12.12.2.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.13.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_012.jpg" id="S3.F9.13.13.3.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.14.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_008.jpg" id="S3.F9.14.14.4.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
<td id="S3.F9.15.15.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_accessory_samples_004.jpg" id="S3.F9.15.15.5.g1" class="ltx_graphics ltx_img_landscape" width="119" height="75" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.F9.21" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F9.21.6" class="ltx_tr">
<td id="S3.F9.16.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_headwear_samples_001.jpg" id="S3.F9.16.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="83" alt="Refer to caption"></td>
<td id="S3.F9.17.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_headwear_samples_004.jpg" id="S3.F9.17.2.2.g1" class="ltx_graphics ltx_img_square" width="100" height="83" alt="Refer to caption"></td>
<td id="S3.F9.18.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_headwear_samples_009.jpg" id="S3.F9.18.3.3.g1" class="ltx_graphics ltx_img_square" width="100" height="83" alt="Refer to caption"></td>
<td id="S3.F9.19.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_headwear_samples_003.jpg" id="S3.F9.19.4.4.g1" class="ltx_graphics ltx_img_square" width="100" height="83" alt="Refer to caption"></td>
<td id="S3.F9.20.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_headwear_samples_006.jpg" id="S3.F9.20.5.5.g1" class="ltx_graphics ltx_img_square" width="100" height="83" alt="Refer to caption"></td>
<td id="S3.F9.21.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_headwear_samples_007.jpg" id="S3.F9.21.6.6.g1" class="ltx_graphics ltx_img_square" width="100" height="83" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.F9.27" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F9.27.6" class="ltx_tr">
<td id="S3.F9.22.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_glasses_samples_000.jpg" id="S3.F9.22.1.1.g1" class="ltx_graphics ltx_img_landscape" width="100" height="39" alt="Refer to caption"></td>
<td id="S3.F9.23.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_glasses_samples_002.jpg" id="S3.F9.23.2.2.g1" class="ltx_graphics ltx_img_landscape" width="100" height="39" alt="Refer to caption"></td>
<td id="S3.F9.24.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_glasses_samples_007.jpg" id="S3.F9.24.3.3.g1" class="ltx_graphics ltx_img_landscape" width="100" height="39" alt="Refer to caption"></td>
<td id="S3.F9.25.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_glasses_samples_005.jpg" id="S3.F9.25.4.4.g1" class="ltx_graphics ltx_img_landscape" width="100" height="39" alt="Refer to caption"></td>
<td id="S3.F9.26.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_glasses_samples_003.jpg" id="S3.F9.26.5.5.g1" class="ltx_graphics ltx_img_landscape" width="100" height="39" alt="Refer to caption"></td>
<td id="S3.F9.27.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-5pt;padding-bottom:-5pt;"><img src="/html/2109.15102/assets/figures/accessories/iccv21_glasses_samples_010.jpg" id="S3.F9.27.6.6.g1" class="ltx_graphics ltx_img_landscape" width="100" height="39" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Each face is dressed in a random outfit assembled from our digital wardrobe – a collection of diverse 3D clothing and accessory assets that can be fit around our 3D head model.</figcaption>
</figure>
<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.4" class="ltx_p">Images of faces often include what someone is wearing, so we dress our faces in 3D clothing.
Our digital wardrobe contains <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mn id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><cn type="integer" id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">30</annotation></semantics></math> upper-body outfits which were manually created using clothing design and simulation software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>.
As shown in <a href="#S3.F9" title="Figure 9 ‣ 3.5 Clothing ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>, these outfits include formal, casual, and athletic clothing.
In addition to upper-body garments, we dress our faces in headwear (<math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="36" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mn id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">36</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><cn type="integer" id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">36</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">36</annotation></semantics></math> items), facewear (<math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><mn id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><cn type="integer" id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">7</annotation></semantics></math> items) and eyewear (<math id="S3.SS5.p1.4.m4.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S3.SS5.p1.4.m4.1a"><mn id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><cn type="integer" id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">11</annotation></semantics></math> items) including helmets, head scarves, face masks, and eyeglasses.
All clothing items were authored on an unclothed body mesh with either the average male or female body proportions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> in a relaxed stance.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">We deform garments with a non-rigid cage-based deformation technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> so they fit snugly around different shaped faces.
Eyeglasses are rigged with a skeleton, and posed using inverse kinematics so the temples and nose-bridge rest on the corresponding parts of the face.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Rendering</h3>

<figure id="S3.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2109.15102/assets/figures/hdri_faces/iccv21_hdris_0.jpg" id="S3.F10.g1" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="85" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2109.15102/assets/figures/hdri_faces/iccv21_hdris_1.jpg" id="S3.F10.g2" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="85" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2109.15102/assets/figures/hdri_faces/iccv21_hdris_2.jpg" id="S3.F10.g3" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="85" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2109.15102/assets/figures/hdri_faces/iccv21_hdris_4.jpg" id="S3.F10.g4" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="85" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2109.15102/assets/figures/hdri_faces/iccv21_hdris_6.jpg" id="S3.F10.g5" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="85" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2109.15102/assets/figures/hdri_faces/iccv21_hdris_3.jpg" id="S3.F10.g6" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="85" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/hdri_faces/iccv21_hdris_7.jpg" id="S3.F10.g7" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="85" height="128" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>We use HDRIs to illuminate the face. The same face can look very different under different illumination.</figcaption>
</figure>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2109.15102/assets/figures/dataset_samples.jpg" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="2048" height="883" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Examples of synthetic faces that we randomly generated and rendered for use as training data.</figcaption>
</figure>
<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">We render face images with Cycles, a photorealistic ray-tracing renderer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>.
We randomly position a camera around the head, and point it towards the face.
The focal length and depth of field are varied to simulate different cameras and lenses.
We employ image-based lighting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> with high dynamic range images (HDRI) to illuminate the face and provide a background (see <a href="#S3.F10" title="Figure 10 ‣ 3.6 Rendering ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>).
For each image, we randomly pick from a collection of 448 HDRIs that include a range of different environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite>.
See <a href="#S3.F11" title="Figure 11 ‣ 3.6 Rendering ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 11</span></a> for examples of faces rendered with our framework.</p>
</div>
<figure id="S3.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F12.7" class="ltx_ERROR ltx_figure_panel undefined">\stackinset</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F12.6" class="ltx_p ltx_figure_panel">l2ptb2.5pt<span id="S3.F12.1.1" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Albedo<span id="S3.F12.1.1.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span><img src="/html/2109.15102/assets/figures/GT%20labels/alb_hero_comp.jpg" id="S3.F12.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="145" alt="Refer to caption"><span id="S3.F12.1.1.2" class="ltx_ERROR undefined">\stackinset</span></span>l2ptb2.5pt<span id="S3.F12.2.2" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Normals<span id="S3.F12.2.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span><img src="/html/2109.15102/assets/figures/GT%20labels/nor_hero.jpg" id="S3.F12.2.2.g1" class="ltx_graphics ltx_img_portrait" width="100" height="145" alt="Refer to caption"><span id="S3.F12.2.2.2" class="ltx_ERROR undefined">\stackinset</span></span>l2ptb2.5pt<span id="S3.F12.3.3" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Depth<span id="S3.F12.3.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span><img src="/html/2109.15102/assets/figures/GT%20labels/depth_vis.jpg" id="S3.F12.3.3.g1" class="ltx_graphics ltx_img_portrait" width="100" height="145" alt="Refer to caption"><span id="S3.F12.3.3.2" class="ltx_ERROR undefined">\stackinset</span></span>l2ptb2.5pt<span id="S3.F12.4.4" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Mask<span id="S3.F12.4.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span><img src="/html/2109.15102/assets/figures/GT%20labels/fg_bg_mask.jpg" id="S3.F12.4.4.g1" class="ltx_graphics ltx_img_portrait" width="100" height="145" alt="Refer to caption"><span id="S3.F12.4.4.2" class="ltx_ERROR undefined">\stackinset</span></span>l2ptb2.5pt<span id="S3.F12.5.5" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">UVs<span id="S3.F12.5.5.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span><img src="/html/2109.15102/assets/figures/GT%20labels/uv_hero_vis.jpg" id="S3.F12.5.5.g1" class="ltx_graphics ltx_img_portrait" width="100" height="145" alt="Refer to caption"><span id="S3.F12.5.5.2" class="ltx_ERROR undefined">\stackinset</span></span>l2ptb2.5pt<span id="S3.F12.6.6" class="ltx_text ltx_font_sansserif" style="font-size:80%;color:#FFFFFF;">Vertices<span id="S3.F12.6.6.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span><img src="/html/2109.15102/assets/figures/GT%20labels/dense_ldmks_vis.jpg" id="S3.F12.6.6.g1" class="ltx_graphics ltx_img_portrait" width="100" height="145" alt="Refer to caption"></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>We also synthesize labels for machine learning. Above are additional label types beyond those shown in <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.</figcaption>
</figure>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">In addition to rendering color images, we generate ground truth labels (see <a href="#S3.F12" title="Figure 12 ‣ 3.6 Rendering ‣ 3 Synthesizing face images ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>).
While our experiments in <a href="#S4" title="4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section 4</span></a> focus on landmark and segmentation annotations, synthetics lets us easily create a variety of rich and accurate labels
that enable new face-related tasks (see <a href="#S4.SS5" title="4.5 Other examples ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection 4.5</span></a>).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Face analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate our synthetic data on two common face analysis tasks: face parsing and landmark localization.
We show that models trained on our synthetic data demonstrate competitive performance to the state of the art.
Note that all evaluations using our models are <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">cross-dataset</span> – we train purely on synthetic data and test on real data, while the state of the art evaluates <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">within-dataset</span>, allowing the models to learn potential biases in the data.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training methodology</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We render a <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">single</span> training dataset for both landmark localization and face parsing, comprising 100,000 images at 512<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation></semantics></math>512 resolution.
It took 48 hours to render using 150 NVIDIA M60 GPUs.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">During training, we perform data augmentation including
rotations, perspective warps, blurs, modulations to brightness and contrast, addition of noise, and conversion to grayscale. Such augmentations
are especially important for synthetic images which are otherwise free of imperfection (see <a href="#S4.SS4" title="4.4 Ablation studies ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection 4.4</span></a>).
While some of these could be done at render time, we perform them at training time in order to randomly apply different augmentations to the same training image.
We implemented neural networks with PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, and trained them with the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Face parsing</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Face parsing assigns a class label to each pixel in an image, e.g. skin, eyes, mouth, or nose.
We evaluate our synthetic training data on two face parsing datasets:
<span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Helen</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> is the best-known benchmark in the literature.
It contains 2,000 training images, 230 validation images, and 100 testing images,
each with 11 classes.
Due to labelling errors in the original dataset, we use Helen* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>, a popular rectified version of the dataset which features corrected training labels, but leaves testing labels unmodified for a fair comparison.
<span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">LaPa</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> is a recently-released dataset which uses the same labels as Helen, but has more images, and exhibits more challenging expressions, poses, and occlusions.
It contains 18,176 training images, 2,000 validation images and 2,000 testing images.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">As is common <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, we use the provided 2D landmarks to align faces before processing.
We scale and crop each image so the landmarks are centered in a <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="512\!\times\!512" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">512</mn><mo lspace="0.052em" rspace="0.052em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">512</cn><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">512\!\times\!512</annotation></semantics></math>px region of interest.
Following prediction, we undo this transform to compute results
against the original label annotation, without any resizing or cropping.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.6" class="ltx_p"><span id="S4.SS2.p3.6.1" class="ltx_text ltx_font_bold">Method</span>
We treat face parsing as image-to-image translation.
Given an input color image <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">x</annotation></semantics></math> containing <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">C</annotation></semantics></math> classes, we wish to predict a <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">C</annotation></semantics></math>-channel label image <math id="S4.SS2.p3.4.m4.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S4.SS2.p3.4.m4.1a"><mover accent="true" id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml"><mi id="S4.SS2.p3.4.m4.1.1.2" xref="S4.SS2.p3.4.m4.1.1.2.cmml">y</mi><mo id="S4.SS2.p3.4.m4.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1"><ci id="S4.SS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1">^</ci><ci id="S4.SS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">\hat{y}</annotation></semantics></math> of the same spatial dimensions that matches the ground truth label image <math id="S4.SS2.p3.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p3.5.m5.1a"><mi id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><ci id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">y</annotation></semantics></math>.
Pixels in <math id="S4.SS2.p3.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p3.6.m6.1a"><mi id="S4.SS2.p3.6.m6.1.1" xref="S4.SS2.p3.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><ci id="S4.SS2.p3.6.m6.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">y</annotation></semantics></math> are one-hot encoded with the index of the true class.
For this, we use a UNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> with ResNet-18 encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>.
We train this network with synthetic data only, minimizing a binary cross-entropy (BCE) loss between predicted and ground truth label images.
Note that there is nothing novel about our choice of architecture or loss function, this is a well-understood approach for this task.
</p>
</div>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2109.15102/assets/x2.png" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>We train a face parsing network (using synthetic data only) followed by a label adaptation network to address systematic differences between synthetic and human-annotated labels.</figcaption>
</figure>
<figure id="S4.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.F14.1" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F14.1.1.1" class="ltx_tr">
<td id="S4.F14.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.1.1.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Input</span></span>
</span>
</td>
<td id="S4.F14.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.1.1.2.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Trained with</span></span>
</span>
</td>
<td id="S4.F14.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.1.1.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">+ label</span></span>
</span>
</td>
<td id="S4.F14.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.1.1.4.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">Trained with</span></span>
</span>
</td>
<td id="S4.F14.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.1.1.5.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.1.1.5.1.1.1" class="ltx_text" style="font-size:80%;">Ground</span></span>
</span>
</td>
</tr>
<tr id="S4.F14.1.2.2" class="ltx_tr">
<td id="S4.F14.1.2.2.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.2.2.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.2.2.1.1.1.1" class="ltx_text" style="font-size:80%;">(LaPa)</span></span>
</span>
</td>
<td id="S4.F14.1.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.2.2.2.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">synth. data</span></span>
</span>
</td>
<td id="S4.F14.1.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.2.2.3.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.2.2.3.1.1.1" class="ltx_text" style="font-size:80%;">adaptation</span></span>
</span>
</td>
<td id="S4.F14.1.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.2.2.4.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">real data</span></span>
</span>
</td>
<td id="S4.F14.1.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:-1.2pt;padding-bottom:-1.2pt;">
<span id="S4.F14.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F14.1.2.2.5.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.F14.1.2.2.5.1.1.1" class="ltx_text" style="font-size:80%;">truth</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/face_parsing/fp_fig_lapa_1815.jpg" id="S4.F14.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/face_parsing/fp_fig_lapa_0825.jpg" id="S4.F14.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/face_parsing/fp_fig_lapa_0193.jpg" id="S4.F14.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="120" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Face parsing results by networks trained with synthetic data (with and without label adaptation) and real data. Label adaptation addresses systematic differences between synthetic and real labels, e.g. the shape of the nose class, or granularity of hair.</figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.2" class="ltx_p"><span id="S4.SS2.p4.2.1" class="ltx_text ltx_font_bold">Label adaptation.</span>
There are bound to be minor systematic differences between synthetic labels and human-annotated labels.
For example, where exactly is the boundary between the nose and the rest of the face?
To evaluate our synthetic data without needing to carefully tweak our synthetic label generation process for a specific real dataset, we use <span id="S4.SS2.p4.2.2" class="ltx_text ltx_font_italic">label adaptation</span>.
Label adaptation transforms labels predicted by our face parsing network (trained with synthetic data alone) into labels that are closer to the distribution in the real dataset (see <a href="#S4.F13" title="Figure 13 ‣ 4.2 Face parsing ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 13</span></a>).
We treat label adaptation as another image-to-image translation task, and use a UNet with ResNet18 encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>.
To ensure this stage is not able to ‘cheat’, it is trained only on pairs of predicted labels <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mover accent="true" id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">y</mi><mo id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><ci id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1">^</ci><ci id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\hat{y}</annotation></semantics></math> and ground truth labels <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mi id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><ci id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">y</annotation></semantics></math>.
It is trained entirely separately from the face parsing network, and never sees any real images.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Results</span>
See Tables <a href="#S4.T1" title="Table 1 ‣ 4.2 Face parsing ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T2" title="Table 2 ‣ 4.2 Face parsing ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for comparisons against the state of the art, and <a href="#S4.F14" title="Figure 14 ‣ 4.2 Face parsing ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 14</span></a> for some example predictions.
Although networks trained with our generic synthetic data do not outperform the state of the art, it is notable that they achieve similar results to previous work trained within-dataset on task-specific data.
</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Comparison to real data.</span>
We also trained a network on the training portion of each real dataset to separate our training methodology from our synthetic data, presented as “Ours (real)” in Tables <a href="#S4.T1" title="Table 1 ‣ 4.2 Face parsing ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T2" title="Table 2 ‣ 4.2 Face parsing ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It can be seen that training with synthetic data alone produces comparable results to training with real data.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>A comparison with the state of the art on the Helen dataset, using F<sub id="S4.T1.2.1" class="ltx_sub">1</sub> score. As is common, scores for hair and other fine-grained categories are omitted to aid comparison to previous work. The overall score is computed by merging the nose, brows, eyes, and mouth categories. Training with our synthetic data achieves results in line with the state of the art, trained with real data.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S4.T1.3.1.1.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<td id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Skin</span></td>
<td id="S4.T1.3.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.4.1" class="ltx_text" style="font-size:90%;">Nose</span></td>
<td id="S4.T1.3.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.5.1" class="ltx_text" style="font-size:90%;">Upper lip</span></td>
<td id="S4.T1.3.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.6.1" class="ltx_text" style="font-size:90%;">Inner mouth</span></td>
<td id="S4.T1.3.1.1.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.7.1" class="ltx_text" style="font-size:90%;">Lower lip</span></td>
<td id="S4.T1.3.1.1.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.8.1" class="ltx_text" style="font-size:90%;">Brows</span></td>
<td id="S4.T1.3.1.1.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.9.1" class="ltx_text" style="font-size:90%;">Eyes</span></td>
<td id="S4.T1.3.1.1.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.1.1.10.1" class="ltx_text" style="font-size:90%;">Mouth</span></td>
<td id="S4.T1.3.1.1.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.3.1.1.11.1" class="ltx_text" style="font-size:90%;">Overall</span></td>
</tr>
<tr id="S4.T1.3.2.2" class="ltx_tr">
<th id="S4.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Guo et al.</span> <span id="S4.T1.3.2.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a><span id="S4.T1.3.2.2.1.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T1.3.2.2.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.3.2.2.2.1" class="ltx_text" style="font-size:80%;">AAAI’18</span></th>
<td id="S4.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.3.1" class="ltx_text" style="font-size:90%;">93.8</span></td>
<td id="S4.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.4.1" class="ltx_text" style="font-size:90%;">94.1</span></td>
<td id="S4.T1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.5.1" class="ltx_text" style="font-size:90%;">75.8</span></td>
<td id="S4.T1.3.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.6.1" class="ltx_text" style="font-size:90%;">83.7</span></td>
<td id="S4.T1.3.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.7.1" class="ltx_text" style="font-size:90%;">83.1</span></td>
<td id="S4.T1.3.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.8.1" class="ltx_text" style="font-size:90%;">80.4</span></td>
<td id="S4.T1.3.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.9.1" class="ltx_text" style="font-size:90%;">87.1</span></td>
<td id="S4.T1.3.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.10.1" class="ltx_text" style="font-size:90%;">92.4</span></td>
<td id="S4.T1.3.2.2.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.3.2.2.11.1" class="ltx_text" style="font-size:90%;">90.5</span></td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wei et al.</span> <span id="S4.T1.3.3.3.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a><span id="S4.T1.3.3.3.1.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T1.3.3.3.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.3.3.2.1" class="ltx_text" style="font-size:80%;">TIP’19</span></th>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.3.1" class="ltx_text" style="font-size:90%;">95.6</span></td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.4.1" class="ltx_text" style="font-size:90%;">95.2</span></td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.5.1" class="ltx_text" style="font-size:90%;">80.0</span></td>
<td id="S4.T1.3.3.3.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.6.1" class="ltx_text" style="font-size:90%;">86.7</span></td>
<td id="S4.T1.3.3.3.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.7.1" class="ltx_text" style="font-size:90%;">86.4</span></td>
<td id="S4.T1.3.3.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.8.1" class="ltx_text" style="font-size:90%;">82.6</span></td>
<td id="S4.T1.3.3.3.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.9.1" class="ltx_text" style="font-size:90%;">89.0</span></td>
<td id="S4.T1.3.3.3.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.3.10.1" class="ltx_text" style="font-size:90%;">93.6</span></td>
<td id="S4.T1.3.3.3.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.3.3.3.11.1" class="ltx_text" style="font-size:90%;">91.6</span></td>
</tr>
<tr id="S4.T1.3.4.4" class="ltx_tr">
<th id="S4.T1.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Lin et al.</span> <span id="S4.T1.3.4.4.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a><span id="S4.T1.3.4.4.1.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T1.3.4.4.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.4.4.2.1" class="ltx_text" style="font-size:80%;">CVPR’19</span></th>
<td id="S4.T1.3.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.3.1" class="ltx_text" style="font-size:90%;">94.5</span></td>
<td id="S4.T1.3.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.4.1" class="ltx_text" style="font-size:90%;">95.6</span></td>
<td id="S4.T1.3.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.5.1" class="ltx_text" style="font-size:90%;">79.6</span></td>
<td id="S4.T1.3.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.6.1" class="ltx_text" style="font-size:90%;">86.7</span></td>
<td id="S4.T1.3.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.7.1" class="ltx_text" style="font-size:90%;">89.8</span></td>
<td id="S4.T1.3.4.4.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.8.1" class="ltx_text" style="font-size:90%;">83.1</span></td>
<td id="S4.T1.3.4.4.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.9.1" class="ltx_text" style="font-size:90%;">89.6</span></td>
<td id="S4.T1.3.4.4.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.10.1" class="ltx_text" style="font-size:90%;">95.0</span></td>
<td id="S4.T1.3.4.4.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.3.4.4.11.1" class="ltx_text" style="font-size:90%;">92.4</span></td>
</tr>
<tr id="S4.T1.3.5.5" class="ltx_tr">
<th id="S4.T1.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Liu et al.</span> <span id="S4.T1.3.5.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a><span id="S4.T1.3.5.5.1.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T1.3.5.5.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.5.5.2.1" class="ltx_text" style="font-size:80%;">AAAI’20</span></th>
<td id="S4.T1.3.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.3.1" class="ltx_text" style="font-size:90%;">94.9</span></td>
<td id="S4.T1.3.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.4.1" class="ltx_text" style="font-size:90%;">95.8</span></td>
<td id="S4.T1.3.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.5.1" class="ltx_text" style="font-size:90%;">83.7</span></td>
<td id="S4.T1.3.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.6.1" class="ltx_text" style="font-size:90%;">89.1</span></td>
<td id="S4.T1.3.5.5.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.7.1" class="ltx_text" style="font-size:90%;">91.4</span></td>
<td id="S4.T1.3.5.5.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.8.1" class="ltx_text" style="font-size:90%;">83.5</span></td>
<td id="S4.T1.3.5.5.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.9.1" class="ltx_text" style="font-size:90%;">89.8</span></td>
<td id="S4.T1.3.5.5.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.5.10.1" class="ltx_text" style="font-size:90%;">96.1</span></td>
<td id="S4.T1.3.5.5.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.3.5.5.11.1" class="ltx_text" style="font-size:90%;">93.1</span></td>
</tr>
<tr id="S4.T1.3.6.6" class="ltx_tr">
<th id="S4.T1.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Te et al.</span> <span id="S4.T1.3.6.6.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a><span id="S4.T1.3.6.6.1.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T1.3.6.6.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.6.6.2.1" class="ltx_text" style="font-size:80%;">ECCV’20</span></th>
<td id="S4.T1.3.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.3.1" class="ltx_text" style="font-size:90%;">94.6</span></td>
<td id="S4.T1.3.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.4.1" class="ltx_text" style="font-size:90%;">96.1</span></td>
<td id="S4.T1.3.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.5.1" class="ltx_text" style="font-size:90%;">83.6</span></td>
<td id="S4.T1.3.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.6.1" class="ltx_text" style="font-size:90%;">89.8</span></td>
<td id="S4.T1.3.6.6.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.7.1" class="ltx_text" style="font-size:90%;">91.0</span></td>
<td id="S4.T1.3.6.6.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.8.1" class="ltx_text" style="font-size:90%;">90.2</span></td>
<td id="S4.T1.3.6.6.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.9.1" class="ltx_text" style="font-size:90%;">84.9</span></td>
<td id="S4.T1.3.6.6.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.6.10.1" class="ltx_text" style="font-size:90%;">95.5</span></td>
<td id="S4.T1.3.6.6.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.3.6.6.11.1" class="ltx_text" style="font-size:90%;">93.2</span></td>
</tr>
<tr id="S4.T1.3.7.7" class="ltx_tr">
<th id="S4.T1.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.3.7.7.1.1" class="ltx_text" style="font-size:90%;">Ours (real)</span></th>
<th id="S4.T1.3.7.7.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.3.1" class="ltx_text" style="font-size:90%;">95.1</span></td>
<td id="S4.T1.3.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.4.1" class="ltx_text" style="font-size:90%;">94.7</span></td>
<td id="S4.T1.3.7.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.5.1" class="ltx_text" style="font-size:90%;">81.6</span></td>
<td id="S4.T1.3.7.7.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.6.1" class="ltx_text" style="font-size:90%;">87.0</span></td>
<td id="S4.T1.3.7.7.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.7.1" class="ltx_text" style="font-size:90%;">88.9</span></td>
<td id="S4.T1.3.7.7.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.8.1" class="ltx_text" style="font-size:90%;">81.5</span></td>
<td id="S4.T1.3.7.7.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.9.1" class="ltx_text" style="font-size:90%;">87.6</span></td>
<td id="S4.T1.3.7.7.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.10.1" class="ltx_text" style="font-size:90%;">94.8</span></td>
<td id="S4.T1.3.7.7.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.3.7.7.11.1" class="ltx_text" style="font-size:90%;">91.6</span></td>
</tr>
<tr id="S4.T1.3.8.8" class="ltx_tr">
<th id="S4.T1.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.8.8.1.1" class="ltx_text" style="font-size:90%;">Ours (synthetic)</span></th>
<th id="S4.T1.3.8.8.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<td id="S4.T1.3.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.3.1" class="ltx_text" style="font-size:90%;">95.1</span></td>
<td id="S4.T1.3.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.4.1" class="ltx_text" style="font-size:90%;">94.5</span></td>
<td id="S4.T1.3.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.5.1" class="ltx_text" style="font-size:90%;">82.3</span></td>
<td id="S4.T1.3.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.6.1" class="ltx_text" style="font-size:90%;">89.1</span></td>
<td id="S4.T1.3.8.8.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.7.1" class="ltx_text" style="font-size:90%;">89.9</span></td>
<td id="S4.T1.3.8.8.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.8.1" class="ltx_text" style="font-size:90%;">83.5</span></td>
<td id="S4.T1.3.8.8.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.9.1" class="ltx_text" style="font-size:90%;">87.3</span></td>
<td id="S4.T1.3.8.8.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.10.1" class="ltx_text" style="font-size:90%;">95.1</span></td>
<td id="S4.T1.3.8.8.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.3.8.8.11.1" class="ltx_text" style="font-size:90%;">92.0</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>A comparison with the state of the art on LaPa, using F<sub id="S4.T2.2.1" class="ltx_sub">1</sub> score. For eyes and brows, L and R are left and right. For lips, U, I, and L are upper, inner, and lower. Training with our synthetic data achieves results in line with the state of the art, trained with real data.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S4.T2.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S4.T2.3.1.1.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Skin</span></th>
<th id="S4.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.4.1" class="ltx_text" style="font-size:90%;">Hair</span></th>
<th id="S4.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.5.1" class="ltx_text" style="font-size:90%;">L-eye</span></th>
<th id="S4.T2.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.6.1" class="ltx_text" style="font-size:90%;">R-eye</span></th>
<th id="S4.T2.3.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.7.1" class="ltx_text" style="font-size:90%;">U-lip</span></th>
<th id="S4.T2.3.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.8.1" class="ltx_text" style="font-size:90%;">I-mouth</span></th>
<th id="S4.T2.3.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.9.1" class="ltx_text" style="font-size:90%;">L-lip</span></th>
<th id="S4.T2.3.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.10.1" class="ltx_text" style="font-size:90%;">Nose</span></th>
<th id="S4.T2.3.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.11.1" class="ltx_text" style="font-size:90%;">L-Brow</span></th>
<th id="S4.T2.3.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.12.1" class="ltx_text" style="font-size:90%;">R-Brow</span></th>
<th id="S4.T2.3.1.1.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.1.1.13.1" class="ltx_text" style="font-size:90%;">Mean</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.2.1" class="ltx_tr">
<th id="S4.T2.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Liu et al.</span> <span id="S4.T2.3.2.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a><span id="S4.T2.3.2.1.1.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T2.3.2.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.3.2.1.2.1" class="ltx_text" style="font-size:80%;">AAAI’20</span></th>
<td id="S4.T2.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.3.1" class="ltx_text" style="font-size:90%;">97.2</span></td>
<td id="S4.T2.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.4.1" class="ltx_text" style="font-size:90%;">96.3</span></td>
<td id="S4.T2.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.5.1" class="ltx_text" style="font-size:90%;">88.1</span></td>
<td id="S4.T2.3.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.6.1" class="ltx_text" style="font-size:90%;">88.0</span></td>
<td id="S4.T2.3.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.7.1" class="ltx_text" style="font-size:90%;">84.4</span></td>
<td id="S4.T2.3.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.8.1" class="ltx_text" style="font-size:90%;">87.6</span></td>
<td id="S4.T2.3.2.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.9.1" class="ltx_text" style="font-size:90%;">85.7</span></td>
<td id="S4.T2.3.2.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.10.1" class="ltx_text" style="font-size:90%;">95.5</span></td>
<td id="S4.T2.3.2.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.11.1" class="ltx_text" style="font-size:90%;">87.7</span></td>
<td id="S4.T2.3.2.1.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.12.1" class="ltx_text" style="font-size:90%;">87.6</span></td>
<td id="S4.T2.3.2.1.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.3.2.1.13.1" class="ltx_text" style="font-size:90%;">89.8</span></td>
</tr>
<tr id="S4.T2.3.3.2" class="ltx_tr">
<th id="S4.T2.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Te et al.</span> <span id="S4.T2.3.3.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a><span id="S4.T2.3.3.2.1.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T2.3.3.2.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.3.3.2.2.1" class="ltx_text" style="font-size:80%;">ECCV’20</span></th>
<td id="S4.T2.3.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.3.1" class="ltx_text" style="font-size:90%;">97.3</span></td>
<td id="S4.T2.3.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.4.1" class="ltx_text" style="font-size:90%;">96.2</span></td>
<td id="S4.T2.3.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.5.1" class="ltx_text" style="font-size:90%;">89.5</span></td>
<td id="S4.T2.3.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.6.1" class="ltx_text" style="font-size:90%;">90.0</span></td>
<td id="S4.T2.3.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.7.1" class="ltx_text" style="font-size:90%;">88.1</span></td>
<td id="S4.T2.3.3.2.8" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.8.1" class="ltx_text" style="font-size:90%;">90.0</span></td>
<td id="S4.T2.3.3.2.9" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.9.1" class="ltx_text" style="font-size:90%;">89.0</span></td>
<td id="S4.T2.3.3.2.10" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.10.1" class="ltx_text" style="font-size:90%;">97.1</span></td>
<td id="S4.T2.3.3.2.11" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.11.1" class="ltx_text" style="font-size:90%;">86.5</span></td>
<td id="S4.T2.3.3.2.12" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.2.12.1" class="ltx_text" style="font-size:90%;">87.0</span></td>
<td id="S4.T2.3.3.2.13" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.3.3.2.13.1" class="ltx_text" style="font-size:90%;">91.1</span></td>
</tr>
<tr id="S4.T2.3.4.3" class="ltx_tr">
<th id="S4.T2.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.3.4.3.1.1" class="ltx_text" style="font-size:90%;">Ours (real)</span></th>
<th id="S4.T2.3.4.3.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T2.3.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.3.1" class="ltx_text" style="font-size:90%;">97.5</span></td>
<td id="S4.T2.3.4.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.4.1" class="ltx_text" style="font-size:90%;">86.9</span></td>
<td id="S4.T2.3.4.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.5.1" class="ltx_text" style="font-size:90%;">91.4</span></td>
<td id="S4.T2.3.4.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.6.1" class="ltx_text" style="font-size:90%;">91.5</span></td>
<td id="S4.T2.3.4.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.7.1" class="ltx_text" style="font-size:90%;">87.3</span></td>
<td id="S4.T2.3.4.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.8.1" class="ltx_text" style="font-size:90%;">89.8</span></td>
<td id="S4.T2.3.4.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.9.1" class="ltx_text" style="font-size:90%;">89.4</span></td>
<td id="S4.T2.3.4.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.10.1" class="ltx_text" style="font-size:90%;">96.9</span></td>
<td id="S4.T2.3.4.3.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.11.1" class="ltx_text" style="font-size:90%;">89.3</span></td>
<td id="S4.T2.3.4.3.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.12.1" class="ltx_text" style="font-size:90%;">89.3</span></td>
<td id="S4.T2.3.4.3.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.3.4.3.13.1" class="ltx_text" style="font-size:90%;">90.9</span></td>
</tr>
<tr id="S4.T2.3.5.4" class="ltx_tr">
<th id="S4.T2.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.3.5.4.1.1" class="ltx_text" style="font-size:90%;">Ours (synthetic)</span></th>
<th id="S4.T2.3.5.4.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<td id="S4.T2.3.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.3.1" class="ltx_text" style="font-size:90%;">97.1</span></td>
<td id="S4.T2.3.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.4.1" class="ltx_text" style="font-size:90%;">85.7</span></td>
<td id="S4.T2.3.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.5.1" class="ltx_text" style="font-size:90%;">90.6</span></td>
<td id="S4.T2.3.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.6.1" class="ltx_text" style="font-size:90%;">90.1</span></td>
<td id="S4.T2.3.5.4.7" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.7.1" class="ltx_text" style="font-size:90%;">85.9</span></td>
<td id="S4.T2.3.5.4.8" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.8.1" class="ltx_text" style="font-size:90%;">88.8</span></td>
<td id="S4.T2.3.5.4.9" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.9.1" class="ltx_text" style="font-size:90%;">88.4</span></td>
<td id="S4.T2.3.5.4.10" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.10.1" class="ltx_text" style="font-size:90%;">96.7</span></td>
<td id="S4.T2.3.5.4.11" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.11.1" class="ltx_text" style="font-size:90%;">88.6</span></td>
<td id="S4.T2.3.5.4.12" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.4.12.1" class="ltx_text" style="font-size:90%;">88.5</span></td>
<td id="S4.T2.3.5.4.13" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.3.5.4.13.1" class="ltx_text" style="font-size:90%;">90.1</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Landmark localization</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Landmark localization finds the position of facial points of interest in 2D.
We evaluate our approach on the <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">300W</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> dataset, which is split into common (554 images), challenging (135 images) and private (600 images) subsets.</p>
</div>
<figure id="S4.F15" class="ltx_figure">
<table id="S4.F15.12" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F15.6.6" class="ltx_tr">
<td id="S4.F15.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/1168_bef_adapt.jpg" id="S4.F15.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/593_bef_adapt.jpg" id="S4.F15.2.2.2.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/607_bef_adapt.jpg" id="S4.F15.3.3.3.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/1062_bef_adapt.jpg" id="S4.F15.4.4.4.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/885_bef_adapt.jpg" id="S4.F15.5.5.5.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/1071_bef_adapt.jpg" id="S4.F15.6.6.6.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
</tr>
<tr id="S4.F15.12.12" class="ltx_tr">
<td id="S4.F15.7.7.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/1168_aft_adapt.jpg" id="S4.F15.7.7.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/593_aft_adapt.jpg" id="S4.F15.8.8.2.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/607_aft_adapt.jpg" id="S4.F15.9.9.3.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/1062_aft_adapt.jpg" id="S4.F15.10.10.4.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/885_aft_adapt.jpg" id="S4.F15.11.11.5.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F15.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/adaptation/1071_aft_adapt.jpg" id="S4.F15.12.12.6.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Predictions before (top) and after (bottom) label adaptation. The main difference is changing the jawline from a 3D-to-2D projection to instead follow the facial outline in the image.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Method</span>
We train a ResNet34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> with mean squared error loss to directly predict 68 2D landmark coordinates per-image.
We use the provided bounding boxes to extract a <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">256\!\times\!256</annotation></semantics></math> pixel region-of-interest from each image.
The private set has no bounding boxes, so we use a tight crop around landmarks.</p>
</div>
<figure id="S4.F16" class="ltx_figure">
<table id="S4.F16.12" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F16.6.6" class="ltx_tr">
<td id="S4.F16.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/1140_real.jpg" id="S4.F16.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/1118_real.jpg" id="S4.F16.2.2.2.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/765_real.jpg" id="S4.F16.3.3.3.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/565_real.jpg" id="S4.F16.4.4.4.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/574_real.jpg" id="S4.F16.5.5.5.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/634_real.jpg" id="S4.F16.6.6.6.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
</tr>
<tr id="S4.F16.12.12" class="ltx_tr">
<td id="S4.F16.7.7.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/1140_aft_adapt.jpg" id="S4.F16.7.7.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/1118_aft_adapt.jpg" id="S4.F16.8.8.2.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/765_aft_adapt.jpg" id="S4.F16.9.9.3.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/565_aft_adapt.jpg" id="S4.F16.10.10.4.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/574_aft_adapt.jpg" id="S4.F16.11.11.5.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
<td id="S4.F16.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-top:-2.5pt;padding-bottom:-2.5pt;"><img src="/html/2109.15102/assets/figures/landmarks/real_vs_synth/634_aft_adapt.jpg" id="S4.F16.12.12.6.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Predictions by networks trained with real (top) and synthetic data (bottom). Note how the synthetic data network generalizes better across expression, illumination, pose, and occlusion.</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Label adaptation</span> is performed using a two-layer perceptron to address systematic differences between synthetic and real landmark labels (<a href="#S4.F15" title="Figure 15 ‣ 4.3 Landmark localization ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 15</span></a>).
This network is never exposed to any real images during training.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Landmark localization results on the common, challenging, and private subsets of 300W. Lower is better in all cases. Note that 0.5 FR rate translates to 3 images, while 0.17 corresponds to 1.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T3.1.2.1.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Common</span></td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.1.4.1" class="ltx_text" style="font-size:90%;">Challenging</span></td>
<td id="S4.T3.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.2.1.5.1" class="ltx_text" style="font-size:90%;">Private</span></td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<th id="S4.T3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.1.2.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S4.T3.1.1.3" class="ltx_td ltx_nopad_l ltx_th ltx_th_row"></th>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.1" class="ltx_text" style="font-size:90%;">NME</span></td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.1" class="ltx_text" style="font-size:90%;">NME</span></td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T3.1.1.1.1" class="ltx_text" style="font-size:90%;">FR</span><sub id="S4.T3.1.1.1.2" class="ltx_sub"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">10%</span></sub>
</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.1.3.2.1.1" class="ltx_text" style="font-size:90%;">DenseReg </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a><span id="S4.T3.1.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T3.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.3.2.2.1" class="ltx_text" style="font-size:80%;">CVPR’17</span></th>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.2.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.2.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T3.1.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.3.2.5.1" class="ltx_text" style="font-size:90%;">3.67</span></td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.4.3.1.1" class="ltx_text" style="font-size:90%;">LAB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a><span id="S4.T3.1.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T3.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">CVPR’18</span></th>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.3.3.1" class="ltx_text" style="font-size:90%;">2.98</span></td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.3.4.1" class="ltx_text" style="font-size:90%;">5.19</span></td>
<td id="S4.T3.1.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.4.3.5.1" class="ltx_text" style="font-size:90%;">0.83</span></td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.5.4.1.1" class="ltx_text" style="font-size:90%;">AWING </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a><span id="S4.T3.1.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T3.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.5.4.2.1" class="ltx_text" style="font-size:80%;">ICCV’19</span></th>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.4.3.1" class="ltx_text" style="font-size:90%;">2.72</span></td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.4.4.1" class="ltx_text" style="font-size:90%;">4.52</span></td>
<td id="S4.T3.1.5.4.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.5.4.5.1" class="ltx_text" style="font-size:90%;">0.33</span></td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<th id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.6.5.1.1" class="ltx_text" style="font-size:90%;">ODN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.6.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a><span id="S4.T3.1.6.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T3.1.6.5.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.6.5.2.1" class="ltx_text" style="font-size:80%;">CVPR’19</span></th>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.5.3.1" class="ltx_text" style="font-size:90%;">3.56</span></td>
<td id="S4.T3.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.5.4.1" class="ltx_text" style="font-size:90%;">6.67</span></td>
<td id="S4.T3.1.6.5.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.6.5.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<th id="S4.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.7.6.1.1" class="ltx_text" style="font-size:90%;">LaplaceKL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.7.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a><span id="S4.T3.1.7.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T3.1.7.6.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.7.6.2.1" class="ltx_text" style="font-size:80%;">ICCV’19</span></th>
<td id="S4.T3.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.6.3.1" class="ltx_text" style="font-size:90%;">3.19</span></td>
<td id="S4.T3.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.6.4.1" class="ltx_text" style="font-size:90%;">6.87</span></td>
<td id="S4.T3.1.7.6.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.7.6.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S4.T3.1.8.7" class="ltx_tr">
<th id="S4.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.8.7.1.1" class="ltx_text" style="font-size:90%;">3FabRec </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.8.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a><span id="S4.T3.1.8.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T3.1.8.7.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.8.7.2.1" class="ltx_text" style="font-size:80%;">CVPR’20</span></th>
<td id="S4.T3.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.7.3.1" class="ltx_text" style="font-size:90%;">3.36</span></td>
<td id="S4.T3.1.8.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.7.4.1" class="ltx_text" style="font-size:90%;">5.74</span></td>
<td id="S4.T3.1.8.7.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.8.7.5.1" class="ltx_text" style="font-size:90%;">0.17</span></td>
</tr>
<tr id="S4.T3.1.9.8" class="ltx_tr">
<th id="S4.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.9.8.1.1" class="ltx_text" style="font-size:90%;">Ours (real)</span></th>
<th id="S4.T3.1.9.8.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T3.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.9.8.3.1" class="ltx_text" style="font-size:90%;">3.37</span></td>
<td id="S4.T3.1.9.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.9.8.4.1" class="ltx_text" style="font-size:90%;">5.77</span></td>
<td id="S4.T3.1.9.8.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.9.8.5.1" class="ltx_text" style="font-size:90%;">1.17</span></td>
</tr>
<tr id="S4.T3.1.10.9" class="ltx_tr">
<th id="S4.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-bottom:3.87498pt;"><span id="S4.T3.1.10.9.1.1" class="ltx_text" style="font-size:90%;">Ours (synthetic)</span></th>
<th id="S4.T3.1.10.9.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row" style="padding-bottom:3.87498pt;"></th>
<td id="S4.T3.1.10.9.3" class="ltx_td ltx_align_center" style="padding-bottom:3.87498pt;"><span id="S4.T3.1.10.9.3.1" class="ltx_text" style="font-size:90%;">3.09</span></td>
<td id="S4.T3.1.10.9.4" class="ltx_td ltx_align_center" style="padding-bottom:3.87498pt;"><span id="S4.T3.1.10.9.4.1" class="ltx_text" style="font-size:90%;">4.86</span></td>
<td id="S4.T3.1.10.9.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-bottom:3.87498pt;"><span id="S4.T3.1.10.9.5.1" class="ltx_text" style="font-size:90%;">0.50</span></td>
</tr>
<tr id="S4.T3.1.11.10" class="ltx_tr">
<th id="S4.T3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="5"><span id="S4.T3.1.11.10.1.1" class="ltx_text" style="font-size:90%;">Ablation studies</span></th>
</tr>
<tr id="S4.T3.1.12.11" class="ltx_tr">
<th id="S4.T3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><span id="S4.T3.1.12.11.1.1" class="ltx_text" style="font-size:90%;">No augmentation</span></th>
<td id="S4.T3.1.12.11.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.12.11.2.1" class="ltx_text" style="font-size:90%;">4.25</span></td>
<td id="S4.T3.1.12.11.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.12.11.3.1" class="ltx_text" style="font-size:90%;">7.87</span></td>
<td id="S4.T3.1.12.11.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.12.11.4.1" class="ltx_text" style="font-size:90%;">4.00</span></td>
</tr>
<tr id="S4.T3.1.13.12" class="ltx_tr">
<th id="S4.T3.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="S4.T3.1.13.12.1.1" class="ltx_text" style="font-size:90%;">Appearance augmentation</span></th>
<td id="S4.T3.1.13.12.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.12.2.1" class="ltx_text" style="font-size:90%;">3.93</span></td>
<td id="S4.T3.1.13.12.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.12.3.1" class="ltx_text" style="font-size:90%;">6.80</span></td>
<td id="S4.T3.1.13.12.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.13.12.4.1" class="ltx_text" style="font-size:90%;">1.83</span></td>
</tr>
<tr id="S4.T3.1.14.13" class="ltx_tr">
<th id="S4.T3.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><span id="S4.T3.1.14.13.1.1" class="ltx_text" style="font-size:90%;">No hair or clothing</span></th>
<td id="S4.T3.1.14.13.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.14.13.2.1" class="ltx_text" style="font-size:90%;">3.36</span></td>
<td id="S4.T3.1.14.13.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.14.13.3.1" class="ltx_text" style="font-size:90%;">5.37</span></td>
<td id="S4.T3.1.14.13.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.14.13.4.1" class="ltx_text" style="font-size:90%;">2.17</span></td>
</tr>
<tr id="S4.T3.1.15.14" class="ltx_tr">
<th id="S4.T3.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="S4.T3.1.15.14.1.1" class="ltx_text" style="font-size:90%;">No clothing</span></th>
<td id="S4.T3.1.15.14.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.14.2.1" class="ltx_text" style="font-size:90%;">3.20</span></td>
<td id="S4.T3.1.15.14.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.14.3.1" class="ltx_text" style="font-size:90%;">5.09</span></td>
<td id="S4.T3.1.15.14.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.15.14.4.1" class="ltx_text" style="font-size:90%;">1.00</span></td>
</tr>
<tr id="S4.T3.1.16.15" class="ltx_tr">
<th id="S4.T3.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><span id="S4.T3.1.16.15.1.1" class="ltx_text" style="font-size:90%;">No label adaptation (synth.)</span></th>
<td id="S4.T3.1.16.15.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.16.15.2.1" class="ltx_text" style="font-size:90%;">5.61</span></td>
<td id="S4.T3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.16.15.3.1" class="ltx_text" style="font-size:90%;">8.43</span></td>
<td id="S4.T3.1.16.15.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.16.15.4.1" class="ltx_text" style="font-size:90%;">4.67</span></td>
</tr>
<tr id="S4.T3.1.17.16" class="ltx_tr">
<th id="S4.T3.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="S4.T3.1.17.16.1.1" class="ltx_text" style="font-size:90%;">No label adaptation (real)</span></th>
<td id="S4.T3.1.17.16.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.17.16.2.1" class="ltx_text" style="font-size:90%;">3.44</span></td>
<td id="S4.T3.1.17.16.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.17.16.3.1" class="ltx_text" style="font-size:90%;">5.71</span></td>
<td id="S4.T3.1.17.16.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.1.17.16.4.1" class="ltx_text" style="font-size:90%;">1.17</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.2" class="ltx_p"><span id="S4.SS3.p4.2.1" class="ltx_text ltx_font_bold">Results</span> As evaluation metrics we use: Normalized Mean Error (NME) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> – normalized by inter-ocular outer eye distance; and Failure Rate below a <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mrow id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mn id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS3.p4.1.m1.1.1.1" xref="S4.SS3.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">10\%</annotation></semantics></math> error threshold (FR<sub id="S4.SS3.p4.2.2" class="ltx_sub"><span id="S4.SS3.p4.2.2.1" class="ltx_text ltx_font_italic">10%</span></sub>).
See <a href="#S4.T3" title="Table 3 ‣ 4.3 Landmark localization ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> for comparisons against state of the art on 300W dataset.
It is clear that the network trained with our synthetic data can detect landmarks with accuracy comparable to recent methods trained with real data.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Comparison to real data</span>
We apply our training methodology (including data augmentations and label adaptation) to the the training and validation portions of the 300W dataset, to more directly compare real and synthetic data.
<a href="#S4.T3" title="Table 3 ‣ 4.3 Landmark localization ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> clearly shows that training with synthetic data leads to better results, even when comparing to a model trained on real data and evaluated within-dataset.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We investigate the effect of synthetic <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">dataset size</span> on landmark accuracy.
<a href="#S4.F17" title="Figure 17 ‣ 4.4 Ablation studies ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 17</span></a> shows that landmark localization improves as we increase the number of training images, before starting to plateau at 100,000 images.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We study the importance of <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">data augmentation</span> when training models on synthetic data.
We train models with:
1) no augmentation;
2) appearance augmentation only (e.g. colour shifts, brightness and contrast);
3) full augmentation, varying both appearance and geometry (e.g. rotation and warping).
<a href="#S4.T3" title="Table 3 ‣ 4.3 Landmark localization ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> shows the importance of augmentation, without which synthetic data does not outperform real.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><a href="#S4.T3" title="Table 3 ‣ 4.3 Landmark localization ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> also shows the importance of <span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">label adaptation</span> when evaluating models trained on synthetic data – using label adaptation to improve label consistency reduces error.
Adding label adaptation to a model trained on real data results in little change in performance, showing that it does not benefit already-consistent within-dataset labels.</p>
</div>
<figure id="S4.F17" class="ltx_figure"><img src="/html/2109.15102/assets/x3.png" id="S4.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Landmark localization accuracy improves as we use more and more synthetic training data.</figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">If we remove <span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">clothing and hair</span>, landmark accuracy suffers (<a href="#S4.T3" title="Table 3 ‣ 4.3 Landmark localization ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a>).
This verifies the importance of our hair library and digital wardrobe, which improve the realism of our data.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">Additional ablation studies analyzing the impact of render quality, and variation in pose, expression, and identity can be found in the supplementary material.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Other examples</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In addition to the quantitative results above, this section qualitatively demonstrates how we can solve additional problems using our synthetic face framework.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">Eye tracking</span>
can be a key feature for virtual or augmented reality devices,
but real training data can be difficult to acquire <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>.
Since our faces look realistic close-up, it is easy for us to set up a synthetic eye tracking camera and render diverse training images, along with ground truth.
<a href="#S4.F18" title="Figure 18 ‣ 4.5 Other examples ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 18</span></a> shows example synthetic training data for such a camera, along with results for semantic segmentation.</p>
</div>
<figure id="S4.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/et_grid_sx.jpg" id="S4.F18.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="296" height="167" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2109.15102/assets/figures/et_grid_real.jpg" id="S4.F18.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="296" height="167" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>It is easy to generate synthetic training data for eye tracking (left) which generalizes well to real-world images (right).</figcaption>
</figure>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p"><span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_bold">Dense landmarks.</span>
In <a href="#S4.SS3" title="4.3 Landmark localization ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection 4.3</span></a>, we presented results for localizing 68 facial landmarks.
What if we wanted to predict ten times as many landmarks?
It would be impossible for a human to annotate this many landmarks consistently and correctly.
However, our approach lets us easily generate accurate dense landmark labels.
<a href="#S4.F19" title="Figure 19 ‣ 4.5 Other examples ‣ 4 Face analysis ‣ Fake it till you make it: face analysis in the wild using synthetic data alone" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 19</span></a> shows the results of modifying our landmark network to regress 679 coordinates instead of 68, and training it with synthetic data.</p>
</div>
<figure id="S4.F19" class="ltx_figure"><img src="/html/2109.15102/assets/figures/landmarks/dense_landmarks_grid_fig.jpg" id="S4.F19.g1" class="ltx_graphics ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>With synthetic data, we can easily train models that accurately predict ten times as many landmarks as usual. Here are some example dense landmark predictions on the 300W dataset.</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Discussion</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">We have shown that it is possible to achieve results comparable with the state of the art for two well-trodden tasks: face parsing and landmark localization, without using a single real image during training.
This is important since it opens the door to many other face-related tasks that can be addressed using synthetic data in the place of real data.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">Limitations remain.
As our parametric face model includes the head and neck only, we cannot simulate clothing with low necklines.
We do not include expression-dependent wrinkling effects, so realism suffers during certain expressions.
Since we sample parts of our model independently, we sometimes get unusual (but not impossible) combinations,
such as feminine faces that have a beard.
We plan to address these limitations with future work.
</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">Photorealistic rendering is computationally expensive, so we must consider the environmental cost. In order to generate the dataset used in this paper, our GPU cluster used approximately 3,000kWh of electricity, equivalent to roughly 1.37 metric tonnes of CO<sub id="S4.SS6.p3.1.1" class="ltx_sub">2</sub>, 100% of which was offset by our cloud computing provider.
This impact is mitigated by the ongoing progress of cloud computing providers to become carbon negative and use renewable energy sources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>.
There is also the financial cost to consider.
Assuming $1 per hour for an M60 GPU (average price across cloud providers), it would cost $7,200 to render 100,000 images. Though this seems expensive, real data collection costs can run much higher, especially if we take annotation into consideration.</p>
</div>
<div id="S4.SS6.p4" class="ltx_para">
<p id="S4.SS6.p4.1" class="ltx_p"><span id="S4.SS6.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Acknowledgements<span id="S4.SS6.p4.1.1.1" class="ltx_text ltx_font_medium">
We thank Pedro Urbina, Jon Hanzelka, Rodney Brunet, and Panagiotis Giannakopoulos for their artistic contributions.
This work was published in ICCV 2021. The author list in the IEEE digital library is missing V.E. and M.J. due to a mistake on our side during the publishing process.</span></span></p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.4.4.1" class="ltx_text" style="font-size:90%;">Amazon [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">
Amazon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">Amazon climate pledge.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aboutamazon.com/planet/climate-pledge" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.aboutamazon.com/planet/climate-pledge</a><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Anderson et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
G. R. Anderson, M. J. Aftosmis, and M. Nemec.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Parametric Deformation of Discrete Geometry for Aerodynamic Shape
Design.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Aircraft</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Bak et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
S. Bak, P. Carr, and J.-F. Lalonde.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Domain Adaptation through Synthesis for Unsupervised Person
Re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Baltrušaitis et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
T. Baltrušaitis, P. Robinson, and L.-P. Morency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">3D Constrained Local Model for Rigid and Non-Rigid Facial Tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.4.4.1" class="ltx_text" style="font-size:90%;">Blanz and Vetter [1999]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">
V. Blanz and T. Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">A morphable model for the synthesis of 3d faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 26th annual conference on Computer
graphics and interactive techniques</em><span id="bib.bib5.10.3" class="ltx_text" style="font-size:90%;">, pages 187–194, 1999.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.4.4.1" class="ltx_text" style="font-size:90%;">Blender Foundation [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">
Blender Foundation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">Cycles renderer.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.cycles-renderer.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.cycles-renderer.org/</a><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.4.4.1" class="ltx_text" style="font-size:90%;">Browatzki and Wallraven [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">
B. Browatzki and C. Wallraven.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">3FabRec: Fast Few-shot Face alignment by Reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib7.10.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Chiang et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
M. J.-Y. Chiang, B. Bitterli, C. Tappan, and B. Burley.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">A practical and controllable hair and fur model for production path
tracing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Graphics Forum</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.4.4.1" class="ltx_text" style="font-size:90%;">Christensen [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">
P. H. Christensen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">An approximate reflectance profile for efficient subsurface
scattering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGGRAPH Talks</em><span id="bib.bib9.10.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.4.4.1" class="ltx_text" style="font-size:90%;">CLO Virtual Fashion Inc. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">
CLO Virtual Fashion Inc.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">Marvelous designer.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.marvelousdesigner.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.marvelousdesigner.com/</a><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">Debevec [2006]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
P. Debevec.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">Image-based lighting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGGRAPH Courses</em><span id="bib.bib11.10.3" class="ltx_text" style="font-size:90%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Gaidon et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
A. Gaidon, Q. Wang, Y. Cabon, and E. Vig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">VirtualWorlds as Proxy for Multi-object Tracking Analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Ganin et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
M. Marchand, and V. Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Domain-adversarial training of neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">JMLR</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Garbin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes, and S. S. Talathi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">OpenEDS: Open eye dataset.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.03702</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Garbin et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
S. J. Garbin, M. Kowalski, M. Johnson, and J. Shotton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">High resolution zero-shot domain adaptation of synthetically rendered
face images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Gecer et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
B. Gecer, B. Bhattarai, J. Kittler, and T.-K. Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Semi-supervised Adversarial Learning to Generate Photorealistic Face
Images of New Identities from 3D Morphable Model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Gerig et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
T. Gerig, A. Forster, C. Blumer, B. Egger, M. Lüthi, S. Schönborn,
and T. Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Morphable face models - an open framework.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Automatic Face and Gesture Recognition</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.4.4.1" class="ltx_text" style="font-size:90%;">Google [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">
Google.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">Google cloud sustainability.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://cloud.google.com/sustainability/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://cloud.google.com/sustainability/</a><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Guo et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
T. Guo, Y. Kim, H. Zhang, D. Qian, B. Yoo, J. Xu, D. Zou, J.-J. Han, and
C. Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Residual encoder decoder network and adaptive prior for face parsing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Güler et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
R. A. Güler, G. Trigeorgis, E. Antonakos, P. Snape, S. Zafeiriou, and
I. Kokkinos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild,
2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Deep Residual Learning for Image Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Hendler et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
D. Hendler, L. Moser, R. Battulwar, D. Corral, P. Cramer, R. Miller,
R. Cloudsdale, and D. Roble.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Avengers: Capturing Thanos’s Complex Face.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGGRAPH Talks</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Hodaň et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
T. Hodaň, V. Vineet, R. Gal, E. Shalev, J. Hanzelka, T. Connell,
P. Urbina, S. N. Sinha, and B. Guenter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Photorealistic image synthesis for object instance detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE International Conference on Image Processing
(ICIP)</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 66–70. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Jeni et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
L. A. Jeni, J. F. Cohn, and T. Kanade.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Dense 3D face alignment from 2D videos in real-time.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">11th IEEE International Conference and Workshops on
Automatic Face and Gesture Recognition (FG)</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Kar et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
A. Kar, A. Prakash, M.-Y. Liu, E. Cameracci, J. Yuan, M. Rusiniak, D. Acuna,
A. Torralba, and S. Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Meta-Sim: Learning to Generate Synthetic Datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Karis et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
B. Karis, T. Antoniades, S. Caulkin, and V. Mastilovic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Digital humans: Crossing the uncanny valley in ue4.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">Game Developers Conference, 2016.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Karras et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Analyzing and improving the image quality of StyleGAN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.4.4.1" class="ltx_text" style="font-size:90%;">Kingma and Ba [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">
D. P. Kingma and J. Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</em><span id="bib.bib28.9.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Kowalski et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
M. Kowalski, S. J. Garbin, V. Estellers, T. Baltrušaitis, M. Johnson, and
J. Shotton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Config: Controllable neural face image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.4.4.1" class="ltx_text" style="font-size:90%;">Kuhnke and Ostermann [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">
F. Kuhnke and J. Ostermann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">Deep head pose estimation using synthetic images and partial
adversarial domain adaption for continuous label spaces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib30.10.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.4.4.1" class="ltx_text" style="font-size:90%;">Kärkkäinen and Joo [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">
K. Kärkkäinen and J. Joo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">Fairface: Face attribute dataset for balanced race, gender, and age.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">WACV</em><span id="bib.bib31.10.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Le et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
V. Le, J. Brandt, Z. Lin, L. Bourdev, and T. S. Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Interactive facial feature localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib32.11.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Lewis et al. [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
J. P. Lewis, M. Cordner, and N. Fong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Pose Space Deformation: A Unified Approach to Shape Interpolation
and Skeleton-Driven Deformation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGGRAPH</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, 2000.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Learning a model of facial shape and expression from 4D scans.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGGRAPH Asia</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
J. Lin, H. Yang, D. Chen, M. Zeng, F. Wen, and L. Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Face Parsing with RoI Tanh-Warping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Y. Liu, H. Shi, H. Shen, Y. Si, X. Wang, and T. Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">A new dataset and boundary-attention semantic segmentation for face
parsing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Loper et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">SMPL: A skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGGRAPH Asia</em><span id="bib.bib37.10.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Lozano et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
I. Lozano, J. Saunier, S. Panhard, and G. Loussouarn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">The diversity of the human hair colour assessed by visual scales and
instrumental measurements. a worldwide survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International journal of cosmetic science</em><span id="bib.bib38.10.2" class="ltx_text" style="font-size:90%;">, 39:101–107, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.4.4.1" class="ltx_text" style="font-size:90%;">Microsoft [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text" style="font-size:90%;">
Microsoft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">Microsoft will be carbon negative by 2030.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/</a><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Mueller et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
F. Mueller, F. Bernard, O. Sotnychenko, D. Mehta, S. Sridhar, D. Casas, and
C. Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Ning et al. [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
H. Ning, W. Xu, Y. Gong, and T. Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Discriminative learning of visual words for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, 2003.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Nojavanasghari et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
B. Nojavanasghari, T. Baltrušaitis, C. E. Hughes, , and L.-P. Morency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Hand2face: Automatic synthesis and recognition of hand over face
occlusions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACII</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Paszke et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">PyTorch: An Imperative Style, High-Performance Deep Learning
Library.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib43.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Qiu et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
W. Qiu, F. Zhong, Y. Zhang, S. Qiao, Z. Xiao, T. S. Kim, Y. Wang, and
A. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Unrealcv: Virtual worlds for computer vision.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Multimedia Open Source Software Competition</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Richardson et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
E. Richardson, M. Sela, and R. Kimmel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">3d face reconstruction by learning from synthetic data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib45.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on 3D Vision</em><span id="bib.bib45.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Richardson et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
E. Richardson, M. Sela, R. Or-El, and R. Kimmel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Learning detailed face reconstruction from a single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Richter et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
S. R. Richter, V. Vineet, S. Roth, and V. Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib47.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European conference on computer vision</em><span id="bib.bib47.11.3" class="ltx_text" style="font-size:90%;">, pages 102–118.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Robinson et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
J. P. Robinson, Y. Li, N. Zhang, Y. Fu, , and S. Tulyakov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Laplace Landmark Localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib48.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib48.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Ronneberger et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
O. Ronneberger, P. Fischer, and T. Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Medical image computing and
computer-assisted intervention</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Ros et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">The SYNTHIA Dataset: A Large Collection of Synthetic Images for
Semantic Segmentation of Urban Scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Rozantsev et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
A. Rozantsev, V. Lepetit, and P. Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">On rendering synthetic images for training an object detector.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVIU</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.4.4.1" class="ltx_text" style="font-size:90%;">Russian3DScanner [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.6.1" class="ltx_text" style="font-size:90%;">
Russian3DScanner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">Wrap3.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.russian3dscanner.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.russian3dscanner.com/</a><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Sagonas et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">300 faces In-the-wild challenge: Database and results.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib53.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Image and Vision Computing (IMAVIS)</em><span id="bib.bib53.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Saito et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and H. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed
Human Digitization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Saito et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
S. Saito, T. Simon, J. Saragih, and H. Joo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">PIFuHD: Multi-Level Pixel-Aligned Implicit Function for
High-Resolution 3D Human Digitization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib55.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib55.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Saleh et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
F. S. Saleh, M. Sadegh Aliakbarian, M. Salzmann, L. Petersson, and J. M.
Alvarez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">Effective Use of Synthetic Data for Urban Scene Semantic
Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib56.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Sankaranarayanan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and R. Chellappa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Learning from Synthetic Data: Addressing Domain Shift for Semantic
Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib57.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib57.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Sela et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
M. Sela, E. Richardson, and R. Kimmel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Unrestricted facial geometry reconstruction using image-to-image
translation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Shotton et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore,
A. Kipman, and A. Blake.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Real-time human pose recognition in parts from single depth images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib59.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib59.11.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text" style="font-size:90%;">Shrivastava et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">
A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text" style="font-size:90%;">Learning from simulated and unsupervised images through adversarial
training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib60.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib60.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Simon et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
T. Simon, H. Joo, I. Matthews, and Y. Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">Hand keypoint detection in single images using multiview
bootstrapping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib61.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib61.11.3" class="ltx_text" style="font-size:90%;">, pages 1145–1153, 2017.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text" style="font-size:90%;">Sugano et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">
Y. Sugano, Y. Matsushita, and Y. Sato.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">Learning-by-Synthesis for Appearance-Based 3D Gaze Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib62.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib62.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.4.4.1" class="ltx_text" style="font-size:90%;">Świrski and Dodgson [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.6.1" class="ltx_text" style="font-size:90%;">
L. Świrski and N. A. Dodgson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">Rendering synthetic ground truth images for eye tracker evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib63.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ETRA</em><span id="bib.bib63.10.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Te et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
G. Te, Y. Liu, W. Hu, H. Shi, and T. Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Edge-aware Graph Representation Learning and Reasoning for Face
Parsing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib64.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib64.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Varol et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black, I. Laptev, and
C. Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib65.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib65.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
X. Wang, L. Bo, and L. Fuxin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">Adaptive Wing Loss for Robust Face Alignment via Heatmap
Regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib66.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib66.11.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text" style="font-size:90%;">Wei et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">
Z. Wei, S. Liu, Y. Sun, and H. Ling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text" style="font-size:90%;">Accurate facial image parsing at real-time speed.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib67.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</em><span id="bib.bib67.10.2" class="ltx_text" style="font-size:90%;">, 28(9):4659–4670, 2019.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Wood et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
E. Wood, T. Baltrušaitis, X. Zhang, Y. Sugano, P. Robinson, and A. Bulling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Rendering of eyes for eye-shape registration and gaze estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib68.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib68.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text" style="font-size:90%;">Wood et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">
E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, and A. Bulling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">Learning an appearance-based gaze estimator from one million
synthesised images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib69.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ETRA</em><span id="bib.bib69.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Look at Boundary: A Boundary-Aware Face Alignment Algorithm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib70.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib70.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">
W. Wu, N. Lu, and E. Xie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text" style="font-size:90%;">Synthetic-to-real unsupervised domain adaptation for scene text
detection in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib71.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACCV</em><span id="bib.bib71.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.4.4.1" class="ltx_text" style="font-size:90%;">Yakubovskiy [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.6.1" class="ltx_text" style="font-size:90%;">
P. Yakubovskiy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">Segmentation models pytorch.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/qubvel/segmentation_models.pytorch" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/qubvel/segmentation_models.pytorch</a><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Yao et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
Y. Yao, L. Zheng, X. Yang, M. Naphade, and T. Gedeon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">Simulating Content Consistent Vehicle Datasets with Attribute
Descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib73.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib73.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Yin et al. [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
L. Yin, X. Chen, Y. Sun, T. Worm, and M. Reale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">A high-resolution 3d dynamic facial expression database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib74.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2008 8th IEEE International Conference on Automatic Face
Gesture Recognition</em><span id="bib.bib74.11.3" class="ltx_text" style="font-size:90%;">, pages 1–6, 2008.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/AFGR.2008.4813324</span><span id="bib.bib74.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.5.5.1" class="ltx_text" style="font-size:90%;">Zaal et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.7.1" class="ltx_text" style="font-size:90%;">
G. Zaal, S. Majboroda, and A. Mischok.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.8.1" class="ltx_text" style="font-size:90%;">HDRI haven.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://hdrihaven.com" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://hdrihaven.com</a><span id="bib.bib75.9.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text" style="font-size:90%;">Zeng et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text" style="font-size:90%;">
X. Zeng, X. Peng, and Y. Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text" style="font-size:90%;">Df2net: A dense-fine-finer network for detailed 3d face
reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib76.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib76.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2017a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text" style="font-size:90%;">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text" style="font-size:90%;">Unpaired Image-to-Image Translation using Cycle-Consistent
Adversarial Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib77.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib77.11.3" class="ltx_text" style="font-size:90%;">, 2017a.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text" style="font-size:90%;">
M. Zhu, D. Shi, M. Zheng, and M. Sadiq.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text" style="font-size:90%;">Robust Facial Landmark Detection via Occlusion-Adaptive Deep
Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib78.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib78.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2017b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text" style="font-size:90%;">
X. Zhu, X. Liu, Z. Lei, and S. Z. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text" style="font-size:90%;">Face alignment in full pose range: A 3d total solution.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib79.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">TPAMI</em><span id="bib.bib79.10.2" class="ltx_text" style="font-size:90%;">, 2017b.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.15101" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.15102" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.15102">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.15102" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.15103" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 03:21:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
