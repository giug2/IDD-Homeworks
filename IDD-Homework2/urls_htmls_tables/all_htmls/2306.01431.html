<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.01431] On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions</title><meta property="og:description" content="As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.01431">

<!--Generated on Thu Feb 29 01:51:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On Knowledge Editing in Federated Learning: Perspectives, 
<br class="ltx_break">Challenges, and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Leijie Wu<sup id="10.2.1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Song Guo<sup id="11.2.1" class="ltx_sup"><span id="11.2.1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junxiao Wang<sup id="12.2.1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zicong Hong<sup id="13.2.1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Zhang<sup id="14.6.1" class="ltx_sup">1</sup>&amp;Jingren Zhou<sup id="15.7.2" class="ltx_sup">3</sup>
<sup id="16.8.3" class="ltx_sup">1</sup>The Hong Kong Polytechnic University
<br class="ltx_break"><sup id="17.9.4" class="ltx_sup">2</sup>The Hong Kong Polytechnic University Shenzhen Research Institute
<br class="ltx_break"><sup id="18.10.5" class="ltx_sup">3</sup>Alibaba Group
lei-jie.wu@connect.polyu.hk,
{song.guo, junxiao.wang}@polyu.edu.hk,
zicong.hong@connect.polyu.hk,
jie-comp.zhang@polyu.edu.hk,
jingren.zhou@alibaba-inc.com
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="19.1" class="ltx_p">As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of tasks results in the phenomenon known as “catastrophic forgetting”.
Consequently, much FL research has centered on devising federated increasing learning methods to alleviate forgetting while augmenting knowledge.
On the other hand, forgetting is not always detrimental.
The selective amnesia, also known as federated unlearning, which entails the elimination of specific knowledge, can address privacy concerns and create additional “space” for acquiring new knowledge.
However, there is a scarcity of extensive surveys that encompass recent advancements and provide a thorough examination of this issue.
In this manuscript, we present an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, with the goal of summarizing the state-of-the-art research and expanding the perspective for various domains.
Initially, we introduce an integrated paradigm, referred to as Federated Editable Learning (FEL), by reevaluating the entire lifecycle of FL.
Secondly, we provide a comprehensive overview of existing methods, evaluate their position within the proposed paradigm, and emphasize the current challenges they face.
Lastly, we explore potential avenues for future research and identify unresolved issues.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">McMahan and et
al (<a href="#bib.bib35" title="" class="ltx_ref">2017</a>)</cite> facilitates the collaborative learning of a global model by multiple local clients, while concurrently ensuring secure protection of privacy for each individual client.
It effectively addresses the issue of data silos without completely compromising the privacy of the clients.
In recent years, FL has garnered significant attention in the academic community and achieved remarkable successes in a variety of industrial applications such as autonomous driving <cite class="ltx_cite ltx_citemacro_cite">Samarakoon and et
al (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite>, wearable technology <cite class="ltx_cite ltx_citemacro_cite">Chen and et al (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, and medical diagnosis <cite class="ltx_cite ltx_citemacro_cite">Rieke and et al (<a href="#bib.bib39" title="" class="ltx_ref">2020</a>); Dayan and et al (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In general, the majority of existing FL methods <cite class="ltx_cite ltx_citemacro_cite">Shoham and et al (<a href="#bib.bib43" title="" class="ltx_ref">2019</a>); Wang and et al (<a href="#bib.bib49" title="" class="ltx_ref">2021</a>); Hong and et al (<a href="#bib.bib22" title="" class="ltx_ref">2021</a>); Yang and et al (<a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite> are formulated for static application scenarios, where the data and tasks of the overall FL framework are fixed and known ahead of time. However, in real-world applications, the situation is often dynamic, where local clients receive new task data in an online manner.
To handle this type of situation, researchers are investigating how FL can be adapted to learn continuously over a sequence of tasks.
It has become widely acknowledged that utilizing straightforward stochastic gradient descent (SGD) on FL when learning over a sequence of tasks results in the phenomenon known as “catastrophic forgetting”, which implies that the model forgets what it had previously learned when acquiring new knowledge <cite class="ltx_cite ltx_citemacro_cite">Huang and et al (<a href="#bib.bib25" title="" class="ltx_ref">2022b</a>)</cite>.
As a result, a significant proportion of researchers have focused on devising methods namely <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">federated increasing learning</em> to augment knowledge while concurrently mitigating the problem of forgetting <cite class="ltx_cite ltx_citemacro_cite">Dong <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">On the other hand, forgetting is not always detrimental. Selective amnesia, also referred to as <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">federated unlearning</em> <cite class="ltx_cite ltx_citemacro_cite">Wu and et al (<a href="#bib.bib54" title="" class="ltx_ref">2022b</a>); Halimi and et al (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>); Wang and et
al (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite>, which involves the elimination of specific knowledge, can address privacy concerns even create additional “space” for acquiring new knowledge. It is possible that in the future, FL will be required to completely remove any indication of having learned a specific data or task.
As we look towards the future, imagine a FL service provider whose system is continuously updated by learning new skills from the data collected from its customers’ daily lives.
Occasionally, the provider may be required to delete previously acquired behaviors and/or knowledge regarding specific tasks or data that have been identified as raising potential fairness <cite class="ltx_cite ltx_citemacro_cite">Ezzeldin and et
al (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, privacy <cite class="ltx_cite ltx_citemacro_cite">Nasr and et al (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>, or security concerns <cite class="ltx_cite ltx_citemacro_cite">Bagdasaryan and et
al (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Nevertheless, there is a scarcity of extensive literature reviews that encompass recent advancements and provide a detailed examination of this subject matter.
As of the time of writing this paper, <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">federated unlearning</em> has not yet been well studied in the <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">federated increasing learning</em> setting where the underlying data distribution can shift over time.
In this paper, we undertake a comprehensive survey of the field of knowledge editing (augmentation/removal) in FL, with the aim of synthesizing the most recent research advancements and broadening the understanding of its potential applications across various domains.
Overall we make the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce an integrated paradigm, referred to as Federated Editable Learning (FEL), by reevaluating the entire lifecycle of FL.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present a thorough examination of existing methods, assess their position within the proposed framework, and highlight the current limitations and challenges they encounter.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We investigate the areas for future research and pinpoint unresolved issues.
Towards efficient lifelong knowledge editing in FL, enabling FL to precisely forget what the user has specified without deteriorating the rest of the acquired knowledge; or, FL to not alter the model’s behavior in other contexts when augmenting knowledge.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Editable Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we will introduce the concept of the Federated Learning Lifecycle, including its background, motivation, and definition.
Different from most existing FL applications, we emphasize that a complete FL lifecycle should not only focus on the learning process to obtain a well-trained model, but also empower the reverse unlearning process to ensure user privacy protection.
Therefore, we propose our Federated Editable Learning (FEL) framework to support the sustainable development of a federated learning system.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2306.01431/assets/Figure/image.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The demonstration of a complete federated learning lifecycle, which defined in Sec.<a href="#S2.SS1" title="2.1 The Lifecycle of Federated Learning ‣ 2 Federated Editable Learning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>. The successful operation of FL lifecycle relies on the Federated Editable Learning framework, which consists of two components: the Federated Increasing Learning in Sec.<a href="#S3" title="3 Federated Increasing Learning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and the Federated Unlearning in Sec.<a href="#S4" title="4 Federated Unlearning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Besides, the verification and auditing mechanisms in Sec.<a href="#S5" title="5 Verification &amp; Auditing ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> is also important to guarantee the FL lifecycle.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Lifecycle of Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Artificial Intelligence (AI) has become an essential component of life today, which achieves significant successes in various domains, such as Computer Vision (CV) <cite class="ltx_cite ltx_citemacro_cite">Dong and et al (<a href="#bib.bib10" title="" class="ltx_ref">2014</a>)</cite>, Natural Language Processing (NLP), etc <cite class="ltx_cite ltx_citemacro_cite">Schmidhuber (<a href="#bib.bib42" title="" class="ltx_ref">2015</a>)</cite>.
With the increase in advanced sensing and computing capabilities of ubiquitous mobile devices, AI architectures are gradually shifting from traditional data-centralized cloud server to the distributed edge.
Besides, considering the importance of user data privacy protection, the federated learning (FL) concept has been proposed.
As an emerging and novel distributed machine learning paradigm, FL adopts collaborative model training on extensive user devices to obtain a model containing globally shared knowledge. Only model parameters are exchanged between the server and user devices, so that the user data never leaves the local side and its privacy protection is guaranteed.
Therefore, involving more user device participation to contribute their data for training is an important and critical principle in the FL scenario.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">However, in practical FL applications <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">Li and et al </a></cite>, the FL system is always in a dynamic changing process, which can be divided into the following cases:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Data Dynamic: The local data of user devices already involved inside the FL system is constantly updated (generating new data &amp; deleting obsolete data).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Device Dynamic: The participated user devices of the FL system are also changing (new devices join &amp; old devices exit). For example, user devices from different time zones have their own available periods.</p>
</div>
</li>
</ul>
<p id="S2.SS1.p2.2" class="ltx_p">In fact, the essence of both two dynamic cases is all about the data flow in the FL system.
Besides, the current mainstream machine learning models also have their own constraints.
Given a specific model architecture, there is an upper limit on the knowledge amount that the model can contain <cite class="ltx_cite ltx_citemacro_cite">Roberts and et al (<a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>. Generally speaking, a larger model with more parameters can learn more knowledge.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Therefore, for an FL system with given model architecture, the global model has to keep updating its knowledge to adapt the above system dynamic and knowledge constraints.
This model updating involves not only learning new knowledge from new data or devices, but also removing the negative effects of obsolete data or devices from the current model.
We refer to them as “Learning process” and “Unlearning process”, respectively.
However, the majority of the existing FL frameworks mainly focus on the learning process, while the unlearning process is neglected <cite class="ltx_cite ltx_citemacro_cite">Yang and et al (<a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>.
A fixation on only learning new knowledge can lead to the model quickly reaching its knowledge upper limit and thus being unable to further adapt to the system dynamic.
The “unlearning process” is also a necessary component of the FL system, which can help remove obsolete knowledge and create space for new knowledge in the future.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Based on the above insights, we are the first to propose the concept of lifecycle for the current FL paradigm, where the demonstration of a complete FL lifecycle is shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Federated Editable Learning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
It incorporates both sides of “Learning” and “Unlearning ” to achieve the expected model knowledge editing, which enables the sustainable development of the FL system.
In addition, auditing the results of model editing is also a crucial element for the FL lifecycle.
In the learning process, we need to ensure that the user device has performed the corresponding training requirements honestly and credibly.
In the unlearning process, we need to ensure that the knowledge of deleted data is fully removed from the current model, while the knowledge of remaining data is kept unchanged.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>What is Federated Editable Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Carrying the above concept of FL lifecycle, we introduce our framework named Federated Editable Learning (FEL) as the cornerstone for a perfect implementation of the FL lifecycle.
The ultimate objective of FEL is to empower user devices to freely control their own private data in the FL system, while ensuring the FL global model can adaptively adjust its knowledge to handle the system dynamics.
On the one hand, the user devices have the right to decide which part of their data will be contributed to participate in the collaborative training process of FL system, and the knowledge contained in limited data can be absorbed into the FL global model.
On the other hand, the user devices should also have the right to revoke their previously participated data from the FL system, i.e., deleting the historical influences induced by participated data from the current FL global model.
The model after deletion operation should behave as if these data never participate in FL training, and those relevant obsolete knowledge in the model also need to be removed.
Therefore, to achieve the objectives of FEL in both aspects, we characterize the existing FL-related works into two categories: <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">Federated Increasing Learning (FIL)</span> and <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">Federated Unlearning (FU)</span>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Federated Increasing Learning (FIL)</span>:
The work in this category focuses on how to obtain new knowledge for the global model from the constant data flowing into the FL system, and there are several critical challenges that need to be addressed in FIL.
First, the contributed data of user devices may only occur once in the FL system, the server must leverage the only opportunity to derive knowledge from this single participation.
To address this challenge, we provide a comprehensive survey on <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">Federated Continual Learning</span> (FCL), where more details are provided in Sec.<a href="#S3.SS2" title="3.2 Federated Continual Learning ‣ 3 Federated Increasing Learning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
Second, user devices are constrained by limited resources (e.g., memory space, computation unit, etc), which may result in a very limited amount of data being generated on them.
As we know, good knowledge representation of AI comes from the big data analysis from massive amounts of data. Thus, how to extract knowledge with generalization from a small amount of specialization data is a serious challenge.
We discover the success of many existing works on <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_italic">Federated Few-shot Learning</span> (FFsL) to handle the above challenge, and provide an exhaustive survey to summarize the current research frontier.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Federated Unlearning (FU)</span>: The work in this category focuses on deleting the obsolete data as well as its historical influence in the current model, which not only protects the user data privacy but also creates ”space” for new knowledge in the future.
A straightforward way is to retrain a new model from scratch with the remaining data only.
However, naive retraining demands huge computational resources and time costs, which is completely unacceptable for an FL system.
Therefore, we provide a detailed survey about the existing advanced or optimized retraining-based methods in Sec.<a href="#S4.SS2" title="4.2 Exact Federated Unlearning ‣ 4 Federated Unlearning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
Except for exact retraining, approximate unlearning methods are the mainstream in the current FU field. Its objective is to generate an approximate unlearning model in a fast and computationally efficient manner, whose behavior is almost equivalent to an exact retraining model. A comprehensive survey about approximate FU is provided in Sec.<a href="#S4.SS3" title="4.3 Approximate Federated Unlearning ‣ 4 Federated Unlearning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Federated Increasing Learning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we are going to consider Federated Increasing Learning (FIL) problems, which involve the federated training over time. In the standard FL setting, the objective is to build a joint model using a certain amount of data from a multitude of coordinated devices in a decentralized way. One typical assumption in standard FL is that the whole training dataset is available from the beginning of the training stage.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">However, this assumption rarely holds in real-world FL applications,
where local clients often collect new data progressively, during several days, or weeks, depending on the context. Moreover, new clients with unseen new data may participate in the FL training, further aggravating the model and could be unable to converge to a solution. For these reasons, we need to introduce Increasing Learning (IL) <cite class="ltx_cite ltx_citemacro_cite">Thrun (<a href="#bib.bib46" title="" class="ltx_ref">1995</a>)</cite> into FL. FIL research gains a lot of importance since it addresses the difficulties of training a model gradually with data collected over different periods of time, adapting to the new instances and trying to preserve the previous knowledge.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary and classification of existing federated increasing learning works.</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:675.9pt;height:234.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.4pt,8.8pt) scale(0.93,0.93) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="2"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Publication</span></td>
<td id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Key Contribution</span></td>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="8"><span id="S3.T1.1.1.2.2.1.1" class="ltx_text">
<span id="S3.T1.1.1.2.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:18.0pt;height:32.4pt;vertical-align:-7.2pt;"><span class="ltx_transformed_inner" style="width:32.4pt;transform:translate(-7.19pt,0pt) rotate(-90deg) ;">
<span id="S3.T1.1.1.2.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.1.1.2.2.1.1.1.1.1" class="ltx_tr">
<span id="S3.T1.1.1.2.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.1.1.2.2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">FCL</span></span></span>
</span>
</span></span></span></td>
<td id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="5"><span id="S3.T1.1.1.2.2.2.1" class="ltx_text">
<span id="S3.T1.1.1.2.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.1.1.2.2.2.1.1.1" class="ltx_tr">
<span id="S3.T1.1.1.2.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">Task-based</span></span>
<span id="S3.T1.1.1.2.2.2.1.1.2" class="ltx_tr">
<span id="S3.T1.1.1.2.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">FCL</span></span>
</span></span></td>
<td id="S3.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Ensemble Learning</td>
<td id="S3.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Casado and et al (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="S3.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Lightweight and real-time framework</td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Parameter Decomposition</td>
<td id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoon and et al (<a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite></td>
<td id="S3.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Selective knowledge aggregation</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Drift Detection</td>
<td id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Casado and et al (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S3.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Autonomous user local training strategy</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Elastic Weight Consolidation</td>
<td id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Shoham and et al (<a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S3.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Penalty term to the loss function</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Elastic Weight Consolidation</td>
<td id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Yao and Sun (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="S3.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Limit updates of important parameter</td>
</tr>
<tr id="S3.T1.1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="3"><span id="S3.T1.1.1.7.7.1.1" class="ltx_text">
<span id="S3.T1.1.1.7.7.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.1.1.7.7.1.1.1.1" class="ltx_tr">
<span id="S3.T1.1.1.7.7.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">Class-based</span></span>
<span id="S3.T1.1.1.7.7.1.1.1.2" class="ltx_tr">
<span id="S3.T1.1.1.7.7.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">FCL</span></span>
</span></span></td>
<td id="S3.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Regularization Term</td>
<td id="S3.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Usmanova and et
al (<a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S3.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Use distillation loss to transfer knowledge</td>
</tr>
<tr id="S3.T1.1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Prototypical Networks</td>
<td id="S3.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Hendryx and et
al (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite></td>
<td id="S3.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Store exemplars from previous tasks</td>
</tr>
<tr id="S3.T1.1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Truncated Cross Entropy</td>
<td id="S3.T1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Legate and et al (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S3.T1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Force user learn internal representation</td>
</tr>
<tr id="S3.T1.1.1.10.10" class="ltx_tr">
<td id="S3.T1.1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="5"><span id="S3.T1.1.1.10.10.1.1" class="ltx_text">
<span id="S3.T1.1.1.10.10.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:18.0pt;height:36pt;vertical-align:-9.0pt;"><span class="ltx_transformed_inner" style="width:35.9pt;transform:translate(-8.96pt,0pt) rotate(-90deg) ;">
<span id="S3.T1.1.1.10.10.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.1.1.10.10.1.1.1.1.1" class="ltx_tr">
<span id="S3.T1.1.1.10.10.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.1.1.10.10.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">FFsL</span></span></span>
</span>
</span></span></span></td>
<td id="S3.T1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;" rowspan="5"><span id="S3.T1.1.1.10.10.2.1" class="ltx_text">
<span id="S3.T1.1.1.10.10.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.1.1.10.10.2.1.1.1" class="ltx_tr">
<span id="S3.T1.1.1.10.10.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">Generalization</span></span>
<span id="S3.T1.1.1.10.10.2.1.1.2" class="ltx_tr">
<span id="S3.T1.1.1.10.10.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">to Unseen Data</span></span>
</span></span></td>
<td id="S3.T1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Meta-Learning</td>
<td id="S3.T1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen and et al (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite></td>
<td id="S3.T1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">First to apply FL on meta-learning</td>
</tr>
<tr id="S3.T1.1.1.11.11" class="ltx_tr">
<td id="S3.T1.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Adversarial Learning</td>
<td id="S3.T1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Fan and et al (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite></td>
<td id="S3.T1.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Produce different feature for unseen data</td>
</tr>
<tr id="S3.T1.1.1.12.12" class="ltx_tr">
<td id="S3.T1.1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Energy-based Weighting</td>
<td id="S3.T1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Dong and et al (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S3.T1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Updating the weights of pseudo examples</td>
</tr>
<tr id="S3.T1.1.1.13.13" class="ltx_tr">
<td id="S3.T1.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Adversarial Learning</td>
<td id="S3.T1.1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Huang and et al (<a href="#bib.bib24" title="" class="ltx_ref">2022a</a>)</cite></td>
<td id="S3.T1.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Latent embedding adaptation</td>
</tr>
<tr id="S3.T1.1.1.14.14" class="ltx_tr">
<td id="S3.T1.1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Client Selection</td>
<td id="S3.T1.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">Xu and et al (<a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S3.T1.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Exclude the malicious user participation</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Challenges in Federated Increasing Learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the FIL setting, each local client collects the training data continuously with its own preference, while new clients with unseen new data could join the FL training at any time. More specifically, the data distributions of the collected classes across the current and newly-added clients are non-independent and identically distributed (non-i.i.d.). FIL requires these local clients to collaboratively train a global model to learn new data continuously, with constraints on privacy preservation and limited memory storage <cite class="ltx_cite ltx_citemacro_cite">Rebuffi and et al (<a href="#bib.bib38" title="" class="ltx_ref">2017</a>); Wu and et al (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Under such circumstances, an ideal framework should recognize new classes and meanwhile maintain discriminability over old classes, which is called Federated Continual Learning (FCL). The main difficulty in FCL is catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">Goodfellow and et
al (<a href="#bib.bib18" title="" class="ltx_ref">2013</a>)</cite>.
Catastrophic forgetting refers to the phenomenon that occurs when optimizing the model with new classes, the formerly acquired knowledge on old classes is quickly forgotten.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Under severe circumstances, only limited novel instances are available to incrementally update the model. Meanwhile, local clients often have very limited storage memory to store few-shot old data. As a result, the task of recognizing few-shot new classes without forgetting old classes is called federated few-shot class-incremental learning. Such lack of data would further exacerbate local forgetting caused by class imbalance at the local clients and global forgetting brought by the non-i.i.d class imbalance across clients.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Federated Continual Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Mostly FCL methods address task-continual learning (task-CL) scenario <cite class="ltx_cite ltx_citemacro_cite">De Lange and et al (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>, where information about the task-ID of examples is known at test time. However, more challenging scenario is class-continual learning (class-CL), where the model has to distinguish among all the classes of all the tasks at test time <cite class="ltx_cite ltx_citemacro_cite">Masana and et al (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>.
In the following part, we will review the literature on task-based FCL and class-based FCL, respectively.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For the task-CL problem in FL, a large number of works focus on the problem of catastrophic forgetting.
For example, LFedCon2 <cite class="ltx_cite ltx_citemacro_cite">Casado and et al (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> aims to use light, traditional classification models, e.g., a generalized linear model (GLM), a decision tree (DT), to support real-time, continuously and autonomously learning phase in a privacy-preserving and decentralized manner.
Yoon et al., <cite class="ltx_cite ltx_citemacro_cite">Yoon and et al (<a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite> propose FedWeIT, in which each client learns a series of tasks from the private local data stream, meanwhile different clients can also learn from others to enhance their learning performance. Specifically, a learnable mask vector is trained to filter the relevant knowledge from other clients during the aggregation phase.
To solve the concept drift (i.e., the underlying distribution of data can change in unforeseen ways over time), CDA-FedAvg <cite class="ltx_cite ltx_citemacro_cite">Casado and et al (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite> designs a detection mechanism to monitor concept drift, so that each device has enough autonomy to decide when to train and what data to use. By such means, the server will simply orchestrate the process.
FedCurv <cite class="ltx_cite ltx_citemacro_cite">Shoham and et al (<a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite> and FedCL <cite class="ltx_cite ltx_citemacro_cite">Yao and Sun (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite> adopt EWC <cite class="ltx_cite ltx_citemacro_cite">Kirkpatrick and et
al (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>, which aims to improve the generalization ability of the federated models.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For class-CL scenarios in FL, FCL methods can be divided into the following three types <cite class="ltx_cite ltx_citemacro_cite">Masana and et al (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Regularization-based approaches</span>, which compute the importance of weights for previous tasks and penalize the model for changing them (i.e., FLwF <cite class="ltx_cite ltx_citemacro_cite">Usmanova and et
al (<a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite> use distillation loss to transfer the knowledge from the server and decrease the forgetting of previously learned tasks);</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><span id="S3.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Exemplar-based approaches</span>, which store exemplars from previous tasks, i.e., Hendryx et al., <cite class="ltx_cite ltx_citemacro_cite">Hendryx and et
al (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> use federated prototypical networks to efficiently learn new classes in sequence;</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><span id="S3.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Bias-correction approaches</span>, which deal explicitly with bias towards recently-learned tasks (Legate et al., <cite class="ltx_cite ltx_citemacro_cite">Legate and et al (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> adopt Truncated Cross Entropy (TCE) to force each client to learn by adapting the model’s internal representation of the classes present in its training data).</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Federated Few-shot Learning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">With the goal of extracting the inductive bias from base classes and generalizing it to unseen classes, few-shot learning has been widely explored in recent years. However, training FSL models on distributed devices is still an open problem.
The first work of this topic was from Chen et al., <cite class="ltx_cite ltx_citemacro_cite">Chen and et al (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> who explored federated meta-learning by applying FedAvg on meta-learning approaches such as MAML <cite class="ltx_cite ltx_citemacro_cite">Finn and et al (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite> in a straightforward way.
Another line of work focuses on data augmentation to alleviate data scarcity, i.e., FewFedWeight <cite class="ltx_cite ltx_citemacro_cite">Dong and et al (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite> proposes an energy-based weighting algorithm for updating the weights of pseudo examples generated by the global model and a dynamic aggregation method based on the performance of client models.
Then, FedFSL <cite class="ltx_cite ltx_citemacro_cite">Fan and et al (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> formulates the training in an adversarial fashion and optimizes the client models to produce a discriminative feature space that can better represent unseen data samples, while FedAffect <cite class="ltx_cite ltx_citemacro_cite">Huang and et al (<a href="#bib.bib24" title="" class="ltx_ref">2022a</a>)</cite> considers a more challenge scenario: local participants design their models independently.
Besides that,
CSFedL <cite class="ltx_cite ltx_citemacro_cite">Xu and et al (<a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite> proposes an adaptive client selection strategy to mitigate the impact caused by malicious participation, to obtain a more effective few-shot model.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Federated Unlearning</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">After the FL training of a model is completed, clients may require the FL server to remove parts of data contribution from the global model to protect the user’s privacy and avoid legal risks.
The scenario is called <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">federated unlearning</em>.
The server should transform the model into an updated one that operates as if those deleted data never participated in FL training.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In the section, we first discuss several major challenges in federated unlearning and then summarize the emerging federated unlearning works from perspectives of <em id="S4.p2.1.1" class="ltx_emph ltx_font_italic">exact federated unlearning</em> and <em id="S4.p2.1.2" class="ltx_emph ltx_font_italic">approximate federated unlearning</em>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Summary and classification of existing federated unlearning works.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:675.7pt;height:273.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.8pt,7.2pt) scale(0.95,0.95) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;" colspan="2"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Scenario</span></td>
<td id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S4.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Publication</span></td>
<td id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S4.T2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Unlearning Requests</span></td>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<td id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;" colspan="2" rowspan="3"><span id="S4.T2.1.1.2.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Exact</span></td>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Ensemble</td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Yan and et al (<a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client, Class, Sample</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<td id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Ensemble</td>
<td id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Yu and et al (<a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client, Class, Sample</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<td id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Cryptography</td>
<td id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Random Forest</td>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Liu and et al (<a href="#bib.bib32" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<td id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;" rowspan="12"><span id="S4.T2.1.1.5.5.1.1" class="ltx_text">
<span id="S4.T2.1.1.5.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:56.9pt;vertical-align:-26.0pt;"><span class="ltx_transformed_inner" style="width:57.0pt;transform:translate(-24.1pt,2.92pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.5.5.1.1.1.1" class="ltx_p"><span id="S4.T2.1.1.5.5.1.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Approximate</span></span>
</span></span></span></td>
<td id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;" rowspan="4"><span id="S4.T2.1.1.5.5.2.1" class="ltx_text">
<span id="S4.T2.1.1.5.5.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.5.2.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.5.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Gradient</span></span>
<span id="S4.T2.1.1.5.5.2.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.5.5.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Recovery</span></span>
</span></span></td>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">History Retaining</td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Liu and et al (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite></td>
<td id="S4.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.6.6" class="ltx_tr">
<td id="S4.T2.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">History Retaining</td>
<td id="S4.T2.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Liu and et al (<a href="#bib.bib32" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="S4.T2.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.7.7" class="ltx_tr">
<td id="S4.T2.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">History Retaining</td>
<td id="S4.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Poisoning Recovery</td>
<td id="S4.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Cao and et al (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="S4.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.8.8" class="ltx_tr">
<td id="S4.T2.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">History Retaining</td>
<td id="S4.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Recommender System</td>
<td id="S4.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Yuan and et al (<a href="#bib.bib62" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="S4.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.9.9" class="ltx_tr">
<td id="S4.T2.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;" rowspan="4"><span id="S4.T2.1.1.9.9.1.1" class="ltx_text">
<span id="S4.T2.1.1.9.9.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.9.9.1.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.9.9.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Parameter</span></span>
<span id="S4.T2.1.1.9.9.1.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.9.9.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Updating</span></span>
</span></span></td>
<td id="S4.T2.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Knowledge Distillation</td>
<td id="S4.T2.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Wu and et al (<a href="#bib.bib53" title="" class="ltx_ref">2022a</a>)</cite></td>
<td id="S4.T2.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.10.10" class="ltx_tr">
<td id="S4.T2.1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Gradient Descent</td>
<td id="S4.T2.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Bayesian Model</td>
<td id="S4.T2.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Gong and et al (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T2.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.11.11" class="ltx_tr">
<td id="S4.T2.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Gradient Descent</td>
<td id="S4.T2.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Halimi and et al (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T2.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
<tr id="S4.T2.1.1.12.12" class="ltx_tr">
<td id="S4.T2.1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Gradient Descent</td>
<td id="S4.T2.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Wu and et al (<a href="#bib.bib54" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="S4.T2.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client, Class, Sample</td>
</tr>
<tr id="S4.T2.1.1.13.13" class="ltx_tr">
<td id="S4.T2.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;" rowspan="2"><span id="S4.T2.1.1.13.13.1.1" class="ltx_text">
<span id="S4.T2.1.1.13.13.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.13.13.1.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.13.13.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Architecture</span></span>
<span id="S4.T2.1.1.13.13.1.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.13.13.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Modification</span></span>
</span></span></td>
<td id="S4.T2.1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Channel Pruning</td>
<td id="S4.T2.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Wang and et
al (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T2.1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Class</td>
</tr>
<tr id="S4.T2.1.1.14.14" class="ltx_tr">
<td id="S4.T2.1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Output Filtering</td>
<td id="S4.T2.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Baumhauer and et
al (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T2.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Class</td>
</tr>
<tr id="S4.T2.1.1.15.15" class="ltx_tr">
<td id="S4.T2.1.1.15.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;" rowspan="2"><span id="S4.T2.1.1.15.15.1.1" class="ltx_text">
<span id="S4.T2.1.1.15.15.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.15.15.1.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.15.15.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Noise</span></span>
<span id="S4.T2.1.1.15.15.1.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.15.15.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;">Perturbation</span></span>
</span></span></td>
<td id="S4.T2.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Differential Privacy</td>
<td id="S4.T2.1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Gupta <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite></td>
<td id="S4.T2.1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client, Class, Sample</td>
</tr>
<tr id="S4.T2.1.1.16.16" class="ltx_tr">
<td id="S4.T2.1.1.16.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Randomized Perturbation</td>
<td id="S4.T2.1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">DNN</td>
<td id="S4.T2.1.1.16.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><cite class="ltx_cite ltx_citemacro_cite">Fraboni and et al (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T2.1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;">Client</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Challenges in Federated Unlearning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Compared with traditional machine learning, the characteristics of FL bring three major challenges to the unlearning technique as follows.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">1) Iterative Learning:</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">At the beginning of each round in FL, the model of each client is the aggregation result for all clients in the previous round.
Such an intertwining of client training results in each round leads to the fundamental challenge of federated unlearning.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">2) Information Isolation:</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">Privacy protection, one of the major advantages of FL, prevent FL servers from accessing the client data.
In other words, every client maintains its data samples and trains the model locally.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">3) Stochastic Training:</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">The process of FL training is non-deterministic.
For each round, the FL server randomly selects the clients for global aggregation while each client randomly selects and orders batches of data for local training.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Exact Federated Unlearning</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">A naive way to make the best FL model that provably forgets the target data is to retrain a new model based on the remaining data from scratch.
However, it is prohibitively expensive for an FL server to fully retrain a model in terms of computation and time overhead.
Some works are designed to achieve the unlearning in such a way that the produced models are effectively the same as the ones obtained with retraining but at a cheaper computing cost.
We call these works exact federated unlearning, which are summarized as follows.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Some ensemble learning-based works are designed for machine unlearning originally, however, their idea can be applied in federated unlearning.
For example,
<cite class="ltx_cite ltx_citemacro_cite">Yan and et al (<a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite> propose an efficient exact unlearning framework.
It divides the dataset into several isolated sub-datasets, each corresponding to a sub-model, accelerating the retraining process and ensuring the retrained model’s accuracy.
<cite class="ltx_cite ltx_citemacro_cite">Yu and et al (<a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite> present a novel neural network named LegoNet, composed of a fixed encoder (i.e., the backbone for representation learning) and multiple isolated adapters to be retrained for unlearning.
The adapters occupy few parameters of LegoNet; thus, the re-trained parameters during unlearning can be significantly reduced.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Moreover, without compromising the privacy of clients in FL, <cite class="ltx_cite ltx_citemacro_cite">Liu and et al (<a href="#bib.bib31" title="" class="ltx_ref">2022a</a>)</cite> develop a cryptography-based approach for federated unlearning.
It presents a revocable federated learning framework for random forest (RF) called RevFRF by designing a customised homomorphic encryption-based protocol.
RevFRF guarantees two levels of unlearning: 1) the remaining participants cannot utilize the data of an honest and leaving participant in the trained model;
2) a dishonest participant cannot get back to utilize the data of the remaining participants memorized by the trained model.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Approximate Federated Unlearning</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Although the existing works for exact federated unlearning alleviate the expense of retraining to some extent, their cost is still unacceptable in most FL applications.
Thus, recent works achieve higher efficiency of federated unlearning by relaxing the effectiveness and certifiability requirements for the new model after unlearning, which is called approximate federated learning.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Gradient Recovery</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To overcome the high resource cost caused by the model retraining of the exact federated unlearning described in Sec.<a href="#S4.SS2" title="4.2 Exact Federated Unlearning ‣ 4 Federated Unlearning ‣ On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, the gradient recovery-based approach reconstructs the unlearned model based on the historical parameter updates of clients that have been retained at the FL server during the training process.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Liu and et al (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> propose the first federated unlearning approach named FedEraser, reconstructing a new model based on the historical parameter updates of clients stored in the FL server.
To speed up the retraining while maintaining the model performance, FedEraser has a calibration method for the stored historical updates.
<cite class="ltx_cite ltx_citemacro_cite">Liu and et al (<a href="#bib.bib32" title="" class="ltx_ref">2022b</a>)</cite> propose an efficient retraining algorithm based on the diagonal empirical Fisher Information Matrix (FIM) for FL, by observing the first-order Taylor expansion of the loss function during the unlearning process.
Moreover, to reduce approximation errors in retraining, the proposed algorithm has an adaptive momentum technique.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Cao and et al (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> propose an FL model recovery method to recover a model from poisoning attacks using historical information rather than training from scratch.
For each recovery, the server can estimate the model update of a client in each round based on its stored historical information during the past training process.
<cite class="ltx_cite ltx_citemacro_cite">Yuan and et al (<a href="#bib.bib62" title="" class="ltx_ref">2023</a>)</cite> propose a federated recommendation unlearning method tailed for FL-based recommendation systems (FedRecs).
The main idea is to revise historical updates and leverage the revised updates to speed up the reconstruction of a FedRec.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Parameter Updating</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">The above works for federated unlearning via gradient recovery require the FL server to store historical updates, which burdens the server.
Therefore, another group of federated unlearning is to scrub the trained FL model of information to be forgotten, which we summarize as follows.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Wu and et al (<a href="#bib.bib53" title="" class="ltx_ref">2022a</a>)</cite> propose a federated unlearning method to eliminate a client’s contribution by subtracting the accumulated historical updates from the model and leveraging the knowledge distillation method to restore the model’s performance without using any data from the clients.
<cite class="ltx_cite ltx_citemacro_cite">Gong and et al (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> develop a Bayesian federated unlearning method called Forget-Stein Variational Gradient Descent (Forget-SVGD) based on SVGD, a particle-based approximate Bayesian inference approach via gradient-based deterministic updates.
<cite class="ltx_cite ltx_citemacro_cite">Halimi and et al (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> allow a client to perform the unlearning by training a model to maximize the empirical loss via a Projected Gradient Descent algorithm.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">The previous works mainly focus on client-level federated unlearning (i.e., removing the data of a specific client from the model).
To solve the limitation, <cite class="ltx_cite ltx_citemacro_cite">Wu and et al (<a href="#bib.bib54" title="" class="ltx_ref">2022b</a>)</cite> propose a general framework covering client-level, class-level, and sample-level federated unlearning.
The framework comprises a reverse stochastic gradient ascent (SGA) algorithm with elastic weight consolidation (EWC) to achieve fine-grained elimination of training data at different levels.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Architecture Modification</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Some approaches implement federated unlearning by modifying the model architecture.
For example, <cite class="ltx_cite ltx_citemacro_cite">Wang and et
al (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite> propose a channel pruning-based method to remove information about particular classes in an FL model.
Its main idea is to quantify the class information learned by each channel without globally accessing the data, and then forget special classes by pruning the channels with the most class discrimination.
<cite class="ltx_cite ltx_citemacro_cite">Baumhauer and et
al (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> propose an output filtering technique to remove particular classes in logit-based classification models by applying linear transformation to the output logits, but do not modify the weights in the models.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Noise Perturbation</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Gupta <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> focuses on randomly perturbing the trained model to unlearn specific data samples, which is motivated by the idea of differential privacy.
<cite class="ltx_cite ltx_citemacro_cite">Fraboni and et al (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> propose a new federated unlearning scheme named <em id="S4.SS3.SSS4.p1.1.1" class="ltx_emph ltx_font_italic">informed federated unlearning</em> that unlearns a client’s contribution with quantifiable unlearning guarantees.
Unlearning guarantees are provided by introducing a randomized mechanism to perturb an intermediate model selected from the training process with client-specific noise.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Verification &amp; Auditing</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The previous literature review summarizes the state-of-the-art approaches for knowledge editing in the FL scenario, which together serve as a support to realize the FL system lifecycle.
In addition, to guarantee our objectives are fully achieved according to specified requirements during the FL lifecycle, the verification for the learning process and auditing for the unlearning process are also critical system components, where a comprehensive survey on them is provided in this section.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Verification Methods for Learning Process</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Although the increasing learning process can enable the Fl global model always to acquire new knowledge from the data to adapt the system dynamics, the expected knowledge can only be obtained by correct training according to the specified requirements.
Thus, the learning process of user devices must be verifiable to ensure knowledge correctness.
We summarize several tools able to verify the FL process as follows.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Trusted Execution Environment</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">As a secure environment maintained by each CPU, Trusted Execution Environment (TEE) is a hardware technology that outsources code execution on a protected memory region named <em id="S5.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">enclave</em> in any untrusted devices and enables the verification of the execution results.
Some existing TEE-based FL frameworks depend on the TEE deployment on FL servers and user devices <cite class="ltx_cite ltx_citemacro_cite">Zhang and et al (<a href="#bib.bib63" title="" class="ltx_ref">2020</a>)</cite>.
Similarly, a verifiable FIL framework can be realized by outsourcing the increasing learning process to the user devices’ TEE.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Proof of Learning</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">The concept of Proof of Learning (PoL) proposed by <cite class="ltx_cite ltx_citemacro_cite">Jia and et al (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> enables a verifier (e.g., an FL server) to assess the integrity of training computations for untrusted workers (e.g., user devices).
Its main idea is to verify if a sequence of intermediate states (i.e., checkpoints of intermediate weights) came from training and are not random (or worse, forged by a malicious party).
To prove it, the workers need to provide a sequence of batch indices for the same intermediate model updates.
Although the PoL is a general approach, it may leak the privacy of user devices in FL, which remains to be solved.</p>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Swarm Learning</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">For auditing and accountability in FIL, it is necessary to record the increasing learning process in a public, transparent, and tamper-proof ledger.
By combining blockchain technology with FL for a new learning scheme called <em id="S5.SS1.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">swarm learning</em> <cite class="ltx_cite ltx_citemacro_cite">Warnat-Herresthal and et
al (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>, the learning process can be fully recorded by such a ledger in a distributed manner despite the existence of malicious user devices.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Auditing Methods for Unlearning Process</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">It’s easy to understand that the auditing mechanism is unnecessary in the category of “exact” FU, such as retraining since the revoked data is never involved in the new retraining model.
However, for the another mainstream of “approximate” FU category, the auditing mechanism is critical and necessary, which can validate the effectiveness of these methods, i.e., How large is the difference between the approximate model and the exact retraining model?</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Membership inference attack</span>: Given a data sample and the black-box access of the trained model, the goal of this kind of attack is to detect whether the data sample is inside the training dataset of this model <cite class="ltx_cite ltx_citemacro_cite">Shokri and et al (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite>.
More specifically, we use adversarial machine learning to obtain an inference model, which can recognize the differences in the target model’s prediction results on the inputs that inside the training dataset versus outside the training dataset <cite class="ltx_cite ltx_citemacro_cite">Nasr and et al (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>); Hu and et al (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>.
Membership inference attack is very effective on detecting data leakage, which can reflect whether the approximate unlearning still contains the information of deleted data or not.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Information Leakage</span>: Many machine learning models will inherently leak some private information during their training process <cite class="ltx_cite ltx_citemacro_cite">Pustozerova and et
al (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>, such as the intermediate gradients .
Many existing works have shown that the raw training data can be recovered with the gradients of each update step, where the gradients are accessible for both user devices and server in the FL scenario.
This kind of information leakage is utilized and called gradient inversion attacks <cite class="ltx_cite ltx_citemacro_cite">Zhang and et al (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>.
Therefore, we can compare the model difference before and after unlearning to infer whether the information of deleted data will still be leaked.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Backdoor attacks</span>: The techniques of backdoor attack are proposed to inject backdoors into the training data samples to poison the model. The derived model will make accurate predictions on clean data samples, but trigger the backdoor to make the wrong predictions on contaminated data samples.
The backdoor attack technique can be utilized to validate the effectiveness of approximate FU.
More specifically, the user devices can contaminate part of their own data samples during the FL training process <cite class="ltx_cite ltx_citemacro_cite">Sun and et al (<a href="#bib.bib45" title="" class="ltx_ref">2019</a>); Bagdasaryan and et
al (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>); Wang and et al (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>.
If the contaminated data samples are successfully deleted, the unlearning model will predict them into their correct class. Otherwise, the unlearning model will trigger the backdoor to assign them to the wrong class.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion &amp; Future Vision</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduce a novel concept of the FL lifecycle, which integrates both federated increasing learning (FIL) and federated unlearning (FU) to achieve knowledge editing for the FL system.
As far as we know, it is the first time providing a comprehensive survey on FL system knowledge editing, including concepts, perspectives, challenges, and future vision.
We summarize the state-of-the-art approaches for knowledge learning &amp; unlearning in FL system and organize a clear taxonomy of them for handling different challenges.
Moreover, we reclassify the representative verification and auditing mechanisms, which ensure that the knowledge editing process follows the specified requirements while the results are consistent with the expectation.
Although each of the current knowledge editing techniques achieves similar purposes, they are still independent of each other because of the different methods used.
Therefore, we discuss some promising directions within the future vision.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Flexible &amp; Unified Knowledge Editing</span>:
So far, knowledge editing needs to find respective solutions for various challenges, which makes their application scenarios very limited.
There is an urgent need for a self-contained and unified knowledge editing framework that can flexibly achieve both learning and unlearning requirements.
For example, gradient descent is applied for the learning process, while gradient ascent may be utilized for the unlearning process.
A unified framework allows for more freedom of knowledge editing within the system and a large number of subsequent derivative efforts based on the same technical kernel can be integrated into the framework as components.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Knowledge Disentanglement &amp; Reassembly</span>:
Another promising direction is knowledge architecture advances. If the knowledge architecture inside the model can achieve free assembly like building blocks, the whole knowledge editing process will become extremely easy.
A few existing works have designed multi-branch models for knowledge disentanglement in FL systems, which enables the user devices to independently extract different knowledge representations.
For example, the whole model knowledge of each user can be disentangled into global shared knowledge and local personalized knowledge in <cite class="ltx_cite ltx_citemacro_cite">Luo and et al (<a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>.
Therefore, the learning and unlearning processes just need to simply add or remove the corresponding branches.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold">Trustworthy FL Community</span>:
With the explosive growth of user devices, user heterogeneity, and their varying relationships, it’s difficult for traditional FL with an authoritative server to manage the whole community. The ultimate future form of FL is an autonomous and trustworthy community with massive participants, where blockchain-based techniques are the foundation to support this future vision.
Each user’s activity details in the FL community are uploaded to their respective blocks for maintenance, including the learning and unlearning records, the data usage (only the data index, not the local raw data itself), the resource allocation, and so on.
Any participants in the community can initiate the verification and auditing for others.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan and et
al [2020]</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan and et al.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">ICAIS</span>, pages 2938–2948. PMLR, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baumhauer and et
al [2022]</span>
<span class="ltx_bibblock">
Thomas Baumhauer and et al.

</span>
<span class="ltx_bibblock">Machine unlearning: Linear filtration for logit-based classifiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Machine Learning</span>, 111(9):3203–3226, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao and et al [2023]</span>
<span class="ltx_bibblock">
X. Cao and et al.

</span>
<span class="ltx_bibblock">Fedrecover: Recovering from poisoning attacks in federated learning
using historical information.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Symposium on Security and Privacy (SP)</span>, pages 326–343,
2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casado and et al [2020]</span>
<span class="ltx_bibblock">
Fernando E Casado and et al.

</span>
<span class="ltx_bibblock">Federated and continual learning for classification tasks in a
society of devices.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv:2006.07129</span>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casado and et al [2022]</span>
<span class="ltx_bibblock">
Fernando E Casado and et al.

</span>
<span class="ltx_bibblock">Concept drift detection and adaptation for federated and continual
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Multimedia Tools and Applications</span>, 81(3):3397–3419, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and et al [2018]</span>
<span class="ltx_bibblock">
Fei Chen and et al.

</span>
<span class="ltx_bibblock">Federated meta-learning with fast convergence and efficient
communication.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.07876</span>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and et al [2020]</span>
<span class="ltx_bibblock">
Yiqiang Chen and et al.

</span>
<span class="ltx_bibblock">Fedhealth: A federated transfer learning framework for wearable
healthcare.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent Systems</span>, 35(4):83–93, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dayan and et al [2021]</span>
<span class="ltx_bibblock">
Ittai Dayan and et al.

</span>
<span class="ltx_bibblock">Federated learning for predicting clinical outcomes in patients with
covid-19.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Nature medicine</span>, 27(10):1735–1743, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Lange and et al [2021]</span>
<span class="ltx_bibblock">
Matthias De Lange and et al.

</span>
<span class="ltx_bibblock">A continual learning survey: Defying forgetting in classification
tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE TPAMI</span>, 44(7):3366–3385, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong and et al [2014]</span>
<span class="ltx_bibblock">
Chao Dong and et al.

</span>
<span class="ltx_bibblock">Learning a deep convolutional network for image super-resolution.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ECCV</span>, pages 184–199. Springer, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong and et al [2022]</span>
<span class="ltx_bibblock">
Weilong Dong and et al.

</span>
<span class="ltx_bibblock">Fewfedweight: Few-shot federated learning framework across multiple
nlp tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv:2212.08354</span>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi Zhu.

</span>
<span class="ltx_bibblock">Federated class-incremental learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 10164–10173, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ezzeldin and et
al [2021]</span>
<span class="ltx_bibblock">
Yahya H Ezzeldin and et al.

</span>
<span class="ltx_bibblock">Fairfed: Enabling group fairness in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2110.00857</span>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan and et al [2021]</span>
<span class="ltx_bibblock">
Chenyou Fan and et al.

</span>
<span class="ltx_bibblock">Federated few-shot learning with adversarial learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">WiOpt</span>, pages 1–8. IEEE, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finn and et al [2017]</span>
<span class="ltx_bibblock">
Chelsea Finn and et al.

</span>
<span class="ltx_bibblock">Model-agnostic meta-learning for fast adaptation of deep networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICML</span>, pages 1126–1135. PMLR, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraboni and et al [2022]</span>
<span class="ltx_bibblock">
Yann Fraboni and et al.

</span>
<span class="ltx_bibblock">Sequential informed federated unlearning: Efficient and provable
client unlearning in federated optimization, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong and et al [2022]</span>
<span class="ltx_bibblock">
Jinu Gong and et al.

</span>
<span class="ltx_bibblock">Forget-svgd: Particle-based bayesian federated unlearning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE DSLW</span>, pages 1–6, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow and et
al [2013]</span>
<span class="ltx_bibblock">
Ian J Goodfellow and et al.

</span>
<span class="ltx_bibblock">An empirical investigation of catastrophic forgetting in
gradient-based neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv:1312.6211</span>, 2013.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta <span id="bib.bib19.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi Malvajerdi,
and Christopher Waites.

</span>
<span class="ltx_bibblock">Adaptive machine unlearning.

</span>
<span class="ltx_bibblock">In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan,
editors, <span id="bib.bib19.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halimi and et al [2022]</span>
<span class="ltx_bibblock">
Anisa Halimi and et al.

</span>
<span class="ltx_bibblock">Federated unlearning: How to efficiently erase a client in fl?

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendryx and et
al [2021]</span>
<span class="ltx_bibblock">
Sean M Hendryx and et al.

</span>
<span class="ltx_bibblock">Federated reconnaissance: Efficient, distributed, class-incremental
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv:2109.00150</span>, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong and et al [2021]</span>
<span class="ltx_bibblock">
Junyuan Hong and et al.

</span>
<span class="ltx_bibblock">Federated adversarial debiasing for fair and transferable
representations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">ACM SIGKDD</span>, pages 617–627, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu and et al [2021]</span>
<span class="ltx_bibblock">
Hongsheng Hu and et al.

</span>
<span class="ltx_bibblock">Source inference attacks in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2021 IEEE International Conference on Data Mining (ICDM)</span>,
pages 1102–1107. IEEE, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and et al [2022a]</span>
<span class="ltx_bibblock">
Wenke Huang and et al.

</span>
<span class="ltx_bibblock">Few-shot model agnostic federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">ACM MM</span>, pages 7309–7316, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and et al [2022b]</span>
<span class="ltx_bibblock">
Wenke Huang and et al.

</span>
<span class="ltx_bibblock">Learn from others and be yourself in heterogeneous federated
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 10143–10153, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia and et al [2021]</span>
<span class="ltx_bibblock">
Hengrui Jia and et al.

</span>
<span class="ltx_bibblock">Proof-of-learning: Definitions and practice.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Symposium on Security and Privacy (SP)</span>, pages
1039–1056, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick and et
al [2017]</span>
<span class="ltx_bibblock">
James Kirkpatrick and et al.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the national academy of sciences</span>,
114(13):3521–3526, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Legate and et al [2022]</span>
<span class="ltx_bibblock">
Gwen Legate and et al.

</span>
<span class="ltx_bibblock">Reducing forgetting in federated learning with truncated
cross-entropy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">NeurIPS 2022 Workshop</span>, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Qinbin Li and et al.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: vision, hype and reality for
data privacy and protection.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">IEEE TKDE</span>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and et al [2021]</span>
<span class="ltx_bibblock">
Gaoyang Liu and et al.

</span>
<span class="ltx_bibblock">Federaser: Enabling efficient client-level data removal from
federated learning models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE/ACM IWQOS</span>, pages 1–10, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and et al [2022a]</span>
<span class="ltx_bibblock">
Yang Liu and et al.

</span>
<span class="ltx_bibblock">Revfrf: Enabling cross-domain random forest training with revocable
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">IEEE TDSC</span>, 19(6):3671–3685, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and et al [2022b]</span>
<span class="ltx_bibblock">
Yi Liu and et al.

</span>
<span class="ltx_bibblock">The right to be forgotten in federated learning: An efficient
realization with rapid retraining.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM</span>, pages 1749–1758, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo and et al [2022]</span>
<span class="ltx_bibblock">
Zhengquan Luo and et al.

</span>
<span class="ltx_bibblock">Disentangled federated learning for tackling attributes skew via
invariant aggregation and diversity transferring.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2206.06818</span>, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masana and et al [2020]</span>
<span class="ltx_bibblock">
Marc Masana and et al.

</span>
<span class="ltx_bibblock">Class-incremental learning: survey and performance evaluation on
image classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">arXiv:2010.15277</span>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan and et
al [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan and et al.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasr and et al [2019]</span>
<span class="ltx_bibblock">
Milad Nasr and et al.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning: Passive and active
white-box inference attacks against centralized and federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE symposium on security and privacy (SP)</span>, pages 739–753,
2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pustozerova and et
al [2020]</span>
<span class="ltx_bibblock">
Anastasia Pustozerova and et al.

</span>
<span class="ltx_bibblock">Information leaks in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the Network and Distributed System Security
Symposium</span>, volume 10, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rebuffi and et al [2017]</span>
<span class="ltx_bibblock">
Sylvestre-Alvise Rebuffi and et al.

</span>
<span class="ltx_bibblock">icarl: Incremental classifier and representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 2001–2010, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieke and et al [2020]</span>
<span class="ltx_bibblock">
Nicola Rieke and et al.

</span>
<span class="ltx_bibblock">The future of digital health with federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">NPJ digital medicine</span>, 3(1):1–7, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts and et al [2020]</span>
<span class="ltx_bibblock">
Adam Roberts and et al.

</span>
<span class="ltx_bibblock">How much knowledge can you pack into the parameters of a language
model?

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.08910</span>, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samarakoon and et
al [2019]</span>
<span class="ltx_bibblock">
Sumudu Samarakoon and et al.

</span>
<span class="ltx_bibblock">Distributed federated learning for ultra-reliable low-latency
vehicular communications.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">IEEE TC</span>, 68(2):1146–1159, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber [2015]</span>
<span class="ltx_bibblock">
Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Deep learning in neural networks: An overview.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Neural networks</span>, 61:85–117, 2015.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoham and et al [2019]</span>
<span class="ltx_bibblock">
Neta Shoham and et al.

</span>
<span class="ltx_bibblock">Overcoming forgetting in federated learning on non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">arXiv:1910.07796</span>, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri and et al [2017]</span>
<span class="ltx_bibblock">
Reza Shokri and et al.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">2017 IEEE symposium on security and privacy (SP)</span>, pages
3–18. IEEE, 2017.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and et al [2019]</span>
<span class="ltx_bibblock">
Ziteng Sun and et al.

</span>
<span class="ltx_bibblock">Can you really backdoor federated learning?

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.07963</span>, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thrun [1995]</span>
<span class="ltx_bibblock">
Sebastian Thrun.

</span>
<span class="ltx_bibblock">A lifelong learning perspective for mobile robot control.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Intelligent robots and systems</span>, pages 201–214. Elsevier,
1995.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Usmanova and et
al [2022]</span>
<span class="ltx_bibblock">
Anastasiia Usmanova and et al.

</span>
<span class="ltx_bibblock">Federated continual learning through distillation in pervasive
computing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">IEEE SMARTCOMP</span>, pages 86–91, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and et al [2020]</span>
<span class="ltx_bibblock">
Hongyi Wang and et al.

</span>
<span class="ltx_bibblock">Attack of the tails: Yes, you really can backdoor federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
33:16070–16084, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and et al [2021]</span>
<span class="ltx_bibblock">
Lixu Wang and et al.

</span>
<span class="ltx_bibblock">Addressing class imbalance in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">AAAI</span>, volume 35, pages 10165–10173, 2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and et
al [2022]</span>
<span class="ltx_bibblock">
Junxiao Wang and et al.

</span>
<span class="ltx_bibblock">Federated unlearning via class-discriminative pruning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM Web Conference 2022</span>, pages 622–632,
2022.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warnat-Herresthal and et
al [2021]</span>
<span class="ltx_bibblock">
Stefanie Warnat-Herresthal and et al.

</span>
<span class="ltx_bibblock">Swarm learning for decentralized and confidential clinical machine
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Nature</span>, 594(7862):265–270, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and et al [2019]</span>
<span class="ltx_bibblock">
Yue Wu and et al.

</span>
<span class="ltx_bibblock">Large scale incremental learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 374–382, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and et al [2022a]</span>
<span class="ltx_bibblock">
Chen Wu and et al.

</span>
<span class="ltx_bibblock">Federated unlearning with knowledge distillation.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.09441</span>, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and et al [2022b]</span>
<span class="ltx_bibblock">
Leijie Wu and et al.

</span>
<span class="ltx_bibblock">Federated unlearning: Guarantee the right of clients to forget.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">IEEE Network</span>, 36(5):129–135, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and et al [2022]</span>
<span class="ltx_bibblock">
Xinlei Xu and et al.

</span>
<span class="ltx_bibblock">Client selection based weighted federated few-shot learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Applied Soft Computing</span>, 128:109488, 2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan and et al [2022]</span>
<span class="ltx_bibblock">
Haonan Yan and et al.

</span>
<span class="ltx_bibblock">Arcane: An efficient architecture for exact machine unlearning.

</span>
<span class="ltx_bibblock">In Lud De Raedt, editor, <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">IJCAI</span>, pages 4006–4013, 7 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and et al [2019]</span>
<span class="ltx_bibblock">
Qiang Yang and et al.

</span>
<span class="ltx_bibblock">Federated learning.

</span>
<span class="ltx_bibblock">13(3):1–207, 2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and et al [2021]</span>
<span class="ltx_bibblock">
Qian Yang and et al.

</span>
<span class="ltx_bibblock">Flop: Federated learning on medical datasets using partial networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">ACM SIGKDD</span>, pages 3845–3853, 2021.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao and Sun [2020]</span>
<span class="ltx_bibblock">
Xin Yao and Lifeng Sun.

</span>
<span class="ltx_bibblock">Continual local training for better initialization of federated
models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">ICIP</span>, pages 1736–1740. IEEE, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon and et al [2021]</span>
<span class="ltx_bibblock">
Jaehong Yoon and et al.

</span>
<span class="ltx_bibblock">Federated continual learning with weighted inter-client transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">ICML</span>, pages 12073–12086. PMLR, 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu and et al [2022]</span>
<span class="ltx_bibblock">
Sihao Yu and et al.

</span>
<span class="ltx_bibblock">Legonet: A fast and exact unlearning architecture.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.16023</span>, 2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan and et al [2023]</span>
<span class="ltx_bibblock">
Wei Yuan and et al.

</span>
<span class="ltx_bibblock">Federated unlearning for on-device recommendation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">ACM WSDM</span>, WSDM ’23. Association for Computing Machinery,
2023.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and et al [2020]</span>
<span class="ltx_bibblock">
Xiaoli Zhang and et al.

</span>
<span class="ltx_bibblock">Enabling execution assurance of federated learning at untrusted
participants.

</span>
<span class="ltx_bibblock">In <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM</span>, pages 1877–1886, 2020.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and et al [2022]</span>
<span class="ltx_bibblock">
Rui Zhang and et al.

</span>
<span class="ltx_bibblock">A survey on gradient inversion: Attacks, defenses and future
directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2206.07284</span>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.01430" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.01431" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.01431">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.01431" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.01432" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 01:51:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
