<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>NARF24: Estimating Articulated Object Structure for Implicit Rendering</title>
<!--Generated on Sun Sep 15 19:04:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09829v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S1" title="In NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S2" title="In NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S3" title="In NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4" title="In NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.SS1" title="In IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Real World Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.SS1.SSS1" title="In IV-A Real World Datasets ‣ IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Progress-Tools Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.SS1.SSS2" title="In IV-A Real World Datasets ‣ IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Sparse Segments Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.SS1.SSS3" title="In IV-A Real World Datasets ‣ IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span>Registration Ablation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.SS2" title="In IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Simulated Articulated Arm</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S5" title="In NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">NARF24: Estimating Articulated Object Structure for Implicit Rendering </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stanley Lewis<sup class="ltx_sup" id="id5.5.id1"><span class="ltx_text ltx_font_italic" id="id5.5.id1.1">1</span></sup>, Tom Gao <sup class="ltx_sup" id="id6.6.id2"><span class="ltx_text ltx_font_italic" id="id6.6.id2.1">1</span></sup> and Odest Chadwicke Jenkins<sup class="ltx_sup" id="id7.7.id3"><span class="ltx_text ltx_font_italic" id="id7.7.id3.1">1</span></sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id8.8.id1"><span class="ltx_text ltx_font_italic" id="id8.8.id1.1">1</span></sup>S. Lewis, Tom Gao, and O.C. Jenkins are with the Robotics Department,
University of Michigan, Ann Arbor, MI 48109 {<span class="ltx_text ltx_font_typewriter" id="id9.9.id2" style="font-size:90%;">stanlew, zimingg, ocj}@umich.edu
<br class="ltx_break"/></span>This work is supported in part by Ford Motor Company, in part by J.P. Morgan AI Research, and in part
by Amazon.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id10.id1">Articulated objects and their representations pose a difficult problem for robots. These objects require not only representations of geometry and texture, but also of the various connections and joint parameters that make up each articulation. We propose a method that learns a common Neural Radiance Field (NeRF) representation across a small number of collected scenes. This representation is combined with a parts-based image segmentation to produce an implicit-space part localization, from which the connectivity and joint parameters of the articulated object can be estimated, thus enabling configuration-conditioned rendering.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Articulated objects pose significant challenges for robots due to their complex degrees of freedom compared to rigid-body objects, complicating tasks like pose estimation and grasp synthesis. The scarcity of suitable datasets exacerbates these challenges. This work introduces NARF24, a parts-based approach leveraging Neural Radiance Fields (NeRFs) to estimate prismatic and revolute joint parameters for non-loopy articulated objects using minimal observed configurations. Our method processes posed RGB images alongside image-space part segmentations. We validate our approach on a real-world robot-collected dataset, and another real-world dataset with sparse segmentation supervision. We show an ablation case for a pipeline component and present results from a serial chain manipulator in a simulated environment.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Articulated objects remain a difficult class of objects for robots to work with. Explicit methods using a parts-based approach combining 3d mesh models and URDF (Unified Robotics Description Format) files have seen success <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib1" title="">1</a>]</cite>. However, creating these models is laborious and difficult. Neural Radiance Fields (NeRF)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib2" title="">2</a>]</cite> have also shown success in breaking the reliance on mesh models<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib3" title="">3</a>]</cite>. Previous works such as NARF22 have continued the parts-based approach to produce NeRF representations of articulated objects. However, they still require a-priori knowledge of the articulated object’s structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib3" title="">3</a>]</cite>. We utilize a shared-scene representation for part extraction and localization from segmentation masks. This enables traditional joint parameter estimation methods to produce a URDF model of the object’s structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Other works have utilized deeply learned approaches to infer object articulations. URDFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib5" title="">5</a>]</cite> utilized simulation assets combined with known URDF models to learn an image-to-URDF transformer model. Weng el al. additionally inferred joint locations and angles from posed RGB input images via part segmentations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="452" id="S2.F1.g1" src="extracted/5856538/media/pitch_fig.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">NARF24 is a pipeline which takes in part-segmented images of an articulated object at a small number of configurations, then utilizes a scene-conditioned neural radiance field to estimate part poses and joint parameters. These create a URDF model and a subsequent articulation enabled NeRF for configurable rendering.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We start by collecting data on an articulated object at different articulation states. Each configuration example is referred to as a ’scene’. We utilize Nerfstudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib7" title="">7</a>]</cite> to train a NeRF model that additionally contains a per-scene embedding. This embedding allows for scene-conditioned rendering and additionally makes the implicit space distribution more consistent between each scene.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We utilize the segmentation masks to create per-part point clouds within each scene. These clouds are registered to each other using Teaser++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib8" title="">8</a>]</cite> initialized with the results of a point-to-point iterative closest point (ICP) registration.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We utilize the approach in Sturm et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib4" title="">4</a>]</cite> to estimate the joint type and parameters between each pair of parts using the registration coordinate frames. We estimate joint connectivity and classification using a chamfer distance computed between the part point clouds, and the expected point clouds based on the predicted joint transforms. These joint predictions can then be used within the NARF22 framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib3" title="">3</a>]</cite> to perform configurable re-rendering of the object.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Real World Datasets</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.4.1.1">IV-A</span>1 </span>Progress-Tools Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">For an initial experiment, we adopt the Progress-Tools Dataset from Pavlasek et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#bib.bib1" title="">1</a>]</cite>, which contains robot-collected data on a handful of articulated tools, along with the ground truth poses and part segmentations. NARF24 is successfully able to extract the articulation estimates for the clamp, as shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.F2" title="Figure 2 ‣ IV-A1 Progress-Tools Dataset ‣ IV-A Real World Datasets ‣ IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_tag">2</span></a>, which shows the learned articulated rendering at both the ground truth configuration, as well as at a counterfactual configuration as compared to the ground truth.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="135" id="S4.F2.g1" src="extracted/5856538/media/clamp_compare.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.5.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.6.2" style="font-size:90%;"> NARF24’s output when trained on the clamp in the ProgressTools dataset. <span class="ltx_text ltx_font_bold" id="S4.F2.6.2.1">Left:</span> the original dataset image. <span class="ltx_text ltx_font_bold" id="S4.F2.6.2.2">Middle:</span> The NARF24 output at the original pose and configuartion values. <span class="ltx_text ltx_font_bold" id="S4.F2.6.2.3">Right:</span> The NARF24 output at a counterfactual configuration (fully closed).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.4.1.1">IV-A</span>2 </span>Sparse Segments Dataset</h4>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S4.F3.g1" src="extracted/5856538/media/manual_result.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.5.2" style="font-size:90%;"> NARF24’s output when trained on the clamp with only 5 percent segmentation labeling. <span class="ltx_text ltx_font_bold" id="S4.F3.5.2.1">Left:</span> The ground truth image. <span class="ltx_text ltx_font_bold" id="S4.F3.5.2.2">Center, Right</span> Two counterfactual renderings of the clamp at different configurations, overlaid on the greyscale original image for context.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">The most manually intensive, least scalable portion of NARF24’s inputs to obtain are the part segmentations. To test how well our approach performs when only a small number of these segmentations are provided, a dataset for the clamp was collected, but only 5% of the images were labelled with segmentation masks. Images without masks were used to train the shared-scene representation, but were not utilized in the parts separation or subsequent registration steps. Even with the lower segmentation coverage, figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.F3" title="Figure 3 ‣ IV-A2 Sparse Segments Dataset ‣ IV-A Real World Datasets ‣ IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_tag">3</span></a> shows that acceptable articulation estimates were obtained.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.4.1.1">IV-A</span>3 </span>Registration Ablation</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">The part registration step is the most sensitive part of the NARF24 pipeline with respect to estimating object structure. Poor per-part localization leads to inaccurate joint estimations, which hinders any downstream parts-based training process. We thus performed an ablation study to show that our ICP initialized Teaser++ method is effective. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.F4" title="Figure 4 ‣ IV-A3 Registration Ablation ‣ IV-A Real World Datasets ‣ IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_tag">4</span></a> shows the resulting point clouds after training a single-part NeRF subsequent to estimating cross-scene part registration with ICP only, Teaser++ only, and Teaser++ with ICP initialization.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="224" id="S4.F4.g1" src="extracted/5856538/media/ablation.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.5.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.6.2" style="font-size:90%;">Qualitative ablation study on training a single-part NeRF subsequent to the part registration step. Top is the ground truth part, and bottom is the NeRF rendering. 
<br class="ltx_break"/>                                                                                                                                                 
<span class="ltx_text ltx_font_bold" id="S4.F4.6.2.1">Left (ICP only):</span> Performs adequately.
<br class="ltx_break"/>                                                                                                                                                 
<span class="ltx_text ltx_font_bold" id="S4.F4.6.2.2">Center (Teaser++ only):</span> fails for single-part NeRF training. 
<br class="ltx_break"/>                                                                                                                                                 
<span class="ltx_text ltx_font_bold" id="S4.F4.6.2.3">Right (ICP &amp; Teaser++):</span> Produces the best output.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Simulated Articulated Arm</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In order to demonstrate the best-case capabilities of the NARF24 system, a simulated dataset was created in PyBullet of a MyCobot 6 Degree of Freedom robot arm. Due to its simulated nature, this dataset has perfect camera poses, part segmentations, and part poses. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09829v1#S4.F5" title="Figure 5 ‣ IV-B Simulated Articulated Arm ‣ IV Results ‣ NARF24: Estimating Articulated Object Structure for Implicit Rendering"><span class="ltx_text ltx_ref_tag">5</span></a> shows an example rendering from the sim environment, along with a pair of renderings overlaid on top of each other, showing each of the arm’s joints being changed.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="457" id="S4.F5.1.g1" src="extracted/5856538/media/pybullet_render.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="457" id="S4.F5.2.g1" src="extracted/5856538/media/mycobot_renderings.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.6.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F5.7.2" style="font-size:90%;">Left:<span class="ltx_text ltx_font_medium" id="S4.F5.7.2.1"> Example output from the simulator environment. </span>Right:<span class="ltx_text ltx_font_medium" id="S4.F5.7.2.2"> Renderings of the arm at two different configurations of every joint, after training on the generated sim data (overlaid at base part)</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">NARF24 adopts a parts-based approach to enable more scalable learning and rendering of NeRF based implicit models for articulated objects. Results show that real-world data can be used to generate configuration-conditioned renderers even with small amounts of segmentation labels, and a simulated data example shows the effectiveness on a complex robot arm.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Pavlasek, S. Lewis, K. Desingh, and O. C. Jenkins, “Parts-based articulated object localization in clutter using belief propagation,” in <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pp. 10595–10602, IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Communications of the ACM</span>, vol. 65, no. 1, pp. 99–106, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Lewis, J. Pavlasek, and O. C. Jenkins, “Narf22: Neural articulated radiance fields for configuration-aware rendering,” in <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pp. 770–777, IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Sturm, C. Stachniss, and W. Burgard, “A probabilistic framework for learning kinematic models of articulated objects,” <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Journal of Artificial Intelligence Research</span>, vol. 41, pp. 477–526, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Q. Chen, M. Memmel, A. Fang, A. Walsman, D. Fox, and A. Gupta, “Urdformer: Constructing interactive realistic scenes from real images via simulation and generative modeling,” in <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Weng, B. Wen, J. Tremblay, V. Blukis, D. Fox, L. Guibas, and S. Birchfield, “Neural implicit representation for building digital twins of unknown articulated objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 3141–3150, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">et al.</span>, “Nerfstudio: A modular framework for neural radiance field development,” <span class="ltx_text ltx_font_italic" id="bib.bib7.2.2">arXiv preprint arXiv:2302.04264</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Yang, J. Shi, and L. Carlone, “Teaser: Fast and certifiable point cloud registration,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Robotics</span>, vol. 37, no. 2, pp. 314–333, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 15 19:04:55 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
