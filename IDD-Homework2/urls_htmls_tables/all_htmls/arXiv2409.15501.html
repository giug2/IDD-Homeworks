<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging</title>
<!--Generated on Mon Sep 23 19:37:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Pathological analysis Segmentation domain generalization COSAS." lang="en" name="keywords"/>
<base href="/html/2409.15501v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S1" title="In Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S2" title="In Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Adenocarcinoma Segmentation Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S3" title="In Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S4" title="In Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S4.SS1" title="In 4 Experiment ‣ Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Training and Optimization Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S4.SS2" title="In 4 Experiment ‣ Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S5" title="In Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>National Heart &amp; Lung Institute, Faculty of Medicine, Imperial College London, United Kingdom </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Turing Research and Innovation Cluster: Digital Twins, The Alan Turing Institute, London, United Kingdom </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Department of Computer Science, University College London, United Kingdom
<br class="ltx_break"/></span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>School of Computer Science and Engineering, University of New South Wales, Sydney, Australia 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id4.1"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">email: </span>{a.qayyum, s.niederer}@imperial.ac.uk</span></span></span>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id4.2"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">email: </span>m.mazher@ucl.ac.uk, imran.razzak@unsw.edu.au</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abdul Qayyum 
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Moona Mazher Imran Razzak
</span><span class="ltx_author_notes">3344</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven A Niederer
</span><span class="ltx_author_notes">1122</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Computer aided pathological analysis has been the gold standard for tumor diagnosis, however domain shift is a significant problem in histopathology. It may be caused by variability in anatomical structures, tissue preparation, and imaging processes—challenges the robustness of segmentation models. In this work, we present a framework consist of pre-trained encoder with a Swin-UNet architecture enhanced by a parallel cross-attention module to tackle the problem of adenocarcinoma segmentation across different organs and scanners, considering both morphological changes and scanner-induced domain variations. Experiment conducted on Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation challenge <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://cosas.grand-challenge.org/</span></span></span> dataset showed that our framework achieved segmentation scores of 0.7469 for the cross-organ track and 0.7597 for the cross-scanner track on the final challenge test sets, and effectively navigates diverse imaging conditions and improves segmentation accuracy across varying domains.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Pathological analysis Segmentation domain generalization COSAS.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Histopathology has become a transformative force, particularly in tumor diagnosis and tissue segmentation, and it can largely be attributed to the advances in deep learning. Recent development has shown tremendous promise in enhancing diagnostic precision, accelerating workflows, and enabling large-scale analysis of pathology data. However, a significant hurdle remains in the scalability and reliability of these AI-driven models: domain shift—the inherent variability in digital pathology images. Domain shift occurs due to various factors such as variations in color, brightness, and contrast, which result from differences in staining and scanner properties. These variations result in noticeable changes in image appearance, color, resolution, and texture, which can undermine the performance of segmentation algorithms when applied to data from domains different from those they were trained on. Diversifying the training data, such as incorporating samples from multiple medical centers, can help mitigate this issue. However, there is no assurance that the trained model will fully generalize to every scenario encountered in real-world applications.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Extensive research has conducted on understanding deep neural networks trained on natural images, exploring factors such as the learned representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib2" title="">2</a>]</cite>, the impact of loss function and batch size on these representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib11" title="">11</a>]</cite>, and how this knowledge can be applied to transfer learning, novelty detection, and identifying adversarial examples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib10" title="">10</a>]</cite>. Recently, Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS-2024) Challenge is organized to help overcome the above challenge address by fostering the development of resilient and generalizable semantic segmentation algorithms. The challenge specifically focuses on improving the adaptability of AI-driven diagnostic systems to domain shift, ensuring consistent performance across diverse scenarios, such as different organs and imaging devices. In this paper, we developed framework consist of pre-trained encoder with a Swin-UNet aided parallel cross-attention module for adenocarcinoma segmentation across different organs and scanners, considering both morphological changes and scanner-induced domain variations.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="317" id="S1.F1.g1" src="extracted/5865617/Framework.png" width="389"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(top) Proposed framework for cross-organ adenocarcinoma segmentation (bottom) pretrained Transformer block used in proposed model </figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Adenocarcinoma Segmentation Framework</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the proposed framework. For cross-domain adenocarcinoma segmentation, we adopt the Swin-UNet architecture. The primary challenge is effectively adapting generic pretrained models for segmentation tasks. We employ an encoder that mirrors the structure of recent vision model approaches, specifically Swin-UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#bib.bib7" title="">7</a>]</cite>, pretrained on the extensive ImageNet dataset. This enables the extraction of multi-scale features with long-range dependencies, reduces overfitting risk, and provides a robust initialization for the Swin-UNet, which is enhanced by a parallel cross-attention module. The Swin-UNet encoder consists of five stages. Stage 1 is a stem stage, featuring a convolutional layer for 2x down-sampling, with a 7<math alttext="\times" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mo id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><times id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">×</annotation></semantics></math>7 kernel, padding size of 3, and stride size of 2, followed by a 2D instance normalization layer. Unlike other models, Swin-UNet uses a gradual down-sampling approach, reducing the resolution by 2x at each stage while preserving low-level details. In the second stage, a patch embedding layer with a 2×2 patch size is employed, maintaining the feature resolution at 1/4 of the original image, similar to vision transformers.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The subsequent stages follow the Swin-UNet design, with each stage consisting of a patch merging layer for 2x down-sampling and several Swin blocks for high-level feature extraction. The number of Swin blocks in stages 2 through 5 are 2, 2, 9, 2, respectively. Feature dimensions increase quadratically across the stages, with D = 48, 96, 192, 384, 768. The Swin blocks and patch merging layers are initialized using ImageNet-pretrained weights from the Swin-Tiny model, though the patch embedding block is not initialized with pretrained weights due to differences in patch size and input channels. The integration of a parallel cross-attention module enhances feature extraction and improves segmentation performance. A Position-wise Attention Block captures spatial dependencies between pixels globally, while a multi-scale fusion attention block (MFAB) captures channel dependencies between feature maps through multi-scale semantic feature fusion. MFAB considers both high-level and low-level feature maps, fusing their channel dependencies to obtain rich multi-scale semantic information and improve network performance."</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="S2.F2.g1" src="extracted/5865617/R1.png" width="389"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Segmentation map produced by our proposed model for three different samples</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Recently, the Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS-2024) Challenge has been organized to tackle these issues by promoting the development of robust and generalizable semantic segmentation algorithms. COSAS-2024 Challenge offers two cross-domain histology datasets, each designed to test the generalization abilities of machine learning algorithms under different conditions. Both tasks consist of 290 ROIs from human adenocarcinoma samples, with tumor lesions manually annotated. In Task 1 Cross-Organ Adenocarcinoma Segmentation dataset consists of image patches extracted from whole slide images (WSIs) representing six distinct types of adenocarcinomas, each originating from a different organ, emphasizing biological variability while using a single scanner to control for technical factors. Task 2, Cross-Scanner Adenocarcinoma Segmentation, focuses on images from invasive breast carcinoma tissues captured by six different scanners each from a distinct manufacturer, testing models’ resilience to technical variability. The variability introduced by the different scanners, such as differences in image resolution, color, contrast, and other scanner-specific factors, presents a significant challenge. The dataset is designed to test the resilience of segmentation models to domain-shifts caused by technical factors, providing a robust benchmark for assessing the adaptability of algorithms to variations in imaging devices.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training and Optimization Techniques</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We trained and optimized our proposed solution on both tasks. These tasks required robust segmentation performance across varying organs and imaging scanners, which presented significant challenges for generalization. To improve model robustness, we applied a series of data augmentation techniques, including random flips, horizontal flips, and vertical flips. These augmentations helped increase the diversity of training samples and reduce overfitting, ensuring that the model learned more generalized features across different variations of the input images. For optimization, we used the Adam optimizer with a learning rate set to 0.0001. The model was trained using a batch size of 32 over 500 epochs, which allowed the network to learn from a large number of iterations, enhancing its ability to generalize to unseen data. Given the large size of the input images, which were approximately 1500x1500 pixels, we implemented a random patch extraction strategy. During training, we extracted random 224<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">×</annotation></semantics></math>224 patches from the original images, which made it computationally feasible to train the model while retaining significant detail in each patch. For testing and validation, we employed a sliding window approach to process the full-size images. The entire training process took approximately 5 hours.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To determine the final ranking for each task, the evaluation process was divided into two phases: the preliminary test phase, which accounts for 20% of the overall ranking, and the final test phase, contributing the remaining 80%. Our framework achieved final ranking scores of 0.7469 and 0.7597 for Task 1 and Task 2, respectively. These results demonstrate that our model performed consistently well across both tasks, delivering comparable performance in cross-organ and cross-scanner segmentation. The relatively high scores suggest that the model effectively generalizes across diverse datasets, highlighting its robustness in handling different anatomical structures (Task 1) and imaging modalities (Task 2). The slightly higher score in Task 2 indicates that the model may be better at adapting to variations introduced by different scanners, likely due to its strong feature extraction and transfer learning capabilities. This performance emphasizes the importance of robust pretraining and the use of advanced architectures like Swin-UNet with a parallel cross-attention module, enabling the model to efficiently capture multi-scale and cross-domain features. Overall, the results suggest that the proposed approach provides a solid foundation for adenocarcinoma segmentation across diverse medical imaging conditions. However, there may still be opportunities for improvement, particularly in cross-organ segmentation.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S2.F2" title="Figure 2 ‣ 2 Adenocarcinoma Segmentation Framework ‣ Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> showcases the segmentation map for Cross-Organ Adenocarcinoma Segmentation generated by our proposed model. This visualization clearly demonstrates the model’s ability to accurately predict adenocarcinoma regions across various anatomical structures. The figure shows that the model precisely delineates tumor boundaries, reflecting its strong capacity to localize and segment tumors effectively, even in the presence of anatomical variability. The segmentation maps further illustrate the model’s consistency across different organs, underscoring its robustness in handling diverse anatomical contexts. Additionally, the fine details of the tumors are well-preserved, showcasing the model’s capability to capture subtle variations in shape and size. Overall, the visualization in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15501v1#S2.F2" title="Figure 2 ‣ 2 Adenocarcinoma Segmentation Framework ‣ Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> validates the effectiveness of our approach in providing precise and reliable adenocarcinoma segmentation in cross-organ applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we presented an efficient framework for adenocarcinoma segmentation, addressing two challenging tasks: Cross-Organ Adenocarcinoma Segmentation and Cross-Scanner Adenocarcinoma Segmentation. By leveraging a Swin-UNet architecture with a parallel cross-attention module, proposed framework demonstrated strong performance in the COSAS-2024 challenge, achieving final ranking scores of 0.7469 for Task 1 and 0.7597 for Task 2. These results highlight the model’s ability to generalize effectively across diverse anatomical structures and imaging modalities.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Das, A., Khan, D.Z., Psychogyios, D., Zhang, Y., Hanrahan, J.G., Vasconcelos, F., Pang, Y., Chen, Z., Wu, J., Zou, X., et al.: Pitvis-2023 challenge: Workflow recognition in videos of endoscopic pituitary surgery. arXiv preprint arXiv:2409.01184 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
De Vente, C., Vermeer, K.A., Jaccard, N., Wang, H., Sun, H., Khader, F., Truhn, D., Aimyshev, T., Zhanibekuly, Y., Le, T.D., et al.: Airogs: artificial intelligence for robust glaucoma screening challenge. IEEE transactions on medical imaging <span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">43</span>(1), 542–557 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Frosst, N., Papernot, N., Hinton, G.: Analyzing and improving representations with the soft nearest neighbor loss. In: International conference on machine learning. pp. 2012–2020. PMLR (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.: Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Javed, S., Khan, T.M., Qayyum, A., Sowmya, A., Razzak, I.: Advancing medical image segmentation with mini-net: A lightweight solution tailored for efficient segmentation of medical images. arXiv preprint arXiv:2405.17520 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Javed, S., Khan, T.M., Qayyum, A., Sowmya, A., Razzak, I.: Region guided attention network for retinal vessel segmentation. arXiv preprint arXiv:2407.18970 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012–10022 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Mazher, M., Razzak, I., Qayyum, A., Tanveer, M., Beier, S., Khan, T., Niederer, S.A.: Self-supervised spatial–temporal transformer fusion based federated framework for 4d cardiovascular image segmentation. Information Fusion <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">106</span>, 102256 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Papernot, N., McDaniel, P.: Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Qayyum, A., Mazher, M., Lee, A., Solis-Lemus, J.A., Razzak, I., Niederer, S.A.: Assessment of left atrium motion deformation through full cardiac cycle. arXiv preprint arXiv:2405.17518 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Qayyum, A., Razzak, I., Mazher, M., Lu, X., Niederer, S.A.: Unsupervised unpaired multiple fusion adaptation aided with self-attention generative adversarial network for scar tissues segmentation framework. Information Fusion <span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">106</span>, 102226 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Stacke, K., Eilertsen, G., Unger, J., Lundström, C.: A closer look at domain shift for deep learning in histopathology. arXiv preprint arXiv:1909.11575 (2019)

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 19:37:55 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
