<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1807.10931] Deep Leaf Segmentation Using Synthetic Data</title><meta property="og:description" content="Automated segmentation of individual leaves of a plant in an image is a prerequisite to measure more complex phenotypic traits in high-throughput phenotyping. Applying state-of-the-art machine learning approaches to ta…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Leaf Segmentation Using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Deep Leaf Segmentation Using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1807.10931">

<!--Generated on Mon Feb 26 18:12:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\addauthor</span>
<p id="p1.2" class="ltx_p">Daniel WardDaniel.Ward@data61.csiro.au1
<span id="p1.2.1" class="ltx_ERROR undefined">\addauthor</span>Peyman MoghadamPeyman.Moghadam@data61.csiro.au2
<span id="p1.2.2" class="ltx_ERROR undefined">\addauthor</span>Nicolas HudsonNicolas.Hudson@data61.csiro.au3
<span id="p1.2.3" class="ltx_ERROR undefined">\addinstitution</span>
Robotics and Autonomous Systems 
<br class="ltx_break">The Commonwealth Scientific and Industrial Research Organisation (CSIRO), Data61
<br class="ltx_break">Brisbane, Australia

Deep Leaf Segmentation Using Synthetic Data</p>
</div>
<h1 class="ltx_title ltx_title_document">Deep Leaf Segmentation Using Synthetic Data</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Automated segmentation of individual leaves of a plant in an image is a prerequisite to measure more complex phenotypic traits in high-throughput phenotyping. Applying state-of-the-art machine learning approaches to tackle leaf instance segmentation requires a large amount of manually annotated training data. Currently,
the benchmark datasets for leaf segmentation contain only a few hundred labeled training images. In this paper, we propose a framework for leaf instance segmentation by augmenting real plant datasets with generated synthetic images of plants inspired by domain randomisation. We train a state-of-the-art deep learning segmentation architecture (Mask-RCNN) with a combination of real and synthetic images of <span id="id1.id1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> plants. Our proposed approach achieves 90% leaf segmentation score on the A1 test set outperforming the-state-of-the-art approaches for the CVPPP Leaf Segmentation Challenge (LSC). Our approach also achieves 81% mean performance over all five test datasets.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">To achieve sustainable agriculture, we need to expedite the breading of new plant varieties which consume less water, land or fertilizer and have greater resistance to parasites and diseases while producing greater crop yields.
Plant phenotyping is the process of relating plant varieties to genotype and environmental conditions and how these affect the observable plant traits.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In recent years, several computer vision and machine learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Scharr et al.(2016)Scharr, Minervini, French, Klukas, Kramer, Liu,
Luengo, Pape, Polder, Vukadinovic, et al.</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Moghadam et al.(2017)Moghadam, Ward, Goan, Jayawardena, Sikka, and
Hernandez</a>]</cite> have been proposed to increase the throughput of non-destructive phenotyping.
Automatic, non-destructive extraction of plant parameters such as plant height, shape, leaf area index (LAI) or growth rate throughout the crop growth cycle are essential for rapid phenotype discovery and analysis.
Phenotypic measurements are often done at two levels: plant-level measurements (projected LAI, height, plant architecture) or leaf-level measurements (individual leaf area, leaf count, leaf growth rate).
This paper focuses on the former category.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite substantial progress, segmentation of individual leaves (leaf instance segmentation) remains extremely challenging owing to the variability in leaf shapes and appearance over the life-cycle of the deformable object (plant).
Another major challenge is that leaves partially overlap and occlude each other as new leaves grow.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Currently, leading instance segmentation techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">He et al.(2017)He, Gkioxari, Dollár, and Girshick</a>]</cite> based on deep convolutional neural networks require huge amounts of annotated training data.
Manual annotation of images for a task such as instance segmentation is often time consuming and expensive.
For leaf instance segmentation there are only a few annotated datasets available and the size of these datasets is very small (a few hundred images) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Minervini et al.(2016)Minervini, Fischbach, Scharr, and
Tsaftaris</a>]</cite>.
In this paper we use the CVPPP dataset from the leading Leaf Segmentation Challenge (LSC). It is a small dataset of top-down 2D visible light images of <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> and tobacco plants from an indoor high throughput plant phenotyping system.
This dataset only contains 27 images of tobacco and 783 <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">Arabidopsis</span> images with pixel-level leaf segmentation labels.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant00075_rgb.jpg" id="S1.F1.1.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant00075_label.jpg" id="S1.F1.2.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant00099_rgb.jpg" id="S1.F1.3.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant00099_label.jpg" id="S1.F1.4.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example top down view of synthetic <span id="S1.F1.6.1" class="ltx_text ltx_font_italic">Arabidopsis</span> plants (a, c) and their segmentation labels (b, d).</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To address these problems, we propose a framework to augment limited annotated training data of real plants with synthetic plant images inspired by domain randomisation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Tremblay et al.(2018)Tremblay, Prakash, Acuna, Brophy, Jampani, Anil,
To, Cameracci, Boochoon, and Birchfield</a>]</cite>. Synthetic plant images come without the cost of data collection and annotation (examples shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Our main contributions can be summarised as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We outperform the state-of-the-art leaf segmentation approach by augmenting real data with synthetic data.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">This is the first time synthetic data has been used for the CVPPP Leaf Segmentation Challenge using a novel plant modeling system.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We investigate the application of domain randomisation concepts on deformable objects (plants) and found that sampling realistic foreground and background textures improved results.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There has been significant progress in the vision community on object instance segmentation algorithms, which requires the detection of each object (or leaf) in the image, and then the precise segmentation of each instance.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In this paper, we consider the Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">He et al.(2017)He, Gkioxari, Dollár, and Girshick</a>]</cite> framework, as it achieved state of the art single-model results on the COCO
instance segmentation task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
Dollár, and Zitnick</a>]</cite> in 2017, and feature pyramid derivatives of it remain the competition leaders. With the addition of the branch for predicting segmentation masks, Mask-RCNN is a low overhead extension of Faster-RCNN, which recently also demonstrated the best detection
performance among a family of object detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Huang et al.(2017)Huang, Rathod, Sun, Zhu, Korattikara, Fathi,
Fischer, Wojna, Song, Guadarrama, et al.</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Training detection or segmentation algorithms takes vast amounts of labeled data: COCO instance segmentation training has 123,287 images; ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Russakovsky et al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei</a>]</cite> detection has over one million images with annotated object bounding boxes, although some classes in it, such as <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">strawberry</em>, will appear in only hundreds of those images. One of the few large scale data sets where objects are broken down further into segmented components is ADE20K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Zhou et al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
Torralba</a>]</cite>, with 20,210 images with all objects, and components within segmented.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In comparison, the CVPPP LSC uses 810 images for training, each with a single plant with multiple leaf instances. Increasing the size of this training data set would lead to increased test set performance. Recent work in classifying birds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">Van Horn and Perona(2017)</a>]</cite>, while varying the training data set from 10 to 10,000 images, suggested a rule where the test set error drops by a factor of 2x each for every 10x increase in the number of training images.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Given a limited training dataset, where it is hard or expensive to annotate more real world data, recent work in computer vision looks at using simulated data to train networks. It known that the <em id="S2.p5.1.1" class="ltx_emph ltx_font_italic">reality gap</em>, or the inability of renders to create the statistics of the real world, lead to results where naive training and evaluations on synthetic images do not transfer to the real world.
The literature has several approaches to incorporating synthetic imagery including: Mixing generated objects over real world scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Dwibedi et al.(2017)Dwibedi, Misra, and Hebert</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Abu Alhaija et al.(2017)Abu Alhaija, Mustikovela, Mescheder, Geiger,
and Rother</a>]</cite>; domain randomisation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Tremblay et al.(2018)Tremblay, Prakash, Acuna, Brophy, Jampani, Anil,
To, Cameracci, Boochoon, and Birchfield</a>]</cite>, where parameters of the simulator creating the images are varied in non-realistic ways to make the networks invariant to them; creating photo-realistic renders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Abu Alhaija et al.(2017)Abu Alhaija, Mustikovela, Mescheder, Geiger,
and Rother</a>]</cite>, where substantial effort is made to reduce the reality gap directly; and General Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Bousmalis et al.(2017)Bousmalis, Silberman, Dohan, Erhan, and
Krishnan</a>]</cite> to automatically learn and compensate for important differences between simulated and real world data. Freezing a pre-trained feature extractor layer in the networks and using simulation to only train output layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hinterstoisser et al.(2017)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite> has shown amazing results for a limited set of rigid objects, when in fact the pre-trained features are expressive enough. Hinterstoisser <em id="S2.p5.1.2" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p5.1.3" class="ltx_ERROR undefined">\bmvaOneDot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hinterstoisser et al.(2017)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite> found that this resulted in features closer to those extracted from real images being extracted from the synthetic images.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">To the best of our knowledge, synthetic data has been used twice before in conjunction with the CVPPP datasets. In the absence of generating synthetic segmentation labels, both approaches focused on estimating the count of leaves in each image. Furthermore, each method’s synthetic data was rendered without a background.
ARIGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Giuffrida et al.(2017)Giuffrida, Scharr, and Tsaftaris</a>]</cite> presents a data driven approach where a GAN learns the distribution of the CVPPP A1 subset dataset and generates new samples to an expected leaf count. GANs are renowned for more accurately modeling texture than geometry and this is evident in the ARIGAN synthetic <span id="S2.p6.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> dataset.
In the second synthetic data approach, Ubbens <em id="S2.p6.1.2" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p6.1.3" class="ltx_ERROR undefined">\bmvaOneDot</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Ubbens et al.(2018)Ubbens, Cieslak, Prusinkiewicz, and
Stavness</a>]</cite> leverages a model based on the L-system formalism. L-systems are commonly used to describe the structure and development of plants. The design of their <span id="S2.p6.1.4" class="ltx_text ltx_font_italic">Arabidopsis</span> L-system model involved defining plant attributes including leaf shape and inclination angle.
These attributes were defined using functions drawn by manually placing control points within an L-systems library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Prusinkiewicz(2002)</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This work augments and compares performance of real world training to using synthetic leaf images. Leveraging ideas in the domain randomization literature, leaf textures are randomised and simulated data is overlayed onto random real world scenes. In addition, we evaluate freezing feature layers in models like in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hinterstoisser et al.(2017)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite>. The work here extends these ideas to deformable plant models where each object is made up of many leaf components, and does not adhere to the rigid body models used in most synthetic data approaches.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Synthetic Data Generation for Arabidopsis</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/1807.10931/assets/images/leafGeneration-crop.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="471" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The synthetic leaf generation pipeline. Each leaf in a synthesised plant is randomly deformed, textured and positioned.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our synthetic <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> generation pipeline (Figures <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.F3" title="Figure 3 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) begins with manual specification of geometric properties. This is done by defining an <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">inspiration leaf</span> rather than multiple functions like in L-systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Ubbens et al.(2018)Ubbens, Cieslak, Prusinkiewicz, and
Stavness</a>]</cite>. First, an inspiration leaf was designed by tracing a randomly chosen <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Arabidopsis</span> leaf image in Blender to produce a 3D mesh by adding and adjusting manipulation points. This was done only once for all synthetic data used in this study. Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, displays the steps involved in generating a single leaf from the original inspiration.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In order to model leaves of different shape and size, every leaf is randomly scaled along each axis independently. This process differs from other applications such as synthetic cars, where the resulting significant changes to object aspect ratio would be undesirable.
To provide variation in texture between leaves, each one is rendered with a different texture. Thirty leaf textures were extracted from the CVPPP training datasets and augmented using the data augmentation as described in Section <a href="#S3.SS2" title="3.2 Training &amp; Evaluation ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> with the addition of random adjustments to image exposure.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.9" class="ltx_p">Being rosettes, the leaves of an <span id="S3.SS1.p3.9.1" class="ltx_text ltx_font_italic">Arabidopsis</span> plant are arranged circularly. Furthermore, all leaves are at a similar height and remain close together <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Mündermann et al.(2005)Mündermann, Erasmus, Lane, Coen, and
Prusinkiewicz</a>]</cite>. Following this, our model can be intuitively thought of as an arrangement of leaves stemming out from a sphere. The location of leaf stems is most likely at the equator and equally likely at any point around the equator in the <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="xy" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑥</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">xy</annotation></semantics></math> plane. With the leaf initialised at the origin in the <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="xy" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></times><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝑥</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">xy</annotation></semantics></math> plane, the final position of the leaf is defined by rotating the mesh around the <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">x</annotation></semantics></math>, <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">y</annotation></semantics></math> and <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">z</annotation></semantics></math> axes independently. To produce the rosette, the rotation around the <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mi id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">z</annotation></semantics></math> axis is sampled from a uniform distribution, <math id="S3.SS1.p3.7.m7.2" class="ltx_Math" alttext="U(0,2\pi)" display="inline"><semantics id="S3.SS1.p3.7.m7.2a"><mrow id="S3.SS1.p3.7.m7.2.2" xref="S3.SS1.p3.7.m7.2.2.cmml"><mi id="S3.SS1.p3.7.m7.2.2.3" xref="S3.SS1.p3.7.m7.2.2.3.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.7.m7.2.2.2" xref="S3.SS1.p3.7.m7.2.2.2.cmml">​</mo><mrow id="S3.SS1.p3.7.m7.2.2.1.1" xref="S3.SS1.p3.7.m7.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.7.m7.2.2.1.1.2" xref="S3.SS1.p3.7.m7.2.2.1.2.cmml">(</mo><mn id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml">0</mn><mo id="S3.SS1.p3.7.m7.2.2.1.1.3" xref="S3.SS1.p3.7.m7.2.2.1.2.cmml">,</mo><mrow id="S3.SS1.p3.7.m7.2.2.1.1.1" xref="S3.SS1.p3.7.m7.2.2.1.1.1.cmml"><mn id="S3.SS1.p3.7.m7.2.2.1.1.1.2" xref="S3.SS1.p3.7.m7.2.2.1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p3.7.m7.2.2.1.1.1.1" xref="S3.SS1.p3.7.m7.2.2.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p3.7.m7.2.2.1.1.1.3" xref="S3.SS1.p3.7.m7.2.2.1.1.1.3.cmml">π</mi></mrow><mo stretchy="false" id="S3.SS1.p3.7.m7.2.2.1.1.4" xref="S3.SS1.p3.7.m7.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.2b"><apply id="S3.SS1.p3.7.m7.2.2.cmml" xref="S3.SS1.p3.7.m7.2.2"><times id="S3.SS1.p3.7.m7.2.2.2.cmml" xref="S3.SS1.p3.7.m7.2.2.2"></times><ci id="S3.SS1.p3.7.m7.2.2.3.cmml" xref="S3.SS1.p3.7.m7.2.2.3">𝑈</ci><interval closure="open" id="S3.SS1.p3.7.m7.2.2.1.2.cmml" xref="S3.SS1.p3.7.m7.2.2.1.1"><cn type="integer" id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">0</cn><apply id="S3.SS1.p3.7.m7.2.2.1.1.1.cmml" xref="S3.SS1.p3.7.m7.2.2.1.1.1"><times id="S3.SS1.p3.7.m7.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.7.m7.2.2.1.1.1.1"></times><cn type="integer" id="S3.SS1.p3.7.m7.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.7.m7.2.2.1.1.1.2">2</cn><ci id="S3.SS1.p3.7.m7.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.7.m7.2.2.1.1.1.3">𝜋</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.2c">U(0,2\pi)</annotation></semantics></math>. Similarly, the leaf pitch and roll are sampled from <math id="S3.SS1.p3.8.m8.2" class="ltx_Math" alttext="U(\frac{-\pi}{4},\frac{\pi}{4})" display="inline"><semantics id="S3.SS1.p3.8.m8.2a"><mrow id="S3.SS1.p3.8.m8.2.3" xref="S3.SS1.p3.8.m8.2.3.cmml"><mi id="S3.SS1.p3.8.m8.2.3.2" xref="S3.SS1.p3.8.m8.2.3.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.8.m8.2.3.1" xref="S3.SS1.p3.8.m8.2.3.1.cmml">​</mo><mrow id="S3.SS1.p3.8.m8.2.3.3.2" xref="S3.SS1.p3.8.m8.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.8.m8.2.3.3.2.1" xref="S3.SS1.p3.8.m8.2.3.3.1.cmml">(</mo><mfrac id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml"><mrow id="S3.SS1.p3.8.m8.1.1.2" xref="S3.SS1.p3.8.m8.1.1.2.cmml"><mo id="S3.SS1.p3.8.m8.1.1.2a" xref="S3.SS1.p3.8.m8.1.1.2.cmml">−</mo><mi id="S3.SS1.p3.8.m8.1.1.2.2" xref="S3.SS1.p3.8.m8.1.1.2.2.cmml">π</mi></mrow><mn id="S3.SS1.p3.8.m8.1.1.3" xref="S3.SS1.p3.8.m8.1.1.3.cmml">4</mn></mfrac><mo id="S3.SS1.p3.8.m8.2.3.3.2.2" xref="S3.SS1.p3.8.m8.2.3.3.1.cmml">,</mo><mfrac id="S3.SS1.p3.8.m8.2.2" xref="S3.SS1.p3.8.m8.2.2.cmml"><mi id="S3.SS1.p3.8.m8.2.2.2" xref="S3.SS1.p3.8.m8.2.2.2.cmml">π</mi><mn id="S3.SS1.p3.8.m8.2.2.3" xref="S3.SS1.p3.8.m8.2.2.3.cmml">4</mn></mfrac><mo stretchy="false" id="S3.SS1.p3.8.m8.2.3.3.2.3" xref="S3.SS1.p3.8.m8.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.2b"><apply id="S3.SS1.p3.8.m8.2.3.cmml" xref="S3.SS1.p3.8.m8.2.3"><times id="S3.SS1.p3.8.m8.2.3.1.cmml" xref="S3.SS1.p3.8.m8.2.3.1"></times><ci id="S3.SS1.p3.8.m8.2.3.2.cmml" xref="S3.SS1.p3.8.m8.2.3.2">𝑈</ci><interval closure="open" id="S3.SS1.p3.8.m8.2.3.3.1.cmml" xref="S3.SS1.p3.8.m8.2.3.3.2"><apply id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1"><divide id="S3.SS1.p3.8.m8.1.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1"></divide><apply id="S3.SS1.p3.8.m8.1.1.2.cmml" xref="S3.SS1.p3.8.m8.1.1.2"><minus id="S3.SS1.p3.8.m8.1.1.2.1.cmml" xref="S3.SS1.p3.8.m8.1.1.2"></minus><ci id="S3.SS1.p3.8.m8.1.1.2.2.cmml" xref="S3.SS1.p3.8.m8.1.1.2.2">𝜋</ci></apply><cn type="integer" id="S3.SS1.p3.8.m8.1.1.3.cmml" xref="S3.SS1.p3.8.m8.1.1.3">4</cn></apply><apply id="S3.SS1.p3.8.m8.2.2.cmml" xref="S3.SS1.p3.8.m8.2.2"><divide id="S3.SS1.p3.8.m8.2.2.1.cmml" xref="S3.SS1.p3.8.m8.2.2"></divide><ci id="S3.SS1.p3.8.m8.2.2.2.cmml" xref="S3.SS1.p3.8.m8.2.2.2">𝜋</ci><cn type="integer" id="S3.SS1.p3.8.m8.2.2.3.cmml" xref="S3.SS1.p3.8.m8.2.2.3">4</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.2c">U(\frac{-\pi}{4},\frac{\pi}{4})</annotation></semantics></math> and <math id="S3.SS1.p3.9.m9.2" class="ltx_Math" alttext="U(0,\frac{\pi}{4})" display="inline"><semantics id="S3.SS1.p3.9.m9.2a"><mrow id="S3.SS1.p3.9.m9.2.3" xref="S3.SS1.p3.9.m9.2.3.cmml"><mi id="S3.SS1.p3.9.m9.2.3.2" xref="S3.SS1.p3.9.m9.2.3.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.9.m9.2.3.1" xref="S3.SS1.p3.9.m9.2.3.1.cmml">​</mo><mrow id="S3.SS1.p3.9.m9.2.3.3.2" xref="S3.SS1.p3.9.m9.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.9.m9.2.3.3.2.1" xref="S3.SS1.p3.9.m9.2.3.3.1.cmml">(</mo><mn id="S3.SS1.p3.9.m9.1.1" xref="S3.SS1.p3.9.m9.1.1.cmml">0</mn><mo id="S3.SS1.p3.9.m9.2.3.3.2.2" xref="S3.SS1.p3.9.m9.2.3.3.1.cmml">,</mo><mfrac id="S3.SS1.p3.9.m9.2.2" xref="S3.SS1.p3.9.m9.2.2.cmml"><mi id="S3.SS1.p3.9.m9.2.2.2" xref="S3.SS1.p3.9.m9.2.2.2.cmml">π</mi><mn id="S3.SS1.p3.9.m9.2.2.3" xref="S3.SS1.p3.9.m9.2.2.3.cmml">4</mn></mfrac><mo stretchy="false" id="S3.SS1.p3.9.m9.2.3.3.2.3" xref="S3.SS1.p3.9.m9.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m9.2b"><apply id="S3.SS1.p3.9.m9.2.3.cmml" xref="S3.SS1.p3.9.m9.2.3"><times id="S3.SS1.p3.9.m9.2.3.1.cmml" xref="S3.SS1.p3.9.m9.2.3.1"></times><ci id="S3.SS1.p3.9.m9.2.3.2.cmml" xref="S3.SS1.p3.9.m9.2.3.2">𝑈</ci><interval closure="open" id="S3.SS1.p3.9.m9.2.3.3.1.cmml" xref="S3.SS1.p3.9.m9.2.3.3.2"><cn type="integer" id="S3.SS1.p3.9.m9.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1">0</cn><apply id="S3.SS1.p3.9.m9.2.2.cmml" xref="S3.SS1.p3.9.m9.2.2"><divide id="S3.SS1.p3.9.m9.2.2.1.cmml" xref="S3.SS1.p3.9.m9.2.2"></divide><ci id="S3.SS1.p3.9.m9.2.2.2.cmml" xref="S3.SS1.p3.9.m9.2.2.2">𝜋</ci><cn type="integer" id="S3.SS1.p3.9.m9.2.2.3.cmml" xref="S3.SS1.p3.9.m9.2.2.3">4</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m9.2c">U(0,\frac{\pi}{4})</annotation></semantics></math> respectively to subtly randomise leaf pose. Uniform distributions were used to provide a wide variety of leaf positions. This process is shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and repeated multiple times according to a normal distribution of leaves per plant centered around the mean leaf count in the CVPPP A1 subset dataset but with wider variance (Figure <a href="#S4.F6" title="Figure 6 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1807.10931/assets/images/plantGeneration.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="432" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The synthetic data pipeline. A 3D plant model of synthetic leaves (Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) is assembled and rendered over random plant pot background.</figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> defines the plant generation pipeline. The number of leaves in a generated plant is first sampled from a provided distribution and the leaf generation pipeline (Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) is called. A top down view of the 3D plant model is then rendered to simulate the CVPPP data. Like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Tremblay et al.(2018)Tremblay, Prakash, Acuna, Brophy, Jampani, Anil,
To, Cameracci, Boochoon, and Birchfield</a>]</cite>, the camera and lighting positions were varied however only a single light source was used. The camera position randomisations were restricted to ensure a top down view was still obtained.
Using a sample from each CVPPP dataset, A1-A4, we extracted the background images to obtain 4 plant pot background samples. These were then processed using the data augmentation described in Section <a href="#S3.SS2" title="3.2 Training &amp; Evaluation ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and used for the render backgrounds. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hinterstoisser et al.(2017)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite>, we applied Gaussian blurring to the rendered plant to better integrate it into the real world scene (background).</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Our synthetic data pipeline was designed in Blender (v2.79b). We use the Cycles renderer to produce the synthetic images with Lambertian and Oren-Nayar diffuse reflection shading. The segmentation labels were rendered using the native Blender renderer with all shading and anti-aliasing disabled to ensure their integrity.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training &amp; Evaluation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We used the Matterport<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/matterport/Mask_RCNN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/matterport/Mask_RCNN</a></span></span></span> Mask-RCNN implementation for our experiments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">He et al.(2017)He, Gkioxari, Dollár, and Girshick</a>]</cite>. Mask-RCNN is a two-stage model, with a feature extractor feeding into a Region Proposal Network (RPN) and then into three heads producing box classification, box regression, and an object mask. We use a ResNet101 backbone with a Feature Pyramid Network for Mask-RCNN. Unlike the original implementation, we use 256 (opposed to 512) regions of interest per image during training.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In each experiment, the model was initialised with weights trained on the COCO dataset. It was then retrained on a combination of real world and synthetic data. Data augmentation is a common practice to improve model robustness and prevent over fitting. In all experiments random: left-right flipping; top-bottom flipping; rotation; zooming and cropping were applied to the training images. Data augmentation was applied to real and synthetic datasets. Our training procedure consisted of splitting the data into 80%, 20% training and cross validation sets respectively; batch normalisation across a batch size of 6 and leveraging early stopping. In experiments where both real and synthetic data were used for training, each batch was balanced such that 50% of images came from each dataset respectively.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To evaluate our methods, we compete in the CVPPP Leaf Segmentation Challenge (LSC)<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://competitions.codalab.org/competitions/18405" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://competitions.codalab.org/competitions/18405</a></span></span></span>. It consists of three <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> plant (A1, A2, A4) datasets and a dataset of young tobacco plant images, A3. Training sets for A1, A2, A3, A4 contain 128, 31, 27, 624 images while the test sets have 33, 9, 65, 168 images respectively. Additionally, test set, A5, is also provided which combines images from all testing datasets in order to evaluate the generalisation of proposed machine learning techniques.
In order to compare our leaf instance segmentation performance with the current state-of-the-art approaches from the leaf segmentation Challenge (LSC), we only use the A1 dataset for training.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Using the method described in Section <a href="#S3.SS1" title="3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, three synthetic Arabidopsis datasets were generated. Each contained 10,000 labeled images and are defined as follows.
In <span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_italic">Synthetic-plant</span>; all leaves in all plants contain the same green texture.
In <span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_italic">Synthetic-leaf</span>; Each leaf in a plant is rendered with a random different green texture as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic Data Generation for Arabidopsis ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Finally, <span id="S3.SS2.p4.1.3" class="ltx_text ltx_font_italic">Synthetic-COCO</span> is generated following the same process however the leaf textures are sampled from the COCO dataset.
<span id="S3.SS2.p4.1.4" class="ltx_text ltx_font_italic">CVPPP-A1</span> was the real dataset used during the experiments which consists of the 128 images from the CVPPP A1 dataset.
The training procedure matching real and synthetic images per batch described above was named <span id="S3.SS2.p4.1.5" class="ltx_text ltx_font_italic">Synthetic+real</span>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results &amp; Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we present our performance on the CVPPP LSC, compare our results to the state-of-the-art and investigate the limitations of the synthetic datasets.
Table <a href="#S4.T1" title="Table 1 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> displays the results for each experiment and dataset in Section <a href="#S3.SS2" title="3.2 Training &amp; Evaluation ‣ 3 Method ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. The results shown are performances on the 5 test sets of the CVPPP LSC. Segmentation performances are quoted in the competition metric, symmetric best dice (SBD) score. Note, the column titled <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">Mean</span> presents the mean per test image performance across all test sets as provided by the competition and not the mean across the table row.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.1.1.1.1.1" class="ltx_text">Training Regime</span></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6">CVPPP test set (SBD)</td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">A1</td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">A2</td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">A3</td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">A4</td>
<td id="S4.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">A5</td>
<td id="S4.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mean</td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">CVPPP-A1 (real)</th>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87</td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71</td>
<td id="S4.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.3.3.4.1" class="ltx_text ltx_font_bold">59</span></td>
<td id="S4.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73</td>
<td id="S4.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70</td>
<td id="S4.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71</td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Synthetic-plant</th>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">81</td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">69</td>
<td id="S4.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">41</td>
<td id="S4.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">84</td>
<td id="S4.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">73</td>
<td id="S4.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r">74</td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Synthetic-leaf</th>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">82</td>
<td id="S4.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">74</td>
<td id="S4.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">32</td>
<td id="S4.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">86</td>
<td id="S4.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r">75</td>
<td id="S4.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r">74</td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Synthetic-COCO</th>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">57</td>
<td id="S4.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">48</td>
<td id="S4.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">25</td>
<td id="S4.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r">48</td>
<td id="S4.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r">43</td>
<td id="S4.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r">44</td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Synthetic+real</th>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.7.7.2.1" class="ltx_text ltx_font_bold">90</span></td>
<td id="S4.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.7.7.3.1" class="ltx_text ltx_font_bold">81</span></td>
<td id="S4.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">51</td>
<td id="S4.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.7.7.5.1" class="ltx_text ltx_font_bold">88</span></td>
<td id="S4.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.7.7.6.1" class="ltx_text ltx_font_bold">82</span></td>
<td id="S4.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.7.7.7.1" class="ltx_text ltx_font_bold">81</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>CVPPP LSC results. Mask-RCNN trained on real data (CVPPP-A1), synthetic data (-plant, -leaf and -COCO) and both real and synthetic (Synthetic+real) data. Segmentation scores are quoted in symmetric best dice (SBD).</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We achieve the best results on A1, A2, A4 and A5 when training on both synthetic and real data. Best performance on the A3 subset, however, is obtained by training on real data only. The generalisation of <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">Synthetic+real</span> is shown by a significantly higher mean result. Furthermore, the effect of varied texture can be seen in <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">Synthetic-plant</span>, <span id="S4.p2.1.3" class="ltx_text ltx_font_italic">Synthetic-leaf</span> and <span id="S4.p2.1.4" class="ltx_text ltx_font_italic">Synthetic-COCO</span>, where training on randomly textured leaves (<span id="S4.p2.1.5" class="ltx_text ltx_font_italic">Synthetic-COCO</span>) achieved the lowest performance. A performance increase is seen when training on plants with different green textured leaves (<span id="S4.p2.1.6" class="ltx_text ltx_font_italic">Synthetic-leaf</span>) on all test sets except A3 where <span id="S4.p2.1.7" class="ltx_text ltx_font_italic">Synthetic-plant</span> excels.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant117_rgb.jpg" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_square" width="120" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:80%;">RGB</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant117_label_dualModel.jpg" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_square" width="120" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:80%;">Synthetic+Real</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant117_label_A1Model.jpg" id="S4.F4.sf3.g1" class="ltx_graphics ltx_img_square" width="120" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S4.F4.sf3.3.2" class="ltx_text" style="font-size:80%;">CVPPP-A1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant117_label_syntheticModel.jpg" id="S4.F4.sf4.g1" class="ltx_graphics ltx_img_square" width="120" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf4.2.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="S4.F4.sf4.3.2" class="ltx_text" style="font-size:80%;">Synthetic-leaf</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Segmentations of plant 117 (a) from the A1 test set. Predictions are presented from models trained on both synthetic and real data (b) and only real data (c) or synthetic data (d). Only the model trained on both synthetic and real data (b) avoided segmenting the background artifact (fiducial marker). Note that no fiducial markers appear in the real CVPPP A1 or any of the synthetic training datasets.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the CVPPP A1 test set prediction for plant 117 from the A1 test set. The artifact in the background of the image, has been incorrectly segmented by both models trained on real only and synthetic data only. The model trained on both real and synthetic data, however, detects more of leaves and does not incorrectly segment the background artifact. No fiducial markers, the artifact, appear in any training data, real or synthetic. The false positive segmentation is likely explained by the curved geometry and edges of the object. Further, a possible reason for the model correctly not segmenting the artifact is the greater variance in geometry in the <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">Synthetic+real</span> dataset.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In addition to the results in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we experimented with freezing the feature extractor layers of Mask-RCNN when training on synthetic data as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hinterstoisser et al.(2017)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite> (see Section <a href="#S2" title="2 Related Work ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). This lead to degraded results, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Tremblay et al.(2018)Tremblay, Prakash, Acuna, Brophy, Jampani, Anil,
To, Cameracci, Boochoon, and Birchfield</a>]</cite>. We are not confident if this is because of using flexible and deformable plant models as opposed to rigid CAD models that match the test set, or if there is some deficiency in our methodology, as we were constrained in producing synthetic sets at the scales used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hinterstoisser et al.(2017)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="3">CVPPP test set (SBD)</th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">A1</th>
<th id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">A2</th>
<th id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">A3</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.3.1" class="ltx_tr">
<th id="S4.T2.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">RIS + CRF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Romera-Paredes and Torr(2016)</a>]</cite>
</th>
<td id="S4.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.6</td>
<td id="S4.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S4.T2.1.4.2" class="ltx_tr">
<th id="S4.T2.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">MSU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Scharr et al.(2016)Scharr, Minervini, French, Klukas, Kramer, Liu,
Luengo, Pape, Polder, Vukadinovic, et al.</a>]</cite>
</th>
<td id="S4.T2.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">66.7</td>
<td id="S4.T2.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">66.6</td>
<td id="S4.T2.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.4.2.4.1" class="ltx_text ltx_font_bold">59.2</span></td>
</tr>
<tr id="S4.T2.1.5.3" class="ltx_tr">
<th id="S4.T2.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Nottingham <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Scharr et al.(2016)Scharr, Minervini, French, Klukas, Kramer, Liu,
Luengo, Pape, Polder, Vukadinovic, et al.</a>]</cite>
</th>
<td id="S4.T2.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">68.3</td>
<td id="S4.T2.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">71.3</td>
<td id="S4.T2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">51.6</td>
</tr>
<tr id="S4.T2.1.6.4" class="ltx_tr">
<th id="S4.T2.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Wageningen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Yin et al.(2014)Yin, Liu, Chen, and Kramer</a>]</cite>
</th>
<td id="S4.T2.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">71.1</td>
<td id="S4.T2.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">75.7</td>
<td id="S4.T2.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">57.6</td>
</tr>
<tr id="S4.T2.1.7.5" class="ltx_tr">
<th id="S4.T2.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">IPK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Pape and Klukas(2014)</a>]</cite>
</th>
<td id="S4.T2.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r">74.4</td>
<td id="S4.T2.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r">76.9</td>
<td id="S4.T2.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">53.3</td>
</tr>
<tr id="S4.T2.1.8.6" class="ltx_tr">
<th id="S4.T2.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Salvador <em id="S4.T2.1.8.6.1.1" class="ltx_emph ltx_font_italic">et al</em><span id="S4.T2.1.8.6.1.2" class="ltx_ERROR undefined">\bmvaOneDot</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Salvador et al.(In Press)Salvador, Bellver, Baradad, Campos,
Marqués, Torres, and Giró-i Nieto</a>]</cite>
</th>
<td id="S4.T2.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r">74.7</td>
<td id="S4.T2.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.1.9.7" class="ltx_tr">
<th id="S4.T2.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Brabandere <em id="S4.T2.1.9.7.1.1" class="ltx_emph ltx_font_italic">et al</em><span id="S4.T2.1.9.7.1.2" class="ltx_ERROR undefined">\bmvaOneDot</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">De Brabandere et al.(2017)De Brabandere, Neven, and
Van Gool</a>]</cite>
</th>
<td id="S4.T2.1.9.7.2" class="ltx_td ltx_align_center ltx_border_r">84.2</td>
<td id="S4.T2.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.1.10.8" class="ltx_tr">
<th id="S4.T2.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Ren <em id="S4.T2.1.10.8.1.1" class="ltx_emph ltx_font_italic">et al</em><span id="S4.T2.1.10.8.1.2" class="ltx_ERROR undefined">\bmvaOneDot</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Ren and Zemel(2017)</a>]</cite>
</th>
<td id="S4.T2.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r">84.9</td>
<td id="S4.T2.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.1.11.9" class="ltx_tr">
<th id="S4.T2.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Ours</th>
<td id="S4.T2.1.11.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.11.9.2.1" class="ltx_text ltx_font_bold">90.0</span></td>
<td id="S4.T2.1.11.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.11.9.3.1" class="ltx_text ltx_font_bold">81.0</span></td>
<td id="S4.T2.1.11.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">51.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>CVPPP LSC results. We compare the performance of our best performing model (<span id="S4.T2.5.1" class="ltx_text ltx_font_italic">Synthetic+real</span>(A1)) to existing approaches and outperform the A1 and A2 state-of-the-art. We achieve comparable results for A3 (<span id="S4.T2.6.2" class="ltx_text ltx_font_italic">Tobacco</span>). Note the real data used to train our model came solely from A1 (<span id="S4.T2.7.3" class="ltx_text ltx_font_italic">Arabidopsis</span>). To the best of our knowledge results for A4 and A5 have not yet been published and are hence omitted. Segmentation scores are quoted in symmetric best dice (SBD).</figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Our proposed approach (trained on real and synthetic data) achieves 90% SBD score outperforming the state-of-the-art leaf instance segmentation algorithms (See Table <a href="#S4.T2" title="Table 2 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) for the A1 test set. The real data used to train our model came solely from the A1 data set. Our model also achieves state-of-the-art and comparable performance on A2 and A3 respectively. The A3 dataset contains images of tobacco plants rather than <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> and unlike the compared methods, our model is only trained on images of <span id="S4.p5.1.2" class="ltx_text ltx_font_italic">Arabidopsis</span> plants. The reduced performance is explored below.
The networks trained only on synthetic data, on average outperformed the models using only the limited A1 training data set (128 images). This likely due, in a large part, to the increase in variance across the distribution size and position of leaves in the synthetic set, and the much larger size of the data set itself (10,000 images vs 128). The positional variation in the synthetic dataset is also likely why the synthetically trained models perform particularly well on the A4 test set, which has larger range of plant age ranges than A1. These reasons also explain the incorrect segmentation phenomena shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/combinedCentroid_cropped_A1.jpg" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_square" width="180" height="180" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:80%;">Real Data                  </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/combinedCentroid_cropped_synth_noAug.jpg" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="180" height="180" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:80%;">Synthetic Data                  </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The distribution of leaf centroid locations in the training images. A wider distribution of leaf locations is presented in the synthetic data (b) while preserving the center bias. Note a uniform distribution across the image could be achieved by tuning the synthetic data pipeline.</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Synthetic data combined with domain randomization provides the ability to increase the variability across a dataset compared to data augmentation (where the real data is cropped, stretched, blurred, etc). Figure <a href="#S4.F5" title="Figure 5 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the distribution of leaf centroid locations between the CVPPP A1 and synthetic data sets. Clearly, in the real dataset (CVPPP A1), the majority of leaves appear around the center of the image. The synthetic data, while biased towards the image center, contains a wider distribution of leaf positions. A wider distribution of data to train on, improves a model’s ability to generalize. Figure <a href="#S4.F6" title="Figure 6 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the distribution of number of leaves per image for CVPPP A1 and synthetic images. While the mean has been matched in the synthetic data, a larger variance and distribution tails demonstrates the wider distribution to learn from.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/leafCountHistogram_A1_titled.jpg" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="180" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F6.sf1.3.2" class="ltx_text" style="font-size:80%;">Real Data</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/leafCountHistogram_synthetic_titled.jpg" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="180" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F6.sf2.3.2" class="ltx_text" style="font-size:80%;">Synthetic Data</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The distribution of the number of leaves per image in the training images. Greater variance is presented in the synthetic data (b) while preserving the mean.</figcaption>
</figure>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Within the models trained on only synthetic data, changing the texture per leaf (<span id="S4.p7.1.1" class="ltx_text ltx_font_italic">Synthetic-leaf</span>), performed the best, except in the case of the tobacco plant (A3 in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The improvement of a per leaf texture over a uniform texture choice per plant led us to consider the basic premise of domain randomization: the model was likely over-fitting to the simplicity and unrealistic distribution of our simulated textures, so increasing the variance in the training data should make the model more invariant to the distribution difference in the test set. However, attempting to add extremely high variance textures (<span id="S4.p7.1.2" class="ltx_text ltx_font_italic">Synthetic-COCO</span>), lead to extremely poor results, indicating that plant texture is a important aspect in segmenting leaves. This relevance of texture can also be seen in the comparatively poor performance for the A3 tobacco plant test set. The larger leaves and image resolution for A3 can lead to more visually prominent leaf veins. The over-segmentation of the leaf along a visually prominent vein can be seen in Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Blurring these images before segmentation saw an improvement in performance for these cases, but resulted in the under-segmentation of leaves on the smaller plants. Resilience to the distinct shadow on the largest leaf (Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) can also be seen. The lack of over-segmentation is likely due to the synthetic data containing hard shadowing effects from the 3D geometric plant models and the rendering pipeline.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant070_rgb.jpg" id="S4.F7.sf1.g1" class="ltx_graphics ltx_img_square" width="180" height="151" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F7.sf1.3.2" class="ltx_text" style="font-size:80%;">RGB</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1807.10931/assets/images/plant070_label.jpg" id="S4.F7.sf2.g1" class="ltx_graphics ltx_img_square" width="180" height="151" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F7.sf2.3.2" class="ltx_text" style="font-size:80%;">Prediction</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Plant 70 of the A3 test set. Prominent textural edges from the leaf veins lead to over-segmentation of the leaf. Further, invariance to shadowing can be seen as it has no effect on the segmentation.</figcaption>
</figure>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">The higher performance of the <span id="S4.p8.1.1" class="ltx_text ltx_font_italic">CVPPP-A1</span> model trained on the tobacco (A3) test set, indicates the real data contains important textural information not present in the simulated data, which leads the model to not over-segment the tobacco leaves. The best results across all CVPPP LSC test sets were from training with mixed batches containing both real and synthetic data. We conclude this is due to the textural information from the real data, and the increased geometric diversity from the simulated plant data.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">In addition we note that training on real and synthetic data also helps reject false positive detection on the background (e.g. Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). In this case, the synthetic data did not contain any backgrounds with any black and white fiducial markers. The extra background variance across the combined set may have enabled the model to not detect this artifact.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">These observations indicate that further performance benefits could be derived by increasing the variation of both the background images and the foreground textures. A simple method to increase background variation would be to physically photograph the nursery before the plants start growing. In addition distractor objects (such as the white fiducial marker) could be added to the foreground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Dwibedi et al.(2017)Dwibedi, Misra, and Hebert</a>]</cite>.
The foreground leaf textures used in our experiments were sampled from a small, manually created, set of 30 leaf crops. Increased performance could then be expected by leveraging more sophisticated textures, both increasing the variation and perhaps by making the image more photo-realistic. There are several leaf databases that would be suitable for extracting this texture data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Kumar et al.(2012)Kumar, Belhumeur, Biswas, Jacobs, Kress, Lopez, and
Soares</a>]</cite>.</p>
</div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p">Furthermore, to increase the variance in textures, further investigations will consider more sophisticated approaches synthesizing leaf and plant geometry leveraging L-systems.
The methods used in this paper were all based off a single inspiration leaf of an <span id="S4.p11.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> plant, with relatively compact leaf shapes. Tobacco plants (A3), however, have broader leaves where are not as well separated from each other. By creating training data sets with larger leaf and plant geometry variance, or at least matching the geometry to new target domains, we expect further increases in performance. This may also include incorporating leaf vein structures in the synthetic data to address the phenomenon presented in Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Results &amp; Discussion ‣ Deep Leaf Segmentation Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S4.p12" class="ltx_para">
<p id="S4.p12.1" class="ltx_p">Our generated synthetic dataset is publicly available at <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://research.csiro.au/robotics/databases" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://research.csiro.au/robotics/databases</a></span></span></span>. The synthetic dataset contains 10,000 top down images of synthetic <span id="S4.p12.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> plants and their corresponding 2D segmentation labels.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have presented and evaluated a framework for generating synthetic <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">Arabidopsis</span> images and accompanying instance segmentation labels. By inspiring geometry and sampling real world textures for background and foreground, we have outperformed state-of-the-art results achieving 90% symmetric best dice score on the CVPPP A1 test set. Furthermore, this framework has the potential to be rapidly adapted to a new plant and automate leaf segmentation within a plant phenotyping setting. We achieved best results across all test sets, except CVPPP A3, when training on both the real data and synthetic data. Presumably, this is due to the extra textural information from the real data and the increased geometric diversity from the simulated plant data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Future work will be directed at improving the variation in geometry and texture of the synthetic data. We would expect a performance and generalisation increase by sampling leaf textures from a wider, domain specific distributions. For geometric variation, supporting a structure description framework such as L-systems would alleviate the ability to simulate different plant species.
<br class="ltx_break"></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Acknowledgements:</span> This research was funded by the CSIRO AgTech Cluster for Robotics and Autonomous Systems. The authors thank the Robotics and Autonomous Systems team for their valuable feedback and discussions, notably Mark Cox, Kamil Grycz, Terry Kung and Micheal Thoreau. Further, we thank Robert Lee for his advice and support.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Abu Alhaija et al.(2017)Abu Alhaija, Mustikovela, Mescheder, Geiger,
and Rother]</span>
<span class="ltx_bibblock">
Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas Geiger,
and Carsten Rother.

</span>
<span class="ltx_bibblock">Augmented reality meets deep learning for car instance segmentation
in urban scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Proceedings of BMVC 2017 and Workshops</em>. British Machine
Vision Association, 2017.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Bousmalis et al.(2017)Bousmalis, Silberman, Dohan, Erhan, and
Krishnan]</span>
<span class="ltx_bibblock">
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip
Krishnan.

</span>
<span class="ltx_bibblock">Unsupervised pixel-level domain adaptation with generative
adversarial networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, volume 1, page 7, 2017.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[De Brabandere et al.(2017)De Brabandere, Neven, and
Van Gool]</span>
<span class="ltx_bibblock">
Bert De Brabandere, Davy Neven, and Luc Van Gool.

</span>
<span class="ltx_bibblock">Semantic instance segmentation with a discriminative loss function.

</span>
<span class="ltx_bibblock"><em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1708.02551</em>, 2017.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Dwibedi et al.(2017)Dwibedi, Misra, and Hebert]</span>
<span class="ltx_bibblock">
Debidatta Dwibedi, Ishan Misra, and Martial Hebert.

</span>
<span class="ltx_bibblock">Cut, paste and learn: Surprisingly easy synthesis for instance
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">The IEEE international conference on computer vision
(ICCV)</em>, 2017.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Giuffrida et al.(2017)Giuffrida, Scharr, and Tsaftaris]</span>
<span class="ltx_bibblock">
Mario Valerio Giuffrida, Hanno Scharr, and Sotirios A Tsaftaris.

</span>
<span class="ltx_bibblock">Arigan: Synthetic arabidopsis plants using generative adversarial
network.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 IEEE International Conference on
Computer Vision Workshop (ICCVW), Venice, Italy</em>, pages 22–29, 2017.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[He et al.(2017)He, Gkioxari, Dollár, and Girshick]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Computer Vision (ICCV), 2017 IEEE International Conference
on</em>, pages 2980–2988. IEEE, 2017.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hinterstoisser et al.(2017)Hinterstoisser, Lepetit, Wohlhart, and
Konolige]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, and Kurt Konolige.

</span>
<span class="ltx_bibblock">On pre-trained image features and synthetic images for deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.10710</em>, 2017.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Huang et al.(2017)Huang, Rathod, Sun, Zhu, Korattikara, Fathi,
Fischer, Wojna, Song, Guadarrama, et al.]</span>
<span class="ltx_bibblock">
Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara,
Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama,
et al.

</span>
<span class="ltx_bibblock">Speed/accuracy trade-offs for modern convolutional object detectors.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">IEEE CVPR</em>, volume 4, 2017.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kumar et al.(2012)Kumar, Belhumeur, Biswas, Jacobs, Kress, Lopez, and
Soares]</span>
<span class="ltx_bibblock">
Neeraj Kumar, Peter N. Belhumeur, Arijit Biswas, David W. Jacobs, W. John
Kress, Ida Lopez, and João V. B. Soares.

</span>
<span class="ltx_bibblock">Leafsnap: A computer vision system for automatic plant species
identification.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">The 12th European Conference on Computer Vision (ECCV)</em>,
October 2012.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
Dollár, and Zitnick]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740–755.
Springer, 2014.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Minervini et al.(2016)Minervini, Fischbach, Scharr, and
Tsaftaris]</span>
<span class="ltx_bibblock">
Massimo Minervini, Andreas Fischbach, Hanno Scharr, and Sotirios A Tsaftaris.

</span>
<span class="ltx_bibblock">Finely-grained annotated datasets for image-based plant phenotyping.

</span>
<span class="ltx_bibblock"><em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">Pattern recognition letters</em>, 81:80–89, 2016.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Moghadam et al.(2017)Moghadam, Ward, Goan, Jayawardena, Sikka, and
Hernandez]</span>
<span class="ltx_bibblock">
Peyman Moghadam, Daniel Ward, Ethan Goan, Srimal Jayawardena, Pavan Sikka, and
Emili Hernandez.

</span>
<span class="ltx_bibblock">Plant disease detection using hyperspectral imaging.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">International Conference on Digital Image Computing:
Techniques and Applications (DICTA)</em>, pages 1–8, 2017.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Mündermann et al.(2005)Mündermann, Erasmus, Lane, Coen, and
Prusinkiewicz]</span>
<span class="ltx_bibblock">
Lars Mündermann, Yvette Erasmus, Brendan Lane, Enrico Coen, and Przemyslaw
Prusinkiewicz.

</span>
<span class="ltx_bibblock">Quantitative modeling of arabidopsis development.

</span>
<span class="ltx_bibblock"><em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Plant physiology</em>, 139(2):960–968, 2005.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Pape and Klukas(2014)]</span>
<span class="ltx_bibblock">
Jean-Michel Pape and Christian Klukas.

</span>
<span class="ltx_bibblock">3-d histogram-based segmentation and leaf detection for rosette
plants.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 61–74.
Springer, 2014.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Prusinkiewicz(2002)]</span>
<span class="ltx_bibblock">
Przemyslaw Prusinkiewicz.

</span>
<span class="ltx_bibblock">Art and science of life: Designing and growing virtual plants with
l-systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">XXVI International Horticultural Congress: Nursery Crops;
Development, Evaluation, Production and Use 630</em>, pages 15–28, 2002.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ren and Zemel(2017)]</span>
<span class="ltx_bibblock">
Mengye Ren and Richard S Zemel.

</span>
<span class="ltx_bibblock">End-to-end instance segmentation with recurrent attention.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), Honolulu, HI, USA</em>, pages 21–26, 2017.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Romera-Paredes and Torr(2016)]</span>
<span class="ltx_bibblock">
Bernardino Romera-Paredes and Philip Hilaire Sean Torr.

</span>
<span class="ltx_bibblock">Recurrent instance segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 312–329.
Springer, 2016.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Russakovsky et al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">ImageNet Large Scale Visual Recognition Challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision (IJCV)</em>, 115(3):211–252, 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1007/s11263-015-0816-y" title="" class="ltx_ref">10.1007/s11263-015-0816-y</a>.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Salvador et al.(In Press)Salvador, Bellver, Baradad, Campos,
Marqués, Torres, and Giró-i Nieto]</span>
<span class="ltx_bibblock">
Amaia Salvador, Míriam Bellver, Manel Baradad, Víctor Campos,
F. Marqués, Jordi Torres, and X. Giró-i Nieto.

</span>
<span class="ltx_bibblock">Recurrent neural networks for semantic instance segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">CVPR 2018 DeepVision Workshop</em>, 06/2018 In Press.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Scharr et al.(2016)Scharr, Minervini, French, Klukas, Kramer, Liu,
Luengo, Pape, Polder, Vukadinovic, et al.]</span>
<span class="ltx_bibblock">
Hanno Scharr, Massimo Minervini, Andrew P French, Christian Klukas, David M
Kramer, Xiaoming Liu, Imanol Luengo, Jean-Michel Pape, Gerrit Polder,
Danijela Vukadinovic, et al.

</span>
<span class="ltx_bibblock">Leaf segmentation in plant phenotyping: a collation study.

</span>
<span class="ltx_bibblock"><em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Machine vision and applications</em>, 27(4):585–606, 2016.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tremblay et al.(2018)Tremblay, Prakash, Acuna, Brophy, Jampani, Anil,
To, Cameracci, Boochoon, and Birchfield]</span>
<span class="ltx_bibblock">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.

</span>
<span class="ltx_bibblock"><em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.06516</em>, 2018.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ubbens et al.(2018)Ubbens, Cieslak, Prusinkiewicz, and
Stavness]</span>
<span class="ltx_bibblock">
Jordan Ubbens, Mikolaj Cieslak, Przemyslaw Prusinkiewicz, and Ian Stavness.

</span>
<span class="ltx_bibblock">The use of plant models in deep learning: an application to leaf
counting in rosette plants.

</span>
<span class="ltx_bibblock"><em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">Plant methods</em>, 14(1):6, 2018.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Van Horn and Perona(2017)]</span>
<span class="ltx_bibblock">
Grant Van Horn and Pietro Perona.

</span>
<span class="ltx_bibblock">The devil is in the tails: Fine-grained classification in the wild.

</span>
<span class="ltx_bibblock"><em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1709.01450</em>, 2017.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Yin et al.(2014)Yin, Liu, Chen, and Kramer]</span>
<span class="ltx_bibblock">
Xi Yin, Xiaoming Liu, Jin Chen, and David M Kramer.

</span>
<span class="ltx_bibblock">Multi-leaf tracking from fluorescence plant videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">Image Processing (ICIP), 2014 IEEE International Conference
on</em>, pages 408–412. IEEE, 2014.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhou et al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
Torralba]</span>
<span class="ltx_bibblock">
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
Torralba.

</span>
<span class="ltx_bibblock">Scene parsing through ade20k dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, 2017.

</span>
</li>
</ul>
</section>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">See pages - of <a href="synthetic-arabidopsis-dataset.pdf" title="" class="ltx_ref">synthetic-arabidopsis-dataset.pdf</a></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1807.10929" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1807.10931" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1807.10931">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1807.10931" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1807.10932" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 18:12:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
