<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.06052] Post-training Model Quantization Using GANs for Synthetic Data Generation</title><meta property="og:description" content="Quantization is a widely adopted technique for deep neural networks to reduce the memory and computational resources required. However, when quantized, most models would need a suitable calibration process to keep theiâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Post-training Model Quantization Using GANs for Synthetic Data Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Post-training Model Quantization Using GANs for Synthetic Data Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.06052">

<!--Generated on Thu Feb 29 08:01:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Post-training Model Quantization Using GANs for Synthetic Data Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Athanasios Masouris
<br class="ltx_break">Delft University of Technology
<br class="ltx_break">Delft, The Netherlands
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">a.masouris@student.tudelft.nl</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mansi Sharma
<br class="ltx_break">Intel
<br class="ltx_break">Washington, USA
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">mansi.sharma@intel.com</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adrian Boguszewski 
<br class="ltx_break">Intel
<br class="ltx_break">Swindon, UK
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">adrian.boguszewski@intel.com</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Kozlov 
<br class="ltx_break">Intel
<br class="ltx_break">Dubai, UAE
<br class="ltx_break"><span id="id6.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">alexander.kozlov@intel.com</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhuo Wu 
<br class="ltx_break">Intel
<br class="ltx_break">Shanghai, China
<br class="ltx_break"><span id="id7.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">zhuo.wu@intel.com</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raymond Lo 
<br class="ltx_break">Intel
<br class="ltx_break">Santa Clara, USA
<br class="ltx_break"><span id="id8.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">raymond.lo@intel.com </span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Quantization is a widely adopted technique for deep neural networks to reduce the memory and computational resources required. However, when quantized, most models would need a suitable calibration process to keep their performance intact, which requires data from the target domain, such as a fraction of the dataset used in model training and model validation (<em id="id9.id1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="id9.id1.2" class="ltx_text"></span> calibration dataset).</p>
<p id="id2.2" class="ltx_p">In this study, we investigate the use of synthetic data as a substitute for the calibration with real data for the quantization method. We propose a data generation method based on Generative Adversarial Networks that are trained prior to the model quantization step. We compare the performance of models quantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN, with quantization using real data and an alternative data generation method based on fractal images. Overall, the results of our experiments demonstrate the potential of leveraging synthetic data for calibration during the quantization process. In our experiments, the percentage of accuracy degradation of the selected models was less than <math id="id1.1.m1.1" class="ltx_Math" alttext="0.6\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">0.6</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn type="float" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">0.6\%</annotation></semantics></math>, with our best performance achieved on <span id="id2.2.1" class="ltx_text ltx_font_italic">MobileNetV2</span> (<math id="id2.2.m2.1" class="ltx_Math" alttext="0.05\%" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">0.05</mn><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">percent</csymbol><cn type="float" id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">0.05\%</annotation></semantics></math>). The code is available at: <a target="_blank" href="https://github.com/ThanosM97/gsoc2022-openvino" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ThanosM97/gsoc2022-openvino</a></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Model optimization is essential in deep learning since it demonstrates efficacy in using computational resources, scalability, and even better performance. Techniques such as model pruning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, weight compression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> have been proven to reduce the model complexity without sacrificing its accuracy.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Another approach that stands out is model quantization. Model quantization is a method used in deep learning to reduce a modelâ€™s memory footprint and computational complexity and enable inference on hardware with limited resources (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.2" class="ltx_text"></span> smartphones, IoT devices). Additionally, quantization facilitates faster inference by reducing the computations required during forward propagation, and thus less power consumption.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Quantization is achieved by reducing the precision of model parameters and activations from their original floating-point precision to a lower-precision representation (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.2" class="ltx_text"></span> 8-bit fixed-point). There are two types of quantization, weight and activation. The goal of the weight quantization process is to find the nearest low-precision weights to the original floating-point ones. Activation quantization reduces the precision of the activations produced by the model. It can be achieved by either setting a precision based on the range of the activations during runtime (<em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p3.1.4" class="ltx_text"></span> dynamic fixed-point quantization) or by setting the same precision for all activations (<em id="S1.p3.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p3.1.6" class="ltx_text"></span> uniform fixed-point quantization).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">While a model can be quantized and trained from scratch (<em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.2" class="ltx_text"></span> <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">quantization-aware training</span>), a more practical approach is <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">post-training quantization</span>, which optimizes the performance of existing pre-trained models. However, post-training quantization can have a significant impact on model performance. The goal is to minimize the accuracy loss caused by quantization errors, while maximizing the efficiency of the end model. Specific methods, such as accuracy-aware quantization, can also be applied. In keeping the modelâ€™s accuracy intact, these methods require a calibration dataset (<em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.6" class="ltx_text"></span> a small set of samples representative of the overall dataset the model was trained on). Nevertheless, due to privacy concerns, private use, or the scale of the dataset, a calibration subset may not always be available. In these cases, synthetic-generated data can be leveraged.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this study, we investigate the use of synthetic data generated by Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> as a substitute for the calibration with real data by the quantization method. GANs are known for their ability to generate new, realistic, and diverse data samples that are similar to the original data distribution. While the publication of a dataset might be hindered by privacy concerns or the sheer size of the data, a GAN model can become publicly available and thus be used to produce synthetic samples according to the usersâ€™ needs or the requirements of a process (<em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p5.1.2" class="ltx_text"></span> quantization).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Prompted by the ever-increasing size of deep neural networks, techniques for reducing their computational costs have been thoroughly studied for efficient learning. Early attempts focused on reducing the number of network parameters by grouping the weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, replacing costly operations such as fully connected layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, or by pruning connections between layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Network quantization has also been studied, with early work representing weights and activations using only a single bit, introducing the Binarized Neural Networks (BNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. While this representation achieved a substantial reduction in computational costs, it also led to accuracy degradation on more complex models and datasets. In Gupta <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the authors demonstrated that using 16-bit fixed-point representation with stochastic rounding when training a CNN leads to negligible degradation in the classification accuracy. In Banner <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the precision was further reduced to an 8-bit representation while they quantized both weights and activations in all layers. Their proposed 8-bit training did not affect the modelsâ€™ accuracy when trained on a large-scale dataset. In Zhang <em id="S2.p1.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, the authors proposed a quantization method by training quantizers from data. In Han <em id="S2.p1.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.8" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, they attempted to compress deep neural networks by combining pruning, trained quantization, and Huffman coding. Although their method could significantly compress the size of deep neural networks, the pruning process can be time-consuming and difficult to optimize.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Post-training quantization has also been the focus of research. In Lin <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, the authors proposed an SQNR-based optimization approach to convert a pre-trained floating point deep convolutional model to its fixed-point equivalent. In Banner <em id="S2.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, 4-bit quantization was proposed using the ACIQ method to optimize the clipping value, while in Choukroun <em id="S2.p2.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, they minimized the quantization MSE for both weights and activations. More recent studies suggested quantization by splitting outlier channels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, using adaptive weight rounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, or bias correction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Additionally, given that post-training quantization requires a small calibration dataset, which may not be readily available, studies have addressed this issue by either employing data-free quantization techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> or using synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this study, we examine the effect of the quantization process on the performance of the models in terms of classification accuracy. To do so, we utilized <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">OpenVINOâ€™s Post-training Optimization Tool (POT)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which supports uniform integer quantization. Using this tool, the weights and activations of the classification models were converted from floating-point to integer precision (8-bit representation).</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">POT provides two quantization methods. First, the <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">Default Quantization</span> process performs the quantization using a non-labeled calibration dataset to estimate the range of activation values. Second, the <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">Accuracy-aware Quantization</span> provides control over the defined accuracy metric, allowing the tool to quantize specific layers of the model while maintaining the accuracy within a predefined range. In contrast with the default quantization, the accuracy-aware quantization requires a labeled calibration dataset.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Given that a calibration dataset may not always be available, in this study, we also investigate the use of synthetic data for the calibration process. We generated synthetic data using Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, StyleGAN2-ADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and our own DiStyleGAN (<a href="#S3.SS2" title="3.2 GAN Training â€£ 3 Methodology â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.2</span></a>), pre-trained to approximate the distribution of the real data the classification models were trained on.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Subsequently, experiments were conducted using the Default Quantization and the Accuracy-aware quantization methods provided by POT. The quantized models were pre-trained on the classification task on the CIFAR-10 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and their performances upon quantization were compared on the CIFAR-10 test set. Furthermore, both quantization techniques were applied using multiple calibration datasets (real, synthetic, and fractal).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Five models were selected to be quantized during the experiments. All of them were pre-trained on the CIFAR-10 dataset for classification in PyTorch and were obtained from PyTorch Hub<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/chenyaofo/pytorch-cifar-models</span></span></span>. The models, along with their corresponding versions, that were quantized were the following: <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">ResNet20 (resnet20)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">VGG16 (vgg16_bn)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, <em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">MobileNetV2 (mobilenetv2_x1_4)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, <em id="S3.SS1.p1.1.4" class="ltx_emph ltx_font_italic">ShuffleNetV2 (shufflenetv2_x2_0)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and <em id="S3.SS1.p1.1.5" class="ltx_emph ltx_font_italic">RepVGG (repvgg_a2)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2305.06052/assets/images/DiStyleGAN.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="319" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Overview of DiStyleGANâ€™s architecture</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>GAN Training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The current state-of-the-art model in class-conditional image generation on CIFAR-10 is StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. While this model can generate synthetic images that look similar to the ones from the CIFAR-10 dataset, generating those images requires substantial computational resources due to its more than 20 million parameters. In an attempt to lower this requirement, we used the knowledge distillation framework described in Chang and Lu <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In particular, we used StyleGAN2 as a teacher network, to train a student network, DiStyleGAN (<a href="#S3.F1" title="In 3.1 Models â€£ 3 Methodology â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>), to produce synthetic images that approximate the CIFAR-10 distribution while requiring only about a tenth of the formerâ€™s parameters.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Black-Box Distillation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.2" class="ltx_p">Similar to Chang <em id="S3.SS2.SSS1.p1.2.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.SSS1.p1.2.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, the teacher network is applied as a black box, requiring only limited access to its input-output pairs. In particular, <math id="S3.SS2.SSS1.p1.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.2a"><mrow id="S3.SS2.SSS1.p1.1.m1.2.3.2" xref="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">50</mn><mo id="S3.SS2.SSS1.p1.1.m1.2.3.2.1" xref="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.SSS1.p1.1.m1.2.2" xref="S3.SS2.SSS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.2b"><list id="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">50</cn><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.2c">50,000</annotation></semantics></math> synthetic images generated by StyleGAN2 were collecte, equally distributed among the <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mn id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><cn type="integer" id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">10</annotation></semantics></math> classes of CIFAR-10, along with the input noise vectors and the corresponding class labels. This collection, or dataset, is then used to train DiStyleGAN in a supervised way. Following this approach, no knowledge of the internal or intermediate features of the teacher model is required and can be discarded afterward.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Objectives</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">For the training of DiStyleGAN, we leveraged the same objectives as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{G}=\mathcal{L}_{KD_{feat}}+\lambda_{1}\mathcal{L}_{KD_{pix}}+\lambda_{2}\mathcal{L}_{KD_{S}}+\lambda_{3}\mathcal{L}_{GAN_{S}}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">â„’</mi><mi id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">G</mi></msub><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">â„’</mi><mrow id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.1" xref="S3.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.3.2" xref="S3.E1.m1.1.1.3.2.3.3.2.cmml">D</mi><mrow id="S3.E1.m1.1.1.3.2.3.3.3" xref="S3.E1.m1.1.1.3.2.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.3.3.2" xref="S3.E1.m1.1.1.3.2.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.3.1" xref="S3.E1.m1.1.1.3.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.2.3.3.3.3" xref="S3.E1.m1.1.1.3.2.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.3.1a" xref="S3.E1.m1.1.1.3.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.2.3.3.3.4" xref="S3.E1.m1.1.1.3.2.3.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.3.1b" xref="S3.E1.m1.1.1.3.2.3.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.2.3.3.3.5" xref="S3.E1.m1.1.1.3.2.3.3.3.5.cmml">t</mi></mrow></msub></mrow></msub><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml">Î»</mi><mn id="S3.E1.m1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.3.3.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.2.cmml">â„’</mi><mrow id="S3.E1.m1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.3.3.1" xref="S3.E1.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.3.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.3.3.2.cmml">D</mi><mrow id="S3.E1.m1.1.1.3.3.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.3.3.3.3.1" xref="S3.E1.m1.1.1.3.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.3.3.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.3.3.3.3.1a" xref="S3.E1.m1.1.1.3.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.3.3.3.3.3.4" xref="S3.E1.m1.1.1.3.3.3.3.3.3.4.cmml">x</mi></mrow></msub></mrow></msub></mrow><mo id="S3.E1.m1.1.1.3.1a" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.4" xref="S3.E1.m1.1.1.3.4.cmml"><msub id="S3.E1.m1.1.1.3.4.2" xref="S3.E1.m1.1.1.3.4.2.cmml"><mi id="S3.E1.m1.1.1.3.4.2.2" xref="S3.E1.m1.1.1.3.4.2.2.cmml">Î»</mi><mn id="S3.E1.m1.1.1.3.4.2.3" xref="S3.E1.m1.1.1.3.4.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.4.1" xref="S3.E1.m1.1.1.3.4.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.3.4.3" xref="S3.E1.m1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.4.3.2" xref="S3.E1.m1.1.1.3.4.3.2.cmml">â„’</mi><mrow id="S3.E1.m1.1.1.3.4.3.3" xref="S3.E1.m1.1.1.3.4.3.3.cmml"><mi id="S3.E1.m1.1.1.3.4.3.3.2" xref="S3.E1.m1.1.1.3.4.3.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.4.3.3.1" xref="S3.E1.m1.1.1.3.4.3.3.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.3.4.3.3.3" xref="S3.E1.m1.1.1.3.4.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.4.3.3.3.2" xref="S3.E1.m1.1.1.3.4.3.3.3.2.cmml">D</mi><mi id="S3.E1.m1.1.1.3.4.3.3.3.3" xref="S3.E1.m1.1.1.3.4.3.3.3.3.cmml">S</mi></msub></mrow></msub></mrow><mo id="S3.E1.m1.1.1.3.1b" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.5" xref="S3.E1.m1.1.1.3.5.cmml"><msub id="S3.E1.m1.1.1.3.5.2" xref="S3.E1.m1.1.1.3.5.2.cmml"><mi id="S3.E1.m1.1.1.3.5.2.2" xref="S3.E1.m1.1.1.3.5.2.2.cmml">Î»</mi><mn id="S3.E1.m1.1.1.3.5.2.3" xref="S3.E1.m1.1.1.3.5.2.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.5.1" xref="S3.E1.m1.1.1.3.5.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.3.5.3" xref="S3.E1.m1.1.1.3.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.5.3.2" xref="S3.E1.m1.1.1.3.5.3.2.cmml">â„’</mi><mrow id="S3.E1.m1.1.1.3.5.3.3" xref="S3.E1.m1.1.1.3.5.3.3.cmml"><mi id="S3.E1.m1.1.1.3.5.3.3.2" xref="S3.E1.m1.1.1.3.5.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.5.3.3.1" xref="S3.E1.m1.1.1.3.5.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.5.3.3.3" xref="S3.E1.m1.1.1.3.5.3.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.5.3.3.1a" xref="S3.E1.m1.1.1.3.5.3.3.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.3.5.3.3.4" xref="S3.E1.m1.1.1.3.5.3.3.4.cmml"><mi id="S3.E1.m1.1.1.3.5.3.3.4.2" xref="S3.E1.m1.1.1.3.5.3.3.4.2.cmml">N</mi><mi id="S3.E1.m1.1.1.3.5.3.3.4.3" xref="S3.E1.m1.1.1.3.5.3.3.4.3.cmml">S</mi></msub></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">â„’</ci><ci id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">ğº</ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><plus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">â„’</ci><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><times id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">ğ¾</ci><apply id="S3.E1.m1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.3.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.3.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.3.2">ğ·</ci><apply id="S3.E1.m1.1.1.3.2.3.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3"><times id="S3.E1.m1.1.1.3.2.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3.1"></times><ci id="S3.E1.m1.1.1.3.2.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3.2">ğ‘“</ci><ci id="S3.E1.m1.1.1.3.2.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3.3">ğ‘’</ci><ci id="S3.E1.m1.1.1.3.2.3.3.3.4.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3.4">ğ‘</ci><ci id="S3.E1.m1.1.1.3.2.3.3.3.5.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3.5">ğ‘¡</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2">ğœ†</ci><cn type="integer" id="S3.E1.m1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2">â„’</ci><apply id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3"><times id="S3.E1.m1.1.1.3.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.3.2">ğ¾</ci><apply id="S3.E1.m1.1.1.3.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3.2">ğ·</ci><apply id="S3.E1.m1.1.1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3.3"><times id="S3.E1.m1.1.1.3.3.3.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.3.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3.3.2">ğ‘</ci><ci id="S3.E1.m1.1.1.3.3.3.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3.3.3">ğ‘–</ci><ci id="S3.E1.m1.1.1.3.3.3.3.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.3.3.3.3.4">ğ‘¥</ci></apply></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.3.4"><times id="S3.E1.m1.1.1.3.4.1.cmml" xref="S3.E1.m1.1.1.3.4.1"></times><apply id="S3.E1.m1.1.1.3.4.2.cmml" xref="S3.E1.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.2.1.cmml" xref="S3.E1.m1.1.1.3.4.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.4.2.2.cmml" xref="S3.E1.m1.1.1.3.4.2.2">ğœ†</ci><cn type="integer" id="S3.E1.m1.1.1.3.4.2.3.cmml" xref="S3.E1.m1.1.1.3.4.2.3">2</cn></apply><apply id="S3.E1.m1.1.1.3.4.3.cmml" xref="S3.E1.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.3.1.cmml" xref="S3.E1.m1.1.1.3.4.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.4.3.2.cmml" xref="S3.E1.m1.1.1.3.4.3.2">â„’</ci><apply id="S3.E1.m1.1.1.3.4.3.3.cmml" xref="S3.E1.m1.1.1.3.4.3.3"><times id="S3.E1.m1.1.1.3.4.3.3.1.cmml" xref="S3.E1.m1.1.1.3.4.3.3.1"></times><ci id="S3.E1.m1.1.1.3.4.3.3.2.cmml" xref="S3.E1.m1.1.1.3.4.3.3.2">ğ¾</ci><apply id="S3.E1.m1.1.1.3.4.3.3.3.cmml" xref="S3.E1.m1.1.1.3.4.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.4.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.4.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.4.3.3.3.2">ğ·</ci><ci id="S3.E1.m1.1.1.3.4.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.4.3.3.3.3">ğ‘†</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.3.5"><times id="S3.E1.m1.1.1.3.5.1.cmml" xref="S3.E1.m1.1.1.3.5.1"></times><apply id="S3.E1.m1.1.1.3.5.2.cmml" xref="S3.E1.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.5.2.1.cmml" xref="S3.E1.m1.1.1.3.5.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.5.2.2.cmml" xref="S3.E1.m1.1.1.3.5.2.2">ğœ†</ci><cn type="integer" id="S3.E1.m1.1.1.3.5.2.3.cmml" xref="S3.E1.m1.1.1.3.5.2.3">3</cn></apply><apply id="S3.E1.m1.1.1.3.5.3.cmml" xref="S3.E1.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.5.3.1.cmml" xref="S3.E1.m1.1.1.3.5.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.5.3.2.cmml" xref="S3.E1.m1.1.1.3.5.3.2">â„’</ci><apply id="S3.E1.m1.1.1.3.5.3.3.cmml" xref="S3.E1.m1.1.1.3.5.3.3"><times id="S3.E1.m1.1.1.3.5.3.3.1.cmml" xref="S3.E1.m1.1.1.3.5.3.3.1"></times><ci id="S3.E1.m1.1.1.3.5.3.3.2.cmml" xref="S3.E1.m1.1.1.3.5.3.3.2">ğº</ci><ci id="S3.E1.m1.1.1.3.5.3.3.3.cmml" xref="S3.E1.m1.1.1.3.5.3.3.3">ğ´</ci><apply id="S3.E1.m1.1.1.3.5.3.3.4.cmml" xref="S3.E1.m1.1.1.3.5.3.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.5.3.3.4.1.cmml" xref="S3.E1.m1.1.1.3.5.3.3.4">subscript</csymbol><ci id="S3.E1.m1.1.1.3.5.3.3.4.2.cmml" xref="S3.E1.m1.1.1.3.5.3.3.4.2">ğ‘</ci><ci id="S3.E1.m1.1.1.3.5.3.3.4.3.cmml" xref="S3.E1.m1.1.1.3.5.3.3.4.3">ğ‘†</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}_{G}=\mathcal{L}_{KD_{feat}}+\lambda_{1}\mathcal{L}_{KD_{pix}}+\lambda_{2}\mathcal{L}_{KD_{S}}+\lambda_{3}\mathcal{L}_{GAN_{S}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p3.5" class="ltx_p">where <math id="S3.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{KD_{feat}}" display="inline"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><msub id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p3.1.m1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p3.1.m1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.1.m1.1.1.3.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS2.SSS2.p3.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2.cmml">D</mi><mrow id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1a" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.4" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1b" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.5" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.5.cmml">t</mi></mrow></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3"><times id="S3.SS2.SSS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.2">ğ¾</ci><apply id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2">ğ·</ci><apply id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3"><times id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.1"></times><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.2">ğ‘“</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.3">ğ‘’</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.4.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.4">ğ‘</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.5.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.5">ğ‘¡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">\mathcal{L}_{KD_{feat}}</annotation></semantics></math> is the feature-level distillation loss calculated from feature maps extracted using the Discriminatorâ€™s network for the images generated by both the student and the teacher networks, <math id="S3.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{KD_{pix}}" display="inline"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><msub id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p3.2.m2.1.1.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p3.2.m2.1.1.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.2.m2.1.1.3.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS2.SSS2.p3.2.m2.1.1.3.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2.cmml">D</mi><mrow id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.1a" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.4" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.4.cmml">x</mi></mrow></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><apply id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3"><times id="S3.SS2.SSS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.2">ğ¾</ci><apply id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2">ğ·</ci><apply id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3"><times id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.1"></times><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.2">ğ‘</ci><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.3">ğ‘–</ci><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.4.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.4">ğ‘¥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">\mathcal{L}_{KD_{pix}}</annotation></semantics></math> is the pixel-level distillation loss calculated as the pixel <math id="S3.SS2.SSS2.p3.3.m3.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><msub id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p3.3.m3.1.1.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml">L</mi><mn id="S3.SS2.SSS2.p3.3.m3.1.1.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><apply id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">L_{1}</annotation></semantics></math> distance between the student and teacher images, <math id="S3.SS2.SSS2.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{KD_{S}}" display="inline"><semantics id="S3.SS2.SSS2.p3.4.m4.1a"><msub id="S3.SS2.SSS2.p3.4.m4.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p3.4.m4.1.1.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p3.4.m4.1.1.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.4.m4.1.1.3.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS2.SSS2.p3.4.m4.1.1.3.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2.cmml">D</mi><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3.cmml">S</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m4.1b"><apply id="S3.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3"><times id="S3.SS2.SSS2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.2">ğ¾</ci><apply id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2">ğ·</ci><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3">ğ‘†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m4.1c">\mathcal{L}_{KD_{S}}</annotation></semantics></math> is the adversarial distillation loss used to train the student generator to approximate the distribution of the teacherâ€™s, and <math id="S3.SS2.SSS2.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{GAN_{S}}" display="inline"><semantics id="S3.SS2.SSS2.p3.5.m5.1a"><msub id="S3.SS2.SSS2.p3.5.m5.1.1" xref="S3.SS2.SSS2.p3.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p3.5.m5.1.1.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p3.5.m5.1.1.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.1.1.3.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.5.m5.1.1.3.1" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.5.m5.1.1.3.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.5.m5.1.1.3.1a" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS2.SSS2.p3.5.m5.1.1.3.4" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.4.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.1.1.3.4.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.4.2.cmml">N</mi><mi id="S3.SS2.SSS2.p3.5.m5.1.1.3.4.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.4.3.cmml">S</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.5.m5.1b"><apply id="S3.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3"><times id="S3.SS2.SSS2.p3.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.1"></times><ci id="S3.SS2.SSS2.p3.5.m5.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.2">ğº</ci><ci id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3">ğ´</ci><apply id="S3.SS2.SSS2.p3.5.m5.1.1.3.4.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.1.1.3.4.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.4">subscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.1.1.3.4.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.4.2">ğ‘</ci><ci id="S3.SS2.SSS2.p3.5.m5.1.1.3.4.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.4.3">ğ‘†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.5.m5.1c">\mathcal{L}_{GAN_{S}}</annotation></semantics></math> is the adversarial GAN loss used to train the student generator to approximate the distribution of the real data.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">For the discriminator, the following objective was used:</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{D}=\mathcal{L}_{KD_{D}}+\lambda_{4}\mathcal{L}_{GAN_{D}}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">â„’</mi><mi id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">D</mi></msub><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.3.2.2" xref="S3.E2.m1.1.1.3.2.2.cmml">â„’</mi><mrow id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.3.2.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.3.1" xref="S3.E2.m1.1.1.3.2.3.1.cmml">â€‹</mo><msub id="S3.E2.m1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.3.2.3.3.cmml"><mi id="S3.E2.m1.1.1.3.2.3.3.2" xref="S3.E2.m1.1.1.3.2.3.3.2.cmml">D</mi><mi id="S3.E2.m1.1.1.3.2.3.3.3" xref="S3.E2.m1.1.1.3.2.3.3.3.cmml">D</mi></msub></mrow></msub><mo id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><msub id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml"><mi id="S3.E2.m1.1.1.3.3.2.2" xref="S3.E2.m1.1.1.3.3.2.2.cmml">Î»</mi><mn id="S3.E2.m1.1.1.3.3.2.3" xref="S3.E2.m1.1.1.3.3.2.3.cmml">4</mn></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">â€‹</mo><msub id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.3.3.3.2.cmml">â„’</mi><mrow id="S3.E2.m1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.3.3.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.3.3.2" xref="S3.E2.m1.1.1.3.3.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.3.3.1" xref="S3.E2.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.3.3.3.3.3" xref="S3.E2.m1.1.1.3.3.3.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.3.3.1a" xref="S3.E2.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><msub id="S3.E2.m1.1.1.3.3.3.3.4" xref="S3.E2.m1.1.1.3.3.3.3.4.cmml"><mi id="S3.E2.m1.1.1.3.3.3.3.4.2" xref="S3.E2.m1.1.1.3.3.3.3.4.2.cmml">N</mi><mi id="S3.E2.m1.1.1.3.3.3.3.4.3" xref="S3.E2.m1.1.1.3.3.3.3.4.3.cmml">D</mi></msub></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">â„’</ci><ci id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">ğ·</ci></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><plus id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></plus><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2">â„’</ci><apply id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3"><times id="S3.E2.m1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.3.2.3.1"></times><ci id="S3.E2.m1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.3.2.3.2">ğ¾</ci><apply id="S3.E2.m1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.3.3.1.cmml" xref="S3.E2.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.3.3.2.cmml" xref="S3.E2.m1.1.1.3.2.3.3.2">ğ·</ci><ci id="S3.E2.m1.1.1.3.2.3.3.3.cmml" xref="S3.E2.m1.1.1.3.2.3.3.3">ğ·</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><apply id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2">ğœ†</ci><cn type="integer" id="S3.E2.m1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.1.1.3.3.2.3">4</cn></apply><apply id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.3.2">â„’</ci><apply id="S3.E2.m1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3.3"><times id="S3.E2.m1.1.1.3.3.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.3.3.2">ğº</ci><ci id="S3.E2.m1.1.1.3.3.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3.3.3">ğ´</ci><apply id="S3.E2.m1.1.1.3.3.3.3.4.cmml" xref="S3.E2.m1.1.1.3.3.3.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.3.3.4.1.cmml" xref="S3.E2.m1.1.1.3.3.3.3.4">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.3.3.4.2.cmml" xref="S3.E2.m1.1.1.3.3.3.3.4.2">ğ‘</ci><ci id="S3.E2.m1.1.1.3.3.3.3.4.3.cmml" xref="S3.E2.m1.1.1.3.3.3.3.4.3">ğ·</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{L}_{D}=\mathcal{L}_{KD_{D}}+\lambda_{4}\mathcal{L}_{GAN_{D}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p6.2" class="ltx_p">where <math id="S3.SS2.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{KD_{D}}" display="inline"><semantics id="S3.SS2.SSS2.p6.1.m1.1a"><msub id="S3.SS2.SSS2.p6.1.m1.1.1" xref="S3.SS2.SSS2.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p6.1.m1.1.1.2" xref="S3.SS2.SSS2.p6.1.m1.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p6.1.m1.1.1.3" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p6.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p6.1.m1.1.1.3.1" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS2.SSS2.p6.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p6.1.m1.1.1.3.3.2" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3.2.cmml">D</mi><mi id="S3.SS2.SSS2.p6.1.m1.1.1.3.3.3" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3.3.cmml">D</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.1.m1.1b"><apply id="S3.SS2.SSS2.p6.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3"><times id="S3.SS2.SSS2.p6.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p6.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.2">ğ¾</ci><apply id="S3.SS2.SSS2.p6.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p6.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p6.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3.2">ğ·</ci><ci id="S3.SS2.SSS2.p6.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3.3">ğ·</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.1.m1.1c">\mathcal{L}_{KD_{D}}</annotation></semantics></math> is the adversarial distillation loss used to encourage the discriminator to distinguish between generated images by the student and teacher networks and <math id="S3.SS2.SSS2.p6.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{GAN_{D}}" display="inline"><semantics id="S3.SS2.SSS2.p6.2.m2.1a"><msub id="S3.SS2.SSS2.p6.2.m2.1.1" xref="S3.SS2.SSS2.p6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p6.2.m2.1.1.2" xref="S3.SS2.SSS2.p6.2.m2.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p6.2.m2.1.1.3" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS2.p6.2.m2.1.1.3.2" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p6.2.m2.1.1.3.1" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p6.2.m2.1.1.3.3" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p6.2.m2.1.1.3.1a" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.1.cmml">â€‹</mo><msub id="S3.SS2.SSS2.p6.2.m2.1.1.3.4" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.4.cmml"><mi id="S3.SS2.SSS2.p6.2.m2.1.1.3.4.2" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.4.2.cmml">N</mi><mi id="S3.SS2.SSS2.p6.2.m2.1.1.3.4.3" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.4.3.cmml">D</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.2.m2.1b"><apply id="S3.SS2.SSS2.p6.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p6.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3"><times id="S3.SS2.SSS2.p6.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS2.p6.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.2">ğº</ci><ci id="S3.SS2.SSS2.p6.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.3">ğ´</ci><apply id="S3.SS2.SSS2.p6.2.m2.1.1.3.4.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p6.2.m2.1.1.3.4.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.4">subscript</csymbol><ci id="S3.SS2.SSS2.p6.2.m2.1.1.3.4.2.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.4.2">ğ‘</ci><ci id="S3.SS2.SSS2.p6.2.m2.1.1.3.4.3.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.4.3">ğ·</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.2.m2.1c">\mathcal{L}_{GAN_{D}}</annotation></semantics></math> is the adversarial GAN loss used to encourage discrimination between student and real images.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F2.sf1.2" class="ltx_inline-block ltx_transformed_outer" style="width:303.1pt;height:82.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.5pt,12.7pt) scale(0.765037013710589,0.765037013710589) ;">
<table id="S3.F2.sf1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.F2.sf1.2.1.1.1" class="ltx_tr">
<th id="S3.F2.sf1.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.F2.sf1.2.1.1.1.1.1" class="ltx_text">Calibration Dataset</span></th>
<th id="S3.F2.sf1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5">Model</th>
</tr>
<tr id="S3.F2.sf1.2.1.2.2" class="ltx_tr">
<th id="S3.F2.sf1.2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">ResNet20</th>
<th id="S3.F2.sf1.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">VGG16</th>
<th id="S3.F2.sf1.2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">MobileNetV2</th>
<th id="S3.F2.sf1.2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">ShuffleNetV2</th>
<th id="S3.F2.sf1.2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">RepVGG</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.F2.sf1.2.1.3.1" class="ltx_tr">
<td id="S3.F2.sf1.2.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR-10</td>
<td id="S3.F2.sf1.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.F2.sf1.2.1.3.1.2.1" class="ltx_text ltx_font_bold">0.28%</span></td>
<td id="S3.F2.sf1.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.F2.sf1.2.1.3.1.3.1" class="ltx_text ltx_font_bold">0.05%</span></td>
<td id="S3.F2.sf1.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.07%</td>
<td id="S3.F2.sf1.2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.F2.sf1.2.1.3.1.5.1" class="ltx_text ltx_font_bold">1.26%</span></td>
<td id="S3.F2.sf1.2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">41.94%</td>
</tr>
<tr id="S3.F2.sf1.2.1.4.2" class="ltx_tr">
<td id="S3.F2.sf1.2.1.4.2.1" class="ltx_td ltx_align_center">StyleGAN2-ADA</td>
<td id="S3.F2.sf1.2.1.4.2.2" class="ltx_td ltx_align_center">0.43%</td>
<td id="S3.F2.sf1.2.1.4.2.3" class="ltx_td ltx_align_center">0.16%</td>
<td id="S3.F2.sf1.2.1.4.2.4" class="ltx_td ltx_align_center"><span id="S3.F2.sf1.2.1.4.2.4.1" class="ltx_text ltx_font_bold">0.05%</span></td>
<td id="S3.F2.sf1.2.1.4.2.5" class="ltx_td ltx_align_center">7.48%</td>
<td id="S3.F2.sf1.2.1.4.2.6" class="ltx_td ltx_align_center"><span id="S3.F2.sf1.2.1.4.2.6.1" class="ltx_text ltx_font_bold">41.37%</span></td>
</tr>
<tr id="S3.F2.sf1.2.1.5.3" class="ltx_tr">
<td id="S3.F2.sf1.2.1.5.3.1" class="ltx_td ltx_align_center">DiStyleGAN</td>
<td id="S3.F2.sf1.2.1.5.3.2" class="ltx_td ltx_align_center">0.46%</td>
<td id="S3.F2.sf1.2.1.5.3.3" class="ltx_td ltx_align_center">0.07%</td>
<td id="S3.F2.sf1.2.1.5.3.4" class="ltx_td ltx_align_center">0.16%</td>
<td id="S3.F2.sf1.2.1.5.3.5" class="ltx_td ltx_align_center">1.49%</td>
<td id="S3.F2.sf1.2.1.5.3.6" class="ltx_td ltx_align_center">42.85%</td>
</tr>
<tr id="S3.F2.sf1.2.1.6.4" class="ltx_tr">
<td id="S3.F2.sf1.2.1.6.4.1" class="ltx_td ltx_align_center ltx_border_bb">Fractal</td>
<td id="S3.F2.sf1.2.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">1.10%</td>
<td id="S3.F2.sf1.2.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.69%</td>
<td id="S3.F2.sf1.2.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb">0.54%</td>
<td id="S3.F2.sf1.2.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb">87.21%</td>
<td id="S3.F2.sf1.2.1.6.4.6" class="ltx_td ltx_align_center ltx_border_bb">41.73%</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F2.sf2.2" class="ltx_inline-block ltx_transformed_outer" style="width:171.4pt;height:81.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.1pt,13.3pt) scale(0.753107933399679,0.753107933399679) ;">
<table id="S3.F2.sf2.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.F2.sf2.2.1.1.1" class="ltx_tr">
<th id="S3.F2.sf2.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.F2.sf2.2.1.1.1.1.1" class="ltx_text">Calibration Dataset</span></th>
<th id="S3.F2.sf2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Model</th>
</tr>
<tr id="S3.F2.sf2.2.1.2.2" class="ltx_tr">
<th id="S3.F2.sf2.2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">ShuffleNetV2</th>
<th id="S3.F2.sf2.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">RepVGG</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.F2.sf2.2.1.3.1" class="ltx_tr">
<td id="S3.F2.sf2.2.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR-10</td>
<td id="S3.F2.sf2.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">1.26%</td>
<td id="S3.F2.sf2.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.F2.sf2.2.1.3.1.3.1" class="ltx_text ltx_font_bold">0.45%</span></td>
</tr>
<tr id="S3.F2.sf2.2.1.4.2" class="ltx_tr">
<td id="S3.F2.sf2.2.1.4.2.1" class="ltx_td ltx_align_center">StyleGAN2-ADA</td>
<td id="S3.F2.sf2.2.1.4.2.2" class="ltx_td ltx_align_center">0.59%</td>
<td id="S3.F2.sf2.2.1.4.2.3" class="ltx_td ltx_align_center">0.48%</td>
</tr>
<tr id="S3.F2.sf2.2.1.5.3" class="ltx_tr">
<td id="S3.F2.sf2.2.1.5.3.1" class="ltx_td ltx_align_center">DiStyleGAN</td>
<td id="S3.F2.sf2.2.1.5.3.2" class="ltx_td ltx_align_center"><span id="S3.F2.sf2.2.1.5.3.2.1" class="ltx_text ltx_font_bold">0.11%</span></td>
<td id="S3.F2.sf2.2.1.5.3.3" class="ltx_td ltx_align_center">0.55%</td>
</tr>
<tr id="S3.F2.sf2.2.1.6.4" class="ltx_tr">
<td id="S3.F2.sf2.2.1.6.4.1" class="ltx_td ltx_align_center ltx_border_bb">Fractal</td>
<td id="S3.F2.sf2.2.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">87.21%</td>
<td id="S3.F2.sf2.2.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">42.16%</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Accuracy drop for each model and calibration dataset, as measured in the classification task on the CIFAR-10 test set.</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Network Architectures</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p"><span id="S3.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">Generator</span>
Initially, the Gaussian random noise vector is projected to 128 dimensions using a Fully Connected layer. Subsequently, the condition embedding and a projected noise vector are concatenated and passed through another Fully Connected layer, followed by three consecutive Upsampling blocks. Each upsampling block consists of an upsample layer (scale_factor=2, mode=â€™nearestâ€™), a 3x3 convolution with padding, a Batch Normalization layer, and a Gated Linear Unit (GLU). Finally, there is a convolutional block consisting of a 3x3 convolution with padding and a hyperbolic tangent activation function (tanh), which produces the fake image.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p"><span id="S3.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Discriminator</span>
DiStyleGANâ€™s discriminator consists of 4 consecutive Downsampling blocks (4x4 strided-convolution, Spectral Normalization, and a LeakyReLU), with each of them reducing the spatial size of the input image by a factor of 2. These four blocks also produce the feature maps that calculate the Feature Loss. Subsequently, the logit is flattened, projected to 128 dimensions, and concatenated with the class condition embedding before being passed through a final fully connected layer to produce the class-conditional discriminator loss.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Calibration Datasets</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The data used for the calibration of the quantization process came from four different distributions. First, we conducted quantization using a subset of the real CIFAR-10 data as a frame of reference. Second, we opted for synthetic images produced by the StyleGAN2-ADA model, the state-of-the-art model in class-conditional image generation on CIFAR-10. Third, we produced synthetic images using DiStyleGAN. Finally, fractal image data generated by Datumaro <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> were also used. While the former two synthetic datasets approximate the CIFAR-10 distribution, and thus could be considered representative, the fractal images do not constitute a representative dataset for the deep learning models pre-trained on CIFAR-10. However, we include them in our experiments since Lazarevich <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> demonstrated that it is possible to perform post-training quantization even with a non-representative dataset.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Samples from each of the four data distributions for the classes <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">â€Airplaneâ€œ</span> and <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">â€Horseâ€œ</span> are illustrated in <a href="#S3.F2" title="In 3.3 Calibration Datasets â€£ 3 Methodology â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, while samples for the rest of the classes can be found in <a href="#A1" title="Appendix A Calibration Datasets Samples â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>Â <span class="ltx_text ltx_ref_tag">A</span></a>. Additionally, <a href="#S3.T2" title="In 3.3 Calibration Datasets â€£ 3 Methodology â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> presents the <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_italic">Inception Score (IS)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> calculated for each data distribution. The inception score is a metric that evaluates the quality of synthetic generated images. The calculated score is based on the ability of a pre-trained InceptionV3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> model to classify the images of a synthetic dataset produced by a generative model. In our case, we calculated the inception scores using datasets of <math id="S3.SS3.p2.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.3.2" xref="S3.SS3.p2.1.m1.2.3.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">50</mn><mo id="S3.SS3.p2.1.m1.2.3.2.1" xref="S3.SS3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><list id="S3.SS3.p2.1.m1.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3.2"><cn type="integer" id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">50</cn><cn type="integer" id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">50,000</annotation></semantics></math> synthetic samples from each distribution.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S3.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Inception Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2.1" class="ltx_tr">
<th id="S3.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">StyleGAN2-ADA</th>
<td id="S3.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">10.34</td>
</tr>
<tr id="S3.T2.2.3.2" class="ltx_tr">
<th id="S3.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DiStyleGAN</th>
<td id="S3.T2.2.3.2.2" class="ltx_td ltx_align_center">6.78</td>
</tr>
<tr id="S3.T2.2.4.3" class="ltx_tr">
<th id="S3.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Fractal</th>
<td id="S3.T2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">3.31</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.4.2" class="ltx_text" style="font-size:90%;">Inception score calculated for each synthetic dataset used in the calibration process (higher is better).</span></figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2305.06052/assets/images/cifar_samples.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Samples from each of the four calibration datasets for the CIFAR-10 classes â€Airplaneâ€ and â€Horseâ€.</span></figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.2" class="ltx_p">For our quantization experiments, <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="5000" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">5000</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><cn type="integer" id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">5000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">5000</annotation></semantics></math> images were used from each of the aforementioned datasets (<math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mn id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><cn type="integer" id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">500</annotation></semantics></math> images per class of the CIFAR-10 dataset). Then, the official CIFAR-10 test set was utilized to evaluate the results of the quantized models for all combinations of calibration datasets, PyTorch models, and quantization techniques.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:def-quant-acc-drop</span> andÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:acc-quant-acc-drop</span> showcase the accuracy degradation percentage of the quantized models with respect to the performance of the original PyTorch models on the classification task on the CIFAR-10 test set. These results were obtained using the two different quantization algorithms, default and accuracy-aware, and the four calibration datasets described in <a href="#S3.SS3" title="3.3 Calibration Datasets â€£ 3 Methodology â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.3</span></a>. In addition, <a href="#S3.T2" title="In 3.3 Calibration Datasets â€£ 3 Methodology â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> presents the inception scores of the synthetic calibration datasets, a quantitative metric that indicates the difference in the quality of the generated images between the four aforementioned datasets.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Discussion</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Based on the results obtained by the experiments, it is noticeable that in most cases, the Default Quantization algorithm can successfully quantize the models while showing only a negligible degradation in accuracy. This especially holds true for the ResNet20, VGG, and MobileNetV2 networks. Even with a non-representative calibration dataset (i.e. Fractal), the accuracy decrease is limited to a maximum of 1.1%, as showcased in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:def-quant-acc-drop</span>, which is in par with the findings of Lazarevich <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">When the default quantization algorithm fails, the accuracy-aware algorithm can be employed. The RepVGG model suffers the most by the default algorithm, with more than a 40% accuracy decrease across all calibration datasets. This could be potentially attributed to the high variance of RepVGGâ€™s activation weights, constituting the weight distribution quite different across different channels. However, further experimentation is required to validate this claim. On the other hand, the results of the accuracy-aware quantization shown in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:acc-quant-acc-drop</span> prove that the same model can be quantized using synthetic data with minimal accuracy degradation (<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">â‰ˆ</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><approx id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\approx</annotation></semantics></math> 0.5%). The same, but less significant, effect can be observed for the ShuffleNetV2 model. It should also be noted that the accuracy-aware quantization requires a representative calibration dataset; thus the corresponding results for the fractal images were expected.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Finally, it is essential to notice that the two synthetic datasets, StyleGAN2-ADA and DiStyleGAN, lead to comparable results with the official CIFAR-10 dataset in the case of quantization. Surprisingly enough, although the DiStyleGAN model was trained through knowledge distillation, with the StyleGAN2-ADA as the teacher network, there are cases where the quantization process using the dataset generated by the former leads to better results compared to when the latter is used. This finding is also in contrast with the quantitative results for the synthetic generated images (<a href="#S3.T2" title="In 3.3 Calibration Datasets â€£ 3 Methodology â€£ Post-training Model Quantization Using GANs for Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>) which clearly showcase the superiority of the StyleGAN2-ADA model compared to our DiStyleGAN, in terms of the quality of their synthetic generated images. However, the inception score metric does not take into consideration how synthetic images compare to real images. Other metrics (<em id="S4.SS2.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.p3.1.2" class="ltx_text"></span> <span id="S4.SS2.p3.1.3" class="ltx_text ltx_font_italic">FrÃ©chet inception distance (FID)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>) that evaluate the synthetic images while taking into account the distribution of the real data, should also be included in further experimentation.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Based on our experiments, post-training quantization leads to minimal accuracy degradation for three of the selected models (ResNet20, VGG16, MobileNetV2), regardless of the used calibration dataset. For the remaining models (ShuffleNetV2, RepVGG), while an accuracy-aware quantization is required, the results indicate similar performance when the real data are used for calibration compared to the synthetic. Insignificant differences were also observed between the quantized models, when synthetic data produced by the complex StyleGAN model were used, compared to when they were produced by DiStyleGAN. This finding suggests that in the case of quantization when the synthetic data of the calibration dataset approximate the distribution of the real data, even a simple generator might be enough. However, further experiments with synthetic calibration datasets of varied quality are required to corroborate this finding.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Jimmy Ba and Rich Caruana.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Do deep nets really need to be deep?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Scalable methods for 8-bit training of neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Ron Banner, Yury Nahshan, and Daniel Soudry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Post training 4-bit quantization of convolutional networks for
rapid-deployment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, MichaelÂ W Mahoney, and Kurt
Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Zeroq: A novel zero shot quantization framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 13169â€“13178, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Ting-Yun Chang and Chi-Jen Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Tinygan: Distilling biggan for conditional image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Asian Conference on Computer Vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Compressing neural networks with the hashing trick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages
2285â€“2294. PMLR, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Low-bit quantization of neural networks for efficient inference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE/CVF International Conference on Computer Vision
Workshop (ICCVW)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 3009â€“3018. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Intel Corporation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Post-training quantization with pot.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://docs.openvino.ai/latest/pot_introduction.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://docs.openvino.ai/latest/pot_introduction.html</a><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">Accessed: 18-01-2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Binaryconnect: Training deep neural networks with binary weights
during propagations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian
Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Repvgg: Making vgg-style convnets great again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 13733â€“13742, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Alexander Finkelstein, Uri Almog, and Mark Grobman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Fighting quantization bias with bias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.03193</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 63(11):139â€“144, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Deep learning with limited numerical precision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages
1737â€“1746. PMLR, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Song Han, Huizi Mao, and WilliamÂ J. Dally.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding, 2015.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Song Han, Jeff Pool, John Tran, and William Dally.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Learning both weights and connections for efficient neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Babak Hassibi, DavidÂ G Stork, and GregoryÂ J Wolff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Optimal brain surgeon and general network pruning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE international conference on neural networks</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages
293â€“299. IEEE, 1993.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 770â€“778, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Yihui He, Xiangyu Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Channel pruning for accelerating very deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 1389â€“1397, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Distilling the knowledge in a neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1503.02531</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2(7), 2015.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Binarized neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 29, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and
Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Training generative adversarial networks with limited data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">,
33:12104â€“12114, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Analyzing and improving the image quality of StyleGAN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Learning multiple layers of features from tiny images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Technical report</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Ivan Lazarevich, Alexander Kozlov, and Nikita Malinin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Post-training deep neural network pruning via layer-wise calibration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 798â€“805, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Yann LeCun, John Denker, and Sara Solla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Optimal brain damage.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2, 1989.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and HansÂ Peter Graf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Pruning filters for efficient convnets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1608.08710</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Fixed point quantization of deep convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages
2849â€“2858. PMLR, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Shufflenet v2: Practical guidelines for efficient cnn architecture
design.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 116â€“131, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Markus Nagel, RanaÂ Ali Amjad, Mart VanÂ Baalen, Christos Louizos, and Tijmen
Blankevoort.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Up or down? adaptive rounding for post-training quantization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages
7197â€“7206. PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Markus Nagel, MartÂ van Baalen, Tijmen Blankevoort, and Max Welling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Data-free quantization through weight equalization and bias
correction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 1325â€“1334, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
Xi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Improved techniques for training gans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 29, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Mobilenetv2: Inverted residuals and linear bottlenecks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 4510â€“4520, 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Suraj Srinivas and RÂ Venkatesh Babu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Data-free parameter pruning for deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1507.06149</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Going deeper with convolutions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 1â€“9, 2015.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and
Zbigniew Wojna.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Rethinking the inception architecture for computer vision, 2015.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
OpenVINO toolkit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Dataset management framework (datumaro), 2020.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/openvinotoolkit/datumaro" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/openvinotoolkit/datumaro</a><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">; Accessed:
12/02/2023.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Lq-nets: Learned quantization for highly accurate and compact deep
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 365â€“382, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris DeÂ Sa, and Zhiru Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Improving neural network quantization without retraining using
outlier channel splitting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages
7543â€“7552. PMLR, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Calibration Datasets Samples</h2>

<figure id="A1.F1" class="ltx_figure"><img src="/html/2305.06052/assets/images/cifar_samples_2.png" id="A1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="723" height="730" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure A.1</span>: </span><span id="A1.F1.3.2" class="ltx_text" style="font-size:90%;">Samples from each of the four calibration datasets for the CIFAR-10 classes: â€Automobileâ€, â€Birdâ€, â€Catâ€, and â€Deerâ€œ.</span></figcaption>
</figure>
<figure id="A1.F2" class="ltx_figure"><img src="/html/2305.06052/assets/images/cifar_samples_3.png" id="A1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="726" height="727" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure A.2</span>: </span><span id="A1.F2.3.2" class="ltx_text" style="font-size:90%;">Samples from each of the four calibration datasets for the CIFAR-10 classes: â€Dogâ€, â€Frogâ€, â€Shipâ€, and â€Truckâ€.</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.06051" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.06052" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.06052">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.06052" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.06053" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 08:01:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
