<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.10150] TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition</title><meta property="og:description" content="Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language underst…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.10150">

<!--Generated on Sun May  5 19:52:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md Mahadi Hasan Nahid 
<br class="ltx_break">University of Alberta 
<br class="ltx_break">mnahid@ualberta.ca
<span id="id1.1.id1" class="ltx_ERROR undefined">\And</span>Davood Rafiei 
<br class="ltx_break">University of Alberta 
<br class="ltx_break">drafiei@ualberta.ca
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose <span id="id2.id1.1" class="ltx_text ltx_font_bold">TabSQLify</span>, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task.
In our comprehensive evaluation on four challenging datasets,
our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Tables serve as the most prevalent forms of structured information across diverse domains, ranging from databases and spreadsheets to open data repositories, web pages and document collections.
Developing natural language interfaces for tabular data poses a significant challenge, primarily in terms of effectively interpreting the semantics of table cells and understanding the relationships between cell values in response to a user query. This challenge is accentuated when tables are enveloped in text, such as titles, captions, and contextual text within a document. In these instances, the scope of reasoning expands beyond the confines of table cells to incorporate the surrounding natural language text.
This reasoning is essential for many downstream tasks such as table-based fact verification and table-based question answering (TableQA). As depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, table-based reasoning is intricate, demanding sophisticated textual, numerical, and logical reasoning across both unstructured text and (semi-)structured tables.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div id="S1.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:408.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.7pt,4.4pt) scale(0.978792797463394,0.978792797463394) ;"><img src="/html/2404.10150/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_square" width="473" height="446" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of table-based question answering.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure">
<div id="S1.F2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:117.4pt;vertical-align:-0.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-724.7pt,195.8pt) scale(0.230272164884598,0.230272164884598) ;"><img src="/html/2404.10150/assets/x2.png" id="S1.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="2007" height="543" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of <span id="S1.F2.5.1" class="ltx_text ltx_font_bold">TabSQLify</span>, consisting of two steps: <span id="S1.F2.6.2" class="ltx_text ltx_font_bold">(1)</span> generating SQL queries from natural language questions or statements and executing the SQL queries on the original tables to obtain sub-tables containing only essential information, and <span id="S1.F2.7.3" class="ltx_text ltx_font_bold">(2)</span> using LLMs with the sub-table and the question or claim to generate the answer.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent studies highlight the impressive capability of LLMs in reasoning over both text and tabular data. However, these works typically utilize the full table as context for reasoning <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, limiting their ability to large tables. In particular, LLMs operate under a maximum token limit, and when processing a large table, there is a risk of potential truncation of the input or hallucination in the output <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>; Ji et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>.
This limitation poses difficulties in handling large tables, making it impractical to encompass the entire table within the maximum token boundary of a prompt. <cite class="ltx_cite ltx_citemacro_citet">Chen (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> highlights that LLMs struggle to generalize when confronted with “large” tables containing 30 or more rows, leading to a decline in accuracy as the table size increases.
While there have been works to decompose both questions and tables using LLMs <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>, this line of work still requires providing the full table to the LLM and cannot scale to large tables.
The question studied in this work is if the size of a table can be reduced before passing it to the language model without impacting its performance.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, our aim is to leverage the symbolic representation capabilities of LLMs to reduce table size and their robustness to natural language variations for addressing formatting differences.
Symbolic models, such as text-to-SQL, are not affected by table size and can reliably scale to large tables. However, for reliable storage and querying in a relational database, tables are expected to adhere to a more rigorous formatting.
Tables in the wild, such as those found on the web, often lack this formatting, necessitating substantial preprocessing and normalization efforts to convert the content <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. LLMs are well-suited for resolving potential differences in the formating of rows and cell values.
This work aims to strike a balance between table reasoning and table decomposition.
Our approach involves using symbolic methods to narrow down the task to a targeted region in a table and then utilizes LLMs to reason over the limited relevant information.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We propose <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">TabSQLify</span>, a novel approach that integrates symbolic methods with the reasoning power of LLMs. TabSQLify leverages text-to-SQL generation to decompose large tables into smaller and relevant sub-tables for table reasoning tasks.
The method involves two key steps: <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">(1)</span> generating SQL queries from natural language questions or statements using LLMs under few-shot prompting, then executing the SQL queries on the original tables to obtain sub-tables containing only essential information for answering questions or verifying statements, and <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">(2)</span> using LLMs with the sub-table and the question or claim to generate the answer.
The core concept of the approach is to utilize the natural language understanding and generation strengths of LLMs while reducing their burden in table encoding and reasoning (see Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Decomposing tables into sub-tables offers several advantages, including <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">(1)</span> reducing input length for improved scalability and efficiency in reasoning tasks involving large tables, <span id="S1.p4.1.5" class="ltx_text ltx_font_bold">(2)</span> filtering out irrelevant and redundant information that do not contribute to the reasoning process, hence making the reasoning more focused, and <span id="S1.p4.1.6" class="ltx_text ltx_font_bold">(3)</span> providing an intermediate representation (in this case, SQL queries and sub-tables) that is more interpretable and explainable for tracing and verification purposes.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We evaluate our method on four challenging table reasoning datasets: WikiTQ <cite class="ltx_cite ltx_citemacro_citep">(Pasupat and Liang, <a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite>, FeTaQA <cite class="ltx_cite ltx_citemacro_citep">(Nan et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, TabFact <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite> and WikiSQL <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite>.
Our evaluation on table-based question answering and fact verification tasks show that our method outperforms other LLM-based baselines, with gpt-3.5-turbo (chatgpt) as the LLM. Moreover, our method can significantly reduce the input length, making it more scalable and efficient for large-scale table reasoning applications than existing methods that require the full table context as input.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The contributions of this paper are as follows:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present a novel approach that utilizes text-to-SQL generation to decompose tables into smaller, contextually relevant sub-tables, particularly designed for table reasoning tasks. This method offers a substantial reduction in table size, proving particularly advantageous for large tables that exceed the maximum allowable context window of LLMs.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our model outperforms some of the leading models that employ multiple responses and self-consistency. Clearly using those techniques can further boost the performance of our method.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our evaluation on challenging table reasoning datasets demonstrates the remarkable performance of our method compared to existing methods that rely on full tables as input. A comprehensive evaluation across various tasks is conducted to elucidate both the advantages and constraints of our approach.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work is closely intertwined with the literature on semantic parsing of questions and table schema (also known as text to data) as well as the reasoning applied to semi-structured tables (alternatively known as data to text).</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Semantic Parsing: Text to Data</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Table-based reasoning conventionally involves semantic parsing of questions and subsequent execution of the generated queries on tables. Traditional models in this domain were often domain-specific, supporting controlled natural language <cite class="ltx_cite ltx_citemacro_citep">(Popescu et al., <a href="#bib.bib23" title="" class="ltx_ref">2003</a>; Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2007</a>)</cite>, and posed challenges in adaptation to new domains or datasets. However, recent models leveraging machine learning techniques or large language models are trained on extensive datasets and query repositories, supporting a shift towards greater domain-independence. In particular, LLMs, when used with few-shot prompting, serve as powerful code generators, and techniques such as controlled decoding further improves the reliability of code generation <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Rajkumar et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>; Pourreza and Rafiei, <a href="#bib.bib24" title="" class="ltx_ref">2023</a>; Chang and Fosler-Lussier, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Ni et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Cross-domain benchmarks such as WikiSQL <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite>, Spider <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib36" title="" class="ltx_ref">2018</a>)</cite>, CoSQL <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib35" title="" class="ltx_ref">2019a</a>)</cite>, SParC <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib37" title="" class="ltx_ref">2019b</a>)</cite>, and BIRD <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> have played a pivotal role in advancing this field, offering diverse examples of natural language queries paired with formal query language counterparts, such as SQL.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Glass et al. <cite class="ltx_cite ltx_citemacro_citep">(Glass et al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite> innovatively explores methods to capture both row and column semantics, improving the model’s query comprehension. Inner Table Retrieval (ITR) <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> employs a similarity-based approach for locating sub-tables. These approaches involve pre-training and fine-tuning, which heavily rely on specific datasets. This reliance makes them inapplicable without access to a corresponding training dataset, while the need for optimal hyperparameters further limits their generalization.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">In this line of work, the reasoning is generally done on questions and table schemata, with the expectation that the data in a table strictly adheres to the table schema (e.g., all values in a column having the same data type).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Table Reasoning: Data to Text</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The relevant models can be categorized into more traditional models and recent LLM-based models. Many early models undergo pre-training on both tables and text to acquire a joint representation, utilizing this representation for reasoning without relying on symbolic execution. Notably, TaPas <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> retrieves masked information from tables, TAPEX <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib19" title="" class="ltx_ref">2022b</a>)</cite> employs the BART model to emulate an SQL executor, ReasTAP <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> instills reasoning skills via pre-training, TABERT <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> encodes a subset of table content most pertinent to the input, and PASTA <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> pre-trains language models to be cognizant of common table-based operations. All these models have contributed to the progress on table-based reasoning. Despite achieving commendable results through pre-training on substantial datasets, these models still necessitate fine-tuning on task-specific datasets <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Large language models have become competitive models in many domains and tasks including table reasoning, with their reasoning capabilities covering math, common sense, and symbolic reasoning. This is often done using few-shot prompts without fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. It has been shown that the reasoning capabilities of these models can be further improved by breaking more complex tasks into steps, using methods such as chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> and ZeroCoT <cite class="ltx_cite ltx_citemacro_citep">(Kojima et al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> or more carefully selecting examples in the prompt <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2022a</a>)</cite>.
The Table-CoT Model <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> generates the final answer to a question by employing in-context learning and chain-of-thought prompting to table-based tasks. In contrast, the BINDER <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> model generates programs in a programming language, extending its capabilities to solve commonsense problems. The DATER <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> approach uses LLMs to decompose tables and questions for solving table-based QA and fact verification tasks.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">ReAcTable <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite> adopts the ReAct paradigm, encompassing step-by-step reasoning, code execution through external tools, intermediate table generation, and a majority voting mechanism. This method leverages LLMs to decompose the problem into multiple steps, each consisting of logical operations in the form of code to process tabular data as required. In a more recent model, LEVER <cite class="ltx_cite ltx_citemacro_citep">(Ni et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite> presents a method to enhance language-to-code generation by training to validate the generated programs based on their execution results.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">StructGPT <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> enhances LLM reasoning for structured data using an Iterative Reading-then-Reasoning approach. However, the complexity and cost of StructGPT are exacerbated by the practice of passing entire tables to LLM in the reading phase, thus limiting the model’s scalability to large tables due to token limits. Chain-of-Table <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite> extends Chain-of-Thought to tables, improving accuracy by transforming input tables and guiding LLM with intermediate tables during reasoning. However, it requires multiple intermediate steps and LLM calls.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Table reasoning approaches typically operate under the assumption that the tables in question are sufficiently small to be directly input into the model. This specific issue is the focus of our investigation in this paper.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our approach capitalizes on the proficiency of LLMs in parsing natural language text and generating SQL to enhance their capabilities in table reasoning. Large language models face challenges in accommodating extensive contextual information, especially when dealing with large tables that exceed their token limits. Increasing this size for large tables is unrealistic due to the quadratic time and memory complexities of self-attention mechanism in input length. Furthermore, LLMs are more likely to produce errors when handling lengthy contexts <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>; Ji et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>. To overcome these challenges, our work efficiently identifies and extracts relevant table parts, optimizing the input prompt size without sacrificing performance.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Table Preprocessing</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Although tabular data is typically stored in a relational database and queried using SQL, many tables collected from web sources lack the rigorous structure and consistency that is needed for SQL queries to retrieve correct answers. It is generally a challenge to fully clean data from different sources or with no clean lineage records. Our hypothesis is that applying some general table cleaning and relaxing the granularity of retrievals to relevant rows and columns that have the answers, instead of the exact answers, makes the SQL engine more reliable. Of course, the exact answer must be extracted at the end. This is done in our reasoning phase (<math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi mathvariant="normal" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\S</annotation></semantics></math> <a href="#S3.SS3" title="3.3 Reasoning and Answer Generation ‣ 3 Methodology ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) where an LLM is used, and it is better equipped to handle formatting differences.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For our table cleaning, we normalized numerical values and date fields. In particular, numerical values frequently feature commas, necessitating preprocessing to ensure consistency. To address this, we uniformly removed commas from all numerical entries. Additionally, the diverse date formats within the tables posed a challenge in generating accurate conditions for SQL queries. To address this, we standardized all date formats to the YYYY-MM-DD format. As an example, we converted numbers like “360,000” to “360000,” and different date formats such as “31 October 2008,” “31 Oct 2008” and “October 31, 2008” to the standardized “2008-10-31”.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Subtable Selection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The subtable selection can be done by three strategies: <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">(1)</span> selecting essential columns, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">(2)</span> selecting essential rows, and <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_bold">(3)</span> selecting both essential columns and rows.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In this step, instead of feeding the entire table to the LLM, we provide essential table information such as the title, column names, and three example rows alongside the question. We utilize few-shot learning for this step, where we provide the LLM with a few examples. Subsequently, the LLM generates an SQL query to select the subtable based on this provided information. Selecting essential rows may require performing grouping and aggregation, and our generated SQL queries can include GROUP BY clauses and aggregation functions.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">By selecting the essential columns and rows, we are reducing the context size while optimizing the relevance of information for subsequent reasoning tasks. When employing strategies (2) or (3), sometimes essential rows may not be safely extracted, for example returning an empty table due to noisy input. In those cases, we opt for the column selection strategy. The format of the prompt used for selecting necessary columns and row is described in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 Subtable Selection ‣ 3 Methodology ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div id="S3.F3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:368.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-299.9pt,254.8pt) scale(0.419580415450421,0.419580415450421) ;"><img src="/html/2404.10150/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_square" width="1103" height="937" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prompt used for the subtable selection step of TabSQLify<sub id="S3.F3.3.1" class="ltx_sub">col+row</sub>.</figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">It is still conceivable that the output of the subtable selection step remains large, for example when finding the top-k most popular products for large values of k. We consider this limitation as inherent to the nature of the task and not specific to our approach since the sub-table containing all the items of the top-k is necessary to answer this question.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Reasoning and Answer Generation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this step, an LLM is employed, wherein we input the SQL derived from the previous step, the subtable obtained by executing the SQL query and the question. Depending on the domain, additional contextual information, such as the surrounding text, may also be incorporated. This approach is adopted to help the model focus on the relevant parts for understanding the context and answering the question. Moreover, we utilize few-shot learning techniques while adhering to the Chain-of-Thought prompting style. The format of the answer generation prompt is described in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3 Reasoning and Answer Generation ‣ 3 Methodology ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div id="S3.F4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:418pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-346.5pt,334.1pt) scale(0.384862086566789,0.384862086566789) ;"><img src="/html/2404.10150/assets/x4.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_square" width="1203" height="1160" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Prompts used for the answer generation step.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We assess our proposed approach across four datasets centered on reasoning with tables. Given our constraints on using LLMs, in terms of the number of requests and associated costs, our method is exclusively evaluated on the test sets of these datasets, with no fine-tuning on the training sets.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">WikiTQ</span> WikiTableQuestions (WikiTQ) contains complex questions annotated by crowd workers based on Wikipedia tables. These questions involve multiple complex operations, such as comparison, aggregation, and arithmetic operations, which require reasoning over multiple entries in a table. The standard test set contains 4,344 samples <cite class="ltx_cite ltx_citemacro_citep">(Pasupat and Liang, <a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">FetaQA</span> Free-form Table Question Answering (FeTaQA) contains free-form table questions that require deep reasoning and understanding. These questions are usually hard because it requires processing information from different parts of the table. Unlike WikiTQ, this dataset annotates long free-form answers. Our approach is evaluated on the test set that contains 2,003 samples <cite class="ltx_cite ltx_citemacro_citep">(Nan et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">TabFact</span> Table-Fact-Checking (TabFact) is a benchmark for verifying facts based on tables, which includes statements created by crowd workers using tables from Wikipedia. For example, a statement must be judged as either “True” or “False” based on the information in a given table. The accuracy is reported on the test-small set, which contains 2,024 statements and 298 tables <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">WikiSQL</span> WikiSQL is a simpler TableQA dataset, necessitating the filtering and aggregation of information from the table content. Each question in WikiSQL is associated with a ground truth SQL query, from which we extract the gold answer and compare it with our results. We present the accuracy achieved on the test set of WikiSQL <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In the experiments, we use gpt-3.5-turbo (chatgpt) as our language model. The prompt format mainly follows <cite class="ltx_cite ltx_citemacro_citet">Chang and Fosler-Lussier (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Tai et al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, which inputs the table schema and the first three table rows. The detail LLM hyper-parameters are provided in Appendix <a href="#A1" title="Appendix A LLM Hyper-parameters ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, and all our code and prompts are available at <a target="_blank" href="https://github.com/mahadi-nahid/TabSQLify" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mahadi-nahid/TabSQLify</a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Baselines</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We compare our approach with several strong baseline methods. These methods can be split into two groups.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Pre-training and fine-tuning based models</span> Our evaluation involves comparing our work with different models ranging from pre-training to fine-tuning. These models, pretrained on a large table corpus, aim to encode a given table as a plain sequence into an encoder and subsequently employ a decoder to generate an answer. As our baselines, we consider Table-BERT <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>, LogicFactChecker <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>, TaPas <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, SAT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite>, TAPEX <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib19" title="" class="ltx_ref">2022b</a>)</cite>, GraPPa <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, PASTA <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> as our baslines. For FeTaQA evaluation, we compare our results against T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>; Nan et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">LLM based models</span>
For the LLM based methods with in-context learning, we compare against TableCoT <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, BINDER <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>, DATER <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>, StructGPT <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>, ReAcTable <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite>, ITR <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>, LEVER <cite class="ltx_cite ltx_citemacro_citep">(Ni et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite> and Chain-of-Table <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite> as our baselines.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation metrics</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">For the WikiTQ and WikiSQL dataset, exact match (EM) accuracy was used to check if the predicted answers were the same as the correct ones. To account for different formatting of date and number fields, we added a pre-mactching check <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>, consistent with preprocessing (<math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi mathvariant="normal" id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\S</annotation></semantics></math> <a href="#S3.SS1" title="3.1 Table Preprocessing ‣ 3 Methodology ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). The accuracy of TabFact was determined using binary classification accuracy. To evaluate FeTaQA, metrics such as ROUGE-1, ROUGE-2, and ROUGE-L <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a href="#bib.bib16" title="" class="ltx_ref">2004</a>)</cite> were used. However, ROUGE score lacks the ability to gauge the faithfulness and correctness of model-generated content. In line with <cite class="ltx_cite ltx_citemacro_citet">Chen (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, a human evaluation was conducted across four aspects: fluency (assessing linguistic errors), correctness (ensuring accurate answers to questions), faithfulness (verifying grounding on the input table), and adequacy (evaluating the comprehensiveness of the generated sentence in covering all answers) <cite class="ltx_cite ltx_citemacro_cite">Nan et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Model accuracy</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">As shown on Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Model accuracy ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, TabSQLify achieves an accuracy of 62.0% and 63.7% on the more challenging WikiTA dataset when reasoning is performed solely using the extracted columns and extracted rows, respectively. By extracting both the necessary columns and rows, we achieve an accuracy of 64.7%. Our model outperforms all pretrained models and LLM-based baselines, with chatgpt used as the LLM, on WikiTQ dataset <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Codex was not available at the time of running our experiments, and the reported results are from the respective papers of our baselines.</span></span></span>. It surpasses BINDER-Codex and achieves accuracy very close to the state-of-the-art model DATER. It is worth noting that, unlike our model, which considers only one response, both BINDER and DATER utilize 20 responses for the WikiTQ dataset to obtain the final answer.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Models</th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<th id="S5.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><cite class="ltx_cite ltx_citemacro_citep">Agarwal et al.,<a href="#bib.bib1" title="" class="ltx_ref">2019</a></cite></th>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">44.1</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<th id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_citep">Wang et al.,<a href="#bib.bib28" title="" class="ltx_ref">2019</a></cite></th>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center">44.5</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<th id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TaPas</th>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_center">48.8</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<th id="S5.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GraPPa</th>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_center">52.7</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<th id="S5.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LEVER</th>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_center">62.9</td>
</tr>
<tr id="S5.T1.1.7.6" class="ltx_tr">
<th id="S5.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ITR</th>
<td id="S5.T1.1.7.6.2" class="ltx_td ltx_align_center">63.4</td>
</tr>
<tr id="S5.T1.1.8.7" class="ltx_tr">
<th id="S5.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-3 CoT</th>
<td id="S5.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">45.7</td>
</tr>
<tr id="S5.T1.1.9.8" class="ltx_tr">
<th id="S5.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TableCoT-Codex</th>
<td id="S5.T1.1.9.8.2" class="ltx_td ltx_align_center">48.8</td>
</tr>
<tr id="S5.T1.1.10.9" class="ltx_tr">
<th id="S5.T1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DATER-Codex</th>
<td id="S5.T1.1.10.9.2" class="ltx_td ltx_align_center">65.9</td>
</tr>
<tr id="S5.T1.1.11.10" class="ltx_tr">
<th id="S5.T1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BINDER-Codex</th>
<td id="S5.T1.1.11.10.2" class="ltx_td ltx_align_center">61.9</td>
</tr>
<tr id="S5.T1.1.12.11" class="ltx_tr">
<th id="S5.T1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ReAcTable-Codex</th>
<td id="S5.T1.1.12.11.2" class="ltx_td ltx_align_center">65.8</td>
</tr>
<tr id="S5.T1.1.13.12" class="ltx_tr">
<th id="S5.T1.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SQL-Codex</th>
<td id="S5.T1.1.13.12.2" class="ltx_td ltx_align_center">61.1</td>
</tr>
<tr id="S5.T1.1.14.13" class="ltx_tr">
<th id="S5.T1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BINDER-chatgpt</th>
<td id="S5.T1.1.14.13.2" class="ltx_td ltx_align_center ltx_border_t">55.4</td>
</tr>
<tr id="S5.T1.1.15.14" class="ltx_tr">
<th id="S5.T1.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DATER-chatgpt</th>
<td id="S5.T1.1.15.14.2" class="ltx_td ltx_align_center">52.8</td>
</tr>
<tr id="S5.T1.1.16.15" class="ltx_tr">
<th id="S5.T1.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ReAcTable-chatgpt</th>
<td id="S5.T1.1.16.15.2" class="ltx_td ltx_align_center">52.5</td>
</tr>
<tr id="S5.T1.1.17.16" class="ltx_tr">
<th id="S5.T1.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SQL-chatgpt</th>
<td id="S5.T1.1.17.16.2" class="ltx_td ltx_align_center">54.1</td>
</tr>
<tr id="S5.T1.1.18.17" class="ltx_tr">
<th id="S5.T1.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TableCoT-chatgpt</th>
<td id="S5.T1.1.18.17.2" class="ltx_td ltx_align_center">52.4</td>
</tr>
<tr id="S5.T1.1.19.18" class="ltx_tr">
<th id="S5.T1.1.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">StructGPT</th>
<td id="S5.T1.1.19.18.2" class="ltx_td ltx_align_center">52.2</td>
</tr>
<tr id="S5.T1.1.20.19" class="ltx_tr">
<th id="S5.T1.1.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Chain-of-Table</th>
<td id="S5.T1.1.20.19.2" class="ltx_td ltx_align_center">59.9</td>
</tr>
<tr id="S5.T1.1.21.20" class="ltx_tr">
<th id="S5.T1.1.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T1.1.21.20.1.1" class="ltx_sub">col</sub>
</th>
<td id="S5.T1.1.21.20.2" class="ltx_td ltx_align_center">62.0</td>
</tr>
<tr id="S5.T1.1.22.21" class="ltx_tr">
<th id="S5.T1.1.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T1.1.22.21.1.1" class="ltx_sub">row</sub>
</th>
<td id="S5.T1.1.22.21.2" class="ltx_td ltx_align_center">63.7</td>
</tr>
<tr id="S5.T1.1.23.22" class="ltx_tr">
<th id="S5.T1.1.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TabSQLify<sub id="S5.T1.1.23.22.1.1" class="ltx_sub">col+row</sub>
</th>
<td id="S5.T1.1.23.22.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.1.23.22.2.1" class="ltx_text ltx_font_bold">64.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy compared to the baselines on WikiTQ with the official evaluator.</figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">For the TabFact dataset, as illustrated in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Model accuracy ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, TabSQLify outperforms all LLM-based state-of-the-art approaches, with ChatGPT as the LLM. We achieve an accuracy of 79.5% when we extract the required sub-table by applying both column and row filtering. It is important to highlight that BINDER and DATER employ multiple responses and self-consistency to obtain the final answer. The reported results on TabFact are based on 50 responses for BINDER, 20 responses for DATER, and only one response for our model, hence it gives a lower bound of our model performance.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Table-BERT</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">68.1</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LogicFactChecker</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center">74.3</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SAT</th>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center">75.5</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TaPas</th>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_center">83.9</td>
</tr>
<tr id="S5.T2.1.6.5" class="ltx_tr">
<th id="S5.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TAPEX</th>
<td id="S5.T2.1.6.5.2" class="ltx_td ltx_align_center">85.9</td>
</tr>
<tr id="S5.T2.1.7.6" class="ltx_tr">
<th id="S5.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SaMoE</th>
<td id="S5.T2.1.7.6.2" class="ltx_td ltx_align_center">86.7</td>
</tr>
<tr id="S5.T2.1.8.7" class="ltx_tr">
<th id="S5.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PASTA</th>
<td id="S5.T2.1.8.7.2" class="ltx_td ltx_align_center">90.8</td>
</tr>
<tr id="S5.T2.1.9.8" class="ltx_tr">
<th id="S5.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Human</th>
<td id="S5.T2.1.9.8.2" class="ltx_td ltx_align_center">92.1</td>
</tr>
<tr id="S5.T2.1.10.9" class="ltx_tr">
<th id="S5.T2.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TableCoT-Codex</th>
<td id="S5.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_t">72.6</td>
</tr>
<tr id="S5.T2.1.11.10" class="ltx_tr">
<th id="S5.T2.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DATER-Codex</th>
<td id="S5.T2.1.11.10.2" class="ltx_td ltx_align_center">85.6</td>
</tr>
<tr id="S5.T2.1.12.11" class="ltx_tr">
<th id="S5.T2.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BINDER-Codex</th>
<td id="S5.T2.1.12.11.2" class="ltx_td ltx_align_center">85.1</td>
</tr>
<tr id="S5.T2.1.13.12" class="ltx_tr">
<th id="S5.T2.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ReAcTable-Codex</th>
<td id="S5.T2.1.13.12.2" class="ltx_td ltx_align_center">83.1</td>
</tr>
<tr id="S5.T2.1.14.13" class="ltx_tr">
<th id="S5.T2.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ReAcTable-chatgpt</th>
<td id="S5.T2.1.14.13.2" class="ltx_td ltx_align_center ltx_border_t">73.1</td>
</tr>
<tr id="S5.T2.1.15.14" class="ltx_tr">
<th id="S5.T2.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TableCoT-chatgpt</th>
<td id="S5.T2.1.15.14.2" class="ltx_td ltx_align_center">73.1</td>
</tr>
<tr id="S5.T2.1.16.15" class="ltx_tr">
<th id="S5.T2.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BINDER-chatgpt</th>
<td id="S5.T2.1.16.15.2" class="ltx_td ltx_align_center">79.1</td>
</tr>
<tr id="S5.T2.1.17.16" class="ltx_tr">
<th id="S5.T2.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DATER-chatgpt</th>
<td id="S5.T2.1.17.16.2" class="ltx_td ltx_align_center">78.0</td>
</tr>
<tr id="S5.T2.1.18.17" class="ltx_tr">
<th id="S5.T2.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Chain-of-Table</th>
<td id="S5.T2.1.18.17.2" class="ltx_td ltx_align_center">80.2</td>
</tr>
<tr id="S5.T2.1.19.18" class="ltx_tr">
<th id="S5.T2.1.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T2.1.19.18.1.1" class="ltx_sub">col</sub>
</th>
<td id="S5.T2.1.19.18.2" class="ltx_td ltx_align_center">77.0</td>
</tr>
<tr id="S5.T2.1.20.19" class="ltx_tr">
<th id="S5.T2.1.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T2.1.20.19.1.1" class="ltx_sub">row</sub>
</th>
<td id="S5.T2.1.20.19.2" class="ltx_td ltx_align_center">78.5</td>
</tr>
<tr id="S5.T2.1.21.20" class="ltx_tr">
<th id="S5.T2.1.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TabSQLify<sub id="S5.T2.1.21.20.1.1" class="ltx_sub">col+row</sub>
</th>
<td id="S5.T2.1.21.20.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.1.21.20.2.1" class="ltx_text ltx_font_bold">79.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental results on TabFact. Here, “Human” indicates the human performance <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite></figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">For FeTaQA dataset, we achive a performance comparable to the baselines. As ROUGE metrics do not reflect the actual correctness of the model’s responses, we manually evaluated 100 randomly chosen sample and quantified their performance in terms of fluency, correctness, adequacy and faithfulness. The performance is summarized in Tables <a href="#S5.T3" title="Table 3 ‣ 5.1 Model accuracy ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S5.T4" title="Table 4 ‣ 5.1 Model accuracy ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. TabSQLify outperforms models based on fine-tuning and pre-training, such as T5-large. The evaluation suggests that the model’s output closely aligns with average human performance in terms of fluency, adequacy, and faithfulness. The correctness is notably impressive, although it falls behind human-level performance. This indicates that, utilizing TabSQLify results in high accuracy without the need for the entire table, showcasing the model’s high level of precision in retrieving the relevant sub-table.
We additionally assess the results using RAGAS <cite class="ltx_cite ltx_citemacro_cite">Gradients (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>. The evaluation outcomes obtained from RAGAS are detailed in Appendix <a href="#A4" title="Appendix D RAGAS Evaluation for FeTaQA ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">R-1</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">R-2</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">R-L</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">T5-small</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">0.55</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.33</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.47</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5-base</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center">0.61</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center">0.39</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center">0.51</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5-large</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center">0.63</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center">0.41</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center">0.53</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TableCoT-Codex</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">0.62</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.40</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">0.52</td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<th id="S5.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DATER-Codex</th>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_center">0.66</td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_align_center">0.45</td>
<td id="S5.T3.1.6.5.4" class="ltx_td ltx_align_center">0.56</td>
</tr>
<tr id="S5.T3.1.7.6" class="ltx_tr">
<th id="S5.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ReAcTable</th>
<td id="S5.T3.1.7.6.2" class="ltx_td ltx_align_center">0.71</td>
<td id="S5.T3.1.7.6.3" class="ltx_td ltx_align_center">0.46</td>
<td id="S5.T3.1.7.6.4" class="ltx_td ltx_align_center">0.61</td>
</tr>
<tr id="S5.T3.1.8.7" class="ltx_tr">
<th id="S5.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TableCoT-chatgpt</th>
<td id="S5.T3.1.8.7.2" class="ltx_td ltx_align_center">0.62</td>
<td id="S5.T3.1.8.7.3" class="ltx_td ltx_align_center">0.39</td>
<td id="S5.T3.1.8.7.4" class="ltx_td ltx_align_center">0.51</td>
</tr>
<tr id="S5.T3.1.9.8" class="ltx_tr">
<th id="S5.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T3.1.9.8.1.1" class="ltx_sub">col</sub>
</th>
<td id="S5.T3.1.9.8.2" class="ltx_td ltx_align_center">0.57</td>
<td id="S5.T3.1.9.8.3" class="ltx_td ltx_align_center">0.34</td>
<td id="S5.T3.1.9.8.4" class="ltx_td ltx_align_center">0.47</td>
</tr>
<tr id="S5.T3.1.10.9" class="ltx_tr">
<th id="S5.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T3.1.10.9.1.1" class="ltx_sub">row</sub>
</th>
<td id="S5.T3.1.10.9.2" class="ltx_td ltx_align_center">0.60</td>
<td id="S5.T3.1.10.9.3" class="ltx_td ltx_align_center">0.37</td>
<td id="S5.T3.1.10.9.4" class="ltx_td ltx_align_center">0.49</td>
</tr>
<tr id="S5.T3.1.11.10" class="ltx_tr">
<th id="S5.T3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TabSQLify<sub id="S5.T3.1.11.10.1.1" class="ltx_sub">col+row</sub>
</th>
<td id="S5.T3.1.11.10.2" class="ltx_td ltx_align_center ltx_border_b">0.58</td>
<td id="S5.T3.1.11.10.3" class="ltx_td ltx_align_center ltx_border_b">0.35</td>
<td id="S5.T3.1.11.10.4" class="ltx_td ltx_align_center ltx_border_b">0.48</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Experimental results on FeTaQA.</figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:345.7pt;height:95.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.6pt,15.8pt) scale(0.75,0.75) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Model</th>
<td id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Fluency</td>
<td id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Correct</td>
<td id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Adequate</td>
<td id="S5.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Faithful</td>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<th id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">T5-large</th>
<td id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">94.6</td>
<td id="S5.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_tt">54.8</td>
<td id="S5.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">50.4</td>
<td id="S5.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_tt">50.4</td>
</tr>
<tr id="S5.T4.1.1.3.3" class="ltx_tr">
<th id="S5.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Human <cite class="ltx_cite ltx_citemacro_cite">Chen (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S5.T4.1.1.3.3.2" class="ltx_td ltx_align_center">95</td>
<td id="S5.T4.1.1.3.3.3" class="ltx_td ltx_align_center">92.4</td>
<td id="S5.T4.1.1.3.3.4" class="ltx_td ltx_align_center">95.6</td>
<td id="S5.T4.1.1.3.3.5" class="ltx_td ltx_align_center">95.6</td>
</tr>
<tr id="S5.T4.1.1.4.4" class="ltx_tr">
<th id="S5.T4.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TableCoT-chatgpt</th>
<td id="S5.T4.1.1.4.4.2" class="ltx_td ltx_align_center">96</td>
<td id="S5.T4.1.1.4.4.3" class="ltx_td ltx_align_center">82</td>
<td id="S5.T4.1.1.4.4.4" class="ltx_td ltx_align_center">75</td>
<td id="S5.T4.1.1.4.4.5" class="ltx_td ltx_align_center">87</td>
</tr>
<tr id="S5.T4.1.1.5.5" class="ltx_tr">
<th id="S5.T4.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TabSQLify<sub id="S5.T4.1.1.5.5.1.1" class="ltx_sub">col</sub>
</th>
<td id="S5.T4.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">98</td>
<td id="S5.T4.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">83</td>
<td id="S5.T4.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">79</td>
<td id="S5.T4.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">85</td>
</tr>
<tr id="S5.T4.1.1.6.6" class="ltx_tr">
<th id="S5.T4.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T4.1.1.6.6.1.1" class="ltx_sub">row</sub>
</th>
<td id="S5.T4.1.1.6.6.2" class="ltx_td ltx_align_center">96</td>
<td id="S5.T4.1.1.6.6.3" class="ltx_td ltx_align_center">80</td>
<td id="S5.T4.1.1.6.6.4" class="ltx_td ltx_align_center">77</td>
<td id="S5.T4.1.1.6.6.5" class="ltx_td ltx_align_center">89</td>
</tr>
<tr id="S5.T4.1.1.7.7" class="ltx_tr">
<th id="S5.T4.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TabSQLify<sub id="S5.T4.1.1.7.7.1.1" class="ltx_sub">col+row</sub>
</th>
<td id="S5.T4.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b">97</td>
<td id="S5.T4.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b">88</td>
<td id="S5.T4.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b">84</td>
<td id="S5.T4.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b">93</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Human evaluation results on FeTaQA.</figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.2.1" class="ltx_tr">
<th id="S5.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">SEQ2SQL</th>
<td id="S5.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">59.4%</td>
</tr>
<tr id="S5.T5.1.3.2" class="ltx_tr">
<th id="S5.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">StructGPT</th>
<td id="S5.T5.1.3.2.2" class="ltx_td ltx_align_center">65.6%</td>
</tr>
<tr id="S5.T5.1.4.3" class="ltx_tr">
<th id="S5.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RCI <cite class="ltx_cite ltx_citemacro_cite">Glass et al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S5.T5.1.4.3.2" class="ltx_td ltx_align_center">89.8%</td>
</tr>
<tr id="S5.T5.1.5.4" class="ltx_tr">
<th id="S5.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TabSQLify<sub id="S5.T5.1.5.4.1.1" class="ltx_sub">col+row</sub>
</th>
<td id="S5.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.1.5.4.2.1" class="ltx_text ltx_font_bold">76.7%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Experimental results on WikiSQL. RCI is a fine tuning based model, and its results may not be directly comparable due to the model’s high reliance on the training set.</figcaption>
</figure>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">TabSQLify shows an outstanding performance on the WikiSQL dataset, as demonstrated in Table <a href="#S5.T5" title="Table 5 ‣ 5.1 Model accuracy ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This dataset appears to be easier compared to the WikiTQ test dataset, with our approach achieving 76.7% accuracy. In 70% of cases, it can produce the answer in the first step, eliminating the need to pass the sub-table and question for the second step.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Scalability and robustness </h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We assessed the scalability and robustness of our model by imposing a token limit on each table across three datasets: WikiTQ, FeTaQA and TabFact. To accomplish this, we established cutoff thresholds to discard tokens exceeding these limits. Subsequently, we evaluated the model’s performance within these constrained token boundaries. For the WikiTQ dataset, we set the cutoff threshold at 2000, while for both the TabFact and FeTaQA datasets, it was set to 600. Table <a href="#S5.T6" title="Table 6 ‣ 5.2 Scalability and robustness ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes the distribution across different classes, illustrating the categories based on the percentage of discarded table tokens (see Appendix <a href="#A2" title="Appendix B Scalability and Robustness Experiment ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for more detail).</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Cut-off (%)</th>
<th id="S5.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">WikiTQ</th>
<th id="S5.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FeTaQA</th>
<th id="S5.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TabFact</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.1.2.1" class="ltx_tr">
<th id="S5.T6.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">0 - 10%</th>
<td id="S5.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">76</td>
<td id="S5.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">81</td>
<td id="S5.T6.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">91</td>
</tr>
<tr id="S5.T6.1.3.2" class="ltx_tr">
<th id="S5.T6.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">10 - 25%</th>
<td id="S5.T6.1.3.2.2" class="ltx_td ltx_align_center">89</td>
<td id="S5.T6.1.3.2.3" class="ltx_td ltx_align_center">143</td>
<td id="S5.T6.1.3.2.4" class="ltx_td ltx_align_center">141</td>
</tr>
<tr id="S5.T6.1.4.3" class="ltx_tr">
<th id="S5.T6.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">25 - 50%</th>
<td id="S5.T6.1.4.3.2" class="ltx_td ltx_align_center">116</td>
<td id="S5.T6.1.4.3.3" class="ltx_td ltx_align_center">202</td>
<td id="S5.T6.1.4.3.4" class="ltx_td ltx_align_center">260</td>
</tr>
<tr id="S5.T6.1.5.4" class="ltx_tr">
<th id="S5.T6.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">50% +</th>
<td id="S5.T6.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">128</td>
<td id="S5.T6.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b">69</td>
<td id="S5.T6.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b">81</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The distribution of samples across various classes as a function of the percentage cut-off of table tokens.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The evaluation results for the WikiTQ dataset are presented in Table <a href="#S5.T7" title="Table 7 ‣ 5.2 Scalability and robustness ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Our model consistently performs well within the specified token boundary. In contrast, the performance of TableCoT is subpar. We have observed a similar trend in the other two datasets (see Tables <a href="#S5.T8" title="Table 8 ‣ 5.2 Scalability and robustness ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and <a href="#S5.T9" title="Table 9 ‣ 5.2 Scalability and robustness ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<figure id="S5.T7" class="ltx_table">
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Cut-off (%)</th>
<th id="S5.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TableCoT</th>
<th id="S5.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TabSQLify<sub id="S5.T7.1.1.1.3.1" class="ltx_sub">col+row</sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.2.1" class="ltx_tr">
<th id="S5.T7.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">0 - 10%</th>
<td id="S5.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">40.7</td>
<td id="S5.T7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">64.4</td>
</tr>
<tr id="S5.T7.1.3.2" class="ltx_tr">
<th id="S5.T7.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">10 - 25%</th>
<td id="S5.T7.1.3.2.2" class="ltx_td ltx_align_center">49.4</td>
<td id="S5.T7.1.3.2.3" class="ltx_td ltx_align_center">60.6</td>
</tr>
<tr id="S5.T7.1.4.3" class="ltx_tr">
<th id="S5.T7.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">25 - 50%</th>
<td id="S5.T7.1.4.3.2" class="ltx_td ltx_align_center">46.5</td>
<td id="S5.T7.1.4.3.3" class="ltx_td ltx_align_center">66.3</td>
</tr>
<tr id="S5.T7.1.5.4" class="ltx_tr">
<th id="S5.T7.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">50% +</th>
<td id="S5.T7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">33.3</td>
<td id="S5.T7.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b">56.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Performance across different classes based on the percentage cut-off of table tokens in the WikiTQ dataset.</figcaption>
</figure>
<figure id="S5.T8" class="ltx_table">
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T8.1.1.1" class="ltx_tr">
<th id="S5.T8.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Cut-off (%)</th>
<th id="S5.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TableCoT</th>
<th id="S5.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TabSQLify<sub id="S5.T8.1.1.1.3.1" class="ltx_sub">col+row</sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T8.1.2.1" class="ltx_tr">
<th id="S5.T8.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">0 - 10%</th>
<td id="S5.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">76.9</td>
<td id="S5.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">79.1</td>
</tr>
<tr id="S5.T8.1.3.2" class="ltx_tr">
<th id="S5.T8.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">10 - 25%</th>
<td id="S5.T8.1.3.2.2" class="ltx_td ltx_align_center">67.3</td>
<td id="S5.T8.1.3.2.3" class="ltx_td ltx_align_center">80.8</td>
</tr>
<tr id="S5.T8.1.4.3" class="ltx_tr">
<th id="S5.T8.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">25 - 50%</th>
<td id="S5.T8.1.4.3.2" class="ltx_td ltx_align_center">63.0</td>
<td id="S5.T8.1.4.3.3" class="ltx_td ltx_align_center">70.0</td>
</tr>
<tr id="S5.T8.1.5.4" class="ltx_tr">
<th id="S5.T8.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">50% +</th>
<td id="S5.T8.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">55.5</td>
<td id="S5.T8.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b">72.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Performance across different classes based on the percentage cut-off of table tokens in the TabFact dataset</figcaption>
</figure>
<figure id="S5.T9" class="ltx_table">
<table id="S5.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T9.1.1.1" class="ltx_tr">
<th id="S5.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">TableCoT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T9.1.2.1" class="ltx_tr">
<td id="S5.T9.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">Cut-off (%)</td>
<td id="S5.T9.1.2.1.2" class="ltx_td ltx_align_left ltx_border_tt">R-1</td>
<td id="S5.T9.1.2.1.3" class="ltx_td ltx_align_left ltx_border_tt">R-2</td>
<td id="S5.T9.1.2.1.4" class="ltx_td ltx_align_left ltx_border_tt">R-L</td>
</tr>
<tr id="S5.T9.1.3.2" class="ltx_tr">
<td id="S5.T9.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">0 -10%</td>
<td id="S5.T9.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">0.58</td>
<td id="S5.T9.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">0.35</td>
<td id="S5.T9.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">0.45</td>
</tr>
<tr id="S5.T9.1.4.3" class="ltx_tr">
<td id="S5.T9.1.4.3.1" class="ltx_td ltx_align_left">10-25%</td>
<td id="S5.T9.1.4.3.2" class="ltx_td ltx_align_left">0.60</td>
<td id="S5.T9.1.4.3.3" class="ltx_td ltx_align_left">0.37</td>
<td id="S5.T9.1.4.3.4" class="ltx_td ltx_align_left">0.50</td>
</tr>
<tr id="S5.T9.1.5.4" class="ltx_tr">
<td id="S5.T9.1.5.4.1" class="ltx_td ltx_align_left">25-50%</td>
<td id="S5.T9.1.5.4.2" class="ltx_td ltx_align_left">0.53</td>
<td id="S5.T9.1.5.4.3" class="ltx_td ltx_align_left">0.30</td>
<td id="S5.T9.1.5.4.4" class="ltx_td ltx_align_left">0.43</td>
</tr>
<tr id="S5.T9.1.6.5" class="ltx_tr">
<td id="S5.T9.1.6.5.1" class="ltx_td ltx_align_left">50% +</td>
<td id="S5.T9.1.6.5.2" class="ltx_td ltx_align_left">0.49</td>
<td id="S5.T9.1.6.5.3" class="ltx_td ltx_align_left">0.28</td>
<td id="S5.T9.1.6.5.4" class="ltx_td ltx_align_left">0.40</td>
</tr>
<tr id="S5.T9.1.7.6" class="ltx_tr">
<th id="S5.T9.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">TabSQLify<sub id="S5.T9.1.7.6.1.1" class="ltx_sub">col+row</sub>
</th>
</tr>
<tr id="S5.T9.1.8.7" class="ltx_tr">
<td id="S5.T9.1.8.7.1" class="ltx_td ltx_align_left ltx_border_tt">Cut-off (%)</td>
<td id="S5.T9.1.8.7.2" class="ltx_td ltx_align_left ltx_border_tt">R-1</td>
<td id="S5.T9.1.8.7.3" class="ltx_td ltx_align_left ltx_border_tt">R-2</td>
<td id="S5.T9.1.8.7.4" class="ltx_td ltx_align_left ltx_border_tt">R-L</td>
</tr>
<tr id="S5.T9.1.9.8" class="ltx_tr">
<td id="S5.T9.1.9.8.1" class="ltx_td ltx_align_left ltx_border_t">0 -10%</td>
<td id="S5.T9.1.9.8.2" class="ltx_td ltx_align_left ltx_border_t">0.62</td>
<td id="S5.T9.1.9.8.3" class="ltx_td ltx_align_left ltx_border_t">0.39</td>
<td id="S5.T9.1.9.8.4" class="ltx_td ltx_align_left ltx_border_t">0.50</td>
</tr>
<tr id="S5.T9.1.10.9" class="ltx_tr">
<td id="S5.T9.1.10.9.1" class="ltx_td ltx_align_left">10-25%</td>
<td id="S5.T9.1.10.9.2" class="ltx_td ltx_align_left">0.64</td>
<td id="S5.T9.1.10.9.3" class="ltx_td ltx_align_left">0.42</td>
<td id="S5.T9.1.10.9.4" class="ltx_td ltx_align_left">0.53</td>
</tr>
<tr id="S5.T9.1.11.10" class="ltx_tr">
<td id="S5.T9.1.11.10.1" class="ltx_td ltx_align_left">25-50%</td>
<td id="S5.T9.1.11.10.2" class="ltx_td ltx_align_left">0.55</td>
<td id="S5.T9.1.11.10.3" class="ltx_td ltx_align_left">0.32</td>
<td id="S5.T9.1.11.10.4" class="ltx_td ltx_align_left">0.44</td>
</tr>
<tr id="S5.T9.1.12.11" class="ltx_tr">
<td id="S5.T9.1.12.11.1" class="ltx_td ltx_align_left ltx_border_b">50% +</td>
<td id="S5.T9.1.12.11.2" class="ltx_td ltx_align_left ltx_border_b">0.51</td>
<td id="S5.T9.1.12.11.3" class="ltx_td ltx_align_left ltx_border_b">0.31</td>
<td id="S5.T9.1.12.11.4" class="ltx_td ltx_align_left ltx_border_b">0.41</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Performance across different classes based on the percentage cut-off of table tokens in the FeTaQA dataset</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">In the WikiTQ dataset, 128 tables contain &gt;4000 tokens exceeding chatGPT’s maximum token limit (4096 tokens including table and question). Table <a href="#S5.T10" title="Table 10 ‣ 5.2 Scalability and robustness ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> reports the performance on these instances. These results reveal that both BINDER <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> and DATER <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> face challenges when dealing with large tables. Specifically, BINDER-Codex achieves only 29.6% accuracy, while DATER achieves an accuracy of 34.6%. BINDER-chatgpt fails to produce any correct answers for these large tables. On the other hand, Chain-of-Table <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite> achieves an accuracy of 44.8%.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In contrast, our model outperforms these baselines significantly. It is crucial to note that our Table-CoT achieves this accuracy because the answers for questions about those large tables are typically in the upper part, fitting within the LLM’s context boundary. If the answer is elsewhere, all models fail. On the other hand, our model has no issue with the answer’s position in a table, making it scalable for large tables.</p>
</div>
<figure id="S5.T10" class="ltx_table">
<table id="S5.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T10.1.1.1" class="ltx_tr">
<th id="S5.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S5.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Acc (Large)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T10.1.2.1" class="ltx_tr">
<th id="S5.T10.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">BINDER-Codex</th>
<td id="S5.T10.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">29.6</td>
</tr>
<tr id="S5.T10.1.3.2" class="ltx_tr">
<th id="S5.T10.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BINDER-chatgpt</th>
<td id="S5.T10.1.3.2.2" class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr id="S5.T10.1.4.3" class="ltx_tr">
<th id="S5.T10.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DATER-chatgpt</th>
<td id="S5.T10.1.4.3.2" class="ltx_td ltx_align_center">34.6</td>
</tr>
<tr id="S5.T10.1.5.4" class="ltx_tr">
<th id="S5.T10.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Table-CoT-chatgpt</th>
<td id="S5.T10.1.5.4.2" class="ltx_td ltx_align_center">35.1</td>
</tr>
<tr id="S5.T10.1.6.5" class="ltx_tr">
<th id="S5.T10.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Chain-of-Table <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>
</th>
<td id="S5.T10.1.6.5.2" class="ltx_td ltx_align_center">44.8</td>
</tr>
<tr id="S5.T10.1.7.6" class="ltx_tr">
<th id="S5.T10.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T10.1.7.6.1.1" class="ltx_sub">col</sub>
</th>
<td id="S5.T10.1.7.6.2" class="ltx_td ltx_align_center">50.0</td>
</tr>
<tr id="S5.T10.1.8.7" class="ltx_tr">
<th id="S5.T10.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="S5.T10.1.8.7.1.1" class="ltx_sub">row</sub>
</th>
<td id="S5.T10.1.8.7.2" class="ltx_td ltx_align_center"><span id="S5.T10.1.8.7.2.1" class="ltx_text ltx_font_bold">57.0</span></td>
</tr>
<tr id="S5.T10.1.9.8" class="ltx_tr">
<th id="S5.T10.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TabSQLify<sub id="S5.T10.1.9.8.1.1" class="ltx_sub">col+row</sub>
</th>
<td id="S5.T10.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b">52.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Experimental results on Large (&gt;4000 tokens) tables from WikiTQ. As the input tables grow larger, we observe a decline in performance for strong baseline models.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<div id="S5.F5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:327pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.4pt,13.9pt) scale(0.921658962261719,0.921658962261719) ;"><img src="/html/2404.10150/assets/reduction2.png" id="S5.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="651" height="491" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Reduction in table size using our row-col filtering across four datasets, showing a significant reduction of the table size.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Table size reduction</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2 Scalability and robustness ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the average reduction in the number of table cells before and after employing TabSQLify<sub id="S5.SS3.p1.1.1" class="ltx_sub">col+row</sub> across three datasets. This reduction, from 183 to 32 cells in WikiTQ, indicates a substantial decrease in sub-table size while maintaining a strong performance. Likewise, similar trends can be observed in the TabFact, FetaQA and WikiSQL datasets. When utilizing both column and row filters to extract the required subtable, direct answers to questions may be found. Specifically, in the WikiTQ dataset, TabSQLify<sub id="S5.SS3.p1.1.2" class="ltx_sub">col+row</sub> successfully retrieves answers in 58% of cases by executing the generated query, requiring the answer generation step only for the remaining 42% of cases.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Error Analysis</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">An important advantage of TabSQLify is its ability to provide the intermediate stages of reasoning path, including SQL queries and sub-tables. To conduct our error analysis, we randomly selected 100 responses generated by TabSQLify<sub id="S5.SS4.p1.1.1" class="ltx_sub">row+col</sub> from the WikiTQ and TabFact test sets. The identified errors are categorized into incorrect columns, incorrect conditions, incorrect reasoning, and false negatives, as listed in Table <a href="#S5.T11" title="Table 11 ‣ 5.4 Error Analysis ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">In this context, a “missing column” refers to instances where TabSQLify either selects incorrect columns or omits necessary columns to answer the question. “missing rows” denotes situations where the generated SQL query contains an erroneous condition within the WHERE clause. Cases where the extracted sub-table is adequate to answer the question, but the LLM fails to provide a correct response, are labeled as “incorrect reasoning”. Additionally, within the dataset, there are instances where the gold answer is incorrect or misjudged by the evaluator, which we classify as “incorrect annotation”.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">From the table, we observe that out of 100 error cases from WikiTQ, 6% involve the generated SQL query missing columns, while 56% miss required rows. The irregular format of the text in the table is identified as the primary cause. Additionally, in 29% of cases, the reasoning is found to be incorrect, while 9% exhibit incorrect annotation. In the TabFact dataset, 10% of the time, the subtable selection query misses required columns, and in 32% of cases, it misses required rows. The main source of errors is incorrect reasoning, accounting for 50% of cases, while 8% involve incorrect annotations.</p>
</div>
<figure id="S5.T11" class="ltx_table">
<table id="S5.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T11.1.1.1" class="ltx_tr">
<th id="S5.T11.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Error Type</th>
<th id="S5.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">WikiTQ</th>
<th id="S5.T11.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TabFact</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T11.1.2.1" class="ltx_tr">
<td id="S5.T11.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">Missing Columns</td>
<td id="S5.T11.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">6%</td>
<td id="S5.T11.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">10%</td>
</tr>
<tr id="S5.T11.1.3.2" class="ltx_tr">
<td id="S5.T11.1.3.2.1" class="ltx_td ltx_align_left">Missing Rows</td>
<td id="S5.T11.1.3.2.2" class="ltx_td ltx_align_center">56%</td>
<td id="S5.T11.1.3.2.3" class="ltx_td ltx_align_center">32%</td>
</tr>
<tr id="S5.T11.1.4.3" class="ltx_tr">
<td id="S5.T11.1.4.3.1" class="ltx_td ltx_align_left">Incorrect Reasoning</td>
<td id="S5.T11.1.4.3.2" class="ltx_td ltx_align_center">29%</td>
<td id="S5.T11.1.4.3.3" class="ltx_td ltx_align_center">50%</td>
</tr>
<tr id="S5.T11.1.5.4" class="ltx_tr">
<td id="S5.T11.1.5.4.1" class="ltx_td ltx_align_left ltx_border_b">Incorrect Annotation</td>
<td id="S5.T11.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">9%</td>
<td id="S5.T11.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b">8%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Error types of 100 samples from WikiTQ and TabFact of TabSQLify<sub id="S5.T11.3.1" class="ltx_sub">col+row</sub></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our proposed decomposition approach has shown promise across different table reasoning tasks, achieving remarkable performance compared to models that require the use of a full table. Our method is novel in leveraging text-to-SQL generation to decompose tables into smaller and relevant sub-tables tailored for table reasoning tasks. This approach provides a new perspective and direction for table reasoning research, and we hope it will inspire more future work on combining natural language understanding and structured data processing.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Our approach is not without its limitations. While it shows promise in reducing table size and maintaining a strong performance, for large tables, the size of a column can exceed the context window size, and the approach may not be applicable. Also, after our preprocessing, the tables are stored in a relational tables. For less regular tables, more preprocessing may be needed.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We extend our sincere gratitude to all anonymous reviewers for their invaluable feedback, insightful suggestions, and positive remarks about our work. This research has been supported by the Natural Sciences and Engineering Research Council of Canada. Also, Md Mahadi Hasan Nahid was supported by the Alberta Innovates Graduate Student Scholarship.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The datasets utilized in this study are accessible through peer-reviewed articles, as specified in the references. Our source code is made openly available for future research under the MIT License. It’s important to note that since our framework relies on gpt-3.5-turbo, it may inherit ethical concerns associated with gpt models, such as potential responses to toxic content or displaying a biased behavior.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et al. (2019)</span>
<span class="ltx_bibblock">
Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. 2019.

</span>
<span class="ltx_bibblock">Learning to generalize from sparse and underspecified rewards.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 130–140. PMLR.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2005.14165" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang and Fosler-Lussier (2023)</span>
<span class="ltx_bibblock">
Shuaichen Chang and Eric Fosler-Lussier. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.11853" title="" class="ltx_ref ltx_href">How to prompt llms for text-to-sql: A study in zero-shot, single-domain, and cross-domain settings</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2023)</span>
<span class="ltx_bibblock">
Wenhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-eacl.83" title="" class="ltx_ref ltx_href">Large language models are few(1)-shot table reasoners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EACL 2023</em>, pages 1120–1130, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1909.02164" title="" class="ltx_ref ltx_href">Tabfact: A large-scale dataset for table-based fact verification</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2023)</span>
<span class="ltx_bibblock">
Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2210.02875" title="" class="ltx_ref ltx_href">Binding language models in symbolic languages</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass et al. (2021)</span>
<span class="ltx_bibblock">
Michael Glass, Mustafa Canim, Alfio Gliozzo, Saneem Chemmengath, Vishwajeet Kumar, Rishav Chakravarti, Avi Sil, Feifei Pan, Samarth Bharadwaj, and Nicolas Rodolfo Fauceglia. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.96" title="" class="ltx_ref ltx_href">Capturing row and column semantics in transformer based question answering over tables</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 1212–1224, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gradients (2023)</span>
<span class="ltx_bibblock">
Exploding Gradients. 2023.

</span>
<span class="ltx_bibblock">Ragas: Evaluation framework for retrieval augmented generation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/explodinggradients/ragas" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/explodinggradients/ragas</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2022)</span>
<span class="ltx_bibblock">
Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.331" title="" class="ltx_ref ltx_href">PASTA: Table-operations aware fact verification via sentence-table cloze pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 4971–4983, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Eisenschlos. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.398" title="" class="ltx_ref ltx_href">TaPas: Weakly supervised table parsing via pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4320–4333, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3571730" title="" class="ltx_ref ltx_href">Survey of hallucination in natural language generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, 55(12).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.574" title="" class="ltx_ref ltx_href">StructGPT: A general framework for large language model to reason over structured data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 9237–9251, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2023)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2205.11916" title="" class="ltx_ref ltx_href">Large language models are zero-shot reasoners</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.03111" title="" class="ltx_ref ltx_href">Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2007)</span>
<span class="ltx_bibblock">
Yunyao Li, Huahai Yang, and HV Jagadish. 2007.

</span>
<span class="ltx_bibblock">Nalix: A generic natural language search environment for xml data.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on database systems (TODS)</em>, 32(4):30–es.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W04-1013" title="" class="ltx_ref ltx_href">ROUGE: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Text Summarization Branches Out</em>, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adria de Gispert, and Gonzalo Iglesias. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.551" title="" class="ltx_ref ltx_href">An inner table retriever for robust table question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 9909–9926, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022a)</span>
<span class="ltx_bibblock">
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.deelio-1.10" title="" class="ltx_ref ltx_href">What makes good in-context examples for GPT-3?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</em>, pages 100–114, Dublin, Ireland and Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022b)</span>
<span class="ltx_bibblock">
Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2107.07653" title="" class="ltx_ref ltx_href">Tapex: Table pre-training via learning a neural sql executor</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nan et al. (2022)</span>
<span class="ltx_bibblock">
Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00446" title="" class="ltx_ref ltx_href">FeTaQA: Free-form table question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 10:35–49.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al. (2023)</span>
<span class="ltx_bibblock">
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. 2023.

</span>
<span class="ltx_bibblock">Lever: Learning to verify language-to-code generation with execution.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning (ICML’23)</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat and Liang (2015)</span>
<span class="ltx_bibblock">
Panupong Pasupat and Percy Liang. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/v1/P15-1142" title="" class="ltx_ref ltx_href">Compositional semantic parsing on semi-structured tables</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 1470–1480, Beijing, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popescu et al. (2003)</span>
<span class="ltx_bibblock">
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003.

</span>
<span class="ltx_bibblock">Towards a theory of natural language interfaces to databases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th international conference on Intelligent user interfaces</em>, pages 149–157.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pourreza and Rafiei (2023)</span>
<span class="ltx_bibblock">
Mohammadreza Pourreza and Davood Rafiei. 2023.

</span>
<span class="ltx_bibblock">Din-sql: Decomposed in-context learning of text-to-sql with self-correction.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.11015</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1910.10683" title="" class="ltx_ref ltx_href">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajkumar et al. (2022)</span>
<span class="ltx_bibblock">
Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2204.00498" title="" class="ltx_ref ltx_href">Evaluating the text-to-sql capabilities of large language models</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tai et al. (2023)</span>
<span class="ltx_bibblock">
Chang-You Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, and Huan Sun. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.14215" title="" class="ltx_ref ltx_href">Exploring chain-of-thought style prompting for text-to-sql</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Bailin Wang, Ivan Titov, and Mirella Lapata. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1391" title="" class="ltx_ref ltx_href">Learning semantic parsers from denotations with latent structured alignments and abstract programs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3774–3785, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2203.11171" title="" class="ltx_ref ltx_href">Self-consistency improves chain of thought reasoning in language models</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, et al. 2024.

</span>
<span class="ltx_bibblock">Chain-of-table: Evolving tables in the reasoning chain for table understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04398</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2201.11903" title="" class="ltx_ref ltx_href">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3539618.3591708" title="" class="ltx_ref ltx_href">Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, SIGIR ’23, page 174–184, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2020)</span>
<span class="ltx_bibblock">
Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.745" title="" class="ltx_ref ltx_href">TaBERT: Pretraining for joint understanding of textual and tabular data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8413–8426, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2020)</span>
<span class="ltx_bibblock">
Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Caiming Xiong, et al. 2020.

</span>
<span class="ltx_bibblock">Grappa: Grammar-augmented pre-training for table semantic parsing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019a)</span>
<span class="ltx_bibblock">
Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. 2019a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1204" title="" class="ltx_ref ltx_href">CoSQL: A conversational text-to-SQL challenge towards cross-domain natural language interfaces to databases</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1962–1979, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2018)</span>
<span class="ltx_bibblock">
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1425" title="" class="ltx_ref ltx_href">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 3911–3921, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019b)</span>
<span class="ltx_bibblock">
Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev. 2019b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1443" title="" class="ltx_ref ltx_href">SParC: Cross-domain semantic parsing in context</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4511–4523, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi Cao, Fuzheng Zhang, and Zhongyuan Wang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.126" title="" class="ltx_ref ltx_href">Table fact verification with structure-aware transformer</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 1624–1629, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen Deep, and Jignesh M. Patel. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.00815" title="" class="ltx_ref ltx_href">Reactable: Enhancing react for table question answering</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2022)</span>
<span class="ltx_bibblock">
Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, and Dragomir Radev. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.615" title="" class="ltx_ref ltx_href">ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 9006–9018, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2017)</span>
<span class="ltx_bibblock">
Victor Zhong, Caiming Xiong, and Richard Socher. 2017.

</span>
<span class="ltx_bibblock">Seq2sql: Generating structured queries from natural language using reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1709.00103.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2020)</span>
<span class="ltx_bibblock">
Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, and Jian Yin. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.539" title="" class="ltx_ref ltx_href">LogicalFactChecker: Leveraging logical operations for fact checking with graph module network</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 6053–6065, Online. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>LLM Hyper-parameters</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We configured the in-context learning hyperparameters for gpt-3.5-turbo according to the specifications outlined in Table <a href="#A1.T12" title="Table 12 ‣ Appendix A LLM Hyper-parameters ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and Table <a href="#A1.T13" title="Table 13 ‣ Appendix A LLM Hyper-parameters ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure id="A1.T12" class="ltx_table">
<table id="A1.T12.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T12.1.1.1" class="ltx_tr">
<th id="A1.T12.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4">Sub table selection using Text-to-SQL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T12.1.2.1" class="ltx_tr">
<th id="A1.T12.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Parameter</th>
<td id="A1.T12.1.2.1.2" class="ltx_td ltx_align_center">WTQA</td>
<td id="A1.T12.1.2.1.3" class="ltx_td ltx_align_center">FeTaQA</td>
<td id="A1.T12.1.2.1.4" class="ltx_td ltx_align_center">TabFact</td>
</tr>
<tr id="A1.T12.1.3.2" class="ltx_tr">
<th id="A1.T12.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">temperature</th>
<td id="A1.T12.1.3.2.2" class="ltx_td ltx_align_center ltx_border_tt">0.3</td>
<td id="A1.T12.1.3.2.3" class="ltx_td ltx_align_center ltx_border_tt">0.3</td>
<td id="A1.T12.1.3.2.4" class="ltx_td ltx_align_center ltx_border_tt">0.3</td>
</tr>
<tr id="A1.T12.1.4.3" class="ltx_tr">
<th id="A1.T12.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">top_p</th>
<td id="A1.T12.1.4.3.2" class="ltx_td ltx_align_center">1</td>
<td id="A1.T12.1.4.3.3" class="ltx_td ltx_align_center">1</td>
<td id="A1.T12.1.4.3.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A1.T12.1.5.4" class="ltx_tr">
<th id="A1.T12.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">sample_n</th>
<td id="A1.T12.1.5.4.2" class="ltx_td ltx_align_center">1</td>
<td id="A1.T12.1.5.4.3" class="ltx_td ltx_align_center">1</td>
<td id="A1.T12.1.5.4.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A1.T12.1.6.5" class="ltx_tr">
<th id="A1.T12.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">max_tokens</th>
<td id="A1.T12.1.6.5.2" class="ltx_td ltx_align_center">100</td>
<td id="A1.T12.1.6.5.3" class="ltx_td ltx_align_center">100</td>
<td id="A1.T12.1.6.5.4" class="ltx_td ltx_align_center">100</td>
</tr>
<tr id="A1.T12.1.7.6" class="ltx_tr">
<th id="A1.T12.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">num_shots</th>
<td id="A1.T12.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b">10</td>
<td id="A1.T12.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b">6</td>
<td id="A1.T12.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b">8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Our hyper-parameter setting of LLM for selecting required column/row</figcaption>
</figure>
<figure id="A1.T13" class="ltx_table">
<table id="A1.T13.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T13.1.1.1" class="ltx_tr">
<th id="A1.T13.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="4">Answer Generation</th>
</tr>
<tr id="A1.T13.1.2.2" class="ltx_tr">
<th id="A1.T13.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Parameter</th>
<th id="A1.T13.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">WTQA</th>
<th id="A1.T13.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">FeTaQA</th>
<th id="A1.T13.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">TabFact</th>
</tr>
<tr id="A1.T13.1.3.3" class="ltx_tr">
<th id="A1.T13.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">temperature</th>
<th id="A1.T13.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">0.7</th>
<th id="A1.T13.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">0.7</th>
<th id="A1.T13.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">0.6</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T13.1.4.1" class="ltx_tr">
<th id="A1.T13.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">top_p</th>
<td id="A1.T13.1.4.1.2" class="ltx_td ltx_align_center">1</td>
<td id="A1.T13.1.4.1.3" class="ltx_td ltx_align_center">1</td>
<td id="A1.T13.1.4.1.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A1.T13.1.5.2" class="ltx_tr">
<th id="A1.T13.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">sample_n</th>
<td id="A1.T13.1.5.2.2" class="ltx_td ltx_align_center">1</td>
<td id="A1.T13.1.5.2.3" class="ltx_td ltx_align_center">1</td>
<td id="A1.T13.1.5.2.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A1.T13.1.6.3" class="ltx_tr">
<th id="A1.T13.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">max_tokens</th>
<td id="A1.T13.1.6.3.2" class="ltx_td ltx_align_center">200</td>
<td id="A1.T13.1.6.3.3" class="ltx_td ltx_align_center">64</td>
<td id="A1.T13.1.6.3.4" class="ltx_td ltx_align_center">100</td>
</tr>
<tr id="A1.T13.1.7.4" class="ltx_tr">
<th id="A1.T13.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">num_shots</th>
<td id="A1.T13.1.7.4.2" class="ltx_td ltx_align_center ltx_border_b">2</td>
<td id="A1.T13.1.7.4.3" class="ltx_td ltx_align_center ltx_border_b">6</td>
<td id="A1.T13.1.7.4.4" class="ltx_td ltx_align_center ltx_border_b">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>Our hyper-parameters setting of LLM for the answer generation</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Scalability and Robustness Experiment</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">LLMs function within a restricted token boundary, allowing us to provide only a limited number of tokens as a prompt to the LLM. We report the impact of the cutoff threshold where tokens beyond the cutoff points are discarded (<math id="A2.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="A2.p1.1.m1.1a"><mi mathvariant="normal" id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><ci id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">\S</annotation></semantics></math> <a href="#S5.SS2" title="5.2 Scalability and robustness ‣ 5 Results ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>).</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">The cutoff percentage denotes the percentage of tokens that are truncated when the threshold is applied. For example, if a table has 4500 tokens and we set the threshold at 2000, then 2500 tokens of the original table are truncated, and the percentage is 2500/4500 = 55.56%. We separated the number of samples in different cutoff ranges (see table 5) and compared the results of those samples from different cutoff ranges in table 6 and 7.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">For the WikiTQ dataset, we set the threshold at 2000 tokens. In this case, out of 4,344 samples, there are 128 samples where more than 50% of the tokens of the original table are truncated if we want to pass the original table to the LLM with a maximum token boundary of 2000. In our approach, TabSQLify selects the relevant limited subtable from the original table for a given question. This allows us to fit the subtable within the maximum token boundary when passing it to the LLM, resulting in improved performance. The aim of this experiment is to demonstrate that TabSQLify can be useful under limited token (context) boundary conditions.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Comparison with other models</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">In this section, we conduct a comparative analysis of our model against two strong baselines, DATER <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> and BINDER <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>.
DATER utilizes Large Language Models (LLMs) for decomposing both questions and tables. On the other hand, BINDER stands out by offering an Application Programming Interface (API) that extends language model (LM) functionalities to programming languages such as SQL and Python. This extension broadens its grammar coverage, enabling the model to address a more diverse range of questions. However, a drawback is that both DATER and BINDER necessitates sending the entire table to the LLM and face challenges when dealing with large tables. Both DATER and BINDER leverage self-consistency <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> strategies to bolster their performance, ensuring a higher level of consistency in their responses.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">In our experiment we did not consider using self-consistence decoding strategy. Using self-consistency we can push the performance even higher. Our implementation does not require any additional processing on the SQL code, unlike BINDER, which necessitates a complex re-implementation of the SQL executor <cite class="ltx_cite ltx_citemacro_cite">Ni et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. BINDER generates a total of 50 samples for a given table and question in the intermediate stages (Generate Neural-SQL: 50); while DATER generates 100 samples in its intermediate stages (table decomposition: 40; Generate Cloze: 20; Generate SQL: 20; reasoning: 20). In contrast, TabSQLify generates only two samples in total, making it simpler and more cost-effective. We summarize the comparison with DATER and BINDER in Table <a href="#A3.T14" title="Table 14 ‣ Appendix C Comparison with other models ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure id="A3.T14" class="ltx_table">
<table id="A3.T14.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T14.1.1.1" class="ltx_tr">
<th id="A3.T14.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">-</th>
<th id="A3.T14.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DATER</th>
<th id="A3.T14.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BINDER</th>
<th id="A3.T14.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TabSQLify</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T14.1.2.1" class="ltx_tr">
<th id="A3.T14.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"># stage</th>
<td id="A3.T14.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">4</td>
<td id="A3.T14.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">2</td>
<td id="A3.T14.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">2</td>
</tr>
<tr id="A3.T14.1.3.2" class="ltx_tr">
<th id="A3.T14.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Max context size</th>
<td id="A3.T14.1.3.2.2" class="ltx_td ltx_align_center">8000</td>
<td id="A3.T14.1.3.2.3" class="ltx_td ltx_align_center">8000</td>
<td id="A3.T14.1.3.2.4" class="ltx_td ltx_align_center">4096</td>
</tr>
<tr id="A3.T14.1.4.3" class="ltx_tr">
<th id="A3.T14.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"># of generated samples</th>
<td id="A3.T14.1.4.3.2" class="ltx_td ltx_align_center">100</td>
<td id="A3.T14.1.4.3.3" class="ltx_td ltx_align_center">50</td>
<td id="A3.T14.1.4.3.4" class="ltx_td ltx_align_center">2</td>
</tr>
<tr id="A3.T14.1.5.4" class="ltx_tr">
<th id="A3.T14.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">sampling_n</th>
<td id="A3.T14.1.5.4.2" class="ltx_td ltx_align_center">20-50</td>
<td id="A3.T14.1.5.4.3" class="ltx_td ltx_align_center">20</td>
<td id="A3.T14.1.5.4.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A3.T14.1.6.5" class="ltx_tr">
<th id="A3.T14.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Self Consistency</th>
<td id="A3.T14.1.6.5.2" class="ltx_td ltx_align_center">yes</td>
<td id="A3.T14.1.6.5.3" class="ltx_td ltx_align_center">yes</td>
<td id="A3.T14.1.6.5.4" class="ltx_td ltx_align_center">no</td>
</tr>
<tr id="A3.T14.1.7.6" class="ltx_tr">
<th id="A3.T14.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Table required</th>
<td id="A3.T14.1.7.6.2" class="ltx_td ltx_align_center">full</td>
<td id="A3.T14.1.7.6.3" class="ltx_td ltx_align_center">full</td>
<td id="A3.T14.1.7.6.4" class="ltx_td ltx_align_center">partial</td>
</tr>
<tr id="A3.T14.1.8.7" class="ltx_tr">
<th id="A3.T14.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Cost</th>
<td id="A3.T14.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b">high</td>
<td id="A3.T14.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b">high</td>
<td id="A3.T14.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b">low</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 14: </span>Comparison with the other LLM based models. TabSQLify is much simpler than the other approach.</figcaption>
</figure>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">Compared to the other LLM-based approach, our approach has several benefits: (1) Unlike other models our approach do not need to provide the table data to LLM to select the target portion of the table. Instead we utilize text-to-sql capability of LLMs. (2) Our approach requires partial table, not full table. (3) Our model can be applied in tight token boundary (4) Considering only one response our model can achive comparable performance while other top performing model uses more than 20 responses (5) Our approach is less costly and it requires less LLM calls which can be vital factor to reduce the cost.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>RAGAS Evaluation for FeTaQA</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Apart from human evaluation, we analyze 100 sample outputs from the FeTaQA dataset using the RAGAS evaluator <cite class="ltx_cite ltx_citemacro_cite">Gradients (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, a framework specifically designed for evaluating Retrieval Augmented Generation (RAG) pipelines. The RAGAS evaluation results is listed in Table <a href="#A4.T15" title="Table 15 ‣ Appendix D RAGAS Evaluation for FeTaQA ‣ TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. RAGAS assesses several key aspects: (1) Faithfulness: Evaluates the factual consistency of the answer concerning the context based on the question, (2) Context Precision: Measures the relevance of the retrieved context to the question, reflecting the quality of the retrieval pipeline, (3) Answer Relevancy: Assesses the relevance of the answer to the question, and (4) Context Recall: Measures the retriever’s capability to retrieve all essential information required to answer the question. The performance of TabSQLify is comparable to that of Table-CoT-chatgpt, which utilized the full table context. Additionally, the RAGAS evaluation shows a similar trend to our human evaluation.</p>
</div>
<figure id="A4.T15" class="ltx_table">
<div id="A4.T15.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:219.0pt;height:63pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.9pt,13.5pt) scale(0.7,0.7) ;">
<table id="A4.T15.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T15.1.1.1.1" class="ltx_tr">
<th id="A4.T15.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="A4.T15.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Precision</th>
<th id="A4.T15.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Recall</th>
<th id="A4.T15.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Relevancy</th>
<th id="A4.T15.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Faithfulness</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T15.1.1.2.1" class="ltx_tr">
<th id="A4.T15.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">TableCoT-chatgpt</th>
<td id="A4.T15.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">0.44</td>
<td id="A4.T15.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.94</td>
<td id="A4.T15.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.94</td>
<td id="A4.T15.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">0.73</td>
</tr>
<tr id="A4.T15.1.1.3.2" class="ltx_tr">
<th id="A4.T15.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="A4.T15.1.1.3.2.1.1" class="ltx_sub">col</sub>
</th>
<td id="A4.T15.1.1.3.2.2" class="ltx_td ltx_align_center">0.42</td>
<td id="A4.T15.1.1.3.2.3" class="ltx_td ltx_align_center">0.92</td>
<td id="A4.T15.1.1.3.2.4" class="ltx_td ltx_align_center">0.93</td>
<td id="A4.T15.1.1.3.2.5" class="ltx_td ltx_align_center">0.67</td>
</tr>
<tr id="A4.T15.1.1.4.3" class="ltx_tr">
<th id="A4.T15.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TabSQLify<sub id="A4.T15.1.1.4.3.1.1" class="ltx_sub">row</sub>
</th>
<td id="A4.T15.1.1.4.3.2" class="ltx_td ltx_align_center">0.45</td>
<td id="A4.T15.1.1.4.3.3" class="ltx_td ltx_align_center">0.97</td>
<td id="A4.T15.1.1.4.3.4" class="ltx_td ltx_align_center">0.94</td>
<td id="A4.T15.1.1.4.3.5" class="ltx_td ltx_align_center">0.73</td>
</tr>
<tr id="A4.T15.1.1.5.4" class="ltx_tr">
<th id="A4.T15.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TabSQLify<sub id="A4.T15.1.1.5.4.1.1" class="ltx_sub">col+row</sub>
</th>
<td id="A4.T15.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">0.44</td>
<td id="A4.T15.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b">0.94</td>
<td id="A4.T15.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b">0.94</td>
<td id="A4.T15.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b">0.72</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 15: </span>RAGAS evaluation results on FeTaQA.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.10149" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.10150" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.10150">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.10150" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.10151" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 19:52:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
