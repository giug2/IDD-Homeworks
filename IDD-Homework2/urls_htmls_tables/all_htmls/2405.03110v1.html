<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Vector Quantization for Recommender Systems: A Review and Outlook</title>
<!--Generated on Mon May  6 02:03:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="recommender system,  vector quantization,  survey" lang="en" name="keywords"/>
<base href="/html/2405.03110v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S1" title="In Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2" title="In Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Overview of VQ Techniques</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS1" title="In 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Standard Vector Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS2" title="In 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Parallel Vector Quantization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS2.SSS1" title="In 2.2. Parallel Vector Quantization ‣ 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Product Quantization (PQ) <cite class="ltx_cite ltx_citemacro_citep">(Juang and Gray, <span class="ltx_ref">1982</span>; Jegou et al<span class="ltx_text">.</span>, <span class="ltx_ref">2010</span>)</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS2.SSS2" title="In 2.2. Parallel Vector Quantization ‣ 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Optimized Product Quantization (OPQ) <cite class="ltx_cite ltx_citemacro_citep">(Ge et al<span class="ltx_text">.</span>, <span class="ltx_ref">2013</span>)</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS3" title="In 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Sequential Vector Quantization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS3.SSS1" title="In 2.3. Sequential Vector Quantization ‣ 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Residual Quantization (RQ) <cite class="ltx_cite ltx_citemacro_citep">(Juang and Gray, <span class="ltx_ref">1982</span>; Martinez et al<span class="ltx_text">.</span>, <span class="ltx_ref">2014</span>)</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS3.SSS2" title="In 2.3. Sequential Vector Quantization ‣ 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Additive Quantization (AQ) <cite class="ltx_cite ltx_citemacro_citep">(Babenko and Lempitsky, <span class="ltx_ref">2014</span>)</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS4" title="In 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Differentiable Vector Quantization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S3" title="In Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Taxonomies of VQ4Rec</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S3.SS1" title="In 3. Taxonomies of VQ4Rec ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Classification by Training Phase</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S3.SS2" title="In 3. Taxonomies of VQ4Rec ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Classification by Application Scenario</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S3.SS3" title="In 3. Taxonomies of VQ4Rec ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Other Classification Frameworks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S4" title="In Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Efficiency-oriented Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S4.SS1" title="In 4. Efficiency-oriented Approaches ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Space Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S4.SS2" title="In 4. Efficiency-oriented Approaches ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Model Acceleration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S4.SS3" title="In 4. Efficiency-oriented Approaches ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Similarity Search</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S5" title="In Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Quality-oriented Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S5.SS1" title="In 5. Quality-oriented Approaches ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Feature Enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S5.SS2" title="In 5. Quality-oriented Approaches ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Modality Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S5.SS3" title="In 5. Quality-oriented Approaches ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Discrete Tokenization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6" title="In Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6.SS1" title="In 6. Future Directions ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Codebook Collapse Problem</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6.SS2" title="In 6. Future Directions ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Item Discovery</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6.SS3" title="In 6. Future Directions ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>User Tokenization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6.SS4" title="In 6. Future Directions ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Multimodal Generative Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6.SS5" title="In 6. Future Directions ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>RS–LLM Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6.SS6" title="In 6. Future Directions ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Codebook Quality Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S6.SS7" title="In 6. Future Directions ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.7 </span>Efficient Large-scale Recommender Systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S7" title="In Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Vector Quantization for Recommender Systems: 
<br class="ltx_break"/>A Review and Outlook</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qijiong Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">The HK PolyU</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Hong Kong SAR</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:liu@qijiong.work">liu@qijiong.work</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoyu Dong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">The HK PolyU</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Hong Kong SAR</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dongxiaoyu7313@gmail.com">dongxiaoyu7313@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiaren Xiao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">The HK PolyU</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Hong Kong SAR</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jiaren.xiao@polyu.edu.hk">jiaren.xiao@polyu.edu.hk</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nuo Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">The HK PolyU</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Hong Kong SAR</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:napnap.chen@connect.polyu.hk">napnap.chen@connect.polyu.hk</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hengchang Hu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">National University of Singapore</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">Singapore</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:hengchang.hu@u.nus.edu">hengchang.hu@u.nus.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jieming Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Huawei Noah’s Ark Lab</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Shenzhen</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jiemingzhu@ieee.org">jiemingzhu@ieee.org</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenxu Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">Huawei Noah’s Ark Lab</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhuchenxu1@huawei.com">zhuchenxu1@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tetsuya Sakai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id22.1.id1">Waseda University</span><span class="ltx_text ltx_affiliation_city" id="id23.2.id2">Tokyo</span><span class="ltx_text ltx_affiliation_country" id="id24.3.id3">Japan</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tetsuyasakai@acm.org">tetsuyasakai@acm.org</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiao-Ming Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">The HK PolyU</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">Hong Kong SAR</span><span class="ltx_text ltx_affiliation_country" id="id27.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xiao-ming.wu@polyu.edu.hk">xiao-ming.wu@polyu.edu.hk</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id28.id1">Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.</p>
</div>
<div class="ltx_keywords">recommender system, vector quantization, survey
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>General and reference Surveys and overviews</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Vector quantization <cite class="ltx_cite ltx_citemacro_citep">(Buzo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib7" title="">1980</a>; Gray, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib21" title="">1984</a>)</cite> (VQ), a cornerstone technique in signal processing, was originally introduced by Gray and his team <cite class="ltx_cite ltx_citemacro_citep">(Buzo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib7" title="">1980</a>)</cite> in the 1980s to compress data representation while preserving the fidelity of the original signal. The foundational standard VQ technique aims to compress the entire representation space into a compact codebook containing multiple codewords, typically using a single code to approximate each vector. To improve the precision of quantization, advanced methods such as product quantization <cite class="ltx_cite ltx_citemacro_citep">(Sabin and Gray, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib67" title="">1984</a>)</cite> and residual quantization <cite class="ltx_cite ltx_citemacro_citep">(Juang and Gray, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib34" title="">1982</a>; Gray and Neuhoff, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib22" title="">1998</a>; Martinez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib56" title="">2014</a>)</cite> were introduced, representing parallel and sequential approaches, respectively. These VQ techniques have proven to be highly effective in domains including speech <cite class="ltx_cite ltx_citemacro_citep">(Makhoul et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib55" title="">1985</a>; Abe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib2" title="">1990</a>)</cite> and image coding <cite class="ltx_cite ltx_citemacro_citep">(Nasrabadi and King, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib60" title="">1988</a>; Cosman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib15" title="">1993</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="538" id="S1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Interest in VQ4Rec over time. <span class="ltx_ERROR undefined" id="S1.F1.2.1">\faFlag</span> denotes a milestone event or a representative paper.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite its early development, it was not until the late 1990s that VQ found application in the field of information retrieval, particularly in image retrieval <cite class="ltx_cite ltx_citemacro_citep">(Lu and Teng, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib52" title="">1999</a>)</cite>. The progress in applying VQ techniques was slow until 2010 when Jegou and his team <cite class="ltx_cite ltx_citemacro_citep">(Jegou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib30" title="">2010</a>)</cite> demonstrated the effectiveness of parallel quantization for approximate nearest neighbor search. This innovation enables fast similarity computations in high-dimensional data spaces. In the same year, Chen and his team <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib12" title="">2010</a>)</cite> investigated the potential of sequential quantization for similar applications.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="164" id="S1.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Illustration of the three classical VQ techniques. <span class="ltx_ERROR undefined" id="S1.F2.2.1">\faSearch</span> indicates nearest neighbor search.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recommender systems, a prominent application in the field of artificial intelligence and data science, typically build upon advancements in information retrieval and machine learning. The integration of VQ into recommender systems started in 2004, initially applied to music recommendation <cite class="ltx_cite ltx_citemacro_citep">(Huang and Jenor, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib26" title="">2004</a>)</cite>. However, a major turning point occurred 15 years later, sparked by the introduction of VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Van Den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib79" title="">2017</a>)</cite> for image generation, which utilized VQ to discretize image representations. This innovation led to the development of PQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Van Balen and Levy, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib78" title="">2019</a>)</cite>, which brought renewed attention to VQ within the recommendation community. The success of VQ-VAE also catalyzed further advancements in residual quantization, leading to the creation of RQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib40" title="">2022</a>)</cite>, which is now at the heart of the burgeoning field of generative recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite>. Furthermore, the emergence of large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib63" title="">2023</a>; Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib77" title="">2023</a>)</cite> has spurred new applications in the recommendation domain <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib48" title="">2024a</a>)</cite>. However, due to their substantial size and latency during inference, there’s a growing trend in recommender systems to adopt VQ to enhance efficiency.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">1</span></a>, there has been a booming interest in vector quantization for recommender systems (<span class="ltx_text ltx_font_bold" id="S1.p4.1.1">VQ4Rec</span>) over recent years.
This body of research can be roughly categorized into <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.2">efficiency-oriented</span> and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.3">quality-oriented</span>.
The former focuses on optimizing large-scale systems, tackling challenges associated with large models, extensive datasets, and computational demands. In this context, VQ proves to be highly effective, significantly improving performance in crucial areas, including <span class="ltx_text ltx_font_bold" id="S1.p4.1.4">similarity search</span> <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib74" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S1.p4.1.5">space compression</span> <cite class="ltx_cite ltx_citemacro_citep">(Imran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib28" title="">2023</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S1.p4.1.6">model acceleration</span> <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>)</cite>. The latter prioritizes recommendation accuracy, concentrating on the refinement of feature usage. This involves optimizing features, fostering interactions among various modalities, and aligning features to enhance generative recommendation processes. It covers sub-scenarios such as <span class="ltx_text ltx_font_bold" id="S1.p4.1.7">feature enhancement</span> <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib54" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S1.p4.1.8">modality alignment</span> <cite class="ltx_cite ltx_citemacro_citep">(Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib23" title="">2023</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S1.p4.1.9">discrete tokenization</span> <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite>.
Moreover, VQ has shown promise in integrating recommender systems with LLMs <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib94" title="">2023</a>)</cite> to improve recommendation quality.
This is achieved by using VQ to effectively tokenize and structure recommendation-related data, such as information about items or users. For instance, generative retrieval methods <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite> leverage VQ to ensure that the recommendation data is well-aligned with LLMs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Despite the growing interest in VQ4Rec amidst new challenges posed by large language models, multimodal data, and generative AI, no work has yet systematically surveyed the application of VQ in recommender systems. This paper aims to bridge this gap through a comprehensive survey. We provide a thorough analysis of VQ4Rec, exploring its uses, challenges, and future directions in the field. The main contents and contributions of this paper are summarized as follows:
</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present an overview of both classical and modern VQ techniques, encompassing standard VQ, parallel VQ, sequential VQ, and differentiable VQ.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We provide systematic taxonomies of VQ4Rec from various perspectives such as training phase, application scenario, VQ techniques, and quantization target.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We conduct a thorough analysis of the strengths, weaknesses, and limitations of existing VQ4Rec methods, focusing on addressing two main challenges in recommender system: efficiency and quality.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We identify key challenges in VQ4Rec and present promising opportunities that can serve as inspiration for future research in this burgeoning field.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison of the three classical VQ techniques. We use <math alttext="\bar{K}=\frac{1}{M}\sum_{i}K_{i}" class="ltx_Math" display="inline" id="S1.T1.7.m1.1"><semantics id="S1.T1.7.m1.1b"><mrow id="S1.T1.7.m1.1.1" xref="S1.T1.7.m1.1.1.cmml"><mover accent="true" id="S1.T1.7.m1.1.1.2" xref="S1.T1.7.m1.1.1.2.cmml"><mi id="S1.T1.7.m1.1.1.2.2" xref="S1.T1.7.m1.1.1.2.2.cmml">K</mi><mo id="S1.T1.7.m1.1.1.2.1" xref="S1.T1.7.m1.1.1.2.1.cmml">¯</mo></mover><mo id="S1.T1.7.m1.1.1.1" xref="S1.T1.7.m1.1.1.1.cmml">=</mo><mrow id="S1.T1.7.m1.1.1.3" xref="S1.T1.7.m1.1.1.3.cmml"><mfrac id="S1.T1.7.m1.1.1.3.2" xref="S1.T1.7.m1.1.1.3.2.cmml"><mn id="S1.T1.7.m1.1.1.3.2.2" xref="S1.T1.7.m1.1.1.3.2.2.cmml">1</mn><mi id="S1.T1.7.m1.1.1.3.2.3" xref="S1.T1.7.m1.1.1.3.2.3.cmml">M</mi></mfrac><mo id="S1.T1.7.m1.1.1.3.1" xref="S1.T1.7.m1.1.1.3.1.cmml">⁢</mo><mrow id="S1.T1.7.m1.1.1.3.3" xref="S1.T1.7.m1.1.1.3.3.cmml"><msub id="S1.T1.7.m1.1.1.3.3.1" xref="S1.T1.7.m1.1.1.3.3.1.cmml"><mo id="S1.T1.7.m1.1.1.3.3.1.2" xref="S1.T1.7.m1.1.1.3.3.1.2.cmml">∑</mo><mi id="S1.T1.7.m1.1.1.3.3.1.3" xref="S1.T1.7.m1.1.1.3.3.1.3.cmml">i</mi></msub><msub id="S1.T1.7.m1.1.1.3.3.2" xref="S1.T1.7.m1.1.1.3.3.2.cmml"><mi id="S1.T1.7.m1.1.1.3.3.2.2" xref="S1.T1.7.m1.1.1.3.3.2.2.cmml">K</mi><mi id="S1.T1.7.m1.1.1.3.3.2.3" xref="S1.T1.7.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.7.m1.1c"><apply id="S1.T1.7.m1.1.1.cmml" xref="S1.T1.7.m1.1.1"><eq id="S1.T1.7.m1.1.1.1.cmml" xref="S1.T1.7.m1.1.1.1"></eq><apply id="S1.T1.7.m1.1.1.2.cmml" xref="S1.T1.7.m1.1.1.2"><ci id="S1.T1.7.m1.1.1.2.1.cmml" xref="S1.T1.7.m1.1.1.2.1">¯</ci><ci id="S1.T1.7.m1.1.1.2.2.cmml" xref="S1.T1.7.m1.1.1.2.2">𝐾</ci></apply><apply id="S1.T1.7.m1.1.1.3.cmml" xref="S1.T1.7.m1.1.1.3"><times id="S1.T1.7.m1.1.1.3.1.cmml" xref="S1.T1.7.m1.1.1.3.1"></times><apply id="S1.T1.7.m1.1.1.3.2.cmml" xref="S1.T1.7.m1.1.1.3.2"><divide id="S1.T1.7.m1.1.1.3.2.1.cmml" xref="S1.T1.7.m1.1.1.3.2"></divide><cn id="S1.T1.7.m1.1.1.3.2.2.cmml" type="integer" xref="S1.T1.7.m1.1.1.3.2.2">1</cn><ci id="S1.T1.7.m1.1.1.3.2.3.cmml" xref="S1.T1.7.m1.1.1.3.2.3">𝑀</ci></apply><apply id="S1.T1.7.m1.1.1.3.3.cmml" xref="S1.T1.7.m1.1.1.3.3"><apply id="S1.T1.7.m1.1.1.3.3.1.cmml" xref="S1.T1.7.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S1.T1.7.m1.1.1.3.3.1.1.cmml" xref="S1.T1.7.m1.1.1.3.3.1">subscript</csymbol><sum id="S1.T1.7.m1.1.1.3.3.1.2.cmml" xref="S1.T1.7.m1.1.1.3.3.1.2"></sum><ci id="S1.T1.7.m1.1.1.3.3.1.3.cmml" xref="S1.T1.7.m1.1.1.3.3.1.3">𝑖</ci></apply><apply id="S1.T1.7.m1.1.1.3.3.2.cmml" xref="S1.T1.7.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S1.T1.7.m1.1.1.3.3.2.1.cmml" xref="S1.T1.7.m1.1.1.3.3.2">subscript</csymbol><ci id="S1.T1.7.m1.1.1.3.3.2.2.cmml" xref="S1.T1.7.m1.1.1.3.3.2.2">𝐾</ci><ci id="S1.T1.7.m1.1.1.3.3.2.3.cmml" xref="S1.T1.7.m1.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.m1.1d">\bar{K}=\frac{1}{M}\sum_{i}K_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.7.m1.1e">over¯ start_ARG italic_K end_ARG = divide start_ARG 1 end_ARG start_ARG italic_M end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to represent the arithmetic mean of <math alttext="K_{i}" class="ltx_Math" display="inline" id="S1.T1.8.m2.1"><semantics id="S1.T1.8.m2.1b"><msub id="S1.T1.8.m2.1.1" xref="S1.T1.8.m2.1.1.cmml"><mi id="S1.T1.8.m2.1.1.2" xref="S1.T1.8.m2.1.1.2.cmml">K</mi><mi id="S1.T1.8.m2.1.1.3" xref="S1.T1.8.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.T1.8.m2.1c"><apply id="S1.T1.8.m2.1.1.cmml" xref="S1.T1.8.m2.1.1"><csymbol cd="ambiguous" id="S1.T1.8.m2.1.1.1.cmml" xref="S1.T1.8.m2.1.1">subscript</csymbol><ci id="S1.T1.8.m2.1.1.2.cmml" xref="S1.T1.8.m2.1.1.2">𝐾</ci><ci id="S1.T1.8.m2.1.1.3.cmml" xref="S1.T1.8.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.m2.1d">K_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.8.m2.1e">italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\hat{K}=\sqrt[M]{\prod_{i}K_{i}}" class="ltx_Math" display="inline" id="S1.T1.9.m3.1"><semantics id="S1.T1.9.m3.1b"><mrow id="S1.T1.9.m3.1.1" xref="S1.T1.9.m3.1.1.cmml"><mover accent="true" id="S1.T1.9.m3.1.1.2" xref="S1.T1.9.m3.1.1.2.cmml"><mi id="S1.T1.9.m3.1.1.2.2" xref="S1.T1.9.m3.1.1.2.2.cmml">K</mi><mo id="S1.T1.9.m3.1.1.2.1" xref="S1.T1.9.m3.1.1.2.1.cmml">^</mo></mover><mo id="S1.T1.9.m3.1.1.1" xref="S1.T1.9.m3.1.1.1.cmml">=</mo><mroot id="S1.T1.9.m3.1.1.3" xref="S1.T1.9.m3.1.1.3.cmml"><mrow id="S1.T1.9.m3.1.1.3.3" xref="S1.T1.9.m3.1.1.3.3.cmml"><msub id="S1.T1.9.m3.1.1.3.3.1" xref="S1.T1.9.m3.1.1.3.3.1.cmml"><mo id="S1.T1.9.m3.1.1.3.3.1.2" xref="S1.T1.9.m3.1.1.3.3.1.2.cmml">∏</mo><mi id="S1.T1.9.m3.1.1.3.3.1.3" xref="S1.T1.9.m3.1.1.3.3.1.3.cmml">i</mi></msub><msub id="S1.T1.9.m3.1.1.3.3.2" xref="S1.T1.9.m3.1.1.3.3.2.cmml"><mi id="S1.T1.9.m3.1.1.3.3.2.2" xref="S1.T1.9.m3.1.1.3.3.2.2.cmml">K</mi><mi id="S1.T1.9.m3.1.1.3.3.2.3" xref="S1.T1.9.m3.1.1.3.3.2.3.cmml">i</mi></msub></mrow><mi id="S1.T1.9.m3.1.1.3.2" xref="S1.T1.9.m3.1.1.3.2.cmml">M</mi></mroot></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.9.m3.1c"><apply id="S1.T1.9.m3.1.1.cmml" xref="S1.T1.9.m3.1.1"><eq id="S1.T1.9.m3.1.1.1.cmml" xref="S1.T1.9.m3.1.1.1"></eq><apply id="S1.T1.9.m3.1.1.2.cmml" xref="S1.T1.9.m3.1.1.2"><ci id="S1.T1.9.m3.1.1.2.1.cmml" xref="S1.T1.9.m3.1.1.2.1">^</ci><ci id="S1.T1.9.m3.1.1.2.2.cmml" xref="S1.T1.9.m3.1.1.2.2">𝐾</ci></apply><apply id="S1.T1.9.m3.1.1.3.cmml" xref="S1.T1.9.m3.1.1.3"><root id="S1.T1.9.m3.1.1.3a.cmml" xref="S1.T1.9.m3.1.1.3"></root><degree id="S1.T1.9.m3.1.1.3b.cmml" xref="S1.T1.9.m3.1.1.3"><ci id="S1.T1.9.m3.1.1.3.2.cmml" xref="S1.T1.9.m3.1.1.3.2">𝑀</ci></degree><apply id="S1.T1.9.m3.1.1.3.3.cmml" xref="S1.T1.9.m3.1.1.3.3"><apply id="S1.T1.9.m3.1.1.3.3.1.cmml" xref="S1.T1.9.m3.1.1.3.3.1"><csymbol cd="ambiguous" id="S1.T1.9.m3.1.1.3.3.1.1.cmml" xref="S1.T1.9.m3.1.1.3.3.1">subscript</csymbol><csymbol cd="latexml" id="S1.T1.9.m3.1.1.3.3.1.2.cmml" xref="S1.T1.9.m3.1.1.3.3.1.2">product</csymbol><ci id="S1.T1.9.m3.1.1.3.3.1.3.cmml" xref="S1.T1.9.m3.1.1.3.3.1.3">𝑖</ci></apply><apply id="S1.T1.9.m3.1.1.3.3.2.cmml" xref="S1.T1.9.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S1.T1.9.m3.1.1.3.3.2.1.cmml" xref="S1.T1.9.m3.1.1.3.3.2">subscript</csymbol><ci id="S1.T1.9.m3.1.1.3.3.2.2.cmml" xref="S1.T1.9.m3.1.1.3.3.2.2">𝐾</ci><ci id="S1.T1.9.m3.1.1.3.3.2.3.cmml" xref="S1.T1.9.m3.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.m3.1d">\hat{K}=\sqrt[M]{\prod_{i}K_{i}}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.9.m3.1e">over^ start_ARG italic_K end_ARG = nth-root start_ARG italic_M end_ARG start_ARG ∏ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> to represent their geometric mean, where <math alttext="i\in\{1,2,\ldots,M\}" class="ltx_Math" display="inline" id="S1.T1.10.m4.4"><semantics id="S1.T1.10.m4.4b"><mrow id="S1.T1.10.m4.4.5" xref="S1.T1.10.m4.4.5.cmml"><mi id="S1.T1.10.m4.4.5.2" xref="S1.T1.10.m4.4.5.2.cmml">i</mi><mo id="S1.T1.10.m4.4.5.1" xref="S1.T1.10.m4.4.5.1.cmml">∈</mo><mrow id="S1.T1.10.m4.4.5.3.2" xref="S1.T1.10.m4.4.5.3.1.cmml"><mo id="S1.T1.10.m4.4.5.3.2.1" stretchy="false" xref="S1.T1.10.m4.4.5.3.1.cmml">{</mo><mn id="S1.T1.10.m4.1.1" xref="S1.T1.10.m4.1.1.cmml">1</mn><mo id="S1.T1.10.m4.4.5.3.2.2" xref="S1.T1.10.m4.4.5.3.1.cmml">,</mo><mn id="S1.T1.10.m4.2.2" xref="S1.T1.10.m4.2.2.cmml">2</mn><mo id="S1.T1.10.m4.4.5.3.2.3" xref="S1.T1.10.m4.4.5.3.1.cmml">,</mo><mi id="S1.T1.10.m4.3.3" mathvariant="normal" xref="S1.T1.10.m4.3.3.cmml">…</mi><mo id="S1.T1.10.m4.4.5.3.2.4" xref="S1.T1.10.m4.4.5.3.1.cmml">,</mo><mi id="S1.T1.10.m4.4.4" xref="S1.T1.10.m4.4.4.cmml">M</mi><mo id="S1.T1.10.m4.4.5.3.2.5" stretchy="false" xref="S1.T1.10.m4.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.10.m4.4c"><apply id="S1.T1.10.m4.4.5.cmml" xref="S1.T1.10.m4.4.5"><in id="S1.T1.10.m4.4.5.1.cmml" xref="S1.T1.10.m4.4.5.1"></in><ci id="S1.T1.10.m4.4.5.2.cmml" xref="S1.T1.10.m4.4.5.2">𝑖</ci><set id="S1.T1.10.m4.4.5.3.1.cmml" xref="S1.T1.10.m4.4.5.3.2"><cn id="S1.T1.10.m4.1.1.cmml" type="integer" xref="S1.T1.10.m4.1.1">1</cn><cn id="S1.T1.10.m4.2.2.cmml" type="integer" xref="S1.T1.10.m4.2.2">2</cn><ci id="S1.T1.10.m4.3.3.cmml" xref="S1.T1.10.m4.3.3">…</ci><ci id="S1.T1.10.m4.4.4.cmml" xref="S1.T1.10.m4.4.4">𝑀</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.m4.4d">i\in\{1,2,\ldots,M\}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.10.m4.4e">italic_i ∈ { 1 , 2 , … , italic_M }</annotation></semantics></math>. Note that when <math alttext="K_{i}=K" class="ltx_Math" display="inline" id="S1.T1.11.m5.1"><semantics id="S1.T1.11.m5.1b"><mrow id="S1.T1.11.m5.1.1" xref="S1.T1.11.m5.1.1.cmml"><msub id="S1.T1.11.m5.1.1.2" xref="S1.T1.11.m5.1.1.2.cmml"><mi id="S1.T1.11.m5.1.1.2.2" xref="S1.T1.11.m5.1.1.2.2.cmml">K</mi><mi id="S1.T1.11.m5.1.1.2.3" xref="S1.T1.11.m5.1.1.2.3.cmml">i</mi></msub><mo id="S1.T1.11.m5.1.1.1" xref="S1.T1.11.m5.1.1.1.cmml">=</mo><mi id="S1.T1.11.m5.1.1.3" xref="S1.T1.11.m5.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.11.m5.1c"><apply id="S1.T1.11.m5.1.1.cmml" xref="S1.T1.11.m5.1.1"><eq id="S1.T1.11.m5.1.1.1.cmml" xref="S1.T1.11.m5.1.1.1"></eq><apply id="S1.T1.11.m5.1.1.2.cmml" xref="S1.T1.11.m5.1.1.2"><csymbol cd="ambiguous" id="S1.T1.11.m5.1.1.2.1.cmml" xref="S1.T1.11.m5.1.1.2">subscript</csymbol><ci id="S1.T1.11.m5.1.1.2.2.cmml" xref="S1.T1.11.m5.1.1.2.2">𝐾</ci><ci id="S1.T1.11.m5.1.1.2.3.cmml" xref="S1.T1.11.m5.1.1.2.3">𝑖</ci></apply><ci id="S1.T1.11.m5.1.1.3.cmml" xref="S1.T1.11.m5.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.11.m5.1d">K_{i}=K</annotation><annotation encoding="application/x-llamapun" id="S1.T1.11.m5.1e">italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_K</annotation></semantics></math>, <math alttext="\bar{K}=\hat{K}=K" class="ltx_Math" display="inline" id="S1.T1.12.m6.1"><semantics id="S1.T1.12.m6.1b"><mrow id="S1.T1.12.m6.1.1" xref="S1.T1.12.m6.1.1.cmml"><mover accent="true" id="S1.T1.12.m6.1.1.2" xref="S1.T1.12.m6.1.1.2.cmml"><mi id="S1.T1.12.m6.1.1.2.2" xref="S1.T1.12.m6.1.1.2.2.cmml">K</mi><mo id="S1.T1.12.m6.1.1.2.1" xref="S1.T1.12.m6.1.1.2.1.cmml">¯</mo></mover><mo id="S1.T1.12.m6.1.1.3" xref="S1.T1.12.m6.1.1.3.cmml">=</mo><mover accent="true" id="S1.T1.12.m6.1.1.4" xref="S1.T1.12.m6.1.1.4.cmml"><mi id="S1.T1.12.m6.1.1.4.2" xref="S1.T1.12.m6.1.1.4.2.cmml">K</mi><mo id="S1.T1.12.m6.1.1.4.1" xref="S1.T1.12.m6.1.1.4.1.cmml">^</mo></mover><mo id="S1.T1.12.m6.1.1.5" xref="S1.T1.12.m6.1.1.5.cmml">=</mo><mi id="S1.T1.12.m6.1.1.6" xref="S1.T1.12.m6.1.1.6.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.12.m6.1c"><apply id="S1.T1.12.m6.1.1.cmml" xref="S1.T1.12.m6.1.1"><and id="S1.T1.12.m6.1.1a.cmml" xref="S1.T1.12.m6.1.1"></and><apply id="S1.T1.12.m6.1.1b.cmml" xref="S1.T1.12.m6.1.1"><eq id="S1.T1.12.m6.1.1.3.cmml" xref="S1.T1.12.m6.1.1.3"></eq><apply id="S1.T1.12.m6.1.1.2.cmml" xref="S1.T1.12.m6.1.1.2"><ci id="S1.T1.12.m6.1.1.2.1.cmml" xref="S1.T1.12.m6.1.1.2.1">¯</ci><ci id="S1.T1.12.m6.1.1.2.2.cmml" xref="S1.T1.12.m6.1.1.2.2">𝐾</ci></apply><apply id="S1.T1.12.m6.1.1.4.cmml" xref="S1.T1.12.m6.1.1.4"><ci id="S1.T1.12.m6.1.1.4.1.cmml" xref="S1.T1.12.m6.1.1.4.1">^</ci><ci id="S1.T1.12.m6.1.1.4.2.cmml" xref="S1.T1.12.m6.1.1.4.2">𝐾</ci></apply></apply><apply id="S1.T1.12.m6.1.1c.cmml" xref="S1.T1.12.m6.1.1"><eq id="S1.T1.12.m6.1.1.5.cmml" xref="S1.T1.12.m6.1.1.5"></eq><share href="https://arxiv.org/html/2405.03110v1#S1.T1.12.m6.1.1.4.cmml" id="S1.T1.12.m6.1.1d.cmml" xref="S1.T1.12.m6.1.1"></share><ci id="S1.T1.12.m6.1.1.6.cmml" xref="S1.T1.12.m6.1.1.6">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.12.m6.1d">\bar{K}=\hat{K}=K</annotation><annotation encoding="application/x-llamapun" id="S1.T1.12.m6.1e">over¯ start_ARG italic_K end_ARG = over^ start_ARG italic_K end_ARG = italic_K</annotation></semantics></math>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.30">
<tr class="ltx_tr" id="S1.T1.30.19">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S1.T1.30.19.1" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.30.19.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.30.19.2.1">Input Dim</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.30.19.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.30.19.3.1">#Codebooks</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.30.19.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.30.19.4.1">#Codes per Book</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.30.19.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.30.19.5.1">Code Dim</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.30.19.6" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.30.19.6.1">Codebook Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.30.19.7" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.30.19.7.1">Feature Space</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.18.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.18.6.7" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.18.6.7.1">Standard VQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.13.1.1" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="D" class="ltx_Math" display="inline" id="S1.T1.13.1.1.m1.1"><semantics id="S1.T1.13.1.1.m1.1a"><mi id="S1.T1.13.1.1.m1.1.1" xref="S1.T1.13.1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.T1.13.1.1.m1.1b"><ci id="S1.T1.13.1.1.m1.1.1.cmml" xref="S1.T1.13.1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.13.1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.13.1.1.m1.1d">italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.14.2.2" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="1" class="ltx_Math" display="inline" id="S1.T1.14.2.2.m1.1"><semantics id="S1.T1.14.2.2.m1.1a"><mn id="S1.T1.14.2.2.m1.1.1" xref="S1.T1.14.2.2.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.T1.14.2.2.m1.1b"><cn id="S1.T1.14.2.2.m1.1.1.cmml" type="integer" xref="S1.T1.14.2.2.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.14.2.2.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.T1.14.2.2.m1.1d">1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.15.3.3" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="K" class="ltx_Math" display="inline" id="S1.T1.15.3.3.m1.1"><semantics id="S1.T1.15.3.3.m1.1a"><mi id="S1.T1.15.3.3.m1.1.1" xref="S1.T1.15.3.3.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S1.T1.15.3.3.m1.1b"><ci id="S1.T1.15.3.3.m1.1.1.cmml" xref="S1.T1.15.3.3.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.15.3.3.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S1.T1.15.3.3.m1.1d">italic_K</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.16.4.4" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="D" class="ltx_Math" display="inline" id="S1.T1.16.4.4.m1.1"><semantics id="S1.T1.16.4.4.m1.1a"><mi id="S1.T1.16.4.4.m1.1.1" xref="S1.T1.16.4.4.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.T1.16.4.4.m1.1b"><ci id="S1.T1.16.4.4.m1.1.1.cmml" xref="S1.T1.16.4.4.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.16.4.4.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.16.4.4.m1.1d">italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.17.5.5" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="K\cdot D" class="ltx_Math" display="inline" id="S1.T1.17.5.5.m1.1"><semantics id="S1.T1.17.5.5.m1.1a"><mrow id="S1.T1.17.5.5.m1.1.1" xref="S1.T1.17.5.5.m1.1.1.cmml"><mi id="S1.T1.17.5.5.m1.1.1.2" xref="S1.T1.17.5.5.m1.1.1.2.cmml">K</mi><mo id="S1.T1.17.5.5.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S1.T1.17.5.5.m1.1.1.1.cmml">⋅</mo><mi id="S1.T1.17.5.5.m1.1.1.3" xref="S1.T1.17.5.5.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.17.5.5.m1.1b"><apply id="S1.T1.17.5.5.m1.1.1.cmml" xref="S1.T1.17.5.5.m1.1.1"><ci id="S1.T1.17.5.5.m1.1.1.1.cmml" xref="S1.T1.17.5.5.m1.1.1.1">⋅</ci><ci id="S1.T1.17.5.5.m1.1.1.2.cmml" xref="S1.T1.17.5.5.m1.1.1.2">𝐾</ci><ci id="S1.T1.17.5.5.m1.1.1.3.cmml" xref="S1.T1.17.5.5.m1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.17.5.5.m1.1c">K\cdot D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.17.5.5.m1.1d">italic_K ⋅ italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.18.6.6" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="K" class="ltx_Math" display="inline" id="S1.T1.18.6.6.m1.1"><semantics id="S1.T1.18.6.6.m1.1a"><mi id="S1.T1.18.6.6.m1.1.1" xref="S1.T1.18.6.6.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S1.T1.18.6.6.m1.1b"><ci id="S1.T1.18.6.6.m1.1.1.cmml" xref="S1.T1.18.6.6.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.18.6.6.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S1.T1.18.6.6.m1.1d">italic_K</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S1.T1.24.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.24.12.7" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.24.12.7.1">Parallel VQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.19.7.1" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="D" class="ltx_Math" display="inline" id="S1.T1.19.7.1.m1.1"><semantics id="S1.T1.19.7.1.m1.1a"><mi id="S1.T1.19.7.1.m1.1.1" xref="S1.T1.19.7.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.T1.19.7.1.m1.1b"><ci id="S1.T1.19.7.1.m1.1.1.cmml" xref="S1.T1.19.7.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.19.7.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.19.7.1.m1.1d">italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.20.8.2" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="M" class="ltx_Math" display="inline" id="S1.T1.20.8.2.m1.1"><semantics id="S1.T1.20.8.2.m1.1a"><mi id="S1.T1.20.8.2.m1.1.1" xref="S1.T1.20.8.2.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S1.T1.20.8.2.m1.1b"><ci id="S1.T1.20.8.2.m1.1.1.cmml" xref="S1.T1.20.8.2.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.20.8.2.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S1.T1.20.8.2.m1.1d">italic_M</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.21.9.3" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="K_{i}" class="ltx_Math" display="inline" id="S1.T1.21.9.3.m1.1"><semantics id="S1.T1.21.9.3.m1.1a"><msub id="S1.T1.21.9.3.m1.1.1" xref="S1.T1.21.9.3.m1.1.1.cmml"><mi id="S1.T1.21.9.3.m1.1.1.2" xref="S1.T1.21.9.3.m1.1.1.2.cmml">K</mi><mi id="S1.T1.21.9.3.m1.1.1.3" xref="S1.T1.21.9.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.T1.21.9.3.m1.1b"><apply id="S1.T1.21.9.3.m1.1.1.cmml" xref="S1.T1.21.9.3.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.21.9.3.m1.1.1.1.cmml" xref="S1.T1.21.9.3.m1.1.1">subscript</csymbol><ci id="S1.T1.21.9.3.m1.1.1.2.cmml" xref="S1.T1.21.9.3.m1.1.1.2">𝐾</ci><ci id="S1.T1.21.9.3.m1.1.1.3.cmml" xref="S1.T1.21.9.3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.21.9.3.m1.1c">K_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.21.9.3.m1.1d">italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.22.10.4" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="D/M" class="ltx_Math" display="inline" id="S1.T1.22.10.4.m1.1"><semantics id="S1.T1.22.10.4.m1.1a"><mrow id="S1.T1.22.10.4.m1.1.1" xref="S1.T1.22.10.4.m1.1.1.cmml"><mi id="S1.T1.22.10.4.m1.1.1.2" xref="S1.T1.22.10.4.m1.1.1.2.cmml">D</mi><mo id="S1.T1.22.10.4.m1.1.1.1" xref="S1.T1.22.10.4.m1.1.1.1.cmml">/</mo><mi id="S1.T1.22.10.4.m1.1.1.3" xref="S1.T1.22.10.4.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.22.10.4.m1.1b"><apply id="S1.T1.22.10.4.m1.1.1.cmml" xref="S1.T1.22.10.4.m1.1.1"><divide id="S1.T1.22.10.4.m1.1.1.1.cmml" xref="S1.T1.22.10.4.m1.1.1.1"></divide><ci id="S1.T1.22.10.4.m1.1.1.2.cmml" xref="S1.T1.22.10.4.m1.1.1.2">𝐷</ci><ci id="S1.T1.22.10.4.m1.1.1.3.cmml" xref="S1.T1.22.10.4.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.22.10.4.m1.1c">D/M</annotation><annotation encoding="application/x-llamapun" id="S1.T1.22.10.4.m1.1d">italic_D / italic_M</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.23.11.5" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="\bar{K}\cdot D" class="ltx_Math" display="inline" id="S1.T1.23.11.5.m1.1"><semantics id="S1.T1.23.11.5.m1.1a"><mrow id="S1.T1.23.11.5.m1.1.1" xref="S1.T1.23.11.5.m1.1.1.cmml"><mover accent="true" id="S1.T1.23.11.5.m1.1.1.2" xref="S1.T1.23.11.5.m1.1.1.2.cmml"><mi id="S1.T1.23.11.5.m1.1.1.2.2" xref="S1.T1.23.11.5.m1.1.1.2.2.cmml">K</mi><mo id="S1.T1.23.11.5.m1.1.1.2.1" xref="S1.T1.23.11.5.m1.1.1.2.1.cmml">¯</mo></mover><mo id="S1.T1.23.11.5.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S1.T1.23.11.5.m1.1.1.1.cmml">⋅</mo><mi id="S1.T1.23.11.5.m1.1.1.3" xref="S1.T1.23.11.5.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.23.11.5.m1.1b"><apply id="S1.T1.23.11.5.m1.1.1.cmml" xref="S1.T1.23.11.5.m1.1.1"><ci id="S1.T1.23.11.5.m1.1.1.1.cmml" xref="S1.T1.23.11.5.m1.1.1.1">⋅</ci><apply id="S1.T1.23.11.5.m1.1.1.2.cmml" xref="S1.T1.23.11.5.m1.1.1.2"><ci id="S1.T1.23.11.5.m1.1.1.2.1.cmml" xref="S1.T1.23.11.5.m1.1.1.2.1">¯</ci><ci id="S1.T1.23.11.5.m1.1.1.2.2.cmml" xref="S1.T1.23.11.5.m1.1.1.2.2">𝐾</ci></apply><ci id="S1.T1.23.11.5.m1.1.1.3.cmml" xref="S1.T1.23.11.5.m1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.23.11.5.m1.1c">\bar{K}\cdot D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.23.11.5.m1.1d">over¯ start_ARG italic_K end_ARG ⋅ italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.24.12.6" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="\hat{K}^{M}" class="ltx_Math" display="inline" id="S1.T1.24.12.6.m1.1"><semantics id="S1.T1.24.12.6.m1.1a"><msup id="S1.T1.24.12.6.m1.1.1" xref="S1.T1.24.12.6.m1.1.1.cmml"><mover accent="true" id="S1.T1.24.12.6.m1.1.1.2" xref="S1.T1.24.12.6.m1.1.1.2.cmml"><mi id="S1.T1.24.12.6.m1.1.1.2.2" xref="S1.T1.24.12.6.m1.1.1.2.2.cmml">K</mi><mo id="S1.T1.24.12.6.m1.1.1.2.1" xref="S1.T1.24.12.6.m1.1.1.2.1.cmml">^</mo></mover><mi id="S1.T1.24.12.6.m1.1.1.3" xref="S1.T1.24.12.6.m1.1.1.3.cmml">M</mi></msup><annotation-xml encoding="MathML-Content" id="S1.T1.24.12.6.m1.1b"><apply id="S1.T1.24.12.6.m1.1.1.cmml" xref="S1.T1.24.12.6.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.24.12.6.m1.1.1.1.cmml" xref="S1.T1.24.12.6.m1.1.1">superscript</csymbol><apply id="S1.T1.24.12.6.m1.1.1.2.cmml" xref="S1.T1.24.12.6.m1.1.1.2"><ci id="S1.T1.24.12.6.m1.1.1.2.1.cmml" xref="S1.T1.24.12.6.m1.1.1.2.1">^</ci><ci id="S1.T1.24.12.6.m1.1.1.2.2.cmml" xref="S1.T1.24.12.6.m1.1.1.2.2">𝐾</ci></apply><ci id="S1.T1.24.12.6.m1.1.1.3.cmml" xref="S1.T1.24.12.6.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.24.12.6.m1.1c">\hat{K}^{M}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.24.12.6.m1.1d">over^ start_ARG italic_K end_ARG start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S1.T1.30.18">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.30.18.7" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.30.18.7.1">Sequential VQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.25.13.1" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="D" class="ltx_Math" display="inline" id="S1.T1.25.13.1.m1.1"><semantics id="S1.T1.25.13.1.m1.1a"><mi id="S1.T1.25.13.1.m1.1.1" xref="S1.T1.25.13.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.T1.25.13.1.m1.1b"><ci id="S1.T1.25.13.1.m1.1.1.cmml" xref="S1.T1.25.13.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.25.13.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.25.13.1.m1.1d">italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.26.14.2" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="M" class="ltx_Math" display="inline" id="S1.T1.26.14.2.m1.1"><semantics id="S1.T1.26.14.2.m1.1a"><mi id="S1.T1.26.14.2.m1.1.1" xref="S1.T1.26.14.2.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S1.T1.26.14.2.m1.1b"><ci id="S1.T1.26.14.2.m1.1.1.cmml" xref="S1.T1.26.14.2.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.26.14.2.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S1.T1.26.14.2.m1.1d">italic_M</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.27.15.3" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="K_{i}" class="ltx_Math" display="inline" id="S1.T1.27.15.3.m1.1"><semantics id="S1.T1.27.15.3.m1.1a"><msub id="S1.T1.27.15.3.m1.1.1" xref="S1.T1.27.15.3.m1.1.1.cmml"><mi id="S1.T1.27.15.3.m1.1.1.2" xref="S1.T1.27.15.3.m1.1.1.2.cmml">K</mi><mi id="S1.T1.27.15.3.m1.1.1.3" xref="S1.T1.27.15.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.T1.27.15.3.m1.1b"><apply id="S1.T1.27.15.3.m1.1.1.cmml" xref="S1.T1.27.15.3.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.27.15.3.m1.1.1.1.cmml" xref="S1.T1.27.15.3.m1.1.1">subscript</csymbol><ci id="S1.T1.27.15.3.m1.1.1.2.cmml" xref="S1.T1.27.15.3.m1.1.1.2">𝐾</ci><ci id="S1.T1.27.15.3.m1.1.1.3.cmml" xref="S1.T1.27.15.3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.27.15.3.m1.1c">K_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.27.15.3.m1.1d">italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.28.16.4" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="D" class="ltx_Math" display="inline" id="S1.T1.28.16.4.m1.1"><semantics id="S1.T1.28.16.4.m1.1a"><mi id="S1.T1.28.16.4.m1.1.1" xref="S1.T1.28.16.4.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.T1.28.16.4.m1.1b"><ci id="S1.T1.28.16.4.m1.1.1.cmml" xref="S1.T1.28.16.4.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.28.16.4.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.28.16.4.m1.1d">italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.29.17.5" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="M\cdot\bar{K}\cdot D" class="ltx_Math" display="inline" id="S1.T1.29.17.5.m1.1"><semantics id="S1.T1.29.17.5.m1.1a"><mrow id="S1.T1.29.17.5.m1.1.1" xref="S1.T1.29.17.5.m1.1.1.cmml"><mi id="S1.T1.29.17.5.m1.1.1.2" xref="S1.T1.29.17.5.m1.1.1.2.cmml">M</mi><mo id="S1.T1.29.17.5.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S1.T1.29.17.5.m1.1.1.1.cmml">⋅</mo><mover accent="true" id="S1.T1.29.17.5.m1.1.1.3" xref="S1.T1.29.17.5.m1.1.1.3.cmml"><mi id="S1.T1.29.17.5.m1.1.1.3.2" xref="S1.T1.29.17.5.m1.1.1.3.2.cmml">K</mi><mo id="S1.T1.29.17.5.m1.1.1.3.1" xref="S1.T1.29.17.5.m1.1.1.3.1.cmml">¯</mo></mover><mo id="S1.T1.29.17.5.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S1.T1.29.17.5.m1.1.1.1.cmml">⋅</mo><mi id="S1.T1.29.17.5.m1.1.1.4" xref="S1.T1.29.17.5.m1.1.1.4.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.29.17.5.m1.1b"><apply id="S1.T1.29.17.5.m1.1.1.cmml" xref="S1.T1.29.17.5.m1.1.1"><ci id="S1.T1.29.17.5.m1.1.1.1.cmml" xref="S1.T1.29.17.5.m1.1.1.1">⋅</ci><ci id="S1.T1.29.17.5.m1.1.1.2.cmml" xref="S1.T1.29.17.5.m1.1.1.2">𝑀</ci><apply id="S1.T1.29.17.5.m1.1.1.3.cmml" xref="S1.T1.29.17.5.m1.1.1.3"><ci id="S1.T1.29.17.5.m1.1.1.3.1.cmml" xref="S1.T1.29.17.5.m1.1.1.3.1">¯</ci><ci id="S1.T1.29.17.5.m1.1.1.3.2.cmml" xref="S1.T1.29.17.5.m1.1.1.3.2">𝐾</ci></apply><ci id="S1.T1.29.17.5.m1.1.1.4.cmml" xref="S1.T1.29.17.5.m1.1.1.4">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.29.17.5.m1.1c">M\cdot\bar{K}\cdot D</annotation><annotation encoding="application/x-llamapun" id="S1.T1.29.17.5.m1.1d">italic_M ⋅ over¯ start_ARG italic_K end_ARG ⋅ italic_D</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.30.18.6" style="padding-top:1pt;padding-bottom:1pt;"><math alttext="\hat{K}^{M}" class="ltx_Math" display="inline" id="S1.T1.30.18.6.m1.1"><semantics id="S1.T1.30.18.6.m1.1a"><msup id="S1.T1.30.18.6.m1.1.1" xref="S1.T1.30.18.6.m1.1.1.cmml"><mover accent="true" id="S1.T1.30.18.6.m1.1.1.2" xref="S1.T1.30.18.6.m1.1.1.2.cmml"><mi id="S1.T1.30.18.6.m1.1.1.2.2" xref="S1.T1.30.18.6.m1.1.1.2.2.cmml">K</mi><mo id="S1.T1.30.18.6.m1.1.1.2.1" xref="S1.T1.30.18.6.m1.1.1.2.1.cmml">^</mo></mover><mi id="S1.T1.30.18.6.m1.1.1.3" xref="S1.T1.30.18.6.m1.1.1.3.cmml">M</mi></msup><annotation-xml encoding="MathML-Content" id="S1.T1.30.18.6.m1.1b"><apply id="S1.T1.30.18.6.m1.1.1.cmml" xref="S1.T1.30.18.6.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.30.18.6.m1.1.1.1.cmml" xref="S1.T1.30.18.6.m1.1.1">superscript</csymbol><apply id="S1.T1.30.18.6.m1.1.1.2.cmml" xref="S1.T1.30.18.6.m1.1.1.2"><ci id="S1.T1.30.18.6.m1.1.1.2.1.cmml" xref="S1.T1.30.18.6.m1.1.1.2.1">^</ci><ci id="S1.T1.30.18.6.m1.1.1.2.2.cmml" xref="S1.T1.30.18.6.m1.1.1.2.2">𝐾</ci></apply><ci id="S1.T1.30.18.6.m1.1.1.3.cmml" xref="S1.T1.30.18.6.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.30.18.6.m1.1c">\hat{K}^{M}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.30.18.6.m1.1d">over^ start_ARG italic_K end_ARG start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Overview of VQ Techniques </h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">VQ targets at grouping similar vectors into clusters by representing them with a small set of prototype vectors (i.e., codes in the codebook). In this section, we offer a comprehensive summary of classical VQ methods and the modern differentiable VQ technique. The conventional VQ approaches include standard VQ, which uses a single codebook, parallel VQ, which utilizes multiple codebooks simultaneously to represent separate vector subspaces, and sequential VQ, which involves using multiple codebooks in a sequence to refine the quantization.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Standard Vector Quantization</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.6">The standard VQ <cite class="ltx_cite ltx_citemacro_citep">(Buzo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib7" title="">1980</a>; Gray, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib21" title="">1984</a>)</cite> serves as the atomic component for the latter two VQ techniques. Formally, given a set of object vectors <math alttext="\mathbf{E}\in\mathbb{R}^{N\times D}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">𝐄</mi><mo id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3.2" xref="S2.SS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.1.m1.1.1.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3.3.2" xref="S2.SS1.p1.1.m1.1.1.3.3.2.cmml">N</mi><mo id="S2.SS1.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p1.1.m1.1.1.3.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><in id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></in><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">𝐄</ci><apply id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3"><times id="S2.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S2.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3.2">𝑁</ci><ci id="S2.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\mathbf{E}\in\mathbb{R}^{N\times D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">bold_E ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, a function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_f</annotation></semantics></math> (e.g., <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_k</annotation></semantics></math>-means) is required to produce a codebook <math alttext="\mathbf{C}\in\mathbb{R}^{K\times D}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">𝐂</mi><mo id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.4.m4.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.3.2" xref="S2.SS1.p1.4.m4.1.1.3.3.2.cmml">K</mi><mo id="S2.SS1.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p1.4.m4.1.1.3.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><in id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"></in><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">𝐂</ci><apply id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3"><times id="S2.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.1"></times><ci id="S2.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.2">𝐾</ci><ci id="S2.SS1.p1.4.m4.1.1.3.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\mathbf{C}\in\mathbb{R}^{K\times D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">bold_C ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> such that the sum of distances between all vectors in <math alttext="\mathbf{E}" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">𝐄</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝐄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\mathbf{E}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">bold_E</annotation></semantics></math> and their corresponding nearest code vectors in <math alttext="\mathbf{C}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">𝐂</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">𝐂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\mathbf{C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">bold_C</annotation></semantics></math> is minimized, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">2</span></a>(a). We can formally express this using the following equations:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx1.EGx1">
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle f" class="ltx_Math" display="inline" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle f</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">italic_f</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle:\mathbf{E}\rightarrow\mathbf{C}," class="ltx_Math" display="inline" id="S2.E1.m2.1"><semantics id="S2.E1.m2.1a"><mrow id="S2.E1.m2.1.1.1" xref="S2.E1.m2.1.1.1.1.cmml"><mrow id="S2.E1.m2.1.1.1.1" xref="S2.E1.m2.1.1.1.1.cmml"><mi id="S2.E1.m2.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.2.cmml"></mi><mo id="S2.E1.m2.1.1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S2.E1.m2.1.1.1.1.1.cmml">:</mo><mrow id="S2.E1.m2.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.3.cmml"><mi id="S2.E1.m2.1.1.1.1.3.2" xref="S2.E1.m2.1.1.1.1.3.2.cmml">𝐄</mi><mo id="S2.E1.m2.1.1.1.1.3.1" stretchy="false" xref="S2.E1.m2.1.1.1.1.3.1.cmml">→</mo><mi id="S2.E1.m2.1.1.1.1.3.3" xref="S2.E1.m2.1.1.1.1.3.3.cmml">𝐂</mi></mrow></mrow><mo id="S2.E1.m2.1.1.1.2" xref="S2.E1.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m2.1b"><apply id="S2.E1.m2.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1"><ci id="S2.E1.m2.1.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1.1">:</ci><csymbol cd="latexml" id="S2.E1.m2.1.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.1.2">absent</csymbol><apply id="S2.E1.m2.1.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.1.3"><ci id="S2.E1.m2.1.1.1.1.3.1.cmml" xref="S2.E1.m2.1.1.1.1.3.1">→</ci><ci id="S2.E1.m2.1.1.1.1.3.2.cmml" xref="S2.E1.m2.1.1.1.1.3.2">𝐄</ci><ci id="S2.E1.m2.1.1.1.1.3.3.cmml" xref="S2.E1.m2.1.1.1.1.3.3">𝐂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.1c">\displaystyle:\mathbf{E}\rightarrow\mathbf{C},</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m2.1d">: bold_E → bold_C ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\textit{where }\mathbf{C}" class="ltx_Math" display="inline" id="S2.E2.m1.1"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2a.cmml">where </mtext><mo id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml">⁢</mo><mi id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml">𝐂</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><times id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"></times><ci id="S2.E2.m1.1.1.2a.cmml" xref="S2.E2.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2">where </mtext></ci><ci id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3">𝐂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\displaystyle\textit{where }\mathbf{C}</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.1d">where bold_C</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\underset{\mathbf{W}\in\mathbb{R}^{K\times D}}{\operatorname{%
argmin}}\sum_{i=1}^{N}d(\mathbf{e}_{i},\mathbf{w}_{x})," class="ltx_Math" display="inline" id="S2.E2.m2.1"><semantics id="S2.E2.m2.1a"><mrow id="S2.E2.m2.1.1.1" xref="S2.E2.m2.1.1.1.1.cmml"><mrow id="S2.E2.m2.1.1.1.1" xref="S2.E2.m2.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.4" xref="S2.E2.m2.1.1.1.1.4.cmml"></mi><mo id="S2.E2.m2.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.3.cmml">=</mo><mrow id="S2.E2.m2.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.2.cmml"><munder accentunder="true" id="S2.E2.m2.1.1.1.1.2.4" xref="S2.E2.m2.1.1.1.1.2.4.cmml"><mi id="S2.E2.m2.1.1.1.1.2.4.2" xref="S2.E2.m2.1.1.1.1.2.4.2.cmml">argmin</mi><mrow id="S2.E2.m2.1.1.1.1.2.4.1" xref="S2.E2.m2.1.1.1.1.2.4.1.cmml"><mi id="S2.E2.m2.1.1.1.1.2.4.1.2" xref="S2.E2.m2.1.1.1.1.2.4.1.2.cmml">𝐖</mi><mo id="S2.E2.m2.1.1.1.1.2.4.1.1" xref="S2.E2.m2.1.1.1.1.2.4.1.1.cmml">∈</mo><msup id="S2.E2.m2.1.1.1.1.2.4.1.3" xref="S2.E2.m2.1.1.1.1.2.4.1.3.cmml"><mi id="S2.E2.m2.1.1.1.1.2.4.1.3.2" xref="S2.E2.m2.1.1.1.1.2.4.1.3.2.cmml">ℝ</mi><mrow id="S2.E2.m2.1.1.1.1.2.4.1.3.3" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3.cmml"><mi id="S2.E2.m2.1.1.1.1.2.4.1.3.3.2" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3.2.cmml">K</mi><mo id="S2.E2.m2.1.1.1.1.2.4.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3.1.cmml">×</mo><mi id="S2.E2.m2.1.1.1.1.2.4.1.3.3.3" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3.3.cmml">D</mi></mrow></msup></mrow></munder><mo id="S2.E2.m2.1.1.1.1.2.3" lspace="0.167em" xref="S2.E2.m2.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S2.E2.m2.1.1.1.1.2.2" xref="S2.E2.m2.1.1.1.1.2.2.cmml"><mstyle displaystyle="true" id="S2.E2.m2.1.1.1.1.2.2.3" xref="S2.E2.m2.1.1.1.1.2.2.3.cmml"><munderover id="S2.E2.m2.1.1.1.1.2.2.3a" xref="S2.E2.m2.1.1.1.1.2.2.3.cmml"><mo id="S2.E2.m2.1.1.1.1.2.2.3.2.2" movablelimits="false" xref="S2.E2.m2.1.1.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S2.E2.m2.1.1.1.1.2.2.3.2.3" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3.cmml"><mi id="S2.E2.m2.1.1.1.1.2.2.3.2.3.2" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3.2.cmml">i</mi><mo id="S2.E2.m2.1.1.1.1.2.2.3.2.3.1" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S2.E2.m2.1.1.1.1.2.2.3.2.3.3" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m2.1.1.1.1.2.2.3.3" xref="S2.E2.m2.1.1.1.1.2.2.3.3.cmml">N</mi></munderover></mstyle><mrow id="S2.E2.m2.1.1.1.1.2.2.2" xref="S2.E2.m2.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m2.1.1.1.1.2.2.2.4" xref="S2.E2.m2.1.1.1.1.2.2.2.4.cmml">d</mi><mo id="S2.E2.m2.1.1.1.1.2.2.2.3" xref="S2.E2.m2.1.1.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S2.E2.m2.1.1.1.1.2.2.2.2.2" xref="S2.E2.m2.1.1.1.1.2.2.2.2.3.cmml"><mo id="S2.E2.m2.1.1.1.1.2.2.2.2.2.3" stretchy="false" xref="S2.E2.m2.1.1.1.1.2.2.2.2.3.cmml">(</mo><msub id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐞</mi><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E2.m2.1.1.1.1.2.2.2.2.2.4" xref="S2.E2.m2.1.1.1.1.2.2.2.2.3.cmml">,</mo><msub id="S2.E2.m2.1.1.1.1.2.2.2.2.2.2" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.cmml"><mi id="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.2" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.2.cmml">𝐰</mi><mi id="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.3" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.3.cmml">x</mi></msub><mo id="S2.E2.m2.1.1.1.1.2.2.2.2.2.5" stretchy="false" xref="S2.E2.m2.1.1.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E2.m2.1.1.1.2" xref="S2.E2.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m2.1b"><apply id="S2.E2.m2.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1"><eq id="S2.E2.m2.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.3"></eq><csymbol cd="latexml" id="S2.E2.m2.1.1.1.1.4.cmml" xref="S2.E2.m2.1.1.1.1.4">absent</csymbol><apply id="S2.E2.m2.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.2"><times id="S2.E2.m2.1.1.1.1.2.3.cmml" xref="S2.E2.m2.1.1.1.1.2.3"></times><apply id="S2.E2.m2.1.1.1.1.2.4.cmml" xref="S2.E2.m2.1.1.1.1.2.4"><apply id="S2.E2.m2.1.1.1.1.2.4.1.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1"><in id="S2.E2.m2.1.1.1.1.2.4.1.1.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.1"></in><ci id="S2.E2.m2.1.1.1.1.2.4.1.2.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.2">𝐖</ci><apply id="S2.E2.m2.1.1.1.1.2.4.1.3.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.3"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.2.4.1.3.1.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.3">superscript</csymbol><ci id="S2.E2.m2.1.1.1.1.2.4.1.3.2.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.3.2">ℝ</ci><apply id="S2.E2.m2.1.1.1.1.2.4.1.3.3.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3"><times id="S2.E2.m2.1.1.1.1.2.4.1.3.3.1.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3.1"></times><ci id="S2.E2.m2.1.1.1.1.2.4.1.3.3.2.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3.2">𝐾</ci><ci id="S2.E2.m2.1.1.1.1.2.4.1.3.3.3.cmml" xref="S2.E2.m2.1.1.1.1.2.4.1.3.3.3">𝐷</ci></apply></apply></apply><ci id="S2.E2.m2.1.1.1.1.2.4.2.cmml" xref="S2.E2.m2.1.1.1.1.2.4.2">argmin</ci></apply><apply id="S2.E2.m2.1.1.1.1.2.2.cmml" xref="S2.E2.m2.1.1.1.1.2.2"><apply id="S2.E2.m2.1.1.1.1.2.2.3.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.2.2.3.1.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3">superscript</csymbol><apply id="S2.E2.m2.1.1.1.1.2.2.3.2.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.2.2.3.2.1.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3">subscript</csymbol><sum id="S2.E2.m2.1.1.1.1.2.2.3.2.2.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3.2.2"></sum><apply id="S2.E2.m2.1.1.1.1.2.2.3.2.3.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3"><eq id="S2.E2.m2.1.1.1.1.2.2.3.2.3.1.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3.1"></eq><ci id="S2.E2.m2.1.1.1.1.2.2.3.2.3.2.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3.2">𝑖</ci><cn id="S2.E2.m2.1.1.1.1.2.2.3.2.3.3.cmml" type="integer" xref="S2.E2.m2.1.1.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E2.m2.1.1.1.1.2.2.3.3.cmml" xref="S2.E2.m2.1.1.1.1.2.2.3.3">𝑁</ci></apply><apply id="S2.E2.m2.1.1.1.1.2.2.2.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2"><times id="S2.E2.m2.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2.3"></times><ci id="S2.E2.m2.1.1.1.1.2.2.2.4.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2.4">𝑑</ci><interval closure="open" id="S2.E2.m2.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2"><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2">𝐞</ci><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.1.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.2">𝐰</ci><ci id="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S2.E2.m2.1.1.1.1.2.2.2.2.2.2.3">𝑥</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m2.1c">\displaystyle=\underset{\mathbf{W}\in\mathbb{R}^{K\times D}}{\operatorname{%
argmin}}\sum_{i=1}^{N}d(\mathbf{e}_{i},\mathbf{w}_{x}),</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m2.1d">= start_UNDERACCENT bold_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_D end_POSTSUPERSCRIPT end_UNDERACCENT start_ARG roman_argmin end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_d ( bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_w start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\textit{and }x" class="ltx_Math" display="inline" id="S2.E3.m1.1"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2a.cmml">and </mtext><mo id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml">⁢</mo><mi id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><times id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"></times><ci id="S2.E3.m1.1.1.2a.cmml" xref="S2.E3.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2">and </mtext></ci><ci id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\displaystyle\textit{and }x</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.1d">and italic_x</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\underset{j=1,\ldots,K}{\operatorname{argmin}}\,d\left(\mathbf{e%
}_{i},\mathbf{w}_{j}\right)," class="ltx_Math" display="inline" id="S2.E3.m2.4"><semantics id="S2.E3.m2.4a"><mrow id="S2.E3.m2.4.4.1" xref="S2.E3.m2.4.4.1.1.cmml"><mrow id="S2.E3.m2.4.4.1.1" xref="S2.E3.m2.4.4.1.1.cmml"><mi id="S2.E3.m2.4.4.1.1.4" xref="S2.E3.m2.4.4.1.1.4.cmml"></mi><mo id="S2.E3.m2.4.4.1.1.3" xref="S2.E3.m2.4.4.1.1.3.cmml">=</mo><mrow id="S2.E3.m2.4.4.1.1.2" xref="S2.E3.m2.4.4.1.1.2.cmml"><munder accentunder="true" id="S2.E3.m2.3.3" xref="S2.E3.m2.3.3.cmml"><mi id="S2.E3.m2.3.3.4" xref="S2.E3.m2.3.3.4.cmml">argmin</mi><mrow id="S2.E3.m2.3.3.3" xref="S2.E3.m2.3.3.3.cmml"><mi id="S2.E3.m2.3.3.3.5" xref="S2.E3.m2.3.3.3.5.cmml">j</mi><mo id="S2.E3.m2.3.3.3.4" xref="S2.E3.m2.3.3.3.4.cmml">=</mo><mrow id="S2.E3.m2.3.3.3.6.2" xref="S2.E3.m2.3.3.3.6.1.cmml"><mn id="S2.E3.m2.1.1.1.1" xref="S2.E3.m2.1.1.1.1.cmml">1</mn><mo id="S2.E3.m2.3.3.3.6.2.1" xref="S2.E3.m2.3.3.3.6.1.cmml">,</mo><mi id="S2.E3.m2.2.2.2.2" mathvariant="normal" xref="S2.E3.m2.2.2.2.2.cmml">…</mi><mo id="S2.E3.m2.3.3.3.6.2.2" xref="S2.E3.m2.3.3.3.6.1.cmml">,</mo><mi id="S2.E3.m2.3.3.3.3" xref="S2.E3.m2.3.3.3.3.cmml">K</mi></mrow></mrow></munder><mo id="S2.E3.m2.4.4.1.1.2.3" lspace="0.167em" xref="S2.E3.m2.4.4.1.1.2.3.cmml">⁢</mo><mi id="S2.E3.m2.4.4.1.1.2.4" xref="S2.E3.m2.4.4.1.1.2.4.cmml">d</mi><mo id="S2.E3.m2.4.4.1.1.2.3a" xref="S2.E3.m2.4.4.1.1.2.3.cmml">⁢</mo><mrow id="S2.E3.m2.4.4.1.1.2.2.2" xref="S2.E3.m2.4.4.1.1.2.2.3.cmml"><mo id="S2.E3.m2.4.4.1.1.2.2.2.3" xref="S2.E3.m2.4.4.1.1.2.2.3.cmml">(</mo><msub id="S2.E3.m2.4.4.1.1.1.1.1.1" xref="S2.E3.m2.4.4.1.1.1.1.1.1.cmml"><mi id="S2.E3.m2.4.4.1.1.1.1.1.1.2" xref="S2.E3.m2.4.4.1.1.1.1.1.1.2.cmml">𝐞</mi><mi id="S2.E3.m2.4.4.1.1.1.1.1.1.3" xref="S2.E3.m2.4.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m2.4.4.1.1.2.2.2.4" xref="S2.E3.m2.4.4.1.1.2.2.3.cmml">,</mo><msub id="S2.E3.m2.4.4.1.1.2.2.2.2" xref="S2.E3.m2.4.4.1.1.2.2.2.2.cmml"><mi id="S2.E3.m2.4.4.1.1.2.2.2.2.2" xref="S2.E3.m2.4.4.1.1.2.2.2.2.2.cmml">𝐰</mi><mi id="S2.E3.m2.4.4.1.1.2.2.2.2.3" xref="S2.E3.m2.4.4.1.1.2.2.2.2.3.cmml">j</mi></msub><mo id="S2.E3.m2.4.4.1.1.2.2.2.5" xref="S2.E3.m2.4.4.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E3.m2.4.4.1.2" xref="S2.E3.m2.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m2.4b"><apply id="S2.E3.m2.4.4.1.1.cmml" xref="S2.E3.m2.4.4.1"><eq id="S2.E3.m2.4.4.1.1.3.cmml" xref="S2.E3.m2.4.4.1.1.3"></eq><csymbol cd="latexml" id="S2.E3.m2.4.4.1.1.4.cmml" xref="S2.E3.m2.4.4.1.1.4">absent</csymbol><apply id="S2.E3.m2.4.4.1.1.2.cmml" xref="S2.E3.m2.4.4.1.1.2"><times id="S2.E3.m2.4.4.1.1.2.3.cmml" xref="S2.E3.m2.4.4.1.1.2.3"></times><apply id="S2.E3.m2.3.3.cmml" xref="S2.E3.m2.3.3"><apply id="S2.E3.m2.3.3.3.cmml" xref="S2.E3.m2.3.3.3"><eq id="S2.E3.m2.3.3.3.4.cmml" xref="S2.E3.m2.3.3.3.4"></eq><ci id="S2.E3.m2.3.3.3.5.cmml" xref="S2.E3.m2.3.3.3.5">𝑗</ci><list id="S2.E3.m2.3.3.3.6.1.cmml" xref="S2.E3.m2.3.3.3.6.2"><cn id="S2.E3.m2.1.1.1.1.cmml" type="integer" xref="S2.E3.m2.1.1.1.1">1</cn><ci id="S2.E3.m2.2.2.2.2.cmml" xref="S2.E3.m2.2.2.2.2">…</ci><ci id="S2.E3.m2.3.3.3.3.cmml" xref="S2.E3.m2.3.3.3.3">𝐾</ci></list></apply><ci id="S2.E3.m2.3.3.4.cmml" xref="S2.E3.m2.3.3.4">argmin</ci></apply><ci id="S2.E3.m2.4.4.1.1.2.4.cmml" xref="S2.E3.m2.4.4.1.1.2.4">𝑑</ci><interval closure="open" id="S2.E3.m2.4.4.1.1.2.2.3.cmml" xref="S2.E3.m2.4.4.1.1.2.2.2"><apply id="S2.E3.m2.4.4.1.1.1.1.1.1.cmml" xref="S2.E3.m2.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m2.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E3.m2.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m2.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E3.m2.4.4.1.1.1.1.1.1.2">𝐞</ci><ci id="S2.E3.m2.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E3.m2.4.4.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E3.m2.4.4.1.1.2.2.2.2.cmml" xref="S2.E3.m2.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m2.4.4.1.1.2.2.2.2.1.cmml" xref="S2.E3.m2.4.4.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E3.m2.4.4.1.1.2.2.2.2.2.cmml" xref="S2.E3.m2.4.4.1.1.2.2.2.2.2">𝐰</ci><ci id="S2.E3.m2.4.4.1.1.2.2.2.2.3.cmml" xref="S2.E3.m2.4.4.1.1.2.2.2.2.3">𝑗</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m2.4c">\displaystyle=\underset{j=1,\ldots,K}{\operatorname{argmin}}\,d\left(\mathbf{e%
}_{i},\mathbf{w}_{j}\right),</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m2.4d">= start_UNDERACCENT italic_j = 1 , … , italic_K end_UNDERACCENT start_ARG roman_argmin end_ARG italic_d ( bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p1.21">where <math alttext="N" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m1.1"><semantics id="S2.SS1.p1.7.m1.1a"><mi id="S2.SS1.p1.7.m1.1.1" xref="S2.SS1.p1.7.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m1.1b"><ci id="S2.SS1.p1.7.m1.1.1.cmml" xref="S2.SS1.p1.7.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m1.1d">italic_N</annotation></semantics></math> is the number of object vectors and <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m2.1"><semantics id="S2.SS1.p1.8.m2.1a"><mi id="S2.SS1.p1.8.m2.1.1" xref="S2.SS1.p1.8.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m2.1b"><ci id="S2.SS1.p1.8.m2.1.1.cmml" xref="S2.SS1.p1.8.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m2.1d">italic_K</annotation></semantics></math> is the number of code vectors in the codebook (usually <math alttext="N\gg K" class="ltx_Math" display="inline" id="S2.SS1.p1.9.m3.1"><semantics id="S2.SS1.p1.9.m3.1a"><mrow id="S2.SS1.p1.9.m3.1.1" xref="S2.SS1.p1.9.m3.1.1.cmml"><mi id="S2.SS1.p1.9.m3.1.1.2" xref="S2.SS1.p1.9.m3.1.1.2.cmml">N</mi><mo id="S2.SS1.p1.9.m3.1.1.1" xref="S2.SS1.p1.9.m3.1.1.1.cmml">≫</mo><mi id="S2.SS1.p1.9.m3.1.1.3" xref="S2.SS1.p1.9.m3.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m3.1b"><apply id="S2.SS1.p1.9.m3.1.1.cmml" xref="S2.SS1.p1.9.m3.1.1"><csymbol cd="latexml" id="S2.SS1.p1.9.m3.1.1.1.cmml" xref="S2.SS1.p1.9.m3.1.1.1">much-greater-than</csymbol><ci id="S2.SS1.p1.9.m3.1.1.2.cmml" xref="S2.SS1.p1.9.m3.1.1.2">𝑁</ci><ci id="S2.SS1.p1.9.m3.1.1.3.cmml" xref="S2.SS1.p1.9.m3.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m3.1c">N\gg K</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.9.m3.1d">italic_N ≫ italic_K</annotation></semantics></math>), <math alttext="\mathbf{e}_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.10.m4.1"><semantics id="S2.SS1.p1.10.m4.1a"><msub id="S2.SS1.p1.10.m4.1.1" xref="S2.SS1.p1.10.m4.1.1.cmml"><mi id="S2.SS1.p1.10.m4.1.1.2" xref="S2.SS1.p1.10.m4.1.1.2.cmml">𝐞</mi><mi id="S2.SS1.p1.10.m4.1.1.3" xref="S2.SS1.p1.10.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m4.1b"><apply id="S2.SS1.p1.10.m4.1.1.cmml" xref="S2.SS1.p1.10.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m4.1.1.1.cmml" xref="S2.SS1.p1.10.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.10.m4.1.1.2.cmml" xref="S2.SS1.p1.10.m4.1.1.2">𝐞</ci><ci id="S2.SS1.p1.10.m4.1.1.3.cmml" xref="S2.SS1.p1.10.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m4.1c">\mathbf{e}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.10.m4.1d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS1.p1.11.m5.1"><semantics id="S2.SS1.p1.11.m5.1a"><mi id="S2.SS1.p1.11.m5.1.1" xref="S2.SS1.p1.11.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m5.1b"><ci id="S2.SS1.p1.11.m5.1.1.cmml" xref="S2.SS1.p1.11.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.11.m5.1d">italic_i</annotation></semantics></math>-th object vector, <math alttext="D" class="ltx_Math" display="inline" id="S2.SS1.p1.12.m6.1"><semantics id="S2.SS1.p1.12.m6.1a"><mi id="S2.SS1.p1.12.m6.1.1" xref="S2.SS1.p1.12.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m6.1b"><ci id="S2.SS1.p1.12.m6.1.1.cmml" xref="S2.SS1.p1.12.m6.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m6.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.12.m6.1d">italic_D</annotation></semantics></math> is the embedding dimension, <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p1.13.m7.1"><semantics id="S2.SS1.p1.13.m7.1a"><mi id="S2.SS1.p1.13.m7.1.1" xref="S2.SS1.p1.13.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.13.m7.1b"><ci id="S2.SS1.p1.13.m7.1.1.cmml" xref="S2.SS1.p1.13.m7.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.13.m7.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.13.m7.1d">italic_d</annotation></semantics></math> represents the distance function (e.g., Euclidean distance or Manhattan distance), <math alttext="\mathbf{W}" class="ltx_Math" display="inline" id="S2.SS1.p1.14.m8.1"><semantics id="S2.SS1.p1.14.m8.1a"><mi id="S2.SS1.p1.14.m8.1.1" xref="S2.SS1.p1.14.m8.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.14.m8.1b"><ci id="S2.SS1.p1.14.m8.1.1.cmml" xref="S2.SS1.p1.14.m8.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.14.m8.1c">\mathbf{W}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.14.m8.1d">bold_W</annotation></semantics></math> denotes any codebook in the same space as <math alttext="\mathbf{C}" class="ltx_Math" display="inline" id="S2.SS1.p1.15.m9.1"><semantics id="S2.SS1.p1.15.m9.1a"><mi id="S2.SS1.p1.15.m9.1.1" xref="S2.SS1.p1.15.m9.1.1.cmml">𝐂</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.15.m9.1b"><ci id="S2.SS1.p1.15.m9.1.1.cmml" xref="S2.SS1.p1.15.m9.1.1">𝐂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.15.m9.1c">\mathbf{C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.15.m9.1d">bold_C</annotation></semantics></math>, and <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.p1.16.m10.1"><semantics id="S2.SS1.p1.16.m10.1a"><mi id="S2.SS1.p1.16.m10.1.1" xref="S2.SS1.p1.16.m10.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.16.m10.1b"><ci id="S2.SS1.p1.16.m10.1.1.cmml" xref="S2.SS1.p1.16.m10.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.16.m10.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.16.m10.1d">italic_x</annotation></semantics></math> is the index of the code vector closest to <math alttext="\mathbf{e}_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.17.m11.1"><semantics id="S2.SS1.p1.17.m11.1a"><msub id="S2.SS1.p1.17.m11.1.1" xref="S2.SS1.p1.17.m11.1.1.cmml"><mi id="S2.SS1.p1.17.m11.1.1.2" xref="S2.SS1.p1.17.m11.1.1.2.cmml">𝐞</mi><mi id="S2.SS1.p1.17.m11.1.1.3" xref="S2.SS1.p1.17.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.17.m11.1b"><apply id="S2.SS1.p1.17.m11.1.1.cmml" xref="S2.SS1.p1.17.m11.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.17.m11.1.1.1.cmml" xref="S2.SS1.p1.17.m11.1.1">subscript</csymbol><ci id="S2.SS1.p1.17.m11.1.1.2.cmml" xref="S2.SS1.p1.17.m11.1.1.2">𝐞</ci><ci id="S2.SS1.p1.17.m11.1.1.3.cmml" xref="S2.SS1.p1.17.m11.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.17.m11.1c">\mathbf{e}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.17.m11.1d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Therefore, we can use <math alttext="\mathbf{c}_{x}" class="ltx_Math" display="inline" id="S2.SS1.p1.18.m12.1"><semantics id="S2.SS1.p1.18.m12.1a"><msub id="S2.SS1.p1.18.m12.1.1" xref="S2.SS1.p1.18.m12.1.1.cmml"><mi id="S2.SS1.p1.18.m12.1.1.2" xref="S2.SS1.p1.18.m12.1.1.2.cmml">𝐜</mi><mi id="S2.SS1.p1.18.m12.1.1.3" xref="S2.SS1.p1.18.m12.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.18.m12.1b"><apply id="S2.SS1.p1.18.m12.1.1.cmml" xref="S2.SS1.p1.18.m12.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.18.m12.1.1.1.cmml" xref="S2.SS1.p1.18.m12.1.1">subscript</csymbol><ci id="S2.SS1.p1.18.m12.1.1.2.cmml" xref="S2.SS1.p1.18.m12.1.1.2">𝐜</ci><ci id="S2.SS1.p1.18.m12.1.1.3.cmml" xref="S2.SS1.p1.18.m12.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.18.m12.1c">\mathbf{c}_{x}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.18.m12.1d">bold_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT</annotation></semantics></math>, the <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.p1.19.m13.1"><semantics id="S2.SS1.p1.19.m13.1a"><mi id="S2.SS1.p1.19.m13.1.1" xref="S2.SS1.p1.19.m13.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.19.m13.1b"><ci id="S2.SS1.p1.19.m13.1.1.cmml" xref="S2.SS1.p1.19.m13.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.19.m13.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.19.m13.1d">italic_x</annotation></semantics></math>-th code in codebook <math alttext="\mathbf{C}" class="ltx_Math" display="inline" id="S2.SS1.p1.20.m14.1"><semantics id="S2.SS1.p1.20.m14.1a"><mi id="S2.SS1.p1.20.m14.1.1" xref="S2.SS1.p1.20.m14.1.1.cmml">𝐂</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.20.m14.1b"><ci id="S2.SS1.p1.20.m14.1.1.cmml" xref="S2.SS1.p1.20.m14.1.1">𝐂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.20.m14.1c">\mathbf{C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.20.m14.1d">bold_C</annotation></semantics></math>, to approximate <math alttext="\mathbf{e}_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.21.m15.1"><semantics id="S2.SS1.p1.21.m15.1a"><msub id="S2.SS1.p1.21.m15.1.1" xref="S2.SS1.p1.21.m15.1.1.cmml"><mi id="S2.SS1.p1.21.m15.1.1.2" xref="S2.SS1.p1.21.m15.1.1.2.cmml">𝐞</mi><mi id="S2.SS1.p1.21.m15.1.1.3" xref="S2.SS1.p1.21.m15.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.21.m15.1b"><apply id="S2.SS1.p1.21.m15.1.1.cmml" xref="S2.SS1.p1.21.m15.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.21.m15.1.1.1.cmml" xref="S2.SS1.p1.21.m15.1.1">subscript</csymbol><ci id="S2.SS1.p1.21.m15.1.1.2.cmml" xref="S2.SS1.p1.21.m15.1.1.2">𝐞</ci><ci id="S2.SS1.p1.21.m15.1.1.3.cmml" xref="S2.SS1.p1.21.m15.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.21.m15.1c">\mathbf{e}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.21.m15.1d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{e}_{i}\approx\mathbf{c}_{x}." class="ltx_Math" display="block" id="S2.E4.m1.1"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.cmml"><mi id="S2.E4.m1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.2.2.cmml">𝐞</mi><mi id="S2.E4.m1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E4.m1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.cmml">≈</mo><msub id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.3.2.cmml">𝐜</mi><mi id="S2.E4.m1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="S2.E4.m1.1.1.1.2" lspace="0em" xref="S2.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><approx id="S2.E4.m1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1"></approx><apply id="S2.E4.m1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.2">𝐞</ci><ci id="S2.E4.m1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.3.2">𝐜</ci><ci id="S2.E4.m1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.3.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\mathbf{e}_{i}\approx\mathbf{c}_{x}.</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.1d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≈ bold_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Parallel Vector Quantization</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.2">As the embedding dimension <math alttext="D" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_D</annotation></semantics></math> increases, standard VQ methods face significant challenges in terms of storage requirements, computational efficiency, and quantization quality. In response to these challenges, approaches like product quantization and optimized product quantization, representative of parallel quantization techniques, emerge as effective solutions.
These methods segment high-dimensional vectors into multiple lower-dimensional sub-vectors and perform quantization on each segment independently. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">1</span></a>, with an increase in the number of segments (<math alttext="M" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_M</annotation></semantics></math>), there is a corresponding reduction in the dimensionality of each code, keeping the codebook storage size unchanged. Yet, the representation space exhibits an exponential growth compared to that of standard VQ.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Product Quantization (PQ) <cite class="ltx_cite ltx_citemacro_citep">(Juang and Gray, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib34" title="">1982</a>; Jegou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib30" title="">2010</a>)</cite>
</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.6">Product Quantization (PQ) represents an initial approach to parallel quantization, where original high-dimensional vectors are segmented into uniformly-sized sub-vectors. This process can be mathematically represented as
<math alttext="\mathbf{E}=\left[\mathbf{E}^{(1)},\mathbf{E}^{(2)},\cdots,\mathbf{E}^{(M)}\right]" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.1.m1.7"><semantics id="S2.SS2.SSS1.p1.1.m1.7a"><mrow id="S2.SS2.SSS1.p1.1.m1.7.7" xref="S2.SS2.SSS1.p1.1.m1.7.7.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.7.7.5" xref="S2.SS2.SSS1.p1.1.m1.7.7.5.cmml">𝐄</mi><mo id="S2.SS2.SSS1.p1.1.m1.7.7.4" xref="S2.SS2.SSS1.p1.1.m1.7.7.4.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.1.m1.7.7.3.3" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.4.cmml"><mo id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.4" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.4.cmml">[</mo><msup id="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.2.cmml">𝐄</mi><mrow id="S2.SS2.SSS1.p1.1.m1.1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.cmml"><mo id="S2.SS2.SSS1.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.cmml">(</mo><mn id="S2.SS2.SSS1.p1.1.m1.1.1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.cmml">1</mn><mo id="S2.SS2.SSS1.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.cmml">)</mo></mrow></msup><mo id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.5" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.4.cmml">,</mo><msup id="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.2" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.2.cmml">𝐄</mi><mrow id="S2.SS2.SSS1.p1.1.m1.2.2.1.3" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.cmml"><mo id="S2.SS2.SSS1.p1.1.m1.2.2.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.cmml">(</mo><mn id="S2.SS2.SSS1.p1.1.m1.2.2.1.1" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.cmml">2</mn><mo id="S2.SS2.SSS1.p1.1.m1.2.2.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.cmml">)</mo></mrow></msup><mo id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.6" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.4.cmml">,</mo><mi id="S2.SS2.SSS1.p1.1.m1.4.4" mathvariant="normal" xref="S2.SS2.SSS1.p1.1.m1.4.4.cmml">⋯</mi><mo id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.7" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.4.cmml">,</mo><msup id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.2" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.2.cmml">𝐄</mi><mrow id="S2.SS2.SSS1.p1.1.m1.3.3.1.3" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.cmml"><mo id="S2.SS2.SSS1.p1.1.m1.3.3.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.cmml">(</mo><mi id="S2.SS2.SSS1.p1.1.m1.3.3.1.1" xref="S2.SS2.SSS1.p1.1.m1.3.3.1.1.cmml">M</mi><mo id="S2.SS2.SSS1.p1.1.m1.3.3.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.cmml">)</mo></mrow></msup><mo id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.8" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.7b"><apply id="S2.SS2.SSS1.p1.1.m1.7.7.cmml" xref="S2.SS2.SSS1.p1.1.m1.7.7"><eq id="S2.SS2.SSS1.p1.1.m1.7.7.4.cmml" xref="S2.SS2.SSS1.p1.1.m1.7.7.4"></eq><ci id="S2.SS2.SSS1.p1.1.m1.7.7.5.cmml" xref="S2.SS2.SSS1.p1.1.m1.7.7.5">𝐄</ci><list id="S2.SS2.SSS1.p1.1.m1.7.7.3.4.cmml" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3"><apply id="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1">superscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.1.1.1.2">𝐄</ci><cn id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.cmml" type="integer" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1">1</cn></apply><apply id="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2">superscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.6.6.2.2.2.2">𝐄</ci><cn id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.cmml" type="integer" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1">2</cn></apply><ci id="S2.SS2.SSS1.p1.1.m1.4.4.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4">⋯</ci><apply id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3">superscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.7.7.3.3.3.2">𝐄</ci><ci id="S2.SS2.SSS1.p1.1.m1.3.3.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.3.1.1">𝑀</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.7c">\mathbf{E}=\left[\mathbf{E}^{(1)},\mathbf{E}^{(2)},\cdots,\mathbf{E}^{(M)}\right]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.1.m1.7d">bold_E = [ bold_E start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , bold_E start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT , ⋯ , bold_E start_POSTSUPERSCRIPT ( italic_M ) end_POSTSUPERSCRIPT ]</annotation></semantics></math>,
where <math alttext="M" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.2.m2.1"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mi id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.2.m2.1d">italic_M</annotation></semantics></math> denotes the number of the segments and the number of the codebooks, and <math alttext="\mathbf{E}^{(i)}\in\mathrm{R}^{N\times\frac{D}{M}}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.3.m3.1"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mrow id="S2.SS2.SSS1.p1.3.m3.1.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.cmml"><msup id="S2.SS2.SSS1.p1.3.m3.1.2.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.2.cmml"><mi id="S2.SS2.SSS1.p1.3.m3.1.2.2.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.2.2.cmml">𝐄</mi><mrow id="S2.SS2.SSS1.p1.3.m3.1.1.1.3" xref="S2.SS2.SSS1.p1.3.m3.1.2.2.cmml"><mo id="S2.SS2.SSS1.p1.3.m3.1.1.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.3.m3.1.2.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.1.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1.1.1.cmml">i</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.1.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.3.m3.1.2.2.cmml">)</mo></mrow></msup><mo id="S2.SS2.SSS1.p1.3.m3.1.2.1" xref="S2.SS2.SSS1.p1.3.m3.1.2.1.cmml">∈</mo><msup id="S2.SS2.SSS1.p1.3.m3.1.2.3" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.cmml"><mi id="S2.SS2.SSS1.p1.3.m3.1.2.3.2" mathvariant="normal" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.2.cmml">R</mi><mrow id="S2.SS2.SSS1.p1.3.m3.1.2.3.3" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.cmml"><mi id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.2.cmml">N</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.1.cmml">×</mo><mfrac id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.cmml"><mi id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.2.cmml">D</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.3" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.3.cmml">M</mi></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.1b"><apply id="S2.SS2.SSS1.p1.3.m3.1.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2"><in id="S2.SS2.SSS1.p1.3.m3.1.2.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.1"></in><apply id="S2.SS2.SSS1.p1.3.m3.1.2.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.3.m3.1.2.2.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.2">superscript</csymbol><ci id="S2.SS2.SSS1.p1.3.m3.1.2.2.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.2.2">𝐄</ci><ci id="S2.SS2.SSS1.p1.3.m3.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1.1.1">𝑖</ci></apply><apply id="S2.SS2.SSS1.p1.3.m3.1.2.3.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.3.m3.1.2.3.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3">superscript</csymbol><ci id="S2.SS2.SSS1.p1.3.m3.1.2.3.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.2">R</ci><apply id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3"><times id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.1"></times><ci id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.2">𝑁</ci><apply id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3"><divide id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3"></divide><ci id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.2">𝐷</ci><ci id="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.3.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.3.3.3.3">𝑀</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">\mathbf{E}^{(i)}\in\mathrm{R}^{N\times\frac{D}{M}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.3.m3.1d">bold_E start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ∈ roman_R start_POSTSUPERSCRIPT italic_N × divide start_ARG italic_D end_ARG start_ARG italic_M end_ARG end_POSTSUPERSCRIPT</annotation></semantics></math>. Each sub-vector is then independently subjected to standard VQ, utilizing a distinct codebook for each segment.
Therefore, the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.4.m4.1"><semantics id="S2.SS2.SSS1.p1.4.m4.1a"><mi id="S2.SS2.SSS1.p1.4.m4.1.1" xref="S2.SS2.SSS1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.4.m4.1b"><ci id="S2.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.4.m4.1d">italic_i</annotation></semantics></math>-th original vector can be approximated by selecting and concatenating each single code vector <math alttext="\mathbf{c}^{(j)}_{x_{j}}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.5.m5.1"><semantics id="S2.SS2.SSS1.p1.5.m5.1a"><msubsup id="S2.SS2.SSS1.p1.5.m5.1.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.cmml"><mi id="S2.SS2.SSS1.p1.5.m5.1.2.2.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.2.2.cmml">𝐜</mi><msub id="S2.SS2.SSS1.p1.5.m5.1.2.3" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.cmml"><mi id="S2.SS2.SSS1.p1.5.m5.1.2.3.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.2.cmml">x</mi><mi id="S2.SS2.SSS1.p1.5.m5.1.2.3.3" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3.cmml">j</mi></msub><mrow id="S2.SS2.SSS1.p1.5.m5.1.1.1.3" xref="S2.SS2.SSS1.p1.5.m5.1.2.cmml"><mo id="S2.SS2.SSS1.p1.5.m5.1.1.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.5.m5.1.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.5.m5.1.1.1.1" xref="S2.SS2.SSS1.p1.5.m5.1.1.1.1.cmml">j</mi><mo id="S2.SS2.SSS1.p1.5.m5.1.1.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.5.m5.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.5.m5.1b"><apply id="S2.SS2.SSS1.p1.5.m5.1.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.5.m5.1.2.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2">subscript</csymbol><apply id="S2.SS2.SSS1.p1.5.m5.1.2.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.5.m5.1.2.2.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2">superscript</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.1.2.2.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.2.2">𝐜</ci><ci id="S2.SS2.SSS1.p1.5.m5.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.1.1.1">𝑗</ci></apply><apply id="S2.SS2.SSS1.p1.5.m5.1.2.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.5.m5.1.2.3.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3">subscript</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.1.2.3.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.2">𝑥</ci><ci id="S2.SS2.SSS1.p1.5.m5.1.2.3.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.5.m5.1c">\mathbf{c}^{(j)}_{x_{j}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.5.m5.1d">bold_c start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> from each sub-codebook <math alttext="\mathbf{C}^{(j)}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.6.m6.1"><semantics id="S2.SS2.SSS1.p1.6.m6.1a"><msup id="S2.SS2.SSS1.p1.6.m6.1.2" xref="S2.SS2.SSS1.p1.6.m6.1.2.cmml"><mi id="S2.SS2.SSS1.p1.6.m6.1.2.2" xref="S2.SS2.SSS1.p1.6.m6.1.2.2.cmml">𝐂</mi><mrow id="S2.SS2.SSS1.p1.6.m6.1.1.1.3" xref="S2.SS2.SSS1.p1.6.m6.1.2.cmml"><mo id="S2.SS2.SSS1.p1.6.m6.1.1.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.6.m6.1.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.6.m6.1.1.1.1" xref="S2.SS2.SSS1.p1.6.m6.1.1.1.1.cmml">j</mi><mo id="S2.SS2.SSS1.p1.6.m6.1.1.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.6.m6.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.6.m6.1b"><apply id="S2.SS2.SSS1.p1.6.m6.1.2.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.6.m6.1.2.1.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.2">superscript</csymbol><ci id="S2.SS2.SSS1.p1.6.m6.1.2.2.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.2.2">𝐂</ci><ci id="S2.SS2.SSS1.p1.6.m6.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.6.m6.1c">\mathbf{C}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.6.m6.1d">bold_C start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math>, which can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{e}_{i}=\left[\cdots,\mathbf{e}_{i}^{(j)},\cdots\right]\approx\left[%
\cdots,\mathbf{c}^{(j)}_{x_{j}},\cdots,\right]\quad\text{for }j\in\{1,2,\ldots%
,M\}," class="ltx_math_unparsed" display="block" id="S2.E5.m1.6"><semantics id="S2.E5.m1.6a"><mrow id="S2.E5.m1.6b"><msub id="S2.E5.m1.6.7"><mi id="S2.E5.m1.6.7.2">𝐞</mi><mi id="S2.E5.m1.6.7.3">i</mi></msub><mo id="S2.E5.m1.6.8">=</mo><mrow id="S2.E5.m1.6.9"><mo id="S2.E5.m1.6.9.1">[</mo><mi id="S2.E5.m1.3.3" mathvariant="normal">⋯</mi><mo id="S2.E5.m1.6.9.2">,</mo><msubsup id="S2.E5.m1.6.9.3"><mi id="S2.E5.m1.6.9.3.2.2">𝐞</mi><mi id="S2.E5.m1.6.9.3.2.3">i</mi><mrow id="S2.E5.m1.1.1.1.3"><mo id="S2.E5.m1.1.1.1.3.1" stretchy="false">(</mo><mi id="S2.E5.m1.1.1.1.1">j</mi><mo id="S2.E5.m1.1.1.1.3.2" stretchy="false">)</mo></mrow></msubsup><mo id="S2.E5.m1.6.9.4">,</mo><mi id="S2.E5.m1.4.4" mathvariant="normal">⋯</mi><mo id="S2.E5.m1.6.9.5">]</mo></mrow><mo id="S2.E5.m1.6.10">≈</mo><mrow id="S2.E5.m1.6.11"><mo id="S2.E5.m1.6.11.1">[</mo><mi id="S2.E5.m1.5.5" mathvariant="normal">⋯</mi><mo id="S2.E5.m1.6.11.2">,</mo><msubsup id="S2.E5.m1.6.11.3"><mi id="S2.E5.m1.6.11.3.2.2">𝐜</mi><msub id="S2.E5.m1.6.11.3.3"><mi id="S2.E5.m1.6.11.3.3.2">x</mi><mi id="S2.E5.m1.6.11.3.3.3">j</mi></msub><mrow id="S2.E5.m1.2.2.1.3"><mo id="S2.E5.m1.2.2.1.3.1" stretchy="false">(</mo><mi id="S2.E5.m1.2.2.1.1">j</mi><mo id="S2.E5.m1.2.2.1.3.2" stretchy="false">)</mo></mrow></msubsup><mo id="S2.E5.m1.6.11.4">,</mo><mi id="S2.E5.m1.6.6" mathvariant="normal">⋯</mi><mo id="S2.E5.m1.6.11.5">,</mo><mo id="S2.E5.m1.6.11.6">]</mo></mrow><mspace id="S2.E5.m1.6.12" width="1em"></mspace><mtext id="S2.E5.m1.6.13">for </mtext><mi id="S2.E5.m1.6.14">j</mi><mo id="S2.E5.m1.6.15">∈</mo><mrow id="S2.E5.m1.6.16"><mo id="S2.E5.m1.6.16.1" stretchy="false">{</mo><mn id="S2.E5.m1.6.16.2">1</mn><mo id="S2.E5.m1.6.16.3">,</mo><mn id="S2.E5.m1.6.16.4">2</mn><mo id="S2.E5.m1.6.16.5">,</mo><mi id="S2.E5.m1.6.16.6" mathvariant="normal">…</mi><mo id="S2.E5.m1.6.16.7">,</mo><mi id="S2.E5.m1.6.16.8">M</mi><mo id="S2.E5.m1.6.16.9" stretchy="false">}</mo></mrow><mo id="S2.E5.m1.6.17">,</mo></mrow><annotation encoding="application/x-tex" id="S2.E5.m1.6c">\mathbf{e}_{i}=\left[\cdots,\mathbf{e}_{i}^{(j)},\cdots\right]\approx\left[%
\cdots,\mathbf{c}^{(j)}_{x_{j}},\cdots,\right]\quad\text{for }j\in\{1,2,\ldots%
,M\},</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.6d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ ⋯ , bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT , ⋯ ] ≈ [ ⋯ , bold_c start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT , ⋯ , ] for italic_j ∈ { 1 , 2 , … , italic_M } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.SSS1.p1.12">where <math alttext="\mathbf{C}^{(j)}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.7.m1.1"><semantics id="S2.SS2.SSS1.p1.7.m1.1a"><msup id="S2.SS2.SSS1.p1.7.m1.1.2" xref="S2.SS2.SSS1.p1.7.m1.1.2.cmml"><mi id="S2.SS2.SSS1.p1.7.m1.1.2.2" xref="S2.SS2.SSS1.p1.7.m1.1.2.2.cmml">𝐂</mi><mrow id="S2.SS2.SSS1.p1.7.m1.1.1.1.3" xref="S2.SS2.SSS1.p1.7.m1.1.2.cmml"><mo id="S2.SS2.SSS1.p1.7.m1.1.1.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.7.m1.1.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.7.m1.1.1.1.1" xref="S2.SS2.SSS1.p1.7.m1.1.1.1.1.cmml">j</mi><mo id="S2.SS2.SSS1.p1.7.m1.1.1.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.7.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.7.m1.1b"><apply id="S2.SS2.SSS1.p1.7.m1.1.2.cmml" xref="S2.SS2.SSS1.p1.7.m1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m1.1.2.1.cmml" xref="S2.SS2.SSS1.p1.7.m1.1.2">superscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m1.1.2.2.cmml" xref="S2.SS2.SSS1.p1.7.m1.1.2.2">𝐂</ci><ci id="S2.SS2.SSS1.p1.7.m1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m1.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.7.m1.1c">\mathbf{C}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.7.m1.1d">bold_C start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math> is the <math alttext="j" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.8.m2.1"><semantics id="S2.SS2.SSS1.p1.8.m2.1a"><mi id="S2.SS2.SSS1.p1.8.m2.1.1" xref="S2.SS2.SSS1.p1.8.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.8.m2.1b"><ci id="S2.SS2.SSS1.p1.8.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.8.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.8.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.8.m2.1d">italic_j</annotation></semantics></math>-th codebook with size <math alttext="K_{j}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.9.m3.1"><semantics id="S2.SS2.SSS1.p1.9.m3.1a"><msub id="S2.SS2.SSS1.p1.9.m3.1.1" xref="S2.SS2.SSS1.p1.9.m3.1.1.cmml"><mi id="S2.SS2.SSS1.p1.9.m3.1.1.2" xref="S2.SS2.SSS1.p1.9.m3.1.1.2.cmml">K</mi><mi id="S2.SS2.SSS1.p1.9.m3.1.1.3" xref="S2.SS2.SSS1.p1.9.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.9.m3.1b"><apply id="S2.SS2.SSS1.p1.9.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.9.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.9.m3.1.1.1.cmml" xref="S2.SS2.SSS1.p1.9.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.9.m3.1.1.2.cmml" xref="S2.SS2.SSS1.p1.9.m3.1.1.2">𝐾</ci><ci id="S2.SS2.SSS1.p1.9.m3.1.1.3.cmml" xref="S2.SS2.SSS1.p1.9.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.9.m3.1c">K_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.9.m3.1d">italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="x_{j}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.10.m4.1"><semantics id="S2.SS2.SSS1.p1.10.m4.1a"><msub id="S2.SS2.SSS1.p1.10.m4.1.1" xref="S2.SS2.SSS1.p1.10.m4.1.1.cmml"><mi id="S2.SS2.SSS1.p1.10.m4.1.1.2" xref="S2.SS2.SSS1.p1.10.m4.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS1.p1.10.m4.1.1.3" xref="S2.SS2.SSS1.p1.10.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.10.m4.1b"><apply id="S2.SS2.SSS1.p1.10.m4.1.1.cmml" xref="S2.SS2.SSS1.p1.10.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.10.m4.1.1.1.cmml" xref="S2.SS2.SSS1.p1.10.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.10.m4.1.1.2.cmml" xref="S2.SS2.SSS1.p1.10.m4.1.1.2">𝑥</ci><ci id="S2.SS2.SSS1.p1.10.m4.1.1.3.cmml" xref="S2.SS2.SSS1.p1.10.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.10.m4.1c">x_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.10.m4.1d">italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the index of the code vector in <math alttext="\mathbf{C}^{(j)}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.11.m5.1"><semantics id="S2.SS2.SSS1.p1.11.m5.1a"><msup id="S2.SS2.SSS1.p1.11.m5.1.2" xref="S2.SS2.SSS1.p1.11.m5.1.2.cmml"><mi id="S2.SS2.SSS1.p1.11.m5.1.2.2" xref="S2.SS2.SSS1.p1.11.m5.1.2.2.cmml">𝐂</mi><mrow id="S2.SS2.SSS1.p1.11.m5.1.1.1.3" xref="S2.SS2.SSS1.p1.11.m5.1.2.cmml"><mo id="S2.SS2.SSS1.p1.11.m5.1.1.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.11.m5.1.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.11.m5.1.1.1.1" xref="S2.SS2.SSS1.p1.11.m5.1.1.1.1.cmml">j</mi><mo id="S2.SS2.SSS1.p1.11.m5.1.1.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.11.m5.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.11.m5.1b"><apply id="S2.SS2.SSS1.p1.11.m5.1.2.cmml" xref="S2.SS2.SSS1.p1.11.m5.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.11.m5.1.2.1.cmml" xref="S2.SS2.SSS1.p1.11.m5.1.2">superscript</csymbol><ci id="S2.SS2.SSS1.p1.11.m5.1.2.2.cmml" xref="S2.SS2.SSS1.p1.11.m5.1.2.2">𝐂</ci><ci id="S2.SS2.SSS1.p1.11.m5.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.11.m5.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.11.m5.1c">\mathbf{C}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.11.m5.1d">bold_C start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math> closest to <math alttext="\mathbf{e}_{i}^{(j)}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.12.m6.1"><semantics id="S2.SS2.SSS1.p1.12.m6.1a"><msubsup id="S2.SS2.SSS1.p1.12.m6.1.2" xref="S2.SS2.SSS1.p1.12.m6.1.2.cmml"><mi id="S2.SS2.SSS1.p1.12.m6.1.2.2.2" xref="S2.SS2.SSS1.p1.12.m6.1.2.2.2.cmml">𝐞</mi><mi id="S2.SS2.SSS1.p1.12.m6.1.2.2.3" xref="S2.SS2.SSS1.p1.12.m6.1.2.2.3.cmml">i</mi><mrow id="S2.SS2.SSS1.p1.12.m6.1.1.1.3" xref="S2.SS2.SSS1.p1.12.m6.1.2.cmml"><mo id="S2.SS2.SSS1.p1.12.m6.1.1.1.3.1" stretchy="false" xref="S2.SS2.SSS1.p1.12.m6.1.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.12.m6.1.1.1.1" xref="S2.SS2.SSS1.p1.12.m6.1.1.1.1.cmml">j</mi><mo id="S2.SS2.SSS1.p1.12.m6.1.1.1.3.2" stretchy="false" xref="S2.SS2.SSS1.p1.12.m6.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.12.m6.1b"><apply id="S2.SS2.SSS1.p1.12.m6.1.2.cmml" xref="S2.SS2.SSS1.p1.12.m6.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.12.m6.1.2.1.cmml" xref="S2.SS2.SSS1.p1.12.m6.1.2">superscript</csymbol><apply id="S2.SS2.SSS1.p1.12.m6.1.2.2.cmml" xref="S2.SS2.SSS1.p1.12.m6.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.12.m6.1.2.2.1.cmml" xref="S2.SS2.SSS1.p1.12.m6.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.12.m6.1.2.2.2.cmml" xref="S2.SS2.SSS1.p1.12.m6.1.2.2.2">𝐞</ci><ci id="S2.SS2.SSS1.p1.12.m6.1.2.2.3.cmml" xref="S2.SS2.SSS1.p1.12.m6.1.2.2.3">𝑖</ci></apply><ci id="S2.SS2.SSS1.p1.12.m6.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.12.m6.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.12.m6.1c">\mathbf{e}_{i}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.12.m6.1d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math>.
Due to its storage efficiency and capability for fast approximate nearest neighbor searches, product quantization has become a popular solution in the information retrieval domain, particularly for image retrieval tasks, as evidenced by several studies <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib8" title="">2017</a>; Jang and Cho, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib29" title="">2021</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib9" title="">2022</a>)</cite>. Nonetheless, it overlooks the potential for significant inter-correlations among sub-vectors, which may affect the quantization performance and subsequent downstream tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Optimized Product Quantization (OPQ) <cite class="ltx_cite ltx_citemacro_citep">(Ge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib19" title="">2013</a>)</cite>
</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">To eliminate the interdependence among multiple subspaces, optimized product quantization is introduced and uses the learnable rotation matrix <math alttext="\mathbf{R}\in\mathbb{R}^{D\times D}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.1.m1.1"><semantics id="S2.SS2.SSS2.p1.1.m1.1a"><mrow id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml">𝐑</mi><mo id="S2.SS2.SSS2.p1.1.m1.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS2.SSS2.p1.1.m1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.1.1.3.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS2.SSS2.p1.1.m1.1.1.3.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3.2.cmml">D</mi><mo id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.1b"><apply id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1"><in id="S2.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.1"></in><ci id="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2">𝐑</ci><apply id="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3"><times id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3.1"></times><ci id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3.2">𝐷</ci><ci id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.1c">\mathbf{R}\in\mathbb{R}^{D\times D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.1.m1.1d">bold_R ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> for automatically selecting the most effective orientation of the data in the high-dimensional space. Such rotation minimizes the interdependence among different subspaces, allowing for a more efficient and independent quantization process, which can be defined as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx1.EGx2">
<tbody id="S2.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{E}^{\prime}" class="ltx_Math" display="inline" id="S2.E6.m1.1"><semantics id="S2.E6.m1.1a"><msup id="S2.E6.m1.1.1" xref="S2.E6.m1.1.1.cmml"><mi id="S2.E6.m1.1.1.2" xref="S2.E6.m1.1.1.2.cmml">𝐄</mi><mo id="S2.E6.m1.1.1.3" xref="S2.E6.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.E6.m1.1b"><apply id="S2.E6.m1.1.1.cmml" xref="S2.E6.m1.1.1"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.cmml" xref="S2.E6.m1.1.1">superscript</csymbol><ci id="S2.E6.m1.1.1.2.cmml" xref="S2.E6.m1.1.1.2">𝐄</ci><ci id="S2.E6.m1.1.1.3.cmml" xref="S2.E6.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m1.1c">\displaystyle\mathbf{E}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.E6.m1.1d">bold_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathbf{E}\times\mathbf{R}," class="ltx_Math" display="inline" id="S2.E6.m2.1"><semantics id="S2.E6.m2.1a"><mrow id="S2.E6.m2.1.1.1" xref="S2.E6.m2.1.1.1.1.cmml"><mrow id="S2.E6.m2.1.1.1.1" xref="S2.E6.m2.1.1.1.1.cmml"><mi id="S2.E6.m2.1.1.1.1.2" xref="S2.E6.m2.1.1.1.1.2.cmml"></mi><mo id="S2.E6.m2.1.1.1.1.1" xref="S2.E6.m2.1.1.1.1.1.cmml">=</mo><mrow id="S2.E6.m2.1.1.1.1.3" xref="S2.E6.m2.1.1.1.1.3.cmml"><mi id="S2.E6.m2.1.1.1.1.3.2" xref="S2.E6.m2.1.1.1.1.3.2.cmml">𝐄</mi><mo id="S2.E6.m2.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E6.m2.1.1.1.1.3.1.cmml">×</mo><mi id="S2.E6.m2.1.1.1.1.3.3" xref="S2.E6.m2.1.1.1.1.3.3.cmml">𝐑</mi></mrow></mrow><mo id="S2.E6.m2.1.1.1.2" xref="S2.E6.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m2.1b"><apply id="S2.E6.m2.1.1.1.1.cmml" xref="S2.E6.m2.1.1.1"><eq id="S2.E6.m2.1.1.1.1.1.cmml" xref="S2.E6.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S2.E6.m2.1.1.1.1.2.cmml" xref="S2.E6.m2.1.1.1.1.2">absent</csymbol><apply id="S2.E6.m2.1.1.1.1.3.cmml" xref="S2.E6.m2.1.1.1.1.3"><times id="S2.E6.m2.1.1.1.1.3.1.cmml" xref="S2.E6.m2.1.1.1.1.3.1"></times><ci id="S2.E6.m2.1.1.1.1.3.2.cmml" xref="S2.E6.m2.1.1.1.1.3.2">𝐄</ci><ci id="S2.E6.m2.1.1.1.1.3.3.cmml" xref="S2.E6.m2.1.1.1.1.3.3">𝐑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m2.1c">\displaystyle=\mathbf{E}\times\mathbf{R},</annotation><annotation encoding="application/x-llamapun" id="S2.E6.m2.1d">= bold_E × bold_R ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{I}" class="ltx_Math" display="inline" id="S2.E7.m1.1"><semantics id="S2.E7.m1.1a"><mi id="S2.E7.m1.1.1" xref="S2.E7.m1.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S2.E7.m1.1b"><ci id="S2.E7.m1.1.1.cmml" xref="S2.E7.m1.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.E7.m1.1c">\displaystyle\mathbf{I}</annotation><annotation encoding="application/x-llamapun" id="S2.E7.m1.1d">bold_I</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathbf{R}^{T}\times\mathbf{R}," class="ltx_Math" display="inline" id="S2.E7.m2.1"><semantics id="S2.E7.m2.1a"><mrow id="S2.E7.m2.1.1.1" xref="S2.E7.m2.1.1.1.1.cmml"><mrow id="S2.E7.m2.1.1.1.1" xref="S2.E7.m2.1.1.1.1.cmml"><mi id="S2.E7.m2.1.1.1.1.2" xref="S2.E7.m2.1.1.1.1.2.cmml"></mi><mo id="S2.E7.m2.1.1.1.1.1" xref="S2.E7.m2.1.1.1.1.1.cmml">=</mo><mrow id="S2.E7.m2.1.1.1.1.3" xref="S2.E7.m2.1.1.1.1.3.cmml"><msup id="S2.E7.m2.1.1.1.1.3.2" xref="S2.E7.m2.1.1.1.1.3.2.cmml"><mi id="S2.E7.m2.1.1.1.1.3.2.2" xref="S2.E7.m2.1.1.1.1.3.2.2.cmml">𝐑</mi><mi id="S2.E7.m2.1.1.1.1.3.2.3" xref="S2.E7.m2.1.1.1.1.3.2.3.cmml">T</mi></msup><mo id="S2.E7.m2.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E7.m2.1.1.1.1.3.1.cmml">×</mo><mi id="S2.E7.m2.1.1.1.1.3.3" xref="S2.E7.m2.1.1.1.1.3.3.cmml">𝐑</mi></mrow></mrow><mo id="S2.E7.m2.1.1.1.2" xref="S2.E7.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E7.m2.1b"><apply id="S2.E7.m2.1.1.1.1.cmml" xref="S2.E7.m2.1.1.1"><eq id="S2.E7.m2.1.1.1.1.1.cmml" xref="S2.E7.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S2.E7.m2.1.1.1.1.2.cmml" xref="S2.E7.m2.1.1.1.1.2">absent</csymbol><apply id="S2.E7.m2.1.1.1.1.3.cmml" xref="S2.E7.m2.1.1.1.1.3"><times id="S2.E7.m2.1.1.1.1.3.1.cmml" xref="S2.E7.m2.1.1.1.1.3.1"></times><apply id="S2.E7.m2.1.1.1.1.3.2.cmml" xref="S2.E7.m2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E7.m2.1.1.1.1.3.2.1.cmml" xref="S2.E7.m2.1.1.1.1.3.2">superscript</csymbol><ci id="S2.E7.m2.1.1.1.1.3.2.2.cmml" xref="S2.E7.m2.1.1.1.1.3.2.2">𝐑</ci><ci id="S2.E7.m2.1.1.1.1.3.2.3.cmml" xref="S2.E7.m2.1.1.1.1.3.2.3">𝑇</ci></apply><ci id="S2.E7.m2.1.1.1.1.3.3.cmml" xref="S2.E7.m2.1.1.1.1.3.3">𝐑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E7.m2.1c">\displaystyle=\mathbf{R}^{T}\times\mathbf{R},</annotation><annotation encoding="application/x-llamapun" id="S2.E7.m2.1d">= bold_R start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT × bold_R ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.SSS2.p1.6">where <math alttext="\mathbf{E}^{\prime}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.2.m1.1"><semantics id="S2.SS2.SSS2.p1.2.m1.1a"><msup id="S2.SS2.SSS2.p1.2.m1.1.1" xref="S2.SS2.SSS2.p1.2.m1.1.1.cmml"><mi id="S2.SS2.SSS2.p1.2.m1.1.1.2" xref="S2.SS2.SSS2.p1.2.m1.1.1.2.cmml">𝐄</mi><mo id="S2.SS2.SSS2.p1.2.m1.1.1.3" xref="S2.SS2.SSS2.p1.2.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.2.m1.1b"><apply id="S2.SS2.SSS2.p1.2.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.2.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m1.1.1">superscript</csymbol><ci id="S2.SS2.SSS2.p1.2.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.2.m1.1.1.2">𝐄</ci><ci id="S2.SS2.SSS2.p1.2.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.2.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.2.m1.1c">\mathbf{E}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.2.m1.1d">bold_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> is the rotated matrix, and <math alttext="\mathbf{I}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.3.m2.1"><semantics id="S2.SS2.SSS2.p1.3.m2.1a"><mi id="S2.SS2.SSS2.p1.3.m2.1.1" xref="S2.SS2.SSS2.p1.3.m2.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.3.m2.1b"><ci id="S2.SS2.SSS2.p1.3.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m2.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.3.m2.1c">\mathbf{I}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.3.m2.1d">bold_I</annotation></semantics></math> represents the identity matrix. Next, <math alttext="\mathbf{E}^{\prime}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.4.m3.1"><semantics id="S2.SS2.SSS2.p1.4.m3.1a"><msup id="S2.SS2.SSS2.p1.4.m3.1.1" xref="S2.SS2.SSS2.p1.4.m3.1.1.cmml"><mi id="S2.SS2.SSS2.p1.4.m3.1.1.2" xref="S2.SS2.SSS2.p1.4.m3.1.1.2.cmml">𝐄</mi><mo id="S2.SS2.SSS2.p1.4.m3.1.1.3" xref="S2.SS2.SSS2.p1.4.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.4.m3.1b"><apply id="S2.SS2.SSS2.p1.4.m3.1.1.cmml" xref="S2.SS2.SSS2.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.4.m3.1.1.1.cmml" xref="S2.SS2.SSS2.p1.4.m3.1.1">superscript</csymbol><ci id="S2.SS2.SSS2.p1.4.m3.1.1.2.cmml" xref="S2.SS2.SSS2.p1.4.m3.1.1.2">𝐄</ci><ci id="S2.SS2.SSS2.p1.4.m3.1.1.3.cmml" xref="S2.SS2.SSS2.p1.4.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.4.m3.1c">\mathbf{E}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.4.m3.1d">bold_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> will be operated by product quantization, as described in Sec <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.SS2.SSS1" title="2.2.1. Product Quantization (PQ) (Juang and Gray, 1982; Jegou et al., 2010) ‣ 2.2. Parallel Vector Quantization ‣ 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>. It is important to note that the rotation matrix <math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.5.m4.1"><semantics id="S2.SS2.SSS2.p1.5.m4.1a"><mi id="S2.SS2.SSS2.p1.5.m4.1.1" xref="S2.SS2.SSS2.p1.5.m4.1.1.cmml">𝐑</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.5.m4.1b"><ci id="S2.SS2.SSS2.p1.5.m4.1.1.cmml" xref="S2.SS2.SSS2.p1.5.m4.1.1">𝐑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.5.m4.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.5.m4.1d">bold_R</annotation></semantics></math> is trained with the codebooks.
Once trained, the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.6.m5.1"><semantics id="S2.SS2.SSS2.p1.6.m5.1a"><mi id="S2.SS2.SSS2.p1.6.m5.1.1" xref="S2.SS2.SSS2.p1.6.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.6.m5.1b"><ci id="S2.SS2.SSS2.p1.6.m5.1.1.cmml" xref="S2.SS2.SSS2.p1.6.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.6.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.6.m5.1d">italic_i</annotation></semantics></math>-th original vector can be approximated by:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(8)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{e}_{i}\approx\left[\cdots,\mathbf{c}^{(j)}_{x_{j}},\cdots,\right]%
\times\mathbf{R}^{T}\quad\text{for }j\in\{1,2,\ldots,M\}." class="ltx_math_unparsed" display="block" id="S2.E8.m1.3"><semantics id="S2.E8.m1.3a"><mrow id="S2.E8.m1.3b"><msub id="S2.E8.m1.3.4"><mi id="S2.E8.m1.3.4.2">𝐞</mi><mi id="S2.E8.m1.3.4.3">i</mi></msub><mo id="S2.E8.m1.3.5">≈</mo><mrow id="S2.E8.m1.3.6"><mo id="S2.E8.m1.3.6.1">[</mo><mi id="S2.E8.m1.2.2" mathvariant="normal">⋯</mi><mo id="S2.E8.m1.3.6.2">,</mo><msubsup id="S2.E8.m1.3.6.3"><mi id="S2.E8.m1.3.6.3.2.2">𝐜</mi><msub id="S2.E8.m1.3.6.3.3"><mi id="S2.E8.m1.3.6.3.3.2">x</mi><mi id="S2.E8.m1.3.6.3.3.3">j</mi></msub><mrow id="S2.E8.m1.1.1.1.3"><mo id="S2.E8.m1.1.1.1.3.1" stretchy="false">(</mo><mi id="S2.E8.m1.1.1.1.1">j</mi><mo id="S2.E8.m1.1.1.1.3.2" stretchy="false">)</mo></mrow></msubsup><mo id="S2.E8.m1.3.6.4">,</mo><mi id="S2.E8.m1.3.3" mathvariant="normal">⋯</mi><mo id="S2.E8.m1.3.6.5">,</mo><mo id="S2.E8.m1.3.6.6" rspace="0.055em">]</mo></mrow><mo id="S2.E8.m1.3.7" rspace="0.222em">×</mo><msup id="S2.E8.m1.3.8"><mi id="S2.E8.m1.3.8.2">𝐑</mi><mi id="S2.E8.m1.3.8.3">T</mi></msup><mspace id="S2.E8.m1.3.9" width="1em"></mspace><mtext id="S2.E8.m1.3.10">for </mtext><mi id="S2.E8.m1.3.11">j</mi><mo id="S2.E8.m1.3.12">∈</mo><mrow id="S2.E8.m1.3.13"><mo id="S2.E8.m1.3.13.1" stretchy="false">{</mo><mn id="S2.E8.m1.3.13.2">1</mn><mo id="S2.E8.m1.3.13.3">,</mo><mn id="S2.E8.m1.3.13.4">2</mn><mo id="S2.E8.m1.3.13.5">,</mo><mi id="S2.E8.m1.3.13.6" mathvariant="normal">…</mi><mo id="S2.E8.m1.3.13.7">,</mo><mi id="S2.E8.m1.3.13.8">M</mi><mo id="S2.E8.m1.3.13.9" stretchy="false">}</mo></mrow><mo id="S2.E8.m1.3.14" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="S2.E8.m1.3c">\mathbf{e}_{i}\approx\left[\cdots,\mathbf{c}^{(j)}_{x_{j}},\cdots,\right]%
\times\mathbf{R}^{T}\quad\text{for }j\in\{1,2,\ldots,M\}.</annotation><annotation encoding="application/x-llamapun" id="S2.E8.m1.3d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≈ [ ⋯ , bold_c start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT , ⋯ , ] × bold_R start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT for italic_j ∈ { 1 , 2 , … , italic_M } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Sequential Vector Quantization</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.4">Standard VQ and parallel VQ typically yield <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.4.1">rough</em> approximations of vectors. Specifically, each dimension of the original vector can only be approximated by one single value from the corresponding code vector, leading to substantial information loss.
Taking standard VQ as an example, the difference between the original vector <math alttext="\mathbf{e}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">𝐞</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝐞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\mathbf{e}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">bold_e</annotation></semantics></math> and its corresponding code <math alttext="\mathbf{c}" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">𝐜</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">𝐜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">\mathbf{c}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">bold_c</annotation></semantics></math>, denoted by <math alttext="\mathbf{e}-\mathbf{c}" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><mrow id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">𝐞</mi><mo id="S2.SS3.p1.3.m3.1.1.1" xref="S2.SS3.p1.3.m3.1.1.1.cmml">−</mo><mi id="S2.SS3.p1.3.m3.1.1.3" xref="S2.SS3.p1.3.m3.1.1.3.cmml">𝐜</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><minus id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1.1"></minus><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">𝐞</ci><ci id="S2.SS3.p1.3.m3.1.1.3.cmml" xref="S2.SS3.p1.3.m3.1.1.3">𝐜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">\mathbf{e}-\mathbf{c}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.3.m3.1d">bold_e - bold_c</annotation></semantics></math>, reflects the unique characteristics that cannot be represented by <math alttext="\mathbf{c}" class="ltx_Math" display="inline" id="S2.SS3.p1.4.m4.1"><semantics id="S2.SS3.p1.4.m4.1a"><mi id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml">𝐜</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><ci id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">𝐜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">\mathbf{c}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.4.m4.1d">bold_c</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">To achieve a more <em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.1">precise</em> quantization, approaches like residual quantization <cite class="ltx_cite ltx_citemacro_citep">(Juang and Gray, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib34" title="">1982</a>; Martinez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib56" title="">2014</a>)</cite> and additive quantization <cite class="ltx_cite ltx_citemacro_citep">(Babenko and Lempitsky, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib3" title="">2014</a>)</cite> have been developed, falling under the umbrella of sequential quantization. This method employs multiple codebooks, with each codebook approximates every dimension of the original vectors. Essentially, every codebook offers a distinct approximation perspective of the vectors, and the accuracy of these approximations improves with an increase in the number of codebooks. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">2</span></a>, using the first layer codebook approximates <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p2.1.2">0.3</span> (the first dimension of the original vector) as <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p2.1.3">0.5</span> (the first dimension of the code vector in the first codebook). After applying the second codebook, it is more accurately approximated as 0.5 + (-0.3) = 0.2 (the first dimension of the code vector in the second codebook).</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1. </span>Residual Quantization (RQ) <cite class="ltx_cite ltx_citemacro_citep">(Juang and Gray, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib34" title="">1982</a>; Martinez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib56" title="">2014</a>)</cite>
</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.13">By designing <math alttext="M" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.1.m1.1"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><mi id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><ci id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.1.m1.1d">italic_M</annotation></semantics></math> individual codebooks where, as depicted in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">1</span></a>, code vectors have the full same length of the input vector, residual quantization aims to approximate the target vectors by compressing their information in a coarse-to-fine manner. Specifically, the codebooks are learned iteratively from the residual representations of the vectors. This process can be formulated as: <math alttext="\mathbf{E}^{(j+1)}=\mathbf{E}^{(j)}-\mathbf{X}^{(j)}\mathbf{C}^{(j)}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.2.m2.4"><semantics id="S2.SS3.SSS1.p1.2.m2.4a"><mrow id="S2.SS3.SSS1.p1.2.m2.4.5" xref="S2.SS3.SSS1.p1.2.m2.4.5.cmml"><msup id="S2.SS3.SSS1.p1.2.m2.4.5.2" xref="S2.SS3.SSS1.p1.2.m2.4.5.2.cmml"><mi id="S2.SS3.SSS1.p1.2.m2.4.5.2.2" xref="S2.SS3.SSS1.p1.2.m2.4.5.2.2.cmml">𝐄</mi><mrow id="S2.SS3.SSS1.p1.2.m2.1.1.1.1" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.cmml"><mo id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.2" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.2" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.2.cmml">j</mi><mo id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.1" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.1.cmml">+</mo><mn id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.3" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.3" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S2.SS3.SSS1.p1.2.m2.4.5.1" xref="S2.SS3.SSS1.p1.2.m2.4.5.1.cmml">=</mo><mrow id="S2.SS3.SSS1.p1.2.m2.4.5.3" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.cmml"><msup id="S2.SS3.SSS1.p1.2.m2.4.5.3.2" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2.cmml"><mi id="S2.SS3.SSS1.p1.2.m2.4.5.3.2.2" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2.2.cmml">𝐄</mi><mrow id="S2.SS3.SSS1.p1.2.m2.2.2.1.3" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2.cmml"><mo id="S2.SS3.SSS1.p1.2.m2.2.2.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2.cmml">(</mo><mi id="S2.SS3.SSS1.p1.2.m2.2.2.1.1" xref="S2.SS3.SSS1.p1.2.m2.2.2.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.2.m2.2.2.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2.cmml">)</mo></mrow></msup><mo id="S2.SS3.SSS1.p1.2.m2.4.5.3.1" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.1.cmml">−</mo><mrow id="S2.SS3.SSS1.p1.2.m2.4.5.3.3" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.cmml"><msup id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.cmml"><mi id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.2" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.2.cmml">𝐗</mi><mrow id="S2.SS3.SSS1.p1.2.m2.3.3.1.3" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.cmml"><mo id="S2.SS3.SSS1.p1.2.m2.3.3.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.cmml">(</mo><mi id="S2.SS3.SSS1.p1.2.m2.3.3.1.1" xref="S2.SS3.SSS1.p1.2.m2.3.3.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.2.m2.3.3.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.cmml">)</mo></mrow></msup><mo id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.1" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.1.cmml">⁢</mo><msup id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.cmml"><mi id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.2" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.2.cmml">𝐂</mi><mrow id="S2.SS3.SSS1.p1.2.m2.4.4.1.3" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.cmml"><mo id="S2.SS3.SSS1.p1.2.m2.4.4.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.cmml">(</mo><mi id="S2.SS3.SSS1.p1.2.m2.4.4.1.1" xref="S2.SS3.SSS1.p1.2.m2.4.4.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.2.m2.4.4.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.cmml">)</mo></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.2.m2.4b"><apply id="S2.SS3.SSS1.p1.2.m2.4.5.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5"><eq id="S2.SS3.SSS1.p1.2.m2.4.5.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.1"></eq><apply id="S2.SS3.SSS1.p1.2.m2.4.5.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.2.m2.4.5.2.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.2.m2.4.5.2.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.2.2">𝐄</ci><apply id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1"><plus id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.1"></plus><ci id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.2">𝑗</ci><cn id="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.p1.2.m2.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.SS3.SSS1.p1.2.m2.4.5.3.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3"><minus id="S2.SS3.SSS1.p1.2.m2.4.5.3.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.1"></minus><apply id="S2.SS3.SSS1.p1.2.m2.4.5.3.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.2.m2.4.5.3.2.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.2.m2.4.5.3.2.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.2.2">𝐄</ci><ci id="S2.SS3.SSS1.p1.2.m2.2.2.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.2.2.1.1">𝑗</ci></apply><apply id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3"><times id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.1"></times><apply id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.2.2">𝐗</ci><ci id="S2.SS3.SSS1.p1.2.m2.3.3.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.3.3.1.1">𝑗</ci></apply><apply id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3">superscript</csymbol><ci id="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.5.3.3.3.2">𝐂</ci><ci id="S2.SS3.SSS1.p1.2.m2.4.4.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.4.4.1.1">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.2.m2.4c">\mathbf{E}^{(j+1)}=\mathbf{E}^{(j)}-\mathbf{X}^{(j)}\mathbf{C}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.2.m2.4d">bold_E start_POSTSUPERSCRIPT ( italic_j + 1 ) end_POSTSUPERSCRIPT = bold_E start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT - bold_X start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT bold_C start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="\mathbf{E}^{1}=\mathbf{E}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.3.m3.1"><semantics id="S2.SS3.SSS1.p1.3.m3.1a"><mrow id="S2.SS3.SSS1.p1.3.m3.1.1" xref="S2.SS3.SSS1.p1.3.m3.1.1.cmml"><msup id="S2.SS3.SSS1.p1.3.m3.1.1.2" xref="S2.SS3.SSS1.p1.3.m3.1.1.2.cmml"><mi id="S2.SS3.SSS1.p1.3.m3.1.1.2.2" xref="S2.SS3.SSS1.p1.3.m3.1.1.2.2.cmml">𝐄</mi><mn id="S2.SS3.SSS1.p1.3.m3.1.1.2.3" xref="S2.SS3.SSS1.p1.3.m3.1.1.2.3.cmml">1</mn></msup><mo id="S2.SS3.SSS1.p1.3.m3.1.1.1" xref="S2.SS3.SSS1.p1.3.m3.1.1.1.cmml">=</mo><mi id="S2.SS3.SSS1.p1.3.m3.1.1.3" xref="S2.SS3.SSS1.p1.3.m3.1.1.3.cmml">𝐄</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.3.m3.1b"><apply id="S2.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS3.SSS1.p1.3.m3.1.1"><eq id="S2.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S2.SS3.SSS1.p1.3.m3.1.1.1"></eq><apply id="S2.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S2.SS3.SSS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS3.SSS1.p1.3.m3.1.1.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS3.SSS1.p1.3.m3.1.1.2.2">𝐄</ci><cn id="S2.SS3.SSS1.p1.3.m3.1.1.2.3.cmml" type="integer" xref="S2.SS3.SSS1.p1.3.m3.1.1.2.3">1</cn></apply><ci id="S2.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S2.SS3.SSS1.p1.3.m3.1.1.3">𝐄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.3.m3.1c">\mathbf{E}^{1}=\mathbf{E}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.3.m3.1d">bold_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = bold_E</annotation></semantics></math>, <math alttext="\mathbf{C}^{(j)}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.4.m4.1"><semantics id="S2.SS3.SSS1.p1.4.m4.1a"><msup id="S2.SS3.SSS1.p1.4.m4.1.2" xref="S2.SS3.SSS1.p1.4.m4.1.2.cmml"><mi id="S2.SS3.SSS1.p1.4.m4.1.2.2" xref="S2.SS3.SSS1.p1.4.m4.1.2.2.cmml">𝐂</mi><mrow id="S2.SS3.SSS1.p1.4.m4.1.1.1.3" xref="S2.SS3.SSS1.p1.4.m4.1.2.cmml"><mo id="S2.SS3.SSS1.p1.4.m4.1.1.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.4.m4.1.2.cmml">(</mo><mi id="S2.SS3.SSS1.p1.4.m4.1.1.1.1" xref="S2.SS3.SSS1.p1.4.m4.1.1.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.4.m4.1.1.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.4.m4.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.4.m4.1b"><apply id="S2.SS3.SSS1.p1.4.m4.1.2.cmml" xref="S2.SS3.SSS1.p1.4.m4.1.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.4.m4.1.2.1.cmml" xref="S2.SS3.SSS1.p1.4.m4.1.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.4.m4.1.2.2.cmml" xref="S2.SS3.SSS1.p1.4.m4.1.2.2">𝐂</ci><ci id="S2.SS3.SSS1.p1.4.m4.1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.4.m4.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.4.m4.1c">\mathbf{C}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.4.m4.1d">bold_C start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math> is the <math alttext="j" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.5.m5.1"><semantics id="S2.SS3.SSS1.p1.5.m5.1a"><mi id="S2.SS3.SSS1.p1.5.m5.1.1" xref="S2.SS3.SSS1.p1.5.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.5.m5.1b"><ci id="S2.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS3.SSS1.p1.5.m5.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.5.m5.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.5.m5.1d">italic_j</annotation></semantics></math>-th codebook with size <math alttext="K_{j}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.6.m6.1"><semantics id="S2.SS3.SSS1.p1.6.m6.1a"><msub id="S2.SS3.SSS1.p1.6.m6.1.1" xref="S2.SS3.SSS1.p1.6.m6.1.1.cmml"><mi id="S2.SS3.SSS1.p1.6.m6.1.1.2" xref="S2.SS3.SSS1.p1.6.m6.1.1.2.cmml">K</mi><mi id="S2.SS3.SSS1.p1.6.m6.1.1.3" xref="S2.SS3.SSS1.p1.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.6.m6.1b"><apply id="S2.SS3.SSS1.p1.6.m6.1.1.cmml" xref="S2.SS3.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.6.m6.1.1.1.cmml" xref="S2.SS3.SSS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p1.6.m6.1.1.2.cmml" xref="S2.SS3.SSS1.p1.6.m6.1.1.2">𝐾</ci><ci id="S2.SS3.SSS1.p1.6.m6.1.1.3.cmml" xref="S2.SS3.SSS1.p1.6.m6.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.6.m6.1c">K_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.6.m6.1d">italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\mathbf{X}^{(j)}\in\mathrm{\{0,1\}}^{N}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.7.m7.3"><semantics id="S2.SS3.SSS1.p1.7.m7.3a"><mrow id="S2.SS3.SSS1.p1.7.m7.3.4" xref="S2.SS3.SSS1.p1.7.m7.3.4.cmml"><msup id="S2.SS3.SSS1.p1.7.m7.3.4.2" xref="S2.SS3.SSS1.p1.7.m7.3.4.2.cmml"><mi id="S2.SS3.SSS1.p1.7.m7.3.4.2.2" xref="S2.SS3.SSS1.p1.7.m7.3.4.2.2.cmml">𝐗</mi><mrow id="S2.SS3.SSS1.p1.7.m7.1.1.1.3" xref="S2.SS3.SSS1.p1.7.m7.3.4.2.cmml"><mo id="S2.SS3.SSS1.p1.7.m7.1.1.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.7.m7.3.4.2.cmml">(</mo><mi id="S2.SS3.SSS1.p1.7.m7.1.1.1.1" xref="S2.SS3.SSS1.p1.7.m7.1.1.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.7.m7.1.1.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.7.m7.3.4.2.cmml">)</mo></mrow></msup><mo id="S2.SS3.SSS1.p1.7.m7.3.4.1" xref="S2.SS3.SSS1.p1.7.m7.3.4.1.cmml">∈</mo><msup id="S2.SS3.SSS1.p1.7.m7.3.4.3" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.cmml"><mrow id="S2.SS3.SSS1.p1.7.m7.3.4.3.2.2" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.2.1.cmml"><mo id="S2.SS3.SSS1.p1.7.m7.3.4.3.2.2.1" stretchy="false" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.2.1.cmml">{</mo><mn id="S2.SS3.SSS1.p1.7.m7.2.2" xref="S2.SS3.SSS1.p1.7.m7.2.2.cmml">0</mn><mo id="S2.SS3.SSS1.p1.7.m7.3.4.3.2.2.2" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.2.1.cmml">,</mo><mn id="S2.SS3.SSS1.p1.7.m7.3.3" xref="S2.SS3.SSS1.p1.7.m7.3.3.cmml">1</mn><mo id="S2.SS3.SSS1.p1.7.m7.3.4.3.2.2.3" stretchy="false" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.2.1.cmml">}</mo></mrow><mi id="S2.SS3.SSS1.p1.7.m7.3.4.3.3" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.3.cmml">N</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.7.m7.3b"><apply id="S2.SS3.SSS1.p1.7.m7.3.4.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4"><in id="S2.SS3.SSS1.p1.7.m7.3.4.1.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.1"></in><apply id="S2.SS3.SSS1.p1.7.m7.3.4.2.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.7.m7.3.4.2.1.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.7.m7.3.4.2.2.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.2.2">𝐗</ci><ci id="S2.SS3.SSS1.p1.7.m7.1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.7.m7.1.1.1.1">𝑗</ci></apply><apply id="S2.SS3.SSS1.p1.7.m7.3.4.3.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.3"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.7.m7.3.4.3.1.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.3">superscript</csymbol><set id="S2.SS3.SSS1.p1.7.m7.3.4.3.2.1.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.2.2"><cn id="S2.SS3.SSS1.p1.7.m7.2.2.cmml" type="integer" xref="S2.SS3.SSS1.p1.7.m7.2.2">0</cn><cn id="S2.SS3.SSS1.p1.7.m7.3.3.cmml" type="integer" xref="S2.SS3.SSS1.p1.7.m7.3.3">1</cn></set><ci id="S2.SS3.SSS1.p1.7.m7.3.4.3.3.cmml" xref="S2.SS3.SSS1.p1.7.m7.3.4.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.7.m7.3c">\mathbf{X}^{(j)}\in\mathrm{\{0,1\}}^{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.7.m7.3d">bold_X start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is a one-hot mapper, where <math alttext="\mathbf{X}^{(j)}_{i,k}=1" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.8.m8.3"><semantics id="S2.SS3.SSS1.p1.8.m8.3a"><mrow id="S2.SS3.SSS1.p1.8.m8.3.4" xref="S2.SS3.SSS1.p1.8.m8.3.4.cmml"><msubsup id="S2.SS3.SSS1.p1.8.m8.3.4.2" xref="S2.SS3.SSS1.p1.8.m8.3.4.2.cmml"><mi id="S2.SS3.SSS1.p1.8.m8.3.4.2.2.2" xref="S2.SS3.SSS1.p1.8.m8.3.4.2.2.2.cmml">𝐗</mi><mrow id="S2.SS3.SSS1.p1.8.m8.3.3.2.4" xref="S2.SS3.SSS1.p1.8.m8.3.3.2.3.cmml"><mi id="S2.SS3.SSS1.p1.8.m8.2.2.1.1" xref="S2.SS3.SSS1.p1.8.m8.2.2.1.1.cmml">i</mi><mo id="S2.SS3.SSS1.p1.8.m8.3.3.2.4.1" xref="S2.SS3.SSS1.p1.8.m8.3.3.2.3.cmml">,</mo><mi id="S2.SS3.SSS1.p1.8.m8.3.3.2.2" xref="S2.SS3.SSS1.p1.8.m8.3.3.2.2.cmml">k</mi></mrow><mrow id="S2.SS3.SSS1.p1.8.m8.1.1.1.3" xref="S2.SS3.SSS1.p1.8.m8.3.4.2.cmml"><mo id="S2.SS3.SSS1.p1.8.m8.1.1.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.8.m8.3.4.2.cmml">(</mo><mi id="S2.SS3.SSS1.p1.8.m8.1.1.1.1" xref="S2.SS3.SSS1.p1.8.m8.1.1.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.8.m8.1.1.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.8.m8.3.4.2.cmml">)</mo></mrow></msubsup><mo id="S2.SS3.SSS1.p1.8.m8.3.4.1" xref="S2.SS3.SSS1.p1.8.m8.3.4.1.cmml">=</mo><mn id="S2.SS3.SSS1.p1.8.m8.3.4.3" xref="S2.SS3.SSS1.p1.8.m8.3.4.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.8.m8.3b"><apply id="S2.SS3.SSS1.p1.8.m8.3.4.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.4"><eq id="S2.SS3.SSS1.p1.8.m8.3.4.1.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.4.1"></eq><apply id="S2.SS3.SSS1.p1.8.m8.3.4.2.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.4.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.8.m8.3.4.2.1.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.4.2">subscript</csymbol><apply id="S2.SS3.SSS1.p1.8.m8.3.4.2.2.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.4.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.8.m8.3.4.2.2.1.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.4.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.8.m8.3.4.2.2.2.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.4.2.2.2">𝐗</ci><ci id="S2.SS3.SSS1.p1.8.m8.1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.8.m8.1.1.1.1">𝑗</ci></apply><list id="S2.SS3.SSS1.p1.8.m8.3.3.2.3.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.3.2.4"><ci id="S2.SS3.SSS1.p1.8.m8.2.2.1.1.cmml" xref="S2.SS3.SSS1.p1.8.m8.2.2.1.1">𝑖</ci><ci id="S2.SS3.SSS1.p1.8.m8.3.3.2.2.cmml" xref="S2.SS3.SSS1.p1.8.m8.3.3.2.2">𝑘</ci></list></apply><cn id="S2.SS3.SSS1.p1.8.m8.3.4.3.cmml" type="integer" xref="S2.SS3.SSS1.p1.8.m8.3.4.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.8.m8.3c">\mathbf{X}^{(j)}_{i,k}=1</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.8.m8.3d">bold_X start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_k end_POSTSUBSCRIPT = 1</annotation></semantics></math> only if the <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.9.m9.1"><semantics id="S2.SS3.SSS1.p1.9.m9.1a"><mi id="S2.SS3.SSS1.p1.9.m9.1.1" xref="S2.SS3.SSS1.p1.9.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.9.m9.1b"><ci id="S2.SS3.SSS1.p1.9.m9.1.1.cmml" xref="S2.SS3.SSS1.p1.9.m9.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.9.m9.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.9.m9.1d">italic_k</annotation></semantics></math>-th code is the nearest to the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.10.m10.1"><semantics id="S2.SS3.SSS1.p1.10.m10.1a"><mi id="S2.SS3.SSS1.p1.10.m10.1.1" xref="S2.SS3.SSS1.p1.10.m10.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.10.m10.1b"><ci id="S2.SS3.SSS1.p1.10.m10.1.1.cmml" xref="S2.SS3.SSS1.p1.10.m10.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.10.m10.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.10.m10.1d">italic_i</annotation></semantics></math>-th vector of <math alttext="\mathbf{E}^{(j)}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.11.m11.1"><semantics id="S2.SS3.SSS1.p1.11.m11.1a"><msup id="S2.SS3.SSS1.p1.11.m11.1.2" xref="S2.SS3.SSS1.p1.11.m11.1.2.cmml"><mi id="S2.SS3.SSS1.p1.11.m11.1.2.2" xref="S2.SS3.SSS1.p1.11.m11.1.2.2.cmml">𝐄</mi><mrow id="S2.SS3.SSS1.p1.11.m11.1.1.1.3" xref="S2.SS3.SSS1.p1.11.m11.1.2.cmml"><mo id="S2.SS3.SSS1.p1.11.m11.1.1.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.11.m11.1.2.cmml">(</mo><mi id="S2.SS3.SSS1.p1.11.m11.1.1.1.1" xref="S2.SS3.SSS1.p1.11.m11.1.1.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.11.m11.1.1.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.11.m11.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.11.m11.1b"><apply id="S2.SS3.SSS1.p1.11.m11.1.2.cmml" xref="S2.SS3.SSS1.p1.11.m11.1.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.11.m11.1.2.1.cmml" xref="S2.SS3.SSS1.p1.11.m11.1.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.11.m11.1.2.2.cmml" xref="S2.SS3.SSS1.p1.11.m11.1.2.2">𝐄</ci><ci id="S2.SS3.SSS1.p1.11.m11.1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.11.m11.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.11.m11.1c">\mathbf{E}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.11.m11.1d">bold_E start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math> in the codebook <math alttext="\mathbf{C}^{(j)}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.12.m12.1"><semantics id="S2.SS3.SSS1.p1.12.m12.1a"><msup id="S2.SS3.SSS1.p1.12.m12.1.2" xref="S2.SS3.SSS1.p1.12.m12.1.2.cmml"><mi id="S2.SS3.SSS1.p1.12.m12.1.2.2" xref="S2.SS3.SSS1.p1.12.m12.1.2.2.cmml">𝐂</mi><mrow id="S2.SS3.SSS1.p1.12.m12.1.1.1.3" xref="S2.SS3.SSS1.p1.12.m12.1.2.cmml"><mo id="S2.SS3.SSS1.p1.12.m12.1.1.1.3.1" stretchy="false" xref="S2.SS3.SSS1.p1.12.m12.1.2.cmml">(</mo><mi id="S2.SS3.SSS1.p1.12.m12.1.1.1.1" xref="S2.SS3.SSS1.p1.12.m12.1.1.1.1.cmml">j</mi><mo id="S2.SS3.SSS1.p1.12.m12.1.1.1.3.2" stretchy="false" xref="S2.SS3.SSS1.p1.12.m12.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.12.m12.1b"><apply id="S2.SS3.SSS1.p1.12.m12.1.2.cmml" xref="S2.SS3.SSS1.p1.12.m12.1.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.12.m12.1.2.1.cmml" xref="S2.SS3.SSS1.p1.12.m12.1.2">superscript</csymbol><ci id="S2.SS3.SSS1.p1.12.m12.1.2.2.cmml" xref="S2.SS3.SSS1.p1.12.m12.1.2.2">𝐂</ci><ci id="S2.SS3.SSS1.p1.12.m12.1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.12.m12.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.12.m12.1c">\mathbf{C}^{(j)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.12.m12.1d">bold_C start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT</annotation></semantics></math>. After iteratively residual approximation, the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.13.m13.1"><semantics id="S2.SS3.SSS1.p1.13.m13.1a"><mi id="S2.SS3.SSS1.p1.13.m13.1.1" xref="S2.SS3.SSS1.p1.13.m13.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.13.m13.1b"><ci id="S2.SS3.SSS1.p1.13.m13.1.1.cmml" xref="S2.SS3.SSS1.p1.13.m13.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.13.m13.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.13.m13.1d">italic_i</annotation></semantics></math>-th original vector can be represented by:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx1.EGx3">
<tbody id="S2.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(9)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{e}_{i}" class="ltx_Math" display="inline" id="S2.E9.m1.1"><semantics id="S2.E9.m1.1a"><msub id="S2.E9.m1.1.1" xref="S2.E9.m1.1.1.cmml"><mi id="S2.E9.m1.1.1.2" xref="S2.E9.m1.1.1.2.cmml">𝐞</mi><mi id="S2.E9.m1.1.1.3" xref="S2.E9.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E9.m1.1b"><apply id="S2.E9.m1.1.1.cmml" xref="S2.E9.m1.1.1"><csymbol cd="ambiguous" id="S2.E9.m1.1.1.1.cmml" xref="S2.E9.m1.1.1">subscript</csymbol><ci id="S2.E9.m1.1.1.2.cmml" xref="S2.E9.m1.1.1.2">𝐞</ci><ci id="S2.E9.m1.1.1.3.cmml" xref="S2.E9.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E9.m1.1c">\displaystyle\mathbf{e}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.E9.m1.1d">bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\approx\sum_{j}^{M}\mathbf{c}^{(j)}_{x_{j}}," class="ltx_Math" display="inline" id="S2.E9.m2.2"><semantics id="S2.E9.m2.2a"><mrow id="S2.E9.m2.2.2.1" xref="S2.E9.m2.2.2.1.1.cmml"><mrow id="S2.E9.m2.2.2.1.1" xref="S2.E9.m2.2.2.1.1.cmml"><mi id="S2.E9.m2.2.2.1.1.2" xref="S2.E9.m2.2.2.1.1.2.cmml"></mi><mo id="S2.E9.m2.2.2.1.1.1" xref="S2.E9.m2.2.2.1.1.1.cmml">≈</mo><mrow id="S2.E9.m2.2.2.1.1.3" xref="S2.E9.m2.2.2.1.1.3.cmml"><mstyle displaystyle="true" id="S2.E9.m2.2.2.1.1.3.1" xref="S2.E9.m2.2.2.1.1.3.1.cmml"><munderover id="S2.E9.m2.2.2.1.1.3.1a" xref="S2.E9.m2.2.2.1.1.3.1.cmml"><mo id="S2.E9.m2.2.2.1.1.3.1.2.2" movablelimits="false" xref="S2.E9.m2.2.2.1.1.3.1.2.2.cmml">∑</mo><mi id="S2.E9.m2.2.2.1.1.3.1.2.3" xref="S2.E9.m2.2.2.1.1.3.1.2.3.cmml">j</mi><mi id="S2.E9.m2.2.2.1.1.3.1.3" xref="S2.E9.m2.2.2.1.1.3.1.3.cmml">M</mi></munderover></mstyle><msubsup id="S2.E9.m2.2.2.1.1.3.2" xref="S2.E9.m2.2.2.1.1.3.2.cmml"><mi id="S2.E9.m2.2.2.1.1.3.2.2.2" xref="S2.E9.m2.2.2.1.1.3.2.2.2.cmml">𝐜</mi><msub id="S2.E9.m2.2.2.1.1.3.2.3" xref="S2.E9.m2.2.2.1.1.3.2.3.cmml"><mi id="S2.E9.m2.2.2.1.1.3.2.3.2" xref="S2.E9.m2.2.2.1.1.3.2.3.2.cmml">x</mi><mi id="S2.E9.m2.2.2.1.1.3.2.3.3" xref="S2.E9.m2.2.2.1.1.3.2.3.3.cmml">j</mi></msub><mrow id="S2.E9.m2.1.1.1.3" xref="S2.E9.m2.2.2.1.1.3.2.cmml"><mo id="S2.E9.m2.1.1.1.3.1" stretchy="false" xref="S2.E9.m2.2.2.1.1.3.2.cmml">(</mo><mi id="S2.E9.m2.1.1.1.1" xref="S2.E9.m2.1.1.1.1.cmml">j</mi><mo id="S2.E9.m2.1.1.1.3.2" stretchy="false" xref="S2.E9.m2.2.2.1.1.3.2.cmml">)</mo></mrow></msubsup></mrow></mrow><mo id="S2.E9.m2.2.2.1.2" xref="S2.E9.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E9.m2.2b"><apply id="S2.E9.m2.2.2.1.1.cmml" xref="S2.E9.m2.2.2.1"><approx id="S2.E9.m2.2.2.1.1.1.cmml" xref="S2.E9.m2.2.2.1.1.1"></approx><csymbol cd="latexml" id="S2.E9.m2.2.2.1.1.2.cmml" xref="S2.E9.m2.2.2.1.1.2">absent</csymbol><apply id="S2.E9.m2.2.2.1.1.3.cmml" xref="S2.E9.m2.2.2.1.1.3"><apply id="S2.E9.m2.2.2.1.1.3.1.cmml" xref="S2.E9.m2.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S2.E9.m2.2.2.1.1.3.1.1.cmml" xref="S2.E9.m2.2.2.1.1.3.1">superscript</csymbol><apply id="S2.E9.m2.2.2.1.1.3.1.2.cmml" xref="S2.E9.m2.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S2.E9.m2.2.2.1.1.3.1.2.1.cmml" xref="S2.E9.m2.2.2.1.1.3.1">subscript</csymbol><sum id="S2.E9.m2.2.2.1.1.3.1.2.2.cmml" xref="S2.E9.m2.2.2.1.1.3.1.2.2"></sum><ci id="S2.E9.m2.2.2.1.1.3.1.2.3.cmml" xref="S2.E9.m2.2.2.1.1.3.1.2.3">𝑗</ci></apply><ci id="S2.E9.m2.2.2.1.1.3.1.3.cmml" xref="S2.E9.m2.2.2.1.1.3.1.3">𝑀</ci></apply><apply id="S2.E9.m2.2.2.1.1.3.2.cmml" xref="S2.E9.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E9.m2.2.2.1.1.3.2.1.cmml" xref="S2.E9.m2.2.2.1.1.3.2">subscript</csymbol><apply id="S2.E9.m2.2.2.1.1.3.2.2.cmml" xref="S2.E9.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E9.m2.2.2.1.1.3.2.2.1.cmml" xref="S2.E9.m2.2.2.1.1.3.2">superscript</csymbol><ci id="S2.E9.m2.2.2.1.1.3.2.2.2.cmml" xref="S2.E9.m2.2.2.1.1.3.2.2.2">𝐜</ci><ci id="S2.E9.m2.1.1.1.1.cmml" xref="S2.E9.m2.1.1.1.1">𝑗</ci></apply><apply id="S2.E9.m2.2.2.1.1.3.2.3.cmml" xref="S2.E9.m2.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E9.m2.2.2.1.1.3.2.3.1.cmml" xref="S2.E9.m2.2.2.1.1.3.2.3">subscript</csymbol><ci id="S2.E9.m2.2.2.1.1.3.2.3.2.cmml" xref="S2.E9.m2.2.2.1.1.3.2.3.2">𝑥</ci><ci id="S2.E9.m2.2.2.1.1.3.2.3.3.cmml" xref="S2.E9.m2.2.2.1.1.3.2.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E9.m2.2c">\displaystyle\approx\sum_{j}^{M}\mathbf{c}^{(j)}_{x_{j}},</annotation><annotation encoding="application/x-llamapun" id="S2.E9.m2.2d">≈ ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT bold_c start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(10)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\textit{where }x_{j}" class="ltx_Math" display="inline" id="S2.E10.m1.1"><semantics id="S2.E10.m1.1a"><mrow id="S2.E10.m1.1.1" xref="S2.E10.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S2.E10.m1.1.1.2" xref="S2.E10.m1.1.1.2a.cmml">where </mtext><mo id="S2.E10.m1.1.1.1" xref="S2.E10.m1.1.1.1.cmml">⁢</mo><msub id="S2.E10.m1.1.1.3" xref="S2.E10.m1.1.1.3.cmml"><mi id="S2.E10.m1.1.1.3.2" xref="S2.E10.m1.1.1.3.2.cmml">x</mi><mi id="S2.E10.m1.1.1.3.3" xref="S2.E10.m1.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.E10.m1.1b"><apply id="S2.E10.m1.1.1.cmml" xref="S2.E10.m1.1.1"><times id="S2.E10.m1.1.1.1.cmml" xref="S2.E10.m1.1.1.1"></times><ci id="S2.E10.m1.1.1.2a.cmml" xref="S2.E10.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S2.E10.m1.1.1.2.cmml" xref="S2.E10.m1.1.1.2">where </mtext></ci><apply id="S2.E10.m1.1.1.3.cmml" xref="S2.E10.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E10.m1.1.1.3.1.cmml" xref="S2.E10.m1.1.1.3">subscript</csymbol><ci id="S2.E10.m1.1.1.3.2.cmml" xref="S2.E10.m1.1.1.3.2">𝑥</ci><ci id="S2.E10.m1.1.1.3.3.cmml" xref="S2.E10.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E10.m1.1c">\displaystyle\textit{where }x_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.E10.m1.1d">where italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\underset{k}{\operatorname{argmin}}\,\mathbf{X}^{(j)}_{i,k}." class="ltx_Math" display="inline" id="S2.E10.m2.4"><semantics id="S2.E10.m2.4a"><mrow id="S2.E10.m2.4.4.1" xref="S2.E10.m2.4.4.1.1.cmml"><mrow id="S2.E10.m2.4.4.1.1" xref="S2.E10.m2.4.4.1.1.cmml"><mi id="S2.E10.m2.4.4.1.1.2" xref="S2.E10.m2.4.4.1.1.2.cmml"></mi><mo id="S2.E10.m2.4.4.1.1.1" xref="S2.E10.m2.4.4.1.1.1.cmml">=</mo><mrow id="S2.E10.m2.4.4.1.1.3" xref="S2.E10.m2.4.4.1.1.3.cmml"><munder accentunder="true" id="S2.E10.m2.4.4.1.1.3.2" xref="S2.E10.m2.4.4.1.1.3.2.cmml"><mi id="S2.E10.m2.4.4.1.1.3.2.2" xref="S2.E10.m2.4.4.1.1.3.2.2.cmml">argmin</mi><mo id="S2.E10.m2.4.4.1.1.3.2.1" xref="S2.E10.m2.4.4.1.1.3.2.1.cmml">𝑘</mo></munder><mo id="S2.E10.m2.4.4.1.1.3.1" lspace="0.167em" xref="S2.E10.m2.4.4.1.1.3.1.cmml">⁢</mo><msubsup id="S2.E10.m2.4.4.1.1.3.3" xref="S2.E10.m2.4.4.1.1.3.3.cmml"><mi id="S2.E10.m2.4.4.1.1.3.3.2.2" xref="S2.E10.m2.4.4.1.1.3.3.2.2.cmml">𝐗</mi><mrow id="S2.E10.m2.3.3.2.4" xref="S2.E10.m2.3.3.2.3.cmml"><mi id="S2.E10.m2.2.2.1.1" xref="S2.E10.m2.2.2.1.1.cmml">i</mi><mo id="S2.E10.m2.3.3.2.4.1" xref="S2.E10.m2.3.3.2.3.cmml">,</mo><mi id="S2.E10.m2.3.3.2.2" xref="S2.E10.m2.3.3.2.2.cmml">k</mi></mrow><mrow id="S2.E10.m2.1.1.1.3" xref="S2.E10.m2.4.4.1.1.3.3.cmml"><mo id="S2.E10.m2.1.1.1.3.1" stretchy="false" xref="S2.E10.m2.4.4.1.1.3.3.cmml">(</mo><mi id="S2.E10.m2.1.1.1.1" xref="S2.E10.m2.1.1.1.1.cmml">j</mi><mo id="S2.E10.m2.1.1.1.3.2" stretchy="false" xref="S2.E10.m2.4.4.1.1.3.3.cmml">)</mo></mrow></msubsup></mrow></mrow><mo id="S2.E10.m2.4.4.1.2" lspace="0em" xref="S2.E10.m2.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E10.m2.4b"><apply id="S2.E10.m2.4.4.1.1.cmml" xref="S2.E10.m2.4.4.1"><eq id="S2.E10.m2.4.4.1.1.1.cmml" xref="S2.E10.m2.4.4.1.1.1"></eq><csymbol cd="latexml" id="S2.E10.m2.4.4.1.1.2.cmml" xref="S2.E10.m2.4.4.1.1.2">absent</csymbol><apply id="S2.E10.m2.4.4.1.1.3.cmml" xref="S2.E10.m2.4.4.1.1.3"><times id="S2.E10.m2.4.4.1.1.3.1.cmml" xref="S2.E10.m2.4.4.1.1.3.1"></times><apply id="S2.E10.m2.4.4.1.1.3.2.cmml" xref="S2.E10.m2.4.4.1.1.3.2"><ci id="S2.E10.m2.4.4.1.1.3.2.1.cmml" xref="S2.E10.m2.4.4.1.1.3.2.1">𝑘</ci><ci id="S2.E10.m2.4.4.1.1.3.2.2.cmml" xref="S2.E10.m2.4.4.1.1.3.2.2">argmin</ci></apply><apply id="S2.E10.m2.4.4.1.1.3.3.cmml" xref="S2.E10.m2.4.4.1.1.3.3"><csymbol cd="ambiguous" id="S2.E10.m2.4.4.1.1.3.3.1.cmml" xref="S2.E10.m2.4.4.1.1.3.3">subscript</csymbol><apply id="S2.E10.m2.4.4.1.1.3.3.2.cmml" xref="S2.E10.m2.4.4.1.1.3.3"><csymbol cd="ambiguous" id="S2.E10.m2.4.4.1.1.3.3.2.1.cmml" xref="S2.E10.m2.4.4.1.1.3.3">superscript</csymbol><ci id="S2.E10.m2.4.4.1.1.3.3.2.2.cmml" xref="S2.E10.m2.4.4.1.1.3.3.2.2">𝐗</ci><ci id="S2.E10.m2.1.1.1.1.cmml" xref="S2.E10.m2.1.1.1.1">𝑗</ci></apply><list id="S2.E10.m2.3.3.2.3.cmml" xref="S2.E10.m2.3.3.2.4"><ci id="S2.E10.m2.2.2.1.1.cmml" xref="S2.E10.m2.2.2.1.1">𝑖</ci><ci id="S2.E10.m2.3.3.2.2.cmml" xref="S2.E10.m2.3.3.2.2">𝑘</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E10.m2.4c">\displaystyle=\underset{k}{\operatorname{argmin}}\,\mathbf{X}^{(j)}_{i,k}.</annotation><annotation encoding="application/x-llamapun" id="S2.E10.m2.4d">= underitalic_k start_ARG roman_argmin end_ARG bold_X start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_k end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.SSS1.p1.14">It is important to note that, as <math alttext="M" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.14.m1.1"><semantics id="S2.SS3.SSS1.p1.14.m1.1a"><mi id="S2.SS3.SSS1.p1.14.m1.1.1" xref="S2.SS3.SSS1.p1.14.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.14.m1.1b"><ci id="S2.SS3.SSS1.p1.14.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.14.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.14.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.14.m1.1d">italic_M</annotation></semantics></math> increases, the approximated representation tends to be finer.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2. </span>Additive Quantization (AQ) <cite class="ltx_cite ltx_citemacro_citep">(Babenko and Lempitsky, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib3" title="">2014</a>)</cite>
</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.5">Similar to residual quantization, additive quantization aims to approximate the target vectors by aggregating one selected code per codebook. However, residual quantization employs a greedy approach by selecting only the <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS2.p1.5.1">nearest</em> neighbor (i.e., <math alttext="\mathbf{c}^{(j)}_{x_{j}}" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.1.m1.1"><semantics id="S2.SS3.SSS2.p1.1.m1.1a"><msubsup id="S2.SS3.SSS2.p1.1.m1.1.2" xref="S2.SS3.SSS2.p1.1.m1.1.2.cmml"><mi id="S2.SS3.SSS2.p1.1.m1.1.2.2.2" xref="S2.SS3.SSS2.p1.1.m1.1.2.2.2.cmml">𝐜</mi><msub id="S2.SS3.SSS2.p1.1.m1.1.2.3" xref="S2.SS3.SSS2.p1.1.m1.1.2.3.cmml"><mi id="S2.SS3.SSS2.p1.1.m1.1.2.3.2" xref="S2.SS3.SSS2.p1.1.m1.1.2.3.2.cmml">x</mi><mi id="S2.SS3.SSS2.p1.1.m1.1.2.3.3" xref="S2.SS3.SSS2.p1.1.m1.1.2.3.3.cmml">j</mi></msub><mrow id="S2.SS3.SSS2.p1.1.m1.1.1.1.3" xref="S2.SS3.SSS2.p1.1.m1.1.2.cmml"><mo id="S2.SS3.SSS2.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S2.SS3.SSS2.p1.1.m1.1.2.cmml">(</mo><mi id="S2.SS3.SSS2.p1.1.m1.1.1.1.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.1.1.cmml">j</mi><mo id="S2.SS3.SSS2.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S2.SS3.SSS2.p1.1.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.1.m1.1b"><apply id="S2.SS3.SSS2.p1.1.m1.1.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.1.m1.1.2.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2">subscript</csymbol><apply id="S2.SS3.SSS2.p1.1.m1.1.2.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.1.m1.1.2.2.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2">superscript</csymbol><ci id="S2.SS3.SSS2.p1.1.m1.1.2.2.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2.2.2">𝐜</ci><ci id="S2.SS3.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.1.1">𝑗</ci></apply><apply id="S2.SS3.SSS2.p1.1.m1.1.2.3.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2.3"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.1.m1.1.2.3.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2.3">subscript</csymbol><ci id="S2.SS3.SSS2.p1.1.m1.1.2.3.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2.3.2">𝑥</ci><ci id="S2.SS3.SSS2.p1.1.m1.1.2.3.3.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.2.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.1.m1.1c">\mathbf{c}^{(j)}_{x_{j}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.1.m1.1d">bold_c start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>) within the current (i.e., <math alttext="j" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.2.m2.1"><semantics id="S2.SS3.SSS2.p1.2.m2.1a"><mi id="S2.SS3.SSS2.p1.2.m2.1.1" xref="S2.SS3.SSS2.p1.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.2.m2.1b"><ci id="S2.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.2.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.2.m2.1d">italic_j</annotation></semantics></math>-th) layer, which does not guarantee the global optimum. Instead, codebooks here are sequentially learned using beam search, where top candidate code combinations (<em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS2.p1.5.2">not the only one</em>) from the first <math alttext="j" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.3.m3.1"><semantics id="S2.SS3.SSS2.p1.3.m3.1a"><mi id="S2.SS3.SSS2.p1.3.m3.1.1" xref="S2.SS3.SSS2.p1.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.3.m3.1b"><ci id="S2.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.3.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.3.m3.1d">italic_j</annotation></semantics></math> codebooks are selected to infer the <math alttext="(j+1)" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.4.m4.1"><semantics id="S2.SS3.SSS2.p1.4.m4.1a"><mrow id="S2.SS3.SSS2.p1.4.m4.1.1.1" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.cmml"><mo id="S2.SS3.SSS2.p1.4.m4.1.1.1.2" stretchy="false" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.SSS2.p1.4.m4.1.1.1.1" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.cmml"><mi id="S2.SS3.SSS2.p1.4.m4.1.1.1.1.2" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.2.cmml">j</mi><mo id="S2.SS3.SSS2.p1.4.m4.1.1.1.1.1" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.1.cmml">+</mo><mn id="S2.SS3.SSS2.p1.4.m4.1.1.1.1.3" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo id="S2.SS3.SSS2.p1.4.m4.1.1.1.3" stretchy="false" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.4.m4.1b"><apply id="S2.SS3.SSS2.p1.4.m4.1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.4.m4.1.1.1"><plus id="S2.SS3.SSS2.p1.4.m4.1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.1"></plus><ci id="S2.SS3.SSS2.p1.4.m4.1.1.1.1.2.cmml" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.2">𝑗</ci><cn id="S2.SS3.SSS2.p1.4.m4.1.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS2.p1.4.m4.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.4.m4.1c">(j+1)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.4.m4.1d">( italic_j + 1 )</annotation></semantics></math>-th codebook. Hence, the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.5.m5.1"><semantics id="S2.SS3.SSS2.p1.5.m5.1a"><mi id="S2.SS3.SSS2.p1.5.m5.1.1" xref="S2.SS3.SSS2.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.5.m5.1b"><ci id="S2.SS3.SSS2.p1.5.m5.1.1.cmml" xref="S2.SS3.SSS2.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.5.m5.1d">italic_i</annotation></semantics></math>-th original vector can be approximated as in Equation <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.E9" title="In 2.3.1. Residual Quantization (RQ) (Juang and Gray, 1982; Martinez et al., 2014) ‣ 2.3. Sequential Vector Quantization ‣ 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Differentiable Vector Quantization</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The technique of VQ fundamentally includes a non-differentiable procedure, which entails identifying the nearest code in the codebook, consequently making the calculation of gradients impractical. This lack of differentiability presents a substantial hurdle in neural network training, which relies heavily on gradient-based optimization methods. Consequently, in the wake of the VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Van Den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib79" title="">2017</a>)</cite>, numerous research initiatives <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib35" title="">2020</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite> have adopted the Straight-Through Estimator (STE) <cite class="ltx_cite ltx_citemacro_citep">(Bengio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib6" title="">2013</a>)</cite> as a leading solution to this challenge.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.2">The core idea of STE is relatively straightforward: during the forward pass of a network, the non-differentiable operation (like quantization) is performed as usual. However, during the backward pass, when gradients are propagated back through the network, STE allows gradients to “pass through” the non-differentiable operation as if it were differentiable. This is typically done by approximating the derivative of the non-differentiable operation with a constant value, often 1, which can be defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(11)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{\partial\mathbf{c}_{x}}{\partial\mathbf{e}_{i}}\approx\frac{\partial%
\mathbf{e}_{i}}{\partial\mathbf{e}_{i}}=\mathbf{I}," class="ltx_Math" display="block" id="S2.E11.m1.1"><semantics id="S2.E11.m1.1a"><mrow id="S2.E11.m1.1.1.1" xref="S2.E11.m1.1.1.1.1.cmml"><mrow id="S2.E11.m1.1.1.1.1" xref="S2.E11.m1.1.1.1.1.cmml"><mfrac id="S2.E11.m1.1.1.1.1.2" xref="S2.E11.m1.1.1.1.1.2.cmml"><mrow id="S2.E11.m1.1.1.1.1.2.2" xref="S2.E11.m1.1.1.1.1.2.2.cmml"><mo id="S2.E11.m1.1.1.1.1.2.2.1" rspace="0em" xref="S2.E11.m1.1.1.1.1.2.2.1.cmml">∂</mo><msub id="S2.E11.m1.1.1.1.1.2.2.2" xref="S2.E11.m1.1.1.1.1.2.2.2.cmml"><mi id="S2.E11.m1.1.1.1.1.2.2.2.2" xref="S2.E11.m1.1.1.1.1.2.2.2.2.cmml">𝐜</mi><mi id="S2.E11.m1.1.1.1.1.2.2.2.3" xref="S2.E11.m1.1.1.1.1.2.2.2.3.cmml">x</mi></msub></mrow><mrow id="S2.E11.m1.1.1.1.1.2.3" xref="S2.E11.m1.1.1.1.1.2.3.cmml"><mo id="S2.E11.m1.1.1.1.1.2.3.1" rspace="0em" xref="S2.E11.m1.1.1.1.1.2.3.1.cmml">∂</mo><msub id="S2.E11.m1.1.1.1.1.2.3.2" xref="S2.E11.m1.1.1.1.1.2.3.2.cmml"><mi id="S2.E11.m1.1.1.1.1.2.3.2.2" xref="S2.E11.m1.1.1.1.1.2.3.2.2.cmml">𝐞</mi><mi id="S2.E11.m1.1.1.1.1.2.3.2.3" xref="S2.E11.m1.1.1.1.1.2.3.2.3.cmml">i</mi></msub></mrow></mfrac><mo id="S2.E11.m1.1.1.1.1.3" xref="S2.E11.m1.1.1.1.1.3.cmml">≈</mo><mfrac id="S2.E11.m1.1.1.1.1.4" xref="S2.E11.m1.1.1.1.1.4.cmml"><mrow id="S2.E11.m1.1.1.1.1.4.2" xref="S2.E11.m1.1.1.1.1.4.2.cmml"><mo id="S2.E11.m1.1.1.1.1.4.2.1" rspace="0em" xref="S2.E11.m1.1.1.1.1.4.2.1.cmml">∂</mo><msub id="S2.E11.m1.1.1.1.1.4.2.2" xref="S2.E11.m1.1.1.1.1.4.2.2.cmml"><mi id="S2.E11.m1.1.1.1.1.4.2.2.2" xref="S2.E11.m1.1.1.1.1.4.2.2.2.cmml">𝐞</mi><mi id="S2.E11.m1.1.1.1.1.4.2.2.3" xref="S2.E11.m1.1.1.1.1.4.2.2.3.cmml">i</mi></msub></mrow><mrow id="S2.E11.m1.1.1.1.1.4.3" xref="S2.E11.m1.1.1.1.1.4.3.cmml"><mo id="S2.E11.m1.1.1.1.1.4.3.1" rspace="0em" xref="S2.E11.m1.1.1.1.1.4.3.1.cmml">∂</mo><msub id="S2.E11.m1.1.1.1.1.4.3.2" xref="S2.E11.m1.1.1.1.1.4.3.2.cmml"><mi id="S2.E11.m1.1.1.1.1.4.3.2.2" xref="S2.E11.m1.1.1.1.1.4.3.2.2.cmml">𝐞</mi><mi id="S2.E11.m1.1.1.1.1.4.3.2.3" xref="S2.E11.m1.1.1.1.1.4.3.2.3.cmml">i</mi></msub></mrow></mfrac><mo id="S2.E11.m1.1.1.1.1.5" xref="S2.E11.m1.1.1.1.1.5.cmml">=</mo><mi id="S2.E11.m1.1.1.1.1.6" xref="S2.E11.m1.1.1.1.1.6.cmml">𝐈</mi></mrow><mo id="S2.E11.m1.1.1.1.2" xref="S2.E11.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E11.m1.1b"><apply id="S2.E11.m1.1.1.1.1.cmml" xref="S2.E11.m1.1.1.1"><and id="S2.E11.m1.1.1.1.1a.cmml" xref="S2.E11.m1.1.1.1"></and><apply id="S2.E11.m1.1.1.1.1b.cmml" xref="S2.E11.m1.1.1.1"><approx id="S2.E11.m1.1.1.1.1.3.cmml" xref="S2.E11.m1.1.1.1.1.3"></approx><apply id="S2.E11.m1.1.1.1.1.2.cmml" xref="S2.E11.m1.1.1.1.1.2"><divide id="S2.E11.m1.1.1.1.1.2.1.cmml" xref="S2.E11.m1.1.1.1.1.2"></divide><apply id="S2.E11.m1.1.1.1.1.2.2.cmml" xref="S2.E11.m1.1.1.1.1.2.2"><partialdiff id="S2.E11.m1.1.1.1.1.2.2.1.cmml" xref="S2.E11.m1.1.1.1.1.2.2.1"></partialdiff><apply id="S2.E11.m1.1.1.1.1.2.2.2.cmml" xref="S2.E11.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E11.m1.1.1.1.1.2.2.2.1.cmml" xref="S2.E11.m1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E11.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.E11.m1.1.1.1.1.2.2.2.2">𝐜</ci><ci id="S2.E11.m1.1.1.1.1.2.2.2.3.cmml" xref="S2.E11.m1.1.1.1.1.2.2.2.3">𝑥</ci></apply></apply><apply id="S2.E11.m1.1.1.1.1.2.3.cmml" xref="S2.E11.m1.1.1.1.1.2.3"><partialdiff id="S2.E11.m1.1.1.1.1.2.3.1.cmml" xref="S2.E11.m1.1.1.1.1.2.3.1"></partialdiff><apply id="S2.E11.m1.1.1.1.1.2.3.2.cmml" xref="S2.E11.m1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.E11.m1.1.1.1.1.2.3.2.1.cmml" xref="S2.E11.m1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S2.E11.m1.1.1.1.1.2.3.2.2.cmml" xref="S2.E11.m1.1.1.1.1.2.3.2.2">𝐞</ci><ci id="S2.E11.m1.1.1.1.1.2.3.2.3.cmml" xref="S2.E11.m1.1.1.1.1.2.3.2.3">𝑖</ci></apply></apply></apply><apply id="S2.E11.m1.1.1.1.1.4.cmml" xref="S2.E11.m1.1.1.1.1.4"><divide id="S2.E11.m1.1.1.1.1.4.1.cmml" xref="S2.E11.m1.1.1.1.1.4"></divide><apply id="S2.E11.m1.1.1.1.1.4.2.cmml" xref="S2.E11.m1.1.1.1.1.4.2"><partialdiff id="S2.E11.m1.1.1.1.1.4.2.1.cmml" xref="S2.E11.m1.1.1.1.1.4.2.1"></partialdiff><apply id="S2.E11.m1.1.1.1.1.4.2.2.cmml" xref="S2.E11.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S2.E11.m1.1.1.1.1.4.2.2.1.cmml" xref="S2.E11.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S2.E11.m1.1.1.1.1.4.2.2.2.cmml" xref="S2.E11.m1.1.1.1.1.4.2.2.2">𝐞</ci><ci id="S2.E11.m1.1.1.1.1.4.2.2.3.cmml" xref="S2.E11.m1.1.1.1.1.4.2.2.3">𝑖</ci></apply></apply><apply id="S2.E11.m1.1.1.1.1.4.3.cmml" xref="S2.E11.m1.1.1.1.1.4.3"><partialdiff id="S2.E11.m1.1.1.1.1.4.3.1.cmml" xref="S2.E11.m1.1.1.1.1.4.3.1"></partialdiff><apply id="S2.E11.m1.1.1.1.1.4.3.2.cmml" xref="S2.E11.m1.1.1.1.1.4.3.2"><csymbol cd="ambiguous" id="S2.E11.m1.1.1.1.1.4.3.2.1.cmml" xref="S2.E11.m1.1.1.1.1.4.3.2">subscript</csymbol><ci id="S2.E11.m1.1.1.1.1.4.3.2.2.cmml" xref="S2.E11.m1.1.1.1.1.4.3.2.2">𝐞</ci><ci id="S2.E11.m1.1.1.1.1.4.3.2.3.cmml" xref="S2.E11.m1.1.1.1.1.4.3.2.3">𝑖</ci></apply></apply></apply></apply><apply id="S2.E11.m1.1.1.1.1c.cmml" xref="S2.E11.m1.1.1.1"><eq id="S2.E11.m1.1.1.1.1.5.cmml" xref="S2.E11.m1.1.1.1.1.5"></eq><share href="https://arxiv.org/html/2405.03110v1#S2.E11.m1.1.1.1.1.4.cmml" id="S2.E11.m1.1.1.1.1d.cmml" xref="S2.E11.m1.1.1.1"></share><ci id="S2.E11.m1.1.1.1.1.6.cmml" xref="S2.E11.m1.1.1.1.1.6">𝐈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E11.m1.1c">\frac{\partial\mathbf{c}_{x}}{\partial\mathbf{e}_{i}}\approx\frac{\partial%
\mathbf{e}_{i}}{\partial\mathbf{e}_{i}}=\mathbf{I},</annotation><annotation encoding="application/x-llamapun" id="S2.E11.m1.1d">divide start_ARG ∂ bold_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ≈ divide start_ARG ∂ bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG = bold_I ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.p2.1">where <math alttext="\mathbf{I}" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><mi id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><ci id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">\mathbf{I}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">bold_I</annotation></semantics></math> is the identity matrix.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">However, training with straight-through estimator often encounters the codebook collapse issue, wherein a significant portion of codes fails to map onto corresponding vectors. Various strategies, such as employing exponential moving average (EMA) <cite class="ltx_cite ltx_citemacro_citep">(Łańcucki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib39" title="">2020</a>)</cite> during training or implementing codebook reset <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib88" title="">2021</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite> mechanisms, have been developed to address this challenge.</p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1">In the above discussion, we have reviewed established vector quantization techniques, but have not delved into recent innovations such as finite scalar quantization (FSQ) <cite class="ltx_cite ltx_citemacro_citep">(Donahue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib17" title="">2019</a>; Dieleman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib16" title="">2021</a>; Mentzer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib58" title="">2023</a>)</cite>. Drawing inspiration from model quantization <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib70" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib10" title="">2023b</a>; Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib87" title="">2023</a>)</cite>, FSQ adopts a straightforward rounding mechanism to approximate the value in each dimension of a vector. FSQ has yielded competitive results comparable to those achieved by VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Mentzer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib58" title="">2023</a>)</cite> in image generation. While FSQ has not yet been applied to recommender systems, it presents a promising avenue for future exploration.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="473" id="S2.F3.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Integration of VQ techniques with the recommender system at different training stages.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Taxonomies of VQ4Rec</h2>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>A list of representative VQ4Rec methods and their features.
“Modality” denotes the type of feature utilized, and “Task” refers to the specific training tasks employed in these methods. Note that all papers pertaining to the post-processing stage are task-agnostic, hence the “-” in the table for these entries. We use “CTR”, “NIP”, “CF”, and “Multi” to denote “click-through rate prediction”, “next item prediction”, “collaborative filtering” and “multiple” tasks, respectively.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:272pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-157.1pt,98.3pt) scale(0.579898710380961,0.579898710380961) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1">Application</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.2" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">Paper</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">Venue</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.4" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.4.1">VQ Type</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.5" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.5.1">VQ Target</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.6" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.6.1">Modality</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.7" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.7.1">Stage</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1.8" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.8.1">Task</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S3.T2.1.1.2.1" style="background-color:#F4EFC2;padding:1pt 4.0pt;"><span class="ltx_text" id="S3.T2.1.1.2.1.1" style="background-color:#F4EFC2;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.1.1.1">Efficiency-Oriented</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.1" rowspan="4" style="padding:1pt 4.0pt;"><span class="ltx_text" id="S3.T2.1.1.3.1.1"><span class="ltx_text" id="S3.T2.1.1.3.1.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.3.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.3.1.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.3.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.3.1.1.2.1.1.1" style="padding:1pt 4.0pt;">Space</span></span>
<span class="ltx_tr" id="S3.T2.1.1.3.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.3.1.1.2.1.2.1" style="padding:1pt 4.0pt;">Compression</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.3.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib50" title="">2024c</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3" style="padding:1pt 4.0pt;">TheWebConf (2024)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.3.4" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.3.4.1" style="color:#7E3838;">Sequential / RQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.3.5" style="padding:1pt 4.0pt;">Item &amp; User</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.3.6" style="padding:1pt 4.0pt;">ID &amp; Text</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.3.7" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.3.8" style="padding:1pt 4.0pt;">CTR</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.4.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Imran et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib28" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.4.2" style="padding:1pt 4.0pt;">TOIS (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.4.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.4.3.1" style="color:#3F793A;">Standard VQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.4.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.4.5" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.4.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.4.7" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.5.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Kang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib35" title="">2020</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.5.2" style="padding:1pt 4.0pt;">TheWebConf (2020)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.5.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.5.3.1" style="color:#465581;">Parallel / PQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.5.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.5.5" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.5.6" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.5.7" style="padding:1pt 4.0pt;">Multi</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.6.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Van Balen and Levy (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib78" title="">2019</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.6.2" style="padding:1pt 4.0pt;">RecSys (2019)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.6.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.6.3.1" style="color:#465581;">Parallel / PQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.6.4" style="padding:1pt 4.0pt;">User</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.6.5" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.6.6" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.6.7" style="padding:1pt 4.0pt;">CF</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.7.1" style="padding:1pt 4.0pt;">Model Acc.</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.7.2" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Wu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.7.3" style="padding:1pt 4.0pt;">TheWebConf (2021)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.7.4" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.7.4.1" style="color:#3F793A;">Standard VQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.7.5" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.7.6" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.7.7" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.7.8" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.8.1" rowspan="7" style="padding:1pt 4.0pt;"><span class="ltx_text" id="S3.T2.1.1.8.1.1"><span class="ltx_text" id="S3.T2.1.1.8.1.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.8.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.8.1.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.8.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.8.1.1.2.1.1.1" style="padding:1pt 4.0pt;">Similarity</span></span>
<span class="ltx_tr" id="S3.T2.1.1.8.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.8.1.1.2.1.2.1" style="padding:1pt 4.0pt;">Search</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.8.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.8.2" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Su et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib74" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.8.3" style="padding:1pt 4.0pt;">SIGIR (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.8.4" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.8.4.1" style="color:#3F793A;">Parallel / PQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.8.5" style="padding:1pt 4.0pt;">User</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.8.6" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.8.7" style="padding:1pt 4.0pt;">Post</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.8.8" style="padding:1pt 4.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.9.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib90" title="">2023a</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.9.2" style="padding:1pt 4.0pt;">AAAI (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.9.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.9.3.1" style="color:#465581;">Parallel / PQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.9.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.9.5" style="padding:1pt 4.0pt;">ID &amp; Text</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.9.6" style="padding:1pt 4.0pt;">Post</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.9.7" style="padding:1pt 4.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.10.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Lu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib53" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.10.2" style="padding:1pt 4.0pt;">TheWebConf (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.10.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.10.3.1" style="color:#465581;">Parallel / OPQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.10.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.10.5" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.10.6" style="padding:1pt 4.0pt;">Post</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.10.7" style="padding:1pt 4.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.11.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Zhao et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib93" title="">2021</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.11.2" style="padding:1pt 4.0pt;">KDD-IRS (2021)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.11.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.11.3.1" style="color:#465581;">Parallel / OPQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.11.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.11.5" style="padding:1pt 4.0pt;">Text</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.11.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.11.7" style="padding:1pt 4.0pt;">CTR</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.12.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Lian et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib42" title="">2020b</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.12.2" style="padding:1pt 4.0pt;">TKDE (2020)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.12.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.12.3.1" style="color:#465581;">Parallel / OPQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.12.4" style="padding:1pt 4.0pt;">Item &amp; User</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.12.5" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.12.6" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.12.7" style="padding:1pt 4.0pt;">CF</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.13.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Lian et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib41" title="">2020a</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.13.2" style="padding:1pt 4.0pt;">TheWebConf (2020)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.13.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.13.3.1" style="color:#7E3838;">Sequential / RQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.13.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.13.5" style="padding:1pt 4.0pt;">ID &amp; Text</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.13.6" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.13.7" style="padding:1pt 4.0pt;">CF</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.14.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Huang and Jenor (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib26" title="">2004</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.14.2" style="padding:1pt 4.0pt;">ICME (2004)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.14.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.14.3.1" style="color:#3F793A;">Standard VQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.14.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.14.5" style="padding:1pt 4.0pt;">Music</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.14.6" style="padding:1pt 4.0pt;">Post</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.14.7" style="padding:1pt 4.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.15">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S3.T2.1.1.15.1" style="background-color:#F4EFC2;padding:1pt 4.0pt;"><span class="ltx_text" id="S3.T2.1.1.15.1.1" style="background-color:#F4EFC2;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.15.1.1.1">Quality-Oriented</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.16.1" rowspan="3" style="padding:1pt 4.0pt;"><span class="ltx_text" id="S3.T2.1.1.16.1.1"><span class="ltx_text" id="S3.T2.1.1.16.1.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.16.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.16.1.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.16.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.16.1.1.2.1.1.1" style="padding:1pt 4.0pt;">Feature</span></span>
<span class="ltx_tr" id="S3.T2.1.1.16.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.16.1.1.2.1.2.1" style="padding:1pt 4.0pt;">Enhancement</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.16.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.16.2" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib49" title="">2024b</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.16.3" style="padding:1pt 4.0pt;">TheWebConf (2024)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.16.4" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.16.4.1" style="color:#3F793A;">Standard VQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.16.5" style="padding:1pt 4.0pt;">Item &amp; User</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.16.6" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.16.7" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.16.8" style="padding:1pt 4.0pt;">Multi</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.17.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib54" title="">2024</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.17.2" style="padding:1pt 4.0pt;">arXiv (2024)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.17.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.17.3.1" style="color:#3F793A;">Standard VQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.17.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.17.5" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.17.6" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.17.7" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.18.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Pan et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib64" title="">2021</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.18.2" style="padding:1pt 4.0pt;">arXiv (2021)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.18.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.18.3.1" style="color:#3F793A;">Standard VQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.18.4" style="padding:1pt 4.0pt;">User</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.18.5" style="padding:1pt 4.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.18.6" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.18.7" style="padding:1pt 4.0pt;">CTR</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.19">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.19.1" rowspan="2" style="padding:1pt 4.0pt;"><span class="ltx_text" id="S3.T2.1.1.19.1.1"><span class="ltx_text" id="S3.T2.1.1.19.1.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.19.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.19.1.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.19.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.19.1.1.2.1.1.1" style="padding:1pt 4.0pt;">Modality</span></span>
<span class="ltx_tr" id="S3.T2.1.1.19.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.19.1.1.2.1.2.1" style="padding:1pt 4.0pt;">Alignment</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.19.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.19.2" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Hu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib25" title="">2024</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.19.3" style="padding:1pt 4.0pt;">ECIR (2024)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.19.4" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.19.4.1" style="color:#465581;">Parallel / PQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.19.5" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.19.6" style="padding:1pt 4.0pt;">Image &amp; Text &amp; ID</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.19.7" style="padding:1pt 4.0pt;">In</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.19.8" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.20">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.20.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Hou et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib23" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.20.2" style="padding:1pt 4.0pt;">TheWebConf (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.20.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.20.3.1" style="color:#465581;">Parallel / OPQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.20.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.20.5" style="padding:1pt 4.0pt;">Text</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.20.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.20.7" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.21">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.21.1" rowspan="6" style="padding:1pt 4.0pt;"><span class="ltx_text" id="S3.T2.1.1.21.1.1"><span class="ltx_text" id="S3.T2.1.1.21.1.1.1"></span> <span class="ltx_text" id="S3.T2.1.1.21.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.21.1.1.2.1">
<span class="ltx_tr" id="S3.T2.1.1.21.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.21.1.1.2.1.1.1" style="padding:1pt 4.0pt;">Discrete</span></span>
<span class="ltx_tr" id="S3.T2.1.1.21.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.21.1.1.2.1.2.1" style="padding:1pt 4.0pt;">Tokenization</span></span>
</span></span> <span class="ltx_text" id="S3.T2.1.1.21.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.21.2" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Zheng et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib94" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.21.3" style="padding:1pt 4.0pt;">ICDE (2024)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.21.4" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.21.4.1" style="color:#7E3838;">Sequential / RQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.21.5" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.21.6" style="padding:1pt 4.0pt;">Text</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.21.7" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.21.8" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.22.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib45" title="">2024d</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.22.2" style="padding:1pt 4.0pt;">arXiv (2024)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.22.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.22.3.1" style="color:#7E3838;">Sequential / RQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.22.4" style="padding:1pt 4.0pt;">Item &amp; User</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.22.5" style="padding:1pt 4.0pt;">Graph</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.22.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.22.7" style="padding:1pt 4.0pt;">CF</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.23">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.23.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Jin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib32" title="">2024</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.23.2" style="padding:1pt 4.0pt;">arXiv (2024)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.23.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.23.3.1" style="color:#7E3838;">Sequential / RQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.23.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.23.5" style="padding:1pt 4.0pt;">Text</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.23.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.23.7" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.24">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.24.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Rajput et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.24.2" style="padding:1pt 4.0pt;">NeurIPS (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.24.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.24.3.1" style="color:#7E3838;">Sequential / RQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.24.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.24.5" style="padding:1pt 4.0pt;">Text</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.24.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.24.7" style="padding:1pt 4.0pt;">NIP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.25">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.25.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Singh et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib71" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.25.2" style="padding:1pt 4.0pt;">arXiv (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.25.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.25.3.1" style="color:#7E3838;">Sequential / RQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.25.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.25.5" style="padding:1pt 4.0pt;">Video</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.25.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.25.7" style="padding:1pt 4.0pt;">CTR</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.26">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.26.1" style="padding:1pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Jin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib31" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.26.2" style="padding:1pt 4.0pt;">arXiv (2023)</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.26.3" style="padding:1pt 4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T2.1.1.26.3.1" style="color:#3F793A;">Standard VQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.26.4" style="padding:1pt 4.0pt;">Item</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.26.5" style="padding:1pt 4.0pt;">Text</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.26.6" style="padding:1pt 4.0pt;">Pre</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.1.26.7" style="padding:1pt 4.0pt;">NIP</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To comprehensively understand the current advances in VQ4Rec, in this section, we categorize previous studies from multiple viewpoints, such as training phase or application scenario, to encapsulate the diverse methodologies and applications in this field.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Classification by Training Phase</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">VQ techniques can be applied to recommender systems at different training stages: pre-processing, in-processing, and post-processing, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S2.F3" title="Figure 3 ‣ 2.4. Differentiable Vector Quantization ‣ 2. Overview of VQ Techniques ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Pre-processing:</span> In this stage, VQ techniques are utilized to optimize or compress input data, such as item features or user sequences, resulting in static quantized inputs for recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib93" title="">2021</a>; Imran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib28" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib23" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">In-processing:</span> Here, VQ is integrated to and trained together with the recommender system, providing dynamically quantized features to enhance the functionality of the system <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib35" title="">2020</a>; Van Balen and Levy, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib78" title="">2019</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Post-processing:</span> This involves applying VQ to the embeddings generated by the recommender systems, aiming to improve search speed or accuracy <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib90" title="">2023a</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib53" title="">2023</a>; Huang and Jenor, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib26" title="">2004</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Classification by Application Scenario</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The use of VQ in recommender systems can be broadly classified into two major scenarios: one that prioritizes efficiency and another that emphasizes quality. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#S3.F4" title="Figure 4 ‣ 3.3. Other Classification Frameworks ‣ 3. Taxonomies of VQ4Rec ‣ Vector Quantization for Recommender Systems: A Review and Outlook"><span class="ltx_text ltx_ref_tag">4</span></a>, each scenario addresses distinct challenges and objectives inherent to the recommender system, leveraging the strengths of VQ to enhance the overall performance and user experience.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Efficiency-oriented approaches</span> primarily focus on enhancing the computational and storage aspects of recommender systems. In this fast-evolving digital era, where data volume and complexity are ever-increasing, these approaches play a instrumental role in maintaining the scalability and responsiveness of recommendation services. They are particularly pertinent in scenarios such as similarity search <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib90" title="">2023a</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib53" title="">2023</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib93" title="">2021</a>)</cite>, space compression <cite class="ltx_cite ltx_citemacro_citep">(Imran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib28" title="">2023</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib35" title="">2020</a>; Van Balen and Levy, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib78" title="">2019</a>)</cite>, and time acceleration <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>; Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib41" title="">2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib42" title="">b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Conversely, <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">quality-oriented approaches</span> aim to enhance the accuracy and relevance of the recommendations. These methods leverage VQ to refine the data and model representations, thereby improving the quality of the output provided to the end-users. They are relevant in scenarios involving feature enhancement <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib49" title="">2024b</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib64" title="">2021</a>)</cite>, modality correlation <cite class="ltx_cite ltx_citemacro_citep">(Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib23" title="">2023</a>)</cite>, and item tokenization <cite class="ltx_cite ltx_citemacro_citep">(Razavi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib66" title="">2019</a>; Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib71" title="">2023</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib31" title="">2023</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib94" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Other Classification Frameworks</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Here, we expand our perspective to explore additional classification frameworks for VQ4Rec. This includes:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Classification by VQ Technique</span>: As previously mentioned, existing studies generally adopt three types of VQ techniques: <span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.2">Standard VQ</span>, as seen in works like <cite class="ltx_cite ltx_citemacro_citep">(Huang and Jenor, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib26" title="">2004</a>; Imran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib28" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.3">Parallel VQ</span>, featured in studies <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib90" title="">2023a</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib53" title="">2023</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib93" title="">2021</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.4">Sequential VQ</span>, highlighted in references <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib41" title="">2020a</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>; Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib71" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Classification by Quantization Target:</span> The majority of existing research has focused on <span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.2">Item Quantization</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang and Jenor, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib26" title="">2004</a>; Imran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib28" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>)</cite>. This is likely because item features are usually static, whereas user preferences are dynamic. Additionally, the need to compress extensive item datasets due to their large scale and rich content has been a driving factor. Nonetheless, there is also some research on <span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.3">User Quantization</span> <cite class="ltx_cite ltx_citemacro_citep">(Van Balen and Levy, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib78" title="">2019</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib64" title="">2021</a>)</cite>, as well as studies that investigate both <span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.4">Item &amp; User Quantization</span> <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib42" title="">2020b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib49" title="">2024b</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="485" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>
Categorization of VQ4Rec methods based on application scenario. The node colors denote different VQ techniques employed. The <span class="ltx_text ltx_font_typewriter" id="S3.F4.7.1" style="color:#3F793A;">standard</span>, <span class="ltx_text ltx_font_typewriter" id="S3.F4.8.2" style="color:#465581;">parallel</span>, and <span class="ltx_text ltx_font_typewriter" id="S3.F4.9.3" style="color:#7E3838;">sequential</span> VQ techniques are denoted by <span class="ltx_text" id="S3.F4.10.4" style="color:#3F793A;">green</span>, <span class="ltx_text" id="S3.F4.11.5" style="color:#465581;">blue</span>, and <span class="ltx_text" id="S3.F4.12.6" style="color:#7E3838;">red</span>, respectively. The overlap between nodes indicates that the application scenarios they represent share certain similarities.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Efficiency-oriented Approaches</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Efficiency in machine learning is crucial for enhancing model speed and optimizing resource use in environments with limited computational power <cite class="ltx_cite ltx_citemacro_citep">(Schwartz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib69" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib51" title="">2024e</a>)</cite>. Advances in technology have led to various solutions to improve model efficiency <cite class="ltx_cite ltx_citemacro_citep">(Menghani, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib57" title="">2023</a>)</cite>, such as model pruning <cite class="ltx_cite ltx_citemacro_citep">(Beel and Brunel, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib5" title="">2019</a>)</cite>,
model distillation <cite class="ltx_cite ltx_citemacro_citep">(Tang and Wang, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib75" title="">2018</a>)</cite>,
and model quantization <cite class="ltx_cite ltx_citemacro_citep">(Ko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib36" title="">2021</a>)</cite>.
Moreover, adopting efficient architectures like parameter-efficient finetuning <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib48" title="">2024a</a>)</cite> or linear attention networks <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib47" title="">2023</a>)</cite> optimizes training and inference processes without increasing space requirements.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">VQ enhances the efficiency of recommender systems with its superior clustering capabilities, being widely used and verified in similarity search, space compression, and model acceleration scenarios.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Space Compression</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Recommender systems typically create a unique embedding vector for each user or item, leading to high memory costs with large datasets. For example, 1 billion users would need 238 GB for 64-dimensional vectors in 32-bit floating point <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib13" title="">2023a</a>)</cite>. To mitigate these costs, techniques like hashing <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib92" title="">2018</a>)</cite> and low-rank factorization <cite class="ltx_cite ltx_citemacro_citep">(Koren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib37" title="">2009</a>)</cite> have been used. However, hashing can cause information loss due to hash collisions, while low-rank factorization might overlook complex data patterns, reducing model accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">One line of research focuses on quantizing and condensing <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.1">sequential data</em>, such as user behavior or item content, using a variational autoencoder mechanism inspired by VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Van Den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib79" title="">2017</a>)</cite> in image generation. These methods integrate sequential knowledge into a unified representation, subsequently compressed into discrete codes. For example, <cite class="ltx_cite ltx_citemacro_citet">Van Balen and Levy (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib78" title="">2019</a>)</cite> introduced PQ-VAE, employing product quantization to derive discrete user representations from user-item interactions for quick prediction of click-through rates. Similarly, ReFRS <cite class="ltx_cite ltx_citemacro_citep">(Imran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib28" title="">2023</a>)</cite> uses a variational autoencoder within a federated learning framework to learn user tokens for decentralized recommendations. Recently, <cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib50" title="">2024c</a>)</cite> introduces residual quantization to condense both user history and item content into short tokens. Compared with embedding-based models, caching these tokens would achieve about 100x space compression rate. Another research approach directly applies VQ to existing <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.2">embedding tables</em>, as exemplified by MGQE <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib35" title="">2020</a>)</cite>, which utilizes differentiable VQ for item embeddings.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">These methods often also accelerate training and inference through more streamlined model architectures. However, VQ techniques have yet to be empirically tested for space compression in large-scale recommendation models, where their feasibility may be challenged by high embedding dimensions.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Model Acceleration</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Prior section has investigated methods for enhancing training and inference efficiency through space compression and dimensionality reduction.
Here, we focus on summarizing research aimed at accelerating the model architecture.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Transformers and attention mechanisms <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib80" title="">2017</a>)</cite>, fundamental to many influential models, exhibit inference efficiency that scales quadratically with sequence length. Consequently, significant researches have been directed toward developing attention modules that operate with linear time complexity. Techniques such as low-rank matrix decomposition (used in Linformer <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib81" title="">2020</a>)</cite> and Performer <cite class="ltx_cite ltx_citemacro_citep">(Choromanski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib14" title="">2020</a>)</cite>) and hashing for matching attention values (used in EcoFormer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib46" title="">2022a</a>)</cite>) have been explored. Additionally, VQ, which applies clustering to condense the attention matrix space, has demonstrated efficacy in fields like time series forecasting and natural language processing.
Notably, Wu et al. propose LISA <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>)</cite> which expedites inference for long-sequence recommender systems. Compared with existing approaches which apply sparse attention patterns where crucial information may be lost, LISA combines the effectiveness of self-attention and the efficiency of sparse attention, enabling full contextual attention through codeword histograms.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Currently, the application of VQ for model optimization and acceleration remains limited.
However, VQ-based linear attention modules are likely to gain popularity with the increase in long sequence features and the emergence of lifelong learning in the era of big data. Additionally, recent studies have employed VQ for the identification and compression of graph structures, followed by distillation of the compressed features into MLP format <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib86" title="">2023</a>)</cite>.
This approach enhances the processing of graph structural information, offering potential benefits for graph-based recommender systems, such as in social recommendation contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Similarity Search</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Similarity search, which relies on recommendation models for learning user and item representations, enables the retrieval of similar users and items. In 2004, <cite class="ltx_cite ltx_citemacro_citet">Huang and Jenor (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib26" title="">2004</a>)</cite> first highlighted the robust matching capabilities of VQ for music recommendation, categorizing new music representations into pre-existing groups using nearest neighbor search. However, conducting exhaustive maximum inner product searches (MIPS) is often costly and impractical with a large number of candidates. To mitigate these issues, a substantial body of research has focused on approximate nearest neighbor search (ANNs) and MIPS techniques, including hashing <cite class="ltx_cite ltx_citemacro_citep">(Neyshabur and Srebro, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib61" title="">2015</a>)</cite>, tree search <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib18" title="">2023</a>)</cite>, and graph search <cite class="ltx_cite ltx_citemacro_citep">(Morozov and Babenko, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib59" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In 2010, <cite class="ltx_cite ltx_citemacro_citet">Jegou et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib30" title="">2010</a>)</cite> pioneered a novel solution in the similarity search domain by employing a divide-and-conquer strategy, which involved subdividing vectors into sub-vectors followed by quantization. This product quantization based method facilitates rapid estimation of approximate distances between vectors represented by codes, through the pre-computation of distance tables for each code. This efficient technique for approximate nearest neighbors (ANNs) quickly became a mainstream approach in similarity search, including <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.1.1">item-item search</em> <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib33" title="">2019</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib53" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib90" title="">2023a</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib93" title="">2021</a>)</cite> and <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.1.2">user-item search</em> <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib42" title="">2020b</a>; Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib74" title="">2023</a>)</cite>. Beyond parallel quantization methods, <cite class="ltx_cite ltx_citemacro_citet">Lian et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib41" title="">2020a</a>)</cite> have explored sequential quantization to discretize item embeddings, thereby enhancing relevance score estimation and reducing memory requirements in recommender systems.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Parallel and sequential quantization both aim to establish one-to-one mappings between vectors and code combinations, expanding horizontally and vertically, respectively, and have been validated in similarity search. However, there is currently no method that combines these approaches to finely segment and represent vectors. Additionally, similarity search techniques for weight matrices and recent low-rank adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib24" title="">2021</a>)</cite> methods share similarities in achieving approximate effects through matrix compression. In the future, these methods may also find application in parameter-efficient finetuning <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib44" title="">2022b</a>)</cite> for recommendations, offering potential new directions for efficiency-oriented applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Quality-oriented Approaches</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Building high-quality recommender systems is imperative to effectively cater to users’ increasing information demands.
Both academia and industry have explored various strategies to this end. These strategies include data augmentation, as demonstrated by <cite class="ltx_cite ltx_citemacro_citep">(Song and Suh, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib73" title="">2022</a>)</cite>, which entails generating synthetic data from existing datasets through techniques like item masking <cite class="ltx_cite ltx_citemacro_citep">(Slokom et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib72" title="">2019</a>)</cite>. Additionally, hyperparameter tuning, exemplified by <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib83" title="">2023</a>)</cite>, automates the optimization of model settings, thereby mitigating the laborious process of grid search. Moreover, feature engineering, as elucidated by <cite class="ltx_cite ltx_citemacro_citep">(Schifferer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib68" title="">2020</a>)</cite>, enhances feature selection and data preprocessing.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">VQ enhances recommender system quality by serving as a foundational step, specifically in item indexing for generative retrieval, a process which is further detailed in discrete tokenization applications. Furthermore, VQ aligns diverse modalities with soft constraints, facilitating multimodal feature learning.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Feature Enhancement</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Presently, recommender systems face challenges in cold-start scenarios where user interactions are sparse.
By integrating features such as item combination patterns and category information through VQ, these systems can be significantly enhanced.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">To effectively utilize the data of active users, <cite class="ltx_cite ltx_citemacro_citet">Pan et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib64" title="">2021</a>)</cite> apply VQ to user interest clusters, facilitating cluster-level contrastive learning, which balances the personalization of representations between inactive and active users.
Their auto-quantized approach captures cluster-level similarities through VQ, in contrast to SimCLR proposed by  <cite class="ltx_cite ltx_citemacro_citet">Chen et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib11" title="">2020</a>)</cite>, which focuses solely on instance-level similarities.
To harness item combination patterns, <cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib54" title="">2024</a>)</cite> propose VQA, which combines neural attention mechanism and VQ to determine the attention of candidate combination patterns.
To continuously generate and optimize the entity category trees over time, another study, CAGE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib49" title="">2024b</a>)</cite> enables the simultaneous learning and refinement of categorical code representations and entity embeddings in an end-to-end manner for ID-based recommendation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">However, these efforts rely on ID-based approaches, which may not be optimal in current diverse multimodal content landscape. Exploring methods to effectively leverage VQ techniques to enhance information from text, images, and other multimodal sources, and integrating it with recommendation features, presents a promising avenue for research.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Modality Alignment</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Another interesting branch of work aims to improve modality alignment in recommender systems.
Transferable recommender systems are becoming increasingly important which can quickly adapt to new domains or scenarios.
However, ensuring the alignment of various modalities and preserving their distinct patterns throughout downstream training models remains a challenge.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Under transferable scenario, VQ can be used for loosening the binding between item <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.1">text</em> and <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.2">ID </em>representation, as a sparse representation technique. <cite class="ltx_cite ltx_citemacro_citet">Hou et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib23" title="">2023</a>)</cite> introduce VQ to represent items in a compact form, capturing the diverse characteristics of the products and addressing the transferability issues in sequential recommender systems.
In contrast, <cite class="ltx_cite ltx_citemacro_citet">Hu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib25" title="">2024</a>)</cite> employ product quantization to impose additional modality constraints, targeting the mitigation of the modality forgetting issue in two-stage sequential recommenders. This involves transforming dissected <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.3">text</em> and <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.4">visual</em> correlations into discrete codebook representations to enforce tighter constraints.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Hence, VQ serves as a potent semantic bridge, particularly with the rise of Large Language Models (LLMs), facilitating connectivity across diverse modalities or domains. However, existing approaches primarily focus on aligning two modalities. Addressing multimodal scenarios involving more than three modalities necessitates novel solutions.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Discrete Tokenization</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Tokenizing items and users in recommender systems has involved numerous strategies. Traditional methods often use atomic item identifiers (IDs), which can result in cold start problems. Later developments, inspired by document retrieval techniques like DSI <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib76" title="">2022</a>)</cite> and NCI <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib82" title="">2022</a>)</cite>, introduced tree IDs using multi-layer K-Means <cite class="ltx_cite ltx_citemacro_citep">(Krishna and Murty, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib38" title="">1999</a>)</cite> to achieve discrete yet partially shared item tokens, though semantic discrepancies remained an issue.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">To address this, one line of research applies <em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.1">embedding-level reconstruction</em> task. For example, <cite class="ltx_cite ltx_citemacro_citet">Rajput et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite> developed the TIGER model based on RQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib40" title="">2022</a>)</cite>, consisting of three steps: extracting item embeddings from content, discretizing these embeddings via residual quantization, and applying the discretized item tokens for sequence recommendation. Due to the inherent nature of residual quantization that can organise the tokens in a hierarchical manner, such approach proved highly successful and foundational for future research <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib71" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib50" title="">2024c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib45" title="">d</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib32" title="">2024</a>)</cite>. Subsequent projects like LC-REC <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib94" title="">2023</a>)</cite> expanded on this by integrating item tokens into large models, hinting at the development of foundational recommendation models. Instead, some researchers <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib31" title="">2023</a>)</cite> optimize this process further at <em class="ltx_emph ltx_font_italic" id="S5.SS3.p2.1.2">text-level reconstruction</em> by treating item tokenization as a translation task within an encoder-decoder-decoder framework, using standard VQ on the top of the first decoder outputs that also achieves substantial performance.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">However, the exploration of multimodal and multi-domain item tokenization remains limited, and this area presents a promising opportunity for advancing foundational recommender systems.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Future Directions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we discuss the current challenges and emerging opportunities for future research in VQ4Rec.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Codebook Collapse Problem</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">There are some limitations associated with the capability of VQ.
For example, the challenge of codebook collapse may arise when only a minor portion of the codebook is effectively utilized.
VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Van Den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib79" title="">2017</a>)</cite> employs STE <cite class="ltx_cite ltx_citemacro_citep">(Bengio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib6" title="">2013</a>)</cite> to grant differentiability to VQ, consequently, many entries in the codebook remain unused or underutilized, restricting the model capacity to accurately represent and reconstruct input data.
This core issue extends its impact to subsequent developments in recommender systems employing PQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Van Balen and Levy, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib78" title="">2019</a>)</cite> and RQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib40" title="">2022</a>)</cite>, which impairs the recommender system’s ability to offer varied and personalized recommendations to users as it fails to capture the diversity of the data.
At present, preliminary endeavors <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib91" title="">2023b</a>; Huh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib27" title="">2023</a>; Baykal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib4" title="">2023</a>)</cite> have yielded encouraging results, with the scholarly community being urged to continue their research efforts in this direction.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Item Discovery</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In item tokenization scenarios, the codebook space significantly exceeds the number of items in the dataset, suggesting that many potential code combinations remain untapped <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite>. Providing human-readable description for these new code combinations, especially in generative recommendation, represents a valuable direction. For instance, in product recommendations, this can help merchants develop products tailored to user demands; in video recommendations, it allows platforms to create personalized content based on the description. Currently, code training mainly relies on item embedding reconstruction tasks <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib94" title="">2023</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>)</cite>. A viable alternative is an end-to-end reconstruction task based on item content such as title and description, where new code combinations are inputted into the decoder to generate the corresponding item content.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>User Tokenization</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Current VQ encoding schemes primarily focus on item discretization and have shown success in generative recommendation scenarios. However, discretizing user representation, i.e., user tokenization, also presents significant opportunities for research. For instance, <cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib50" title="">2024c</a>)</cite> has achieved substantial storage efficiency by applying discretization to both user and item in click through rate prediction. A pressing challenge is to enhance the quality of user tokens, which could enable large models to offer personalized responses through model personalization <cite class="ltx_cite ltx_citemacro_citep">(Ning et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib62" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Multimodal Generative Recommendation</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Item semantic tokenization is currently the leading method for indexing items in generative recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib65" title="">2023</a>; Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib71" title="">2023</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib31" title="">2023</a>)</cite>. However, current methods are mostly text-based, although multimodal semantic tokenization has begun to emerge in tasks such as text-to-image <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib95" title="">2024</a>)</cite> and video segmentation <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib85" title="">2024</a>)</cite>. In the big data era, leveraging multimodal features offers a more comprehensive representation of items. Therefore, the development and application of multimodal tokenization techniques in recommender systems represents a critical advancement.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5. </span>RS–LLM Alignment</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">The significant success of large language models <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib63" title="">2023</a>)</cite> has established them as foundational elements across multiple fields. Current efforts increasingly focus on aligning object features from diverse domains with LLMs, enhancing their explainability and multimodal understanding <cite class="ltx_cite ltx_citemacro_citep">(Ge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib20" title="">2023</a>; Zhan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib89" title="">2024</a>)</cite>. For example, LC-Rec <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib94" title="">2023</a>)</cite> has successfully finetuned discretized item IDs obtained by RQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib40" title="">2022</a>)</cite> on the LLaMA model <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib77" title="">2023</a>)</cite>, validating this strategy in the recommendation domain. Future endeavors could involve integrating data from various domains to develop a foundational recommendation model with versatile skills.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6. </span>Codebook Quality Evaluation</h3>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">In some scenarios, the process of codebook generation and the recommendation task are not executed through end-to-end training. For instance, in item tokenization, item tokens are initially derived from item semantics before being evaluated in applications like sequential recommendation. Evaluating code quality through downstream tasks is both time-consuming and resource-intensive, suggesting a need for optimization. Therefore, the exploration of methodologies for assessing code quality through the comparison of generated tokens against original inputs represents a significant and promising research direction.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.7. </span>Efficient Large-scale Recommender Systems</h3>
<div class="ltx_para" id="S6.SS7.p1">
<p class="ltx_p" id="S6.SS7.p1.1">As large-scale models proliferate, the demand for efficient model training and inference is escalating within the recommendation community.
VQ is emerging as a promising tool for enhancing the efficiency of large recommender systems, alongside other popular techniques like distillation and quantization. For instance, <cite class="ltx_cite ltx_citemacro_citet">Lingle (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib43" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Wu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib84" title="">2021</a>)</cite> have demonstrated that optimizing the attention mechanism through VQ can achieve linear time complexity in image generation and recommendation task, respectively.
However, these approaches typically involve smaller models and embedding dimensions that can be efficiently handled using a single codebook. In contrast, for larger models like LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.03110v1#bib.bib77" title="">2023</a>)</cite>, which has embedding dimensions as large as 4096, the straightforward use of VQ may not be as effective.
Exploring the integration of parallel quantization techniques with linear attention could potentially offer a viable solution.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">VQ has become a pivotal element in the development of innovative solutions across various scenarios in recommender systems. With the advent of large language models, there has been a notable shift towards generative recommendation methods, where residual quantization has been widely adopted for its inherent advantages.
However, the research of VQ4Rec is still in its early stage. This paper offers a comprehensive overview of current research in VQ4Rec, highlighting both efficiency-oriented and quality-oriented approaches. Additionally, we identify and discuss the open challenges and potential avenues for advancement. We hope this survey will foster continued exploration and innovation in VQ4Rec.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Qijiong Liu is grateful to Prof. Min-Yen Kan from the National University of Singapore for his valuable comments and advice on this work during Liu’s visit to NUS.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abe et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (1990)</span>
<span class="ltx_bibblock">
Masanobu Abe, Satoshi Nakamura, Kiyohiro Shikano, and Hisao Kuwabara. 1990.

</span>
<span class="ltx_bibblock">Voice conversion through vector quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Journal of the Acoustical Society of Japan (E)</em> 11, 2 (1990), 71–76.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babenko and Lempitsky (2014)</span>
<span class="ltx_bibblock">
Artem Babenko and Victor Lempitsky. 2014.

</span>
<span class="ltx_bibblock">Additive quantization for extreme vector compression. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 931–938.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baykal et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gulcin Baykal, Melih Kandemir, and Gozde Unal. 2023.

</span>
<span class="ltx_bibblock">EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Available at SSRN 4671725</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beel and Brunel (2019)</span>
<span class="ltx_bibblock">
Joeran Beel and Victor Brunel. 2019.

</span>
<span class="ltx_bibblock">Data pruning in recommender systems research: Best-practice or malpractice.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ACM RecSys</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.

</span>
<span class="ltx_bibblock">Estimating or propagating gradients through stochastic neurons for conditional computation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">arXiv preprint arXiv:1308.3432</em> (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buzo et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (1980)</span>
<span class="ltx_bibblock">
Andrés Buzo, A Gray, R Gray, and John Markel. 1980.

</span>
<span class="ltx_bibblock">Speech coding based upon vector quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">IEEE Transactions on Acoustics, Speech, and Signal Processing</em> 28, 5 (1980), 562–574.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Yue Cao, Mingsheng Long, Jianmin Wang, and Shichen Liu. 2017.

</span>
<span class="ltx_bibblock">Deep visual-semantic quantization for efficient image retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 1328–1337.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Bin Chen, Yan Feng, Tao Dai, Jiawang Bai, Yong Jiang, Shu-Tao Xia, and Xuan Wang. 2022.

</span>
<span class="ltx_bibblock">Adversarial examples generation for deep product quantization networks on image retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 45, 2 (2022), 1388–1404.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Huiyuan Chen, Kaixiong Zhou, Kwei Herng Lai, Chin-Chia Michael Yeh, Yan Zheng, Xia Hu, and Hao Yang. 2023b.

</span>
<span class="ltx_bibblock">Hessian-aware Quantized Node Embeddings for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 757–762.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">International conference on machine learning</em>. PMLR, 1597–1607.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Yongjian Chen, Tao Guan, and Cheng Wang. 2010.

</span>
<span class="ltx_bibblock">Approximate nearest neighbor search by residual vector quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Sensors</em> 10, 12 (2010), 11259–11273.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yizhou Chen, Guangda Huzhang, Anxiang Zeng, Qingtao Yu, Hui Sun, Hengyi Li, Jingyi Li, Yabo Ni, Han Yu, and Zhiming Zhou. 2023a.

</span>
<span class="ltx_bibblock">Clustered Embedding Learning for Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">arXiv preprint arXiv:2302.01478</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choromanski et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al<span class="ltx_text" id="bib.bib14.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Rethinking attention with performers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.4.1">arXiv preprint arXiv:2009.14794</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cosman et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (1993)</span>
<span class="ltx_bibblock">
Pamela C Cosman, Karen L Oehler, Eve A Riskin, and Robert M Gray. 1993.

</span>
<span class="ltx_bibblock">Using vector quantization for image processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proc. IEEE</em> 81, 9 (1993), 1326–1341.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dieleman et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Sander Dieleman, Charlie Nash, Jesse Engel, and Karen Simonyan. 2021.

</span>
<span class="ltx_bibblock">Variable-rate discrete representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2103.06089</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Donahue et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Chris Donahue, Ian Simon, and Sander Dieleman. 2019.

</span>
<span class="ltx_bibblock">Piano genie. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 24th International Conference on Intelligent User Interfaces</em>. 160–164.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chao Feng, Defu Lian, Xiting Wang, Zheng Liu, Xing Xie, and Enhong Chen. 2023.

</span>
<span class="ltx_bibblock">Reinforcement routing on proximity graph for efficient recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">ACM Transactions on Information Systems</em> 41, 1 (2023), 1–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013.

</span>
<span class="ltx_bibblock">Optimized product quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">IEEE transactions on pattern analysis and machine intelligence</em> 36, 4 (2013), 744–755.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. 2023.

</span>
<span class="ltx_bibblock">Planting a seed of vision in large language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2307.08041</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gray (1984)</span>
<span class="ltx_bibblock">
Robert Gray. 1984.

</span>
<span class="ltx_bibblock">Vector quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">IEEE Assp Magazine</em> 1, 2 (1984), 4–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gray and Neuhoff (1998)</span>
<span class="ltx_bibblock">
Robert M. Gray and David L. Neuhoff. 1998.

</span>
<span class="ltx_bibblock">Quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE transactions on information theory</em> 44, 6 (1998), 2325–2383.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023.

</span>
<span class="ltx_bibblock">Learning vector-quantized item representation for transferable sequential recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the ACM Web Conference 2023</em>. 1162–1171.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv preprint arXiv:2106.09685</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hengchang Hu, Qijiong Liu, Chuang Li, and Min-Yen Kan. 2024.

</span>
<span class="ltx_bibblock">Lightweight Modality Adaptation to Sequential Recommendation via Correlation Supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">European Conference on Information Retrieval</em>. Springer International Publishing, Glasgow, Scotland, UK.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Jenor (2004)</span>
<span class="ltx_bibblock">
Yao-Chang Huang and Shyh-Kang Jenor. 2004.

</span>
<span class="ltx_bibblock">An audio recommendation system based on audio signature description scheme in mpeg-7 audio. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">2004 IEEE International Conference on Multimedia and Expo (ICME)(IEEE Cat. No. 04TH8763)</em>, Vol. 1. IEEE, 639–642.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huh et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Minyoung Huh, Brian Cheung, Pulkit Agrawal, and Phillip Isola. 2023.

</span>
<span class="ltx_bibblock">Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">International Conference on Machine Learning</em>. PMLR, 14096–14113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imran et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mubashir Imran, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Alexander Zhou, and Kai Zheng. 2023.

</span>
<span class="ltx_bibblock">ReFRS: Resource-efficient federated recommender system for dynamic and diversified user preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">ACM Transactions on Information Systems</em> 41, 3 (2023), 1–30.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang and Cho (2021)</span>
<span class="ltx_bibblock">
Young Kyun Jang and Nam Ik Cho. 2021.

</span>
<span class="ltx_bibblock">Self-supervised product quantization for deep unsupervised image retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>. 12085–12094.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jegou et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010.

</span>
<span class="ltx_bibblock">Product quantization for nearest neighbor search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">IEEE transactions on pattern analysis and machine intelligence</em> 33, 1 (2010), 117–128.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, et al<span class="ltx_text" id="bib.bib31.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Language Models As Semantic Indexers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.4.1">arXiv preprint arXiv:2310.07815</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mengqun Jin, Zexuan Qiu, Jieming Zhu, Zhenhua Dong, and Xiu Li. 2024.

</span>
<span class="ltx_bibblock">Contrastive Quantization based Semantic Code for Generative Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">arXiv preprint arXiv:2404.14774</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">IEEE Transactions on Big Data</em> 7, 3 (2019), 535–547.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juang and Gray (1982)</span>
<span class="ltx_bibblock">
Biing-Hwang Juang and A Gray. 1982.

</span>
<span class="ltx_bibblock">Multiple stage vector quantization for speech coding. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">ICASSP’82. IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, Vol. 7. IEEE, 597–600.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin, Lichan Hong, and Ed H Chi. 2020.

</span>
<span class="ltx_bibblock">Learning multi-granular quantized embeddings for large-vocab categorical features in recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Companion Proceedings of the Web Conference 2020</em>. 562–566.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yunyong Ko, Jae-Seo Yu, Hong-Kyun Bae, Yongjun Park, Dongwon Lee, and Sang-Wook Kim. 2021.

</span>
<span class="ltx_bibblock">MASCOT: A Quantization Framework for Efficient Matrix Factorization in Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">2021 IEEE International Conference on Data Mining (ICDM)</em>. IEEE, 290–299.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koren et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.

</span>
<span class="ltx_bibblock">Matrix factorization techniques for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Computer</em> 42, 8 (2009), 30–37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna and Murty (1999)</span>
<span class="ltx_bibblock">
K Krishna and M Narasimha Murty. 1999.

</span>
<span class="ltx_bibblock">Genetic K-means algorithm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</em> 29, 3 (1999), 433–439.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Łańcucki et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Adrian Łańcucki, Jan Chorowski, Guillaume Sanchez, Ricard Marxer, Nanxin Chen, Hans JGA Dolfing, Sameer Khurana, Tanel Alumäe, and Antoine Laurent. 2020.

</span>
<span class="ltx_bibblock">Robust training of vector quantized bottleneck models. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">2020 International Joint Conference on Neural Networks (IJCNN)</em>. IEEE, 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. 2022.

</span>
<span class="ltx_bibblock">Autoregressive image generation using residual quantization. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 11523–11532.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Defu Lian, Haoyu Wang, Zheng Liu, Jianxun Lian, Enhong Chen, and Xing Xie. 2020a.

</span>
<span class="ltx_bibblock">Lightrec: A memory and search-efficient recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of The Web Conference 2020</em>. 695–705.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Defu Lian, Xing Xie, Enhong Chen, and Hui Xiong. 2020b.

</span>
<span class="ltx_bibblock">Product quantized collaborative filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 33, 9 (2020), 3284–3296.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lingle (2023)</span>
<span class="ltx_bibblock">
Lucas D Lingle. 2023.

</span>
<span class="ltx_bibblock">Transformer-vq: Linear-time transformers via vector quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2309.16354</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022b.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 1950–1965.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Han Liu, Yinwei Wei, Xuemeng Song, Weili Guan, Yuan-Fang Li, and Liqiang Nie. 2024d.

</span>
<span class="ltx_bibblock">MMGRec: Multimodal Generative Recommendation with Transformer Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">arXiv preprint arXiv:2404.16555</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, and Bohan Zhuang. 2022a.

</span>
<span class="ltx_bibblock">Ecoformer: Energy-saving attention with linear complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 10295–10308.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Langming Liu, Liu Cai, Chi Zhang, Xiangyu Zhao, Jingtong Gao, Wanyu Wang, Yifu Lv, Wenqi Fan, Yiqi Wang, Ming He, et al<span class="ltx_text" id="bib.bib47.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Linrec: Linear attention mechanism for long-term sequential recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.4.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 289–299.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024a.

</span>
<span class="ltx_bibblock">Once: Boosting content-based recommendation with both open-and closed-source large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</em>. 452–461.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Qijiong Liu, Lu Fan, Jiaren Xiao, Jieming Zhu, and Xiao-Ming Wu. 2024b.

</span>
<span class="ltx_bibblock">Learning Category Trees for ID-Based Recommendation: Exploring the Power of Differentiable Vector Quantization. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the ACM Web Conference 2024</em>. Singapore.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, and Xiao-Ming Wu. 2024c.

</span>
<span class="ltx_bibblock">Discrete Semantic Tokenization for Deep CTR Prediction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-Ming Wu. 2024e.

</span>
<span class="ltx_bibblock">Benchmarking News Recommendation in the Era of Green AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">arXiv preprint arXiv:2403.04736</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu and Teng (1999)</span>
<span class="ltx_bibblock">
Guojun Lu and Shyhwei Teng. 1999.

</span>
<span class="ltx_bibblock">A novel image retrieval technique based on vector quantization. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of International Conference on Computational Intelligence for Modeling, Control and Automation</em>. Citeseer, 36–41.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zepu Lu, Defu Lian, Jin Zhang, Zaixi Zhang, Chao Feng, Hao Wang, and Enhong Chen. 2023.

</span>
<span class="ltx_bibblock">Differentiable Optimized Product Quantization and Beyond. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Proceedings of the ACM Web Conference 2023</em>. 3353–3363.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kai Luo, Tianshu Shen, Lan Yao, Ga Wu, Aaron Liblong, Istvan Fehervari, Ruijian An, Jawad Ahmed, Harshit Mishra, and Charu Pujari. 2024.

</span>
<span class="ltx_bibblock">Within-basket Recommendation via Neural Pattern Associator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">arXiv preprint arXiv:2401.16433</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Makhoul et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (1985)</span>
<span class="ltx_bibblock">
John Makhoul, Salim Roucos, and Herbert Gish. 1985.

</span>
<span class="ltx_bibblock">Vector quantization in speech coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">Proc. IEEE</em> 73, 11 (1985), 1551–1588.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Julieta Martinez, Holger H Hoos, and James J Little. 2014.

</span>
<span class="ltx_bibblock">Stacked quantizers for compositional vector compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">arXiv preprint arXiv:1411.2173</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Menghani (2023)</span>
<span class="ltx_bibblock">
Gaurav Menghani. 2023.

</span>
<span class="ltx_bibblock">Efficient deep learning: A survey on making deep learning models smaller, faster, and better.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Comput. Surveys</em> 55, 12 (2023), 1–37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mentzer et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. 2023.

</span>
<span class="ltx_bibblock">Finite scalar quantization: Vq-vae made simple.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">arXiv preprint arXiv:2309.15505</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morozov and Babenko (2018)</span>
<span class="ltx_bibblock">
Stanislav Morozov and Artem Babenko. 2018.

</span>
<span class="ltx_bibblock">Non-metric similarity graphs for maximum inner product search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in Neural Information Processing Systems</em> 31 (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasrabadi and King (1988)</span>
<span class="ltx_bibblock">
Nasser M Nasrabadi and Robert A King. 1988.

</span>
<span class="ltx_bibblock">Image coding using vector quantization: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">IEEE Transactions on communications</em> 36, 8 (1988), 957–971.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neyshabur and Srebro (2015)</span>
<span class="ltx_bibblock">
Behnam Neyshabur and Nathan Srebro. 2015.

</span>
<span class="ltx_bibblock">On symmetric and asymmetric lshs for inner product search. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">International Conference on Machine Learning</em>. PMLR, 1926–1934.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ning et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O’Banion, and Jun Xie. 2024.

</span>
<span class="ltx_bibblock">User-LLM: Efficient LLM Contextualization with User Embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">arXiv preprint arXiv:2402.13598</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
R OpenAI. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report. arxiv 2303.08774.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">View in Article</em> 2, 5 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yujie Pan, Jiangchao Yao, Bo Han, Kunyang Jia, Ya Zhang, and Hongxia Yang. 2021.

</span>
<span class="ltx_bibblock">Click-through rate prediction with auto-quantized contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">arXiv preprint arXiv:2109.13921</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajput et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al<span class="ltx_text" id="bib.bib65.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Recommender Systems with Generative Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.4.1">arXiv preprint arXiv:2305.05065</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Razavi et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. 2019.

</span>
<span class="ltx_bibblock">Generating diverse high-fidelity images with vq-vae-2.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">Advances in neural information processing systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabin and Gray (1984)</span>
<span class="ltx_bibblock">
ML Sabin and R Gray. 1984.

</span>
<span class="ltx_bibblock">Product code vector quantizers for waveform and voice coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">IEEE transactions on acoustics, speech, and signal processing</em> 32, 3 (1984), 474–488.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schifferer et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Benedikt Schifferer, Gilberto Titericz, Chris Deotte, Christof Henkel, Kazuki Onodera, Jiwei Liu, Bojan Tunguz, Even Oldridge, Gabriel De Souza Pereira Moreira, and Ahmet Erdem. 2020.

</span>
<span class="ltx_bibblock">GPU accelerated feature engineering and training for recommender systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the Recommender Systems Challenge 2020</em>. 16–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020.

</span>
<span class="ltx_bibblock">Green ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Commun. ACM</em> 63, 12 (2020), 54–63.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lingfeng Shi, Yuang Liu, Jun Wang, and Wei Zhang. 2023.

</span>
<span class="ltx_bibblock">Quantize Sequential Recommenders Without Private Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">Proceedings of the ACM Web Conference 2023</em>. 1043–1052.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Anima Singh, Trung Vu, Raghunandan Keshavan, Nikhil Mehta, Xinyang Yi, Lichan Hong, Lukasz Heldt, Li Wei, Ed Chi, and Maheswaran Sathiamoorthy. 2023.

</span>
<span class="ltx_bibblock">Better Generalization with Semantic IDs: A case study in Ranking for Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">arXiv preprint arXiv:2306.08121</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slokom et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Manel Slokom, Martha Larson, and Alan Hanjalic. 2019.

</span>
<span class="ltx_bibblock">Data masking for recommender systems: prediction performance and rating hiding.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song and Suh (2022)</span>
<span class="ltx_bibblock">
Joo-yeong Song and Bongwon Suh. 2022.

</span>
<span class="ltx_bibblock">Data Augmentation Strategies for Improving Sequential Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv e-prints</em> (2022), arXiv–2203.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Liangcai Su, Fan Yan, Jieming Zhu, Xi Xiao, Haoyi Duan, Zhou Zhao, Zhenhua Dong, and Ruiming Tang. 2023.

</span>
<span class="ltx_bibblock">Beyond Two-Tower Matching: Learning Sparse Retrievable Cross-Interactions for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 548–557.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Wang (2018)</span>
<span class="ltx_bibblock">
Jiaxi Tang and Ke Wang. 2018.

</span>
<span class="ltx_bibblock">Ranking distillation: Learning compact ranking models with high performance for recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 2289–2298.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al<span class="ltx_text" id="bib.bib76.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Transformer memory as a differentiable search index.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 21831–21843.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al<span class="ltx_text" id="bib.bib77.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.4.1">arXiv preprint arXiv:2307.09288</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Balen and Levy (2019)</span>
<span class="ltx_bibblock">
Jan Van Balen and Mark Levy. 2019.

</span>
<span class="ltx_bibblock">PQ-VAE: Efficient Recommendation Using Quantized Embeddings.. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">RecSys (Late-Breaking Results)</em>. 46–50.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Den Oord et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Aaron Van Den Oord, Oriol Vinyals, et al<span class="ltx_text" id="bib.bib79.3.1">.</span> 2017.

</span>
<span class="ltx_bibblock">Neural discrete representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.4.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.

</span>
<span class="ltx_bibblock">Linformer: Self-attention with linear complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">arXiv preprint arXiv:2006.04768</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, et al<span class="ltx_text" id="bib.bib82.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">A neural corpus indexer for document retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 25600–25614.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Di Wu, Bo Sun, and Mingsheng Shang. 2023.

</span>
<span class="ltx_bibblock">Hyperparameter learning for deep learning-based recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">IEEE Transactions on Services Computing</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yongji Wu, Defu Lian, Neil Zhenqiang Gong, Lu Yin, Mingyang Yin, Jingren Zhou, and Hongxia Yang. 2021.

</span>
<span class="ltx_bibblock">Linear-time self attention with codeword histogram for efficient recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">Proceedings of the Web Conference 2021</em>. 1262–1273.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yan Xia, Hai Huang, Jieming Zhu, and Zhou Zhao. 2024.

</span>
<span class="ltx_bibblock">Achieving Cross Modal Generalization with Multimodal Unified Representation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, and Jure Leskovec. 2023.

</span>
<span class="ltx_bibblock">Vqgraph: Graph vector-quantization for bridging gnns and mlps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">arXiv preprint arXiv:2308.02117</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023.

</span>
<span class="ltx_bibblock">LlamaRec: Two-stage recommendation using large language models for ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">arXiv preprint arXiv:2311.02089</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeghidour et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021.

</span>
<span class="ltx_bibblock">Soundstream: An end-to-end neural audio codec.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 30 (2021), 495–507.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhan et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al<span class="ltx_text" id="bib.bib89.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.4.1">arXiv preprint arXiv:2402.12226</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jin Zhang, Defu Lian, Haodi Zhang, Baoyun Wang, and Enhong Chen. 2023a.

</span>
<span class="ltx_bibblock">Query-Aware Quantization for Maximum Inner Product Search. In <em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 37. 4875–4883.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shijian Lu. 2023b.

</span>
<span class="ltx_bibblock">Regularized vector quantization for tokenized image synthesis. In <em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 18467–18476.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Kunpeng Zhang, Shaokun Fan, and Harry Jiannan Wang. 2018.

</span>
<span class="ltx_bibblock">An efficient recommender system using locality sensitive hashing.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jing Zhao, Jingya Wang, Madhav Sigdel, Bopeng Zhang, Phuong Hoang, Mengshu Liu, and Mohammed Korayem. 2021.

</span>
<span class="ltx_bibblock">Embedding-based recommender system for job to candidate matching on scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">arXiv preprint arXiv:2107.00221</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">arXiv preprint arXiv:2311.09049</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, and Zongqing Lu. 2024.

</span>
<span class="ltx_bibblock">UniCode: Learning a Unified Codebook for Multimodal Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">arXiv preprint arXiv:2403.09072</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May  6 02:03:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
