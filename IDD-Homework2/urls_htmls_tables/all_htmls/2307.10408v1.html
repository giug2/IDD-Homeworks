<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.10408] Explaining Autonomous Driving Actions with Visual Question Answering</title><meta property="og:description" content="The end-to-end learning ability of self-driving vehicles has achieved significant milestones over the last decade owing to rapid advances in deep learning and computer vision algorithms. However, as autonomous driving …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Explaining Autonomous Driving Actions with Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Explaining Autonomous Driving Actions with Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.10408">

<!--Generated on Wed Feb 28 17:28:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Explaining Autonomous Driving Actions with Visual Question Answering
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shahin Atakishiyev<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Mohammad Salameh<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Housam Babiker<sup id="id7.7.id3" class="ltx_sup"><span id="id7.7.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Randy Goebel<sup id="id8.8.id4" class="ltx_sup"><span id="id8.8.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"> This work was supported by
the Alberta Machine Intelligence Institute (Amii), Computing Science Department of the University of Alberta, and the Natural Sciences and Engineering Research Council of Canada (NSERC).Shahin Atakishiyev, Housam Babiker, and Randy Goebel are with the Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada.
Mohammad Salameh is with Huawei Technologies Canada Co., Ltd., Edmonton, Alberta, Canada.Correspondence: <span id="id9.9.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">shahin.atakishiyev@ualberta.ca</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">The end-to-end learning ability of self-driving vehicles has achieved significant milestones over the last decade owing to rapid advances in deep learning and computer vision algorithms. However, as autonomous driving technology is a safety-critical application of artificial intelligence (AI), road accidents and established regulatory principles necessitate the need for the explainability of intelligent action choices for self-driving vehicles. To facilitate interpretability of decision-making in autonomous driving, we present a Visual Question Answering (VQA) framework, which explains driving actions with question-answering-based causal reasoning. To do so, we first collect driving videos in a simulation environment using reinforcement learning (RL) and extract consecutive frames from this log data uniformly for five selected action categories. Further, we manually annotate the extracted frames using question-answer pairs as justifications for the actions chosen in each scenario. Finally, we evaluate the correctness of the VQA-predicted answers for actions on unseen driving scenes. The empirical results suggest that the VQA mechanism can provide support to interpret real-time decisions of autonomous vehicles and help enhance overall driving safety.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Urban autonomous driving is one of the most challenging tasks for self-driving vehicles, especially considering the potential interaction with other cars, road-crossing pedestrians, bystanders, traffic lights, and other conditions of dynamically changing environments. As highly automated vehicles increasingly rely on mapping sensory data to control the commands of a vehicle, applicable end-to-end learning techniques should be acceptably safe and computationally transparent. In particular, the remarkable success of deep learning and computer vision algorithms has expedited progress in safe autonomous driving on real roads and urban areas. For example, in February 2023, Waymo reported that their autonomous vehicle drove more than one million rider-only miles across several US cities with no reported injuries or events involving vulnerable road participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The report also describes that Waymo’s vehicle was involved in two accidents, where one of the accidents was caused by the driver of another car being distracted by their phone while approaching a red traffic light, according to Waymo’s claim. Moreover, other recently reported traffic accidents</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.10408/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of the most probable answers with softmax probability scores predicted by our VQA framework on the action of an ego vehicle.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">with self-driving cars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and resulting fatalities call for a scrutinized regulation of vehicle autonomy within a legal framework. Such road mishaps trigger safety, transparency, and other legal culpability issues. Inherently, a self-driving vehicle also needs to justify its temporal decisions with some form of explanation.
As self-driving decisions directly impact passengers on board and other road users, consumers and transportation jurisdictions intrinsically expect transparency and rely on the correctness of such decisions. As a concrete example, the European Union (EU) adopted the General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> that proposed a recital of the <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">right of explanation</span>, which entitles consumers to receive an explanation on decisions of autonomous systems. Article 22 of GDPR also describes general principles regarding stakeholders’ rights and responsibilities to use automated decision-making systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Thus, the need for the explainability of autonomous driving decisions has legal, socio-technical, psychological, and philosophical perspectives, in general <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
<br class="ltx_break">The delivery of explanations is another important topic in autonomous driving. As both consumers and engaged technical people have different backgrounds and knowledge about how self-driving cars work, it is necessary that explanations are provided in accordance with an explanation receiver’s (i.e., an explainee) relevant identity, as described in the recent surveys of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In this context, self-driving explanations must be correct, sufficiently informative, and intelligible for explainees. 
<br class="ltx_break">In this study, we propose a VQA-based explanation approach<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The source code, data, and related resources are available at https://github.com/Shahin-01/VQA-AD.</span></span></span> to justify RL-based self-driving decisions in a simulation environment. At its core, VQA is a task in the intersection of natural language processing and computer</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2307.10408/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="530" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The most common approaches to explaining autonomous driving actions.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">vision, which produces an answer to a text-based question about an image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Such an objective makes this reasoning technique intuitively applicable to autonomous driving. When humans drive or are a passenger on board, they inherently analyze real-time and upcoming traffic scenes and think about relevant causal, temporal, and descriptive questions, such as “Why is the car turning left?”, “What action will the car in the left lane perform at the T-junction?” and “What is the speed of the vehicle in front?” as examples. Getting answers to such questions by any means helps us have a reliable and safe trip. In this regard, we leverage the VQA mechanism to pose a question on an autonomous car’s chosen action within the driving scene and justify the question with a causal answer reflecting the car’s decision-making in that scenario. 
<br class="ltx_break">Motivated by this point, we build our framework as follows. We train an RL agent (i.e., an ego car) to operate in an autonomous driving environment and record its decisions (<span id="S1.p3.1.1" class="ltx_text ltx_font_italic">actions</span>) in correspondence to the video frames (<span id="S1.p3.1.2" class="ltx_text ltx_font_italic">states</span>). We then utilize a VQA system to justify actions of the ego car: the VQA framework inputs an <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">image frame</span> with a <span id="S1.p3.1.4" class="ltx_text ltx_font_italic">question</span> reflecting the action of the car in the scene and tries to predict the relevant answer for such an action (e.g., Figure <a href="#S1.F1" title="Figure 1 ‣ I INTRODUCTION ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). 
<br class="ltx_break">Overall, the main contributions of our paper can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present the first empirical study on explaining autonomous driving actions with a VQA approach.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We release a dataset of image-question-answer triplets justifying an autonomous car’s actions in the scene.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We show that connecting vision and natural language could rationalize an RL agent’s decision-making in an intelligible way.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We propose further directions to develop more rigorous VQA frameworks for explanatory self-driving actions.</p>
</div>
</li>
</ul>
<p id="S1.p3.2" class="ltx_p">The rest of the paper is organized as follows. In Section II, we provide state-of-the-art explainability approaches for autonomous driving. We then present details of the data generated by our RL agent and visual feature extraction in Section III. Finally, we report empirical results and the discussion of these results in Section IV and sum up the article with concluding remarks and future directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Since Bojarski et al.’s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> CNN-based end-to-end learning approach, the autonomous driving community has shown increasing interest in interpreting self-driving decisions. In general, primarily explored explanation provision techniques for autonomous driving are <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">visual, textual explanations</span>, <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">feature importance scores</span>, and <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">hybrid</span> or <span id="S2.p1.1.4" class="ltx_text ltx_font_italic">multimodal explanations</span> comprising two or more of these methods (see Figure  <a href="#S1.F2" title="Figure 2 ‣ I INTRODUCTION ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for the relevant classification). 
<br class="ltx_break">Visual explanations in the context of autonomous driving identify which parts of the perceived image (i.e., driving scene) have more influence on the vehicle’s decision, as justifications for the performed action <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. For instance, a visual explanation can show an image of a red traffic light captured by the vehicle’s video camera as a <span id="S2.p1.1.5" class="ltx_text ltx_font_italic">saliency map</span> (i.e., a heatmap) pointing out that the perception algorithm classified it as a primary reason for stopping. In this context, Kim and Canny proposed a causal attention-based visualization technique to show which groups of pixel values (i.e., blobs) have a true causal impact on the model’s prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. After analyzing attention maps in a post-hoc manner, they remove more than half of the blobs and analyze the model’s output. The empirical results show that the network produces more convincing and correct predictions in driving decisions, just like real drivers do in a realistic environment. Furthermore, as an augmented version of their initial work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Bojarski et al. developed <span id="S2.p1.1.6" class="ltx_text ltx_font_italic">VisualBackProp</span>, a saliency map-based visual explanation framework highlighting which sets of input pixels have more influence on a vehicle’s decisions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. They show that the VisualBackProp method correctly identifies the most important traffic elements, such as lane markings and other cars in the scene, as a basis for decision-making. In addition, VisualBackProp has been proven to be an effective interpretable approach to detecting the failure cases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> in the original vision-based end-to-end learning method of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
<br class="ltx_break">Another popular vision-based rationalization technique uses the idea of <span id="S2.p1.1.7" class="ltx_text ltx_font_italic">counterfactual visual explanations</span>. These explanations aim to identify whether changing some parts of the original image leads to a different prediction than the original prediction made on the original input. Bansal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> modified hand-crafted inputs by removing some objects in the image to see whether their introduced <span id="S2.p1.1.8" class="ltx_text ltx_font_italic">ChauffeurNet</span> makes different predictions with the altered image. A similar strategy is followed by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, where the goal is to find the <span id="S2.p1.1.9" class="ltx_text ltx_font_italic">risk objects</span> for driving. They show that manipulated</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2307.10408/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="248" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An aerial view of Town 1 and 2 on the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">removal of a pedestrian in an intersection changes the driving command from “Stop” to “Go”; thus, the pedestrian is considered a “risk object”, which causes the driving decision to change to the contrastive class. Finally, as a more recent counterfactual analysis, Jacob et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> investigated a style modification of image regions on the driving model’s predictions. The experimental study shows that their presented framework, <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">STEEX</span>, generates counterfactual explanations in case of manual interventions to the driving scene. Therefore, visual explanations can enable people to ensure that the intelligent driving system accurately senses the operational environment. 
<br class="ltx_break">Textual descriptions are another way of conveying rationales to the end-users for driving decisions. This approach generates natural language text that explains driving actions with descriptive, temporal, and causal information. The first successful textual explanation work is Kim et al.’s study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, where the authors leverage an attention-based video-to-text approach to generate textual explanations for autonomous vehicles. They further extend this work by incorporating human advice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and observation-to-action rules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to the underlying model and provide text-based explanations on performed actions. In another study, Xu et al. introduce <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">BDD-OIA <cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S2.p2.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>, an extension of the BDD100K dataset. Based on the action-inducing objects, they provide 21 explanations for a set of 4 actions (move forward, stop/slow down, turn left, turn right). Lastly, in this context, Ben Younes et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">BEEF</span>, an architecture that explains the behavior of trajectory prediction with textual justifications based on features fused from multi-levels, such as late features comprising the system-wise decisions and spatio-temporal features consisting of perceptual driving information. 
<br class="ltx_break">Feature importance scores, as well-known quantitative evaluation metrics, have also recently been investigated in various autonomous driving tasks. The applications of these methods to autonomous driving include decision trees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Shapley values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and partial dependence plots <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The primary goal of these methods in self-driving is to understand the weights and contributions of scene features used in predictive modeling across the explored self-driving tasks. 
<br class="ltx_break">Finally, except for visual, textual, and quantitative explanations, recent studies have attempted to use multi-modal explanatory techniques to convey information on the chosen course of actions of self-driving vehicles. For example, in</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2307.10408/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="288" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>State space representation of the ego car in its environment. An ideal state is that the car follows the direction of the lane within the lane.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">their two studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, Schneider et al. propose a combination of visual, textual, audio, light, and vibration feedback to provide retrospective and live explanations on action decisions of autonomous driving. These studies show that while visualization and light-based driving information improve the user experience (UX), multi-modal explanations can enhance perceived control and understanding of a vehicle’s decision-making by connecting UX, autonomous driving, and explainable AI. Moreover, real-time driving information delivered via vibration, tactile sensation, or haptic feedback with a relevant degree of an alert may have a crucial role in the smooth and timely transfer of control between a self-driving vehicle and a backup driver. 
<br class="ltx_break">With the inherent ability to reason about visual information, such as images, videos, and related multimedia data, VQA has recently been explored in several safety-critical and security-concerning domains. These works include applications to the healthcare and medical field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and visual surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Interestingly, the topic has not been investigated deeply in autonomous driving. As far as we know, there are only two instances that utilize the VQA mechanism in the transportation domain. The first is the CLEVRER dataset, which describes the collision events with video representation and reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. The other contribution is the SUTD-TrafficQA benchmark, which basically predicts traffic situations with question-answer pairs ranging from basic understanding (i.e., <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">What is the type of the road?</span>) to reverse reasoning (i.e., <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">What might have happened moments ago?</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. In our study, we focus on action-based explanations and show that question-answering-based causal event reasoning has significant benefits for explaining real-time decisions of self-driving cars. We describe the details of the framework and the experimental results in the following sections.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimental Design and Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our framework is designed in three primary steps. First, we use a deep RL agent to control an autonomous car in a simulation environment and collect a driving video from
</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Annotated question-answer pairs in our VQA framework</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">Action category</span></td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S3.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.1.1" class="ltx_p"><span id="S3.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span>
</td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.3.1" class="ltx_text ltx_font_bold">Answer</span></td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Go straight</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.1.1" class="ltx_p">Why is the car going straight?</span>
</span>
</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">Because the road is clear.</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_center">Turn left</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1" class="ltx_p">Why is the car turning to the left?</span>
</span>
</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_center">Because the road is bending to the left.</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_center">Turn left at T-junction</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.2.1.1" class="ltx_p">Why is the car turning left at T-junction?</span>
</span>
</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_align_center">Because there is no obstacle on the right side and turning left can be performed safely.</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_center">Turn right</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.2.1.1" class="ltx_p">Why is the car turning to the right?</span>
</span>
</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_align_center">Because the road is bending to the right.</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">Turn right at T-junction</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.2.1.1" class="ltx_p">Why is the car turning right at T-junction?</span>
</span>
</td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb">Because there is no obstacle on the left side and turning right can be performed safely.</td>
</tr>
</table>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2307.10408/assets/Learning_curve.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Learning curve of DDPG in Town 1 with the specified parameters. Our VQA framework is further fine-tuned on driving data collected here.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">the simulator. We then convert this recorded video to image sequences uniformly. Finally, we select five specific action categories in the extracted driving frames and annotate them using question-answer pairs that justify the car’s action in the scene (Table <a href="#S3.T1" title="TABLE I ‣ III Experimental Design and Methodology ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>). The high-level description of the components and overall architecture is provided in Figure <a href="#S3.F6" title="Figure 6 ‣ III-1 Data Collection ‣ III Experimental Design and Methodology ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Given such a setup, the objective of our architecture is to predict the correct answer to a posed question about an autonomous car’s performed action in an unseen driving scene. The details of the data collection, data annotation, and question-answering steps are described in the following subsections.</p>
</div>
<section id="S3.SS0.SSS1" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS0.SSS1.4.1.1" class="ltx_text">III-</span>1 </span>Data Collection</h3>

<div id="S3.SS0.SSS1.p1" class="ltx_para">
<p id="S3.SS0.SSS1.p1.3" class="ltx_p">To obtain driving data, we trained an RL agent (i.e., a self-driving car) on the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We used the Deep Deterministic Policy Gradient (DDPG) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> for the control of a self-driving car in a simulation environment. Control commands of automated driving have continuous actions including braking, acceleration, and steering angle which themselves can have a broad range of values as a representation. DDPG, as an augmented version of the Deep Q-learning algorithm, is particularly well-adapted for continuous action spaces and therefore is appropriate for driving control tasks. Furthermore, DDPG uses <span id="S3.SS0.SSS1.p1.3.1" class="ltx_text ltx_font_italic">experience replay</span>, a memory storing the agent’s past experiences (<math id="S3.SS0.SSS1.p1.1.m1.4" class="ltx_Math" alttext="s_{t},a_{t},r_{t},s_{t+1}" display="inline"><semantics id="S3.SS0.SSS1.p1.1.m1.4a"><mrow id="S3.SS0.SSS1.p1.1.m1.4.4.4" xref="S3.SS0.SSS1.p1.1.m1.4.4.5.cmml"><msub id="S3.SS0.SSS1.p1.1.m1.1.1.1.1" xref="S3.SS0.SSS1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS0.SSS1.p1.1.m1.1.1.1.1.2" xref="S3.SS0.SSS1.p1.1.m1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS0.SSS1.p1.1.m1.1.1.1.1.3" xref="S3.SS0.SSS1.p1.1.m1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS0.SSS1.p1.1.m1.4.4.4.5" xref="S3.SS0.SSS1.p1.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS1.p1.1.m1.2.2.2.2" xref="S3.SS0.SSS1.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS0.SSS1.p1.1.m1.2.2.2.2.2" xref="S3.SS0.SSS1.p1.1.m1.2.2.2.2.2.cmml">a</mi><mi id="S3.SS0.SSS1.p1.1.m1.2.2.2.2.3" xref="S3.SS0.SSS1.p1.1.m1.2.2.2.2.3.cmml">t</mi></msub><mo id="S3.SS0.SSS1.p1.1.m1.4.4.4.6" xref="S3.SS0.SSS1.p1.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS1.p1.1.m1.3.3.3.3" xref="S3.SS0.SSS1.p1.1.m1.3.3.3.3.cmml"><mi id="S3.SS0.SSS1.p1.1.m1.3.3.3.3.2" xref="S3.SS0.SSS1.p1.1.m1.3.3.3.3.2.cmml">r</mi><mi id="S3.SS0.SSS1.p1.1.m1.3.3.3.3.3" xref="S3.SS0.SSS1.p1.1.m1.3.3.3.3.3.cmml">t</mi></msub><mo id="S3.SS0.SSS1.p1.1.m1.4.4.4.7" xref="S3.SS0.SSS1.p1.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS1.p1.1.m1.4.4.4.4" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.cmml"><mi id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.2" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.2.cmml">s</mi><mrow id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.cmml"><mi id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.2" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.2.cmml">t</mi><mo id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.1" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.1.cmml">+</mo><mn id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.3" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.1.m1.4b"><list id="S3.SS0.SSS1.p1.1.m1.4.4.5.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4"><apply id="S3.SS0.SSS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS0.SSS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS0.SSS1.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS0.SSS1.p1.1.m1.1.1.1.1.2">𝑠</ci><ci id="S3.SS0.SSS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS0.SSS1.p1.1.m1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.SS0.SSS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS0.SSS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS0.SSS1.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS0.SSS1.p1.1.m1.2.2.2.2.2">𝑎</ci><ci id="S3.SS0.SSS1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS0.SSS1.p1.1.m1.2.2.2.2.3">𝑡</ci></apply><apply id="S3.SS0.SSS1.p1.1.m1.3.3.3.3.cmml" xref="S3.SS0.SSS1.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p1.1.m1.3.3.3.3.1.cmml" xref="S3.SS0.SSS1.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS1.p1.1.m1.3.3.3.3.2.cmml" xref="S3.SS0.SSS1.p1.1.m1.3.3.3.3.2">𝑟</ci><ci id="S3.SS0.SSS1.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS0.SSS1.p1.1.m1.3.3.3.3.3">𝑡</ci></apply><apply id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.1.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4">subscript</csymbol><ci id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.2.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.2">𝑠</ci><apply id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3"><plus id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.1.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.1"></plus><ci id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.2.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.2">𝑡</ci><cn type="integer" id="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.3.cmml" xref="S3.SS0.SSS1.p1.1.m1.4.4.4.4.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.1.m1.4c">s_{t},a_{t},r_{t},s_{t+1}</annotation></semantics></math>), out of which the algorithm can sample randomly to train the agent. This ability to reuse samples makes DDPG a computationally efficient learning approach. Moreover, DDPG has an actor-critic architecture, in which the actor learns an observation-to-action mapping, and the critic learns to evaluate the quality of an agent’s chosen actions. DDPG also uses <span id="S3.SS0.SSS1.p1.3.2" class="ltx_text ltx_font_italic">target networks</span> - the target actor network <math id="S3.SS0.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\mu^{\prime}" display="inline"><semantics id="S3.SS0.SSS1.p1.2.m2.1a"><msup id="S3.SS0.SSS1.p1.2.m2.1.1" xref="S3.SS0.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS0.SSS1.p1.2.m2.1.1.2" xref="S3.SS0.SSS1.p1.2.m2.1.1.2.cmml">μ</mi><mo id="S3.SS0.SSS1.p1.2.m2.1.1.3" xref="S3.SS0.SSS1.p1.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.2.m2.1b"><apply id="S3.SS0.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS0.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS1.p1.2.m2.1.1.2">𝜇</ci><ci id="S3.SS0.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS1.p1.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.2.m2.1c">\mu^{\prime}</annotation></semantics></math>, and target critic networks <math id="S3.SS0.SSS1.p1.3.m3.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S3.SS0.SSS1.p1.3.m3.1a"><msup id="S3.SS0.SSS1.p1.3.m3.1.1" xref="S3.SS0.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS0.SSS1.p1.3.m3.1.1.2" xref="S3.SS0.SSS1.p1.3.m3.1.1.2.cmml">Q</mi><mo id="S3.SS0.SSS1.p1.3.m3.1.1.3" xref="S3.SS0.SSS1.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.3.m3.1b"><apply id="S3.SS0.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS0.SSS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS0.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS0.SSS1.p1.3.m3.1.1.2">𝑄</ci><ci id="S3.SS0.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS0.SSS1.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.3.m3.1c">Q^{\prime}</annotation></semantics></math>. These networks are time-delayed copies of their original networks that help stabilize the training process. The parameters of target networks are updated as follows:</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The training parameters of DDPG on CARLA</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1" class="ltx_p">Actor learning rate</span>
</span>
</td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.1.1" class="ltx_p">Critic learning rate</span>
</span>
</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.3.1.1" class="ltx_p">Target network hyper-parameter</span>
</span>
</td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.4.1.1" class="ltx_p">Replay buffer size</span>
</span>
</td>
<td id="S3.T2.1.1.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.5.1.1" class="ltx_p">Batch size</span>
</span>
</td>
<td id="S3.T2.1.1.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.6.1.1" class="ltx_p">Discount factor</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T2.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.1.1" class="ltx_p">0.0001</span>
</span>
</td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T2.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.1.1" class="ltx_p">0.001</span>
</span>
</td>
<td id="S3.T2.1.2.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T2.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.3.1.1" class="ltx_p">0.001</span>
</span>
</td>
<td id="S3.T2.1.2.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T2.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.4.1.1" class="ltx_p">100000</span>
</span>
</td>
<td id="S3.T2.1.2.5" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T2.1.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.5.1.1" class="ltx_p">32</span>
</span>
</td>
<td id="S3.T2.1.2.6" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T2.1.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.6.1.1" class="ltx_p">0.99</span>
</span>
</td>
</tr>
</table>
</figure>
<div id="S3.SS0.SSS1.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\theta^{Q^{\prime}}\leftarrow\tau\theta^{Q}+(1-\tau)\theta^{Q^{\prime}}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msup id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">θ</mi><msup id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">Q</mi><mo id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">′</mo></msup></msup><mo stretchy="false" id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">←</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">τ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.3.1.cmml">​</mo><msup id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.3.3.2.cmml">θ</mi><mi id="S3.E1.m1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.3.3.3.cmml">Q</mi></msup></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">​</mo><msup id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">θ</mi><msup id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml">Q</mi><mo id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml">′</mo></msup></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">←</ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝜃</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">superscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝑄</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">′</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><plus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></plus><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><times id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">𝜏</ci><apply id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.3.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.3.3.2">𝜃</ci><ci id="S3.E1.m1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3.3">𝑄</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">𝜏</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">𝜃</ci><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">𝑄</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\theta^{Q^{\prime}}\leftarrow\tau\theta^{Q}+(1-\tau)\theta^{Q^{\prime}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS0.SSS1.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\theta^{\mu^{\prime}}\leftarrow\tau\theta^{\mu}+(1-\tau)\theta^{\mu^{\prime}}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msup id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">θ</mi><msup id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">μ</mi><mo id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">′</mo></msup></msup><mo stretchy="false" id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">←</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml">τ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.3.1.cmml">​</mo><msup id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.3.3.2.cmml">θ</mi><mi id="S3.E2.m1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.3.3.3.cmml">μ</mi></msup></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">​</mo><msup id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">θ</mi><msup id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">μ</mi><mo id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">′</mo></msup></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">←</ci><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝜃</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3">superscript</csymbol><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">𝜇</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">′</ci></apply></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><plus id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></plus><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><times id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2">𝜏</ci><apply id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.3.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.3.3.2">𝜃</ci><ci id="S3.E2.m1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3.3">𝜇</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">𝜏</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝜃</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">𝜇</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\theta^{\mu^{\prime}}\leftarrow\tau\theta^{\mu}+(1-\tau)\theta^{\mu^{\prime}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS0.SSS1.p4" class="ltx_para">
<p id="S3.SS0.SSS1.p4.2" class="ltx_p">where <math id="S3.SS0.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS0.SSS1.p4.1.m1.1a"><mi id="S3.SS0.SSS1.p4.1.m1.1.1" xref="S3.SS0.SSS1.p4.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p4.1.m1.1b"><ci id="S3.SS0.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS0.SSS1.p4.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p4.1.m1.1c">\tau</annotation></semantics></math> <math id="S3.SS0.SSS1.p4.2.m2.1" class="ltx_Math" alttext="\ll" display="inline"><semantics id="S3.SS0.SSS1.p4.2.m2.1a"><mo id="S3.SS0.SSS1.p4.2.m2.1.1" xref="S3.SS0.SSS1.p4.2.m2.1.1.cmml">≪</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p4.2.m2.1b"><csymbol cd="latexml" id="S3.SS0.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS0.SSS1.p4.2.m2.1.1">much-less-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p4.2.m2.1c">\ll</annotation></semantics></math> 1. For an effective action exploration, the term additive noise is usually added to the exploration policy and action is selected accordingly:</p>
</div>
<div id="S3.SS0.SSS1.p5" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="a_{t}=\mu(s_{t}|\theta^{\mu})+\mathcal{N}_{t}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">a</mi><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">μ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.2.cmml">s</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">|</mo><msup id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml">θ</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml">μ</mi></msup></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">+</mo><msub id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml">𝒩</mi><mi id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑎</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑡</ci></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><plus id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></plus><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">𝜇</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.2">𝑠</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2">𝜃</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3">𝜇</ci></apply></apply></apply><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2">𝒩</ci><ci id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">a_{t}=\mu(s_{t}|\theta^{\mu})+\mathcal{N}_{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS0.SSS1.p5.1" class="ltx_p">Such a learning technique enables the DDPG agent to learn a policy that maximizes its expected reward while also considering the quality of the chosen actions. 
<br class="ltx_break"><span id="S3.SS0.SSS1.p5.1.1" class="ltx_text ltx_font_italic">RL Training Details:</span> We generated driving data by training the agent on Town 1 within CARLA. Town 1 (see Figure <a href="#S2.F3" title="Figure 3 ‣ II Related Work ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, a) is a map containing straight lines, left turns, right turns, T-junctions, traffic lights, speed signs, and various stationary objects around the curbs. We first use the A* motion planning algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to generate a route with an initial and final point of a motion trajectory inside the simulated town, which shows consecutive waypoints linking these points. In our experiment, we set the number of waypoints to 15. By</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2307.10408/assets/x5.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="530" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A diagram of the proposed VQA architecture for autonomous driving.</figcaption>
</figure>
<div id="S3.SS0.SSS1.p6" class="ltx_para">
<p id="S3.SS0.SSS1.p6.1" class="ltx_p">default, the waypoints are referenced to the origin point (0,0,0) in the map. To ensure that they are referenced to the dynamic position of the self-driving car while in motion, we use Perez et al.s’ methodology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and apply a transformation matrix
to represent the state of the agent with these points, the
vehicle’s yaw angle, and its global position on the map as follows:</p>
</div>
<div id="S3.SS0.SSS1.p7" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\begin{bmatrix}\@add@centering\centering\cos\phi_{c}&amp;-\sin\phi_{c}&amp;0&amp;X_{c}\\
\sin\phi_{c}&amp;\cos\phi_{c}&amp;0&amp;Y_{c}\\
0&amp;0&amp;1&amp;Z_{c}\\
0&amp;0&amp;1&amp;1\end{bmatrix}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mtr id="S3.E4.m1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1b" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E4.m1.1.1.1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">⁡</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.cmml">ϕ</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.cmml">c</mi></msub></mrow></mtd><mtd id="S3.E4.m1.1.1.1.1c" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml"><mo rspace="0.167em" id="S3.E4.m1.1.1.1.1.1.2.1a" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">−</mo><mrow id="S3.E4.m1.1.1.1.1.1.2.1.2" xref="S3.E4.m1.1.1.1.1.1.2.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2.1.2.1" xref="S3.E4.m1.1.1.1.1.1.2.1.2.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E4.m1.1.1.1.1.1.2.1.2a" xref="S3.E4.m1.1.1.1.1.1.2.1.2.cmml">⁡</mo><msub id="S3.E4.m1.1.1.1.1.1.2.1.2.2" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2.1.2.2.2" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2.2.cmml">ϕ</mi><mi id="S3.E4.m1.1.1.1.1.1.2.1.2.2.3" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2.3.cmml">c</mi></msub></mrow></mrow></mtd><mtd id="S3.E4.m1.1.1.1.1d" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.3.1.cmml">0</mn></mtd><mtd id="S3.E4.m1.1.1.1.1e" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.4.1" xref="S3.E4.m1.1.1.1.1.1.4.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.4.1.2" xref="S3.E4.m1.1.1.1.1.1.4.1.2.cmml">X</mi><mi id="S3.E4.m1.1.1.1.1.1.4.1.3" xref="S3.E4.m1.1.1.1.1.1.4.1.3.cmml">c</mi></msub></mtd></mtr><mtr id="S3.E4.m1.1.1.1.1f" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1g" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.1.1" xref="S3.E4.m1.1.1.1.1.2.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2.1.1.1" xref="S3.E4.m1.1.1.1.1.2.1.1.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E4.m1.1.1.1.1.2.1.1a" xref="S3.E4.m1.1.1.1.1.2.1.1.cmml">⁡</mo><msub id="S3.E4.m1.1.1.1.1.2.1.1.2" xref="S3.E4.m1.1.1.1.1.2.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.1.1.2.2.cmml">ϕ</mi><mi id="S3.E4.m1.1.1.1.1.2.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.1.1.2.3.cmml">c</mi></msub></mrow></mtd><mtd id="S3.E4.m1.1.1.1.1h" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.2.1" xref="S3.E4.m1.1.1.1.1.2.2.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E4.m1.1.1.1.1.2.2.1a" xref="S3.E4.m1.1.1.1.1.2.2.1.cmml">⁡</mo><msub id="S3.E4.m1.1.1.1.1.2.2.1.2" xref="S3.E4.m1.1.1.1.1.2.2.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.1.2.2.cmml">ϕ</mi><mi id="S3.E4.m1.1.1.1.1.2.2.1.2.3" xref="S3.E4.m1.1.1.1.1.2.2.1.2.3.cmml">c</mi></msub></mrow></mtd><mtd id="S3.E4.m1.1.1.1.1i" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.2.3.1.cmml">0</mn></mtd><mtd id="S3.E4.m1.1.1.1.1j" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.2.4.1" xref="S3.E4.m1.1.1.1.1.2.4.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2.4.1.2" xref="S3.E4.m1.1.1.1.1.2.4.1.2.cmml">Y</mi><mi id="S3.E4.m1.1.1.1.1.2.4.1.3" xref="S3.E4.m1.1.1.1.1.2.4.1.3.cmml">c</mi></msub></mtd></mtr><mtr id="S3.E4.m1.1.1.1.1k" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1l" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.3.1.1" xref="S3.E4.m1.1.1.1.1.3.1.1.cmml">0</mn></mtd><mtd id="S3.E4.m1.1.1.1.1m" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.3.2.1" xref="S3.E4.m1.1.1.1.1.3.2.1.cmml">0</mn></mtd><mtd id="S3.E4.m1.1.1.1.1n" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml">1</mn></mtd><mtd id="S3.E4.m1.1.1.1.1o" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.3.4.1" xref="S3.E4.m1.1.1.1.1.3.4.1.cmml"><mi id="S3.E4.m1.1.1.1.1.3.4.1.2" xref="S3.E4.m1.1.1.1.1.3.4.1.2.cmml">Z</mi><mi id="S3.E4.m1.1.1.1.1.3.4.1.3" xref="S3.E4.m1.1.1.1.1.3.4.1.3.cmml">c</mi></msub></mtd></mtr><mtr id="S3.E4.m1.1.1.1.1p" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1q" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.4.1.1" xref="S3.E4.m1.1.1.1.1.4.1.1.cmml">0</mn></mtd><mtd id="S3.E4.m1.1.1.1.1r" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.4.2.1" xref="S3.E4.m1.1.1.1.1.4.2.1.cmml">0</mn></mtd><mtd id="S3.E4.m1.1.1.1.1s" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.4.3.1" xref="S3.E4.m1.1.1.1.1.4.3.1.cmml">1</mn></mtd><mtd id="S3.E4.m1.1.1.1.1t" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.4.4.1" xref="S3.E4.m1.1.1.1.1.4.4.1.cmml">1</mn></mtd></mtr></mtable><mo id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="latexml" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><matrixrow id="S3.E4.m1.1.1.1.1a.cmml" xref="S3.E4.m1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><cos id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"></cos><apply id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2">italic-ϕ</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3">𝑐</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1"><minus id="S3.E4.m1.1.1.1.1.1.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1"></minus><apply id="S3.E4.m1.1.1.1.1.1.2.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2"><sin id="S3.E4.m1.1.1.1.1.1.2.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.1"></sin><apply id="S3.E4.m1.1.1.1.1.1.2.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.2.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.2.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2.2">italic-ϕ</ci><ci id="S3.E4.m1.1.1.1.1.1.2.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2.3">𝑐</ci></apply></apply></apply><cn type="integer" id="S3.E4.m1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.3.1">0</cn><apply id="S3.E4.m1.1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.1.1.4.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.4.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.4.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.4.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.4.1.2">𝑋</ci><ci id="S3.E4.m1.1.1.1.1.1.4.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.4.1.3">𝑐</ci></apply></matrixrow><matrixrow id="S3.E4.m1.1.1.1.1b.cmml" xref="S3.E4.m1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1"><sin id="S3.E4.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.1"></sin><apply id="S3.E4.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.2.2">italic-ϕ</ci><ci id="S3.E4.m1.1.1.1.1.2.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.2.3">𝑐</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1"><cos id="S3.E4.m1.1.1.1.1.2.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1"></cos><apply id="S3.E4.m1.1.1.1.1.2.2.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.2.2">italic-ϕ</ci><ci id="S3.E4.m1.1.1.1.1.2.2.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.2.3">𝑐</ci></apply></apply><cn type="integer" id="S3.E4.m1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.3.1">0</cn><apply id="S3.E4.m1.1.1.1.1.2.4.1.cmml" xref="S3.E4.m1.1.1.1.1.2.4.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.4.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.4.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.4.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.4.1.2">𝑌</ci><ci id="S3.E4.m1.1.1.1.1.2.4.1.3.cmml" xref="S3.E4.m1.1.1.1.1.2.4.1.3">𝑐</ci></apply></matrixrow><matrixrow id="S3.E4.m1.1.1.1.1c.cmml" xref="S3.E4.m1.1.1.1.1"><cn type="integer" id="S3.E4.m1.1.1.1.1.3.1.1.cmml" xref="S3.E4.m1.1.1.1.1.3.1.1">0</cn><cn type="integer" id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2.1">0</cn><cn type="integer" id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1">1</cn><apply id="S3.E4.m1.1.1.1.1.3.4.1.cmml" xref="S3.E4.m1.1.1.1.1.3.4.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.4.1.1.cmml" xref="S3.E4.m1.1.1.1.1.3.4.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.4.1.2.cmml" xref="S3.E4.m1.1.1.1.1.3.4.1.2">𝑍</ci><ci id="S3.E4.m1.1.1.1.1.3.4.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3.4.1.3">𝑐</ci></apply></matrixrow><matrixrow id="S3.E4.m1.1.1.1.1d.cmml" xref="S3.E4.m1.1.1.1.1"><cn type="integer" id="S3.E4.m1.1.1.1.1.4.1.1.cmml" xref="S3.E4.m1.1.1.1.1.4.1.1">0</cn><cn type="integer" id="S3.E4.m1.1.1.1.1.4.2.1.cmml" xref="S3.E4.m1.1.1.1.1.4.2.1">0</cn><cn type="integer" id="S3.E4.m1.1.1.1.1.4.3.1.cmml" xref="S3.E4.m1.1.1.1.1.4.3.1">1</cn><cn type="integer" id="S3.E4.m1.1.1.1.1.4.4.1.cmml" xref="S3.E4.m1.1.1.1.1.4.4.1">1</cn></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\begin{bmatrix}\@add@centering\centering\cos\phi_{c}&amp;-\sin\phi_{c}&amp;0&amp;X_{c}\\
\sin\phi_{c}&amp;\cos\phi_{c}&amp;0&amp;Y_{c}\\
0&amp;0&amp;1&amp;Z_{c}\\
0&amp;0&amp;1&amp;1\end{bmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS0.SSS1.p7.4" class="ltx_p">The goal of the task is that the ego car follows this predefined route and reaches the final destination by performing the relevant actions along its trip.
<br class="ltx_break">As seen from Figure <a href="#S2.F4" title="Figure 4 ‣ II Related Work ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the agent acquires a driving vector <math id="S3.SS0.SSS1.p7.1.m1.1" class="ltx_Math" alttext="f_{t}" display="inline"><semantics id="S3.SS0.SSS1.p7.1.m1.1a"><msub id="S3.SS0.SSS1.p7.1.m1.1.1" xref="S3.SS0.SSS1.p7.1.m1.1.1.cmml"><mi id="S3.SS0.SSS1.p7.1.m1.1.1.2" xref="S3.SS0.SSS1.p7.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS0.SSS1.p7.1.m1.1.1.3" xref="S3.SS0.SSS1.p7.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p7.1.m1.1b"><apply id="S3.SS0.SSS1.p7.1.m1.1.1.cmml" xref="S3.SS0.SSS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p7.1.m1.1.1.1.cmml" xref="S3.SS0.SSS1.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS0.SSS1.p7.1.m1.1.1.2.cmml" xref="S3.SS0.SSS1.p7.1.m1.1.1.2">𝑓</ci><ci id="S3.SS0.SSS1.p7.1.m1.1.1.3.cmml" xref="S3.SS0.SSS1.p7.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p7.1.m1.1c">f_{t}</annotation></semantics></math>= (<math id="S3.SS0.SSS1.p7.2.m2.1" class="ltx_Math" alttext="v_{t}" display="inline"><semantics id="S3.SS0.SSS1.p7.2.m2.1a"><msub id="S3.SS0.SSS1.p7.2.m2.1.1" xref="S3.SS0.SSS1.p7.2.m2.1.1.cmml"><mi id="S3.SS0.SSS1.p7.2.m2.1.1.2" xref="S3.SS0.SSS1.p7.2.m2.1.1.2.cmml">v</mi><mi id="S3.SS0.SSS1.p7.2.m2.1.1.3" xref="S3.SS0.SSS1.p7.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p7.2.m2.1b"><apply id="S3.SS0.SSS1.p7.2.m2.1.1.cmml" xref="S3.SS0.SSS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p7.2.m2.1.1.1.cmml" xref="S3.SS0.SSS1.p7.2.m2.1.1">subscript</csymbol><ci id="S3.SS0.SSS1.p7.2.m2.1.1.2.cmml" xref="S3.SS0.SSS1.p7.2.m2.1.1.2">𝑣</ci><ci id="S3.SS0.SSS1.p7.2.m2.1.1.3.cmml" xref="S3.SS0.SSS1.p7.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p7.2.m2.1c">v_{t}</annotation></semantics></math>, <math id="S3.SS0.SSS1.p7.3.m3.1" class="ltx_Math" alttext="d_{t}" display="inline"><semantics id="S3.SS0.SSS1.p7.3.m3.1a"><msub id="S3.SS0.SSS1.p7.3.m3.1.1" xref="S3.SS0.SSS1.p7.3.m3.1.1.cmml"><mi id="S3.SS0.SSS1.p7.3.m3.1.1.2" xref="S3.SS0.SSS1.p7.3.m3.1.1.2.cmml">d</mi><mi id="S3.SS0.SSS1.p7.3.m3.1.1.3" xref="S3.SS0.SSS1.p7.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p7.3.m3.1b"><apply id="S3.SS0.SSS1.p7.3.m3.1.1.cmml" xref="S3.SS0.SSS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p7.3.m3.1.1.1.cmml" xref="S3.SS0.SSS1.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS0.SSS1.p7.3.m3.1.1.2.cmml" xref="S3.SS0.SSS1.p7.3.m3.1.1.2">𝑑</ci><ci id="S3.SS0.SSS1.p7.3.m3.1.1.3.cmml" xref="S3.SS0.SSS1.p7.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p7.3.m3.1c">d_{t}</annotation></semantics></math>, <math id="S3.SS0.SSS1.p7.4.m4.1" class="ltx_Math" alttext="\phi_{t}" display="inline"><semantics id="S3.SS0.SSS1.p7.4.m4.1a"><msub id="S3.SS0.SSS1.p7.4.m4.1.1" xref="S3.SS0.SSS1.p7.4.m4.1.1.cmml"><mi id="S3.SS0.SSS1.p7.4.m4.1.1.2" xref="S3.SS0.SSS1.p7.4.m4.1.1.2.cmml">ϕ</mi><mi id="S3.SS0.SSS1.p7.4.m4.1.1.3" xref="S3.SS0.SSS1.p7.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p7.4.m4.1b"><apply id="S3.SS0.SSS1.p7.4.m4.1.1.cmml" xref="S3.SS0.SSS1.p7.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS1.p7.4.m4.1.1.1.cmml" xref="S3.SS0.SSS1.p7.4.m4.1.1">subscript</csymbol><ci id="S3.SS0.SSS1.p7.4.m4.1.1.2.cmml" xref="S3.SS0.SSS1.p7.4.m4.1.1.2">italic-ϕ</ci><ci id="S3.SS0.SSS1.p7.4.m4.1.1.3.cmml" xref="S3.SS0.SSS1.p7.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p7.4.m4.1c">\phi_{t}</annotation></semantics></math>) from the simulation environment where these parameters reflect the vehicle’s velocity, lateral distance, and yaw angle, respectively. Ideally, the goal of driving is to move on in the direction of the lane as long as possible without lane departure and collisions. In this sense, the reward shaping can be conditioned for the vehicle’s 1) perfect longitudinal direction, 2) deviation from the lane direction with yaw angle, and 3) lane departure and collision. Based on these criteria, we adopt the relevant reward formulation from Perez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> for an ego car:</p>
</div>
<div id="S3.SS0.SSS1.p8" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center">
<div id="S3.E5.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:433.6pt;height:59pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.2pt,-11.9pt) scale(1.67270761515032,1.67270761515032) ;">
<p id="S3.E5.m1.1.1.1.1" class="ltx_p"><math id="S3.E5.m1.1.1.1.1.m1.6" class="ltx_Math" alttext="R=\begin{cases}-200&amp;\text{road departures or collisions},\\
\sum_{t}\left|v_{t}\cos\phi_{t}\right|-\left|v_{t}\sin\phi_{t}\right|-\left|v_{t}\right|\left|d_{t}\right|&amp;\text{ driving inside the lane},\\
+100&amp;\text{ arriving at the goal position}.\end{cases}" display="inline"><semantics id="S3.E5.m1.1.1.1.1.m1.6a"><mrow id="S3.E5.m1.1.1.1.1.m1.6.7" xref="S3.E5.m1.1.1.1.1.m1.6.7.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.6.7.2" xref="S3.E5.m1.1.1.1.1.m1.6.7.2.cmml">R</mi><mo id="S3.E5.m1.1.1.1.1.m1.6.7.1" xref="S3.E5.m1.1.1.1.1.m1.6.7.1.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.m1.6.6" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mo id="S3.E5.m1.1.1.1.1.m1.6.6.7" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E5.m1.1.1.1.1.m1.6.6.6" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mtr id="S3.E5.m1.1.1.1.1.m1.6.6.6a" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.1.1.1.1.m1.6.6.6b" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1a" xref="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml">200</mn></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.1.1.1.1.m1.6.6.6c" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.3" xref="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.1a.cmml"><mtext id="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.1" xref="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.1.cmml">road departures or collisions</mtext><mo id="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.3.1" xref="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.1a.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E5.m1.1.1.1.1.m1.6.6.6d" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.1.1.1.1.m1.6.6.6e" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.cmml"><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.2.cmml">∑</mo><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.3.cmml">t</mi></msub><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.2.cmml"><mo lspace="0em" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.2.cmml">v</mi><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo lspace="0.167em" rspace="0em" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3a" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.cmml">⁡</mo><msub id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.2.cmml">ϕ</mi><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.3.cmml">t</mi></msub></mrow></mrow><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.2.1.cmml">|</mo></mrow></mrow><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.5" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.5.cmml">−</mo><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.2.cmml"><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.2.1.cmml">|</mo><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.2.cmml">v</mi><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.3.cmml">t</mi></msub><mo lspace="0.167em" rspace="0em" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.1.cmml">​</mo><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3a" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.cmml">⁡</mo><msub id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.2.cmml">ϕ</mi><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.3.cmml">t</mi></msub></mrow></mrow><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.2.1.cmml">|</mo></mrow><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.5a" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.5.cmml">−</mo><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.2.cmml"><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.2.1.cmml">|</mo><msub id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.cmml">v</mi><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.cmml">t</mi></msub><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.2.1.cmml">|</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.3.cmml">​</mo><mrow id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.2.cmml"><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.2.1.cmml">|</mo><msub id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.2.cmml">d</mi><mi id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.cmml">t</mi></msub><mo id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.3" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.2.1.cmml">|</mo></mrow></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.1.1.1.1.m1.6.6.6f" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.3" xref="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.1a.cmml"><mtext id="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.1" xref="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.1.cmml"> driving inside the lane</mtext><mo id="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.3.1" xref="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.1a.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E5.m1.1.1.1.1.m1.6.6.6g" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.1.1.1.1.m1.6.6.6h" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1" xref="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1a" xref="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.cmml">+</mo><mn id="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.2" xref="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.2.cmml">100</mn></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.1.1.1.1.m1.6.6.6i" xref="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.3" xref="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.1a.cmml"><mtext id="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.1" xref="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.1.cmml"> arriving at the goal position</mtext><mo lspace="0em" id="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.3.1" xref="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.1a.cmml">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1.1.1.1.m1.6b"><apply id="S3.E5.m1.1.1.1.1.m1.6.7.cmml" xref="S3.E5.m1.1.1.1.1.m1.6.7"><eq id="S3.E5.m1.1.1.1.1.m1.6.7.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.6.7.1"></eq><ci id="S3.E5.m1.1.1.1.1.m1.6.7.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.6.7.2">𝑅</ci><apply id="S3.E5.m1.1.1.1.1.m1.6.7.3.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.6.6"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.m1.6.7.3.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.6.6.7">cases</csymbol><apply id="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1"><minus id="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.1.1.1.1.1.1.2">200</cn></apply><ci id="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.1a.cmml" xref="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.3"><mtext id="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.2.2.2.2.2.1.1">road departures or collisions</mtext></ci><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1"><minus id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.5.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.5"></minus><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1"><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.2"></sum><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1"><abs id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.2"></abs><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.1"></times><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.2">𝑣</ci><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3"><cos id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.1"></cos><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.2">italic-ϕ</ci><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.1.1.1.1.3.2.3">𝑡</ci></apply></apply></apply></apply></apply><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1"><abs id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.2"></abs><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1"><times id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.1"></times><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.2">𝑣</ci><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.2.3">𝑡</ci></apply><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3"><sin id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.1"></sin><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.2">italic-ϕ</ci><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.2.1.1.3.2.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4"><times id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.3"></times><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1"><abs id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.2"></abs><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.2">𝑣</ci><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.3.1.1.1.3">𝑡</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1"><abs id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.2"></abs><apply id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.2">𝑑</ci><ci id="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.m1.3.3.3.3.1.1.4.2.1.1.3">𝑡</ci></apply></apply></apply></apply><ci id="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.1a.cmml" xref="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.3"><mtext id="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.4.4.4.4.2.1.1"> driving inside the lane</mtext></ci><apply id="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1"><plus id="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1"></plus><cn type="integer" id="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.m1.5.5.5.5.1.1.2">100</cn></apply><ci id="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.1a.cmml" xref="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.3"><mtext id="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.1.cmml" xref="S3.E5.m1.1.1.1.1.m1.6.6.6.6.2.1.1"> arriving at the goal position</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1.1.1.1.m1.6c">R=\begin{cases}-200&amp;\text{road departures or collisions},\\
\sum_{t}\left|v_{t}\cos\phi_{t}\right|-\left|v_{t}\sin\phi_{t}\right|-\left|v_{t}\right|\left|d_{t}\right|&amp;\text{ driving inside the lane},\\
+100&amp;\text{ arriving at the goal position}.\end{cases}</annotation></semantics></math></p>
</span></div>
</td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS0.SSS1.p9" class="ltx_para">
<p id="S3.SS0.SSS1.p9.1" class="ltx_p">Finally, action space is continuous and can receive values from the interval [-1,1]. By defining this setting, we trained our agent in Town 1. The training parameters of DDPG can be seen in Table <a href="#S3.T2" title="TABLE II ‣ III-1 Data Collection ‣ III Experimental Design and Methodology ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2307.10408/assets/x6.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="514" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example scenarios from an ego vehicle’s field of view on CARLA. During the decision-making process of the agent, we are given visual signals and we ask action-related questions and try to find an answer given the current state. The green arrow shows the ego car’s chosen action and the white arrows indicate the other route at T-junction scenarios. We show the top 5 answers predicted by our model. The green-colored text shows the correct answer to the question for the performed action of the car. Except for the <span id="S3.F7.2.1" class="ltx_text ltx_font_italic">turn left</span> scenario, justifications for other actions are predicted correctly by the model.</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Number of correct predictions for each action category</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.1.1" class="ltx_text ltx_font_bold">Go straight</span></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.2.1" class="ltx_text ltx_font_bold">Turn left</span></td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.3.1" class="ltx_text ltx_font_bold">Turn left at T-junction</span></td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.4.1" class="ltx_text ltx_font_bold">Turn right</span></td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.5.1" class="ltx_text ltx_font_bold">Turn right at T-junction</span></td>
<td id="S3.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.6.1" class="ltx_text ltx_font_bold">Total</span></td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">20/20</td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0/20</td>
<td id="S3.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">20/20</td>
<td id="S3.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">20/20</td>
<td id="S3.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">20/20</td>
<td id="S3.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">80/100</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS0.SSS2" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS0.SSS2.4.1.1" class="ltx_text">III-</span>2 </span>Data Annotation</h3>

<div id="S3.SS0.SSS2.p1" class="ltx_para">
<p id="S3.SS0.SSS2.p1.1" class="ltx_p">As we obtained the driving video with the DDPG agent, we selected 5 action categories (<span id="S3.SS0.SSS2.p1.1.1" class="ltx_text ltx_font_italic">go straight, turn left, turn right, turn left at T-junction, and turn right at T-junction</span>), and extracted consecutive frames uniformly (30 frames per second) for 5 video segments. We then chose 10 frames from each segment. We ensured that these frames were extracted from driving segments, where the car followed the predefined route and performed the relevant action safely without lane departure or collision. We distinguish left and right turns in the current line from left and right turns at T-junction as in the latter an ego car also has an alternative route. So, our training data includes 5 action categories with 50 high-quality frames per category, denoting a total of 250 driving scenes obtained from the recorded video. We manually annotated the training data with five causal question-answer (QA) pairs (see Table <a href="#S3.T1" title="TABLE I ‣ III Experimental Design and Methodology ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) ensuring the annotations reflected the scene correctly. Each of 250 frames has its single and scenario-related annotation. As test data, we selected a collection of 100 frames from both Town 1 and Town 2 on the CARLA simulator, as the map of Town 2 is similar to Town 1. Similar to the training data annotation, we selected 20 frames for each action category from various segments of Town 1 and Town 2 and annotated each of them with a relevant QA pair. The goal is to assess the generalization ability of the employed VQA framework on these action categories in unseen traffic scenarios.</p>
</div>
</section>
<section id="S3.SS0.SSS3" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS0.SSS3.4.1.1" class="ltx_text">III-</span>3 </span>Question-Answering Framework</h3>

<div id="S3.SS0.SSS3.p1" class="ltx_para">
<p id="S3.SS0.SSS3.p1.2" class="ltx_p">On the question-answering side, we fine-tune the original VQA framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> trained on the MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. At the highest level, our VQA model takes an encoded driving image and a question embedding as input, to predict the answer (i.e., explanation) for a performed action in the scene. The model is composed of two neural networks. The first one is a multilayer-feedforward network with 2 hidden layers each containing 1000 hidden units and <span id="S3.SS0.SSS3.p1.2.1" class="ltx_text ltx_font_italic">tanh</span> activation function. We apply the dropout regularization with 0.5 in each layer. Finally, a long short-term memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> followed by a softmax layer is employed to produce an answer for the asked question about the driving action. On the image encoding part, we eliminate the output layer and use the last hidden layer of the pre-trained VGG-19 architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, producing a 4096-dimensional feature vector. Further, a linear transformation is applied to make the image features 1024-dimensional. The LSTM model for the question encoder has 2 hidden layers with 512 hidden units, and thus it is a 1024-dimensional vector, the same as image features. An interesting aspect is the unification of the question and image vectors from a mathematical perspective. Previous studies have generally either preferred the concatenation or multiplication of these vectors, but <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> have shown that multiplying the image and question encoder usually leads to a better joint representation. Consequently, given the image vector, <math id="S3.SS0.SSS3.p1.1.m1.1" class="ltx_Math" alttext="V_{i}" display="inline"><semantics id="S3.SS0.SSS3.p1.1.m1.1a"><msub id="S3.SS0.SSS3.p1.1.m1.1.1" xref="S3.SS0.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS3.p1.1.m1.1.1.2" xref="S3.SS0.SSS3.p1.1.m1.1.1.2.cmml">V</mi><mi id="S3.SS0.SSS3.p1.1.m1.1.1.3" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS3.p1.1.m1.1b"><apply id="S3.SS0.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS0.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.2">𝑉</ci><ci id="S3.SS0.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS3.p1.1.m1.1c">V_{i}</annotation></semantics></math>, and question embedding <math id="S3.SS0.SSS3.p1.2.m2.1" class="ltx_Math" alttext="V_{q}" display="inline"><semantics id="S3.SS0.SSS3.p1.2.m2.1a"><msub id="S3.SS0.SSS3.p1.2.m2.1.1" xref="S3.SS0.SSS3.p1.2.m2.1.1.cmml"><mi id="S3.SS0.SSS3.p1.2.m2.1.1.2" xref="S3.SS0.SSS3.p1.2.m2.1.1.2.cmml">V</mi><mi id="S3.SS0.SSS3.p1.2.m2.1.1.3" xref="S3.SS0.SSS3.p1.2.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS3.p1.2.m2.1b"><apply id="S3.SS0.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS0.SSS3.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS3.p1.2.m2.1.1.2">𝑉</ci><ci id="S3.SS0.SSS3.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS3.p1.2.m2.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS3.p1.2.m2.1c">V_{q}</annotation></semantics></math>, the resulting vector passed to the fully connected layer of the VQA pipeline is represented as their element-wise multiplication, as a fused feature vector:</p>
</div>
<div id="S3.SS0.SSS3.p2" class="ltx_para">
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="V_{r}=V_{i}\times V_{q}" display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><msub id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.2.2" xref="S3.E6.m1.1.1.2.2.cmml">V</mi><mi id="S3.E6.m1.1.1.2.3" xref="S3.E6.m1.1.1.2.3.cmml">r</mi></msub><mo id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml"><msub id="S3.E6.m1.1.1.3.2" xref="S3.E6.m1.1.1.3.2.cmml"><mi id="S3.E6.m1.1.1.3.2.2" xref="S3.E6.m1.1.1.3.2.2.cmml">V</mi><mi id="S3.E6.m1.1.1.3.2.3" xref="S3.E6.m1.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.1.1.3.1" xref="S3.E6.m1.1.1.3.1.cmml">×</mo><msub id="S3.E6.m1.1.1.3.3" xref="S3.E6.m1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.3.3.2" xref="S3.E6.m1.1.1.3.3.2.cmml">V</mi><mi id="S3.E6.m1.1.1.3.3.3" xref="S3.E6.m1.1.1.3.3.3.cmml">q</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"></eq><apply id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.2.2">𝑉</ci><ci id="S3.E6.m1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.2.3">𝑟</ci></apply><apply id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3"><times id="S3.E6.m1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.3.1"></times><apply id="S3.E6.m1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.3.2.2">𝑉</ci><ci id="S3.E6.m1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.3.2.3">𝑖</ci></apply><apply id="S3.E6.m1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.3.3.2">𝑉</ci><ci id="S3.E6.m1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.3.3.3">𝑞</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">V_{r}=V_{i}\times V_{q}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS0.SSS3.p3" class="ltx_para">
<p id="S3.SS0.SSS3.p3.1" class="ltx_p">We use the question and answer vocabularies of the original VQA framework, which have sizes of more than 17K unique tokens and 1000 candidate answers (which are either single tokens such as “yes,” “white,” or expressions consisting of two or more strings such as “playing video game”), respectively, obtained by descriptions from the MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> images. We customize candidate answers by adding our answers of 5 action questions to that answer vocabulary. The expectation is that our VQA model picks the most correct answer with the highest softmax probability score out of the 1K candidates for the asked “Why” question about the action within the driving scene.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental Results and Discussion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">On the data collection side, we trained the DDPG agent on the CARLA 0.9.11 version in 500 episodes using a TensorFlow backend to get a driving video. As described above, we used 250 frames from Town 1 for training our VQA network and evaluated its performance on 100 frames collected from Town 1 and Town 2 (Figure <a href="#S2.F3" title="Figure 3 ‣ II Related Work ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). We used the PyTorch backend for training and evaluating our VQA architecture. The experiments were performed on an NVIDIA RTX 3090 GPU machine with a 32 GB memory size. All the frames were set to have a size of 640 <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><times id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\times</annotation></semantics></math> 480 both in training and test. As we have ground-truth answers (see Table <a href="#S3.T1" title="TABLE I ‣ III Experimental Design and Methodology ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) for the asked question about an image, we compare the top prediction of our model on the test data (i.e., an answer with the highest softmax probability score) with these ground-truth answers. Thus, we use accuracy as an evaluation metric, which is defined as follows:</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2307.10408/assets/average_softmax_scores.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The average softmax probability scores for top predictions in each action category.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.1" class="ltx_Math" alttext="\text{Accuracy}=\frac{\#~{}frames~{}with~{}correct~{}predictions}{total~{}number~{}of~{}test~{}frames}" display="block"><semantics id="S4.E7.m1.1a"><mrow id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml"><mtext id="S4.E7.m1.1.1.2" xref="S4.E7.m1.1.1.2a.cmml">Accuracy</mtext><mo id="S4.E7.m1.1.1.1" xref="S4.E7.m1.1.1.1.cmml">=</mo><mfrac id="S4.E7.m1.1.1.3" xref="S4.E7.m1.1.1.3.cmml"><mrow id="S4.E7.m1.1.1.3.2" xref="S4.E7.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="S4.E7.m1.1.1.3.2.2" xref="S4.E7.m1.1.1.3.2.2.cmml">#</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.2.1" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.3" xref="S4.E7.m1.1.1.3.2.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1a" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.4" xref="S4.E7.m1.1.1.3.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1b" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.5" xref="S4.E7.m1.1.1.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1c" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.6" xref="S4.E7.m1.1.1.3.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1d" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.7" xref="S4.E7.m1.1.1.3.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1e" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.8" xref="S4.E7.m1.1.1.3.2.8.cmml">s</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.2.1f" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.9" xref="S4.E7.m1.1.1.3.2.9.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1g" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.10" xref="S4.E7.m1.1.1.3.2.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1h" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.11" xref="S4.E7.m1.1.1.3.2.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1i" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.12" xref="S4.E7.m1.1.1.3.2.12.cmml">h</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.2.1j" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.13" xref="S4.E7.m1.1.1.3.2.13.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1k" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.14" xref="S4.E7.m1.1.1.3.2.14.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1l" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.15" xref="S4.E7.m1.1.1.3.2.15.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1m" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.16" xref="S4.E7.m1.1.1.3.2.16.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1n" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.17" xref="S4.E7.m1.1.1.3.2.17.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1o" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.18" xref="S4.E7.m1.1.1.3.2.18.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1p" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.19" xref="S4.E7.m1.1.1.3.2.19.cmml">t</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.2.1q" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.20" xref="S4.E7.m1.1.1.3.2.20.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1r" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.21" xref="S4.E7.m1.1.1.3.2.21.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1s" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.22" xref="S4.E7.m1.1.1.3.2.22.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1t" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.23" xref="S4.E7.m1.1.1.3.2.23.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1u" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.24" xref="S4.E7.m1.1.1.3.2.24.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1v" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.25" xref="S4.E7.m1.1.1.3.2.25.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1w" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.26" xref="S4.E7.m1.1.1.3.2.26.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1x" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.27" xref="S4.E7.m1.1.1.3.2.27.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1y" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.28" xref="S4.E7.m1.1.1.3.2.28.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1z" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.29" xref="S4.E7.m1.1.1.3.2.29.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.2.1aa" xref="S4.E7.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.2.30" xref="S4.E7.m1.1.1.3.2.30.cmml">s</mi></mrow><mrow id="S4.E7.m1.1.1.3.3" xref="S4.E7.m1.1.1.3.3.cmml"><mi id="S4.E7.m1.1.1.3.3.2" xref="S4.E7.m1.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.3" xref="S4.E7.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1a" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.4" xref="S4.E7.m1.1.1.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1b" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.5" xref="S4.E7.m1.1.1.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1c" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.6" xref="S4.E7.m1.1.1.3.3.6.cmml">l</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.3.1d" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.7" xref="S4.E7.m1.1.1.3.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1e" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.8" xref="S4.E7.m1.1.1.3.3.8.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1f" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.9" xref="S4.E7.m1.1.1.3.3.9.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1g" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.10" xref="S4.E7.m1.1.1.3.3.10.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1h" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.11" xref="S4.E7.m1.1.1.3.3.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1i" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.12" xref="S4.E7.m1.1.1.3.3.12.cmml">r</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.3.1j" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.13" xref="S4.E7.m1.1.1.3.3.13.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1k" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.14" xref="S4.E7.m1.1.1.3.3.14.cmml">f</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.3.1l" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.15" xref="S4.E7.m1.1.1.3.3.15.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1m" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.16" xref="S4.E7.m1.1.1.3.3.16.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1n" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.17" xref="S4.E7.m1.1.1.3.3.17.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1o" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.18" xref="S4.E7.m1.1.1.3.3.18.cmml">t</mi><mo lspace="0.330em" rspace="0em" id="S4.E7.m1.1.1.3.3.1p" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.19" xref="S4.E7.m1.1.1.3.3.19.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1q" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.20" xref="S4.E7.m1.1.1.3.3.20.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1r" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.21" xref="S4.E7.m1.1.1.3.3.21.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1s" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.22" xref="S4.E7.m1.1.1.3.3.22.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1t" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.23" xref="S4.E7.m1.1.1.3.3.23.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.3.3.1u" xref="S4.E7.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.3.3.24" xref="S4.E7.m1.1.1.3.3.24.cmml">s</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.1b"><apply id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1"><eq id="S4.E7.m1.1.1.1.cmml" xref="S4.E7.m1.1.1.1"></eq><ci id="S4.E7.m1.1.1.2a.cmml" xref="S4.E7.m1.1.1.2"><mtext id="S4.E7.m1.1.1.2.cmml" xref="S4.E7.m1.1.1.2">Accuracy</mtext></ci><apply id="S4.E7.m1.1.1.3.cmml" xref="S4.E7.m1.1.1.3"><divide id="S4.E7.m1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.3"></divide><apply id="S4.E7.m1.1.1.3.2.cmml" xref="S4.E7.m1.1.1.3.2"><times id="S4.E7.m1.1.1.3.2.1.cmml" xref="S4.E7.m1.1.1.3.2.1"></times><ci id="S4.E7.m1.1.1.3.2.2.cmml" xref="S4.E7.m1.1.1.3.2.2">#</ci><ci id="S4.E7.m1.1.1.3.2.3.cmml" xref="S4.E7.m1.1.1.3.2.3">𝑓</ci><ci id="S4.E7.m1.1.1.3.2.4.cmml" xref="S4.E7.m1.1.1.3.2.4">𝑟</ci><ci id="S4.E7.m1.1.1.3.2.5.cmml" xref="S4.E7.m1.1.1.3.2.5">𝑎</ci><ci id="S4.E7.m1.1.1.3.2.6.cmml" xref="S4.E7.m1.1.1.3.2.6">𝑚</ci><ci id="S4.E7.m1.1.1.3.2.7.cmml" xref="S4.E7.m1.1.1.3.2.7">𝑒</ci><ci id="S4.E7.m1.1.1.3.2.8.cmml" xref="S4.E7.m1.1.1.3.2.8">𝑠</ci><ci id="S4.E7.m1.1.1.3.2.9.cmml" xref="S4.E7.m1.1.1.3.2.9">𝑤</ci><ci id="S4.E7.m1.1.1.3.2.10.cmml" xref="S4.E7.m1.1.1.3.2.10">𝑖</ci><ci id="S4.E7.m1.1.1.3.2.11.cmml" xref="S4.E7.m1.1.1.3.2.11">𝑡</ci><ci id="S4.E7.m1.1.1.3.2.12.cmml" xref="S4.E7.m1.1.1.3.2.12">ℎ</ci><ci id="S4.E7.m1.1.1.3.2.13.cmml" xref="S4.E7.m1.1.1.3.2.13">𝑐</ci><ci id="S4.E7.m1.1.1.3.2.14.cmml" xref="S4.E7.m1.1.1.3.2.14">𝑜</ci><ci id="S4.E7.m1.1.1.3.2.15.cmml" xref="S4.E7.m1.1.1.3.2.15">𝑟</ci><ci id="S4.E7.m1.1.1.3.2.16.cmml" xref="S4.E7.m1.1.1.3.2.16">𝑟</ci><ci id="S4.E7.m1.1.1.3.2.17.cmml" xref="S4.E7.m1.1.1.3.2.17">𝑒</ci><ci id="S4.E7.m1.1.1.3.2.18.cmml" xref="S4.E7.m1.1.1.3.2.18">𝑐</ci><ci id="S4.E7.m1.1.1.3.2.19.cmml" xref="S4.E7.m1.1.1.3.2.19">𝑡</ci><ci id="S4.E7.m1.1.1.3.2.20.cmml" xref="S4.E7.m1.1.1.3.2.20">𝑝</ci><ci id="S4.E7.m1.1.1.3.2.21.cmml" xref="S4.E7.m1.1.1.3.2.21">𝑟</ci><ci id="S4.E7.m1.1.1.3.2.22.cmml" xref="S4.E7.m1.1.1.3.2.22">𝑒</ci><ci id="S4.E7.m1.1.1.3.2.23.cmml" xref="S4.E7.m1.1.1.3.2.23">𝑑</ci><ci id="S4.E7.m1.1.1.3.2.24.cmml" xref="S4.E7.m1.1.1.3.2.24">𝑖</ci><ci id="S4.E7.m1.1.1.3.2.25.cmml" xref="S4.E7.m1.1.1.3.2.25">𝑐</ci><ci id="S4.E7.m1.1.1.3.2.26.cmml" xref="S4.E7.m1.1.1.3.2.26">𝑡</ci><ci id="S4.E7.m1.1.1.3.2.27.cmml" xref="S4.E7.m1.1.1.3.2.27">𝑖</ci><ci id="S4.E7.m1.1.1.3.2.28.cmml" xref="S4.E7.m1.1.1.3.2.28">𝑜</ci><ci id="S4.E7.m1.1.1.3.2.29.cmml" xref="S4.E7.m1.1.1.3.2.29">𝑛</ci><ci id="S4.E7.m1.1.1.3.2.30.cmml" xref="S4.E7.m1.1.1.3.2.30">𝑠</ci></apply><apply id="S4.E7.m1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.3.3"><times id="S4.E7.m1.1.1.3.3.1.cmml" xref="S4.E7.m1.1.1.3.3.1"></times><ci id="S4.E7.m1.1.1.3.3.2.cmml" xref="S4.E7.m1.1.1.3.3.2">𝑡</ci><ci id="S4.E7.m1.1.1.3.3.3.cmml" xref="S4.E7.m1.1.1.3.3.3">𝑜</ci><ci id="S4.E7.m1.1.1.3.3.4.cmml" xref="S4.E7.m1.1.1.3.3.4">𝑡</ci><ci id="S4.E7.m1.1.1.3.3.5.cmml" xref="S4.E7.m1.1.1.3.3.5">𝑎</ci><ci id="S4.E7.m1.1.1.3.3.6.cmml" xref="S4.E7.m1.1.1.3.3.6">𝑙</ci><ci id="S4.E7.m1.1.1.3.3.7.cmml" xref="S4.E7.m1.1.1.3.3.7">𝑛</ci><ci id="S4.E7.m1.1.1.3.3.8.cmml" xref="S4.E7.m1.1.1.3.3.8">𝑢</ci><ci id="S4.E7.m1.1.1.3.3.9.cmml" xref="S4.E7.m1.1.1.3.3.9">𝑚</ci><ci id="S4.E7.m1.1.1.3.3.10.cmml" xref="S4.E7.m1.1.1.3.3.10">𝑏</ci><ci id="S4.E7.m1.1.1.3.3.11.cmml" xref="S4.E7.m1.1.1.3.3.11">𝑒</ci><ci id="S4.E7.m1.1.1.3.3.12.cmml" xref="S4.E7.m1.1.1.3.3.12">𝑟</ci><ci id="S4.E7.m1.1.1.3.3.13.cmml" xref="S4.E7.m1.1.1.3.3.13">𝑜</ci><ci id="S4.E7.m1.1.1.3.3.14.cmml" xref="S4.E7.m1.1.1.3.3.14">𝑓</ci><ci id="S4.E7.m1.1.1.3.3.15.cmml" xref="S4.E7.m1.1.1.3.3.15">𝑡</ci><ci id="S4.E7.m1.1.1.3.3.16.cmml" xref="S4.E7.m1.1.1.3.3.16">𝑒</ci><ci id="S4.E7.m1.1.1.3.3.17.cmml" xref="S4.E7.m1.1.1.3.3.17">𝑠</ci><ci id="S4.E7.m1.1.1.3.3.18.cmml" xref="S4.E7.m1.1.1.3.3.18">𝑡</ci><ci id="S4.E7.m1.1.1.3.3.19.cmml" xref="S4.E7.m1.1.1.3.3.19">𝑓</ci><ci id="S4.E7.m1.1.1.3.3.20.cmml" xref="S4.E7.m1.1.1.3.3.20">𝑟</ci><ci id="S4.E7.m1.1.1.3.3.21.cmml" xref="S4.E7.m1.1.1.3.3.21">𝑎</ci><ci id="S4.E7.m1.1.1.3.3.22.cmml" xref="S4.E7.m1.1.1.3.3.22">𝑚</ci><ci id="S4.E7.m1.1.1.3.3.23.cmml" xref="S4.E7.m1.1.1.3.3.23">𝑒</ci><ci id="S4.E7.m1.1.1.3.3.24.cmml" xref="S4.E7.m1.1.1.3.3.24">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.1c">\text{Accuracy}=\frac{\#~{}frames~{}with~{}correct~{}predictions}{total~{}number~{}of~{}test~{}frames}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Based on this evaluation criterion, our VQA model predicted 80 correct answers to the asked questions for 100 images. Hence, the accuracy of the prediction is 0.8 or 80%.</p>
</div>
<section id="S4.SS0.SSS1" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS1.4.1.1" class="ltx_text">IV-</span>1 </span>Discussion</h3>

<div id="S4.SS0.SSS1.p1" class="ltx_para">
<p id="S4.SS0.SSS1.p1.1" class="ltx_p">Except for <span id="S4.SS0.SSS1.p1.1.1" class="ltx_text ltx_font_italic">turn left</span> actions, our model predicted explanatory answers correctly for all remaining action classes. Interestingly, in frames with turn-left scenarios, the VQA framework primarily recognized these actions as <span id="S4.SS0.SSS1.p1.1.2" class="ltx_text ltx_font_italic">turn right</span>. In Figure <a href="#S3.F7" title="Figure 7 ‣ III-1 Data Collection ‣ III Experimental Design and Methodology ‣ Explaining Autonomous Driving Actions with Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we provide exemplary driving scenes for the five action categories. As seen, the model was able to predict the highest probability scores for all actions in the scenes correctly, except for the misclassified <span id="S4.SS0.SSS1.p1.1.3" class="ltx_text ltx_font_italic">turn left</span> action in the second image. This misclassification could be due to ambiguity in the tested driving frames, the shape of curves in the scene, and road conditions in the training data. Hence, it is important to increase the size of the training data considering the shapes of road lanes and curves, lighting, and other road objects to potentially improve the accuracy of the predictions of the VQA network on self-driving actions.
<br class="ltx_break">Another implication of our work is that unifying computer vision with a natural language provides an opportunity to explain temporal actions of an RL agent. As explored in a recent study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, explaining RL in sequential decision-making problems is an important and emerging topic, particularly when explanation receivers do not have a technical background. As autonomous driving is a safety-critical application area, justifying reinforcement learning-based decisions to end users with natural language-based reasoning is an effective and easily understandable approach. A natural foundation for explainable reinforcement learning (XRL) would be to provide reward-based justifications on action decisions. However, as self-driving explanations are intended for a general community, it is essential to ensure that such explanations are intelligible and informative. While <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> has attempted to build an inherently explainable RL architecture, we build our explanations independent of an agent’s decisions. We also acknowledge the need to be cautious about providing explanations that are independent of an agent’s behavior; it is possible that post-hoc explanations may <span id="S4.SS0.SSS1.p1.1.4" class="ltx_text ltx_font_italic">not</span> always reflect an agent’s real decision-making process. For example, in an actual <span id="S4.SS0.SSS1.p1.1.5" class="ltx_text ltx_font_italic">left turn</span> scenario, a model’s response to the question “Why is the car turning to the <span id="S4.SS0.SSS1.p1.1.6" class="ltx_text ltx_font_italic">right</span>?” as ”Because the road is bending to the <span id="S4.SS0.SSS1.p1.1.7" class="ltx_text ltx_font_italic">right</span>.” may be a hallucination of a VQA architecture. Consequently, it is important to further investigate the topic of generating linguistic explanations for an agent’s actions and evaluate such explanations with human-adversarial examples as well.</p>
</div>
</section>
<section id="S4.SS0.SSS2" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS2.4.1.1" class="ltx_text">IV-</span>2 </span>Limitations</h3>

<div id="S4.SS0.SSS2.p1" class="ltx_para">
<p id="S4.SS0.SSS2.p1.1" class="ltx_p">Real roads are more complex and dynamic with the presence of traffic lights, bystanders, passengers, other vehicles, and adverse weather conditions. In the current version of our framework, the ego car only interacts with the stationary environment and explains actions associated with such interactions. Moreover, our dataset is small in size. Hence, these features are limitations of our present framework, and as a next step, we plan to work on explaining self-driving actions in more dynamic and complex scenarios with enriched data, where details are provided in the conclusions section.</p>
</div>
</section>
<section id="S4.SS0.SSS3" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS3.4.1.1" class="ltx_text">IV-</span>3 </span>Practical use cases</h3>

<div id="S4.SS0.SSS3.p1" class="ltx_para">
<p id="S4.SS0.SSS3.p1.1" class="ltx_p">In practice, the VQA mechanism can be leveraged at least in two ways on real autonomous vehicles. First, it can help passengers on board monitor driving safety by “judging” the vehicle’s decisions. For instance, a user interface or dashboard set up on a back seat may provide voice-to-text functionality, and a passenger can observe driving surrounding, ask a question about the vehicle’s chosen action, and get an answer. Such a feature can help monitor the reliability of self-driving and instill trust in vehicle autonomy during the trip. Another practical application is to retain a history of action-question-answer triplets (…<math id="S4.SS0.SSS3.p1.1.m1.6" class="ltx_Math" alttext="a_{t},q_{t},ans_{t},a_{t+1},q_{t+1},ans_{t+1}..." display="inline"><semantics id="S4.SS0.SSS3.p1.1.m1.6a"><mrow id="S4.SS0.SSS3.p1.1.m1.6.6.6" xref="S4.SS0.SSS3.p1.1.m1.6.6.7.cmml"><msub id="S4.SS0.SSS3.p1.1.m1.1.1.1.1" xref="S4.SS0.SSS3.p1.1.m1.1.1.1.1.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.1.1.1.1.2" xref="S4.SS0.SSS3.p1.1.m1.1.1.1.1.2.cmml">a</mi><mi id="S4.SS0.SSS3.p1.1.m1.1.1.1.1.3" xref="S4.SS0.SSS3.p1.1.m1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.SS0.SSS3.p1.1.m1.6.6.6.7" xref="S4.SS0.SSS3.p1.1.m1.6.6.7.cmml">,</mo><msub id="S4.SS0.SSS3.p1.1.m1.2.2.2.2" xref="S4.SS0.SSS3.p1.1.m1.2.2.2.2.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.2.2.2.2.2" xref="S4.SS0.SSS3.p1.1.m1.2.2.2.2.2.cmml">q</mi><mi id="S4.SS0.SSS3.p1.1.m1.2.2.2.2.3" xref="S4.SS0.SSS3.p1.1.m1.2.2.2.2.3.cmml">t</mi></msub><mo id="S4.SS0.SSS3.p1.1.m1.6.6.6.8" xref="S4.SS0.SSS3.p1.1.m1.6.6.7.cmml">,</mo><mrow id="S4.SS0.SSS3.p1.1.m1.3.3.3.3" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.2" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.1" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.3" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.1a" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.1.cmml">​</mo><msub id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.2" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.2.cmml">s</mi><mi id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.3" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.3.cmml">t</mi></msub></mrow><mo id="S4.SS0.SSS3.p1.1.m1.6.6.6.9" xref="S4.SS0.SSS3.p1.1.m1.6.6.7.cmml">,</mo><msub id="S4.SS0.SSS3.p1.1.m1.4.4.4.4" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.2" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.2.cmml">a</mi><mrow id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.2" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.2.cmml">t</mi><mo id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.1" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.1.cmml">+</mo><mn id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.3" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.3.cmml">1</mn></mrow></msub><mo id="S4.SS0.SSS3.p1.1.m1.6.6.6.10" xref="S4.SS0.SSS3.p1.1.m1.6.6.7.cmml">,</mo><msub id="S4.SS0.SSS3.p1.1.m1.5.5.5.5" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.2" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.2.cmml">q</mi><mrow id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.2" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.2.cmml">t</mi><mo id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.1" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.1.cmml">+</mo><mn id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.3" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.3.cmml">1</mn></mrow></msub><mo id="S4.SS0.SSS3.p1.1.m1.6.6.6.11" xref="S4.SS0.SSS3.p1.1.m1.6.6.7.cmml">,</mo><mrow id="S4.SS0.SSS3.p1.1.m1.6.6.6.6" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.2" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1.cmml">​</mo><mi id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.3" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1a" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1.cmml">​</mo><msub id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.2" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.2.cmml">s</mi><mrow id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.cmml"><mi id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.2" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.2.cmml">t</mi><mo id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.1" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.1.cmml">+</mo><mn id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.3" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1b" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.5" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.5.cmml">…</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS3.p1.1.m1.6b"><list id="S4.SS0.SSS3.p1.1.m1.6.6.7.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6"><apply id="S4.SS0.SSS3.p1.1.m1.1.1.1.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS3.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS0.SSS3.p1.1.m1.1.1.1.1.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.1.1.1.1.2">𝑎</ci><ci id="S4.SS0.SSS3.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.1.1.1.1.3">𝑡</ci></apply><apply id="S4.SS0.SSS3.p1.1.m1.2.2.2.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS0.SSS3.p1.1.m1.2.2.2.2.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.SS0.SSS3.p1.1.m1.2.2.2.2.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.2.2.2.2.2">𝑞</ci><ci id="S4.SS0.SSS3.p1.1.m1.2.2.2.2.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.2.2.2.2.3">𝑡</ci></apply><apply id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3"><times id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.1"></times><ci id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.2">𝑎</ci><ci id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.3">𝑛</ci><apply id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4"><csymbol cd="ambiguous" id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4">subscript</csymbol><ci id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.2">𝑠</ci><ci id="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.3.3.3.3.4.3">𝑡</ci></apply></apply><apply id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.cmml" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4">subscript</csymbol><ci id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.2">𝑎</ci><apply id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3"><plus id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.1"></plus><ci id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.2">𝑡</ci><cn type="integer" id="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.4.4.4.4.3.3">1</cn></apply></apply><apply id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.cmml" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5"><csymbol cd="ambiguous" id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5">subscript</csymbol><ci id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.2">𝑞</ci><apply id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3"><plus id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.1"></plus><ci id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.2">𝑡</ci><cn type="integer" id="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.5.5.5.5.3.3">1</cn></apply></apply><apply id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6"><times id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.1"></times><ci id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.2">𝑎</ci><ci id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.3">𝑛</ci><apply id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4"><csymbol cd="ambiguous" id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4">subscript</csymbol><ci id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.2">𝑠</ci><apply id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3"><plus id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.1.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.1"></plus><ci id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.2.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.2">𝑡</ci><cn type="integer" id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.3.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.4.3.3">1</cn></apply></apply><ci id="S4.SS0.SSS3.p1.1.m1.6.6.6.6.5.cmml" xref="S4.SS0.SSS3.p1.1.m1.6.6.6.6.5">…</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS3.p1.1.m1.6c">a_{t},q_{t},ans_{t},a_{t+1},q_{t+1},ans_{t+1}...</annotation></semantics></math>) and use it for forensic analysis in possible accident investigations with self-driving vehicles. Such log data can help understand why the self-driving vehicle made a specific decision at a particular time just before being involved in an accident.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have presented a preliminary study on explaining autonomous driving actions with a VQA approach. We used driving data generated by an RL agent on the CARLA simulator and developed our question-answering system as an explanatory approach to the agent’s decisions. The experimental results show that a simple and straightforward VQA mechanism can help interpret the real-time decisions of an autonomous car and also help understand its correct and incorrect decisions as safety implications. The results also suggest that unifying VQA with RL-based decision-making will likely do well for actions in a dynamic environment, provided that we have more training dataset. In this sense, we plan to explore three potential directions: 
<br class="ltx_break"><span id="S5.p1.1.1" class="ltx_text ltx_font_italic">1. Augmenting data and using other VQA architectures:</span> We will increase the size of training data (ideally <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><gt id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">&gt;</annotation></semantics></math>50K driving frames), perform fine-tuning on our model using more recent ConvNet architectures, such as Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and try out other VQA frameworks as well. By making a comparative analysis of these pre-trained deep neural architectures on driving data, we can observe their empirical performance in terms of accuracy and potentially produce a large-scale and curated benchmark dataset. 
<br class="ltx_break"><span id="S5.p1.1.2" class="ltx_text ltx_font_italic">2. Training an RL agent on dynamic environments:</span> We will run the RL agent in other towns on the CARLA simulator, which have more vehicles, pedestrians, and complex intersections, and annotate the ego vehicle’s interaction with them accordingly to provide more image-question-answer triplets. 
<br class="ltx_break"><span id="S5.p1.1.3" class="ltx_text ltx_font_italic">3. Leveraging large language models (LLMs):</span> Finally, a recent breakthrough in LLMs gives a reason to use this architecture in autonomous driving problems. As our current task combines vision and natural language-based reasoning for explaining self-driving actions, multimodal transformers (e.g., GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and its variations) could serve a purpose in this context. As multimodal transformers can input an image and text, and provide contextual information about the joint semantics, it seems promising to fine-tune such state-of-the-art learning architectures for the self-driving domain and generate rigorously structured explanations. 
<br class="ltx_break">We believe that the empirical work and further directions proposed in this paper can help improve safety, transparency, and trustworthiness of autonomous driving technology.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Waymo’s Blog, “First Million Rider-Only Miles: How the Waymo Driver is
Improving Road Safety,”  Accessed online on March 12, 2023. [Online].
Available:
https://blog.waymo.com/2023/02/first-million-rider-only-miles-how.html

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
NTS Board, “Collision between a sport utility vehicle operating with partial
driving automation and a crash attenuator Mountain View, California.” 
Accessed online on March 9, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. Voigt and A. Von dem Bussche, “The EU General Data Protection Regulation
(GDPR),” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</em>, vol. 10, no. 3152676, pp. 10–5555, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
GDPR, “Art. 22 GDPR Automated individual decision-making, including
profiling ,” (Accessed on March 9, 2023). [Online]. Available:
https://gdpr-info.eu/art-22-gdpr/

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D. Omeiza, H. Webb, M. Jirotka, and L. Kunze, “Explanations in Autonomous
Driving: A Survey,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation
Systems</em>, vol. 23, no. 8, pp. 10 142–10 162, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Atakishiyev, M. Salameh, H. Yao, and R. Goebel, “Explainable Artificial
Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide
for Future Research Directions,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.11561</em>,
2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
É. Zablocki, H. Ben-Younes, P. Pérez, and M. Cord, “Explainability of
Deep Vision-Based Autonomous Driving Systems: Review and Challenges,”
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, vol. 130, no. 10, pp.
2425–2452, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Kafle and C. Kanan, “Visual question answering: Datasets, algorithms, and
future challenges,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, vol.
163, pp. 3–20, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D.
Jackel, M. Monfort, U. Muller, J. Zhang, <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “End to end learning
for self-driving cars,” <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.07316</em>, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Kim and J. Canny, “Interpretable Learning for Self-Driving Cars by
Visualizing Causal Attention,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
International Conference on Computer Vision</em>, 2017, pp. 2942–2950.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Bojarski, A. Choromanska, K. Choromanski, B. Firner, L. J. Ackel, U. Muller,
P. Yeres, and K. Zieba, “VisualBackProp: Efficient Visualization of CNNs
for Autonomous Driving,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on
Robotics and Automation (ICRA)</em>.   IEEE, 2018, pp. 4701–4708.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Mohseni, A. Jagadeesh, and Z. Wang, “Predicting model failure using
saliency maps in autonomous driving systems,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ICML 2019 Workshop on
Uncertainty and Robustness in Deep Learning</em>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Bansal, A. Krizhevsky, and A. Ogale, “ChauffeurNet: Learning to Drive by
Imitating the Best and Synthesizing the Worst,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Robotics: Science and
Systems</em>, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Li, S. H. Chan, and Y.-T. Chen, “Who Make Drivers Stop? Towards
Driver-centric Risk Assessment: Risk Object Identification via Causal
Inference,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)</em>.   IEEE,
2020, pp. 10 711–10 718.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA: An
Open Urban Driving Simulator,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Conference on Robot
Learning</em>.   PMLR, 2017, pp. 1–16.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
P. Jacob, É. Zablocki, H. Ben-Younes, M. Chen, P. Pérez, and M. Cord,
“STEEX: Steering Counterfactual Explanations with Semantics,” in
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em>.   Springer, 2022, pp. 387–403.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Kim, A. Rohrbach, T. Darrell, J. Canny, and Z. Akata, “Textual
Explanations for Self-Driving Vehicles,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
European Conference on Computer Vision (ECCV)</em>, 2018, pp. 563–578.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Kim, T. Misu, Y.-T. Chen, A. Tawari, and J. Canny, “Grounding
Human-to-Vehicle Advice for Self-driving Vehicles,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>,
2019, pp. 10 591–10 599.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Kim, S. Moon, A. Rohrbach, T. Darrell, and J. Canny, “Advisable Learning
for Self-Driving Vehicles by Internalizing Observation-to-Action Rules,” in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2020, pp. 9661–9670.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Xu, X. Yang, L. Gong, H.-C. Lin, T.-Y. Wu, Y. Li, and N. Vasconcelos,
“Explainable Object-induced Action Decision for Autonomous Vehicles,” in
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2020, pp. 9523–9532.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. Ben-Younes, É. Zablocki, P. Pérez, and M. Cord, “Driving behavior
explanation with multi-level fusion,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, vol. 123,
p. 108421, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Brewitt, B. Gyevnar, S. Garcin, and S. V. Albrecht, “GRIT: Fast,
Interpretable, and Verifiable Goal Recognition with Learned Decision Trees
for Autonomous Driving,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS)</em>.   IEEE, 2021, pp. 1023–1030.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Almalioglu, M. Turan, N. Trigoni, and A. Markham, “Deep learning-based
robust positioning for all-weather autonomous driving,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Nature Machine
Intelligence</em>, vol. 4, no. 9, pp. 749–760, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D. Lee, J. Mulrow, C. J. Haboucha, S. Derrible, and Y. Shiftan, “Attitudes on
Autonomous Vehicle Adoption using Interpretable Gradient Boosting Machine,”
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Transportation Research Record</em>, vol. 2673, no. 11, pp. 865–878, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
T. Schneider, S. Ghellal, S. Love, and A. R. Gerlicher, “Increasing the User
Experience in Autonomous Driving through different Feedback Modalities,” in
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">26th International Conference on Intelligent User Interfaces</em>, 2021,
pp. 7–10.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Schneider, J. Hois, A. Rosenstein, S. Ghellal, D. Theofanou-Fülbier, and
A. R. Gerlicher, “ExplAIn Yourself! Transparency for Positive UX in
Autonomous Driving,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems</em>, 2021, pp. 1–12.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P.
Bigham, “VizWiz Grand Challenge: Answering Visual Questions From Blind
People,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, 2018, pp. 3608–3617.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
L.-M. Zhan, B. Liu, L. Fan, J. Chen, and X.-M. Wu, “Medical Visual Question
Answering via Conditional Reasoning,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM
International Conference on Multimedia</em>, 2020, pp. 2345–2354.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
D. Li, Z. Zhang, K. Yu, K. Huang, and T. Tan, “ISEE: An Intelligent Scene
Exploration and Evaluation Platform for Large-Scale Visual Surveillance,”
<em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em>, vol. 30,
no. 12, pp. 2743–2758, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum,
“CLEVRER: Collision Events for Video Representation and Reasoning,” in
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
L. Xu, H. Huang, and J. Liu, “SUTD-TrafficQA: A Question Answering Benchmark
and an Efficient Network for Video Reasoning Over Traffic Events,” in
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2021, pp. 9878–9888.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous Control with Deep Reinforcement
Learning,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>,
2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. E. Hart, N. J. Nilsson, and B. Raphael, “A Formal Basis for the Heuristic
Determination of Minimum Cost Paths,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Systems
Science and Cybernetics</em>, vol. 4, no. 2, pp. 100–107, 1968.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ó. Pérez-Gil, R. Barea, E. López-Guillén, L. M. Bergasa,
C. Gomez-Huelamo, R. Gutiérrez, and A. Diaz-Diaz, “Deep reinforcement
learning based control for Autonomous Vehicles in CARLA,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Multimedia
Tools and Applications</em>, vol. 81, no. 3, pp. 3553–3576, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “VQA: Visual Question Answering,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE International Conference on Computer Vision</em>, 2015, pp. 2425–2433.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft COCO: Common Objects in
Context,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Neural
computation</em>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for
Large-Scale Image Recognition,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S. Garg and R. Srivastava, “Object sequences: encoding categorical and spatial
information for a yes/no visual question answering task,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IET Computer
Vision</em>, vol. 12, no. 8, pp. 1141–1150, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
X. Peng, M. Riedl, and P. Ammanabrolu, “Inherently Explainable Reinforcement
Learning in Natural Language,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, vol. 35, pp. 16 178–16 190, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
and N. Houlsby, “An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
OpenAI, “GPT-4 Technical Report,” https://arxiv.org/abs/2303.08774,
2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.10407" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.10408" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.10408">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.10408" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.10409" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 17:28:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
