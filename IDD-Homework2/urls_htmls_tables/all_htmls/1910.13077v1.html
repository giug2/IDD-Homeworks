<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1910.13077] Learning Rich Image Region Representation for Visual Question Answering</title><meta property="og:description" content="We propose to boost VQA by leveraging more powerful feature extractors by improving the representation ability of both visual and text features and the ensemble of models. For visual feature, some detection techniques â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning Rich Image Region Representation for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning Rich Image Region Representation for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1910.13077">

<!--Generated on Sat Mar 16 10:48:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning Rich Image Region Representation for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bei Liu, Zhicheng Huang, Zhaoyang Zeng, Zheyu Chen, Jianlong Fu
<br class="ltx_break">Microsoft Research Asia
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{bei.liu, t-zhihua, v-zhazen, t-zheche, jianf}@microsoft.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">We propose to boost VQA by leveraging more powerful feature extractors by improving the representation ability of both visual and text features and the ensemble of models. For visual feature, some detection techniques are used to improve the detector. For text feature, we adopt BERT as the language model and find that it can significantly improve VQA performance. Our solution won the second place in the VQA Challenge 2019.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The task of Visual Question Answering (VQA) requires our model to answer text question based on the input image. Most works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> leverage visual features extracted from images and text features extracted from question to perform classification to obtain answers. Thus, visual and textual features serve as basic components which can directly impact the final performance. In this paper, we propose to improve the performance of VQA by extracting more powerful visual and text features.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">For visual features, most existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> adopt bottom-up-attention features released by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, whose feature extractor is a Faster R-CNN object detector built upon a ResNet-101 backbone. We adopt more powerful backbones (i.e. ResNeXt-101, ResNeXt-152) to train stronger detectors. Some techniques (i.e. FPN, multi-scale training) that are useful to improve the accuracy of detectors can also help to boost the performance of VQA.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">For text features, we build upon recent state-of-art techniques in the NLP community. Large-scale language models such as ELMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, have shown excellent results for various NLP tasks in both token and sentence level. BERT uses masked language models to enable pre-trained deep bidirectional representations and allows the representation to fuse the right and left context. While in VQA model, to get the question answer, we need the token level features to contain questionsâ€™ contextual information to fuse with the visual tokens for the reasoning. So we adopt the BERT as our language mode.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Experiments on VQA 2.0 dataset shows the effectiveness of each component of our solution. Our final model achieves <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="74.89\%" display="inline"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mn id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">74.89</mn><mo id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><csymbol cd="latexml" id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.p4.1.m1.1.1.2.cmml" xref="S1.p4.1.m1.1.1.2">74.89</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">74.89\%</annotation></semantics></math> accuracy on <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">test-standard</span> split, which won the second place in the VQA Challenge 2019.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Feature Representation</h2>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Split</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Backbone</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">FPN dim</span></th>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Attribute</span></td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Language</span></td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Yes/No</span></td>
<td id="S2.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">Num</span></td>
<td id="S2.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.8.1" class="ltx_text ltx_font_bold">Others</span></td>
<td id="S2.T1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.9.1" class="ltx_text ltx_font_bold">Score</span></td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<th id="S2.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="14"><span id="S2.T1.1.2.2.1.1" class="ltx_text ltx_font_italic">test-dev</span></th>
<th id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Bottom-up-attention (ResNet-101)</th>
<th id="S2.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">-</th>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S2.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Glove</td>
<td id="S2.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.42</td>
<td id="S2.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">54.04</td>
<td id="S2.T1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.52</td>
<td id="S2.T1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.04</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<th id="S2.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">FaceBook pythia (ResNeXt-101)</th>
<th id="S2.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">512</th>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">Glove</td>
<td id="S2.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r">85.56</td>
<td id="S2.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r">52.68</td>
<td id="S2.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r">60.87</td>
<td id="S2.T1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r">70.11</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<th id="S2.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNeXt-101</th>
<th id="S2.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">256</th>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Glove</td>
<td id="S2.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.1</td>
<td id="S2.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.0</td>
<td id="S2.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.62</td>
<td id="S2.T1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.64</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<th id="S2.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-101</th>
<th id="S2.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">256</th>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">Glove</td>
<td id="S2.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">85.44</td>
<td id="S2.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r">54.2</td>
<td id="S2.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r">60.87</td>
<td id="S2.T1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r">70.23</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<th id="S2.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNeXt-152</th>
<th id="S2.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">256</th>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Glove</td>
<td id="S2.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.42</td>
<td id="S2.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.11</td>
<td id="S2.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.88</td>
<td id="S2.T1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.22</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<th id="S2.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-152</th>
<th id="S2.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">512</th>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">Glove</td>
<td id="S2.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r">86.59</td>
<td id="S2.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r">56.44</td>
<td id="S2.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r">62.06</td>
<td id="S2.T1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r">71.53</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<th id="S2.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-152 (ms-train)</th>
<th id="S2.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">256</th>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">Glove</td>
<td id="S2.T1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r">86.46</td>
<td id="S2.T1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r">56.37</td>
<td id="S2.T1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r">62.24</td>
<td id="S2.T1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r">71.55</td>
</tr>
<tr id="S2.T1.1.9.9" class="ltx_tr">
<th id="S2.T1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-152 (ms-train)</th>
<th id="S2.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">512</th>
<td id="S2.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">Glove</td>
<td id="S2.T1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r">86.54</td>
<td id="S2.T1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r">56.90</td>
<td id="S2.T1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r">62.31</td>
<td id="S2.T1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r">71.68</td>
</tr>
<tr id="S2.T1.1.10.10" class="ltx_tr">
<th id="S2.T1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNeXt-152</th>
<th id="S2.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">256</th>
<td id="S2.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S2.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BERT</td>
<td id="S2.T1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.00</td>
<td id="S2.T1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.28</td>
<td id="S2.T1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.90</td>
<td id="S2.T1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.48</td>
</tr>
<tr id="S2.T1.1.11.11" class="ltx_tr">
<th id="S2.T1.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-152</th>
<th id="S2.T1.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">512</th>
<td id="S2.T1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">BERT</td>
<td id="S2.T1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r">88.15</td>
<td id="S2.T1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r">56.79</td>
<td id="S2.T1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r">62.98</td>
<td id="S2.T1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r">72.64</td>
</tr>
<tr id="S2.T1.1.12.12" class="ltx_tr">
<th id="S2.T1.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-152 (ms-train)</th>
<th id="S2.T1.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">256</th>
<td id="S2.T1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r">BERT</td>
<td id="S2.T1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r">88.14</td>
<td id="S2.T1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r">56.74</td>
<td id="S2.T1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r">63.3</td>
<td id="S2.T1.1.12.12.8" class="ltx_td ltx_align_center ltx_border_r">72.79</td>
</tr>
<tr id="S2.T1.1.13.13" class="ltx_tr">
<th id="S2.T1.1.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-152 (ms-train)</th>
<th id="S2.T1.1.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">512</th>
<td id="S2.T1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S2.T1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r">BERT</td>
<td id="S2.T1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r">88.18</td>
<td id="S2.T1.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r">55.35</td>
<td id="S2.T1.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r">63.16</td>
<td id="S2.T1.1.13.13.8" class="ltx_td ltx_align_center ltx_border_r">72.58</td>
</tr>
<tr id="S2.T1.1.14.14" class="ltx_tr">
<th id="S2.T1.1.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Ensemble (5 models)</th>
<th id="S2.T1.1.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">-</th>
<td id="S2.T1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.65</td>
<td id="S2.T1.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.53</td>
<td id="S2.T1.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.27</td>
<td id="S2.T1.1.14.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.55</td>
</tr>
<tr id="S2.T1.1.15.15" class="ltx_tr">
<th id="S2.T1.1.15.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ensemble (20 models)</th>
<th id="S2.T1.1.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<td id="S2.T1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S2.T1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S2.T1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r">89.81</td>
<td id="S2.T1.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r">58.89</td>
<td id="S2.T1.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r">65.39</td>
<td id="S2.T1.1.15.15.8" class="ltx_td ltx_align_center ltx_border_r">74.71</td>
</tr>
<tr id="S2.T1.1.16.16" class="ltx_tr">
<th id="S2.T1.1.16.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.16.16.1.1" class="ltx_text ltx_font_italic">test-std</span></th>
<th id="S2.T1.1.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Ensemble (20 models)</th>
<th id="S2.T1.1.16.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">-</th>
<td id="S2.T1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.16.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.16.16.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">89.81</td>
<td id="S2.T1.1.16.16.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">58.36</td>
<td id="S2.T1.1.16.16.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">65.69</td>
<td id="S2.T1.1.16.16.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">74.89</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experiment results on VQA 2.0 <span id="S2.T1.4.1" class="ltx_text ltx_font_italic">test-dev</span> and <span id="S2.T1.5.2" class="ltx_text ltx_font_italic">test-std</span> splits. We adopt BAN as VQA model in all settings. The first two rows indicate the results of models we train on released features. â€œms-trainâ€ means using multi-scale strategy in detectors training.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Feature</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> show that detection features are more powerful than classification features on VQA task, therefore we train object detectors on large-scale dataset for feature extraction.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.5" class="ltx_p"><span id="S2.SS1.p2.5.1" class="ltx_text ltx_font_bold">Dataset.</span> We adopt Visual Genome 1.2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> as object detection dataset. Following the setting in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we adopt <math id="S2.SS1.p2.1.m1.2" class="ltx_Math" alttext="1,600" display="inline"><semantics id="S2.SS1.p2.1.m1.2a"><mrow id="S2.SS1.p2.1.m1.2.3.2" xref="S2.SS1.p2.1.m1.2.3.1.cmml"><mn id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">1</mn><mo id="S2.SS1.p2.1.m1.2.3.2.1" xref="S2.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS1.p2.1.m1.2.2" xref="S2.SS1.p2.1.m1.2.2.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.2b"><list id="S2.SS1.p2.1.m1.2.3.1.cmml" xref="S2.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">1</cn><cn type="integer" id="S2.SS1.p2.1.m1.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2">600</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.2c">1,600</annotation></semantics></math> object classes and <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mn id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><cn type="integer" id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">400</annotation></semantics></math> attribute classes as training categories. The dataset is divided into <span id="S2.SS1.p2.5.2" class="ltx_text ltx_font_italic">train</span>, <span id="S2.SS1.p2.5.3" class="ltx_text ltx_font_italic">val</span>, and <span id="S2.SS1.p2.5.4" class="ltx_text ltx_font_italic">test</span> splits, which contain <math id="S2.SS1.p2.3.m3.2" class="ltx_Math" alttext="98,077" display="inline"><semantics id="S2.SS1.p2.3.m3.2a"><mrow id="S2.SS1.p2.3.m3.2.3.2" xref="S2.SS1.p2.3.m3.2.3.1.cmml"><mn id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">98</mn><mo id="S2.SS1.p2.3.m3.2.3.2.1" xref="S2.SS1.p2.3.m3.2.3.1.cmml">,</mo><mn id="S2.SS1.p2.3.m3.2.2" xref="S2.SS1.p2.3.m3.2.2.cmml">077</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.2b"><list id="S2.SS1.p2.3.m3.2.3.1.cmml" xref="S2.SS1.p2.3.m3.2.3.2"><cn type="integer" id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">98</cn><cn type="integer" id="S2.SS1.p2.3.m3.2.2.cmml" xref="S2.SS1.p2.3.m3.2.2">077</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.2c">98,077</annotation></semantics></math>, <math id="S2.SS1.p2.4.m4.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S2.SS1.p2.4.m4.2a"><mrow id="S2.SS1.p2.4.m4.2.3.2" xref="S2.SS1.p2.4.m4.2.3.1.cmml"><mn id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">5</mn><mo id="S2.SS1.p2.4.m4.2.3.2.1" xref="S2.SS1.p2.4.m4.2.3.1.cmml">,</mo><mn id="S2.SS1.p2.4.m4.2.2" xref="S2.SS1.p2.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.2b"><list id="S2.SS1.p2.4.m4.2.3.1.cmml" xref="S2.SS1.p2.4.m4.2.3.2"><cn type="integer" id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">5</cn><cn type="integer" id="S2.SS1.p2.4.m4.2.2.cmml" xref="S2.SS1.p2.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.2c">5,000</annotation></semantics></math> and <math id="S2.SS1.p2.5.m5.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S2.SS1.p2.5.m5.2a"><mrow id="S2.SS1.p2.5.m5.2.3.2" xref="S2.SS1.p2.5.m5.2.3.1.cmml"><mn id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">5</mn><mo id="S2.SS1.p2.5.m5.2.3.2.1" xref="S2.SS1.p2.5.m5.2.3.1.cmml">,</mo><mn id="S2.SS1.p2.5.m5.2.2" xref="S2.SS1.p2.5.m5.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.2b"><list id="S2.SS1.p2.5.m5.2.3.1.cmml" xref="S2.SS1.p2.5.m5.2.3.2"><cn type="integer" id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">5</cn><cn type="integer" id="S2.SS1.p2.5.m5.2.2.cmml" xref="S2.SS1.p2.5.m5.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.2c">5,000</annotation></semantics></math> images, respectively. We train detectors on <span id="S2.SS1.p2.5.5" class="ltx_text ltx_font_italic">train</span> split, and use <span id="S2.SS1.p2.5.6" class="ltx_text ltx_font_italic">val</span> and <span id="S2.SS1.p2.5.7" class="ltx_text ltx_font_italic">test</span> as validation set to tune parameters.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Detector.</span> We follow the pipeline of Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to build our detectors. We adopt ResNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> with FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> as backbone, and use parameters pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for initialization. We use RoIAlign <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to wrap region features into fixed size, and embed them into <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mn id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><cn type="integer" id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">2048</annotation></semantics></math>-dim via two fully connected layer. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we extend a classification branch on the top of region feature, and utilize attribute annotations as additional supervision. Such attribute branch is only used to enhance feature representation ability in the training stage, and will be discarded in the feature extraction stage.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.3" class="ltx_p"><span id="S2.SS1.p4.3.1" class="ltx_text ltx_font_bold">Feature.</span> Given an image, we first feed it into the trained detector, and apply non-maximum suppression (NMS) on each category to remove duplicate bbox. Then we seek <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mn id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><cn type="integer" id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">100</annotation></semantics></math> boxes with highest object confidence, and extract their <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mn id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><cn type="integer" id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">2048</annotation></semantics></math>-dim FC feature. These <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mn id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><cn type="integer" id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">100</annotation></semantics></math> boxes with their features are considered as the representation of the given image.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Language Feature</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.8" class="ltx_p">The BERT model, introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, can be seen as a multi-layer bidirectional Transformer based on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The model consists of Embedding layers, Transformer blocks and self-attention heads, which has two different model size. For the base model, there are <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mn id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><cn type="integer" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">12</annotation></semantics></math> layers Transformer blocks, the hidden size is <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mn id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><cn type="integer" id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">768</annotation></semantics></math>-dim, and the number of self-attention heads is <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mn id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><cn type="integer" id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">12</annotation></semantics></math>. The total parameters are <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="110" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mn id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">110</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><cn type="integer" id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">110</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">110</annotation></semantics></math>M. For the large model size, the model consists of <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><mn id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><cn type="integer" id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">24</annotation></semantics></math> Transformer blocks which hidden size is <math id="S2.SS2.p1.6.m6.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S2.SS2.p1.6.m6.1a"><mn id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><cn type="integer" id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">1024</annotation></semantics></math>, and <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><mn id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><cn type="integer" id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">16</annotation></semantics></math> self-attention heads. And the model total parameters is <math id="S2.SS2.p1.8.m8.1" class="ltx_Math" alttext="340" display="inline"><semantics id="S2.SS2.p1.8.m8.1a"><mn id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml">340</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><cn type="integer" id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1">340</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">340</annotation></semantics></math>M The model can process a single text sentence or a pair of text sentences (i.e.,[Question, Answer]) in one token sequence. To separate the pair of text sentence, we can add special token ([SEP]) between two sentences, add a learned sentence A embedding to every token of the first sentence and a sentence B embedding to every token of the second sentence. For VQA task, there is only one sentence, we only use the sentence A embedding.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Considering that the total parameter of VQA model is less than <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mn id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><cn type="integer" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">100</annotation></semantics></math>M, we use the base BERT as our language model to extract question features. To get each tokenâ€™s representation, we only use the hidden state corresponding to the last attention block features of the full sequence. Pre-trained BERT model has shown to be effective for boosting many natural language processing tasks, we adopt the base BERT uncased pre-train weight as our initial parameters.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>VQA Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">Recent years, there are many VQA models, which have achieved surprising results. We adopt the Bilinear Attention Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (BAN) as our base model. The single model with eight-glimpse can get <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="70.04" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">70.04</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="float" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">70.04</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">70.04</annotation></semantics></math> on VQA2.0 test-dev subset. The BAN model uses Glove and GRU as the language model. And the language feature is a vector <math id="S3.p1.2.m2.2" class="ltx_Math" alttext="[questionlength,1280]" display="inline"><semantics id="S3.p1.2.m2.2a"><mrow id="S3.p1.2.m2.2.2.1" xref="S3.p1.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.p1.2.m2.2.2.1.2" xref="S3.p1.2.m2.2.2.2.cmml">[</mo><mrow id="S3.p1.2.m2.2.2.1.1" xref="S3.p1.2.m2.2.2.1.1.cmml"><mi id="S3.p1.2.m2.2.2.1.1.2" xref="S3.p1.2.m2.2.2.1.1.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.3" xref="S3.p1.2.m2.2.2.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1a" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.4" xref="S3.p1.2.m2.2.2.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1b" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.5" xref="S3.p1.2.m2.2.2.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1c" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.6" xref="S3.p1.2.m2.2.2.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1d" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.7" xref="S3.p1.2.m2.2.2.1.1.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1e" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.8" xref="S3.p1.2.m2.2.2.1.1.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1f" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.9" xref="S3.p1.2.m2.2.2.1.1.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1g" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.10" xref="S3.p1.2.m2.2.2.1.1.10.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1h" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.11" xref="S3.p1.2.m2.2.2.1.1.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1i" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.12" xref="S3.p1.2.m2.2.2.1.1.12.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1j" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.13" xref="S3.p1.2.m2.2.2.1.1.13.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1k" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.14" xref="S3.p1.2.m2.2.2.1.1.14.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.2.1.1.1l" xref="S3.p1.2.m2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.p1.2.m2.2.2.1.1.15" xref="S3.p1.2.m2.2.2.1.1.15.cmml">h</mi></mrow><mo id="S3.p1.2.m2.2.2.1.3" xref="S3.p1.2.m2.2.2.2.cmml">,</mo><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">1280</mn><mo stretchy="false" id="S3.p1.2.m2.2.2.1.4" xref="S3.p1.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.2b"><interval closure="closed" id="S3.p1.2.m2.2.2.2.cmml" xref="S3.p1.2.m2.2.2.1"><apply id="S3.p1.2.m2.2.2.1.1.cmml" xref="S3.p1.2.m2.2.2.1.1"><times id="S3.p1.2.m2.2.2.1.1.1.cmml" xref="S3.p1.2.m2.2.2.1.1.1"></times><ci id="S3.p1.2.m2.2.2.1.1.2.cmml" xref="S3.p1.2.m2.2.2.1.1.2">ğ‘</ci><ci id="S3.p1.2.m2.2.2.1.1.3.cmml" xref="S3.p1.2.m2.2.2.1.1.3">ğ‘¢</ci><ci id="S3.p1.2.m2.2.2.1.1.4.cmml" xref="S3.p1.2.m2.2.2.1.1.4">ğ‘’</ci><ci id="S3.p1.2.m2.2.2.1.1.5.cmml" xref="S3.p1.2.m2.2.2.1.1.5">ğ‘ </ci><ci id="S3.p1.2.m2.2.2.1.1.6.cmml" xref="S3.p1.2.m2.2.2.1.1.6">ğ‘¡</ci><ci id="S3.p1.2.m2.2.2.1.1.7.cmml" xref="S3.p1.2.m2.2.2.1.1.7">ğ‘–</ci><ci id="S3.p1.2.m2.2.2.1.1.8.cmml" xref="S3.p1.2.m2.2.2.1.1.8">ğ‘œ</ci><ci id="S3.p1.2.m2.2.2.1.1.9.cmml" xref="S3.p1.2.m2.2.2.1.1.9">ğ‘›</ci><ci id="S3.p1.2.m2.2.2.1.1.10.cmml" xref="S3.p1.2.m2.2.2.1.1.10">ğ‘™</ci><ci id="S3.p1.2.m2.2.2.1.1.11.cmml" xref="S3.p1.2.m2.2.2.1.1.11">ğ‘’</ci><ci id="S3.p1.2.m2.2.2.1.1.12.cmml" xref="S3.p1.2.m2.2.2.1.1.12">ğ‘›</ci><ci id="S3.p1.2.m2.2.2.1.1.13.cmml" xref="S3.p1.2.m2.2.2.1.1.13">ğ‘”</ci><ci id="S3.p1.2.m2.2.2.1.1.14.cmml" xref="S3.p1.2.m2.2.2.1.1.14">ğ‘¡</ci><ci id="S3.p1.2.m2.2.2.1.1.15.cmml" xref="S3.p1.2.m2.2.2.1.1.15">â„</ci></apply><cn type="integer" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">1280</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.2c">[questionlength,1280]</annotation></semantics></math>. To improve the VQA model performance, we replace the language model with base BERT and modify the BAN language input feature dimension. To Train the BAN with BERT, we use all settings from BAN, but set the max epoch is <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.p1.3.m3.1a"><mn id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><cn type="integer" id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">20</annotation></semantics></math> with costing learn rate scheduler. To use the base BERT pre-trained parameters we set the learning rate of the BERT module to <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="5e-5" display="inline"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mrow id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml"><mn id="S3.p1.4.m4.1.1.2.2" xref="S3.p1.4.m4.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S3.p1.4.m4.1.1.2.1" xref="S3.p1.4.m4.1.1.2.1.cmml">â€‹</mo><mi id="S3.p1.4.m4.1.1.2.3" xref="S3.p1.4.m4.1.1.2.3.cmml">e</mi></mrow><mo id="S3.p1.4.m4.1.1.1" xref="S3.p1.4.m4.1.1.1.cmml">âˆ’</mo><mn id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><minus id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1"></minus><apply id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2"><times id="S3.p1.4.m4.1.1.2.1.cmml" xref="S3.p1.4.m4.1.1.2.1"></times><cn type="integer" id="S3.p1.4.m4.1.1.2.2.cmml" xref="S3.p1.4.m4.1.1.2.2">5</cn><ci id="S3.p1.4.m4.1.1.2.3.cmml" xref="S3.p1.4.m4.1.1.2.3">ğ‘’</ci></apply><cn type="integer" id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">5e-5</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Ablation Experiments</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.6" class="ltx_p">TableÂ <a href="#S2.T1" title="Table 1 â€£ 2 Feature Representation â€£ Learning Rich Image Region Representation for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows all our ablation study on each component, including attribute head, FPN dimension, language model. From <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="3^{rd}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msup id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">3</mn><mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">3</cn><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><times id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3.1"></times><ci id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.SS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">3^{rd}</annotation></semantics></math> and <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="4^{th}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msup id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">4</mn><mrow id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.p1.2.m2.1.1.3.2" xref="S4.SS1.p1.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.1.3.1" xref="S4.SS1.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p1.2.m2.1.1.3.3" xref="S4.SS1.p1.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">4</cn><apply id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3"><times id="S4.SS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.p1.2.m2.1.1.3.1"></times><ci id="S4.SS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.p1.2.m2.1.1.3.2">ğ‘¡</ci><ci id="S4.SS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">4^{th}</annotation></semantics></math> row, we can find that the attribute head can bring more than <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="4\%" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mn id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">4</mn><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">4\%</annotation></semantics></math> improvement to the final performance, which shows the effectiveness of such module. From <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="5^{th}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><msup id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mn id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">5</mn><mrow id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.2" xref="S4.SS1.p1.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.3.1" xref="S4.SS1.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p1.4.m4.1.1.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">5</cn><apply id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><times id="S4.SS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.1"></times><ci id="S4.SS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.2">ğ‘¡</ci><ci id="S4.SS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">5^{th}</annotation></semantics></math> to <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="12^{th}" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><msup id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><mn id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml">12</mn><mrow id="S4.SS1.p1.5.m5.1.1.3" xref="S4.SS1.p1.5.m5.1.1.3.cmml"><mi id="S4.SS1.p1.5.m5.1.1.3.2" xref="S4.SS1.p1.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.5.m5.1.1.3.1" xref="S4.SS1.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p1.5.m5.1.1.3.3" xref="S4.SS1.p1.5.m5.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2">12</cn><apply id="S4.SS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3"><times id="S4.SS1.p1.5.m5.1.1.3.1.cmml" xref="S4.SS1.p1.5.m5.1.1.3.1"></times><ci id="S4.SS1.p1.5.m5.1.1.3.2.cmml" xref="S4.SS1.p1.5.m5.1.1.3.2">ğ‘¡</ci><ci id="S4.SS1.p1.5.m5.1.1.3.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">12^{th}</annotation></semantics></math> row, we find that BERT can boost the performance by more than <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mn id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><cn type="integer" id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">1</annotation></semantics></math> point improvement stably. Besides, increasing FPN dimension and adopting multi-scale training can both slightly improve the VQA accuracy.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison with Others</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.4" class="ltx_p">We select BAN trained on Bottom-up-attention and Facebook features as baselines. Our single model result achieves <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="72.79\%" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">72.79</mn><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">72.79</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">72.79\%</annotation></semantics></math> accuracy on <span id="S4.SS2.p1.4.1" class="ltx_text ltx_font_italic">test-dev</span> split, which significantly outperforms all existing state-of-the-arts. We also ensemble several models we trained by averaging their probabilities output. The result by <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">20</annotation></semantics></math> modelsâ€™ ensemble achieves <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="74.71\%" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">74.71</mn><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">74.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">74.71\%</annotation></semantics></math> and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="74.89\%" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">74.89</mn><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">74.89</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">74.89\%</annotation></semantics></math> accuracy on VQA <span id="S4.SS2.p1.4.2" class="ltx_text ltx_font_italic">test-dev</span> and <span id="S4.SS2.p1.4.3" class="ltx_text ltx_font_italic">test-std</span> splits, respectively. Such result won the second place in the VQA Challenge 2019.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have shown that for VQA task, the representation capacity of both visual and textual features is critical for the final performance.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 6077â€“6086, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 248â€“255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.04805</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 2961â€“2969, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Bilinear Attention Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems 31</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages
1571â€“1581, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A Shamma, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 123(1):32â€“73, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan,
and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 2117â€“2125, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Gao Peng, Hongsheng Li, Haoxuan You, Zhengkai Jiang, Pan Lu, Steven Hoi, and
Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Dynamic fusion with intra-and inter-modality attention flow for
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.05252</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
MatthewÂ E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Deep contextualized word representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of NAACL</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Improving language understanding with unsupervised learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">Technical report, Technical report, OpenAI, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages
91â€“99, 2015.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Damien Teney, Peter Anderson, Xiaodong He, and Anton vanÂ den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Tips and tricks for visual question answering: Learnings from the
2017 challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 4223â€“4232, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages
5998â€“6008, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Aggregated residual transformations for deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 1492â€“1500, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Beyond bilinear: Generalized multimodal factorized high-order pooling
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Neural Networks and Learning Systems</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">,
29(12):5947â€“5959, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1910.13076" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1910.13077" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1910.13077">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1910.13077" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1910.13079" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 10:48:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
