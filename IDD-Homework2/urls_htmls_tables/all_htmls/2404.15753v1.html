<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Introducing EEG Analyses to Help Personal Music Preference Prediction</title>
<!--Generated on Tue Apr 30 19:59:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Preference prediction,  Electroencephalography,  Brain signals,  Dry electrodes" lang="en" name="keywords"/>
<base href="/html/2404.15753v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S1" title="In Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S2" title="In Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S2.SS1" title="In 2. Related Work ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Personalized Music Recommender Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S2.SS2" title="In 2. Related Work ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>EEG Based Preference Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S2.SS3" title="In 2. Related Work ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Other EEG Related Research</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3" title="In Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>User Study Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS1" title="In 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>User Study Procedure</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS1.SSS1" title="In 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Procedure Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS1.SSS2" title="In 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS1.SSS3" title="In 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Music data preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS1.SSS4" title="In 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.4 </span>Apparatus</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS2" title="In 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>User Behavior and EEG Data Preprocessing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS2.SSS1" title="In 3.2. User Behavior and EEG Data Preprocessing ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Data Alignment and Denoising</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS2.SSS2" title="In 3.2. User Behavior and EEG Data Preprocessing ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>EEG Feature Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.SS2.SSS3" title="In 3.2. User Behavior and EEG Data Preprocessing ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Label Statistics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4" title="In Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>EEG Analysis and Findings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.SS1" title="In 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Relationship Between Music Preference and EEG signals</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.SS2" title="In 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Relationship Between Mood and EEG signals</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5" title="In Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>EEG-aware Music Preference Prediction Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS1" title="In 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experiment Settings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS1.SSS1" title="In 5.1. Experiment Settings ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Baseline Features</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS1.SSS2" title="In 5.1. Experiment Settings ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Based models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS2" title="In 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Rating Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS3" title="In 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Preference Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS4" title="In 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Feature Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS4.SSS1" title="In 5.4. Feature Analysis ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.SS4.SSS2" title="In 5.4. Feature Analysis ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span>Feature Importance Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S6" title="In Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion and Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S6.SS1" title="In 6. Discussion and Limitations ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S6.SS1.SSS1" title="In 6.1. Discussion ‣ 6. Discussion and Limitations ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>EEG’s Contribution to Personal Music Preferences</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S6.SS1.SSS2" title="In 6.1. Discussion ‣ 6. Discussion and Limitations ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Adoption of Portable Devices to Record EEG Signals</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S6.SS1.SSS3" title="In 6.1. Discussion ‣ 6. Discussion and Limitations ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Privacy Concern in Personalized Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S6.SS2" title="In 6. Discussion and Limitations ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Limitation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S7" title="In Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Introducing EEG Analyses to Help Personal Music Preference Prediction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiyu He
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:hezy22@mails.tsinghua.edu.cn">hezy22@mails.tsinghua.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiayu Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jy-li20@mails.tsinghua.edu.cn">jy-li20@mails.tsinghua.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weizhi Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mawz@tsinghua.edu.cn">mawz@tsinghua.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Min Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:z-m@tsinghua.edu.cn">z-m@tsinghua.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiqun Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yiqunliu@tsinghua.edu.cn">yiqunliu@tsinghua.edu.cn</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shaoping Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:msp@tsinghua.edu.cn">msp@tsinghua.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">
<br class="ltx_break"/>Department of Computer Science and Technology,
Institute for Artificial Intelligence,
<br class="ltx_break"/>Beijing National Research Center for Information Science and Technology,

<br class="ltx_break"/>Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id4.id1">Nowadays, personalized recommender systems play an increasingly important role in music scenarios in our daily life with the preference prediction ability.
However, existing methods mainly rely on users’ implicit feedback (e.g., click, dwell time) which ignores the detailed user experience.
This paper introduces Electroencephalography (EEG) signals to personal music preferences as a basis for the personalized recommender system.
To realize collection in daily life, we use a dry-electrodes portable device to collect data.
We perform a user study where participants listen to music and record preferences and moods. Meanwhile, EEG signals are collected with a portable device.
Analysis of the collected data indicates a significant relationship between music preference, mood, and EEG signals.
Furthermore, we conduct experiments to predict personalized music preference with the features of EEG signals. Experiments show significant improvement in rating prediction and preference classification with the help of EEG. Our work demonstrates the possibility of introducing EEG signals in personal music preference with portable devices. Moreover, our approach is not restricted to the music scenario, and the EEG signals as explicit feedback can be used in personalized recommendation tasks.</p>
</div>
<div class="ltx_keywords">Preference prediction, Electroencephalography, Brain signals, Dry electrodes
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct conference title from your rights confirmation email; August 18–21,2022; China</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Ubiquitous and mobile computing theory, concepts and paradigms</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the era of information explosion, the recommender system plays an increasingly important role, which helps users filter preferred items, such as products, news, and music (<cite class="ltx_cite ltx_citemacro_citep">(Bobadilla et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib3" title="">2013</a>)</cite>).
The most difficult part of a recommender system is accurately predicting users’ preferences. However, existing methods mainly rely on users’ implicit feedback such as click and dwell time, and attempt to connect them with users’ subjective feelings. But this implicit feedback is usually inaccurate and noisy. It ignores the detailed user experience and may be different from the true feelings.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recently, with the development of neuroscience technology (e.g., Electroencephalogram (EEG)), access to users’ explicit feedback to circumvent the problems mentioned above has become possible. Since EEG records the firing information of neuronal activity in the brain, it attracts great attention and is applied to research in text inputting, controlling PC <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib18" title="">2021</a>)</cite>, and brain activities analysis, especially mood recognition <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib41" title="">2014</a>)</cite>. While mood plays an essential role in personal preference judgment, EEG can be decoded as explicit feedback. Thus, we introduce brain signals into the personalized recommender system.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Wet electrodes are commonly used to measure EEG signals. The application of conductive gel on skin yield strong EEG signals. However, these processes are typically troublesome for users. A measurement device that can be realized in people’s daily life is particularly important. <cite class="ltx_cite ltx_citemacro_citet">Lin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib17" title="">2017</a>)</cite> proposes to apply EEG in real-life settings.
Therefore, we use a commercially available EEG Bluetooth headset, a comfortable portable device to measure the users’ forehead using dry electrodes. It can provide users with easy and fast daily monitoring to collect the brain signals when they listen to music effectively.
The recommender system can infer users’ moods and experiences with EEG signals, which can be used as explicit real-time feedback.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Nevertheless, inferring personal music preference with portable devices is challenging. Collecting EEG signals with dry electrodes may cause a low signal-to-noise ratio (<cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib17" title="">2017</a>)</cite>). As suggested by <cite class="ltx_cite ltx_citemacro_citet">Cross (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib8" title="">2001</a>)</cite>, music preference judgment is a higher mental process that involves complex cognitive behaviors. Therefore, a sizeable cognitive gap exists between music preferences and EEG signals collected with portable devices. Considering the above challenges, we pay great attention to data preprocessing in this work.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Since music is closely related to daily life, we conduct an experiment in the music scenario.
Psychologists have studied the associations between mood and music. <cite class="ltx_cite ltx_citemacro_citet">Xue and Li (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib36" title="">2018</a>)</cite> clarifies the impact of different self-centered moods on music preference, which shows that current mood influences music choice. On the other hand, music is universal partly because it regulates the effect on mood <cite class="ltx_cite ltx_citemacro_citep">(Swaminathan and Schellenberg, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib30" title="">2015</a>)</cite>. The so-called Music Mood Induction Procedure (MMIP) relies on music to produce changes in experience and has been utilized to study the impact on moods <cite class="ltx_cite ltx_citemacro_citep">(Västfjäll, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib34" title="">2001</a>)</cite>.
Since mood affects the choice of music, it is crucial to get people’s emotions in time. However, without interruption, we could not have explicit labels on users’ emotions. Our work shows that EEG signals from the portable device reflect moods. From this point, perceived preference is possible.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Thus, in our work, we collected EEG signals when listening to music and the feedback of preference and mood the participant labeled for every track of music.
Then, we preprocess data for noise reduction and extract typical EEG features based on the collected data. Then we show the relationship between EEG and preference as well as mood.
In the end, we conduct preference prediction with EEG signals. User, music, and EEG features are fused as input for preference prediction. The performance shows a significant improvement by EEG signals.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The main contributions of this work are as follows:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a direction to introduce EEG signals in personal music preference. The performance significantly improved with the help of EEG signals, showing the power of EEG signals to identify users’ explicit feedback.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A user study is conducted in music scenarios. We preprocess EEG signals and analysis the preference association. We discuss the influence of EEG features over electrodes and frequency bands.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We collect the data by portable EEG device, which reveal a new way for future new generation music recommendation when portable devices are popular and convenient <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The code and data is available at https://github.com/hezy18/EEG_music</span></span></span>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">The remainder of this paper is organized as follows: We review related work in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S2" title="2. Related Work ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>. Then we introduce our user study methodology in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3" title="3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">3</span></a> and provide analysis and findings in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4" title="4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">4</span></a>.
Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5" title="5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">5</span></a> presents the experiments for preference prediction and further analysis.
And we discuss our concerns and limitations in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S6" title="6. Discussion and Limitations ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">6</span></a>. Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S7" title="7. Conclusion and Future Work ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">7</span></a> provides the conclusions of our work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we review related work on personalized music recommender systems, EEG-based preference detection, and other EEG-related research.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Personalized Music Recommender Systems</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Music is one of the most critical scenarios for recommender systems.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">There are many proposals for recommendation methods, most of which are based on collaborative filtering (CF) <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib35" title="">2021</a>)</cite>.
These approaches made predictions about a user’s preference (e.g., rating) on an item based on the users or items with similar preferences.
Collaborative filtering based on user id and item id has also been applied to music recommendation successfully <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib29" title="">2012</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Since users’ music preferences are dynamic and diverse, music recommendation relies on context attributes <cite class="ltx_cite ltx_citemacro_citep">(Deldjoo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib9" title="">2021</a>; Murthy and Koolagudi, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib21" title="">2018</a>)</cite>. In conventional content-based music recommendation, music audio signals and descriptive metadata were used as item-level content <cite class="ltx_cite ltx_citemacro_citep">(Hansen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib13" title="">2020</a>; Yoshii et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib39" title="">2006</a>; Van Den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib33" title="">2013</a>; Sachdeva et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib25" title="">2018</a>; Basilico and Hofmann, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib2" title="">2004</a>)</cite>.
In Cheng’s work <cite class="ltx_cite ltx_citemacro_citep">(Cheng and Shen, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib7" title="">2016</a>)</cite>, the characteristics of venues and music were mapped into a latent semantic space to measure the similarity between music and locations.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">As more user profiles can be collected from online behaviors, user-generated context information has been considered to improve the recommendation performance. <cite class="ltx_cite ltx_citemacro_citet">Yapriady and Uitdenbogerd (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib38" title="">2005</a>)</cite> applied user demographic data, and <cite class="ltx_cite ltx_citemacro_citet">Shen et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib27" title="">2020</a>)</cite> extracted emotion-oriented user features from the user blog text for music recommendations.
The social context was also considered, such as online social networks on Facebook <cite class="ltx_cite ltx_citemacro_citep">(Mesnage et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib20" title="">2011</a>)</cite>, social influence <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib4" title="">2019</a>)</cite>, and cultures of users <cite class="ltx_cite ltx_citemacro_citep">(Zangerle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib40" title="">2018</a>; Schedl et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib26" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">However, these works focused on the context data on the Internet, which is not real-time.
With real-time EEG signals in the physical world, it is possible to predict users’ music preferences in real-time.
Moreover, a better understanding of users’ preferences will lead to better recommendation results. Traditional music recommender systems mainly use implicit feedback from users, such as click and dwell time.
But hearing does not mean preferring, and the clicks do not show preference. As for explicit feedback such as rating, it is difficult for users to provide it every time proactively. Instead, we perform personalized recommendations with EEG signals from the portable device to capture the real-time changes in users’ music preferences. It can replace explicit feedback, which is hard to obtain.
</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>EEG Based Preference Detection</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Several research works have been proposed to use EEG signals in predicting the self-reported preferences (in terms of ratings) and the consumers’ behavior (e.g., the choice of the products) in various scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In some works, a recommendation framework is proposed with the use of EEG to obtain the psychological state before information need is expressed. <cite class="ltx_cite ltx_citemacro_citet">Yadava et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib37" title="">2013</a>)</cite> proposed a framework of a recommendation system for e-commerce products by fusing the pre-purchase and post-purchase ratings. EEG signals of users have been recorded while watching 3D products. The relative power of EEG was used to reflect the pre-purchase rating.
<cite class="ltx_cite ltx_citemacro_citet">Lee et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib16" title="">2020</a>)</cite> focused on music scenarios to provide users for depression therapy with a list of soothing music tracks.
These previous works proposed the concept of replacing part of the preference system with EEG but lacked the analysis and computing of the relationship between EEG and preference.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Guo and Elgendi (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib12" title="">2017</a>)</cite> proposed a neuro-signal-based framework to predict the consumer choices for online products.
Participants chose one of three images of the same product type and used EEG signals to predict which product was preferred.
Features were modeled with HMM classifier and achieved an accuracy of 70.33%.
However, it manually restricted the scope of product selection, and the experimental settings differed from the product selection process in practice.
<cite class="ltx_cite ltx_citemacro_citet">Kumar et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib15" title="">2019</a>)</cite> proposed a multi-modal framework to integrate physiological signals with product reviews to obtain the overall evaluation of a product.
It focused on the quality of products rather than a personalized preference for the items.
Moreover, these works used wet electrode EEG, which sacrifices convenience and is impossible for day-to-day usage.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">These previous works indicate that EEG has great potential to capture the preferences of people.
However, they paid little attention to applying EEG for preference prediction in the personalized recommendation scenario.
In our work, we try to incorporate the EEG signals explicitly into personalized recommendations.
Furthermore, dry electrode portable device could be used in daily life in the future.
So we implement our experiments with commercial dry electrode devices for EEG collection.
</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Unlike previous work, we genuinely introduce EEG analysis into personalized preference detection in the music recommendation scenario.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Other EEG Related Research</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">A growing number of studies are applying EEG signals, such as controlling keyboards and mice, inputting speech and text with thoughts, and monitoring human emotions.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">EEG signals have been applied to perform some simple interactions for people in the human-computer-interaction applications.
<cite class="ltx_cite ltx_citemacro_citet">T et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib31" title="">2010</a>)</cite> tried to develop a BMI system composed of a spelling module and a web-browsing tool. They trained an SVM model to predict the target button based on EEG signals and showed that participants could use some basic functions of a web search engine with the system.
<cite class="ltx_cite ltx_citemacro_citet">Perego et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib24" title="">2011</a>)</cite> designed a hand-free BCI-based search system that illustrates the possibility of using BCI to replace keyboard and mouse in real life.
Recently, EEG’s potential in information retrieval has been proposed by <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib18" title="">2021</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Chen et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib6" title="">2021</a>)</cite> proposed a prototype for virtual typing and searching tasks on computers with EEG devices. <cite class="ltx_cite ltx_citemacro_citet">P et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib22" title="">2018</a>)</cite> showed that people with tetraplegia could use an intracortical BMI to control a computer to use some popular applications, e.g., web browsing, email, and chatting.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Furthermore, EEG signals have made inputting text and speech with thoughts possible.
<cite class="ltx_cite ltx_citemacro_citet">Herff et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib14" title="">2015</a>)</cite> showed for the first time that continuously spoken speech can be decoded into the expressed words from BMI recordings. Their system can achieve word error rates as low as 25% and phone error rates below 50%.
Furthermore, brain signals could be translated into speech based on a neural decoder that explicitly leverages kinematic and sound representations encoded in human cortical
activity to synthesize audible speech <cite class="ltx_cite ltx_citemacro_citep">(G.K. et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib10" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">In the psychological scenario, EEG signals have been utilized to recognize and analyze human moods.
In Zheng’s work <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib41" title="">2014</a>)</cite>, the combination of deep belief networks and the hidden Markov model was used to classify two kinds of emotions (positive and negative) using EEG signals.
<cite class="ltx_cite ltx_citemacro_citet">Soleymani et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib28" title="">2012</a>)</cite> proposed a user-independent emotion recognition method to recover effective tags for videos using EEG signals, pupillary response, and gaze distance.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">These existing research efforts show the application potential of EEG in various fields.
We believe EEG is also helpful for personalized recommendation scenarios.
However, existing research is not feasible to apply to everyday life as they are mainly based on the wet-electrode EEG device or invasive electrodes.
In our work, we use a portable dry-electrode device, which has little effect on user experience.
Although it is a great challenge that the collected signals are with low spatial resolution and poor signal-to-noise ratio, we have seen the prospect of its application <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib17" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1">Besides, previous works show that EEG has a relationship with mood, and music is also related to mood <cite class="ltx_cite ltx_citemacro_citep">(Xue and Li, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib36" title="">2018</a>; Swaminathan and Schellenberg, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib30" title="">2015</a>)</cite>. So we believe mood is also a variable to help personalized music recommendations by indicating users’ preferences.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>User Study Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We conducted a user study to collect EEG signals, moods, and preferences while listening to music.
Participants were required to wear a commercial EEG Bluetooth headset with dry electrodes, listen to designated music, and give preference and mood feedback. They would be paid according to the experiment time. The whole user study has been reviewed and approved by the Department of Psychology Ethics Committee, Tsinghua University (THU202118).
This section will introduce our user study procedures and data preprocessing methods.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>User Study Procedure</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Procedure Overview</h4>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S3.F1.g1" src="extracted/2404.15753v1/Figures/8.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">User study procedure and the user-computer interface of the experiments.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The overall illustration of the user study procedure is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.F1" title="Figure 1 ‣ 3.1.1. Procedure Overview ‣ 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>(a).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">At the beginning of the user study, every participant was well informed of the experiment process and required to sign the informed consent.
Then questionnaires about their basic information, such as gender and age, would be filled out.
Then the experiment for music preference and EEG signals collection was conducted.
Firstly, we helped the participant wear a 6-electrode dry electrode EEG headset and confirmed the Bluetooth connection to ensure the recording and transferring of EEG signals.
Then, Each participant listened to 20 randomly selected tracks.
They are suggested to listen to each track for at least one minute without moving their body to minimize noise. Then the participant would label their preference rating (in five levels) and mood (with a two-dimensional Thayer model map <cite class="ltx_cite ltx_citemacro_citep">(Thayer, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib32" title="">1990</a>)</cite>) in each track.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="260" id="S3.F2.g1" src="extracted/2404.15753v1/Figures/10.jpg" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Two-dimensional description map of Thayer mood, where the X-axis represents valence, and the Y-axis represents arousal. </span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">A concise website interface was designed for music listening as well as preference and mood labeling. The main page is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.F1" title="Figure 1 ‣ 3.1.1. Procedure Overview ‣ 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>(b).
As we required participants to record mood, an intuitive method to label mood was necessary. Balancing the label precision and complexity, we implemented the Thayer mood model <cite class="ltx_cite ltx_citemacro_citep">(Thayer, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib32" title="">1990</a>)</cite> with a two-dimensional description map of mood, i.e., valence (positive or negative) and arousal (energetic degree), as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.F2" title="Figure 2 ‣ 3.1.1. Procedure Overview ‣ 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>. During the user study, participants will choose the corresponding location on the page for the mood brought about by listening to the music. Before the experiment, all participants were trained to use the map to describe moods.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">After the experiment, we collected the basic information through questionnaires, the EEG signals when listening to music, and the user feedback in terms of preference and mood of each track. The obtained dataset will then be preprocessed.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1">Participants, music preparation, and apparatus in the experiment are detailed below.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Participants</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">enrolled in this study, and one of them had no data records due to equipment reasons.
Among the remaining 19 participants, 11 were female and 8 were male, aged 20-26 (M=22.26, SD=1.56<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>M for mean value, SD for standard deviation</span></span></span>).
All participants are native Chinese speakers mastering college-level Chinese reading and writing skills. And they admit that they are right-handed and do not suffer from any neurological disease.
Signed consent was obtained from each participant before the experiment was carried out.
It takes about one hour to complete the whole task for each participant, and each participant is paid $7.91 after they complete the tasks.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Music data preparation</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">We selected 1000 music tracks from the Million Song Dataset, which includes audio features and metadata for a million contemporary popular music tracks.
Then, we divided the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS3.p1.1.1">music valence</span> feature into ten equal intervals and selected the top 1000 popular music tracks from each interval. After music tracks with <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS3.p1.1.2">demo</span> or <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS3.p1.1.3">live</span> in their names being excluded, 9704 music tracks were left.
Then, we randomly selected 20 tracks from the remaining 9704 tracks, and all participants were required to listen to these 20 tracks.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4. </span>Apparatus</h4>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S3.F3.g1" src="extracted/2404.15753v1/Figures/2.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">The portable EEG headset deployed in our lab study with the mark of electrodes and their corresponding sensor locations. Red circles are the recorded electrodes, and the blue circle is the reference electrode.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">Our study used a laptop computer with a 17-inch monitor with a resolution of 1,600×900.
We deployed a 6-dry-electrode headset to capture the participants’ EEG data during the experiment continuously. The electrodes were placed on the scalp, and Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.F3" title="Figure 3 ‣ 3.1.4. Apparatus ‣ 3.1. User Study Procedure ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">3</span></a> shows the 6-electrode EEG cap layout used in this study. The signals were digitized at 250 Hz and stored in a PC for offline analysis.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p2">
<p class="ltx_p" id="S3.SS1.SSS4.p2.1">Among six electrodes, O2 is the reference electrode. The value of each electrode is represented by the difference between it and O2. Electrode AF8, Fp2, Fp1, and AF7 are in the frontal lobe. O1 and O2 are at occipital lobel.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>User Behavior and EEG Data Preprocessing</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We achieved a series of data by user study, including user demographics, preferences, moods, and EEG signals. However, we cannot use EEG signals in the data analyses. Alignment, normalization, filtering, and feature extraction need to be fulfilled in this process.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Data Alignment and Denoising</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">In the user study, each pair of labels (preferences and moods) was expected to correspond to a segment of EEG signals.
We regarded the time of clicking on ’music start’ as when the EEG signals start. As participants should listen for at least 90 seconds, we truncated each EEG segment for 90s (that is, 3000 sampling points). We noted the interval time from the ’music start’ button to the ’music pause’ button of the same piece of music. Cases with less than 60s (due to the equipment reason) would be removed. Then, For the same electrodes for each participant, we did a min-max normalization to minimize the difference between each participant.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">A critical step is filtering the data to remove low-frequency drifts. The slow drifts are problematic because they reduce the independence of the assumed-to-be-independent sources (e.g., during a slow upward drift, the neural activity, heartbeat, blink, and other muscular sources will all tend to have higher values), making it harder for the algorithm to find an accurate solution. Thus, we use a high-pass filter with a 1 Hz cutoff frequency. We enumerate a sample in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.F4" title="Figure 4 ‣ 3.2.1. Data Alignment and Denoising ‣ 3.2. User Behavior and EEG Data Preprocessing ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">4</span></a>, reflecting the change of time series data before and after filtering.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S3.F4.g1" src="extracted/2404.15753v1/Figures/3.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Time series data before and after removing low-frequency drift for a sample.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>EEG Feature Extraction</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">While EEG signals look like a waveform that vibrates in a very complex pattern, power spectrum analysis that classifies according to frequency is used frequently when observing EEG. Power spectrum analysis assumes that the EEG signal is a linear combination of simple vibrations that vibrate at a specific frequency. Thus, we can decompose each frequency component in this signal to indicate its magnitude (or power). In our study, the frequency power of trials was extracted with Welch’s method with windows of 250 samples. The changes of power were averaged over the frequency bands of theta (4-8 Hz), alpha (8-12 Hz), beta (12-30 Hz), gamma (30-45 Hz), and the whole frequency.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">We compute PSD <cite class="ltx_cite ltx_citemacro_citep">(Gramfort et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib11" title="">2013</a>)</cite> over the five frequency band mentioned above of time series at each electrode. Thus, five passbands (4-8 Hz, 8-12 Hz, 12-30 Hz, 30-45 Hz, and all-pass bands) are combined with five electrodes (AF8, Fp2, Fp1, AF7, and O1) in pairs. We get a total of 25 EEG features for each case.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Label Statistics</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">After this process, 319 cases from 19 participants remain. Each case shows a participant’s feedback to an item (i.e., music) with preference and mood labels, as well as a segment of EEG signals during listening to the music. As for the preference (in terms of rating), cases were rated 1-5. Specifically, among the 319 cases, 35 cases (10.97%) were rated 1, 62 cases (19.44%) were rated 2, 112 (35.11%) were rated 3, 77 (24.14%) and 33 cases (10.34%) were rated 4 and 5 respectively. Most records were rated 2, 3, and 4.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">Next, we analyse the relationship between mood and preference. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.F5" title="Figure 5 ‣ 3.2.3. Label Statistics ‣ 3.2. User Behavior and EEG Data Preprocessing ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">5</span></a>(a) shows the locations of each cases on the arousal-valence plane with the colors standing for preference. Valence ranged 0.299-0.71 (M=0.528, SD=0.093) and arousal ranged 0.083-0.863 (M=0.456, SD=0.212).
The magnitude of arousal is greater than that of valence.
Furthermore, preferences tend to be positively correlated to valences, and extreme preferences (i.e., rating 1 and 5) are usually accompanied with low arousal.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S3.F5" title="Figure 5 ‣ 3.2.3. Label Statistics ‣ 3.2. User Behavior and EEG Data Preprocessing ‣ 3. User Study Methodology ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">5</span></a>(b) better indicates this conclusion.
The mean values in each preference category demonstrate that as the preference increases, the valence also increases (F-statics=34, p-value&lt;1e-23 <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Results come from ANOVA test</span></span></span>).
Meanwhile, moderate preference corresponds to higher arousal, and extreme preference tends to have lower arousal (F-statics=4, p-value&lt;0.005). The analysis shows that different preferences bring different moods. Moreover, valence and arousal have different meanings in the description of preference.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="249" id="S3.F5.g1" src="extracted/2404.15753v1/Figures/5.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.4.2.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S3.F5.2.1" style="font-size:90%;">The distribution of mood (valence and arousal) in each preference (in terms of rating) and the relationship between mood and preference. The colors represent preferences(in terms of ratings). P stands for <math alttext="p" class="ltx_Math" display="inline" id="S3.F5.2.1.m1.1"><semantics id="S3.F5.2.1.m1.1b"><mi id="S3.F5.2.1.m1.1.1" xref="S3.F5.2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.F5.2.1.m1.1c"><ci id="S3.F5.2.1.m1.1.1.cmml" xref="S3.F5.2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.1.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="S3.F5.2.1.m1.1e">italic_p</annotation></semantics></math> value from the ANOVA test between 5 groups(rating 1-5).</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>EEG Analysis and Findings</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We found a significant relationship between moods and preference in the previous section.
In this section, we provide a statistical analysis of music preferences, mood records, and EEG signals in the user study to explore the usefulness of EEG features.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Relationship Between Music Preference and EEG signals</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To explore the correlation between EEG and preference,
we compute the Pearson correlated coefficients between the PSD and preference (in terms of rating, in the following, we will mix the usage of terms <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">preference</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">rating</span>) of each case. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.F6" title="Figure 6 ‣ 4.1. Relationship Between Music Preference and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">6</span></a> shows the correlations with significantly (p&lt;0.05) correlating electrodes highlighted. For electrodes with no records, we set the correlation coefficient to 0.
We can find that as the frequency of the passband increases, the overall correlation of PSD and preference turns positive to negative.
The correlation coefficient ranged from -0.18 to 0.03, demonstrating that the EEG signals vary greatly at different frequencies in different locations.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="112" id="S4.F6.g1" src="extracted/2404.15753v1/Figures/6.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">The correlations (overall cases) of the preference (in terms of rating) with PSD in the broad frequency bands of theta (4-8 Hz), alpha (8-12 Hz), beta (12-30 Hz), gamma (30-45 Hz) and all-pass. The highlighted stars indicate a significant correlation (p&lt;0.05) between preference and PSD features.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Generally, the PSD and preference have a certain linear correlation at the partial electrode of the partial band.
The PSD at electrodes AF8 and Fp2 in the broad frequency bands of 30-45 Hz significantly correlate. Besides, PSD at all electrodes except O1 in the all-pass band has a relatively higher correlation with preference, which is a negative correlation. Therefore, we further analyze these specific electrodes for 30-45 Hz and all-pass band.
</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S4.F7.g1" src="extracted/2404.15753v1/Figures/7.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.6.3.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S4.F7.4.2" style="font-size:90%;">Mean values of PSD at specific EEG electrodes for 30-45Hz band and all-passed band in negative, neutral, and positive preference. <math alttext="p" class="ltx_Math" display="inline" id="S4.F7.3.1.m1.1"><semantics id="S4.F7.3.1.m1.1b"><mi id="S4.F7.3.1.m1.1.1" xref="S4.F7.3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.F7.3.1.m1.1c"><ci id="S4.F7.3.1.m1.1.1.cmml" xref="S4.F7.3.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.3.1.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="S4.F7.3.1.m1.1e">italic_p</annotation></semantics></math> indicates P values from the ANOVA test between 3 groups of preference. <math alttext="p^{*}" class="ltx_Math" display="inline" id="S4.F7.4.2.m2.1"><semantics id="S4.F7.4.2.m2.1b"><msup id="S4.F7.4.2.m2.1.1" xref="S4.F7.4.2.m2.1.1.cmml"><mi id="S4.F7.4.2.m2.1.1.2" xref="S4.F7.4.2.m2.1.1.2.cmml">p</mi><mo id="S4.F7.4.2.m2.1.1.3" xref="S4.F7.4.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.F7.4.2.m2.1c"><apply id="S4.F7.4.2.m2.1.1.cmml" xref="S4.F7.4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.F7.4.2.m2.1.1.1.cmml" xref="S4.F7.4.2.m2.1.1">superscript</csymbol><ci id="S4.F7.4.2.m2.1.1.2.cmml" xref="S4.F7.4.2.m2.1.1.2">𝑝</ci><times id="S4.F7.4.2.m2.1.1.3.cmml" xref="S4.F7.4.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.4.2.m2.1d">p^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.F7.4.2.m2.1e">italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> stands for p-value of a two-sided t-test conducted between negative and positive groups for 30-45 Hz band.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">In the self-assessment of preferences, a closed question (like it or not) is easier to answer than an open one (the degree of preference).
Thus,
we divide the cases into three groups according to the scores of preference (ranged from 1 to 5, discretely). Scores 1 and 2 are considered to be negative, label 3 is represented by neutral, and 4-5 are positive. The number of cases in each group is 97, 112, and 110, which is almost evenly distributed.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">For each specific EEG electrode, we compute the mean values of PSD for the all-pass band and 30-45Hz band. Then we conduct an analysis of variance between PSD in three groups. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.F7" title="Figure 7 ‣ 4.1. Relationship Between Music Preference and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">7</span></a> shows the Analysis results.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">As for AF8 at 30-45 Hz band, The p-value from Pearson correlation in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.F8" title="Figure 8 ‣ 4.2. Relationship Between Mood and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">8</span></a> is 0.048, almost near to 0.05. Besides, we further analyze the standard deviation of each group. Compared with Fp2 at 30-45 Hz band, groups of PSD in AF8 have greater deviation, which brings more confounding and insignificant factors.
As the preference changes from positive to negative, the PSD value decreases overall.
Besides, as the neutral group is more like in the fuzzy zone, positive and negative preference is easier to distinguish.
The difference between the three groups (positive, negative, and neutral) is significant, especially at electrodes AF8, Fp1, and Af7 for the all-pass band, which demonstrates the significance and capabilities of the EEG signals at the all-pass band.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Relationship Between Mood and EEG signals</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Correlation statistics are also applied to the relationship between mood and EEG signals. The heat in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.F8" title="Figure 8 ‣ 4.2. Relationship Between Mood and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">8</span></a> represents the coefficient computed by Pearson correlation between each case’s power changes and preference. Significant electrodes (p&lt;0.05) are highlighted.
As the frequency of the passband changes from low to high, the correlation with valence changes from positive to negative, while the correlation with arousal changes from negative to positive. Thus, the mood is depicted in two dimensions that correlate differently with EEG signals.
Besides, PSD at electrodes for the 4-8 Hz band has a relatively higher correlation with valence, which is a positive correlation. Meanwhile, PSD at electrodes for the all-pass band has a significant positive correlation with arousal.
Therefore, we further analyze the 4-8 Hz band for the valence dimension and the all-pass band for the arousal dimension.
</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="207" id="S4.F8.g1" src="extracted/2404.15753v1/Figures/11.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">The mean correlations (overall participants) of the valence and arousal with PSD in the broad frequency bands of theta (4-8 Hz), alpha (8-12 Hz), beta (12-30 Hz), gamma (30-45 Hz) and all-pass. The highlighted stars correlate significantly (p&lt;0.05) with the valence or arousal.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">As for mood, we divide the cases into three groups according to valence and arousal (ranged 0-1, continuously), respectively.
On a 0-1 scale, the threshold for dividing positive and negative is 0.5, which is a clear line when self-assessing valence. Besides, 35% of cases are negative. People are more inclined to label positive moods. Moods without clear positive and negative tendencies are more likely to be marked in the positive position in the middle. Thus, we use 0.5 and 0.6 as the threshold to divide negative, neutral, and positive groups. As for arousal, we use 0.33 and 0.67  (the trisection point between 0-1) as the threshold to divide the three groups. At last, the number of cases in each group is 107, 136, 76, and 107, 136, 76 for valence and arousal, respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="194" id="S4.F9.g1" src="extracted/2404.15753v1/Figures/4.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.3.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" id="S4.F9.4.2" style="font-size:90%;">Mean values of PSD on 4-8 Hz band and all-pass band (in terms of different EEG electrodes) in negative, neutral, and positive groups divided by valence (left) and arousal (right) scores. <span class="ltx_text ltx_font_bold" id="S4.F9.4.2.1">p</span> stands for the p values from the ANOVA test between negative, neutral, and positive groups.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Also, for each EEG electrode, we compute the means of PSD at all electrodes for the 4-8 Hz and all-pass bands. Then we analyze the variance between PSD in three groups for valence and arousal, respectively.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.F9" title="Figure 9 ‣ 4.2. Relationship Between Mood and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">9</span></a> (left),
greater valence gets higher EEG power, while there is little difference between neutral and positive. At electrode Fp1, The difference between negative, neutral, and positive groups is significant.
As for arousal, we can see the obvious distinction between the three groups in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.F9" title="Figure 9 ‣ 4.2. Relationship Between Mood and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">9</span></a> (right). The difference is significant at electrodes AF8, Fp2, AF7, and O1. High arousal has high EEG power overall, especially in the positive group.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">As the arousal dimension is more direct, we see a stronger relationship with arousal than with valence using EEG. The relationship between mood and EEG is closer than the relationship with preference, which is also in line with our perception. As mood and preference are related, adding nonlinear factors, EEG can better describe preference.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.13">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.13.14.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.13.14.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="5" id="S4.T1.13.14.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.13.14.1.2.1">Valence, 4-8 Hz band</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_t" colspan="5" id="S4.T1.13.14.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">  <span class="ltx_text ltx_font_bold" id="S4.T1.13.14.1.3.1">Arousal, all-pass band</span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T1.13.15.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T1.13.15.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">AF8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Fp2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Fp1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">AF7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.13.15.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">O1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">AF8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">Fp2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">Fp1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">AF7</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S4.T1.13.15.2.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">O1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.3.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Negative &amp; Neutral</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.33<sup class="ltx_sup" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.1.1.1.1">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.89<sup class="ltx_sup" id="S4.T1.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.2.2.2.1.1">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.39<sup class="ltx_sup" id="S4.T1.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.3.3.3.1.1">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">-0.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">-0.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">-0.35</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.3.3.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">-1.24</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.7.7.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Neutral &amp; Positive</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.09</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.56</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.62</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.7.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.36</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.60<sup class="ltx_sup" id="S4.T1.4.4.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.31<sup class="ltx_sup" id="S4.T1.5.5.2.1">∗</sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.96</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.47<sup class="ltx_sup" id="S4.T1.6.6.3.1">∗</sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.7.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.57<sup class="ltx_sup" id="S4.T1.7.7.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.7.7.4.1.1">∗</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.13.13.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">Negative &amp; Positive</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.13.13.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.95</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.8.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.09<sup class="ltx_sup" id="S4.T1.8.8.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.8.8.1.1.1">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.9.9.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.02<sup class="ltx_sup" id="S4.T1.9.9.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.9.9.2.1.1">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.13.13.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.58</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.13.13.10" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.59</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.10.10.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.68<sup class="ltx_sup" id="S4.T1.10.10.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.10.10.3.1.1">∗∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.11.11.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.91<sup class="ltx_sup" id="S4.T1.11.11.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.11.11.4.1.1">∗∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.13.13.11" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.40</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.12.12.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.73<sup class="ltx_sup" id="S4.T1.12.12.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.12.12.5.1.1">∗∗</span></sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b" id="S4.T1.13.13.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">3.50<sup class="ltx_sup" id="S4.T1.13.13.6.1"><span class="ltx_text ltx_font_italic" id="S4.T1.13.13.6.1.1">∗∗</span></sup>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.20.3.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S4.T1.17.2" style="font-size:90%;">T statistics by two-side t-test of PSD on 4-8 Hz band and all-pass band (in terms of different EEG electrodes) in positive, neutral, and negative groups divided by valence and arousal scores, respectively. <sup class="ltx_sup" id="S4.T1.17.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.17.2.1.1">∗</span></sup>and <math alttext="{{}^{*}*}" class="ltx_math_unparsed" display="inline" id="S4.T1.17.2.m2.1"><semantics id="S4.T1.17.2.m2.1b"><mmultiscripts id="S4.T1.17.2.m2.1.1"><mo id="S4.T1.17.2.m2.1.1.2">∗</mo><mprescripts id="S4.T1.17.2.m2.1.1b"></mprescripts><mrow id="S4.T1.17.2.m2.1.1c"></mrow><mo id="S4.T1.17.2.m2.1.1.3">∗</mo></mmultiscripts><annotation encoding="application/x-tex" id="S4.T1.17.2.m2.1c">{{}^{*}*}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.2.m2.1d">start_FLOATSUPERSCRIPT ∗ end_FLOATSUPERSCRIPT ∗</annotation></semantics></math> indicate p-value&lt;0.05 and p-value&lt;0.01, respectively.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">We further analyze the difference between the two groups. A two-side independent t-test is conducted between negative and neutral groups, neutral and positive groups, and negative and positive groups. The result is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.T1" title="Table 1 ‣ 4.2. Relationship Between Mood and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">1</span></a>.
As for valence, the difference between the negative and neutral groups is more significant than the difference between the neutral and positive groups is significantly different from the other two groups. Thus, the negative group is more separable.
As for arousal, the positive group is more different from the other groups.
Thus, negative valence and high arousal have better differentiation compared with other conditions using PSD features of EEG signals.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>EEG-aware Music Preference Prediction Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Since we have found a close relationship between EEG signals and music preference, EEG signals and mood, as well as mood and preference, we further introduce EEG signals to predict music preferences (in terms of ratings) <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The code implementation is available at https://github.com/hezy18/EEG_music</span></span></span>.
We use representative models for rating prediction and preference classification tasks to explore the improvement by adding EEG.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Experiment Settings</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In this section, we will introduce the basis experiment settings of our work.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Baseline Features</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">Considering that traditional recommender systems predict preference by the user and item features, we use these features as the baseline. Age and gender represent each <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.1">user</span>, and <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.2">music</span> is described with 12 audio features from Spotify<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://developer.spotify.com/console/get-audio-features-track/</span></span></span>.
Detailed features are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.T2" title="Table 2 ‣ 5.1.1. Baseline Features ‣ 5.1. Experiment Settings ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">It’s worth mentioning that we embed the feature name ”tags” from Spotify. We keep the top 14 most common labels and convert them to a 0-1 sequence using a multi-label binarizer.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.2" style="width:423.1pt;height:130.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.5pt,7.2pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.1.1">Group</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.2.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.3.1">Feature Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.2.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.4.1">Data type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.1.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.5.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.1.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.6.1">Feature Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.1.1.7" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.7.1">Data type</span></th>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.1.2.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">user</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.2.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Questionnaire</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.2.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">age</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.2.1.2.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">int</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.2.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Questionnaire</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.2.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">gender</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.2.2.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">bool</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.2.1.3.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.1.3.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">popularity</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.2.1.3.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">int</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">year</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.1.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">int</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.4.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.1.4.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">danceability</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.2.1.4.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">energy</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.2.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.1.5.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">item</th>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">loudness</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.2.1.5.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">speechiness</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.3.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.6.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.1.6.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.6.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.6.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">valence</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.2.1.6.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.6.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.6.4.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">acousticness</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.6.4.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.7.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.1.7.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">instrumentalness</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.2.1.7.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.5.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.5.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">liveness</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.5.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.8.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T2.2.1.8.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.1.8.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.1.8.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">tempo</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.2.1.8.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">float</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.1.8.6.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">Spotify</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.1.8.6.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">tags</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.1.8.6.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">0-1 sequence</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.3.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S5.T2.4.2" style="font-size:90%;">Fourteen features for baseline were extracted from the questionnaire and Spotify acoustic feature set.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Based models</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">To explore the effect of adding EEG on a variety of typical models, we choose based models from both traditional machine learning models and neural models. The selected model and related settings are described below.
</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">As for traditional machine learning models, we use models with ensemble learning.
<span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p2.1.1">Gradient Boostintg Decision Tree (GBDT)</span>, <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p2.1.2">Random Forest (RF)<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="S5.SS1.SSS2.p2.1.2.1.1">(</span>Pedregosa et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S5.SS1.SSS2.p2.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib23" title="">2011</a><span class="ltx_text ltx_font_medium" id="S5.SS1.SSS2.p2.1.2.3.3">)</span></cite></span> and eXtreme Gradient Boosting <cite class="ltx_cite ltx_citemacro_citep">(Chen and Guestrin, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib5" title="">2016</a>)</cite>(XGBoost).
<span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p2.1.3">Multi-layer Perceptron (MLP)</span> is used as neural model. We conduct three layers. LBFGS was used to optimize the loss of squared error and log-loss function for rating prediction and preference classification, respectively.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Rating Prediction</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Recommender system is based on predicting the user’s preference for the item. To introduce EEG into recommendation, we start with EEG prediction preference (in terms of rating, ranging from 1 to 5). Five times 5-fold validation are conducted on both baseline and PSD-added condition. The task is music and participant dependent. The performance is estimated by Mean Squared Error (MSE) and is shown in Table<a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.T3" title="Table 3 ‣ 5.2. Rating Prediction ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">3</span></a> . We fairly call out the best results for each model under various parameters.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">It can be seen that after adding EEG, the effect of prediction is improved significantly on these typical models. XGBoost has improved for 23.5% from the baseline. The best results are obtained with MLP with the mean MSE over 5 tiems of 5-fold cross validation is 0.9025.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.4.5.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.5.1.1.1">Features</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.5.1.2.1">GBDT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.5.1.3.1">RF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.5.1.4.1">XGBoost</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.5.1.5.1">MLP</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.4.6.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">user,item</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.6.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.9801</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.6.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.9811</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.6.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">1.1953</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.6.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.9471</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.4.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">user,item,PSD</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.9274<sup class="ltx_sup" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.1.1.1.1.1">∗∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.9336<sup class="ltx_sup" id="S5.T3.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S5.T3.2.2.2.1.1">∗∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.9146<sup class="ltx_sup" id="S5.T3.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S5.T3.3.3.3.1.1">∗∗</span></sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.4.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.4.4.4.1">0.9025</span><sup class="ltx_sup" id="S5.T3.4.4.4.2"><span class="ltx_text ltx_font_italic" id="S5.T3.4.4.4.2.1">∗</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.7.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T3.4.7.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.4.7.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">(-5.4%)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.4.7.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">(-4.8%)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.4.7.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">(-23.5%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T3.4.7.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">(-4.7%)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.12.3.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S5.T3.8.2" style="font-size:90%;">Overall performance of baseline method (using users and items features) and our method (adding EEG signals, specifically, the PSD) to predict ratings in terms of Mean Squared Error (MSE, the lower the better). Two-side t-test is conducted for 5 times 5-fold cross validation. <sup class="ltx_sup" id="S5.T3.8.2.1"><span class="ltx_text ltx_font_italic" id="S5.T3.8.2.1.1">∗</span></sup>and <sup class="ltx_sup" id="S5.T3.8.2.2"><span class="ltx_text ltx_font_italic" id="S5.T3.8.2.2.1">∗∗</span></sup> indicate p-value&lt;0.05 and p-value&lt;0.005, respectively. underlineUnderline indicates the best results.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Preference Classification</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In the self-assessment of preferences, a closed question (like it or not) is easier to answer than an open one (the degree of preference). Therefore, in practical applications, it is sufficient for people to use binary labels of preferences.
Thus, we conduct the task to classify preference into three groups (negative, neutral and positive) and use accuracy as evaluation metric. Also, five times 5-fold validation are conducted with music and participant dependent for both baseline and PSD-added condition. We call out the best performance with parameter tuning for each model.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.3.4.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.3.4.1.1.1">Features</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.4.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.3.4.1.2.1">GBDT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.4.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.3.4.1.3.1">RF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.4.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.3.4.1.4.1">XGBoost</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.4.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.3.4.1.5.1">MLP</span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.3.5.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">user,item</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.5.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.4540</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.5.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.4351</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.5.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.4150</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.5.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.4639</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.3.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">user,item,PSD</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.4703</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.4915<sup class="ltx_sup" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T4.1.1.1.1.1">∗∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">0.4865<sup class="ltx_sup" id="S5.T4.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.2.2.2.1.1">∗</span></sup>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4.3.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.3.3.1">0.4927</span><sup class="ltx_sup" id="S5.T4.3.3.3.2"><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.3.2.1">∗</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.6.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T4.3.6.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.3.6.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">(+3.6%)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.3.6.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">(+13.0%)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.3.6.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">(+5.4%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T4.3.6.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">(+6.2%)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.11.3.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S5.T4.7.2" style="font-size:90%;">Overall performance of baseline method (using users and items features) and our method (adding EEG signals, specifically, the PSD) to classify preference into negative, neutral and positive groups. The performance is estimated by accuracy (the higher the better). Two-side t-test is conducted for 5 times 5-fold cross validation. <sup class="ltx_sup" id="S5.T4.7.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.7.2.1.1">∗</span></sup>and <sup class="ltx_sup" id="S5.T4.7.2.2"><span class="ltx_text ltx_font_italic" id="S5.T4.7.2.2.1">∗∗</span></sup> indicate p-value&lt;0.05 and p-value&lt;0.005, respectively. underlineUnderline indicates the best results.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">The performance of models improve with the help of EEG. Random forest (RF) improved 13.0% from the baseline. The best results are obtained with MLP with the mean accuracy over 5 times of 5-fold cross validation is 0.4927.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Feature Analysis</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">To further analyse the importance of difference features how they affect the model, we conduct ablation study and feature importance analysis.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1. </span>Ablation Study</h4>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.1">We conducted an ablation study to further analyse the usefulness of EEG features in the model.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.F10" title="Figure 10 ‣ 5.4.1. Ablation Study ‣ 5.4. Feature Analysis ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">10</span></a>, experiments are performed with removing each band or electrodes, that is, removing five features at a time. We held the experiments based on the best models with theirs settings and parameters for rating prediction and preference classification, respectively.</p>
</div>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S5.F10.g1" src="extracted/2404.15753v1/Figures/1.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.2.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" id="S5.F10.3.2" style="font-size:90%;">The performance of the best model after removing each group of bands (a and c) or each group of electrodes (b and d) in the rating prediction (a and b) and preference classification tasks (c and d), respectively. Red dotted lines represent the performance with all the features.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.SSS1.p2">
<p class="ltx_p" id="S5.SS4.SSS1.p2.1">As for the electrodes, whether in the rating prediction or the preference classification task, the first two models with the smallest performance loss are to remove Fp2 and O1, especially O1.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4.F6" title="Figure 6 ‣ 4.1. Relationship Between Music Preference and EEG signals ‣ 4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">6</span></a>) in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4" title="4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">4</span></a>, only these two electrodes (Fp2 and O1) did not have strongly correlated bands with preference. Besides, because the distance between O1 and the reference electrode O2 is the closest, and the position is symmetrical, it’s explainable that O1 has no significant correlation with preference. Furthermore, AF8 and AF7 perform well for both tasks.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS1.p3">
<p class="ltx_p" id="S5.SS4.SSS1.p3.1">As for the frequency bands, removing the 4-8 Hz band brings a great loss to the model performance. Besides, the 8-12 Hz band seems important to the preference classification model. 4-8 Hz and 8-12 Hz bands behave opposite over two tasks. We speculate this is due to differences in rating prediction and preference classification models. The activate function for preference classification is ’tanh’, which adds nonlinearity to the model, while rating prediction is ’identity’.
Besides, after removing the 30-45 Hz band in preference classification, the model performance is about the same as before (p = 0.57 from the two-side t-test). The result demonstrates that the 30-45 Hz band has little effect on the model’s performance in preference classification tasks.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS1.p4">
<p class="ltx_p" id="S5.SS4.SSS1.p4.1">Two sum up, relatively uniform performance is present on the different electrodes for both tasks. Electrode AF8 and AF7 are important, while O1 and Fp2 perform poorly overall.
Difference frequency bands are useful to the model differently. Features performance changes due to the settings of the task. 4-8 Hz band is important to rating prediction while 8-12 Hz band is important to preference classification.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2. </span>Feature Importance Analysis</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.1">The frequency bands and the electrodes are analyzed in the ablation study. Here we look at the specific features and the importance of PSD features in all features.</p>
</div>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="390" id="S5.F11.g1" src="extracted/2404.15753v1/Figures/9.jpg" width="589"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.2.1.1" style="font-size:90%;">Figure 11</span>. </span><span class="ltx_text" id="S5.F11.3.2" style="font-size:90%;">SHAP values (in terms of impact on model output) of top 10 and lowest 10 features for every sample in rating prediction task (a) and preference classification task (b), respectively. The features are sorted by their sum of SHAP magnitudes over fifty samples. The color represents the feature value (red high, blue low).</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.SSS2.p2">
<p class="ltx_p" id="S5.SS4.SSS2.p2.1">We randomly divide the train set and test set with the features of users, items, and EEG signals. Then we train the model (MLP) for rating prediction and preference classification tasks on our best performance. Then, SHapley Additive exPlanations <cite class="ltx_cite ltx_citemacro_citep">(Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib19" title="">2017</a>)</cite> ((SHAP) is used to analyze the importance of each feature. We randomly select fifty samples and calculate the SHAP values of every feature for every sample. Then we sort them by the sum of SHAP magnitudes over all samples. The
Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5.F11" title="Figure 11 ‣ 5.4.2. Feature Importance Analysis ‣ 5.4. Feature Analysis ‣ 5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">11</span></a> shows the top 10 and the lowest 10 features from fifty-seven features. SHAP values show the distribution of the impacts each feature has on the model output.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p3">
<p class="ltx_p" id="S5.SS4.SSS2.p3.1">As for the rating prediction task, among the top-10 important features, eight of ten are item features, one is the user’s feature, and the other is the EEG feature. It demonstrates that item features are irreplaceable.
Besides, it reveals that a high PSD at AF7 of 4-8 Hz band lowers the predicted ratings obviously. It seems with the conclusion of the ablation study that electrode AF7 and the 4-8 Hz frequency band show importance.
From the lowest-10 features, electrodes and bands vary. Features in the all-pass band are opposite to ratings, obviously. It illustrates the negative correlation between them and ratings, which we have analyzed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4" title="4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p4">
<p class="ltx_p" id="S5.SS4.SSS2.p4.1">As for the preference classification task, two PSD features are in the top 10.
They are PSD from the 8-12 Hz band, showing the importance of the 8-12 Hz band in this task. The conclusion is the same as with the ablation study.
Besides, it reveals the totally different contribution of PSD at O1 and AF8 of the 8-12 Hz band. High PSD in the former lowers the preference (from positive to negative) and in the latter higher the preference (from negative to positive).
Furthermore, the lowest-2 features are from the 30-45 Hz band, which is shown in the ablation study that it has little effect on the performance.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p5">
<p class="ltx_p" id="S5.SS4.SSS2.p5.1">Overall, some EEG features are more important than some user or item features, which indicates their significant supplement to the traditional recommendation using only user and item information.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion and Limitations</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Discussion</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Our work shows the potential to introduce EEG signals in personalized music recommendations with portable EEG devices.
Here we discuss its contribution to personal music preferences, adoption of portable devices to record EEG signals, and privacy concerns.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>EEG’s Contribution to Personal Music Preferences</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">Since EEG has recently demonstrated its value in various fields, some researchers are starting to apply it to the IR field. They use EEG signals to reveal relevance in information retrieval <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib18" title="">2021</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib6" title="">2021</a>)</cite>.
Besides, psychological research on affective computing with EEG illustrates the relationship between EEG and emotion. At the same time, there is an interactive relationship between music and emotions. Preferences and moods are related, so we use EEG signals to study people’s preferences in the music scene, which is the basis of personalized recommender systems. To our knowledge, this is the first work that genuinely introduces EEG analysis into personalized music recommendations.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">In our work, we collected the data of 319 cases from 20 participants with EEG data when listening to music and feedback on preference and mood for listening to the music. PSD is extracted as the features of EEG signals. We analyze the relationship between EEG signals and preferences and the relationship between EEG signals and mood in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S4" title="4. EEG Analysis and Findings ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">4</span></a>. The results illustrate that EEG signals can distinguish moods well. Besides, EEG is associated with preference, which varies across electrodes and frequency bands.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p3">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">And experiments in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#S5" title="5. EEG-aware Music Preference Prediction Experiments ‣ Introducing EEG Analyses to Help Personal Music Preference Prediction"><span class="ltx_text ltx_ref_tag">5</span></a> illustrate that PSD extracted from portable EEG signals can help promote music preference prediction. There is significant improvement after introducing EEG to rating prediction and preference classification tasks via different typical models.
Further analysis demonstrates feature importance for rating prediction.
Therefore, promoting the prediction of music preference with the help of EEG signals is practical despite the cognitive gap between high-level music preference and portable EEG data.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p4">
<p class="ltx_p" id="S6.SS1.SSS1.p4.1">Moreover, our approach is not limited to preference prediction in music recommendations. Promising results with EEG signals also encourage applications of preference prediction in other scenarios.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Adoption of Portable Devices to Record EEG Signals</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">In this work, we collected EEG signals with the dry-electrode portable device.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p2">
<p class="ltx_p" id="S6.SS1.SSS2.p2.1">Although EEG signals have been widely used in previous works for mood detection (<cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib41" title="">2014</a>; Soleymani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib28" title="">2012</a>)</cite>), wet-electrode with higher sampling frequency and much higher signal-to-noise ratio. So adoption of dry-electrode portable devices limits the utilization of EEG signals. However, it is of great value to attempt dry-electrode portable devices since they are closer to real-life applications in the foreseeable future than lab-based professional equipment such as wet-electrode devices.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p3">
<p class="ltx_p" id="S6.SS1.SSS2.p3.1">It is a great challenge that the collected signals with dry-electrode EEG devices have low spatial resolution and high noise (<cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15753v1#bib.bib17" title="">2017</a>)</cite>). To solve the problem, we preprocessed the EEG signals by high-frequency filtering, normalization, and extracting typical features, reducing noise and getting clear information from EEG.
The encouraging results of our experiments show the possibility of adopting these portable EEG devices in research and application.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p4">
<p class="ltx_p" id="S6.SS1.SSS2.p4.1">Meanwhile, we must point out that we believe the better results can be achieved with high-precision devices.
We are looking forward to more attempts in personal music preference with more accurate dry-electrode portable EEG devices, as the cost of these devices will fall with the development of technology.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>Privacy Concern in Personalized Recommendation</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">Compared with conventional personalized recommendation methods, the opportunities of constructing personalized recommender systems come with risks for privacy challenges.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p2">
<p class="ltx_p" id="S6.SS1.SSS3.p2.1">On the one hand, no personally identifiable information is collected in the experiment. On the other hand, users must be informed about the content of the collected data, the method of collection, and the data usage.
In our work, we explained the information collection and storage strategy with written informed consent and researchers face-to-face with each participant.
They also agreed that their data would be used in the following experiments and made public after full anonymity.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p3">
<p class="ltx_p" id="S6.SS1.SSS3.p3.1">However, we must admit that our privacy protection strategy is not enough if the recommender system with EEG portable device is used on a large scale.
In the future, we will try to deal with the privacy issue from the perspective of data collection recommender models.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Limitation</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Despite our best efforts, this work has several limitations.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">First of all, as our experiment was conducted limited to the stability of EEG apparatus, the scale of available data collection is only 319 cases from 20 participants.
The dataset sample size makes it impossible to conduct deeper and more effective models such as the state-of-the-art recommender system to better predict preference. Besides, limited by sample size, it’s hard to analyze the relationship of PSD with preference and examine the usefulness of bracelet data more thoroughly.
Moreover, participants for the questionnaire survey and user study are mostly university students, so the demographic diversity is insufficient.
However, these limitations should be evaluated under the consideration that ours is the first study to introduce EEG signals in personal music preference and bring great potential for integration in the field of recommender systems.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Besides, high data noise and rare electrodes give a hard-to-ignore impact, which is surely the disadvantage of dry-electrode portable devices. Although we try our best to reduce noise, it still brings unexplained differences over electrodes and frequency bands. Noise reduction signal decomposition may be a great problem in EEG study, but it is definitely a great challenge in our device. But based on the shared prospection of using EEG for daily life recommendation systems, the problems posed by dry electrodes need to be overcome.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">Lastly, the overall performance and ablation study shows limited promotion with EEG signals.
On the one hand, it is related to the low precision of the device discussed in the previous subsection.
On the other hand, we only extract PSD as the feature of EEG signals.
We believe that more improvement can be achieved with the utilization of more EEG features and specific-designed methods for denoising in the future.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we introduce portable EEG data to personal music preferences.
Then we conduct a user study with 20 participants to collect EEG signals with a dry-electrodes portable EEG device. EEG signals are collected in the process of listening to music, while feedback on mood and preference are also labeled.
Based on the data collection, we preprocess the EEG signals with the extraction of PSD features. Then, we analyze the relationship between EEG signals and preference as well as mood.
Experiments on rating prediction and preference classification demonstrate significant improvement with PSD features of EEG signals. Further analysis of the models shows the difference in importance of PSD over electrodes and frequency band. Theta  (4-8 Hz) band and alpha (8-12 Hz) band shows the contribution to rating prediction and preference classification, respectively.
The promising results of data analysis and experiments illustrate that introducing EEG signals to personal music preference in a recommendation scenario is possible.
Our work provides a new direction for conducting music recommendations in daily life. Moreover, our approach is not limited to the music scenario. The EEG signals as explicit feedback (in terms of preference) can be used in personalized recommendation tasks, which we leave as future work.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work is supported by the Natural Science Foundation of China (Grant No. U21B2026) and Tsinghua University Guoqiang Research Institute.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">We would like to thank all subjects who participated in the study and all partners in the THUIR lab for their support, assistance, and input throughout the research.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Basilico and Hofmann (2004)</span>
<span class="ltx_bibblock">
Justin Basilico and
Thomas Hofmann. 2004.

</span>
<span class="ltx_bibblock">Unifying Collaborative and Content-Based
Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Twenty-First
International Conference on Machine Learning</em> (Banff, Alberta, Canada)
<em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">(ICML ’04)</em>. Association for
Computing Machinery, New York, NY, USA,
9.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1015330.1015394" title="">https://doi.org/10.1145/1015330.1015394</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bobadilla et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Jesús Bobadilla,
Fernando Ortega, Antonio Hernando, and
Abraham Gutiérrez. 2013.

</span>
<span class="ltx_bibblock">Recommender systems survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Knowledge-based systems</em>
46 (2013), 109–132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jinpeng Chen, Pinguang
Ying, and Ming Zou. 2019.

</span>
<span class="ltx_bibblock">Improving music recommendation by incorporating
social influence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Multimedia Tools and Applications</em>
78, 3 (2019),
2667–2687.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Guestrin (2016)</span>
<span class="ltx_bibblock">
Tianqi Chen and Carlos
Guestrin. 2016.

</span>
<span class="ltx_bibblock">XGBoost: A Scalable Tree Boosting System. In
<em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining</em> (San Francisco,
California, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">(KDD ’16)</em>.
ACM, New York, NY, USA,
785–794.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2939672.2939785" title="">https://doi.org/10.1145/2939672.2939785</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xuesong Chen, Ziyi Ye,
Xiaohui Xie, Yiqun Liu,
Weihang Su, Shuqi Zhu,
Min Zhang, and Shaoping Ma.
2021.

</span>
<span class="ltx_bibblock">Web Search via an Efficient and Effective
Brain-Machine Interface.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">CoRR</em> abs/2110.07225
(2021).

</span>
<span class="ltx_bibblock">arXiv:2110.07225

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.07225" title="">https://arxiv.org/abs/2110.07225</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng and Shen (2016)</span>
<span class="ltx_bibblock">
Zhiyong Cheng and Jialie
Shen. 2016.

</span>
<span class="ltx_bibblock">On effective location-aware music recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ACM Transactions on Information Systems
(TOIS)</em> 34, 2 (2016),
1–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cross (2001)</span>
<span class="ltx_bibblock">
Ian Cross.
2001.

</span>
<span class="ltx_bibblock">Music, cognition, culture, and evolution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Annals of the New York Academy of sciences</em>
930, 1 (2001),
28–42.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deldjoo et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yashar Deldjoo, Markus
Schedl, and Peter Knees.
2021.

</span>
<span class="ltx_bibblock">Content-driven Music Recommendation: Evolution,
State of the Art, and Challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">arXiv preprint arXiv:2107.11803</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">G.K. et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Anumanchipalli G.K.,
Chartier J., and Chang E.F.
2019.

</span>
<span class="ltx_bibblock">Speech synthesis from neural decoding of spoken
sentences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Nature</em> 568
(2019).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41586-019-1119-1" title="">https://doi.org/10.1038/s41586-019-1119-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gramfort et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Alexandre Gramfort, Martin
Luessi, Eric Larson, Denis A. Engemann,
Daniel Strohmeier, Christian Brodbeck,
Roman Goj, Mainak Jas,
Teon Brooks, Lauri Parkkonen, and
Matti S. Hämäläinen.
2013.

</span>
<span class="ltx_bibblock">MEG and EEG Data Analysis with
MNE-Python.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Frontiers in Neuroscience</em>
7, 267 (2013),
1–13.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3389/fnins.2013.00267" title="">https://doi.org/10.3389/fnins.2013.00267</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo and Elgendi (2017)</span>
<span class="ltx_bibblock">
Guibing Guo and Mohamed
Elgendi. 2017.

</span>
<span class="ltx_bibblock">Analysis of EEG signals and its application to
neuromarketing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Multimedia Tools and Applications</em>
76 (2017), 19087–19111.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11042-017-4580-6" title="">https://doi.org/10.1007/s11042-017-4580-6</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Casper Hansen, Christian
Hansen, Lucas Maystre, Rishabh Mehrotra,
Brian Brost, Federico Tomasi, and
Mounia Lalmas. 2020.

</span>
<span class="ltx_bibblock">Contextual and sequential user embeddings for
large-scale music recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Fourteenth
ACM Conference on Recommender Systems</em>. 53–62.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herff et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Christian Herff, Dominic
Heger, Adriana de Pesters, Dominic
Telaar, Peter Brunner, Gerwin Schalk,
and Tanja Schultz. 2015.

</span>
<span class="ltx_bibblock">Brain-to-text: Decoding spoken phrases from phone
representations in the brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Frontiers in Neuroscience</em>
9, MAY (2015).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3389/fnins.2015.00217" title="">https://doi.org/10.3389/fnins.2015.00217</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Sudhanshu Kumar, Mahendra
Yadava, and Partha Pratim Roy.
2019.

</span>
<span class="ltx_bibblock">Fusion of EEG response and sentiment analysis of
products review to predict customer satisfaction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Information Fusion</em> 52
(2019), 41–52.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.inffus.2018.11.001" title="">https://doi.org/10.1016/j.inffus.2018.11.001</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Shih-Hsiung Lee, Tzu-Yu
Chen, Yu-Ting Hsien, and Lin-Roung
Cao. 2020.

</span>
<span class="ltx_bibblock">A Music Recommendation System for Depression
Therapy Based on EEG. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">2020 IEEE International
Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)</em>.
1–2.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCE-Taiwan49838.2020.9258021" title="">https://doi.org/10.1109/ICCE-Taiwan49838.2020.9258021</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Chin-Teng Lin, Chun-Hsiang
Chuang, Zehong Cao, Avinash Kumar Singh,
Chih-Sheng Hung, Yi-Hsin Yu,
Mauro Nascimben, Yu-Ting Liu,
Jung-Tai King, Tung-Ping Su, and
Shuu-Jiun Wang. 2017.

</span>
<span class="ltx_bibblock">Forehead EEG in Support of Future Feasible Personal
Healthcare Solutions: Sleep Management, Headache Prevention, and Depression
Treatment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">IEEE Access</em> 5
(2017), 10612–10621.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ACCESS.2017.2675884" title="">https://doi.org/10.1109/ACCESS.2017.2675884</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yiqun Liu, Jiaxin Mao,
Xiaohui Xie, Min Zhang, and
Shaoping Ma. 2021.

</span>
<span class="ltx_bibblock">Challenges in Designing a Brain-Machine Search
Interface.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">SIGIR Forum</em> 54,
2, Article 3 (aug
2021), 13 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3483382.3483387" title="">https://doi.org/10.1145/3483382.3483387</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lundberg and Lee (2017)</span>
<span class="ltx_bibblock">
Scott M Lundberg and
Su-In Lee. 2017.

</span>
<span class="ltx_bibblock">A Unified Approach to Interpreting Model
Predictions. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information
Processing Systems</em>, I. Guyon,
U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates,
Inc.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mesnage et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Cédric S Mesnage, Asma
Rafiq, Simon Dixon, and Romain P
Brixtel. 2011.

</span>
<span class="ltx_bibblock">Music discovery with social networks. In
<em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Workshop on Music Recommendation and Discovery</em>.
Association for Computing Machinery New York, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murthy and Koolagudi (2018)</span>
<span class="ltx_bibblock">
YV Srinivasa Murthy and
Shashidhar G Koolagudi. 2018.

</span>
<span class="ltx_bibblock">Content-based music information retrieval (cb-mir)
and its applications toward the music industry: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ACM Computing Surveys (CSUR)</em>
51, 3 (2018),
1–46.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">P et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Nuyujukian P,
Albites Sanabria J, Saab J,
Pandarinath C, Jarosiewicz B, and
Blabe CH. 2018.

</span>
<span class="ltx_bibblock">Cortical control of a tablet computer by people
with paralysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">PLoS ONE</em> (2018).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1371/journal.pone.0204566" title="">https://doi.org/10.1371/journal.pone.0204566</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pedregosa et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
F. Pedregosa, G.
Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J.
Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and
E. Duchesnay. 2011.

</span>
<span class="ltx_bibblock">Scikit-learn: Machine Learning in Python.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Journal of Machine Learning Research</em>
12 (2011), 2825–2830.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perego et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
P. Perego, A. C.
Turconi, G. Andreoni, L. Maggi,
E. Beretta, S. Parini, and
C. Gagliardi. 2011.

</span>
<span class="ltx_bibblock">Cognitive ability assessment by Brain-Computer
Interface. Validation of a new assessment method for cognitive abilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Journal of Neuroscience Methods</em>
201, 1 (2011),
239–250.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.jneumeth.2011.06.025" title="">https://doi.org/10.1016/j.jneumeth.2011.06.025</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachdeva et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Noveen Sachdeva, Kartik
Gupta, and Vikram Pudi.
2018.

</span>
<span class="ltx_bibblock">Attentive neural architecture incorporating song
features for music recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings
of the 12th ACM Conference on Recommender Systems</em>.
417–421.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schedl et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Markus Schedl, Christine
Bauer, Wolfgang Reisinger, Dominik
Kowald, and Elisabeth Lex.
2020.

</span>
<span class="ltx_bibblock">Listener Modeling and Context-Aware Music
Recommendation Based on Country Archetypes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Frontiers in Artificial Intelligence</em>
3 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tiancheng Shen, Jia Jia,
Yan Li, Yihui Ma, Yaohua
Bu, Hanjie Wang, Bo Chen,
Tat-Seng Chua, and Wendy Hall.
2020.

</span>
<span class="ltx_bibblock">Peia: Personality and emotion integrated attentive
model for music recommendation on social media platforms. In
<em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, Vol. 34. 206–213.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soleymani et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Mohammad Soleymani, Maja
Pantic, and Thierry Pun.
2012.

</span>
<span class="ltx_bibblock">Multimodal Emotion Recognition in Response to
Videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">IEEE Transactions on Affective Computing</em>
3, 2 (2012),
211–223.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/T-AFFC.2011.37" title="">https://doi.org/10.1109/T-AFFC.2011.37</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Yading Song, Simon Dixon,
and Marcus Pearce. 2012.

</span>
<span class="ltx_bibblock">A survey of music recommendation systems and future
perspectives. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">9th international symposium on
computer music modeling and retrieval</em>, Vol. 4. Citeseer,
395–410.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Swaminathan and Schellenberg (2015)</span>
<span class="ltx_bibblock">
Swathi Swaminathan and
E. Glenn Schellenberg. 2015.

</span>
<span class="ltx_bibblock">Current Emotion Research in Music Psychology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Emotion Review</em> 7,
2 (2015), 189–197.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/1754073914558282" title="">https://doi.org/10.1177/1754073914558282</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">T et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Liu T, Goldberg L,
Gao S, and Hong B.
2010.

</span>
<span class="ltx_bibblock">An online brain-computer interface using
non-flashing visual evoked potentials.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Journal of neural engineering</em>
(2010).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1088/1741-2560/7/3/036003" title="">https://doi.org/10.1088/1741-2560/7/3/036003</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thayer (1990)</span>
<span class="ltx_bibblock">
Robert E Thayer.
1990.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">The biopsychology of mood and arousal</em>.

</span>
<span class="ltx_bibblock">Oxford University Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Den Oord et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Aäron Van Den Oord,
Sander Dieleman, and Benjamin
Schrauwen. 2013.

</span>
<span class="ltx_bibblock">Deep content-based music recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Neural Information Processing Systems Conference
(NIPS 2013)</em>, Vol. 26. Neural Information Processing
Systems Foundation (NIPS).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Västfjäll (2001)</span>
<span class="ltx_bibblock">
Daniel Västfjäll.
2001.

</span>
<span class="ltx_bibblock">Emotion induction through music: A review of the
musical mood induction procedure.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Musicae Scientiae</em> 5,
1_suppl (2001), 173–211.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Le Wu, Xiangnan He,
Xiang Wang, Kun Zhang, and
Meng Wang. 2021.

</span>
<span class="ltx_bibblock">A survey on neural recommendation: From
collaborative filtering to content and context enriched recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:2104.13030</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue and Li (2018)</span>
<span class="ltx_bibblock">
Chao Xue and Tian Li.
2018.

</span>
<span class="ltx_bibblock">The influence of induced mood on music preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Cognitive Processing</em> 19
(2018), 517–525.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s10339-018-0872-7" title="">https://doi.org/10.1007/s10339-018-0872-7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yadava et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Mahendra Yadava, Pradeep
Kumar, Rajkumar Saini, Partha Pratim
Roy, and Debi Prosad Dogra.
2013.

</span>
<span class="ltx_bibblock">A New Recommender System for 3D E-Commerce: An EEG
Based Approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Journal of Advanced Management Sciencee</em>
1, 1 (2013),
61–65.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.12720/joams.1.1.61-65" title="">https://doi.org/10.12720/joams.1.1.61-65</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yapriady and Uitdenbogerd (2005)</span>
<span class="ltx_bibblock">
Billy Yapriady and
Alexandra L Uitdenbogerd. 2005.

</span>
<span class="ltx_bibblock">Combining demographic data with collaborative
filtering for automatic music recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">International Conference on Knowledge-Based and
Intelligent Information and Engineering Systems</em>. Springer,
201–207.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoshii et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Kazuyoshi Yoshii, Masataka
Goto, Kazunori Komatani, Tetsuya Ogata,
and Hiroshi G Okuno. 2006.

</span>
<span class="ltx_bibblock">Hybrid Collaborative and Content-based Music
Recommendation Using Probabilistic Model with Latent User Preferences.. In
<em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">ISMIR</em>, Vol. 6.
296–301.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zangerle et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Eva Zangerle, Martin
Pichl, and Markus Schedl.
2018.

</span>
<span class="ltx_bibblock">Culture-aware music recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the 26th Conference on User
Modeling, Adaptation and Personalization</em>. 357–358.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Wei-Long Zheng, Jia-Yi
Zhu, Yong Peng, and Bao-Liang Lu.
2014.

</span>
<span class="ltx_bibblock">EEG-based emotion classification using deep belief
networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">2014 IEEE International Conference on
Multimedia and Expo (ICME)</em>. 1–6.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICME.2014.6890166" title="">https://doi.org/10.1109/ICME.2014.6890166</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr 30 19:59:45 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
