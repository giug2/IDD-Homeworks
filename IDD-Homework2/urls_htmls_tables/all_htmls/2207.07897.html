<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.07897] 1 Abstract</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Abstract">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="1 Abstract">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.07897">

<!--Generated on Wed Mar 13 13:41:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Transfer learning for time series classification using synthetic data generation</span></p>
<p id="id1.p1.2" class="ltx_p ltx_align_center">Yarden Rotem ,  Nathaniel Shimoni,  Lior Rokach,  Bracha Shapira</p>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Software and Information Systems Engineering</span></p>
<p id="id1.p1.4" class="ltx_p ltx_align_center"><span id="id1.p1.4.1" class="ltx_text ltx_font_italic">Ben-Gurion University of the Negev
<br class="ltx_break"></span>Be’er Sheva, Israel</p>
<p id="id1.p1.5" class="ltx_p ltx_align_center">Email: {rotemyar, nathanie, liorrk, bshapira}@post.bgu.ac.il</p>
<p id="id1.p1.6" class="ltx_p ltx_align_center">In this paper<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This preprint has not undergone peer review or any post-submission improvement or corrections. The Version of Record of this contribution is published in LNCS 13301, CSCML 2022, and is available online at <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-07689-3_18" title="" class="ltx_ref">https://link.springer.com/chapter/10.1007/978-3-031-07689-3_18</a></span></span></span>, we propose an innovative Transfer learning for Time series classification method. Instead of using an existing dataset from the UCR archive as the source dataset, we generated a 15,000,000 synthetic univariate time series dataset that was created using our unique synthetic time series generator algorithm which can generate data with diverse patterns and angles and different sequence lengths. Furthermore, instead of using classification tasks provided by the UCR archive as the source task as previous studies did,we used our own 55 regression tasks as the source tasks, which produced better results than selecting classification tasks from the UCR archive.</p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Abstract</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Both time series classification and transfer learning have increasingly been the focus of research in recent years. However, only a limited number of studies have combined time series classification with transfer learning.
<br class="ltx_break">Time series classification (TSC) is the task of training a classifier to map a given time series input to a probability distribution over the possible class values. Typically, transfer learning (TL) algorithms learn from a source dataset and task and then apply the knowledge gained to another target dataset and task.
<br class="ltx_break">TL has received considerable attention in the domains of computer vision and natural language processing, but less research attention has been devoted to the task of TSC, which is lacking a state-of-the-art pretrained model that can serve as a good starting point for new TSC tasks.
All previous research in the domain of TL for TSC
relied on existing datasets from the UCR archive, the largest publicly available TSC benchmark, in order to choose the optimal source dataset and task; there are some limitations to this approach, however. First,
searching for the optimal source dataset in the UCR archive can be time- and resource-consuming. Second, there is no guarantee that the optimal source dataset exists in the UCR archive. 
<br class="ltx_break">In this paper, we propose an innovative TL for TSC method which addresses the limitations mentioned above. Instead of using an existing dataset from the UCR archive as the source dataset, we generated a 15,000,000 synthetic univariate time series dataset
that was created using our unique synthetic time series generator algorithm which can generate data with diverse patterns and angles and different sequence lengths. Furthermore, instead of using classification tasks provided by the UCR archive as the source task as previous studies did,
we used our own 55 regression tasks as the source tasks, which produced better results than selecting classification tasks from the UCR archive. 
<br class="ltx_break">With our unique source dataset and tasks, we pretrained a CNN (convolutional neural network) model and using 85 TSC datasets from the UCR archive to serve as target dataset, we performed an extensive evaluation of our method. We also reduced the training set of each dataset to only 10% training data in order to emphasize the benefits of using TL for TSC when there is insufficient labeled data.
<br class="ltx_break">Our experimental results show that (1)
on datasets with seasonal characteristics, our method outperforms all other TSC methods (both TL and non-TL methods) on 17 of the 34 seasonal datasets in the UCR archive, whereas the second-best methods outperform on only seven of the 34 seasonal datasets; and
(2) the use of our method improves the test set’s accuracy while reducing training time by 85%, without compromising performance.
We published the code for the entire method which includes a synthetic time series data and regression task generator algorithm and a pretraining and fine-tuning process. We also published the 15,000,000 sample synthetic dataset and the pretrained CNN model.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Introduction</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Transfer learning (TL)</span> is a machine learning (ML) technique that tries to utilize knowledge learned from a source domain in a relevant target domain. The relevant knowledge is applied to the target domain in order to improve the performance of the prediction function of the target domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. The need for sufficient training data exists in most ML tasks, but obtaining labeled data data can be expensive, time-consuming, or in some cases - infeasible. TL is a promising technique which can address this problem by transferring the knowledge across domains, preventing the need for labeled data in sparse domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">TL has also shown to be effective at addressing some of the challenges with training a deep learning model which typically is time-consuming and requires high computational resources. Moreover, when lacking training data, ML models encounter the overfitting problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Transfer learning has also been widely used in computer vision, with state-of-the-art neural network (NN) models such as AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and ViT-G/14 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which is the current leader in terms of top-1 accuracy on the ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> dataset. Evaluation of models pretrained on ImageNet show that accuracy is improved when using TL on new target datasets as opposed to training with the same architecture from scratch  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. TL has also been used effectively for natural language processing (NLP) tasks with pretrained models using word2vec and BERT models, and BERT’s later versions were considered state of the art  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Many studies used pretrained NLP models (such as BERT) to serve as a good starting point for new target datasets. However, for time series (TS) tasks, limited effort has been invested in developing a state-of-the-art, generic, and robust pretrained model that provides a good starting point for a new task.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.4" class="ltx_p">A <span id="S2.p4.4.1" class="ltx_text ltx_font_bold">time series</span> is a series of data samples in a time-based domain, which are typically sampled at a uniform time interval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. There are two main types of TS: univariate time series (UTS) and multivariate time series (MTS)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
An MTS is an <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p4.1.m1.1a"><mi id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><ci id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">M</annotation></semantics></math>-dimensional TS where each data sample consists of <math id="S2.p4.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p4.2.m2.1a"><mi id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><ci id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">M</annotation></semantics></math> real values, e.g., an MTS can be data acquired by measuring multiple climate sensors, such as temperature, humidity, and wind speed, once an hour; this is an <math id="S2.p4.3.m3.1" class="ltx_Math" alttext="M=3" display="inline"><semantics id="S2.p4.3.m3.1a"><mrow id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml"><mi id="S2.p4.3.m3.1.1.2" xref="S2.p4.3.m3.1.1.2.cmml">M</mi><mo id="S2.p4.3.m3.1.1.1" xref="S2.p4.3.m3.1.1.1.cmml">=</mo><mn id="S2.p4.3.m3.1.1.3" xref="S2.p4.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><apply id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1"><eq id="S2.p4.3.m3.1.1.1.cmml" xref="S2.p4.3.m3.1.1.1"></eq><ci id="S2.p4.3.m3.1.1.2.cmml" xref="S2.p4.3.m3.1.1.2">𝑀</ci><cn type="integer" id="S2.p4.3.m3.1.1.3.cmml" xref="S2.p4.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">M=3</annotation></semantics></math> MTS. A UTS is simply an MTS where <math id="S2.p4.4.m4.1" class="ltx_Math" alttext="M=1" display="inline"><semantics id="S2.p4.4.m4.1a"><mrow id="S2.p4.4.m4.1.1" xref="S2.p4.4.m4.1.1.cmml"><mi id="S2.p4.4.m4.1.1.2" xref="S2.p4.4.m4.1.1.2.cmml">M</mi><mo id="S2.p4.4.m4.1.1.1" xref="S2.p4.4.m4.1.1.1.cmml">=</mo><mn id="S2.p4.4.m4.1.1.3" xref="S2.p4.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.4.m4.1b"><apply id="S2.p4.4.m4.1.1.cmml" xref="S2.p4.4.m4.1.1"><eq id="S2.p4.4.m4.1.1.1.cmml" xref="S2.p4.4.m4.1.1.1"></eq><ci id="S2.p4.4.m4.1.1.2.cmml" xref="S2.p4.4.m4.1.1.2">𝑀</ci><cn type="integer" id="S2.p4.4.m4.1.1.3.cmml" xref="S2.p4.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m4.1c">M=1</annotation></semantics></math>; a UTS can be data acquired by sampling the heartbeat of a patient every 10 seconds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
In this paper, we focus only on UTS data.
TS data is relevant for many domains, including the analysis of financial transactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, monitoring network traffic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, the analysis of time-based medical events  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In fact, TS data mining was mentioned as one of the top 10 data mining problems by Yang and Wu <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
<br class="ltx_break">Time series data analysis is a highly focused research domain that has a number of different applications. The three main applications are: <span id="S2.p4.4.2" class="ltx_text ltx_font_bold">time series classification (TSC)</span> - the task of training a classifier to map a given input to a probability over the possible class values (labels) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, time series forecasting - the task of predicting future values of a given sequence using previous data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and time series clustering - the task of dividing a set of TS data into groups, where similar TS samples are put in the same cluster <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. In this work, we focus only on time series classification.
<br class="ltx_break"><span id="S2.p4.4.3" class="ltx_text ltx_font_bold">TL for TSC</span> has not been extensively studied, and a generic, robust, and scalable pretrained model that can serve as a good starting point for new datasets is needed, especially when there is insufficient labeled data. Due to the time-consuming process of collecting and labeling data, the availability of such a pretrained model is essential and would reduce the training time and cost, and in some cases, these models could lead to better overall results. 
<br class="ltx_break">
<br class="ltx_break">In this study, we propose an innovative, generic, scalable, and architecture-agnostic TL for TSC method based on (1) our new algorithm for generating synthetic data and (2) 55 corresponding regression tasks. Our method can be applied to any deep learning CNN-based architecture.
As opposed to previously proposed TL for TSC methods, our model only needs to be pretrained once, and there is no need to search for the optimal source dataset for every new target dataset. Using our unique algorithm, 15,000,000 synthetic samples of UTS data with various angles, sequence lengths, and patterns were used to pretrain our CNN (convolutional neural network) model.
<br class="ltx_break">Using 85 datasets from the UCR archive as target datasets, we perform a comprehensive evaluation of our method.
For datasets with seasonal characteristics, when the amount of training data was reduced to 10%, our method outperforms all other TSC methods (both TL and non-TL methods) on 17 of the 34 seasonal datasets in the UCR archive, whereas the second-best methods outperform on only seven of the 34 seasonal datasets.
<br class="ltx_break">Additionally, using
our method improves the test set’s accuracy while reducing training time by 85%, without compromising performance.
We thus believe that our method can serve as a good starting point for any new target dataset.
<br class="ltx_break">The contributions of this paper are as follows:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><em id="S2.I1.i1.p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Synthetic UTS data and regression task generator algorithm: </em>In this paper, we contribute a new architecture-agnostic TL for TSC method. Unlike previously proposed TL for TSC methods which use an existing source dataset and classification task from the UCR archive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we propose a new algorithm which generates synthetic UTS data and creates 55 corresponding regression tasks which can be used as a source dataset and task. 
<br class="ltx_break">Using existing datasets from the UCR archive as the source dataset has some limitations that our synthetic data overcomes. First, given a target dataset, datasets from the UCR archive may not always be similar or generic enough to serve as a good source dataset. Since our synthetic 15,000,000 sample dataset has a wide variety of patterns, angles, and sequence lengths, it could be a more generic source dataset and therefore be a better fit. A second limitation is that using UCR datasets is not scalable: each update to the UCR archive requires that TL for TSC methods perform a new pretraining procedure to incorporate the new datasets. Since our method relies on the synthetic 15,000,000 sample dataset as a source dataset, no additional pretraining is necessary. Finally, searching for the optimal source dataset from the UCR archive can be time- and resource- consuming. Since we do not use datasets from the UCR but instead use our synthetic dataset, no such search is needed. 
<br class="ltx_break">In this paper, we demonstrate the superiority of a dataset consisting of synthetic data over existing datasets from the UCR archive, by addressing all of the above mentioned issues.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><em id="S2.I1.i2.p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Code contribution: </em>Our code<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Code availble at: <a target="_blank" href="https://github.com/YR234/TL-for-TSC" title="" class="ltx_ref">https://github.com/YR234/TL-for-TSC</a></span></span></span> includes the following:</p>
<ul id="S2.I1.i2.I1" class="ltx_itemize">
<li id="S2.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p1.1" class="ltx_p"><em id="S2.I1.i2.I1.i1.p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">UTS data and regression tasks generator: </em>We created an algorithm to generate synthetic UTS data with a wide range of UTS patterns, angles, and sequence lengths that can serve as a source dataset.</p>
</div>
</li>
<li id="S2.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.I1.i2.p1.1" class="ltx_p"><em id="S2.I1.i2.I1.i2.p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Complete framework: </em>a comprehensive easy-to-use framework that covers data and regression task generation through fine-tuning the pretrained model on a new target dataset and task.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">We publish both the synthetic dataset with 15,000,000 UTS samples and the pretrained CNN model with the <math id="S2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.I1.i3.p1.1.m1.1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.I1.i3.p1.1.m1.1.1.1a" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S2.I1.i3.p1.1.m1.1.1.4" xref="S2.I1.i3.p1.1.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><times id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></times><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">𝐶</ci><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">𝑇</ci><ci id="S2.I1.i3.p1.1.m1.1.1.4.cmml" xref="S2.I1.i3.p1.1.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">CTN</annotation></semantics></math> architecture that was pretrained on that dataset, making them available for use by researchers and the entire ML community.</p>
</div>
</li>
</ol>
<p id="S2.p4.5" class="ltx_p">The remainder of the paper is structured as follows: In section <a href="#S3" title="3 Background and related work" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we provide the necessary background and introduce related work on TSC and TL for TSC methods. Following this, in section <a href="#S4" title="4 Method" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we describe our method, from data generation through fine-tuning the pretrained model on a new target dataset and task.
In section <a href="#S5" title="5 Experimental setup" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we describe the experimental setup, while section <a href="#S6" title="6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> presents our results. Finally, in section <a href="#S7" title="7 Conclusion and future work" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we present our conclusions and plans for future work.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background and related work</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first discuss on related work regarding TSC and TL for TSC methods, and we highlight the differences between those methods and ours.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>TSC related work</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this subsection, we discuss previously proposed TSC methods. 
<br class="ltx_break"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">MultiRocket</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is a TSC method that achieved SOTA (state-of-the-art) results on the entire UCR archive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> at a rate orders of magnitude faster than any other competing method.
<br class="ltx_break">MultiRocket is, in practice, a single-layer convolutional neural network,
where the transformed features from the convolutional kernels form the input for a linear classifier. 
<br class="ltx_break">MultiRocket uses as many as 10,000 convolutional kernels with a wide range of length, padding, dilation, and random weights. After the kernels are generated, each kernel is applied to each input time series, resulting in a feature map. MultiRocket then computes a set of features from the feature map that includes PPV (portion of positive values) plus a randomly selected features from
a set of five candidate features. These features serve as the input for a linear classifier, such as a ridge regression classifier or logistic regression.
<br class="ltx_break">MultiRocket does not use a nonlinear function or have any hidden layers, thus allowing it to be orders of magnitude faster than any other method.
<br class="ltx_break"><span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold">OS-CNN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is a TSC method that uses omni-scale (OS) blocks, which does not need to tune the feature extraction scales. Usually, a core challenge of a CNN is to determine the proper scales of feature extraction. This method uses OS blocks which are made up of OS layers that can be configured automatically from the input size based on a list of kernel sizes; by stacking those layers, this method can achieve full receptive field coverage of the total length of the input (sequence length) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
<br class="ltx_break"><span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold">InceptionTime</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a TSC method that uses an ensemble of five deep CNN models, which was inspired by the Inception-V4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> architecture. This architecture includes several techniques commonly used when constructing a CNN model, such as residual block with shortcut connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and inception modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Each of the five models is given equal weight in the final prediction decision. 
<br class="ltx_break">Because of our TL-based approach, our method differs entirely from the TSC methods mentioned above. In the absence of sufficient labeled data, TL techniques are useful. In this paper, we leverage this by reducing the amount of labeled training data to 10%. Our experimental results indicate that when it comes to seasonal datasets, our method outperforms all other methods, and with all datasets (both seasonal and non-seasonal) our method is only second to MultiRocket, however the difference in the performance of the two methods was not shown to be significant when the Nemenyi statistical test was performed.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2207.07897/assets/waves.png" id="S3.F1.g1" class="ltx_graphics ltx_img_square" width="658" height="665" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The 12 UTS patterns generated in our work.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>TL for TSC related work</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.8" class="ltx_p">The use of TL for TSC has been proposed in a number of studies. In this subsection, we discuss the existing TL for TSC methods and how our TL for TSC method differs from these methods.
<br class="ltx_break">An overview of the general TL for TSC process is presented in Fig. <a href="#S4.F2" title="Figure 2 ‣ 4 Method" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This process consists of the following five steps: First, a source dataset is selected. Second, a source task is selected. In step 3, the model’s architecture is chosen. In step 4, the model chosen in step 3 is pretrained on the source dataset and task selected respectively in steps 1 and 2. The final step consists of fine-tuning the pretrained model from step 4 on a new target dataset and task. 
<br class="ltx_break">While all previous TL for TSC studies used existing datasets and classification tasks from the UCR archive as the source dataset and task for steps 1 and 2, in this paper, we generate synthetic data for the source dataset and use regression tasks instead of classification as the source task, and demonstrate how those two decisions can result in better generalization while eliminating the need for an exhaustive search for the best source dataset.
<br class="ltx_break"><span id="S3.SS2.p1.8.1" class="ltx_text ltx_font_bold">Fawaz et al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> suggested using DTW (dynamic time warping), a technique for finding the optimal alignment between two given time series sequences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, as a similarity measure for finding the most similar source dataset from the UCR archive. The source task is chosen according to the source dataset (provided by the UCR archive). 
<br class="ltx_break">While our method may only differ from the method of Fawaz et al in terms of steps 1 and 2 of the TL for TSC process, our novel approach for creating the source dataset and task from synthetic data and regression tasks instead of using an existing dataset and classification task from the UCR archive addresses other issues that we will discuss later in the paper.
<br class="ltx_break">Our experimental results on the UCR archive showed that the method proposed by Fawaz et al performed positive transfer learning on <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="71/85" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">71</mn><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">/</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">85</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><divide id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></divide><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">71</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">71/85</annotation></semantics></math> datasets, however this approach has some disadvantages. 
<br class="ltx_break"><span id="S3.SS2.p1.8.2" class="ltx_text ltx_font_bold">Kashiparekh et al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> suggested using a convolutional neural network (CNN) based architecture with a multi-head approach for training a given <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">S</annotation></semantics></math> source dataset (<math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">D</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">D_{S}</annotation></semantics></math>) and corresponding <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">S</annotation></semantics></math> classification tasks (<math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="T_{S}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">T</mi><mi id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝑇</ci><ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">T_{S}</annotation></semantics></math>) from the UCR archive.
<br class="ltx_break">The CNN core architecture consists of convolutional layers followed by skip connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which make this architecture a deep one. However, instead of standard fully connected layers followed by a dense layer with the softmax activation function, the authors used <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">S</annotation></semantics></math> fully connected layers and <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">S</annotation></semantics></math> dense layers with the softmax activation function - one for each source dataset and task. 
<br class="ltx_break">The authors randomly selected <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="S=24" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mrow id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mi id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">S</mi><mo id="S3.SS2.p1.8.m8.1.1.1" xref="S3.SS2.p1.8.m8.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">24</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><eq id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1"></eq><ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">𝑆</ci><cn type="integer" id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3">24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">S=24</annotation></semantics></math> datasets from the UCR archive for training and validation, and the remaining 41 datasets were used as test sets (the authors used sequence lengths up to 512, and therefore not all 85 datasets of the UCR archive were evaluated).

<br class="ltx_break">As noted earlier, none of these methods provides a real solution when it comes to real-world problems in the domain of TL for TSC. Since they are limited to the available datasets in the UCR archive, they may not always be able to find the optimal source dataset.
Not only that, when using the method proposed by Fawaz et al, one would have to perform an exhaustive search to find the most similar source dataset for a new target dataset and task.
<br class="ltx_break">In contrast to prior work, our method does not require an exhaustive search, and it is not restricted to datasets available in the UCR archives or any specific sequence length. Since it is based on diverse synthetic data that was generated by our new algorithm, it can be applied to a variety of new target datasets and tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section,We will discuss on our five-step TL for TSC method (see Fig. <a href="#S4.F2" title="Figure 2 ‣ 4 Method" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The first two steps describe the process of generating the source dataset and regression source task. We then describe steps 3-5 where we select and pretrain the CNN architecture and fine-tune the pretrained model on a new target dataset and task.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2207.07897/assets/method_overview.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1076" height="323" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S4.F2.4.1" class="ltx_text ltx_font_bold">Method overview</span>: In step 1, we generate a a 15,000,000 sample UTS source dataset using our Algorithm. After that, we calculate 55 regression tasks for each UTS in the source dataset to be our source tasks. In step 3, we select the CNN architecture (we chose to use <math id="S4.F2.2.m1.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S4.F2.2.m1.1b"><mrow id="S4.F2.2.m1.1.1" xref="S4.F2.2.m1.1.1.cmml"><mi id="S4.F2.2.m1.1.1.2" xref="S4.F2.2.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.F2.2.m1.1.1.1" xref="S4.F2.2.m1.1.1.1.cmml">​</mo><mi id="S4.F2.2.m1.1.1.3" xref="S4.F2.2.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.F2.2.m1.1.1.1b" xref="S4.F2.2.m1.1.1.1.cmml">​</mo><mi id="S4.F2.2.m1.1.1.4" xref="S4.F2.2.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><apply id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1"><times id="S4.F2.2.m1.1.1.1.cmml" xref="S4.F2.2.m1.1.1.1"></times><ci id="S4.F2.2.m1.1.1.2.cmml" xref="S4.F2.2.m1.1.1.2">𝐶</ci><ci id="S4.F2.2.m1.1.1.3.cmml" xref="S4.F2.2.m1.1.1.3">𝑇</ci><ci id="S4.F2.2.m1.1.1.4.cmml" xref="S4.F2.2.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">CTN</annotation></semantics></math>). Then in step 4, we train the CNN with the source dataset and task. Finally, in step 5, we fine-tune the pretrained CNN model on a new target dataset and task.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data generation - source dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We created a synthetic UTS data generator algorithm; using this algorithm, we created a 15,000,000
sample dataset that contains a wide range of UTS with different segment patterns, angles, sequence lengths. This dataset will serve as our source dataset <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">D</mi><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝐷</ci><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">D_{S}</annotation></semantics></math>.
<br class="ltx_break">In our study, we generated only 12 UTS patterns.
However, using our algorithm, many more patterns can be generated.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data generation - source tasks</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Upon generating the source dataset, we proceed to the source task.
Because our target task is TSC (classification), it would be natural to use classification as our source task, however when comparing classification and
regression as source tasks,
we found that regression achieves more accurate results,
and therefore it was chosen as our source task.</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">The 55 tasks are:</p>
<ol id="S4.I1.i1.I1" class="ltx_enumerate">
<li id="S4.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Maximum (Task 1):</span> Given an input UTS, the purpose of the task is to accurately predict the maximum value of the UTS.</p>
</div>
</li>
<li id="S4.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Minimum (Task 2):</span> Given an input UTS, the purpose of the task is to accurately predict the minimum value of the UTS.</p>
</div>
</li>
<li id="S4.I1.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i1.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">STD (Task 3):</span> Given an input UTS, the purpose of the task is to accurately predict the STD (standard deviation) value of the UTS.</p>
</div>
</li>
<li id="S4.I1.i1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i1.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Peaks (Task 4):</span> Given an input UTS, the purpose of the task is to accurately predict the number of high and low peaks.</p>
</div>
</li>
<li id="S4.I1.i1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i1.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Cross median (Task 5):</span> Given an input UTS, the purpose of the task is to accurately predict the number of times the UTS crosses the median value from up to down and vice versa.</p>
</div>
</li>
<li id="S4.I1.i1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I1.i1.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">10 splits (Tasks 6-55):</span> Given an input UTS, we first divide the UTS into 10 equal length segments. For each segment we calculate tasks 1-5 and concatenate them into a 50 value task (10 segments * 5 tasks).</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>CNN model’s architecture</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Our method is architecture-agnostic, meaning that any deep learning network with a convolutional layer based architecture (CNN) can be used. In our research we used the same CNN architecture as Kashiparekh et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> whose work showed it to be an effective architecture for TL. Just one change was made to their architecture; unlike the multi-head approach used by Kashiparekh et al., we used only one dense layer with the sotfmax activation function. This architecture will be denoted as <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.1a" xref="S4.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.4" xref="S4.SS3.p1.1.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝐶</ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝑇</ci><ci id="S4.SS3.p1.1.m1.1.1.4.cmml" xref="S4.SS3.p1.1.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">CTN</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>CNN pretraining</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.8" class="ltx_p">The next step of our method is pretraining the <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.1a" xref="S4.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.4" xref="S4.SS4.p1.1.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><times id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></times><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">𝐶</ci><ci id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">𝑇</ci><ci id="S4.SS4.p1.1.m1.1.1.4.cmml" xref="S4.SS4.p1.1.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">CTN</annotation></semantics></math> model on the source dataset <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">D</mi><mi id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">𝐷</ci><ci id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">D_{S}</annotation></semantics></math> with the source task <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="T_{S}" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><msub id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">T</mi><mi id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">𝑇</ci><ci id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">T_{S}</annotation></semantics></math>.
<br class="ltx_break">To create the training and validation sets, we randomly divided the source dataset <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><msub id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">D</mi><mi id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">𝐷</ci><ci id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">D_{S}</annotation></semantics></math> into an 80%-20% split.
We pretrained the <math id="S4.SS4.p1.5.m5.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S4.SS4.p1.5.m5.1a"><mrow id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml"><mi id="S4.SS4.p1.5.m5.1.1.2" xref="S4.SS4.p1.5.m5.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.5.m5.1.1.1" xref="S4.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.5.m5.1.1.3" xref="S4.SS4.p1.5.m5.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.5.m5.1.1.1a" xref="S4.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.5.m5.1.1.4" xref="S4.SS4.p1.5.m5.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><apply id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1"><times id="S4.SS4.p1.5.m5.1.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1.1"></times><ci id="S4.SS4.p1.5.m5.1.1.2.cmml" xref="S4.SS4.p1.5.m5.1.1.2">𝐶</ci><ci id="S4.SS4.p1.5.m5.1.1.3.cmml" xref="S4.SS4.p1.5.m5.1.1.3">𝑇</ci><ci id="S4.SS4.p1.5.m5.1.1.4.cmml" xref="S4.SS4.p1.5.m5.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">CTN</annotation></semantics></math> model for 100 epochs with a batch size of 128, while performing early stopping on the validation loss. We used the <math id="S4.SS4.p1.6.m6.1" class="ltx_Math" alttext="Adam" display="inline"><semantics id="S4.SS4.p1.6.m6.1a"><mrow id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml"><mi id="S4.SS4.p1.6.m6.1.1.2" xref="S4.SS4.p1.6.m6.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.6.m6.1.1.1" xref="S4.SS4.p1.6.m6.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.6.m6.1.1.3" xref="S4.SS4.p1.6.m6.1.1.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.6.m6.1.1.1a" xref="S4.SS4.p1.6.m6.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.6.m6.1.1.4" xref="S4.SS4.p1.6.m6.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.6.m6.1.1.1b" xref="S4.SS4.p1.6.m6.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.6.m6.1.1.5" xref="S4.SS4.p1.6.m6.1.1.5.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><apply id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1"><times id="S4.SS4.p1.6.m6.1.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1.1"></times><ci id="S4.SS4.p1.6.m6.1.1.2.cmml" xref="S4.SS4.p1.6.m6.1.1.2">𝐴</ci><ci id="S4.SS4.p1.6.m6.1.1.3.cmml" xref="S4.SS4.p1.6.m6.1.1.3">𝑑</ci><ci id="S4.SS4.p1.6.m6.1.1.4.cmml" xref="S4.SS4.p1.6.m6.1.1.4">𝑎</ci><ci id="S4.SS4.p1.6.m6.1.1.5.cmml" xref="S4.SS4.p1.6.m6.1.1.5">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">Adam</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> optimizer and <math id="S4.SS4.p1.7.m7.1" class="ltx_Math" alttext="MSE" display="inline"><semantics id="S4.SS4.p1.7.m7.1a"><mrow id="S4.SS4.p1.7.m7.1.1" xref="S4.SS4.p1.7.m7.1.1.cmml"><mi id="S4.SS4.p1.7.m7.1.1.2" xref="S4.SS4.p1.7.m7.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.7.m7.1.1.1" xref="S4.SS4.p1.7.m7.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.7.m7.1.1.3" xref="S4.SS4.p1.7.m7.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.7.m7.1.1.1a" xref="S4.SS4.p1.7.m7.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.7.m7.1.1.4" xref="S4.SS4.p1.7.m7.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m7.1b"><apply id="S4.SS4.p1.7.m7.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1"><times id="S4.SS4.p1.7.m7.1.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1.1"></times><ci id="S4.SS4.p1.7.m7.1.1.2.cmml" xref="S4.SS4.p1.7.m7.1.1.2">𝑀</ci><ci id="S4.SS4.p1.7.m7.1.1.3.cmml" xref="S4.SS4.p1.7.m7.1.1.3">𝑆</ci><ci id="S4.SS4.p1.7.m7.1.1.4.cmml" xref="S4.SS4.p1.7.m7.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m7.1c">MSE</annotation></semantics></math> (mean square error) as the loss function. 
<br class="ltx_break">The pretraining process was lengthy, taking almost 10 days on a single GPU processor, however this process only needs to happen once. Once the <math id="S4.SS4.p1.8.m8.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S4.SS4.p1.8.m8.1a"><mrow id="S4.SS4.p1.8.m8.1.1" xref="S4.SS4.p1.8.m8.1.1.cmml"><mi id="S4.SS4.p1.8.m8.1.1.2" xref="S4.SS4.p1.8.m8.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.8.m8.1.1.1" xref="S4.SS4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.8.m8.1.1.3" xref="S4.SS4.p1.8.m8.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.8.m8.1.1.1a" xref="S4.SS4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.8.m8.1.1.4" xref="S4.SS4.p1.8.m8.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.8.m8.1b"><apply id="S4.SS4.p1.8.m8.1.1.cmml" xref="S4.SS4.p1.8.m8.1.1"><times id="S4.SS4.p1.8.m8.1.1.1.cmml" xref="S4.SS4.p1.8.m8.1.1.1"></times><ci id="S4.SS4.p1.8.m8.1.1.2.cmml" xref="S4.SS4.p1.8.m8.1.1.2">𝐶</ci><ci id="S4.SS4.p1.8.m8.1.1.3.cmml" xref="S4.SS4.p1.8.m8.1.1.3">𝑇</ci><ci id="S4.SS4.p1.8.m8.1.1.4.cmml" xref="S4.SS4.p1.8.m8.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.8.m8.1c">CTN</annotation></semantics></math> model has been pretrained, we save the weights of the model’s core layers. These weights are used later to fine-tune new target datasets and tasks.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Fine-tuning a new target dataset</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.4" class="ltx_p">The final step of our method is to fine-tune the pretrained <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mrow id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mi id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.1.m1.1.1.1" xref="S4.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.1.m1.1.1.1a" xref="S4.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS5.p1.1.m1.1.1.4" xref="S4.SS5.p1.1.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><times id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1.1"></times><ci id="S4.SS5.p1.1.m1.1.1.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2">𝐶</ci><ci id="S4.SS5.p1.1.m1.1.1.3.cmml" xref="S4.SS5.p1.1.m1.1.1.3">𝑇</ci><ci id="S4.SS5.p1.1.m1.1.1.4.cmml" xref="S4.SS5.p1.1.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">CTN</annotation></semantics></math> model on a new target dataset <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="D_{T}" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><msub id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml"><mi id="S4.SS5.p1.2.m2.1.1.2" xref="S4.SS5.p1.2.m2.1.1.2.cmml">D</mi><mi id="S4.SS5.p1.2.m2.1.1.3" xref="S4.SS5.p1.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><apply id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS5.p1.2.m2.1.1.2.cmml" xref="S4.SS5.p1.2.m2.1.1.2">𝐷</ci><ci id="S4.SS5.p1.2.m2.1.1.3.cmml" xref="S4.SS5.p1.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">D_{T}</annotation></semantics></math> with a new target task <math id="S4.SS5.p1.3.m3.1" class="ltx_Math" alttext="T_{T}" display="inline"><semantics id="S4.SS5.p1.3.m3.1a"><msub id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml"><mi id="S4.SS5.p1.3.m3.1.1.2" xref="S4.SS5.p1.3.m3.1.1.2.cmml">T</mi><mi id="S4.SS5.p1.3.m3.1.1.3" xref="S4.SS5.p1.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.1b"><apply id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.3.m3.1.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS5.p1.3.m3.1.1.2.cmml" xref="S4.SS5.p1.3.m3.1.1.2">𝑇</ci><ci id="S4.SS5.p1.3.m3.1.1.3.cmml" xref="S4.SS5.p1.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.1c">T_{T}</annotation></semantics></math>. 
<br class="ltx_break">We start by initializing the <math id="S4.SS5.p1.4.m4.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S4.SS5.p1.4.m4.1a"><mrow id="S4.SS5.p1.4.m4.1.1" xref="S4.SS5.p1.4.m4.1.1.cmml"><mi id="S4.SS5.p1.4.m4.1.1.2" xref="S4.SS5.p1.4.m4.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.4.m4.1.1.1" xref="S4.SS5.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS5.p1.4.m4.1.1.3" xref="S4.SS5.p1.4.m4.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.4.m4.1.1.1a" xref="S4.SS5.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS5.p1.4.m4.1.1.4" xref="S4.SS5.p1.4.m4.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.4.m4.1b"><apply id="S4.SS5.p1.4.m4.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1"><times id="S4.SS5.p1.4.m4.1.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1.1"></times><ci id="S4.SS5.p1.4.m4.1.1.2.cmml" xref="S4.SS5.p1.4.m4.1.1.2">𝐶</ci><ci id="S4.SS5.p1.4.m4.1.1.3.cmml" xref="S4.SS5.p1.4.m4.1.1.3">𝑇</ci><ci id="S4.SS5.p1.4.m4.1.1.4.cmml" xref="S4.SS5.p1.4.m4.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.4.m4.1c">CTN</annotation></semantics></math> model’s core layers with the pretrained weights that were saved in the last step. Once initialization is complete, newly added fully connected layers can be adjusted so that they better fit the new target dataset and task.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental setup</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we describe our experimental setup, including the datasets, preprocessing, and settings used, as well as the methods we compare our method to.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To evaluate our method, we used the UCR archive, which is the benchmark archive for TSC. In 2002, the archive contained 45 datasets; this increased to 85 datasets in 2015, and as of 2019, 128 datasets are available. All of the datasets contain UTS samples. The archive’s datasets vary in terms of the time series domain covered; the domains include traffic, sound, sensors, motion, image, HAR (human activity recognition), financial, medical, and more. Furthermore, they are diverse in terms of the sequence length (ranging from 8-5,000), number of classes (2-60), number of training samples (12-139,000), and number of test samples (15-139,000). Due to running time considerations, we evaluated our results on just the 85 dataset version of the UCR archive and not on the most update version that includes 128 datasets.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Data preprocessing (reducing the labeled training data to 10%)</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The original train-test split provided by the UCR archive was used. However, instead of using all of the training data, we reduced each dataset’s training data to only 10% of the original, while keeping the same class distribution, e.g., given a dataset of 100 training samples with 70 samples of class 1 and 30 samples of class 2, we reduced the training data to 10 samples, with seven class 1 samples and three class 2 samples. Therefore, the 70-30% class distribution was maintained. 
<br class="ltx_break">The following are some key points regarding the reduction process:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">The remaining 10% of the training samples were chosen at random.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">The reduction process was only performed once.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">All of the test data samples were evaluated (the test data was not reduced).</p>
</div>
</li>
</ol>
<p id="S5.SS2.p1.2" class="ltx_p">This reduction process was used to emphasize the importance of transfer learning, since when there is a lack of labeled data, the pretraining process (steps 1-4 of our method) is expected to provide a better start for learning a new target dataset and task than learning from scratch.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Methods used for comparison</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.6" class="ltx_p">We compare our method, which will now be denoted as <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="CTN\_our" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.1a" xref="S5.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.1.m1.1.1.4" xref="S5.SS3.p1.1.m1.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.1b" xref="S5.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p1.1.m1.1.1.5" xref="S5.SS3.p1.1.m1.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.1c" xref="S5.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.1.m1.1.1.6" xref="S5.SS3.p1.1.m1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.1d" xref="S5.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.1.m1.1.1.7" xref="S5.SS3.p1.1.m1.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.1e" xref="S5.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.1.m1.1.1.8" xref="S5.SS3.p1.1.m1.1.1.8.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><times id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></times><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝐶</ci><ci id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">𝑇</ci><ci id="S5.SS3.p1.1.m1.1.1.4.cmml" xref="S5.SS3.p1.1.m1.1.1.4">𝑁</ci><ci id="S5.SS3.p1.1.m1.1.1.5.cmml" xref="S5.SS3.p1.1.m1.1.1.5">_</ci><ci id="S5.SS3.p1.1.m1.1.1.6.cmml" xref="S5.SS3.p1.1.m1.1.1.6">𝑜</ci><ci id="S5.SS3.p1.1.m1.1.1.7.cmml" xref="S5.SS3.p1.1.m1.1.1.7">𝑢</ci><ci id="S5.SS3.p1.1.m1.1.1.8.cmml" xref="S5.SS3.p1.1.m1.1.1.8">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">CTN\_our</annotation></semantics></math>, to the methods covered in the related work section: Fawaz et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> (denoted as <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="Fawaz" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mi id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.2.m2.1.1.1a" xref="S5.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.2.m2.1.1.4" xref="S5.SS3.p1.2.m2.1.1.4.cmml">w</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.2.m2.1.1.1b" xref="S5.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.2.m2.1.1.5" xref="S5.SS3.p1.2.m2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.2.m2.1.1.1c" xref="S5.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.2.m2.1.1.6" xref="S5.SS3.p1.2.m2.1.1.6.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><times id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1"></times><ci id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">𝐹</ci><ci id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3">𝑎</ci><ci id="S5.SS3.p1.2.m2.1.1.4.cmml" xref="S5.SS3.p1.2.m2.1.1.4">𝑤</ci><ci id="S5.SS3.p1.2.m2.1.1.5.cmml" xref="S5.SS3.p1.2.m2.1.1.5">𝑎</ci><ci id="S5.SS3.p1.2.m2.1.1.6.cmml" xref="S5.SS3.p1.2.m2.1.1.6">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">Fawaz</annotation></semantics></math>), Kashiparekh et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (denoted as <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="ConvTime" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mi id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.3.m3.1.1.1a" xref="S5.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.3.m3.1.1.4" xref="S5.SS3.p1.3.m3.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.3.m3.1.1.1b" xref="S5.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.3.m3.1.1.5" xref="S5.SS3.p1.3.m3.1.1.5.cmml">v</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.3.m3.1.1.1c" xref="S5.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.3.m3.1.1.6" xref="S5.SS3.p1.3.m3.1.1.6.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.3.m3.1.1.1d" xref="S5.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.3.m3.1.1.7" xref="S5.SS3.p1.3.m3.1.1.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.3.m3.1.1.1e" xref="S5.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.3.m3.1.1.8" xref="S5.SS3.p1.3.m3.1.1.8.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.3.m3.1.1.1f" xref="S5.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.3.m3.1.1.9" xref="S5.SS3.p1.3.m3.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><times id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1"></times><ci id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">𝐶</ci><ci id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3">𝑜</ci><ci id="S5.SS3.p1.3.m3.1.1.4.cmml" xref="S5.SS3.p1.3.m3.1.1.4">𝑛</ci><ci id="S5.SS3.p1.3.m3.1.1.5.cmml" xref="S5.SS3.p1.3.m3.1.1.5">𝑣</ci><ci id="S5.SS3.p1.3.m3.1.1.6.cmml" xref="S5.SS3.p1.3.m3.1.1.6">𝑇</ci><ci id="S5.SS3.p1.3.m3.1.1.7.cmml" xref="S5.SS3.p1.3.m3.1.1.7">𝑖</ci><ci id="S5.SS3.p1.3.m3.1.1.8.cmml" xref="S5.SS3.p1.3.m3.1.1.8">𝑚</ci><ci id="S5.SS3.p1.3.m3.1.1.9.cmml" xref="S5.SS3.p1.3.m3.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">ConvTime</annotation></semantics></math>), MultiRocket <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, OS-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and InceptionTime <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>; we also examine the <math id="S5.SS3.p1.4.m4.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S5.SS3.p1.4.m4.1a"><mrow id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml"><mi id="S5.SS3.p1.4.m4.1.1.2" xref="S5.SS3.p1.4.m4.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.4.m4.1.1.1" xref="S5.SS3.p1.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.4.m4.1.1.3" xref="S5.SS3.p1.4.m4.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.4.m4.1.1.1a" xref="S5.SS3.p1.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.4.m4.1.1.4" xref="S5.SS3.p1.4.m4.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><apply id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1"><times id="S5.SS3.p1.4.m4.1.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1.1"></times><ci id="S5.SS3.p1.4.m4.1.1.2.cmml" xref="S5.SS3.p1.4.m4.1.1.2">𝐶</ci><ci id="S5.SS3.p1.4.m4.1.1.3.cmml" xref="S5.SS3.p1.4.m4.1.1.3">𝑇</ci><ci id="S5.SS3.p1.4.m4.1.1.4.cmml" xref="S5.SS3.p1.4.m4.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">CTN</annotation></semantics></math> architecture without the pretraining phase (steps 1-4 in our method), which is denoted as <math id="S5.SS3.p1.5.m5.1" class="ltx_Math" alttext="CTN\_S" display="inline"><semantics id="S5.SS3.p1.5.m5.1a"><mrow id="S5.SS3.p1.5.m5.1.1" xref="S5.SS3.p1.5.m5.1.1.cmml"><mi id="S5.SS3.p1.5.m5.1.1.2" xref="S5.SS3.p1.5.m5.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.5.m5.1.1.1" xref="S5.SS3.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.5.m5.1.1.3" xref="S5.SS3.p1.5.m5.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.5.m5.1.1.1a" xref="S5.SS3.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.5.m5.1.1.4" xref="S5.SS3.p1.5.m5.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.5.m5.1.1.1b" xref="S5.SS3.p1.5.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p1.5.m5.1.1.5" xref="S5.SS3.p1.5.m5.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.5.m5.1.1.1c" xref="S5.SS3.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.5.m5.1.1.6" xref="S5.SS3.p1.5.m5.1.1.6.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m5.1b"><apply id="S5.SS3.p1.5.m5.1.1.cmml" xref="S5.SS3.p1.5.m5.1.1"><times id="S5.SS3.p1.5.m5.1.1.1.cmml" xref="S5.SS3.p1.5.m5.1.1.1"></times><ci id="S5.SS3.p1.5.m5.1.1.2.cmml" xref="S5.SS3.p1.5.m5.1.1.2">𝐶</ci><ci id="S5.SS3.p1.5.m5.1.1.3.cmml" xref="S5.SS3.p1.5.m5.1.1.3">𝑇</ci><ci id="S5.SS3.p1.5.m5.1.1.4.cmml" xref="S5.SS3.p1.5.m5.1.1.4">𝑁</ci><ci id="S5.SS3.p1.5.m5.1.1.5.cmml" xref="S5.SS3.p1.5.m5.1.1.5">_</ci><ci id="S5.SS3.p1.5.m5.1.1.6.cmml" xref="S5.SS3.p1.5.m5.1.1.6">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m5.1c">CTN\_S</annotation></semantics></math> (no transfer learning was applied). We included <math id="S5.SS3.p1.6.m6.1" class="ltx_Math" alttext="CTN\_S" display="inline"><semantics id="S5.SS3.p1.6.m6.1a"><mrow id="S5.SS3.p1.6.m6.1.1" xref="S5.SS3.p1.6.m6.1.1.cmml"><mi id="S5.SS3.p1.6.m6.1.1.2" xref="S5.SS3.p1.6.m6.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.6.m6.1.1.1" xref="S5.SS3.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.6.m6.1.1.3" xref="S5.SS3.p1.6.m6.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.6.m6.1.1.1a" xref="S5.SS3.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.6.m6.1.1.4" xref="S5.SS3.p1.6.m6.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.6.m6.1.1.1b" xref="S5.SS3.p1.6.m6.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p1.6.m6.1.1.5" xref="S5.SS3.p1.6.m6.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.6.m6.1.1.1c" xref="S5.SS3.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.6.m6.1.1.6" xref="S5.SS3.p1.6.m6.1.1.6.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m6.1b"><apply id="S5.SS3.p1.6.m6.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1"><times id="S5.SS3.p1.6.m6.1.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1.1"></times><ci id="S5.SS3.p1.6.m6.1.1.2.cmml" xref="S5.SS3.p1.6.m6.1.1.2">𝐶</ci><ci id="S5.SS3.p1.6.m6.1.1.3.cmml" xref="S5.SS3.p1.6.m6.1.1.3">𝑇</ci><ci id="S5.SS3.p1.6.m6.1.1.4.cmml" xref="S5.SS3.p1.6.m6.1.1.4">𝑁</ci><ci id="S5.SS3.p1.6.m6.1.1.5.cmml" xref="S5.SS3.p1.6.m6.1.1.5">_</ci><ci id="S5.SS3.p1.6.m6.1.1.6.cmml" xref="S5.SS3.p1.6.m6.1.1.6">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m6.1c">CTN\_S</annotation></semantics></math>, so we can examine our results in terms of positive and negative transfer learning and more.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Hyperparameters and other settings</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">For all deep learning methods (Fawaz et al, Kashiparekh et al, OS-CNN, Inception-Time), including ours, we trained each dataset with 2,000 epochs, using cross-entropy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> as the loss function and Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as the optimizer. 
<br class="ltx_break">For MultiRocket (a linear classifier), we used the default parameters provided by the authors.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>The evaluation process</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">The evaluation process includes applying all of the methods on the datasets with the necessary data preprocessing and with the hyperparameters - all this was described at this section. 
<br class="ltx_break">A summary of the results is provided in the next section.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This section begins with a brief summary of the results. We then explore each aspect mentioned in the brief summary in more detail. 
<br class="ltx_break"></p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Results appendices</h3>

<figure id="S6.F3" class="ltx_figure"><img src="/html/2207.07897/assets/x1.png" id="S6.F3.g1" class="ltx_graphics ltx_img_landscape" width="1162" height="446" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Empirical results, from left to right: mean average rank, seasonal wins, wins, losses. Each method is associated with a colored bar.</figcaption>
</figure>
<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">A summary of the results can be seen in Table <a href="#S6.T1" title="Table 1 ‣ 6.1 Results appendices ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and a more visual representation can be found in Fig. <a href="#S6.F3" title="Figure 3 ‣ 6.1 Results appendices ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S6.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of the results in terms of the number of wins, number of losses, seasonal wins, and mean average rank</figcaption>
<table id="S6.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T1.1.1.1" class="ltx_tr">
<th id="S6.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Method</th>
<th id="S6.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Wins</th>
<th id="S6.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Losses</th>
<th id="S6.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Seasonal Wins</th>
<th id="S6.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Mean Avg Rank</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T1.1.2.1" class="ltx_tr">
<td id="S6.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">ConvTime</td>
<td id="S6.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">6</td>
<td id="S6.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">10</td>
<td id="S6.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S6.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">4.4</td>
</tr>
<tr id="S6.T1.1.3.2" class="ltx_tr">
<td id="S6.T1.1.3.2.1" class="ltx_td ltx_align_center">CTN_S</td>
<td id="S6.T1.1.3.2.2" class="ltx_td ltx_align_center">7</td>
<td id="S6.T1.1.3.2.3" class="ltx_td ltx_align_center">12</td>
<td id="S6.T1.1.3.2.4" class="ltx_td ltx_align_center">4</td>
<td id="S6.T1.1.3.2.5" class="ltx_td ltx_align_center">4.365</td>
</tr>
<tr id="S6.T1.1.4.3" class="ltx_tr">
<td id="S6.T1.1.4.3.1" class="ltx_td ltx_align_center">Fawaz</td>
<td id="S6.T1.1.4.3.2" class="ltx_td ltx_align_center">15</td>
<td id="S6.T1.1.4.3.3" class="ltx_td ltx_align_center">14</td>
<td id="S6.T1.1.4.3.4" class="ltx_td ltx_align_center">4</td>
<td id="S6.T1.1.4.3.5" class="ltx_td ltx_align_center">4.2</td>
</tr>
<tr id="S6.T1.1.5.4" class="ltx_tr">
<td id="S6.T1.1.5.4.1" class="ltx_td ltx_align_center">InceptionTime</td>
<td id="S6.T1.1.5.4.2" class="ltx_td ltx_align_center">9</td>
<td id="S6.T1.1.5.4.3" class="ltx_td ltx_align_center"><span id="S6.T1.1.5.4.3.1" class="ltx_text ltx_font_bold ltx_font_italic">7</span></td>
<td id="S6.T1.1.5.4.4" class="ltx_td ltx_align_center">6</td>
<td id="S6.T1.1.5.4.5" class="ltx_td ltx_align_center">3.906</td>
</tr>
<tr id="S6.T1.1.6.5" class="ltx_tr">
<td id="S6.T1.1.6.5.1" class="ltx_td ltx_align_center">OSCNN</td>
<td id="S6.T1.1.6.5.2" class="ltx_td ltx_align_center">14</td>
<td id="S6.T1.1.6.5.3" class="ltx_td ltx_align_center">16</td>
<td id="S6.T1.1.6.5.4" class="ltx_td ltx_align_center">7</td>
<td id="S6.T1.1.6.5.5" class="ltx_td ltx_align_center">3.753</td>
</tr>
<tr id="S6.T1.1.7.6" class="ltx_tr">
<td id="S6.T1.1.7.6.1" class="ltx_td ltx_align_center">CTN_our</td>
<td id="S6.T1.1.7.6.2" class="ltx_td ltx_align_center">26</td>
<td id="S6.T1.1.7.6.3" class="ltx_td ltx_align_center">9</td>
<td id="S6.T1.1.7.6.4" class="ltx_td ltx_align_center"><span id="S6.T1.1.7.6.4.1" class="ltx_text ltx_font_bold ltx_font_italic">17</span></td>
<td id="S6.T1.1.7.6.5" class="ltx_td ltx_align_center">3.494</td>
</tr>
<tr id="S6.T1.1.8.7" class="ltx_tr">
<td id="S6.T1.1.8.7.1" class="ltx_td ltx_align_center">MultiRocket</td>
<td id="S6.T1.1.8.7.2" class="ltx_td ltx_align_center"><span id="S6.T1.1.8.7.2.1" class="ltx_text ltx_font_bold ltx_font_italic">28</span></td>
<td id="S6.T1.1.8.7.3" class="ltx_td ltx_align_center">12</td>
<td id="S6.T1.1.8.7.4" class="ltx_td ltx_align_center">7</td>
<td id="S6.T1.1.8.7.5" class="ltx_td ltx_align_center"><span id="S6.T1.1.8.7.5.1" class="ltx_text ltx_font_bold ltx_font_italic">3.271</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Brief summary of the results</h3>

<div id="S6.SS2.p1" class="ltx_para">
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Thirty-four of the 85 UCR archive datasets have seasonality characteristics. Of these datasets, our method outperforms all other examined methods on 17 datasets; the second best method only outperforms all other methods on seven datasets.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Positive transfer learning occurs in all 2,000 epochs except the first seven epochs, and using our method can save 85% of the training time while achieving the same results (see Fig. <a href="#S6.F4" title="Figure 4 ‣ 6.4 Positive transfer learning ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Our method obtains a mean average rank of 3.494, which is second only to MultiRocket with a mean average rank of 3.271 (the difference between the two values is not significant. In terms of the win/lose rate, our method obtains a 26/9 rate, while MultiRocket’s rate is 28/12; MultiRocket has two more wins, but it also has three more loses than our proposed method.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p">As can been Fig. <a href="#S6.F3" title="Figure 3 ‣ 6.1 Results appendices ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our method comes in at least second place in each case, something no other method achieved.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Seasonality evaluation</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.3" class="ltx_p">We first evaluate the results in terms of seasonality. The autocorrelation values of a given UTS are in the range of <math id="S6.SS3.p1.1.m1.2" class="ltx_Math" alttext="[-1,1]" display="inline"><semantics id="S6.SS3.p1.1.m1.2a"><mrow id="S6.SS3.p1.1.m1.2.2.1" xref="S6.SS3.p1.1.m1.2.2.2.cmml"><mo stretchy="false" id="S6.SS3.p1.1.m1.2.2.1.2" xref="S6.SS3.p1.1.m1.2.2.2.cmml">[</mo><mrow id="S6.SS3.p1.1.m1.2.2.1.1" xref="S6.SS3.p1.1.m1.2.2.1.1.cmml"><mo id="S6.SS3.p1.1.m1.2.2.1.1a" xref="S6.SS3.p1.1.m1.2.2.1.1.cmml">−</mo><mn id="S6.SS3.p1.1.m1.2.2.1.1.2" xref="S6.SS3.p1.1.m1.2.2.1.1.2.cmml">1</mn></mrow><mo id="S6.SS3.p1.1.m1.2.2.1.3" xref="S6.SS3.p1.1.m1.2.2.2.cmml">,</mo><mn id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml">1</mn><mo stretchy="false" id="S6.SS3.p1.1.m1.2.2.1.4" xref="S6.SS3.p1.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.2b"><interval closure="closed" id="S6.SS3.p1.1.m1.2.2.2.cmml" xref="S6.SS3.p1.1.m1.2.2.1"><apply id="S6.SS3.p1.1.m1.2.2.1.1.cmml" xref="S6.SS3.p1.1.m1.2.2.1.1"><minus id="S6.SS3.p1.1.m1.2.2.1.1.1.cmml" xref="S6.SS3.p1.1.m1.2.2.1.1"></minus><cn type="integer" id="S6.SS3.p1.1.m1.2.2.1.1.2.cmml" xref="S6.SS3.p1.1.m1.2.2.1.1.2">1</cn></apply><cn type="integer" id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.2c">[-1,1]</annotation></semantics></math>. Generally, as the autocorrelation values approach zero there is no seasonality and vice versa. So, in this paper, we took the absolute value of the autocorrelation function, and the values will eventually be in the range of <math id="S6.SS3.p1.2.m2.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S6.SS3.p1.2.m2.2a"><mrow id="S6.SS3.p1.2.m2.2.3.2" xref="S6.SS3.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S6.SS3.p1.2.m2.2.3.2.1" xref="S6.SS3.p1.2.m2.2.3.1.cmml">[</mo><mn id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml">0</mn><mo id="S6.SS3.p1.2.m2.2.3.2.2" xref="S6.SS3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS3.p1.2.m2.2.2" xref="S6.SS3.p1.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S6.SS3.p1.2.m2.2.3.2.3" xref="S6.SS3.p1.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.2b"><interval closure="closed" id="S6.SS3.p1.2.m2.2.3.1.cmml" xref="S6.SS3.p1.2.m2.2.3.2"><cn type="integer" id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1">0</cn><cn type="integer" id="S6.SS3.p1.2.m2.2.2.cmml" xref="S6.SS3.p1.2.m2.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.2c">[0,1]</annotation></semantics></math>. Higher absolute values of the autocorrelation function indicate strong seasonality and vice versa. 
<br class="ltx_break">To empirically define datasets with seasonality we calculated a seasonality metric for each dataset, which is denoted as <math id="S6.SS3.p1.3.m3.1" class="ltx_Math" alttext="SM" display="inline"><semantics id="S6.SS3.p1.3.m3.1a"><mrow id="S6.SS3.p1.3.m3.1.1" xref="S6.SS3.p1.3.m3.1.1.cmml"><mi id="S6.SS3.p1.3.m3.1.1.2" xref="S6.SS3.p1.3.m3.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1" xref="S6.SS3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.3" xref="S6.SS3.p1.3.m3.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.3.m3.1b"><apply id="S6.SS3.p1.3.m3.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1"><times id="S6.SS3.p1.3.m3.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1"></times><ci id="S6.SS3.p1.3.m3.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.2">𝑆</ci><ci id="S6.SS3.p1.3.m3.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">SM</annotation></semantics></math> (seasonality metric). 
<br class="ltx_break"></p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.2" class="ltx_p">All datasets with <math id="S6.SS3.p2.1.m1.1" class="ltx_math_unparsed" alttext="SM=&gt;0.5" display="inline"><semantics id="S6.SS3.p2.1.m1.1a"><mrow id="S6.SS3.p2.1.m1.1b"><mi id="S6.SS3.p2.1.m1.1.1">S</mi><mi id="S6.SS3.p2.1.m1.1.2">M</mi><mo rspace="0em" id="S6.SS3.p2.1.m1.1.3">=</mo><mo lspace="0em" id="S6.SS3.p2.1.m1.1.4">&gt;</mo><mn id="S6.SS3.p2.1.m1.1.5">0.5</mn></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">SM=&gt;0.5</annotation></semantics></math> will be considered as seasonal datasets.
The results show that 34 of of the 85 UCR archive datasets are seasonal datasets. Of these 34 datasets, <math id="S6.SS3.p2.2.m2.1" class="ltx_Math" alttext="CTN\_our" display="inline"><semantics id="S6.SS3.p2.2.m2.1a"><mrow id="S6.SS3.p2.2.m2.1.1" xref="S6.SS3.p2.2.m2.1.1.cmml"><mi id="S6.SS3.p2.2.m2.1.1.2" xref="S6.SS3.p2.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.2.m2.1.1.1" xref="S6.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS3.p2.2.m2.1.1.3" xref="S6.SS3.p2.2.m2.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.2.m2.1.1.1a" xref="S6.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS3.p2.2.m2.1.1.4" xref="S6.SS3.p2.2.m2.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.2.m2.1.1.1b" xref="S6.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.SS3.p2.2.m2.1.1.5" xref="S6.SS3.p2.2.m2.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.2.m2.1.1.1c" xref="S6.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS3.p2.2.m2.1.1.6" xref="S6.SS3.p2.2.m2.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.2.m2.1.1.1d" xref="S6.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS3.p2.2.m2.1.1.7" xref="S6.SS3.p2.2.m2.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.2.m2.1.1.1e" xref="S6.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS3.p2.2.m2.1.1.8" xref="S6.SS3.p2.2.m2.1.1.8.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.2.m2.1b"><apply id="S6.SS3.p2.2.m2.1.1.cmml" xref="S6.SS3.p2.2.m2.1.1"><times id="S6.SS3.p2.2.m2.1.1.1.cmml" xref="S6.SS3.p2.2.m2.1.1.1"></times><ci id="S6.SS3.p2.2.m2.1.1.2.cmml" xref="S6.SS3.p2.2.m2.1.1.2">𝐶</ci><ci id="S6.SS3.p2.2.m2.1.1.3.cmml" xref="S6.SS3.p2.2.m2.1.1.3">𝑇</ci><ci id="S6.SS3.p2.2.m2.1.1.4.cmml" xref="S6.SS3.p2.2.m2.1.1.4">𝑁</ci><ci id="S6.SS3.p2.2.m2.1.1.5.cmml" xref="S6.SS3.p2.2.m2.1.1.5">_</ci><ci id="S6.SS3.p2.2.m2.1.1.6.cmml" xref="S6.SS3.p2.2.m2.1.1.6">𝑜</ci><ci id="S6.SS3.p2.2.m2.1.1.7.cmml" xref="S6.SS3.p2.2.m2.1.1.7">𝑢</ci><ci id="S6.SS3.p2.2.m2.1.1.8.cmml" xref="S6.SS3.p2.2.m2.1.1.8">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m2.1c">CTN\_our</annotation></semantics></math> outperforms all other methods on 17 datasets. The second best methods are OSCNN and MultiRocket which outperform other methods on only seven of the 34 seasonal datasets. These results are presented in Table <a href="#S6.T1" title="Table 1 ‣ 6.1 Results appendices ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and more visually in Fig. <a href="#S6.F3" title="Figure 3 ‣ 6.1 Results appendices ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
These results indicate that our method is superior when it comes to seasonal datasets.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Positive transfer learning</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.2" class="ltx_p">To demonstrate that transfer learning using our method can improve the performance of a given CNN architecture, we compered the following two methods: <math id="S6.SS4.p1.1.m1.1" class="ltx_Math" alttext="CTN\_our" display="inline"><semantics id="S6.SS4.p1.1.m1.1a"><mrow id="S6.SS4.p1.1.m1.1.1" xref="S6.SS4.p1.1.m1.1.1.cmml"><mi id="S6.SS4.p1.1.m1.1.1.2" xref="S6.SS4.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.m1.1.1.1" xref="S6.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.1.m1.1.1.3" xref="S6.SS4.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.m1.1.1.1a" xref="S6.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.1.m1.1.1.4" xref="S6.SS4.p1.1.m1.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.m1.1.1.1b" xref="S6.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.SS4.p1.1.m1.1.1.5" xref="S6.SS4.p1.1.m1.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.m1.1.1.1c" xref="S6.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.1.m1.1.1.6" xref="S6.SS4.p1.1.m1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.m1.1.1.1d" xref="S6.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.1.m1.1.1.7" xref="S6.SS4.p1.1.m1.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.m1.1.1.1e" xref="S6.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.1.m1.1.1.8" xref="S6.SS4.p1.1.m1.1.1.8.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.1.m1.1b"><apply id="S6.SS4.p1.1.m1.1.1.cmml" xref="S6.SS4.p1.1.m1.1.1"><times id="S6.SS4.p1.1.m1.1.1.1.cmml" xref="S6.SS4.p1.1.m1.1.1.1"></times><ci id="S6.SS4.p1.1.m1.1.1.2.cmml" xref="S6.SS4.p1.1.m1.1.1.2">𝐶</ci><ci id="S6.SS4.p1.1.m1.1.1.3.cmml" xref="S6.SS4.p1.1.m1.1.1.3">𝑇</ci><ci id="S6.SS4.p1.1.m1.1.1.4.cmml" xref="S6.SS4.p1.1.m1.1.1.4">𝑁</ci><ci id="S6.SS4.p1.1.m1.1.1.5.cmml" xref="S6.SS4.p1.1.m1.1.1.5">_</ci><ci id="S6.SS4.p1.1.m1.1.1.6.cmml" xref="S6.SS4.p1.1.m1.1.1.6">𝑜</ci><ci id="S6.SS4.p1.1.m1.1.1.7.cmml" xref="S6.SS4.p1.1.m1.1.1.7">𝑢</ci><ci id="S6.SS4.p1.1.m1.1.1.8.cmml" xref="S6.SS4.p1.1.m1.1.1.8">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.1.m1.1c">CTN\_our</annotation></semantics></math> and <math id="S6.SS4.p1.2.m2.1" class="ltx_Math" alttext="CTN\_S" display="inline"><semantics id="S6.SS4.p1.2.m2.1a"><mrow id="S6.SS4.p1.2.m2.1.1" xref="S6.SS4.p1.2.m2.1.1.cmml"><mi id="S6.SS4.p1.2.m2.1.1.2" xref="S6.SS4.p1.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.m2.1.1.1" xref="S6.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.2.m2.1.1.3" xref="S6.SS4.p1.2.m2.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.m2.1.1.1a" xref="S6.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.2.m2.1.1.4" xref="S6.SS4.p1.2.m2.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.m2.1.1.1b" xref="S6.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.SS4.p1.2.m2.1.1.5" xref="S6.SS4.p1.2.m2.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.m2.1.1.1c" xref="S6.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS4.p1.2.m2.1.1.6" xref="S6.SS4.p1.2.m2.1.1.6.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.2.m2.1b"><apply id="S6.SS4.p1.2.m2.1.1.cmml" xref="S6.SS4.p1.2.m2.1.1"><times id="S6.SS4.p1.2.m2.1.1.1.cmml" xref="S6.SS4.p1.2.m2.1.1.1"></times><ci id="S6.SS4.p1.2.m2.1.1.2.cmml" xref="S6.SS4.p1.2.m2.1.1.2">𝐶</ci><ci id="S6.SS4.p1.2.m2.1.1.3.cmml" xref="S6.SS4.p1.2.m2.1.1.3">𝑇</ci><ci id="S6.SS4.p1.2.m2.1.1.4.cmml" xref="S6.SS4.p1.2.m2.1.1.4">𝑁</ci><ci id="S6.SS4.p1.2.m2.1.1.5.cmml" xref="S6.SS4.p1.2.m2.1.1.5">_</ci><ci id="S6.SS4.p1.2.m2.1.1.6.cmml" xref="S6.SS4.p1.2.m2.1.1.6">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.2.m2.1c">CTN\_S</annotation></semantics></math>. The comparison was made while considered the following three aspects: positive transfer learning after each epoch, training time, and final accuracy. The results can be seen in Fig. <a href="#S6.F4" title="Figure 4 ‣ 6.4 Positive transfer learning ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">A more detailed evaluation for each of the aspects mentioned above is provided below:</p>
<ol id="S6.I2" class="ltx_enumerate">
<li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I2.i1.p1" class="ltx_para">
<p id="S6.I2.i1.p1.2" class="ltx_p"><em id="S6.I2.i1.p1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">Positive transfer learning: </em>Except for the first seven epochs, <math id="S6.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="CTN\_our" display="inline"><semantics id="S6.I2.i1.p1.1.m1.1a"><mrow id="S6.I2.i1.p1.1.m1.1.1" xref="S6.I2.i1.p1.1.m1.1.1.cmml"><mi id="S6.I2.i1.p1.1.m1.1.1.2" xref="S6.I2.i1.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.1.m1.1.1.1" xref="S6.I2.i1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.1.m1.1.1.3" xref="S6.I2.i1.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.1.m1.1.1.1a" xref="S6.I2.i1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.1.m1.1.1.4" xref="S6.I2.i1.p1.1.m1.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.1.m1.1.1.1b" xref="S6.I2.i1.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.I2.i1.p1.1.m1.1.1.5" xref="S6.I2.i1.p1.1.m1.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.1.m1.1.1.1c" xref="S6.I2.i1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.1.m1.1.1.6" xref="S6.I2.i1.p1.1.m1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.1.m1.1.1.1d" xref="S6.I2.i1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.1.m1.1.1.7" xref="S6.I2.i1.p1.1.m1.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.1.m1.1.1.1e" xref="S6.I2.i1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.1.m1.1.1.8" xref="S6.I2.i1.p1.1.m1.1.1.8.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.I2.i1.p1.1.m1.1b"><apply id="S6.I2.i1.p1.1.m1.1.1.cmml" xref="S6.I2.i1.p1.1.m1.1.1"><times id="S6.I2.i1.p1.1.m1.1.1.1.cmml" xref="S6.I2.i1.p1.1.m1.1.1.1"></times><ci id="S6.I2.i1.p1.1.m1.1.1.2.cmml" xref="S6.I2.i1.p1.1.m1.1.1.2">𝐶</ci><ci id="S6.I2.i1.p1.1.m1.1.1.3.cmml" xref="S6.I2.i1.p1.1.m1.1.1.3">𝑇</ci><ci id="S6.I2.i1.p1.1.m1.1.1.4.cmml" xref="S6.I2.i1.p1.1.m1.1.1.4">𝑁</ci><ci id="S6.I2.i1.p1.1.m1.1.1.5.cmml" xref="S6.I2.i1.p1.1.m1.1.1.5">_</ci><ci id="S6.I2.i1.p1.1.m1.1.1.6.cmml" xref="S6.I2.i1.p1.1.m1.1.1.6">𝑜</ci><ci id="S6.I2.i1.p1.1.m1.1.1.7.cmml" xref="S6.I2.i1.p1.1.m1.1.1.7">𝑢</ci><ci id="S6.I2.i1.p1.1.m1.1.1.8.cmml" xref="S6.I2.i1.p1.1.m1.1.1.8">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i1.p1.1.m1.1c">CTN\_our</annotation></semantics></math> outperforms <math id="S6.I2.i1.p1.2.m2.1" class="ltx_Math" alttext="CTN\_S" display="inline"><semantics id="S6.I2.i1.p1.2.m2.1a"><mrow id="S6.I2.i1.p1.2.m2.1.1" xref="S6.I2.i1.p1.2.m2.1.1.cmml"><mi id="S6.I2.i1.p1.2.m2.1.1.2" xref="S6.I2.i1.p1.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.2.m2.1.1.1" xref="S6.I2.i1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.2.m2.1.1.3" xref="S6.I2.i1.p1.2.m2.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.2.m2.1.1.1a" xref="S6.I2.i1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.2.m2.1.1.4" xref="S6.I2.i1.p1.2.m2.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.2.m2.1.1.1b" xref="S6.I2.i1.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.I2.i1.p1.2.m2.1.1.5" xref="S6.I2.i1.p1.2.m2.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.I2.i1.p1.2.m2.1.1.1c" xref="S6.I2.i1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i1.p1.2.m2.1.1.6" xref="S6.I2.i1.p1.2.m2.1.1.6.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.I2.i1.p1.2.m2.1b"><apply id="S6.I2.i1.p1.2.m2.1.1.cmml" xref="S6.I2.i1.p1.2.m2.1.1"><times id="S6.I2.i1.p1.2.m2.1.1.1.cmml" xref="S6.I2.i1.p1.2.m2.1.1.1"></times><ci id="S6.I2.i1.p1.2.m2.1.1.2.cmml" xref="S6.I2.i1.p1.2.m2.1.1.2">𝐶</ci><ci id="S6.I2.i1.p1.2.m2.1.1.3.cmml" xref="S6.I2.i1.p1.2.m2.1.1.3">𝑇</ci><ci id="S6.I2.i1.p1.2.m2.1.1.4.cmml" xref="S6.I2.i1.p1.2.m2.1.1.4">𝑁</ci><ci id="S6.I2.i1.p1.2.m2.1.1.5.cmml" xref="S6.I2.i1.p1.2.m2.1.1.5">_</ci><ci id="S6.I2.i1.p1.2.m2.1.1.6.cmml" xref="S6.I2.i1.p1.2.m2.1.1.6">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i1.p1.2.m2.1c">CTN\_S</annotation></semantics></math>. Accordingly, we can conclude that our method’s performance improves when training continues beyond the first few initial training epochs.</p>
</div>
</li>
<li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I2.i2.p1" class="ltx_para">
<p id="S6.I2.i2.p1.2" class="ltx_p"><em id="S6.I2.i2.p1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">85% Less training time: </em>It took <math id="S6.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="CTN\_S" display="inline"><semantics id="S6.I2.i2.p1.1.m1.1a"><mrow id="S6.I2.i2.p1.1.m1.1.1" xref="S6.I2.i2.p1.1.m1.1.1.cmml"><mi id="S6.I2.i2.p1.1.m1.1.1.2" xref="S6.I2.i2.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.1.m1.1.1.1" xref="S6.I2.i2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.1.m1.1.1.3" xref="S6.I2.i2.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.1.m1.1.1.1a" xref="S6.I2.i2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.1.m1.1.1.4" xref="S6.I2.i2.p1.1.m1.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.1.m1.1.1.1b" xref="S6.I2.i2.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.I2.i2.p1.1.m1.1.1.5" xref="S6.I2.i2.p1.1.m1.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.1.m1.1.1.1c" xref="S6.I2.i2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.1.m1.1.1.6" xref="S6.I2.i2.p1.1.m1.1.1.6.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.I2.i2.p1.1.m1.1b"><apply id="S6.I2.i2.p1.1.m1.1.1.cmml" xref="S6.I2.i2.p1.1.m1.1.1"><times id="S6.I2.i2.p1.1.m1.1.1.1.cmml" xref="S6.I2.i2.p1.1.m1.1.1.1"></times><ci id="S6.I2.i2.p1.1.m1.1.1.2.cmml" xref="S6.I2.i2.p1.1.m1.1.1.2">𝐶</ci><ci id="S6.I2.i2.p1.1.m1.1.1.3.cmml" xref="S6.I2.i2.p1.1.m1.1.1.3">𝑇</ci><ci id="S6.I2.i2.p1.1.m1.1.1.4.cmml" xref="S6.I2.i2.p1.1.m1.1.1.4">𝑁</ci><ci id="S6.I2.i2.p1.1.m1.1.1.5.cmml" xref="S6.I2.i2.p1.1.m1.1.1.5">_</ci><ci id="S6.I2.i2.p1.1.m1.1.1.6.cmml" xref="S6.I2.i2.p1.1.m1.1.1.6">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i2.p1.1.m1.1c">CTN\_S</annotation></semantics></math> 1,515 epochs to achieve its highest level of accuracy on all 85 datasets (indicated by the grey ’X’ in Fig. <a href="#S6.F4" title="Figure 4 ‣ 6.4 Positive transfer learning ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), whereas <math id="S6.I2.i2.p1.2.m2.1" class="ltx_Math" alttext="CTN\_our" display="inline"><semantics id="S6.I2.i2.p1.2.m2.1a"><mrow id="S6.I2.i2.p1.2.m2.1.1" xref="S6.I2.i2.p1.2.m2.1.1.cmml"><mi id="S6.I2.i2.p1.2.m2.1.1.2" xref="S6.I2.i2.p1.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.2.m2.1.1.1" xref="S6.I2.i2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.2.m2.1.1.3" xref="S6.I2.i2.p1.2.m2.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.2.m2.1.1.1a" xref="S6.I2.i2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.2.m2.1.1.4" xref="S6.I2.i2.p1.2.m2.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.2.m2.1.1.1b" xref="S6.I2.i2.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.I2.i2.p1.2.m2.1.1.5" xref="S6.I2.i2.p1.2.m2.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.2.m2.1.1.1c" xref="S6.I2.i2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.2.m2.1.1.6" xref="S6.I2.i2.p1.2.m2.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.2.m2.1.1.1d" xref="S6.I2.i2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.2.m2.1.1.7" xref="S6.I2.i2.p1.2.m2.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.I2.i2.p1.2.m2.1.1.1e" xref="S6.I2.i2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i2.p1.2.m2.1.1.8" xref="S6.I2.i2.p1.2.m2.1.1.8.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.I2.i2.p1.2.m2.1b"><apply id="S6.I2.i2.p1.2.m2.1.1.cmml" xref="S6.I2.i2.p1.2.m2.1.1"><times id="S6.I2.i2.p1.2.m2.1.1.1.cmml" xref="S6.I2.i2.p1.2.m2.1.1.1"></times><ci id="S6.I2.i2.p1.2.m2.1.1.2.cmml" xref="S6.I2.i2.p1.2.m2.1.1.2">𝐶</ci><ci id="S6.I2.i2.p1.2.m2.1.1.3.cmml" xref="S6.I2.i2.p1.2.m2.1.1.3">𝑇</ci><ci id="S6.I2.i2.p1.2.m2.1.1.4.cmml" xref="S6.I2.i2.p1.2.m2.1.1.4">𝑁</ci><ci id="S6.I2.i2.p1.2.m2.1.1.5.cmml" xref="S6.I2.i2.p1.2.m2.1.1.5">_</ci><ci id="S6.I2.i2.p1.2.m2.1.1.6.cmml" xref="S6.I2.i2.p1.2.m2.1.1.6">𝑜</ci><ci id="S6.I2.i2.p1.2.m2.1.1.7.cmml" xref="S6.I2.i2.p1.2.m2.1.1.7">𝑢</ci><ci id="S6.I2.i2.p1.2.m2.1.1.8.cmml" xref="S6.I2.i2.p1.2.m2.1.1.8">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i2.p1.2.m2.1c">CTN\_our</annotation></semantics></math> attains the same level of accuracy after only 193 epochs. In other words, using our method can save as much as 85% of the training time and still achieve the same generalization.</p>
</div>
</li>
<li id="S6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I2.i3.p1" class="ltx_para">
<p id="S6.I2.i3.p1.2" class="ltx_p"><em id="S6.I2.i3.p1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">Accuracy: </em>When considering the average accuracy after 2,000 epochs across the 85 datasets from the UCR archive, <math id="S6.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="CTN\_our" display="inline"><semantics id="S6.I2.i3.p1.1.m1.1a"><mrow id="S6.I2.i3.p1.1.m1.1.1" xref="S6.I2.i3.p1.1.m1.1.1.cmml"><mi id="S6.I2.i3.p1.1.m1.1.1.2" xref="S6.I2.i3.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.1.m1.1.1.1" xref="S6.I2.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.1.m1.1.1.3" xref="S6.I2.i3.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.1.m1.1.1.1a" xref="S6.I2.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.1.m1.1.1.4" xref="S6.I2.i3.p1.1.m1.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.1.m1.1.1.1b" xref="S6.I2.i3.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.I2.i3.p1.1.m1.1.1.5" xref="S6.I2.i3.p1.1.m1.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.1.m1.1.1.1c" xref="S6.I2.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.1.m1.1.1.6" xref="S6.I2.i3.p1.1.m1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.1.m1.1.1.1d" xref="S6.I2.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.1.m1.1.1.7" xref="S6.I2.i3.p1.1.m1.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.1.m1.1.1.1e" xref="S6.I2.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.1.m1.1.1.8" xref="S6.I2.i3.p1.1.m1.1.1.8.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.I2.i3.p1.1.m1.1b"><apply id="S6.I2.i3.p1.1.m1.1.1.cmml" xref="S6.I2.i3.p1.1.m1.1.1"><times id="S6.I2.i3.p1.1.m1.1.1.1.cmml" xref="S6.I2.i3.p1.1.m1.1.1.1"></times><ci id="S6.I2.i3.p1.1.m1.1.1.2.cmml" xref="S6.I2.i3.p1.1.m1.1.1.2">𝐶</ci><ci id="S6.I2.i3.p1.1.m1.1.1.3.cmml" xref="S6.I2.i3.p1.1.m1.1.1.3">𝑇</ci><ci id="S6.I2.i3.p1.1.m1.1.1.4.cmml" xref="S6.I2.i3.p1.1.m1.1.1.4">𝑁</ci><ci id="S6.I2.i3.p1.1.m1.1.1.5.cmml" xref="S6.I2.i3.p1.1.m1.1.1.5">_</ci><ci id="S6.I2.i3.p1.1.m1.1.1.6.cmml" xref="S6.I2.i3.p1.1.m1.1.1.6">𝑜</ci><ci id="S6.I2.i3.p1.1.m1.1.1.7.cmml" xref="S6.I2.i3.p1.1.m1.1.1.7">𝑢</ci><ci id="S6.I2.i3.p1.1.m1.1.1.8.cmml" xref="S6.I2.i3.p1.1.m1.1.1.8">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i3.p1.1.m1.1c">CTN\_our</annotation></semantics></math> outperforms <math id="S6.I2.i3.p1.2.m2.1" class="ltx_Math" alttext="CTN\_S" display="inline"><semantics id="S6.I2.i3.p1.2.m2.1a"><mrow id="S6.I2.i3.p1.2.m2.1.1" xref="S6.I2.i3.p1.2.m2.1.1.cmml"><mi id="S6.I2.i3.p1.2.m2.1.1.2" xref="S6.I2.i3.p1.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.2.m2.1.1.1" xref="S6.I2.i3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.2.m2.1.1.3" xref="S6.I2.i3.p1.2.m2.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.2.m2.1.1.1a" xref="S6.I2.i3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.2.m2.1.1.4" xref="S6.I2.i3.p1.2.m2.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.2.m2.1.1.1b" xref="S6.I2.i3.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.I2.i3.p1.2.m2.1.1.5" xref="S6.I2.i3.p1.2.m2.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.I2.i3.p1.2.m2.1.1.1c" xref="S6.I2.i3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.I2.i3.p1.2.m2.1.1.6" xref="S6.I2.i3.p1.2.m2.1.1.6.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.I2.i3.p1.2.m2.1b"><apply id="S6.I2.i3.p1.2.m2.1.1.cmml" xref="S6.I2.i3.p1.2.m2.1.1"><times id="S6.I2.i3.p1.2.m2.1.1.1.cmml" xref="S6.I2.i3.p1.2.m2.1.1.1"></times><ci id="S6.I2.i3.p1.2.m2.1.1.2.cmml" xref="S6.I2.i3.p1.2.m2.1.1.2">𝐶</ci><ci id="S6.I2.i3.p1.2.m2.1.1.3.cmml" xref="S6.I2.i3.p1.2.m2.1.1.3">𝑇</ci><ci id="S6.I2.i3.p1.2.m2.1.1.4.cmml" xref="S6.I2.i3.p1.2.m2.1.1.4">𝑁</ci><ci id="S6.I2.i3.p1.2.m2.1.1.5.cmml" xref="S6.I2.i3.p1.2.m2.1.1.5">_</ci><ci id="S6.I2.i3.p1.2.m2.1.1.6.cmml" xref="S6.I2.i3.p1.2.m2.1.1.6">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i3.p1.2.m2.1c">CTN\_S</annotation></semantics></math>, as can be see in Fig. <a href="#S6.F4" title="Figure 4 ‣ 6.4 Positive transfer learning ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</li>
</ol>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2207.07897/assets/x2.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance of the <math id="S6.F4.2.m1.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S6.F4.2.m1.1b"><mrow id="S6.F4.2.m1.1.1" xref="S6.F4.2.m1.1.1.cmml"><mi id="S6.F4.2.m1.1.1.2" xref="S6.F4.2.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.F4.2.m1.1.1.1" xref="S6.F4.2.m1.1.1.1.cmml">​</mo><mi id="S6.F4.2.m1.1.1.3" xref="S6.F4.2.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.F4.2.m1.1.1.1b" xref="S6.F4.2.m1.1.1.1.cmml">​</mo><mi id="S6.F4.2.m1.1.1.4" xref="S6.F4.2.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F4.2.m1.1c"><apply id="S6.F4.2.m1.1.1.cmml" xref="S6.F4.2.m1.1.1"><times id="S6.F4.2.m1.1.1.1.cmml" xref="S6.F4.2.m1.1.1.1"></times><ci id="S6.F4.2.m1.1.1.2.cmml" xref="S6.F4.2.m1.1.1.2">𝐶</ci><ci id="S6.F4.2.m1.1.1.3.cmml" xref="S6.F4.2.m1.1.1.3">𝑇</ci><ci id="S6.F4.2.m1.1.1.4.cmml" xref="S6.F4.2.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.2.m1.1d">CTN</annotation></semantics></math> architecture on all of the test sets from the UCR archive after each epoch, with and without our method.</figcaption>
</figure>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Mean average rank and win/lose rate</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">In terms of the mean average rank, <math id="S6.SS5.p1.1.m1.1" class="ltx_Math" alttext="CTN\_our" display="inline"><semantics id="S6.SS5.p1.1.m1.1a"><mrow id="S6.SS5.p1.1.m1.1.1" xref="S6.SS5.p1.1.m1.1.1.cmml"><mi id="S6.SS5.p1.1.m1.1.1.2" xref="S6.SS5.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS5.p1.1.m1.1.1.1" xref="S6.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS5.p1.1.m1.1.1.3" xref="S6.SS5.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.SS5.p1.1.m1.1.1.1a" xref="S6.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS5.p1.1.m1.1.1.4" xref="S6.SS5.p1.1.m1.1.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.SS5.p1.1.m1.1.1.1b" xref="S6.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.SS5.p1.1.m1.1.1.5" xref="S6.SS5.p1.1.m1.1.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.SS5.p1.1.m1.1.1.1c" xref="S6.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS5.p1.1.m1.1.1.6" xref="S6.SS5.p1.1.m1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.SS5.p1.1.m1.1.1.1d" xref="S6.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS5.p1.1.m1.1.1.7" xref="S6.SS5.p1.1.m1.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.SS5.p1.1.m1.1.1.1e" xref="S6.SS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS5.p1.1.m1.1.1.8" xref="S6.SS5.p1.1.m1.1.1.8.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.1.m1.1b"><apply id="S6.SS5.p1.1.m1.1.1.cmml" xref="S6.SS5.p1.1.m1.1.1"><times id="S6.SS5.p1.1.m1.1.1.1.cmml" xref="S6.SS5.p1.1.m1.1.1.1"></times><ci id="S6.SS5.p1.1.m1.1.1.2.cmml" xref="S6.SS5.p1.1.m1.1.1.2">𝐶</ci><ci id="S6.SS5.p1.1.m1.1.1.3.cmml" xref="S6.SS5.p1.1.m1.1.1.3">𝑇</ci><ci id="S6.SS5.p1.1.m1.1.1.4.cmml" xref="S6.SS5.p1.1.m1.1.1.4">𝑁</ci><ci id="S6.SS5.p1.1.m1.1.1.5.cmml" xref="S6.SS5.p1.1.m1.1.1.5">_</ci><ci id="S6.SS5.p1.1.m1.1.1.6.cmml" xref="S6.SS5.p1.1.m1.1.1.6">𝑜</ci><ci id="S6.SS5.p1.1.m1.1.1.7.cmml" xref="S6.SS5.p1.1.m1.1.1.7">𝑢</ci><ci id="S6.SS5.p1.1.m1.1.1.8.cmml" xref="S6.SS5.p1.1.m1.1.1.8">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.1.m1.1c">CTN\_our</annotation></semantics></math> achieved a score of <span id="S6.SS5.p1.1.1" class="ltx_text ltx_font_bold">3.494</span>, which is only second to MultiRocket
with a score of 3.271; the Nemenyi statistical test indicated that there was no significant difference in the results of the two methods</p>
</div>
<div id="S6.SS5.p2" class="ltx_para">
<p id="S6.SS5.p2.1" class="ltx_p">(see Table <a href="#S6.T1" title="Table 1 ‣ 6.1 Results appendices ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> which presents the mean average rank of all of the examined methods).
<br class="ltx_break">In terms of the win/lose rate, our method obtains a <span id="S6.SS5.p2.1.1" class="ltx_text ltx_font_bold">26/9</span> win/lose rate. While MultiRocket achieves a 28/12 win/lose rate, winning two more times but losing three more times. InceptionTime has the fewest losses, with a win/lose rate of 9/7.
<br class="ltx_break">Our method comes in at least second place on every empirical measure (mean average rank, seasonal wins, win, lose), which no other method is capable of (the results of all of the examined methods can be seen in Table <a href="#S6.T1" title="Table 1 ‣ 6.1 Results appendices ‣ 6 Results" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and future work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we introduced:</p>
<ol id="S7.I1" class="ltx_enumerate">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">A novel architecture-agnostic TL for TSC method. In contrast to previous TL for TSC methods using existing UCR’s datasets and tasks as the source dataset and task, in this paper, we introduce a new algorithm that generates UTS data and creates 55 corresponding regression tasks to be used as a source dataset and task.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">Open-source code for generating custom synthetic data, producing regression tasks, pretraining, and fine-tuning on a new target dataset and task. 
<br class="ltx_break">Our 15,000,000 sample synthetic dataset, the 55 regression tasks, and the pretrained model with the <math id="S7.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="CTN" display="inline"><semantics id="S7.I1.i2.p1.1.m1.1a"><mrow id="S7.I1.i2.p1.1.m1.1.1" xref="S7.I1.i2.p1.1.m1.1.1.cmml"><mi id="S7.I1.i2.p1.1.m1.1.1.2" xref="S7.I1.i2.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p1.1.m1.1.1.1" xref="S7.I1.i2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S7.I1.i2.p1.1.m1.1.1.3" xref="S7.I1.i2.p1.1.m1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p1.1.m1.1.1.1a" xref="S7.I1.i2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S7.I1.i2.p1.1.m1.1.1.4" xref="S7.I1.i2.p1.1.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p1.1.m1.1b"><apply id="S7.I1.i2.p1.1.m1.1.1.cmml" xref="S7.I1.i2.p1.1.m1.1.1"><times id="S7.I1.i2.p1.1.m1.1.1.1.cmml" xref="S7.I1.i2.p1.1.m1.1.1.1"></times><ci id="S7.I1.i2.p1.1.m1.1.1.2.cmml" xref="S7.I1.i2.p1.1.m1.1.1.2">𝐶</ci><ci id="S7.I1.i2.p1.1.m1.1.1.3.cmml" xref="S7.I1.i2.p1.1.m1.1.1.3">𝑇</ci><ci id="S7.I1.i2.p1.1.m1.1.1.4.cmml" xref="S7.I1.i2.p1.1.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p1.1.m1.1c">CTN</annotation></semantics></math> architecture are published for further use by the ML community.</p>
</div>
</li>
</ol>
<p id="S7.p1.2" class="ltx_p">Our study shows that the use of our method can not only improve the performance of a given CNN architecture but also decreases training time by 85%. 
<br class="ltx_break">When it comes to seasonal datasets, our method outperforms all other existing TSC methods.
<br class="ltx_break">For future work, we would like to do the following:</p>
<ol id="S7.I2" class="ltx_enumerate">
<li id="S7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I2.i1.p1" class="ltx_para">
<p id="S7.I2.i1.p1.1" class="ltx_p">Explore other regression tasks beside the 55 tasks we used. The new tasks could include Fourier transform, autocorrelation, etc.</p>
</div>
</li>
<li id="S7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I2.i2.p1" class="ltx_para">
<p id="S7.I2.i2.p1.1" class="ltx_p">Examine a new architecture for transfer learning based on both CNN and LSTM layers which will be trained on our source dataset and task, as suggest by Wang et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</li>
<li id="S7.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S7.I2.i3.p1" class="ltx_para">
<p id="S7.I2.i3.p1.1" class="ltx_p">Expand our source dataset to include more patterns than the 12 patterns we generated.</p>
</div>
</li>
<li id="S7.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S7.I2.i4.p1" class="ltx_para">
<p id="S7.I2.i4.p1.1" class="ltx_p">Extend our method so that it will be suitable for MTS (multivariate time series) datasets.</p>
</div>
</li>
<li id="S7.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S7.I2.i5.p1" class="ltx_para">
<p id="S7.I2.i5.p1.1" class="ltx_p">Explore the use of a generative adversarial network (GAN) to automatically generate synthetic data that is more similar to a specific given dataset.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu,
Shaghayegh Gharghabi, Chotirat Annh Ratanamahatana, and Eamonn Keogh.

</span>
<span class="ltx_bibblock">The ucr time series archive.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE/CAA Journal of Automatica Sinica</span>, 6(6):1293–1305, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein.

</span>
<span class="ltx_bibblock">A tutorial on the cross-entropy method.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Annals of operations research</span>, 134(1):19–67, 2005.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and
Pierre-Alain Muller.

</span>
<span class="ltx_bibblock">Transfer learning for time series classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Big Data (Big Data)</span>,
pages 1367–1376. IEEE, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and
Pierre-Alain Muller.

</span>
<span class="ltx_bibblock">Deep learning for time series classification: a review.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Data Mining and Knowledge Discovery</span>, 33(4):917–963, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,
Daniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar,
Pierre-Alain Muller, and François Petitjean.

</span>
<span class="ltx_bibblock">Inceptiontime: Finding alexnet for time series classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Data Mining and Knowledge Discovery</span>, 34(6):1936–1962, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Leonardo N Ferreira and Liang Zhao.

</span>
<span class="ltx_bibblock">Time series clustering via community detection in networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Information Sciences</span>, 326:227–242, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Douglas M Hawkins.

</span>
<span class="ltx_bibblock">The problem of overfitting.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Journal of chemical information and computer sciences</span>,
44(1):1–12, 2004.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,
Daniel F. Schmidt, Jonathan Weber, Geoffrey I. Webb, Lhassane Idoumghar,
Pierre-Alain Muller, and François Petitjean.

</span>
<span class="ltx_bibblock">Inceptiontime: Finding alexnet for time series classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford.

</span>
<span class="ltx_bibblock">Multivariate lstm-fcns for time series classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 116:237–245, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kathan Kashiparekh, Jyoti Narwariya, Pankaj Malhotra, Lovekesh Vig, and Gautam
Shroff.

</span>
<span class="ltx_bibblock">Convtimenet: A pre-trained deep convolutional neural network for time
series classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8. IEEE, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Eamonn Keogh, Selina Chu, David Hart, and Michael Pazzani.

</span>
<span class="ltx_bibblock">An online algorithm for segmenting time series.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings 2001 IEEE international conference on data
mining</span>, pages 289–296. IEEE, 2001.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>, 2014.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
1097–1105, 2012.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Afonso Menegola, Michel Fornaciali, Ramon Pires, Flávia Vasques
Bittencourt, Sandra Avila, and Eduardo Valle.

</span>
<span class="ltx_bibblock">Knowledge transfer for melanoma screening with deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2017 IEEE 14th International Symposium on Biomedical Imaging
(ISBI 2017)</span>, pages 297–300. IEEE, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Meinard Müller.

</span>
<span class="ltx_bibblock">Dynamic time warping.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Information retrieval for music and motion</span>, pages 69–84, 2007.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Spiros Papadimitriou and Philip Yu.

</span>
<span class="ltx_bibblock">Optimal multi-scale patterns in time series streams.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2006 ACM SIGMOD international conference
on Management of data</span>, pages 647–658, 2006.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Alaa Sagheer and Mostafa Kotb.

</span>
<span class="ltx_bibblock">Time series forecasting of petroleum production using deep lstm
recurrent networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 323:203–213, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.

</span>
<span class="ltx_bibblock">Inception-v4, inception-resnet and the impact of residual connections
on learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Thirty-first AAAI conference on artificial intelligence</span>,
2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Chang Wei Tan, Angus Dempster, Christoph Bergmeir, and Geoffrey I Webb.

</span>
<span class="ltx_bibblock">Multirocket: Effective summary statistics for convolutional outputs
in time series classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.00457</span>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Wensi Tang, Guodong Long, Lu Liu, Tianyi Zhou, Jing Jiang, and Michael
Blumenstein.

</span>
<span class="ltx_bibblock">Rethinking 1d-cnn for time series classification: A stronger
baseline.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.10061</span>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Jun Wang, Wenfeng Wang, Shaoming Wei, Yajun Zeng, and Feixiang Luo.

</span>
<span class="ltx_bibblock">Time series sequences classification with inception and lstm module.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Integrated Circuits,
Technologies and Applications (ICTA)</span>, pages 51–55. IEEE, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lin Wang, Zhigang Wang, and Shan Liu.

</span>
<span class="ltx_bibblock">An effective multivariate time series classification approach using
echo state network and adaptive differential evolution algorithm.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Expert Systems with Applications</span>, 43:237–249, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Qiang Yang and Xindong Wu.

</span>
<span class="ltx_bibblock">10 challenging problems in data mining research.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">International Journal of Information Technology &amp; Decision
Making</span>, 5(04):597–604, 2006.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Scaling vision transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.04560</span>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Wei Zhao.

</span>
<span class="ltx_bibblock">Research on the deep learning of the small sample data based on
transfer learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">AIP Conference Proceedings</span>, volume 1864, page 020018. AIP
Publishing LLC, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Yunyue Zhu and Dennis Shasha.

</span>
<span class="ltx_bibblock">Statstream: Statistical monitoring of thousands of data streams in
real time.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">VLDB’02: Proceedings of the 28th International Conference on
Very Large Databases</span>, pages 358–369. Elsevier, 2002.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui
Xiong, and Qing He.

</span>
<span class="ltx_bibblock">A comprehensive survey on transfer learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.02685</span>, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.07896" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.07897" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.07897">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.07897" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.07898" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 13:41:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
