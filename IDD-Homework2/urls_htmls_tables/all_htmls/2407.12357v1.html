<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Evaluating graph-based explanations for AI-based recommender systems</title>
<!--Generated on Wed Jul 17 07:26:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.12357v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S1" title="In Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S2" title="In Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S2.SS1" title="In 2. Related work ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Explainable AI for recommender systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S2.SS2" title="In 2. Related work ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Explainable AI in Computer Science</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S2.SS3" title="In 2. Related work ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Explainable AI in Human Computer Interaction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3" title="In Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.SS1" title="In 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Study design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.SS2" title="In 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.SS2.SSS1" title="In 3.2. Results ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>A high level of perceived understanding, based on three main criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.SS2.SSS2" title="In 3.2. Results ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Strong link between context knowledge and recommendation understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.SS2.SSS3" title="In 3.2. Results ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Graphs as objects modeling both similarities and popularity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.SS2.SSS4" title="In 3.2. Results ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>Graph-based explanations are considered interpretable but should contain item information</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4" title="In Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Study 2: Influence of explanation design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS1" title="In 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Study design</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS1.SSS1" title="In 4.1. Study design ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Recommender system</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS1.SSS2" title="In 4.1. Study design ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Explanation design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS1.SSS3" title="In 4.1. Study design ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Experimental conditions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS1.SSS4" title="In 4.1. Study design ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Evaluation constructs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS2" title="In 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS2.SSS1" title="In 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Text rather than graph design for higher objective understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS2.SSS2" title="In 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Graph and text designs increase usability.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS2.SSS3" title="In 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Higher expertise increases curiosity for graph and text designs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS2.SSS4" title="In 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>No effect of explanation design or expertise level on subjective understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.SS2.SSS5" title="In 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.5 </span>Graph-based design is preferred, regardless of the expertise level</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S5" title="In Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S5.SS1" title="In 5. Discussion ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Graph vs. textual explanations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S5.SS2" title="In 5. Discussion ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Impact of expertise level</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S5.SS3" title="In 5. Discussion ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S6" title="In Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S7" title="In Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Ethical concerns</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Evaluating graph-based explanations for AI-based recommender systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simon Delarue
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:simon.delarue@telecom-paris.fr">simon.delarue@telecom-paris.fr</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">LTCI, Télécom Paris, 
<br class="ltx_break"/>Institut Polytechnique de Paris</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3"></span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4"></span><span class="ltx_text ltx_affiliation_country" id="id5.5.id5">France</span><span class="ltx_text ltx_affiliation_postcode" id="id6.6.id6"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Astrid Bertrand
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">LTCI, Télécom Paris, 
<br class="ltx_break"/>Institut Polytechnique de Paris</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3"></span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4"></span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">France</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6"></span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiphaine Viard
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">i3, Télécom Paris, 
<br class="ltx_break"/>Institut Polytechnique de Paris</span><span class="ltx_text ltx_affiliation_streetaddress" id="id14.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id15.3.id3"></span><span class="ltx_text ltx_affiliation_state" id="id16.4.id4"></span><span class="ltx_text ltx_affiliation_country" id="id17.5.id5">France</span><span class="ltx_text ltx_affiliation_postcode" id="id18.6.id6"></span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">Recent years have witnessed a rapid growth of recommender systems, providing suggestions in numerous applications with potentially high social impact, such as health or justice. Meanwhile, in Europe, the upcoming AI Act mentions <em class="ltx_emph ltx_font_italic" id="id19.id1.1">transparency</em> as a requirement for critical AI systems in order to “mitigate the risks to fundamental rights”. Post-hoc explanations seamlessly align with this goal and extensive literature on the subject produced several forms of such objects, graphs being one of them. Early studies in visualization demonstrated the graphs’ ability to improve user understanding, positioning them as potentially ideal explanations. However, it remains unclear how graph-based explanations compare to other explanation designs. In this work, we aim to determine the effectiveness of graph-based explanations in improving users’ perception of AI-based recommendations using a mixed-methods approach. We first conduct a qualitative study to collect users’ requirements for graph explanations. We then run a larger quantitative study in which we evaluate the influence of various explanation designs, including enhanced graph-based ones, on aspects such as understanding, usability and curiosity toward the AI system. We find that users perceive graph-based explanations as more usable than designs involving feature importance. However, we also reveal that textual explanations lead to higher objective understanding than graph-based designs. Most importantly, we highlight the strong contrast between participants’ expressed preferences for graph design and their actual ratings using it, which are lower compared to textual design. These findings imply that meeting stakeholders’ expressed preferences might not alone guarantee “good” explanations. Therefore, crafting hybrid designs successfully balancing social expectations with downstream performance emerges as a significant challenge.</p>
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommender systems have emerged as fundamental tools for delivering personalized services to users. These frameworks find application across diverse domains, ranging from commonplace ones like online library and e-shopping, to more contentious ones such as finance, law, education or health. In cases where recommendations carry significant implications for users, it becomes essential to provide additional explanatory mechanisms. In Europe, ongoing legal discussions highlight the likelihood of transparency becoming a requirement for high-risk Artificial Intelligence (AI) systems <cite class="ltx_cite ltx_citemacro_citep">(European Commission, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib15" title="">2021</a>)</cite>. From the academic perspective, several works underscore a positive correlation between users’ understanding of the model and their trust in such systems <cite class="ltx_cite ltx_citemacro_citep">(Sinha and Swearingen, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib48" title="">2002</a>; Tintarev, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib50" title="">2007</a>; Wilkenfeld, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib55" title="">2014</a>; Zhang and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib59" title="">2020</a>; Langer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib27" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Explanations aim to address the <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">“why”</em> question in relation to a recommender system or a specific prediction <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib59" title="">2020</a>)</cite>. Emerging from two primary research fields, namely Computer Science (CS) and Human Computer Interaction (HCI), various forms of explanations, such as text <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib53" title="">2022</a>)</cite>, charts <cite class="ltx_cite ltx_citemacro_citep">(Herlocker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib19" title="">2000</a>; Kouki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib25" title="">2017</a>)</cite>, matrices <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib9" title="">2018</a>)</cite>, hybrid designs <cite class="ltx_cite ltx_citemacro_citep">(Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib29" title="">2017</a>; Di Giacomo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib10" title="">2021</a>; Bertrand et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib5" title="">2023</a>)</cite> or graphs <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib54" title="">2019</a>; Pope et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib42" title="">2019</a>; Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib56" title="">2019</a>)</cite>, have been proposed to enhance user satisfaction toward AI systems. Nevertheless, determining whether an explanation qualifies as a “good” one is not straightforward. On the one hand, Miller <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib32" title="">2019</a>)</cite> argues that “good” explanations should be (i) <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">contrastive</em>, highlighting contrast with alternatives, (ii) <em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">selected</em>, recognizing that people seldom expect the complete cause of an event and (iii) preferring <em class="ltx_emph ltx_font_italic" id="S1.p2.1.4">causal</em> over probabilistic reasoning. Meanwhile, authors in <cite class="ltx_cite ltx_citemacro_citep">(Langer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib27" title="">2021</a>)</cite> suggest that it is essential to ensure that explanations align with <em class="ltx_emph ltx_font_italic" id="S1.p2.1.5">stakeholders’ desiderata</em>, sometimes referred to as <em class="ltx_emph ltx_font_italic" id="S1.p2.1.6">social expectations</em> <cite class="ltx_cite ltx_citemacro_citep">(Hilton, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib21" title="">1990</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We hypothesize that graphs, as objects that can model selected relational data and exhibit causality, appears to align intuitively with Miller’s criteria and may have the ability to fulfill various users’ requirements. More specifically, the recommendation task can be naturally framed as a <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">link prediction</em> task in a bipartite graph (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.F1" title="Figure 1 ‣ 3.1. Study design ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">1</span></a>) that includes both users and items. The relevance of graph structures in problem-solving is not novel, as its roots trace back hundreds of years to Euler’s solution to the Königsberg bridges problem <cite class="ltx_cite ltx_citemacro_citep">(Euler, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib14" title="">1741</a>)</cite>. More recently, within the HCI field, several works have explored the impact of graph visualization through extended user studies <cite class="ltx_cite ltx_citemacro_citep">(Herman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib20" title="">2000</a>; Burch et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib7" title="">2020</a>)</cite>. However, only a few works <cite class="ltx_cite ltx_citemacro_citep">(Kouki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib25" title="">2017</a>; Ghazimatin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib16" title="">2020</a>)</cite> have explored the application of such designs in the explanatory context of recommender systems. In the CS community, when graphs are considered as explanations, they are often evaluated from an algorithmic perspective, also referred to as <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">functionally-grounded evaluation</em> <cite class="ltx_cite ltx_citemacro_citep">(Doshi-Velez and Kim, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib11" title="">2018</a>)</cite>. This setup involves the use of machine learning-oriented proxy metrics that operate without human oversight, contradicting the criteria outlined in <cite class="ltx_cite ltx_citemacro_citep">(Hilton, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib21" title="">1990</a>; Langer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib27" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we seek to bridge the gap between the HCI field, where graph designs are seldom examined in the context of explainability, and the CS field, where graphs are ubiquitous in explainable recommender systems – either as components of models or explanation designs – but are primarily evaluated from an algorithmic standpoint. To achieve this, we conduct a qualitative user study to characterize users’ needs in terms of graph-based designs. Specifically, we gather requirements from users with varying levels of expertise in AI systems. Leveraging this knowledge, we develop an enhanced graph-based design and compare it to two commonly used forms of explanations – textual and SHAP <cite class="ltx_cite ltx_citemacro_citep">(Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib29" title="">2017</a>)</cite>-based explanations – through a quantitative user study. Through these studies, our goal is to answer the following research questions:</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">RQ1</span>: <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.2">What are the different stakeholders’ desiderata regarding graph-based explanations for recommender systems?</em></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">RQ2</span>: <em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.2">How does enhanced graph-based explanation design influence users of AI recommendations?</em></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">RQ3</span>: <em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.2">How does enhanced graph-based explanation design compare to other explanation designs?</em></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">RQ4</span>: <em class="ltx_emph ltx_font_italic" id="S1.I1.i4.p1.1.2">How does the user’s expertise level toward AI systems affect their explanation design preferences?</em></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our contributions encompass the following key aspects. Firstly, we investigate stakeholders’ desiderata for graph-based explanations through a qualitative user study involving participants with diverse expertise levels, highlighting the preference for item-based design over user-based design. Secondly, drawing upon these insights, we build an enhanced graph-based explanation using item-oriented graph projection. Lastly, we conduct a larger scale quantitative study to discuss the impact of visual design on understanding, curiosity and usability. Consequently, we emphasize the contrast between users’ expressed preference, both qualitatively and quantitatively, for graph-based explanations and their higher ratings when using dialogic explanations.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Explainable AI for recommender systems</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Explainable recommendations offer user personalized item suggestions while explicitly clarifying <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">why</em> a particular item is being proposed. The design of explanations for recommendations has been studied for a long time <cite class="ltx_cite ltx_citemacro_citep">(Herlocker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib19" title="">2000</a>; Sinha and Swearingen, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib48" title="">2002</a>)</cite>. Numerous studies emphasized the correlation between understanding the system and placing trust in it <cite class="ltx_cite ltx_citemacro_citep">(Sinha and Swearingen, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib48" title="">2002</a>; Tintarev, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib50" title="">2007</a>; Wilkenfeld, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib55" title="">2014</a>; Zhang and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib59" title="">2020</a>; Langer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib27" title="">2021</a>)</cite>, amplifying the significance of such explanations. While recommender systems can be crafted to naturally include interpretable elements (model-intrinsic) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib60" title="">2014</a>)</cite>, our focus is on approaches involving <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">post-hoc</em> explanations (model-agnostic), also known as post-hoc interpretability <cite class="ltx_cite ltx_citemacro_citep">(Lipton, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib28" title="">2018</a>; Miller, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib32" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Designing such explanation is a task situated at the intersection of two research fields, namely Computer Science (CS) and Human Computer Interaction (HCI), each with its distinct focus and requirements.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Explainable AI in Computer Science</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">From the CS perspective, building explainable recommendations involves a variety of techniques and explanation types. This includes rule mining <cite class="ltx_cite ltx_citemacro_citep">(Peake and Wang, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib41" title="">2018</a>)</cite>, approximation using simple models as with LIME <cite class="ltx_cite ltx_citemacro_citep">(Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib44" title="">2016</a>)</cite>, feature importance methods such as SHAP <cite class="ltx_cite ltx_citemacro_citep">(Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib29" title="">2017</a>)</cite>, attention maps <cite class="ltx_cite ltx_citemacro_citep">(Selvaraju et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib45" title="">2017</a>)</cite> or graphs <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib54" title="">2019</a>; Pope et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib42" title="">2019</a>; Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib56" title="">2019</a>)</cite>. Concerning graphs, authors often highlight their natural ability to reveal user-item connectivity <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib54" title="">2019</a>)</cite> or extract small subgraphs containing elements most influential for predictions <cite class="ltx_cite ltx_citemacro_citep">(Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib56" title="">2019</a>)</cite>. In this context, justifications for using graphs as explanations for recommendations seem to align seamlessly with Miller’s description of good explanations <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib32" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">However, the evaluation of such explanations in the CS field heavily relies on <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">functionally-grounded</em> approaches <cite class="ltx_cite ltx_citemacro_citep">(Doshi-Velez and Kim, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib11" title="">2018</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">i.e.</em> algorithmic-oriented techniques that measure proxy metrics such as <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.3">fidelity</em> or <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.4">correctedness</em> <cite class="ltx_cite ltx_citemacro_citep">(Nauta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib35" title="">2023</a>)</cite>, but lack human control. While such an evaluation approach offers advantages by saving time and avoiding ethical concerns potentially associated with human participation, authors in <cite class="ltx_cite ltx_citemacro_citep">(Miller et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib33" title="">2017</a>; Doshi-Velez and Kim, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib11" title="">2018</a>)</cite> emphasize the limitations of these approaches in terms of “real-world impact” and advocate for their use solely after conducting user studies. To tackle this issue, recent work integrated both graph-based explanations and an evaluation framework through a user study <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib58" title="">2023</a>)</cite>. However, the authors restricted their comparison to graph designs among themselves only, preventing a comprehensive assessment of the validity of graph explanations against alternative designs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In this work, we go beyond the conventional algorithmic-oriented evaluation typically employed in the CS field and introduce both qualitative and quantitative user studies to evaluate the validity of graph-based explanations compared to other designs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Explainable AI in Human Computer Interaction</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">It is acknowledged that visualization can provide cognitive support through various mechanisms, <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">e.g.</em> helping in pattern discovery, summarizing large volumes of data, or reducing search time <cite class="ltx_cite ltx_citemacro_citep">(Tory and Moller, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib51" title="">2004</a>)</cite>. The HCI field extensively studies the influence of design choices, with several works emphasizing their significance in impacting users’ understanding and ability to contextualize problems. In particular, numerous studies have explored the impact of graph visualizations and showed the influence of the overall setting on their performance. For instance, several works demonstrated that node-link representations were indeed advantageous over matrix representations, when faced with small graphs and when path-oriented tasks were involved <cite class="ltx_cite ltx_citemacro_citep">(Ghoniem et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib17" title="">2004</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib18" title="">2005</a>; Okoe and Jianu, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib38" title="">2015</a>; Keller et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib24" title="">2006</a>; Okoe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib39" title="">2018</a>)</cite>. For other tasks, such as weighted graph comparison <cite class="ltx_cite ltx_citemacro_citep">(Alper et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib4" title="">2013</a>)</cite> or suspicious node detection <cite class="ltx_cite ltx_citemacro_citep">(McBride and Caldara, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib30" title="">2013</a>)</cite>, matrix representations have proven to be a superior choice over node-link diagrams. More recently, authors in <cite class="ltx_cite ltx_citemacro_citep">(Di Giacomo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib10" title="">2021</a>)</cite> analyzed hybrid visualizations that combine node-link and matrix representations. Their goal was to provide a comprehensive tool for analyzing real-world networks that are globally sparse but locally dense. They showed that in such configurations, their mixed model overcomes the limitations of using the node-link diagram alone. While all these approaches systematically involve user study evaluations, they are not specific to the explanatory context of recommender systems.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Few works from the HCI field address both graphs and their evaluation as explanations. In an early study, authors in <cite class="ltx_cite ltx_citemacro_citep">(O’Donovan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib37" title="">2008</a>)</cite> carried out a user study to assess the impact of various interactive graph-based representations of a recommender system. They showed that 78% of users <em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.1">“felt that the system provided a good explanation of collaborative filtering”</em>. However, the graph representation itself was not challenged and the study only focused on the layout of the system (profile-based or not) and its interactivity. More recently, authors in <cite class="ltx_cite ltx_citemacro_citep">(Kouki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib25" title="">2017</a>)</cite> involved graphs to highlight user preferences for <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.2">item-based</span> explanations. However, their graphs consist in concentric circle diagrams or pathways between columns, which we argue do not fully leverage the capabilities of graphs. We draw a similar conclusion for <cite class="ltx_cite ltx_citemacro_citep">(Ghazimatin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib16" title="">2020</a>)</cite>, where authors show participants’ preference for action-oriented over connection-oriented explanations, but where graphs are limited to the paths they can exhibit.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">In this work, similar to the aforementioned studies, we maintain the evaluation process that involves user studies to assess design performance. However, we deviate from them by integrating our analysis directly into the explanatory context.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We seek to answer <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">RQ1</span> by exploring stakeholders’ expectations regarding graph-based explanations for AI-based recommender systems. To achieve this, we conduct a qualitative study involving participants with varying expertise levels in AI systems, employing a think-aloud case study to collect insights and remarks.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Study design</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We interviewed 12 participants. All participants were volunteers, recruited through an email campaign within the university with which the authors are affiliated. We conducted a 30-minute interview with each participant. To ensure data privacy, participants signed a consent form designed and approved in collaboration with the Data Protection Officer. Each interview was then divided into three parts.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In the first part of the session, we assessed the perceived expertise level of participants regarding AI-based systems in general and recommender systems specifically, using a preliminary questionnaire. From these answers, we obtained three distinct groups. The first group consists of <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.1">Experts</em> (4 participants); people who consider themselves <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.2">really familiar</em> with AI-based systems and either have a <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.3">precise idea</em> of what recommender systems are, or developed such algorithms. On the other side of the spectrum, another group includes <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.4">non-Experts</em> (2 participants); these users consider themselves <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.5">slightly familiar</em> with AI-based systems and are only interacting with recommender systems as users, or have a <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.6">general idea</em> of how these systems work. The last group lies in-between these two groups and includes <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.7">Insiders</em> (6 participants); people who are <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.8">familiar</em> with AI-based systems and have a <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.9">general idea</em> of how recommender systems work.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">During the second part of the interview, we evaluated participants’ perceived understanding of AI-based systems. They answered questions about their anticipated risks for recommender systems, their comprehension of the recommendations when they receive some, and their understanding of the criteria influencing these recommendations.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Lastly, we introduced a task-oriented think-aloud scenario. In this scenario, we presented participants with an AI-based book recommendation. To increase participant engagement, we collected their book preferences prior to the interview and used this information to create a personalized context incorporated into the AI system, thus making the experiment more realistic. The system recommendation was presented within a graph-based explanation, featuring a subset of the participants’ previously read books alongside other users and their readings. This explanation was displayed as a <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.1">bipartite graph</em>, where users were connected to the books they had read, with a distinct link indicating the system recommendation. Lastly, we provided information about book preferences using link weighting (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.F1" title="Figure 1 ‣ 3.1. Study design ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">1</span></a> for an illustration). We did not disclose to the participants the details of the recommendation algorithm or how we had chosen the other visible users. We asked the participants to discuss the relevance of the recommendation to them, their understanding of the elements that led to this recommendation, and what kind of information was missing to better understand the recommendation.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S3.F1.g1" src="extracted/5736934/img/50951843.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.3.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S3.F1.4.2" style="font-size:90%;">Bipartite graph-based explanation containing two sets of distinct nodes. Blue nodes represent users (along with their identifier) and red nodes represent books (along with their title). A user is linked to a book if the former has read and rated the latter. Thin links denote low ratings, while thick links denote high ratings. The <span class="ltx_text" id="S3.F1.4.2.1" style="color:#0000FF;">blue link</span> corresponds to the system recommendation.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Results</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We extracted key insights from the interviews, focusing on participants’ understanding of the recommendation through the lens of graph-based explanation. Additionally, we performed an inductive content analysis <cite class="ltx_cite ltx_citemacro_citep">(Elo and Kyngäs, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib13" title="">2008</a>)</cite> of the notes taken during the interviews, identifying main themes related to participants’ desiderata toward graph-based explanation. These results are summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.T1" title="Table 1 ‣ 3.2.4. Graph-based explanations are considered interpretable but should contain item information ‣ 3.2. Results ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>A high level of perceived understanding, based on three main criteria</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">To the question <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">“To what extent do you understand the suggestions made to you by AI-based recommendation systems?”</em>, the participants, whatever their level of expertise, said they understood the recommendations in most of the cases. In the few cases when participants find the recommendations unclear, they suggest that their profile diverges from a hypothetical <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.2">“basic”</em> user, or they propose that the algorithm’s designers intentionally introduced a mechanism to propose a <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.3">“percentage of new things”</em> unknown to the users.
When questioned about their understanding of the factors influencing a system to recommend a specific item, participants identified three main criteria. Both <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.4">Insider</em> and <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.5">non-Expert</em> participants reported that the recommendations are primarily built based on user characteristics, such as <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.6">“the person’s gender, age”</em> or <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.7">“the person’s interests”</em>. Some <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.8">Experts</em> and a few <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.9">Insiders</em> believed that the <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.10">“similarity”</em> between their profile and the profiles of other users takes precedence in the recommendation algorithm’s decision. According to this group, the cross-knowledge of a large number of users and their preferences enables the system to suggest new items. Lastly, some participants considered that a recommendation is primarily influenced by their personal history of interactions with the platform. These participants mentioned factors such as <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.11">“clicking on an ad”</em>, the <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.12">“frequency of viewing”</em>, or the <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.1.13">“purchase history”</em> as elements at the core of the system’s suggestions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Strong link between context knowledge and recommendation understanding</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">When asked <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">“Do you think this recommendation is relevant? and why?”</em>, participants unfamiliar with the item suggested by the system either expressed a lack of understanding of the recommendation (<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p1.1.2">“I do not know this book therefore I don’t know if the recommendation is valuable.”</em>) or asked for additional information about the book’s characteristics (<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p1.1.3">e.g.</em> author or literary genre) before answering. Conversely, a participant who is familiar with the recommendation or, after reviewing its related characteristics identifies familiar elements (other books by the same author), will immediately deem the recommendation relevant and give it credibility: <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p1.1.4">“The recommendation seems relevant […] because I know the author, and it makes me want to read the book.”</em>. In one case, the participant was familiar with the recommendation, had already read the suggested book, yet found it less relevant. This judgment was based on the perception that the recommendation did not align with the literary genre they prefer. In summary, regardless of their level of expertise, participants tend to draw parallels between the predicted book and their reading history, considering factors such as literary genre, period, or author. The analysis of similarities with other users is only taken into consideration at a later stage and appears to be optional for the positive or negative judgment of the recommendation. In one instance, the recommendation aligns with the participant’s literary preferences (known and appreciated author) and is described as relevant, even if the participant does not see the connection with the elements provided in the explanation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Graphs as objects modeling both similarities and popularity</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Participants’ utilization of the graph-based explanation can be characterized along two dimensions. Firstly, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.1">Insiders</em> and <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.2">Experts</em> follow the links between users and books to highlight similarities among users. These similarities are described through the sharing of literary tastes and represent the manifestation of their a priori understanding of how a recommender system operates. In such cases, the weight (thickness) associated with the links, especially when positive, plays an important role: <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.3">“It visually strikes me a bit”</em>. Secondly, the graph-based explanation leads to an understanding of predictions in terms of popularity, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.4">i.e.</em> participants focus on the quantity of users who have liked the recommendation. In these cases, the recommendation is seen as a consequence of the enthusiasm surrounding it, rather than its relevance to the individual user.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">A few remarks about the use of graphs by participants should be noted. The conclusions drawn regarding user similarities through graph analysis require time (a few seconds), even for <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p2.1.1">Experts</em> who, at the end of the interview, considered the graph easy to use. To derive insights about the mechanisms of the system, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p2.1.2">non-Experts</em> tend not to analyze the graph through its links and users, but rather to focus on already-known item characteristics (genre or author). Finally, none of the participants mention graph-specific notions, such as <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p2.1.3">clique</em>, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p2.1.4">i.e.</em> fully connected subset elements in a graph or <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p2.1.5">density</em>. Intuitively, such elements could have been used to deeply understand subgroups of users that led to the recommendation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4. </span>Graph-based explanations are considered interpretable but should contain item information</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">Participants, regardless of their level of expertise, all agree that it would be useful to have more information about the characteristics of the suggested item, such as literary genre, period, etc. This would enable the creation of <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p1.1.1">“connections between different books”</em> and provide a <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p1.1.2">“general context”</em> to the decision: <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p1.1.3">“What I miss is the tool’s knowledge about the world.”</em>, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p1.1.4">“I lack an idea of which types of books are similar to each other.”</em>. To address this, a participant suggests using colors to highlight the proximity between groups of books. Furthermore, we noticed that the needs for additional item information align perfectly with the understanding of the recommendation; more than any other feature, participants require item characteristics that they can rely on to judge the relevance of a given recommendation.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS4.p2">
<p class="ltx_p" id="S3.SS2.SSS4.p2.1">An <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.1">Expert</em> and an <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.2">Insider</em> discussed the value of links representing a weak attraction to items, mentioning that they helped establish a <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.3">“contrast”</em> with other users, thereby enhancing their understanding of <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.4">“similarities in their profile with other users.”</em>. Some <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.5">Insiders</em> evoked adding numerical information on links, rather than playing on thickness. However, the relevance of information about disliked items, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.6">i.e.</em> thin links in the graph, is not shared by all users: <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.7">“I don’t really get the thin links in this graph.”</em>, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p2.1.8">“The fact that we have a thick line and a very thin one […], side by side, confuses me a bit.”</em>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS4.p3">
<p class="ltx_p" id="S3.SS2.SSS4.p3.1"><em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.1">Experts</em> find graph explanations <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.2">“very easily interpretable”</em> or <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.3">“sufficient”</em> and do not see the need for them to be <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.4">“transformed into natural language.”</em>. One of them interprets them as the visual counterpart to the classic formula found on platforms using recommendation: <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.5">“Other users than you, who liked similar things to you, also liked…”</em>. Moreover, they <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.6">“do not expect to have an exhaustive representation”</em> of the context of the recommendation. Yet, even <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.7">Experts</em> can have trouble distinguishing clear relations with other users; <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS4.p3.1.8">“I don’t see clearly whether I am close or not to other users. […] I don’t fully see the connection between me and other users.”</em>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.6.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S3.T1.7.2" style="font-size:90%;">Main themes on graph-based explanation design’s desiderata emerging from interviews.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T1.4" style="width:433.6pt;height:6276.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.4.4.4">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T1.1.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S3.T1.1.1.1.1.1.p1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1.p1.1"><span class="ltx_text" id="S3.T1.1.1.1.1.1.p1.1.1"></span> <span class="ltx_text" id="S3.T1.1.1.1.1.1.p1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.1.1.1.p1.1.2.1">
<span class="ltx_tr" id="S3.T1.1.1.1.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.1.1.1.p1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.p1.1.2.1.1.1.1">Explanation</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.1.1.1.p1.1.3"></span></span>
</span></span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.2.2.2.2"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T1.2.2.2.2.1">
<span class="ltx_para ltx_noindent" id="S3.T1.2.2.2.2.1.p1">
<span class="ltx_p" id="S3.T1.2.2.2.2.1.p1.1"><span class="ltx_text" id="S3.T1.2.2.2.2.1.p1.1.1"></span> <span class="ltx_text" id="S3.T1.2.2.2.2.1.p1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.2.2.2.2.1.p1.1.2.1">
<span class="ltx_tr" id="S3.T1.2.2.2.2.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.2.2.2.1.p1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.2.1.p1.1.2.1.1.1.1">Experts</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.2.2.2.2.1.p1.1.3"></span></span>
</span></span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.3.3.3.3"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T1.3.3.3.3.1">
<span class="ltx_para ltx_noindent" id="S3.T1.3.3.3.3.1.p1">
<span class="ltx_p" id="S3.T1.3.3.3.3.1.p1.1"><span class="ltx_text" id="S3.T1.3.3.3.3.1.p1.1.1"></span> <span class="ltx_text" id="S3.T1.3.3.3.3.1.p1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.3.3.3.3.1.p1.1.2.1">
<span class="ltx_tr" id="S3.T1.3.3.3.3.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.3.3.3.1.p1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.1.p1.1.2.1.1.1.1">Insiders</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.3.3.3.3.1.p1.1.3"></span></span>
</span></span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.4.4.4.4"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T1.4.4.4.4.1">
<span class="ltx_para ltx_noindent" id="S3.T1.4.4.4.4.1.p1">
<span class="ltx_p" id="S3.T1.4.4.4.4.1.p1.1"><span class="ltx_text" id="S3.T1.4.4.4.4.1.p1.1.1"></span> <span class="ltx_text" id="S3.T1.4.4.4.4.1.p1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.4.4.4.4.1.p1.1.2.1">
<span class="ltx_tr" id="S3.T1.4.4.4.4.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.4.4.4.4.1.p1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.4.4.4.1.p1.1.2.1.1.1.1">non-Experts</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.4.4.4.4.1.p1.1.3"></span></span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.4.4.5.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.4.4.5.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.5.1.1.1">
<span class="ltx_p" id="S3.T1.4.4.5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.4.5.1.1.1.1.1">Global design choice</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.4.4.5.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.5.1.2.1">
<span class="ltx_p" id="S3.T1.4.4.5.1.2.1.1">Natural language explanation, no need to transform it into natural language, not structured, contrastive (with link thickness), not exhaustive, [graphs] are really not bad, very easily interpretable, interactions between groups, I don’t fully see the connection between me and other users</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.4.4.5.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.5.1.3.1">
<span class="ltx_p" id="S3.T1.4.4.5.1.3.1.1">Links between books, themes of books, adding numbered information to links, The fact that we have a thick line and a very thin one […], side by side, confuses me a bit.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.4.4.5.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.5.1.4.1">
<span class="ltx_p" id="S3.T1.4.4.5.1.4.1.1">I don’t see what [other users] liked about [an item]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.6.2">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.4.4.6.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.6.2.1.1">
<span class="ltx_p" id="S3.T1.4.4.6.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.4.6.2.1.1.1.1">Item-based vs. user-based</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.4.4.6.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.6.2.2.1">
<span class="ltx_p" id="S3.T1.4.4.6.2.2.1.1">You like this book and there is this other similar book, connections between different books vs. I don’t see if I’m close to other users, I miss visualization of my similarity with other users</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.4.4.6.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.6.2.3.1">
<span class="ltx_p" id="S3.T1.4.4.6.2.3.1.1">How to find the book characteristics?, I lack an idea of which types of books are similar to each other, thicker links if books are of the same kind, links between books and themes vs. both this user and I did not like this book</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.4.4.6.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.4.4.6.2.4.1">
<span class="ltx_p" id="S3.T1.4.4.6.2.4.1.1">[About books] Do you consider classics? Where do authors come from? Did the books receive prizes?</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Study 2: Influence of explanation design</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To address <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">RQ2</span>, <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">RQ3</span> and <span class="ltx_text ltx_font_bold" id="S4.p1.1.3">RQ4</span>, we conduct a qualitative user study in which we investigate the influence of explanation design, among them a graph-based explanation, on various AI-based recommender system users. An example of our interface is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#A0.F5" title="Figure 5 ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">5</span></a> in the Appendix.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Study design</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Recommender system</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">For this purpose, we use the setup introduced in our qualitative experiment (see Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3" title="3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">3</span></a>): we build an AI-based book recommender system which goal is to provide the participant with a book suggestion given pre-selected reading preferences. We did not share the functioning details of the recommender system with the participants, but specified that the recommendation they were given was AI-based.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Explanation design</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">We introduced three different explanation designs, that were detailed to each participant through a small paragraph, as well as a reading key. We used the well-known <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.1">SHAP</em> <cite class="ltx_cite ltx_citemacro_citep">(Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib29" title="">2017</a>)</cite> approach to design a feature importance oriented explanation design. This choice stems from the popularity of this method in the Explainable AI (XAI) community as well as for its tight bounds with the users’ expectations obtained from our first study, <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.2">i.e.</em> book characteristics are considered important for the quality of the explanation. We also presented a <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.3">Text</em> explanation, containing concise item-based sentences stating <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.4">why</em> this recommendation was made. This choice was influenced by recent studies indicating that text explanations, specifically with item-based wording, were perceived more persuasive than other visual formats <cite class="ltx_cite ltx_citemacro_citep">(Kouki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib26" title="">2019</a>)</cite> and were effective in providing the user with personalized feeling, which is correlated with trust in the system <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Curley, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib57" title="">2018</a>)</cite>. Finally, we improved the <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p1.1.5">Graph</em> design introduced in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S3.F1" title="Figure 1 ‣ 3.1. Study design ‣ 3. Study 1: Qualitative exploration of stakeholders’ expectations regarding graph-based explanations ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">1</span></a> based on participants’ feedback from our qualitative study. Specifically, we focused on item characteristics by projecting the initial bipartite graph into an item-oriented one. In this representation, each book is a node, and edges connect books that share common readers in our database. Additionally, we incorporated information about book characteristics (literary genre, author, publication year, etc) through node colors. Consequently, this final graph representation aligns more closely with users’ expectations. We illustrate these designs in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F2" title="Figure 2 ‣ 4.1.2. Explanation design ‣ 4.1. Study design ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S4.F2.sf1.g1" src="extracted/5736934/img/expl_text_362371.001.jpeg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F2.sf1.3.2" style="font-size:90%;">Text-based explanation. Item-based wording is chosen accordingly with results from <cite class="ltx_cite ltx_citemacro_citep">(Kouki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib26" title="">2019</a>)</cite>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S4.F2.sf2.g1" src="extracted/5736934/img/expl_graph_362371.001.jpeg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F2.sf2.3.2" style="font-size:90%;">Graph-based explanation. Nodes represent the books read by the user, except for the black-circled node which corresponds to the recommendation. Node colors denote similarities between book characteristics. A link between two books exists if they both have been read by some user. Thickness of links stands for the number of common readers for two books.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S4.F2.sf3.g1" src="extracted/5736934/img/expl_shap_362371.001.jpeg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.F2.sf3.3.2" style="font-size:90%;">SHAP-based explanation <cite class="ltx_cite ltx_citemacro_citep">(Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib29" title="">2017</a>)</cite>. Recommendation is explained through feature importance.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">Explanation designs used in the second user study.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Experimental conditions</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Overall, 66 participants were recruited through two channels: (i) students, doctoral students and researchers from the author’s university and affiliated laboratories, and (ii) users of the Reddit forum <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p1.1.1">“SampleSize”</em><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reddit.com/r/SampleSize/" title="">https://www.reddit.com/r/SampleSize/</a></span></span></span> dedicated to user studies. All participants were asked to fill out an online form divided into two parts. First, as in the previous study, participants answered preliminary questions about their perceived expertise level toward AI-based systems in general and AI-based recommender systems in particular. We ended up with 11 <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p1.1.2">non-Experts</em> (16.7%), 32 <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p1.1.3">Insiders</em> (48.5%) and 23 <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p1.1.4">Experts</em> (34.8%). We then asked each participant to choose their favorite book selection from among 5 sets of 10 books and used our AI-based recommender system to suggest a new book. Each participant was shown all explanation designs for this recommendation (in a random order), and for each design, they were asked to answer a set of questions related to our evaluation measures. Finally, we collected participants’ design preferences by explicitly asking them to rank the three designs. The whole study lasted around 10 minutes.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">In summary, this study is constructed using a Mixed Factorial design, involving the participants’ <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p2.1.1">expertise level</em> as a between-subject variable and the <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p2.1.2">explanation design</em> as a within subject variable. Therefore, in the following analysis, we assess three effects: (i) the influence of the within-subject variable on the measures, (ii) the influence of the between-subject variable on the measures, and (iii) the effect of the interaction between the two factors on the measures.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4. </span>Evaluation constructs</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">The same set of questions was used for each explanation design. These questions are built according to previous literature on explanation evaluation within AI-based frameworks <cite class="ltx_cite ltx_citemacro_citep">(O’Brien and Cairns, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib40" title="">2015</a>; Hoffman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib22" title="">2018</a>; Purificato et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib43" title="">2021</a>; Bertrand et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib5" title="">2023</a>)</cite> and are shaped to fit our specific experimental design. All questions have the same structure; given an explanation design, we ask the participant <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS4.p1.1.1">“Please evaluate your level of agreement with the following sentences.”</em>. Each participant answers using a 5-point Likert scale ranging from <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS4.p1.1.2">Strongly disagree</em> to <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS4.p1.1.3">Strongly agree</em>. We organized the study around four global constructs (detailed below). We verified the internal validity of the questionnaire by computing Pearson correlations between answers within each construct and the total of each construct (at this stage we discarded results from one question), and its internal reliability using McDonald’s <math alttext="\omega" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.1.m1.1"><semantics id="S4.SS1.SSS4.p1.1.m1.1a"><mi id="S4.SS1.SSS4.p1.1.m1.1.1" xref="S4.SS1.SSS4.p1.1.m1.1.1.cmml">ω</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.1.m1.1b"><ci id="S4.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1">𝜔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.1.m1.1c">\omega</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.1.m1.1d">italic_ω</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(McDonald, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib31" title="">2013</a>)</cite>. Detailed questions and corresponding categories are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.T2" title="Table 2 ‣ 4.1.4. Evaluation constructs ‣ 4.1. Study design ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.1">Subjective understanding.</span> We measured subjective understanding by asking participants if they understood the recommendation made by the system or if they were able to derive insights about its internal mechanics.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p3">
<p class="ltx_p" id="S4.SS1.SSS4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p3.1.1">Objective understanding.</span> We measured objective understanding of the recommendations by asking participants about the features used by the system to provide the recommendation. These questions were specifically designed for the study but encompass different recommender systems elements that have been emphasized in our first study as well as in previous studies from the literature <cite class="ltx_cite ltx_citemacro_citep">(Kouki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib26" title="">2019</a>; Bertrand et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib5" title="">2023</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS4.p3.1.2">e.g.</em> item-based influence, user-based influence or self-historical information influence.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p4">
<p class="ltx_p" id="S4.SS1.SSS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p4.1.1">Usability.</span> Usability was measured by asking participants how difficult the explanation was to read and understand. We also asked participants about how easily other people would learn to read this explanation <cite class="ltx_cite ltx_citemacro_citep">(Alabi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib3" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p5">
<p class="ltx_p" id="S4.SS1.SSS4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p5.1.1">Curiosity.</span> We measured the curiosity induced by the explanation design using questions adapted from <cite class="ltx_cite ltx_citemacro_citep">(O’Brien and Cairns, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib40" title="">2015</a>; Hoffman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib22" title="">2018</a>)</cite>. We asked the participants if they were curious to know why the recommender system did not provide another suggestion, and if the recommendation incited curiosity.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.17.3.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Questions asked to participants for each explanation design, with the <math alttext="p" class="ltx_Math" display="inline" id="S4.T2.3.1.m1.1"><semantics id="S4.T2.3.1.m1.1b"><mi id="S4.T2.3.1.m1.1.1" xref="S4.T2.3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.m1.1c"><ci id="S4.T2.3.1.m1.1.1.cmml" xref="S4.T2.3.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.1.m1.1e">italic_p</annotation></semantics></math>-value corresponding to the validity test (Person’s correlation between each question and its corresponding global construct) and the McDonald’s <math alttext="\omega" class="ltx_Math" display="inline" id="S4.T2.4.2.m2.1"><semantics id="S4.T2.4.2.m2.1b"><mi id="S4.T2.4.2.m2.1.1" xref="S4.T2.4.2.m2.1.1.cmml">ω</mi><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.m2.1c"><ci id="S4.T2.4.2.m2.1.1.cmml" xref="S4.T2.4.2.m2.1.1">𝜔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.m2.1d">\omega</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.2.m2.1e">italic_ω</annotation></semantics></math> score.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.15" style="width:433.6pt;height:8077.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.15.11">
<tr class="ltx_tr" id="S4.T2.5.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.1.2.1">Construct</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.1.1.1">Reliability <math alttext="\omega" class="ltx_Math" display="inline" id="S4.T2.5.1.1.1.1.m1.1"><semantics id="S4.T2.5.1.1.1.1.m1.1a"><mi id="S4.T2.5.1.1.1.1.m1.1.1" xref="S4.T2.5.1.1.1.1.m1.1.1.cmml">ω</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.1.1.m1.1b"><ci id="S4.T2.5.1.1.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.1.1.m1.1.1">𝜔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.1.1.m1.1c">\omega</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.1.1.1.1.m1.1d">italic_ω</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.1.3.1">Validity</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.1.4.1">Question</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.6.2.2.2" rowspan="2"><span class="ltx_text" id="S4.T2.6.2.2.2.1">Subjective understanding</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.2.2.3" rowspan="2"><span class="ltx_text" id="S4.T2.6.2.2.3.1">84.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.2.2.1"><span class="ltx_text" id="S4.T2.6.2.2.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.6.2.2.1.1.m1.1"><semantics id="S4.T2.6.2.2.1.1.m1.1a"><mrow id="S4.T2.6.2.2.1.1.m1.1.1" xref="S4.T2.6.2.2.1.1.m1.1.1.cmml"><mi id="S4.T2.6.2.2.1.1.m1.1.1.2" xref="S4.T2.6.2.2.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.6.2.2.1.1.m1.1.1.1" xref="S4.T2.6.2.2.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.6.2.2.1.1.m1.1.1.3" xref="S4.T2.6.2.2.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.2.2.1.1.m1.1b"><apply id="S4.T2.6.2.2.1.1.m1.1.1.cmml" xref="S4.T2.6.2.2.1.1.m1.1.1"><lt id="S4.T2.6.2.2.1.1.m1.1.1.1.cmml" xref="S4.T2.6.2.2.1.1.m1.1.1.1"></lt><ci id="S4.T2.6.2.2.1.1.m1.1.1.2.cmml" xref="S4.T2.6.2.2.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.6.2.2.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.6.2.2.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.2.2.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.2.2.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T2.6.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.6.2.2.4.1">
<span class="ltx_p" id="S4.T2.6.2.2.4.1.1">I understand why this recommendation was made to me.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.3.3">
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.3.1"><span class="ltx_text" id="S4.T2.7.3.3.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.7.3.3.1.1.m1.1"><semantics id="S4.T2.7.3.3.1.1.m1.1a"><mrow id="S4.T2.7.3.3.1.1.m1.1.1" xref="S4.T2.7.3.3.1.1.m1.1.1.cmml"><mi id="S4.T2.7.3.3.1.1.m1.1.1.2" xref="S4.T2.7.3.3.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.7.3.3.1.1.m1.1.1.1" xref="S4.T2.7.3.3.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.7.3.3.1.1.m1.1.1.3" xref="S4.T2.7.3.3.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.3.3.1.1.m1.1b"><apply id="S4.T2.7.3.3.1.1.m1.1.1.cmml" xref="S4.T2.7.3.3.1.1.m1.1.1"><lt id="S4.T2.7.3.3.1.1.m1.1.1.1.cmml" xref="S4.T2.7.3.3.1.1.m1.1.1.1"></lt><ci id="S4.T2.7.3.3.1.1.m1.1.1.2.cmml" xref="S4.T2.7.3.3.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.7.3.3.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.7.3.3.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.3.3.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.3.3.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify" id="S4.T2.7.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.3.3.2.1">
<span class="ltx_p" id="S4.T2.7.3.3.2.1.1">I can figure out the internal mechanics of the recommender system.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.8.4.4.2" rowspan="3"><span class="ltx_text" id="S4.T2.8.4.4.2.1">Objective understanding</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.8.4.4.3" rowspan="3"><span class="ltx_text" id="S4.T2.8.4.4.3.1">63.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.8.4.4.1"><span class="ltx_text" id="S4.T2.8.4.4.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.8.4.4.1.1.m1.1"><semantics id="S4.T2.8.4.4.1.1.m1.1a"><mrow id="S4.T2.8.4.4.1.1.m1.1.1" xref="S4.T2.8.4.4.1.1.m1.1.1.cmml"><mi id="S4.T2.8.4.4.1.1.m1.1.1.2" xref="S4.T2.8.4.4.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.8.4.4.1.1.m1.1.1.1" xref="S4.T2.8.4.4.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.8.4.4.1.1.m1.1.1.3" xref="S4.T2.8.4.4.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.4.4.1.1.m1.1b"><apply id="S4.T2.8.4.4.1.1.m1.1.1.cmml" xref="S4.T2.8.4.4.1.1.m1.1.1"><lt id="S4.T2.8.4.4.1.1.m1.1.1.1.cmml" xref="S4.T2.8.4.4.1.1.m1.1.1.1"></lt><ci id="S4.T2.8.4.4.1.1.m1.1.1.2.cmml" xref="S4.T2.8.4.4.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.8.4.4.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.8.4.4.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.4.4.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.4.4.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.8.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.8.4.4.4.1">
<span class="ltx_p" id="S4.T2.8.4.4.4.1.1">I believe this book was recommended to me because it is similar (literary genre, author, etc.) to the books I chose previously.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.5.5">
<td class="ltx_td ltx_align_center" id="S4.T2.9.5.5.1"><span class="ltx_text" id="S4.T2.9.5.5.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.9.5.5.1.1.m1.1"><semantics id="S4.T2.9.5.5.1.1.m1.1a"><mrow id="S4.T2.9.5.5.1.1.m1.1.1" xref="S4.T2.9.5.5.1.1.m1.1.1.cmml"><mi id="S4.T2.9.5.5.1.1.m1.1.1.2" xref="S4.T2.9.5.5.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.9.5.5.1.1.m1.1.1.1" xref="S4.T2.9.5.5.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.9.5.5.1.1.m1.1.1.3" xref="S4.T2.9.5.5.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.5.5.1.1.m1.1b"><apply id="S4.T2.9.5.5.1.1.m1.1.1.cmml" xref="S4.T2.9.5.5.1.1.m1.1.1"><lt id="S4.T2.9.5.5.1.1.m1.1.1.1.cmml" xref="S4.T2.9.5.5.1.1.m1.1.1.1"></lt><ci id="S4.T2.9.5.5.1.1.m1.1.1.2.cmml" xref="S4.T2.9.5.5.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.9.5.5.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.9.5.5.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.5.5.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.5.5.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify" id="S4.T2.9.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.9.5.5.2.1">
<span class="ltx_p" id="S4.T2.9.5.5.2.1.1">The fact that I share preferences with other readers only counted a little by the algorithm.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.10.6.6">
<td class="ltx_td ltx_align_center" id="S4.T2.10.6.6.1"><span class="ltx_text" id="S4.T2.10.6.6.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.10.6.6.1.1.m1.1"><semantics id="S4.T2.10.6.6.1.1.m1.1a"><mrow id="S4.T2.10.6.6.1.1.m1.1.1" xref="S4.T2.10.6.6.1.1.m1.1.1.cmml"><mi id="S4.T2.10.6.6.1.1.m1.1.1.2" xref="S4.T2.10.6.6.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.10.6.6.1.1.m1.1.1.1" xref="S4.T2.10.6.6.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.10.6.6.1.1.m1.1.1.3" xref="S4.T2.10.6.6.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.6.6.1.1.m1.1b"><apply id="S4.T2.10.6.6.1.1.m1.1.1.cmml" xref="S4.T2.10.6.6.1.1.m1.1.1"><lt id="S4.T2.10.6.6.1.1.m1.1.1.1.cmml" xref="S4.T2.10.6.6.1.1.m1.1.1.1"></lt><ci id="S4.T2.10.6.6.1.1.m1.1.1.2.cmml" xref="S4.T2.10.6.6.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.10.6.6.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.10.6.6.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.6.6.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.6.6.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify" id="S4.T2.10.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.10.6.6.2.1">
<span class="ltx_p" id="S4.T2.10.6.6.2.1.1">I believe that this book was recommended to me due to its popularity among other readers.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.11.7.7.2" rowspan="2"><span class="ltx_text" id="S4.T2.11.7.7.2.1">Usability</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.7.7.3" rowspan="2"><span class="ltx_text" id="S4.T2.11.7.7.3.1">48.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.7.7.1"><span class="ltx_text" id="S4.T2.11.7.7.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.11.7.7.1.1.m1.1"><semantics id="S4.T2.11.7.7.1.1.m1.1a"><mrow id="S4.T2.11.7.7.1.1.m1.1.1" xref="S4.T2.11.7.7.1.1.m1.1.1.cmml"><mi id="S4.T2.11.7.7.1.1.m1.1.1.2" xref="S4.T2.11.7.7.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.11.7.7.1.1.m1.1.1.1" xref="S4.T2.11.7.7.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.11.7.7.1.1.m1.1.1.3" xref="S4.T2.11.7.7.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.7.7.1.1.m1.1b"><apply id="S4.T2.11.7.7.1.1.m1.1.1.cmml" xref="S4.T2.11.7.7.1.1.m1.1.1"><lt id="S4.T2.11.7.7.1.1.m1.1.1.1.cmml" xref="S4.T2.11.7.7.1.1.m1.1.1.1"></lt><ci id="S4.T2.11.7.7.1.1.m1.1.1.2.cmml" xref="S4.T2.11.7.7.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.11.7.7.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.11.7.7.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.7.7.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.7.7.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.11.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.11.7.7.4.1">
<span class="ltx_p" id="S4.T2.11.7.7.4.1.1">I would imagine that most people would learn to read this explanation very quickly.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.8.8">
<td class="ltx_td ltx_align_center" id="S4.T2.12.8.8.1"><span class="ltx_text" id="S4.T2.12.8.8.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.12.8.8.1.1.m1.1"><semantics id="S4.T2.12.8.8.1.1.m1.1a"><mrow id="S4.T2.12.8.8.1.1.m1.1.1" xref="S4.T2.12.8.8.1.1.m1.1.1.cmml"><mi id="S4.T2.12.8.8.1.1.m1.1.1.2" xref="S4.T2.12.8.8.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.12.8.8.1.1.m1.1.1.1" xref="S4.T2.12.8.8.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.12.8.8.1.1.m1.1.1.3" xref="S4.T2.12.8.8.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.8.8.1.1.m1.1b"><apply id="S4.T2.12.8.8.1.1.m1.1.1.cmml" xref="S4.T2.12.8.8.1.1.m1.1.1"><lt id="S4.T2.12.8.8.1.1.m1.1.1.1.cmml" xref="S4.T2.12.8.8.1.1.m1.1.1.1"></lt><ci id="S4.T2.12.8.8.1.1.m1.1.1.2.cmml" xref="S4.T2.12.8.8.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.12.8.8.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.12.8.8.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.8.8.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.8.8.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify" id="S4.T2.12.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.12.8.8.2.1">
<span class="ltx_p" id="S4.T2.12.8.8.2.1.1">I think the recommender system is interpretable.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.13.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.13.9.9.2"><span class="ltx_text" id="S4.T2.13.9.9.2.1"><em class="ltx_emph ltx_font_italic" id="S4.T2.13.9.9.2.1.1">Usability (not used)</em></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.13.9.9.3"><span class="ltx_text" id="S4.T2.13.9.9.3.1">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.13.9.9.1"><span class="ltx_text" id="S4.T2.13.9.9.1.1"><math alttext="p&gt;.05" class="ltx_Math" display="inline" id="S4.T2.13.9.9.1.1.m1.1"><semantics id="S4.T2.13.9.9.1.1.m1.1a"><mrow id="S4.T2.13.9.9.1.1.m1.1.1" xref="S4.T2.13.9.9.1.1.m1.1.1.cmml"><mi id="S4.T2.13.9.9.1.1.m1.1.1.2" xref="S4.T2.13.9.9.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.13.9.9.1.1.m1.1.1.1" xref="S4.T2.13.9.9.1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T2.13.9.9.1.1.m1.1.1.3" xref="S4.T2.13.9.9.1.1.m1.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.13.9.9.1.1.m1.1b"><apply id="S4.T2.13.9.9.1.1.m1.1.1.cmml" xref="S4.T2.13.9.9.1.1.m1.1.1"><gt id="S4.T2.13.9.9.1.1.m1.1.1.1.cmml" xref="S4.T2.13.9.9.1.1.m1.1.1.1"></gt><ci id="S4.T2.13.9.9.1.1.m1.1.1.2.cmml" xref="S4.T2.13.9.9.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.13.9.9.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.13.9.9.1.1.m1.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.9.9.1.1.m1.1c">p&gt;.05</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.9.9.1.1.m1.1d">italic_p &gt; .05</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.13.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.13.9.9.4.1">
<span class="ltx_p" id="S4.T2.13.9.9.4.1.1"><em class="ltx_emph ltx_font_italic" id="S4.T2.13.9.9.4.1.1.1">I found it difficult to read and understand the recommendation context.</em></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.14.10.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.14.10.10.2" rowspan="2"><span class="ltx_text" id="S4.T2.14.10.10.2.1">Curiosity (0.510)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.14.10.10.3" rowspan="2"><span class="ltx_text" id="S4.T2.14.10.10.3.1">60.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.14.10.10.1"><span class="ltx_text" id="S4.T2.14.10.10.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.14.10.10.1.1.m1.1"><semantics id="S4.T2.14.10.10.1.1.m1.1a"><mrow id="S4.T2.14.10.10.1.1.m1.1.1" xref="S4.T2.14.10.10.1.1.m1.1.1.cmml"><mi id="S4.T2.14.10.10.1.1.m1.1.1.2" xref="S4.T2.14.10.10.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.14.10.10.1.1.m1.1.1.1" xref="S4.T2.14.10.10.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.14.10.10.1.1.m1.1.1.3" xref="S4.T2.14.10.10.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.14.10.10.1.1.m1.1b"><apply id="S4.T2.14.10.10.1.1.m1.1.1.cmml" xref="S4.T2.14.10.10.1.1.m1.1.1"><lt id="S4.T2.14.10.10.1.1.m1.1.1.1.cmml" xref="S4.T2.14.10.10.1.1.m1.1.1.1"></lt><ci id="S4.T2.14.10.10.1.1.m1.1.1.2.cmml" xref="S4.T2.14.10.10.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.14.10.10.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.14.10.10.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.10.10.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.10.10.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.14.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.14.10.10.4.1">
<span class="ltx_p" id="S4.T2.14.10.10.4.1.1">I am curious about why the recommender system did not make other decisions.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.15.11.11">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.15.11.11.1"><span class="ltx_text" id="S4.T2.15.11.11.1.1"><math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.T2.15.11.11.1.1.m1.1"><semantics id="S4.T2.15.11.11.1.1.m1.1a"><mrow id="S4.T2.15.11.11.1.1.m1.1.1" xref="S4.T2.15.11.11.1.1.m1.1.1.cmml"><mi id="S4.T2.15.11.11.1.1.m1.1.1.2" xref="S4.T2.15.11.11.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.T2.15.11.11.1.1.m1.1.1.1" xref="S4.T2.15.11.11.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.15.11.11.1.1.m1.1.1.3" xref="S4.T2.15.11.11.1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.15.11.11.1.1.m1.1b"><apply id="S4.T2.15.11.11.1.1.m1.1.1.cmml" xref="S4.T2.15.11.11.1.1.m1.1.1"><lt id="S4.T2.15.11.11.1.1.m1.1.1.1.cmml" xref="S4.T2.15.11.11.1.1.m1.1.1.1"></lt><ci id="S4.T2.15.11.11.1.1.m1.1.1.2.cmml" xref="S4.T2.15.11.11.1.1.m1.1.1.2">𝑝</ci><cn id="S4.T2.15.11.11.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.15.11.11.1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.11.11.1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.11.11.1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T2.15.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.15.11.11.2.1">
<span class="ltx_p" id="S4.T2.15.11.11.2.1.1">The recommended book incited my curiosity.</span>
</span>
</td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We used a repeated-measure analysis of variance (RM-ANOVA) to analyze the collected measures for each participant. Each measure consisted in the average of the ratings for the corresponding questions. All measures passed the sphericity assumption (constant variance across repeated measure), either using the traditional Mauchly’s test or after applying Greenhouse-Geisser correction. We confirmed the homogeneity of variance for all levels of the repeated measures using Levene’s test. When significant effects were observed, we conducted a post-hoc Bonferroni test for pairwise comparisons. Results are summarized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F3" title="Figure 3 ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="651" id="S4.F3.sf1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F3.sf1.3.2" style="font-size:90%;">Objective understanding</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="598" id="S4.F3.sf2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F3.sf2.3.2" style="font-size:90%;">Subjective understanding</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="593" id="S4.F3.sf3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.F3.sf3.3.2" style="font-size:90%;">Curiosity</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_missing ltx_missing_image" id="S4.F3.sf4.g1" src=""/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S4.F3.sf4.3.2" style="font-size:90%;">Usability</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.10.5.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S4.F3.8.4" style="font-size:90%;">Results for quantitative study. Vertical bars are confidence intervals at <math alttext="95\%" class="ltx_Math" display="inline" id="S4.F3.5.1.m1.1"><semantics id="S4.F3.5.1.m1.1b"><mrow id="S4.F3.5.1.m1.1.1" xref="S4.F3.5.1.m1.1.1.cmml"><mn id="S4.F3.5.1.m1.1.1.2" xref="S4.F3.5.1.m1.1.1.2.cmml">95</mn><mo id="S4.F3.5.1.m1.1.1.1" xref="S4.F3.5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.5.1.m1.1c"><apply id="S4.F3.5.1.m1.1.1.cmml" xref="S4.F3.5.1.m1.1.1"><csymbol cd="latexml" id="S4.F3.5.1.m1.1.1.1.cmml" xref="S4.F3.5.1.m1.1.1.1">percent</csymbol><cn id="S4.F3.5.1.m1.1.1.2.cmml" type="integer" xref="S4.F3.5.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.1.m1.1d">95\%</annotation><annotation encoding="application/x-llamapun" id="S4.F3.5.1.m1.1e">95 %</annotation></semantics></math>. Significance levels are reported as follows: <math alttext="***=p\leq.001,**=p\leq.01,*=p\leq.05" class="ltx_math_unparsed" display="inline" id="S4.F3.6.2.m2.4"><semantics id="S4.F3.6.2.m2.4b"><mrow id="S4.F3.6.2.m2.4c"><mo id="S4.F3.6.2.m2.1.1" rspace="0em">∗</mo><mo id="S4.F3.6.2.m2.2.2" lspace="0em" rspace="0em">∗</mo><mo id="S4.F3.6.2.m2.3.3" lspace="0em" rspace="0em">∗</mo><mo id="S4.F3.6.2.m2.4.4" lspace="0em">=</mo><mi id="S4.F3.6.2.m2.4.5">p</mi><mo id="S4.F3.6.2.m2.4.6">≤</mo><mn id="S4.F3.6.2.m2.4.7">.001</mn><mo id="S4.F3.6.2.m2.4.8" rspace="0em">,</mo><mo id="S4.F3.6.2.m2.4.9" lspace="0em" rspace="0em">∗</mo><mo id="S4.F3.6.2.m2.4.10" lspace="0em" rspace="0em">∗</mo><mo id="S4.F3.6.2.m2.4.11" lspace="0em">=</mo><mi id="S4.F3.6.2.m2.4.12">p</mi><mo id="S4.F3.6.2.m2.4.13">≤</mo><mn id="S4.F3.6.2.m2.4.14">.01</mn><mo id="S4.F3.6.2.m2.4.15" rspace="0em">,</mo><mo id="S4.F3.6.2.m2.4.16" lspace="0em" rspace="0em">∗</mo><mo id="S4.F3.6.2.m2.4.17" lspace="0em">=</mo><mi id="S4.F3.6.2.m2.4.18">p</mi><mo id="S4.F3.6.2.m2.4.19">≤</mo><mn id="S4.F3.6.2.m2.4.20">.05</mn></mrow><annotation encoding="application/x-tex" id="S4.F3.6.2.m2.4d">***=p\leq.001,**=p\leq.01,*=p\leq.05</annotation><annotation encoding="application/x-llamapun" id="S4.F3.6.2.m2.4e">∗ ∗ ∗ = italic_p ≤ .001 , ∗ ∗ = italic_p ≤ .01 , ∗ = italic_p ≤ .05</annotation></semantics></math>, <math alttext="\cdot=p\leq.07" class="ltx_math_unparsed" display="inline" id="S4.F3.7.3.m3.2"><semantics id="S4.F3.7.3.m3.2b"><mrow id="S4.F3.7.3.m3.2c"><mo id="S4.F3.7.3.m3.1.1" rspace="0em">⋅</mo><mo id="S4.F3.7.3.m3.2.2" lspace="0em">=</mo><mi id="S4.F3.7.3.m3.2.3">p</mi><mo id="S4.F3.7.3.m3.2.4">≤</mo><mn id="S4.F3.7.3.m3.2.5">.07</mn></mrow><annotation encoding="application/x-tex" id="S4.F3.7.3.m3.2d">\cdot=p\leq.07</annotation><annotation encoding="application/x-llamapun" id="S4.F3.7.3.m3.2e">⋅ = italic_p ≤ .07</annotation></semantics></math> and n.s. non significant. <em class="ltx_emph ltx_font_italic" id="S4.F3.8.4.1">Reading key: Objective understanding (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F3.sf1" title="In Figure 3 ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">3(a)</span></a>) was statistically significantly higher (<math alttext="p\leq.05" class="ltx_Math" display="inline" id="S4.F3.8.4.1.m1.1"><semantics id="S4.F3.8.4.1.m1.1b"><mrow id="S4.F3.8.4.1.m1.1.1" xref="S4.F3.8.4.1.m1.1.1.cmml"><mi id="S4.F3.8.4.1.m1.1.1.2" xref="S4.F3.8.4.1.m1.1.1.2.cmml">p</mi><mo id="S4.F3.8.4.1.m1.1.1.1" xref="S4.F3.8.4.1.m1.1.1.1.cmml">≤</mo><mn id="S4.F3.8.4.1.m1.1.1.3" xref="S4.F3.8.4.1.m1.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.8.4.1.m1.1c"><apply id="S4.F3.8.4.1.m1.1.1.cmml" xref="S4.F3.8.4.1.m1.1.1"><leq id="S4.F3.8.4.1.m1.1.1.1.cmml" xref="S4.F3.8.4.1.m1.1.1.1"></leq><ci id="S4.F3.8.4.1.m1.1.1.2.cmml" xref="S4.F3.8.4.1.m1.1.1.2">𝑝</ci><cn id="S4.F3.8.4.1.m1.1.1.3.cmml" type="float" xref="S4.F3.8.4.1.m1.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.8.4.1.m1.1d">p\leq.05</annotation><annotation encoding="application/x-llamapun" id="S4.F3.8.4.1.m1.1e">italic_p ≤ .05</annotation></semantics></math>) when participants used textual explanation rather than graph explanation.</em></span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Text rather than graph design for higher objective understanding</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.4">We found a statistically significant main effect of explanation design on participants’ objective understanding (<math alttext="p=.043" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.1.m1.1"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml">.043</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><eq id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS1.p1.1.m1.1.1.3">.043</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">p=.043</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.1.m1.1d">italic_p = .043</annotation></semantics></math>). Post-hoc analysis with a Bonferroni adjustment revealed that text-based design led to significantly higher objective understanding than graph-based design (<math alttext="p=.046" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.2.m2.1"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mrow id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS1.p1.2.m2.1.1.2" xref="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.p1.2.m2.1.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS1.p1.2.m2.1.1.3" xref="S4.SS2.SSS1.p1.2.m2.1.1.3.cmml">.046</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><apply id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1"><eq id="S4.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.2">𝑝</ci><cn id="S4.SS2.SSS1.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS2.SSS1.p1.2.m2.1.1.3">.046</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">p=.046</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.2.m2.1d">italic_p = .046</annotation></semantics></math>) (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F3.sf1" title="In Figure 3 ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">3(a)</span></a>), but that there was no statistically significant difference for the levels of objective understanding between graph and SHAP-based designs, nor between text and SHAP-based designs. The main effect of the expertise level on objective understanding was not statistically significant (<math alttext="p=.424" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.3.m3.1"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mrow id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.1.1.2" xref="S4.SS2.SSS1.p1.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.p1.3.m3.1.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS1.p1.3.m3.1.1.3" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.cmml">.424</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><apply id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1"><eq id="S4.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.2">𝑝</ci><cn id="S4.SS2.SSS1.p1.3.m3.1.1.3.cmml" type="float" xref="S4.SS2.SSS1.p1.3.m3.1.1.3">.424</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">p=.424</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.3.m3.1d">italic_p = .424</annotation></semantics></math>), <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.4.1">i.e.</em>, if we ignore the explanation design being evaluated, <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.4.2">Experts, Insiders</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.4.3">non-Experts</em> gave similar ratings considering the objective understanding measure. However, within the <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.4.4">Expert</em> group, objective understanding was statistically significantly higher for text-based design than for graph-based design (<math alttext="p=.022" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.4.m4.1"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><mrow id="S4.SS2.SSS1.p1.4.m4.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS1.p1.4.m4.1.1.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.p1.4.m4.1.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS1.p1.4.m4.1.1.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml">.022</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m4.1b"><apply id="S4.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1"><eq id="S4.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.1"></eq><ci id="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.2">𝑝</ci><cn id="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml" type="float" xref="S4.SS2.SSS1.p1.4.m4.1.1.3">.022</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m4.1c">p=.022</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.4.m4.1d">italic_p = .022</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Graph and text designs increase usability. </h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.8">There was a statistically significant main effect of the explanation design on usability (<math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.1.m1.1"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><lt id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.1.m1.1d">italic_p &lt; .001</annotation></semantics></math>), but no statistically significant influence of the level of expertise on this measure (<math alttext="p=.398" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.2.m2.1"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.2.m2.1.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS2.p1.2.m2.1.1.3" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml">.398</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"><eq id="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.2.m2.1.1.3">.398</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">p=.398</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.2.m2.1d">italic_p = .398</annotation></semantics></math>). Post-hoc tests with a Bonferroni adjustment revealed that SHAP-based design led to statistically significantly lower usability compared to graph (<math alttext="p=.043" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.3.m3.1"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mrow id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS2.p1.3.m3.1.1.2" xref="S4.SS2.SSS2.p1.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.3.m3.1.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS2.p1.3.m3.1.1.3" xref="S4.SS2.SSS2.p1.3.m3.1.1.3.cmml">.043</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><apply id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1"><eq id="S4.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.3.m3.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.3.m3.1.1.3">.043</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">p=.043</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.3.m3.1d">italic_p = .043</annotation></semantics></math>) or text-based (<math alttext="p&lt;.001" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.4.m4.1"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><mrow id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS2.p1.4.m4.1.1.2" xref="S4.SS2.SSS2.p1.4.m4.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.4.m4.1.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS2.p1.4.m4.1.1.3" xref="S4.SS2.SSS2.p1.4.m4.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><apply id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1"><lt id="S4.SS2.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.1"></lt><ci id="S4.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.4.m4.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.4.m4.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">p&lt;.001</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.4.m4.1d">italic_p &lt; .001</annotation></semantics></math>) designs (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F3.sf4" title="In Figure 3 ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">3(d)</span></a>). However, we did not find any statistically significant difference between graph and text-based designs (<math alttext="p=.418" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.5.m5.1"><semantics id="S4.SS2.SSS2.p1.5.m5.1a"><mrow id="S4.SS2.SSS2.p1.5.m5.1.1" xref="S4.SS2.SSS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.SSS2.p1.5.m5.1.1.2" xref="S4.SS2.SSS2.p1.5.m5.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.5.m5.1.1.1" xref="S4.SS2.SSS2.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS2.p1.5.m5.1.1.3" xref="S4.SS2.SSS2.p1.5.m5.1.1.3.cmml">.418</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.5.m5.1b"><apply id="S4.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.1"><eq id="S4.SS2.SSS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.1.1"></eq><ci id="S4.SS2.SSS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.5.m5.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.5.m5.1.1.3">.418</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.5.m5.1c">p=.418</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.5.m5.1d">italic_p = .418</annotation></semantics></math>). Within expertise levels, SHAP-based design led to statistically significantly lower level of usability than text-based design, for both <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.8.1">Insiders</em> (<math alttext="p=.007" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.6.m6.1"><semantics id="S4.SS2.SSS2.p1.6.m6.1a"><mrow id="S4.SS2.SSS2.p1.6.m6.1.1" xref="S4.SS2.SSS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.SSS2.p1.6.m6.1.1.2" xref="S4.SS2.SSS2.p1.6.m6.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.6.m6.1.1.1" xref="S4.SS2.SSS2.p1.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS2.p1.6.m6.1.1.3" xref="S4.SS2.SSS2.p1.6.m6.1.1.3.cmml">.007</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.6.m6.1b"><apply id="S4.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS2.SSS2.p1.6.m6.1.1"><eq id="S4.SS2.SSS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.SSS2.p1.6.m6.1.1.1"></eq><ci id="S4.SS2.SSS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.SSS2.p1.6.m6.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.6.m6.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.6.m6.1.1.3">.007</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.6.m6.1c">p=.007</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.6.m6.1d">italic_p = .007</annotation></semantics></math>) and <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.8.2">Experts</em> (<math alttext="p=.006" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.7.m7.1"><semantics id="S4.SS2.SSS2.p1.7.m7.1a"><mrow id="S4.SS2.SSS2.p1.7.m7.1.1" xref="S4.SS2.SSS2.p1.7.m7.1.1.cmml"><mi id="S4.SS2.SSS2.p1.7.m7.1.1.2" xref="S4.SS2.SSS2.p1.7.m7.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.7.m7.1.1.1" xref="S4.SS2.SSS2.p1.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS2.p1.7.m7.1.1.3" xref="S4.SS2.SSS2.p1.7.m7.1.1.3.cmml">.006</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.7.m7.1b"><apply id="S4.SS2.SSS2.p1.7.m7.1.1.cmml" xref="S4.SS2.SSS2.p1.7.m7.1.1"><eq id="S4.SS2.SSS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.SSS2.p1.7.m7.1.1.1"></eq><ci id="S4.SS2.SSS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.SSS2.p1.7.m7.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.7.m7.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.7.m7.1.1.3">.006</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.7.m7.1c">p=.006</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.7.m7.1d">italic_p = .006</annotation></semantics></math>). Among <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.8.3">Experts</em>, usability was also statistically significantly higher for text-based design than for graph-based design (<math alttext="p=.008" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.8.m8.1"><semantics id="S4.SS2.SSS2.p1.8.m8.1a"><mrow id="S4.SS2.SSS2.p1.8.m8.1.1" xref="S4.SS2.SSS2.p1.8.m8.1.1.cmml"><mi id="S4.SS2.SSS2.p1.8.m8.1.1.2" xref="S4.SS2.SSS2.p1.8.m8.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS2.p1.8.m8.1.1.1" xref="S4.SS2.SSS2.p1.8.m8.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS2.p1.8.m8.1.1.3" xref="S4.SS2.SSS2.p1.8.m8.1.1.3.cmml">.008</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.8.m8.1b"><apply id="S4.SS2.SSS2.p1.8.m8.1.1.cmml" xref="S4.SS2.SSS2.p1.8.m8.1.1"><eq id="S4.SS2.SSS2.p1.8.m8.1.1.1.cmml" xref="S4.SS2.SSS2.p1.8.m8.1.1.1"></eq><ci id="S4.SS2.SSS2.p1.8.m8.1.1.2.cmml" xref="S4.SS2.SSS2.p1.8.m8.1.1.2">𝑝</ci><cn id="S4.SS2.SSS2.p1.8.m8.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p1.8.m8.1.1.3">.008</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.8.m8.1c">p=.008</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.8.m8.1d">italic_p = .008</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Higher expertise increases curiosity for graph and text designs.</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.5">Our analysis determined that there was no statistically significant effect of explanation design on participants’ curiosity (<math alttext="p=.514" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.1.m1.1"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><mrow id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.p1.1.m1.1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.p1.1.m1.1.1.3.cmml">.514</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><apply id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1"><eq id="S4.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS3.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS3.p1.1.m1.1.1.3">.514</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">p=.514</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.1.m1.1d">italic_p = .514</annotation></semantics></math>). However, the expertise level had a statistically significant influence on curiosity (<math alttext="p=.016" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.2.m2.1"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mrow id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.p1.2.m2.1.1.2" xref="S4.SS2.SSS3.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.p1.2.m2.1.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.p1.2.m2.1.1.3" xref="S4.SS2.SSS3.p1.2.m2.1.1.3.cmml">.016</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><apply id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1"><eq id="S4.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.2">𝑝</ci><cn id="S4.SS2.SSS3.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS2.SSS3.p1.2.m2.1.1.3">.016</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">p=.016</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.2.m2.1d">italic_p = .016</annotation></semantics></math>). Post-hoc tests with a Bonferroni adjustment revealed that being <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.5.1">Insider</em> led to significantly lower levels of curiosity compared to being <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.5.2">Expert</em> (<math alttext="p=.02" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.3.m3.1"><semantics id="S4.SS2.SSS3.p1.3.m3.1a"><mrow id="S4.SS2.SSS3.p1.3.m3.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS3.p1.3.m3.1.1.2" xref="S4.SS2.SSS3.p1.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.p1.3.m3.1.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.p1.3.m3.1.1.3" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.cmml">.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.3.m3.1b"><apply id="S4.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1"><eq id="S4.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.2">𝑝</ci><cn id="S4.SS2.SSS3.p1.3.m3.1.1.3.cmml" type="float" xref="S4.SS2.SSS3.p1.3.m3.1.1.3">.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.3.m3.1c">p=.02</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.3.m3.1d">italic_p = .02</annotation></semantics></math>). Specifically, when faced with graph-based designs, <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.5.3">Experts</em> showed statistically significantly higher levels of curiosity than <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.5.4">Insiders</em> (<math alttext="p=.041" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.4.m4.1"><semantics id="S4.SS2.SSS3.p1.4.m4.1a"><mrow id="S4.SS2.SSS3.p1.4.m4.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS3.p1.4.m4.1.1.2" xref="S4.SS2.SSS3.p1.4.m4.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.p1.4.m4.1.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.p1.4.m4.1.1.3" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.cmml">.041</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.4.m4.1b"><apply id="S4.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1"><eq id="S4.SS2.SSS3.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1.1"></eq><ci id="S4.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1.2">𝑝</ci><cn id="S4.SS2.SSS3.p1.4.m4.1.1.3.cmml" type="float" xref="S4.SS2.SSS3.p1.4.m4.1.1.3">.041</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.4.m4.1c">p=.041</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.4.m4.1d">italic_p = .041</annotation></semantics></math>). Similarly, when faced with text-based designs, <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.5.5">Experts</em> were significantly more curious than <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.5.6">Insiders</em> (<math alttext="p=.032" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.5.m5.1"><semantics id="S4.SS2.SSS3.p1.5.m5.1a"><mrow id="S4.SS2.SSS3.p1.5.m5.1.1" xref="S4.SS2.SSS3.p1.5.m5.1.1.cmml"><mi id="S4.SS2.SSS3.p1.5.m5.1.1.2" xref="S4.SS2.SSS3.p1.5.m5.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.p1.5.m5.1.1.1" xref="S4.SS2.SSS3.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.p1.5.m5.1.1.3" xref="S4.SS2.SSS3.p1.5.m5.1.1.3.cmml">.032</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.5.m5.1b"><apply id="S4.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS3.p1.5.m5.1.1"><eq id="S4.SS2.SSS3.p1.5.m5.1.1.1.cmml" xref="S4.SS2.SSS3.p1.5.m5.1.1.1"></eq><ci id="S4.SS2.SSS3.p1.5.m5.1.1.2.cmml" xref="S4.SS2.SSS3.p1.5.m5.1.1.2">𝑝</ci><cn id="S4.SS2.SSS3.p1.5.m5.1.1.3.cmml" type="float" xref="S4.SS2.SSS3.p1.5.m5.1.1.3">.032</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.5.m5.1c">p=.032</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.5.m5.1d">italic_p = .032</annotation></semantics></math>) (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F3.sf3" title="In Figure 3 ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">3(c)</span></a>). However, we did not find any statistically significant difference in curiosity levels between <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.5.7">non-Experts</em> and other users. Furthermore, we did not find any statistically significant difference in ratings between participants when using SHAP-based design.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>No effect of explanation design or expertise level on subjective understanding</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.2">We did not observe any statistically significant influence of explanation design on participants’ subjective understanding (<math alttext="p=.104" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.1.m1.1"><semantics id="S4.SS2.SSS4.p1.1.m1.1a"><mrow id="S4.SS2.SSS4.p1.1.m1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS4.p1.1.m1.1.1.2" xref="S4.SS2.SSS4.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS4.p1.1.m1.1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS4.p1.1.m1.1.1.3" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.cmml">.104</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.1.m1.1b"><apply id="S4.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1"><eq id="S4.SS2.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS4.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS4.p1.1.m1.1.1.3">.104</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.1.m1.1c">p=.104</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.1.m1.1d">italic_p = .104</annotation></semantics></math>). Similarly, the expertise level of participants did not have any statistically significant influence on their subjective understanding (<math alttext="p=.185" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.2.m2.1"><semantics id="S4.SS2.SSS4.p1.2.m2.1a"><mrow id="S4.SS2.SSS4.p1.2.m2.1.1" xref="S4.SS2.SSS4.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS4.p1.2.m2.1.1.2" xref="S4.SS2.SSS4.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS4.p1.2.m2.1.1.1" xref="S4.SS2.SSS4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS4.p1.2.m2.1.1.3" xref="S4.SS2.SSS4.p1.2.m2.1.1.3.cmml">.185</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.2.m2.1b"><apply id="S4.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1"><eq id="S4.SS2.SSS4.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.SSS4.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1.2">𝑝</ci><cn id="S4.SS2.SSS4.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS2.SSS4.p1.2.m2.1.1.3">.185</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.2.m2.1c">p=.185</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.2.m2.1d">italic_p = .185</annotation></semantics></math>) (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F3.sf2" title="In Figure 3 ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">3(b)</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5. </span>Graph-based design is preferred, regardless of the expertise level</h4>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.2">At the end of the questionnaire, participants were asked to rank explanation designs according to their preferences. A Chi-Square Goodness of Fit Test was performed to determine whether the design preferences were equally distributed across the 6 possible outcomes. Our results revealed that the obtained proportions significantly differed (<math alttext="p=.015" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p1.1.m1.1"><semantics id="S4.SS2.SSS5.p1.1.m1.1a"><mrow id="S4.SS2.SSS5.p1.1.m1.1.1" xref="S4.SS2.SSS5.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS5.p1.1.m1.1.1.2" xref="S4.SS2.SSS5.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS5.p1.1.m1.1.1.1" xref="S4.SS2.SSS5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS5.p1.1.m1.1.1.3" xref="S4.SS2.SSS5.p1.1.m1.1.1.3.cmml">.015</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p1.1.m1.1b"><apply id="S4.SS2.SSS5.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p1.1.m1.1.1"><eq id="S4.SS2.SSS5.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS5.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS5.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS5.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS5.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS5.p1.1.m1.1.1.3">.015</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p1.1.m1.1c">p=.015</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p1.1.m1.1d">italic_p = .015</annotation></semantics></math>). We display in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F4" title="Figure 4 ‣ 4.2.5. Graph-based design is preferred, regardless of the expertise level ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">4</span></a> the number of occurrences of each design at each ranking position. We show that participants preferred graph-based design more frequently than other designs. When graph-based design was not ranked in first position, it was ranked in second position more often than in third position. The second best-ranked design was text-based explanation. SHAP-based explanation was ranked in last position more than 35 times. Finally, we explored the relationship between participants’ expertise level and design preference. We ran a Chi-Square test and did not find any statistically significant evidence of correlation between the two variables (<math alttext="p=.841" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p1.2.m2.1"><semantics id="S4.SS2.SSS5.p1.2.m2.1a"><mrow id="S4.SS2.SSS5.p1.2.m2.1.1" xref="S4.SS2.SSS5.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS5.p1.2.m2.1.1.2" xref="S4.SS2.SSS5.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS5.p1.2.m2.1.1.1" xref="S4.SS2.SSS5.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS5.p1.2.m2.1.1.3" xref="S4.SS2.SSS5.p1.2.m2.1.1.3.cmml">.841</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p1.2.m2.1b"><apply id="S4.SS2.SSS5.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p1.2.m2.1.1"><eq id="S4.SS2.SSS5.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS5.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.SSS5.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS5.p1.2.m2.1.1.2">𝑝</ci><cn id="S4.SS2.SSS5.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS2.SSS5.p1.2.m2.1.1.3">.841</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p1.2.m2.1c">p=.841</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p1.2.m2.1d">italic_p = .841</annotation></semantics></math>).</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S4.F4.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S4.F4.4.2" style="font-size:90%;">Number of occurrences of each design at each ranking position. <em class="ltx_emph ltx_font_italic" id="S4.F4.4.2.1">Reading key: Graph-based design has been ranked 29 times in #1 position, 24 times in #2 position and 13 times in #3 position.</em></span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Graph vs. textual explanations</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">According to Miller <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib32" title="">2019</a>)</cite>, a “good” explanation should be <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.1">contrastive, selected, dialogic</em> and <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.2">causal</em>. We argued that graphs inherently model causal and contrastive relationships between entities through their node-link structure. They achieve the <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.3">selected</em> criterion by potentially representing a subset of the original data, <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.4">e.g.</em> in the vicinity of a recommender system’s prediction. Finally, the node or link attributes they may possess contribute to them being considered dialogic. Consequently, we aligned with previous studies that acknowledged the strong expressive power of graphs and the fact that they were especially well-suited for applications like recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Afchar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib2" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Initial findings from our qualitative study affirmed these statements and emphasized the potential effectiveness of graph-based explanations. With the exception of one <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.1">Expert</em> who preferred textual explanations, participants found graph-based design <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.2">“very easily interpretable”</em> and highlighted the importance of showcasing <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.3">“relations between groups”</em>. This was further confirmed quantitatively; when asked for their preferred explanation type, the majority of participants in our second study favored graph-based explanation over text or SHAP designs (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#S4.F4" title="Figure 4 ‣ 4.2.5. Graph-based design is preferred, regardless of the expertise level ‣ 4.2. Results ‣ 4. Study 2: Influence of explanation design ‣ Evaluating graph-based explanations for AI-based recommender systems"><span class="ltx_text ltx_ref_tag">4</span></a>). Specifically, users expressed a preference for item-centric graph explanations over user-centric ones, aligning with results from Kouki <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.4">et al.</em>’s work <cite class="ltx_cite ltx_citemacro_citep">(Kouki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib25" title="">2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib26" title="">2019</a>)</cite>, which compared textual and visual explanations, although not specifically graphs.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">However, despite their expressed preferences for graph-based design, our second study revealed that participants had significantly higher levels of objective understanding and usability when using textual designs. Such contrast between expressed desiderata and measured performance aligns with previous research <cite class="ltx_cite ltx_citemacro_citep">(Szymanski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib49" title="">2021</a>; Buçinca et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib6" title="">2021</a>)</cite> where authors emphasized users’ preference for visual explanations (though not specifically graphs) over texts, despite their poorer performance when using the former kind. A plausible explanation for this tendency may be grounded in the findings of a recent perception study <cite class="ltx_cite ltx_citemacro_citep">(Muth et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib34" title="">2015</a>)</cite> in which authors showed that interest and attractiveness of a stimuli can be well predicted by the complexity of this stimuli; in our case, the visual complexity of our graph explanation, compared to textual explanation could encourage participants to prefer this design over others. More recent research <cite class="ltx_cite ltx_citemacro_citep">(Carbon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib8" title="">2018</a>)</cite> explored this direction in the context of graph designs and further confirmed the close relationship between complex visual structure and interest.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Overall, our findings suggest that depending solely on stakeholders’ expressed desiderata for crafting graph-based explanation designs may not be sufficient. Considering both their preferences and measured performance when using such explanations could provide a more comprehensive understanding of what constitutes a good explanation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Impact of expertise level</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Despite its enhanced design tailored to the needs of users with various levels of expertise, our graph explanation did not lead to significantly higher ratings on the different measures. Nevertheless, noteworthy insights can be derived from this visual design. For example, we showed that high expertise level positively influenced participants’ objective understanding when facing textual explanation vs. graph explanation, but we did not reveal any difference in ratings for <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.1">non-Experts</em>. This contrasts with the findings in <cite class="ltx_cite ltx_citemacro_citep">(Szymanski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib49" title="">2021</a>)</cite> showing how lay users performed better with textual explanations vs. visual explanations. While authors evoke the plausible effects of <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.2">confirmation bias, i.e.</em> favoring elements confirming preconceived ideas, on lay users to explain their results, we did not notice such behavior in our qualitative study. An interpretation of our results could be that <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.3">Experts</em> may be more inclined than <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.4">non-Experts</em> to have preconceived notions, or <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.5">false narratives</em> about the internal functioning of the system, a behavior that has been particularly observed for experienced users by previous works <cite class="ltx_cite ltx_citemacro_citep">(Tversky and Kahneman, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib52" title="">1974</a>; Kaur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib23" title="">2020</a>)</cite>. These <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.6">a priori</em> may be more contradicted by complex designs, such as SHAP or graph, compared to relatively vague explanations like the textual design we proposed.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">We also demonstrated higher curiosity levels for <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.1">Experts</em> compared to <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.2">Insiders</em>, when using graph and text designs, which aligns with previous research <cite class="ltx_cite ltx_citemacro_citep">(Niu and Al-Doulat, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib36" title="">2021</a>)</cite> stating that curiosity is strongly associated with intrinsic interest and motivation. We hypothesize that <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.3">Expert</em> participants are more engaged in the proposed tasks due to their interest in the topic. However, this tendency is not observed with SHAP-based explanation, warranting further analysis for comprehensive understanding.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Our analysis finally revealed that both <em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.1">Insiders</em> and <em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.2">Experts</em> considered text-based design more usable than SHAP-based design, but only the most experienced participants found text-based explanations more usable than graph-based ones. This could be attributed to users with moderate expertise being more persuadable by graphical design and experiencing some kind of <em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.3">over-trust</em> <cite class="ltx_cite ltx_citemacro_citep">(Eiband et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib12" title="">2019</a>)</cite>. Or it may be the result of <em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.4">Experts</em> being more inclined to over-analyze graph explanations to align with their knowledge of recommender systems and network layouts. Additionally, we did not find any statistically significant difference between levels of expertise when measuring graph usability. This observation aligns with previous research in the biological field <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib47" title="">2018</a>)</cite>, showing that novice users could create and evaluate graph layouts as effectively as domain experts.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">In summary, we highlighted how graph-based explanations were subject to different cognitive biases according to the epxertise level of the end-users. To address this limit, future work could involve further inclusion of <em class="ltx_emph ltx_font_italic" id="S5.SS2.p4.1.1">cognitive</em> factors <cite class="ltx_cite ltx_citemacro_citep">(Shin and Kim, <a class="ltx_ref" href="https://arxiv.org/html/2407.12357v1#bib.bib46" title="">2019</a>)</cite> in explanations to further enhance perceived designs.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Limitations</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In our quantitative study, we compelled participants to choose from pre-defined user profiles based on a selection of representative books. This action further determined the algorithm recommendation and consequently the content of each explanation. While this approach allows us to use pre-computed answers, which significantly simplifies the questionnaire procedure, we acknowledge that it might hinder participants from fully recognizing their tastes within the recommender system’s choices, potentially influencing their design preferences. To address this concern, we verified that the user profile chosen by participants was not correlated with their design preferences. For this purpose, we ran a Chi-Square test between the two variables and the results revealed that there is not enough evidence to suggest an association between the selected user profile and design preferences (<math alttext="p=.549" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">.549</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><eq id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></eq><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝑝</ci><cn id="S5.SS3.p1.1.m1.1.1.3.cmml" type="float" xref="S5.SS3.p1.1.m1.1.1.3">.549</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">p=.549</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_p = .549</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">In this work, we did not impose any time constraint on participants during their analysis of design, and the only time indication provided was that the questionnaire should take around 10 minutes to complete. Consequently, participants had sufficient time to explore and analyze each recommendation, which may not entirely replicate a real-world scenario where end users require an explanation to make a decision. Future research aimed at assessing the generalizability of our findings could involve time-constrained tasks tailored for specific contexts.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">More generally, the results presented in this work are derived from data related to book recommendations. This simplistic use case facilitated the building of the study since it mitigated potential ethical concerns. While our results may have applicability in other domains, additional experiments would be necessary to confirm this.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we have conducted a qualitative study to understand the needs for graph-based explanation designs expressed by users with various levels of expertise. From these results, we built an enhanced graph-based design emphasizing several key recommendations such as a content-based approach obtained through bipartite graph projection or the focus on item similarity rather than user-similarity. We then conducted a quantitative study in which we investigated the influence of SHAP, textual and graph-based explanation designs regarding four constructs: objective understanding, subjective understanding, curiosity and usability. Our results revealed that text-based explanations significantly improved objective understanding to graph-based explanation, and that it was specifically marked for users with a higher level of expertise. On the other hand, we did not find any statistically significant evidence of influence of explanation design on subjective understanding. We also showed that both graph and text explanations were considered more usable than SHAP-based design, and that it was particularly true for users with middle to high level of expertise. Moreover, we highlighted the influence of expertise level on curiosity by showing how expert users were more curious than insiders toward graph and textual explanations. Lastly, we emphasized the discrepancy between participants’ expressed preferences for graph-based explanations during qualitative interviews and further confirmed quantitatively, and their higher ratings regarding understanding or usability for textual designs. This outcome suggests that solely fulfilling stakeholders’ desiderata may not suffice to achieve “good” explanations. Crafting hybrid designs achieving a balance between social expectation and effective downstream performance emerges as a significant challenge.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Ethical concerns</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">To ensure participants’ privacy, the qualitative study was designed and approved in collaboration with the Data Protection Officer (DPO) of the authors’ affiliated school. Anonymized versions of interview notes for data analysis are securely stored, with access restricted to the authors. We did not collected any personal information for the quantitative study.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afchar et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Darius Afchar, Alessandro
Melchiorre, Markus Schedl, Romain
Hennequin, Elena Epure, and Manuel
Moussallam. 2022.

</span>
<span class="ltx_bibblock">Explainability in music recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">AI Magazine</em> 43,
2 (2022), 190–208.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alabi et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Rasheed Omobolaji Alabi,
Alhadi Almangush, Mohammed Elmusrati,
Ilmo Leivo, and Antti Mäkitie.
2022.

</span>
<span class="ltx_bibblock">Measuring the usability and quality of explanations
of a machine learning web-based tool for Oral Tongue Cancer Prognostication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">International Journal of Environmental
Research and Public Health</em> 19, 14
(2022), 8366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alper et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Basak Alper, Benjamin
Bach, Nathalie Henry Riche, Tobias
Isenberg, and Jean-Daniel Fekete.
2013.

</span>
<span class="ltx_bibblock">Weighted graph comparison techniques for brain
connectivity analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems</em>.
ACM, 483–492.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertrand et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Astrid Bertrand, James R
Eagan, and Winston Maxwell.
2023.

</span>
<span class="ltx_bibblock">Questioning the ability of feature-based
explanations to empower non-experts in robo-advised financial
decision-making. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 2023 ACM
Conference on Fairness, Accountability, and Transparency</em>.
943–958.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buçinca et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zana Buçinca,
Maja Barbara Malaya, and Krzysztof Z
Gajos. 2021.

</span>
<span class="ltx_bibblock">To trust or to think: cognitive forcing functions
can reduce overreliance on AI in AI-assisted decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the ACM on Human-Computer
Interaction</em> 5, CSCW1
(2021), 1–21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burch et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Michael Burch, Weidong
Huang, Mathew Wakefield, Helen C
Purchase, Daniel Weiskopf, and Jie
Hua. 2020.

</span>
<span class="ltx_bibblock">The state of the art in empirical user evaluation
of graph visualizations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">IEEE Access</em> 9
(2020), 4173–4198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carbon et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Claus-Christian Carbon,
Tamara Mchedlidze, Marius Hans Raab,
and Hannes Wächter. 2018.

</span>
<span class="ltx_bibblock">The power of shape: How shape of node-link diagrams
impacts aesthetic appreciation and triggers interest.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">i-Perception</em> 9,
5 (2018),
2041669518796851.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Xu Chen, Hongteng Xu,
Yongfeng Zhang, Jiaxi Tang,
Yixin Cao, Zheng Qin, and
Hongyuan Zha. 2018.

</span>
<span class="ltx_bibblock">Sequential recommendation with user memory
networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the eleventh ACM
international conference on web search and data mining</em>.
108–116.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Giacomo et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Emilio Di Giacomo, Walter
Didimo, Fabrizio Montecchiani, and
Alessandra Tappini. 2021.

</span>
<span class="ltx_bibblock">A user study on hybrid graph visualizations. In
<em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">International Symposium on Graph Drawing and
Network Visualization</em>. Springer, 21–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doshi-Velez and Kim (2018)</span>
<span class="ltx_bibblock">
Finale Doshi-Velez and
Been Kim. 2018.

</span>
<span class="ltx_bibblock">Considerations for evaluation and generalization in
interpretable machine learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Explainable and interpretable models in
computer vision and machine learning</em> (2018),
3–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eiband et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Malin Eiband, Daniel
Buschek, Alexander Kremer, and Heinrich
Hussmann. 2019.

</span>
<span class="ltx_bibblock">The impact of placebic explanations on trust in
intelligent systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Extended abstracts of the
2019 CHI conference on human factors in computing systems</em>.
1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elo and Kyngäs (2008)</span>
<span class="ltx_bibblock">
Satu Elo and Helvi
Kyngäs. 2008.

</span>
<span class="ltx_bibblock">The qualitative content analysis process.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Journal of advanced nursing</em>
62, 1 (2008),
107–115.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Euler (1741)</span>
<span class="ltx_bibblock">
Leonhard Euler.
1741.

</span>
<span class="ltx_bibblock">Solutio problematis ad geometriam situs
pertinentis. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">MAA Euler Archive</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Commission (2021)</span>
<span class="ltx_bibblock">
European Commission.
21/04/2021.

</span>
<span class="ltx_bibblock">Proposal for a REGULATION OF THE EUROPEAN
PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL
INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION
LEGISLATIVE ACTS.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">COM/2021/206 final</em>
(21/04/2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghazimatin et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Azin Ghazimatin, Oana
Balalau, Rishiraj Saha Roy, and Gerhard
Weikum. 2020.

</span>
<span class="ltx_bibblock">PRINCE: Provider-side Interpretability with
Counterfactual Explanations in Recommender Systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the 13th International Conference on
Web Search and Data Mining</em>. ACM,
196–204.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghoniem et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Mohammad Ghoniem, J-D
Fekete, and Philippe Castagliola.
2004.

</span>
<span class="ltx_bibblock">A comparison of the readability of graphs using
node-link and matrix-based representations. In
<em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">IEEE symposium on information visualization</em>.
IEEE, 17–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghoniem et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Mohammad Ghoniem,
Jean-Daniel Fekete, and Philippe
Castagliola. 2005.

</span>
<span class="ltx_bibblock">On the readability of graphs using node-link and
matrix-based representations: a controlled experiment and statistical
analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Information visualization</em>
4, 2 (2005),
114–135.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herlocker et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2000)</span>
<span class="ltx_bibblock">
Jonathan L. Herlocker,
Joseph A. Konstan, and John Riedl.
2000.

</span>
<span class="ltx_bibblock">Explaining collaborative filtering
recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 2000 ACM
conference on Computer supported cooperative work</em>.
ACM, 241–250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herman et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2000)</span>
<span class="ltx_bibblock">
Ivan Herman, Guy
Melançon, and M Scott Marshall.
2000.

</span>
<span class="ltx_bibblock">Graph visualization and navigation in information
visualization: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">IEEE Transactions on visualization and
computer graphics</em> 6, 1
(2000), 24–43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hilton (1990)</span>
<span class="ltx_bibblock">
Denis J Hilton.
1990.

</span>
<span class="ltx_bibblock">Conversational processes and causal explanation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Psychological Bulletin</em>
107, 1 (1990),
65.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffman et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Robert R Hoffman, Shane T
Mueller, Gary Klein, and Jordan
Litman. 2018.

</span>
<span class="ltx_bibblock">Metrics for explainable AI: Challenges and
prospects.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:1812.04608</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaur et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Harmanpreet Kaur, Harsha
Nori, Samuel Jenkins, Rich Caruana,
Hanna Wallach, and Jennifer
Wortman Vaughan. 2020.

</span>
<span class="ltx_bibblock">Interpreting interpretability: understanding data
scientists’ use of interpretability tools for machine learning. In
<em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 2020 CHI conference on human
factors in computing systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keller et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
René Keller, Claudia M
Eckert, and P John Clarkson.
2006.

</span>
<span class="ltx_bibblock">Matrices or node-link diagrams: which visual
representation is better for visualising connectivity models?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Information Visualization</em>
5, 1 (2006),
62–76.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kouki et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Pigi Kouki, James
Schaffer, Jay Pujara, John O’Donovan,
and Lise Getoor. 2017.

</span>
<span class="ltx_bibblock">User Preferences for Hybrid Explanations. In
<em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the Eleventh ACM Conference on
Recommender Systems</em>. ACM, 84–88.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kouki et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Pigi Kouki, James
Schaffer, Jay Pujara, John O’Donovan,
and Lise Getoor. 2019.

</span>
<span class="ltx_bibblock">Personalized explanations for hybrid recommender
systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 24th International
Conference on Intelligent User Interfaces</em>. 379–390.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Langer et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Markus Langer, Daniel
Oster, Timo Speith, Holger Hermanns,
Lena Kästner, Eva Schmidt,
Andreas Sesing, and Kevin Baum.
2021.

</span>
<span class="ltx_bibblock">What Do We Want From Explainable Artificial
Intelligence (XAI)?– A Stakeholder Perspective on XAI and a Conceptual Model
Guiding Interdisciplinary XAI Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Artificial Intelligence</em>
296 (2021), 103473.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipton (2018)</span>
<span class="ltx_bibblock">
Zachary C Lipton.
2018.

</span>
<span class="ltx_bibblock">The Mythos of Model Interpretability: In machine
learning, the concept of interpretability is both important and slippery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Queue</em> 16,
3 (2018), 31–57.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lundberg and Lee (2017)</span>
<span class="ltx_bibblock">
Scott M Lundberg and
Su-In Lee. 2017.

</span>
<span class="ltx_bibblock">A unified approach to interpreting model
predictions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McBride and Caldara (2013)</span>
<span class="ltx_bibblock">
Michael McBride and
Michael Caldara. 2013.

</span>
<span class="ltx_bibblock">The efficacy of tables versus graphs in disrupting
dark networks: An experimental study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Social Networks</em> 35,
3 (2013), 406–422.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McDonald (2013)</span>
<span class="ltx_bibblock">
Roderick P McDonald.
2013.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Test theory: A unified treatment</em>.

</span>
<span class="ltx_bibblock">psychology press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (2019)</span>
<span class="ltx_bibblock">
Tim Miller.
2019.

</span>
<span class="ltx_bibblock">Explanation in artificial intelligence: Insights
from the social sciences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Artificial intelligence</em>
267 (2019), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Tim Miller, Piers Howe,
and Liz Sonenberg. 2017.

</span>
<span class="ltx_bibblock">Explainable AI: Beware of inmates running the
asylum or: How I learnt to stop worrying and love the social and behavioural
sciences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:1712.00547</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muth et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Claudia Muth, Marius H
Raab, and Claus-Christian Carbon.
2015.

</span>
<span class="ltx_bibblock">The stream of experience when watching artistic
movies. Dynamic aesthetic effects revealed by the Continuous Evaluation
Procedure (CEP).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Frontiers in Psychology</em>
6 (2015), 365.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nauta et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Meike Nauta, Jan Trienes,
Shreyasi Pathak, Elisa Nguyen,
Michelle Peters, Yasmin Schmitt,
Jörg Schlötterer, Maurice van
Keulen, and Christin Seifert.
2023.

</span>
<span class="ltx_bibblock">From anecdotal evidence to quantitative evaluation
methods: A systematic review on evaluating explainable ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Comput. Surveys</em> 55,
13s (2023), 1–42.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu and Al-Doulat (2021)</span>
<span class="ltx_bibblock">
Xi Niu and Ahmad
Al-Doulat. 2021.

</span>
<span class="ltx_bibblock">LuckyFind: Leveraging surprise to improve user
satisfaction and inspire curiosity in a recommender system. In
<em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2021 Conference on Human
Information Interaction and Retrieval</em>. 163–172.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Donovan et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
John O’Donovan, Barry
Smyth, Brynjar Gretarsson, Svetlin
Bostandjiev, and Tobias Höllerer.
2008.

</span>
<span class="ltx_bibblock">PeerChooser: visual interactive recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems</em>. 1085–1088.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Okoe and Jianu (2015)</span>
<span class="ltx_bibblock">
Mershack Okoe and Radu
Jianu. 2015.

</span>
<span class="ltx_bibblock">Graphunit: Evaluating interactive graph
visualizations using crowdsourcing. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Computer
Graphics Forum</em>, Vol. 34. Wiley Online Library,
451–460.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Okoe et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Mershack Okoe, Radu
Jianu, and Stephen Kobourov.
2018.

</span>
<span class="ltx_bibblock">Node-link or adjacency matrices: Old question, new
insights.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">IEEE transactions on visualization and
computer graphics</em> 25, 10
(2018), 2940–2952.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Brien and Cairns (2015)</span>
<span class="ltx_bibblock">
Heather O’Brien and
Paul Cairns. 2015.

</span>
<span class="ltx_bibblock">An empirical evaluation of the User Engagement
Scale (UES) in online news environments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Information Processing &amp; Management</em>
51, 4 (2015),
413–427.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peake and Wang (2018)</span>
<span class="ltx_bibblock">
Georgina Peake and Jun
Wang. 2018.

</span>
<span class="ltx_bibblock">Explanation mining: Post hoc interpretability of
latent factor models for recommendation systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery &amp; Data Mining</em>.
2060–2069.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pope et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Phillip E. Pope, Soheil
Kolouri, Mohammad Rostami, Charles E.
Martin, and Heiko Hoffmann.
2019.

</span>
<span class="ltx_bibblock">Explainability Methods for Graph Convolutional
Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.
10764–10773.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Purificato et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Erasmo Purificato,
Baalakrishnan Aiyer Manikandan,
Prasanth Vaidya Karanam,
Mahantesh Vishvanath Pattadkal, and
Ernesto William De Luca. 2021.

</span>
<span class="ltx_bibblock">Evaluating Explainable Interfaces for a Knowledge
Graph-Based Recommender System.. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">IntRS@
RecSys</em>. 73–88.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Marco Tulio Ribeiro,
Sameer Singh, and Carlos Guestrin.
2016.

</span>
<span class="ltx_bibblock">” Why should i trust you?” Explaining the
predictions of any classifier. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining</em>. 1135–1144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ramprasaath R Selvaraju,
Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. 2017.

</span>
<span class="ltx_bibblock">Grad-cam: Visual explanations from deep networks
via gradient-based localization. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of
the IEEE international conference on computer vision</em>.
618–626.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin and Kim (2019)</span>
<span class="ltx_bibblock">
Dajung Diane Shin and
Sung-il Kim. 2019.

</span>
<span class="ltx_bibblock">Homo curious: Curious or interested?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Educational Psychology Review</em>
31 (2019), 853–874.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Divit P Singh, Lee Lisle,
TM Murali, and Kurt Luther.
2018.

</span>
<span class="ltx_bibblock">Crowdlayout: Crowdsourced design and evaluation of
biological network visualizations. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings
of the 2018 CHI Conference on Human Factors in Computing Systems</em>.
1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha and Swearingen (2002)</span>
<span class="ltx_bibblock">
Rashmi Sinha and Kirsten
Swearingen. 2002.

</span>
<span class="ltx_bibblock">The role of transparency in recommender systems.
In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">CHI’02 extended abstracts on Human factors in
computing systems</em>. 830–831.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szymanski et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Maxwell Szymanski, Martijn
Millecamp, and Katrien Verbert.
2021.

</span>
<span class="ltx_bibblock">Visual, textual or hybrid: the effect of user
expertise on different explanations. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">26th
International Conference on Intelligent User Interfaces</em>.
109–119.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tintarev (2007)</span>
<span class="ltx_bibblock">
Nava Tintarev.
2007.

</span>
<span class="ltx_bibblock">Explanations of recommendations. In
<em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 2007 ACM conference on
Recommender systems</em>. 203–206.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tory and Moller (2004)</span>
<span class="ltx_bibblock">
M. Tory and T. Moller.
2004.

</span>
<span class="ltx_bibblock">Human factors in visualization research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">IEEE transactions on visualization and
computer graphics</em> 10 (2004),
72–84.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tversky and Kahneman (1974)</span>
<span class="ltx_bibblock">
Amos Tversky and Daniel
Kahneman. 1974.

</span>
<span class="ltx_bibblock">Judgment under Uncertainty: Heuristics and Biases:
Biases in judgments reveal some heuristics of thinking under uncertainty.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">science</em> 185,
4157 (1974), 1124–1131.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Peng Wang, Renqin Cai,
and Hongning Wang. 2022.

</span>
<span class="ltx_bibblock">Graph-based Extractive Explainer for
Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Proceedings of the ACM Web
Conference 2022</em>. ACM, 2163–2171.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Xiang Wang, Dingxian
Wang, Canran Xu, Xiangnan He,
Yixin Cao, and Tat-Seng Chua.
2019.

</span>
<span class="ltx_bibblock">Explainable reasoning over knowledge graphs for
recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Proceedings of the AAAI
conference on artificial intelligence</em>, Vol. 33.
5329–5336.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilkenfeld (2014)</span>
<span class="ltx_bibblock">
Daniel A Wilkenfeld.
2014.

</span>
<span class="ltx_bibblock">Functional explaining: a new approach to the
philosophy of explanation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Synthese</em> 191,
14 (2014), 3367–3391.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhitao Ying, Dylan
Bourgeois, Jiaxuan You, Marinka Zitnik,
and Jure Leskovec. 2019.

</span>
<span class="ltx_bibblock">Gnnexplainer: Generating explanations for graph
neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Advances in neural information processing
systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Curley (2018)</span>
<span class="ltx_bibblock">
Jingjing Zhang and
Shawn P Curley. 2018.

</span>
<span class="ltx_bibblock">Exploring explanation effects on consumers’ trust
in online recommender agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">International Journal of Human–Computer
Interaction</em> 34, 5
(2018), 421–432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shichang Zhang, Jiani
Zhang, Xiang Song, Soji Adeshina,
Da Zheng, Christos Faloutsos, and
Yizhou Sun. 2023.

</span>
<span class="ltx_bibblock">PaGE-Link: Path-based Graph Neural Network
Explanation for Heterogeneous Link Prediction. In
<em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Proceedings of the ACM Web Conference 2023</em>.
ACM, 3784–3793.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Chen (2020)</span>
<span class="ltx_bibblock">
Yongfeng Zhang and Xu
Chen. 2020.

</span>
<span class="ltx_bibblock">Explainable Recommendation: A Survey and New
Perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Foundations and Trends® in Information
Retrieval</em> 14, 1 (2020),
1–101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Yongfeng Zhang, Guokun
Lai, Min Zhang, Yi Zhang,
Yiqun Liu, and Shaoping Ma.
2014.

</span>
<span class="ltx_bibblock">Explicit factor models for explainable
recommendation based on phrase-level sentiment analysis. In
<em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proceedings of the 37th international ACM SIGIR
conference on Research &amp; development in information retrieval</em>.
83–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<figure class="ltx_figure" id="A0.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A0.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="938" id="A0.F5.sf1.g1" src="extracted/5736934/img/questionnaire_form_screenshot_book_select.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A0.F5.sf1.3.2" style="font-size:90%;">Participants are asked to select a user profile based on a selection of books.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A0.F5.fig1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="A0.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="560" id="A0.F5.sf2.g1" src="extracted/5736934/img/questionnaire_form_screenshot.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A0.F5.sf2.3.2" style="font-size:90%;">Participants are reminded of their profile choice and are recommended a new book.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="A0.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="585" id="A0.F5.sf3.g1" src="extracted/5736934/img/questionnaire_form_screenshot_graph_expl.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F5.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="A0.F5.sf3.3.2" style="font-size:90%;">Example of explanation design description (here graph-based design.)</span></figcaption>
</figure>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F5.2.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="A0.F5.3.2" style="font-size:90%;">Example of a participant’s progression through the questionnaire.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 17 07:26:23 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
