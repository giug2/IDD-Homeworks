<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Trustworthy AI in practice: an analysis of practitioners’ needs and challenges</title>
<!--Generated on Wed May 15 13:00:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Artificial Intelligence,  Software Engineering,  Trustworthy AI,  Mixed-method Research,  Systematic Investigation,  Survey,  Semi-structured Interview" lang="en" name="keywords"/>
<base href="/html/2407.12135v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S1" title="In Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S2" title="In Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S2.SS1" title="In 2. Background ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>AI Principles proliferation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S2.SS2" title="In 2. Background ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Trustworthy AI principles definitions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S3" title="In Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S3.SS1" title="In 3. Method ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S3.SS2" title="In 3. Method ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>The survey</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S3.SS3" title="In 3. Method ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Interview Study Protocol</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S3.SS4" title="In 3. Method ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Data Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4" title="In Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and findings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS1" title="In 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Preliminary concepts knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS2" title="In 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Practices in Preventing untrustworthiness in AI</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS3" title="In 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Practices in Discovering Untrustworthiness Issues</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS4" title="In 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Practices in Addressing Untrustworthiness Issues</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS5" title="In 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Demographics and background information</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS6" title="In 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Summary of key findings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S5" title="In Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S5.SS1" title="In 5. Discussion ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Practical implications and recommendations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S6" title="In Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Threats to validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S7" title="In Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Trustworthy AI in practice: an analysis of practitioners’ needs and challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Teresa Baldassarre
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-8589-2850" title="ORCID identifier">0000-0001-8589-2850</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Bari ”A. Moro”</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Bari</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Italy</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mariateresa.baldassarre@uniba.it">mariateresa.baldassarre@uniba.it</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Domenico Gigante
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3589-6970" title="ORCID identifier">0000-0003-3589-6970</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Ser&amp;Practices Srl</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Bari</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Italy</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:d.gigante@serandp.com">d.gigante@serandp.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marcos Kalinowski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-1445-3425" title="ORCID identifier">0000-0003-1445-3425</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Pontifical Catholic University of Rio de Janeiro (PUC-Rio)</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Rio de Janeiro</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Brazil</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kalinowski@inf.puc-rio.br">kalinowski@inf.puc-rio.br</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Azzurra Ragone
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-3537-7663" title="ORCID identifier">0000-0002-3537-7663</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">University of Bari ”A. Moro”</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Bari</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">Italy</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:azzurra.ragone@uniba.it">azzurra.ragone@uniba.it</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sara Tibidò
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Scuola IMT Alti Studi Lucca</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Bari</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">Italy</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sara.tibido@imtlucca.it">sara.tibido@imtlucca.it</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id16.id1">Recently, there has been growing attention on behalf of both academic and practice communities towards the ability of Artificial Intelligence (AI) systems to operate responsibly and ethically.
As a result, a plethora of frameworks and guidelines have appeared to support practitioners in implementing Trustworthy AI applications (TAI).
However, little research has been done to investigate whether such frameworks are being used and how.
In this work, we study the vision AI practitioners have on TAI principles, how they address them, and what they would like to have – in terms of tools, knowledge, or guidelines – when they attempt to incorporate such principles into the systems they develop.
Through a survey and semi-structured interviews, we systematically investigated practitioners’ challenges and needs in developing TAI systems.
Based on these practical findings, we highlight
recommendations to help AI practitioners develop Trustworthy AI applications.
</p>
</div>
<div class="ltx_keywords">Artificial Intelligence, Software Engineering, Trustworthy AI, Mixed-method Research, Systematic Investigation, Survey, Semi-structured Interview
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>28th International Conference on Evaluation and Assessment in Software Engineering; June 18–21, 2024; Salerno, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>28th International Conference on Evaluation and Assessment in Software Engineering (EASE 2024), June 18–21, 2024, Salerno, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3661167.3661214</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>The 28th International Conference on Evaluation and Assessment in Software Engineering; 18–21 June, 2024; Salerno, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-1701-7/24/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Artificial intelligence</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Software and its engineering Software creation and management</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Artificial intelligence (AI) systems increasingly exert an extensive impact on various facets of our existence, encompassing the realm of healthcare and the quality of education we receive <cite class="ltx_cite ltx_citemacro_citep">(Bosch et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib11" title="">2016</a>; Esteva et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib18" title="">2017</a>; Holstein et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib30" title="">2018</a>)</cite>, the determination of which news articles or social media posts we encounter <cite class="ltx_cite ltx_citemacro_citep">(Alvarado and Wærn, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib4" title="">2018</a>; Bucher, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib14" title="">2017</a>; Rader and Gray, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib40" title="">2015</a>)</cite>, the allocation of employment opportunities <cite class="ltx_cite ltx_citemacro_citep">(HireVue.com, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib28" title="">2019</a>; pymetrics.ai, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib39" title="">2018</a>)</cite>, the detention decisions <cite class="ltx_cite ltx_citemacro_citep">(Chouldechova, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib15" title="">2016</a>)</cite>, and the intensification of policing efforts in some areas <cite class="ltx_cite ltx_citemacro_citep">(Lum and Isaac, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib35" title="">2016</a>; Veale et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib47" title="">2018</a>)</cite>, just to name a few.
With this expansion, the risk of AI increasing social inequities has generated escalating attention across several communities, including the media. Indeed it is common to observe reports in mainstream media of systemic dangerous behaviors observed in widely used AI systems, such as a smart algorithm guiding assistance for tens of millions of people biased against dark-skinned patients<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/d41586-019-03228-6" title="">https://www.nature.com/articles/d41586-019-03228-6</a></span></span></span>, or an AI chatbot suspended for making homophobic slurs and leaking user information<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ic3.gov/Media/News/2021/210310-2.pdf" title="">https://www.ic3.gov/Media/News/2021/210310-2.pdf</a></span></span></span>.
These risks are even more pronounced with the recent advent of Generative AI and the impact these systems have on various societal aspects <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib6" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this context, the concept of Trustworthy Artificial Intelligence (TAI) has been defined: ”<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Trustworthy AI has three components, which should be met throughout the system’s entire life cycle: (1) it should be lawful, complying with all applicable laws and regulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI</span>” <cite class="ltx_cite ltx_citemacro_citep">(High-Level Expert Group on AI (2018a), <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib26" title="">AIHLEG</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Several public and private organizations have responded to these societal fears by developing different kinds of resources: ethical requirements, principles, guidelines, best practices, tools, and frameworks <cite class="ltx_cite ltx_citemacro_citep">(Barletta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib9" title="">2023</a>; Pardo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib38" title="">2011</a>)</cite>.
As the field progresses, integrated toolkits are being developed with the objective of rendering these methods more broadly accessible and usable (<span class="ltx_text ltx_font_italic" id="S1.p3.1.1">e.g.</span>, Enisa’s Machine Learning Security <cite class="ltx_cite ltx_citemacro_citep">(enisaml, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib17" title="">2021</a>)</cite>, Aequitas <cite class="ltx_cite ltx_citemacro_citep">(aequitas.dssg.io, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib3" title="">2018</a>)</cite> or Google’s What-if-Tool <cite class="ltx_cite ltx_citemacro_citep">(whatiftool, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib48" title="">2018</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Despite growth in the development and dissemination of toolkits, there has been little research investigating how AI practitioners actually use them in practice throughout the entire Software Development Life Cycle (SDLC). A few studies have already explored practitioners’ perceptions and desires around open-source toolkits but are focused only on the Fairness aspect of TAI <cite class="ltx_cite ltx_citemacro_citep">(Lee and Singh, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib34" title="">2021</a>; Richardson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib42" title="">2021</a>)</cite>.
However, neither of these two works investigated what professionals need during the entire SDLC.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our research builds on the results of our previous mapping study <cite class="ltx_cite ltx_citemacro_citep">(Barletta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib9" title="">2023</a>)</cite> and aims to investigate how AI professionals deal with TAI issues on a day-to-day basis.
To better understand practitioners’ needs we distributed a <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">survey</span> and conducted semi-structured one-on-one <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">interviews</span>, collecting data from a total of <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">34 practitioners</span>
employed in companies of different sizes.
</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">As the main contribution, this work intends to deeply investigate practitioners’ views, needs, and challenges in developing TAI systems throughout the entire SDLC.
The novel contribution can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We have analyzed the <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">existing procedures</span> that development teams adopt when implementing Trustworthiness in AI;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We have investigated the <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">impediments</span> encountered in the attempt to implement Trustworthiness in AI;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We have identified a range of practitioners’ <span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">needs</span> that academic and industrial research should seek to address.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S1.p6.2">Our study has identified a range of practitioner needs that have thus far been overlooked in the literature.
For example,
the majority of our interviewees report a lack of tools for the late stages of the SDLC, as well as of knowledge bases and practical guidelines with suggestions on implementing TAI across the entire SDLC.
</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The paper is organized as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S2" title="2. Background ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">2</span></a> provides some background definitions.
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S3" title="3. Method ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">3</span></a> describes the methodology we adopted to conduct this study, including research questions and how we collected, extracted, and analyzed the data. Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4" title="4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">4</span></a> presents the quantitative and qualitative results together with some preliminary findings.
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S5" title="5. Discussion ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">5</span></a> discusses the results, and practical implications and recommendations for the AI industry and research community.
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S6" title="6. Threats to validity ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">6</span></a> addresses threats to validity, followed by the conclusion in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S7" title="7. Conclusion ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>AI Principles proliferation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">National and global entities have established specialized expert committees in the field of Artificial Intelligence (AI) to address the associated risks stemming from AI development. These committees often have the mandate of formulating policy documents. Prominent examples of such organizations include the High-Level Expert Group on Artificial Intelligence initiated by the European Commission <cite class="ltx_cite ltx_citemacro_citep">(High-Level Expert Group on AI (2018b), <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib27" title="">AIHLEG</a>)</cite>,
the UNESCO Ad Hoc Expert Group (AHEG) tasked with the Recommendation on the Ethics of Artificial Intelligence <cite class="ltx_cite ltx_citemacro_citep">(UNESCO Ad Hoc Expert Group (2021), <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib46" title="">AHEG</a>)</cite>,
the Advisory Council on the Ethical Use of Artificial Intelligence and Data in Singapore <cite class="ltx_cite ltx_citemacro_citep">(Holborn Law LLC, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib29" title="">2018</a>)</cite>,
the NASA Artificial Intelligence Group <cite class="ltx_cite ltx_citemacro_citep">(NASA Artificial Intelligence Group, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib37" title="">2018</a>)</cite>
and the UK AI Council <cite class="ltx_cite ltx_citemacro_citep">(UK AI Council, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib45" title="">2019</a>)</cite>,
among others.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">These committees bear the crucial role of generating comprehensive reports and guidelines about Trustworthy AI (TAI). A parallel endeavour is observable within the commercial landscape, particularly among enterprises heavily reliant on AI technologies. Corporations such as Sony<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sony.com/en/SonyInfo/sony_ai/responsible_ai.html" title="">https://www.sony.com/en/SonyInfo/sony_ai/responsible_ai.html</a></span></span></span> and Meta<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/" title="">https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/</a></span></span></span> have made their AI policies and principles publicly accessible. Concurrently, professional organizations and non-profit entities, such as UNI Global Union<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.thefutureworldofwork.org/media/35420/uni_ethical_ai.pdf" title="">http://www.thefutureworldofwork.org/media/35420/uni_ethical_ai.pdf</a></span></span></span> and the Internet Society<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.internetsociety.org/resources/doc/2017/artificial-intelligence-and-machine-learning-policy-paper/" title="">https://www.internetsociety.org/resources/doc/2017/artificial-intelligence-and-machine-learning-policy-paper/</a></span></span></span>, have issued statements and recommendations.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The substantial efforts of this diverse spectrum of stakeholders in crafting TAI principles and policies not only underscore the imperative need for ethical guidance but also exemplify their vested interest in shaping AI ethics to align with their specific objectives <cite class="ltx_cite ltx_citemacro_citep">(Greene et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib24" title="">2019</a>)</cite>. It is noteworthy that the private sector’s engagement in the gap of AI ethics has undergone a thorough check, with contentions suggesting that high-level soft policies may be employed to either transform a social issue into a purely technical one <cite class="ltx_cite ltx_citemacro_citep">(Greene et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib24" title="">2019</a>)</cite> or to potentially circumvent regulatory measures <cite class="ltx_cite ltx_citemacro_citep">(bay, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib2" title="">2018</a>; Jobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib31" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Nevertheless, a set of research endeavors has brought attention to the divergent nature of these proposals, giving rise to a complex challenge often referred to as <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.1">”principle proliferation”<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote7.1.1.1">7</span></span><span class="ltx_text ltx_font_upright" id="footnote7.5">It is ”</span><span class="ltx_text" id="footnote7.6">the proliferation of soft-law efforts</span><span class="ltx_text ltx_font_upright" id="footnote7.7">”. Jobin’s analysis </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_upright" id="footnote7.8.1">(</span>Jobin et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_upright" id="footnote7.9.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib31" title="">2019</a><span class="ltx_text ltx_font_upright" id="footnote7.10.3">)</span></cite><span class="ltx_text ltx_font_upright" id="footnote7.11"> shows the emergence of an apparent cross-stakeholder convergence on promoting the ethical principles […] [but] unclarity remains as to which ethical principles should be prioritized, how conflicts between ethical principles should be resolved, who should enforce ethical oversight on AI and how researchers and institutions can comply with the resulting guidelines.</span></span></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Jobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib31" title="">2019</a>)</cite>. Consequently, efforts have been undertaken to address this challenge. For instance, Jobin et al. <cite class="ltx_cite ltx_citemacro_citep">(Jobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib31" title="">2019</a>)</cite> conducted a comprehensive study, that culminated in the identification of a global convergence around five ethical principles: <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.2">transparency</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.3">justice and fairness</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.4">non-maleficence</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.5">responsibility</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.6">privacy</span>. Jobin et al. <cite class="ltx_cite ltx_citemacro_citep">(Jobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib31" title="">2019</a>)</cite> also observed that, while no single document they reviewed encompassed all of these ethical principles, these five principles were mentioned in over half of the sources examined. Furthermore, their detailed thematic analysis unveiled significant semantic and conceptual variations in the interpretation of these principles and the specific recommendations or areas of concern derived from each one.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Trustworthy AI principles definitions</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">As set out in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S2.SS1" title="2.1. AI Principles proliferation ‣ 2. Background ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">2.1</span></a>, a notable degree of ambiguity and subtlety exists in demarcating the principles that predominantly characterize Trustworthy AI (TAI). Notably, TAI is sometimes used interchangeably with Responsible or Ethical AI. In our investigation, we confront the challenge of <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">principle proliferation</span> by choosing to focus on a specific subset of principles that characterize TAI. Specifically, we concentrate on the most recurrent four principles identified by Jobin et al. <cite class="ltx_cite ltx_citemacro_citep">(Jobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib31" title="">2019</a>)</cite>, while opting to exclude the principle of <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">responsibility</span> due to its infrequent occurrence and lack of a clear, universally accepted definition.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Furthermore, in this work, we have decided to adopt the definitions put forth by the High-Level Expert Group on Artificial Intelligence (AIHLEG)
— an entity established by the European Commission — explained in their ”<span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">Ethics guidelines for trustworthy AI</span>” <cite class="ltx_cite ltx_citemacro_citep">(High-Level Expert Group on AI (2018a), <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib26" title="">AIHLEG</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Given these premises, we mapped the four selected principles identified by Jobin et al. <cite class="ltx_cite ltx_citemacro_citep">(Jobin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib31" title="">2019</a>)</cite>, with the formal definition delineated by the AIHLEG <cite class="ltx_cite ltx_citemacro_citep">(High-Level Expert Group on AI (2018a), <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib26" title="">AIHLEG</a>)</cite>. The mapping has been carried out based on the contents of the definitions and not merely on the nomenclatures, as, in most cases, they differ.
For the sake of simplicity, we have shortened and labeled each TAI principle as follows: <span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Transparency</span>, <span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.2">Fairness</span>, <span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.3">Security</span>, and <span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.4">Privacy</span>. We will use these labels throughout the paper.
<span class="ltx_text" id="S2.SS2.p3.1.5" style="color:#000000;">Definitions of the terms adopted for each principle are provided in our online appendix
<cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib7" title="">2024a</a>)</cite></span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The goal of our study is to investigate the <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">state of the practice</span> to understand common practices as well as challenges and difficulties encountered by AI practitioners in implementing TAI systems through the entire SDLC.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Since traditional phases of the SDLC do not necessarily map well with the activities required to develop an AI-enabled system, we have extended each phase with one or more activities mentioned by Zhengxin et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhengxin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib50" title="">2023</a>)</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S3.T1" title="Table 1 ‣ 3. Method ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">1</span></a> graphically shows how each traditional SDLC phase’s definition has been extended.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Activities integrated into the traditional SDLC phases to support AI-enabled systems development.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1" style="font-size:120%;">Traditional SDLC phase</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1" style="font-size:120%;">Integrated activity</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.2.1"><span class="ltx_text" id="S3.T1.1.2.2.1.1" style="font-size:120%;">Requirements Elicitation</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.2"><span class="ltx_text" id="S3.T1.1.2.2.2.1" style="font-size:120%;">Model Requirement</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.3.3.1"><span class="ltx_text" id="S3.T1.1.3.3.1.1" style="font-size:120%;">Design</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.3.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.3.3.2.1">
<tr class="ltx_tr" id="S3.T1.1.3.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.3.3.2.1.1.1"><span class="ltx_text" id="S3.T1.1.3.3.2.1.1.1.1" style="font-size:120%;">Data Collection,</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.3.3.2.1.2.1"><span class="ltx_text" id="S3.T1.1.3.3.2.1.2.1.1" style="font-size:120%;">Data Preparation</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.4.4.1"><span class="ltx_text" id="S3.T1.1.4.4.1.1" style="font-size:120%;">Development</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.4.4.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.4.4.2.1">
<tr class="ltx_tr" id="S3.T1.1.4.4.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.4.4.2.1.1.1"><span class="ltx_text" id="S3.T1.1.4.4.2.1.1.1.1" style="font-size:120%;">Feature Engineering,</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.4.4.2.1.2.1"><span class="ltx_text" id="S3.T1.1.4.4.2.1.2.1.1" style="font-size:120%;">Model Training</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.5.5.1"><span class="ltx_text" id="S3.T1.1.5.5.1.1" style="font-size:120%;">Test</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.5.5.2"><span class="ltx_text" id="S3.T1.1.5.5.2.1" style="font-size:120%;">Model Evaluation</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.6.6.1"><span class="ltx_text" id="S3.T1.1.6.6.1.1" style="font-size:120%;">Deployment</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.6.6.2"><span class="ltx_text" id="S3.T1.1.6.6.2.1" style="font-size:120%;">Model Deployment</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.T1.1.7.7.1"><span class="ltx_text" id="S3.T1.1.7.7.1.1" style="font-size:120%;">Monitoring</span></th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S3.T1.1.7.7.2"><span class="ltx_text" id="S3.T1.1.7.7.2.1" style="font-size:120%;">Model Monitoring</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Given our main goal, and based on the analysis of the shortcomings and common practices that emerged from the mapping study carried out in our previous work <cite class="ltx_cite ltx_citemacro_citep">(Barletta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib9" title="">2023</a>)</cite>,
we defined the following Research Questions (RQs):</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">RQ1</span>: What vision/opinion do practitioners have about untrustworthiness issues and how often do they encounter them?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">RQ2</span>: How do practitioners address untrustworthiness issues?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">RQ3</span>: What tools/support do practitioners desire to address untrustworthiness problems better?</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Data collection</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For our study, we recruited practitioners<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>By ”AI practitioners” we mean those who work in any role
on a team developing products or services involving AI.</span></span></span> working on AI products and services through a combination of purposive and snowball sampling <cite class="ltx_cite ltx_citemacro_citep">(Heckathorn, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib25" title="">2011</a>)</cite>.
To recruit participants for our study, we chose to not widely distribute our survey online, but rather to conduct a brief screening procedure to define the inclusion criteria and target suitable participants for the study.
The target participants for the survey were AI practitioners involved in the development of AI-based systems, with at least some basic knowledge of TAI principles and/or who had previously addressed TAI in their professional work.
We started by sending personal emails to contacts within our network, working in industry or academia, explaining the purpose of the study and the inclusion criteria, we asked them to help us recruit other participants by spreading the invitation through their networks.
Next, we verified that the inclusion criteria were met by asking some specific questions in the demographics section of the survey.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The invitation to participate in the study was sent by email and included an explanation of the study’s purpose. <span class="ltx_text" id="S3.SS1.p2.1.1" style="color:#000000;">In order to meet all needs (<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1.1">e.g.</span>, restrictions related to tight schedules, time zones, commitments), we gave participants the chance to choose between (a) survey (asynchronous interaction) or (b) semi-structured interviews (synchronous interaction) mode. This allowed us to include a broader amount of subjects and collect a higher number of answers. All participants answered the same set of questions and had the opportunity to add any non-listed practices or suggestions/feedback related to the closed-ended question in the open-text fields (asynchronous mode) whereas, interviewed participants (synchronous) could “discuss their answers out loud” and further elaborate their considerations with the interviewers. The answers were all transcribed to be included later in the thematic analysis.</span></p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text" id="S3.SS1.p3.1.1" style="color:#000000;">Participation was on a voluntary basis and not rewarded by any means.</span> The survey can be accessed at <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib8" title="">2024b</a>)</cite>. Overall, we obtained <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.2">23 answers</span> for the <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.3">survey</span> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.4">11 participants</span> attended the <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.5">interview</span>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS5" title="4.5. Demographics and background information ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">4.5</span></a> provides details about participants’ demographics and their relevant experience.
Specific details about their companies and working environment have been abstracted to preserve anonymity.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>The survey</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The survey contains six main sections <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib8" title="">2024b</a>)</cite>:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">1. <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">Informed consent request</span>. This page asks the participants to provide their informed consent and explains the purpose of the research, the participants’ requirements, confidentiality rules, participation on a voluntary basis, and the time needed to complete the survey.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">2. <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">Preliminary concepts knowledge</span>. We clarified the semantics and interpretation of each TAI principle for participants by providing a definition for each principle. By listing a definition, we wanted to build a shared understanding of each principle to answer the remaining questions.
In addition, we asked the participants about their vision and previous experience with TAI.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">3. <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.1">Practices in preventing untrustworthiness in AI</span>. We inquired participants about the main strategies they adopt to prevent TAI issues (<span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.2">e.g.</span>, balancing the dataset or choosing a specific algorithm).</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">4. <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1">Practices in discovering untrustworthiness issues in AI</span>. We asked participants to share their strategies to find possible sources of untrustworthiness (<span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.2">e.g.</span>, do auditing tasks, compute metrics, learn from user feedback).</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">5. <span class="ltx_text ltx_font_italic" id="S3.SS2.p6.1.1">Practices in addressing trustworthiness issues</span>. We investigated the different approaches used to address TAI issues (<span class="ltx_text ltx_font_italic" id="S3.SS2.p6.1.2">e.g.</span>, dataset augmentation, instance weighting).</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">6. <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.1">Demographics and background information</span>. Participants were asked about gender, level of education, country, role, years of experience, size of the organization, etc.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1">All the questions of the survey sections (3), (4), (5), illustrated above, as well as the options for the answers, were inspired from and based on our previous work <cite class="ltx_cite ltx_citemacro_citep">(Barletta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib9" title="">2023</a>)</cite>, which set current shortcomings and common practices in literature.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">The survey
was anonymous and did not ask for any directly identifying information.
Most of the survey questions were closed-answer and mandatory, but there were also optional open-text ones.
Through the latter, we were able to collect further qualitative data as many of the participants provided information on the practices they usually implement. All survey data and raw material can be accessed in the online appendix <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib7" title="">2024a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Interview Study Protocol</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Before scheduling the interviews with participants, to understand the challenges and requirements for conducting remotely semi-structured interviews, as well as to help us refine our study protocol, we conducted two pilot interviews.
These preliminary interviews helped us define the protocol and the setting we applied to the final, larger sample.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">All interviews were conducted via Microsoft Teams<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/it-it/microsoft-teams" title="">https://www.microsoft.com/it-it/microsoft-teams</a></span></span></span>; after asking each participant for their consent, we enabled <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.1">Recording &amp; Transcription</span> Teams features. Once the interview ended, we proof-checked the transcription in order to correct any misspellings, anonymized any Personal Identifiable Information (PII) and finally deleted the recording.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The interview study consisted of think-aloud semi-structured interviews, each one lasting between 45 and 90 minutes. During the live interview, we periodically asked participants to elaborate on their responses, especially for the open ones. We also encouraged participants to ”think aloud” <cite class="ltx_cite ltx_citemacro_citep">(Kupis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib33" title="">2019</a>; Someren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib43" title="">1994</a>)</cite> and discuss the information that was being displayed and how their understanding of the question was developing.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">To give a standard structure to each interview, we used the survey as a canvas.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Data Analysis</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In this step, we extracted all relevant data using quantitative and qualitative data analysis techniques to summarize and interpret the collected data. For quantitative data, we used descriptive statistics <cite class="ltx_cite ltx_citemacro_citep">(George and Mallery, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib23" title="">2018</a>)</cite>, and for qualitative data, we used thematic analysis <cite class="ltx_cite ltx_citemacro_citep">(Cruzes and Dybå, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib16" title="">2011</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">We used an <span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">inductive thematic analysis</span> approach <cite class="ltx_cite ltx_citemacro_citep">(Braun and Clarke, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib12" title="">2006</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib13" title="">2012</a>)</cite> to analyze about 11.5 hours of video recordings and their corresponding (automatically generated and manually proof-checked) transcripts. The entire analysis was done through Atlas.ti<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://atlasti.com/" title="">https://atlasti.com/</a></span></span></span>.
<span class="ltx_text" id="S3.SS4.p2.1.2" style="color:#000000;">Two authors worked independently and used the tool to conduct an open coding of the transcripts for each quotation. Next, they manually reviewed each code and decided which to include/exclude annotating any comments. Once this step was completed, they joined to compare and discuss results. The total number of analyzed quotations was 23. The calculated Cohen’s Kappa <cite class="ltx_cite ltx_citemacro_citep">(Bakeman and Quera, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib5" title="">2011</a>)</cite> is 0.259. All the details about the coding procedure and the generated codes are provided in the online appendix <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib7" title="">2024a</a>)</cite>.</span></p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text" id="S3.SS4.p3.1.1" style="color:#000000;">Once finalized, the codes were shared with the entire research team and grouped into higher-level themes concerning the practitioners’ knowledge and practices. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4" title="4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">4</span></a> we discuss the findings identified from these codes and themes, together with implications for future TAI developments.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results and findings</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We present findings from our think-aloud interviews study and the survey answers, divided into three main sections,</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Practices in <span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">preventing</span> untrustworthiness in AI</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Practices in <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">discovering</span> untrustworthiness issues</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Practices in <span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">addressing</span> untrustworthiness issues</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.p1.2">Across all three phases, we discovered different nuances of practitioners’ needs around TAI issues. We supplement data from the closed-ended questions (quantitative results) with the thematic analysis performed on the answers from all the open-ended questions (qualitative results).
<span class="ltx_text" id="S4.p1.2.1" style="color:#000000;">We performed analysis on the disaggregated data with respect to subgroups such as company size, gender, education, and number of projects deployed. To understand if the differences were statistically significant, we conducted pairwise comparisons using Fisher’s exact test coupled with the <span class="ltx_text ltx_font_italic" id="S4.p1.2.1.1">Benjamini-Hochberg</span> <cite class="ltx_cite ltx_citemacro_citep">(Benjamini and Hochberg, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib10" title="">1995</a>)</cite> correction to obtain the adjusted p-values. Since there is no statistical significance in any of the cases except for company size, detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.SS2" title="4.2. Practices in Preventing untrustworthiness in AI ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">4.2</span></a>, the graphs in the paper report the results of the analyses in aggregated form.</span></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Preliminary concepts knowledge</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text" id="S4.SS1.p1.1.1" style="color:#000000;">All discussed tables and graphs, from now on, bring together the answers from both the interviews (11) and the survey (23).</span>
In the survey section ”2. <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">Preliminary concepts knowledge</span>”,
we observed that the TAI principle <span class="ltx_text" id="S4.SS1.p1.1.3" style="color:#000000;">participants have encountered most frequently in their projects</span> is <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.4">Privacy</span> (20 answers), followed by <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.5">Transparency</span> (18 answers) and <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.6">Security</span> (17 answers), while the least experienced is <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.7">Fairness</span> (13 answers).
This answer should be further investigated because perhaps sometimes practitioners may not recognize or be aware of the need to address some issues related to these principles.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In addition, Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.T2" title="Table 2 ‣ 4.1. Preliminary concepts knowledge ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">2</span></a> shows the reasons participants agreed on concerning why they care about TAI.
The most agreed reasons were <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">Avoid violating legal requirements</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">Improve the overall quality</span>.
While, the least agreed one was <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3">Retain users/avoid losing the activity</span>, with eight disagreements.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Reasons why participants care about TAI principles.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.1.1" style="font-size:80%;">Reason</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.2.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.2.1.1.1.1" style="font-size:80%;">Disagreement</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.2.1.2.1.1" style="font-size:80%;">(1-2)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.3.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.3.1.1.1.1" style="font-size:80%;">Neutral</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.3.1.2.1.1" style="font-size:80%;">(3)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.4.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.4.1.1.1.1" style="font-size:80%;">Agreement</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.4.1.2.1.1" style="font-size:80%;">(4-5)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.5.1" style="font-size:80%;">N/A</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.3.2.1.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.2.1.1.1">
<tr class="ltx_tr" id="S4.T2.3.2.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.2.1.1.1.1.1"><span class="ltx_text" id="S4.T2.3.2.1.1.1.1.1.1" style="font-size:80%;">Doing something</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.2.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.2.1.1.1.2.1"><span class="ltx_text" id="S4.T2.3.2.1.1.1.2.1.1" style="font-size:80%;">about trustworthiness</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.2.1.1.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.2.1.1.1.3.1"><span class="ltx_text" id="S4.T2.3.2.1.1.1.3.1.1" style="font-size:80%;">in AI/ML</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.2"><span class="ltx_text" id="S4.T2.3.2.1.2.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.3"><span class="ltx_text" id="S4.T2.3.2.1.3.1" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.4"><span class="ltx_text" id="S4.T2.3.2.1.4.1" style="font-size:80%;">23</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.5"><span class="ltx_text" id="S4.T2.3.2.1.5.1" style="font-size:80%;">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.3.3.2.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.2.1.1">
<tr class="ltx_tr" id="S4.T2.3.3.2.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.3.2.1.1.1.1"><span class="ltx_text" id="S4.T2.3.3.2.1.1.1.1.1" style="font-size:80%;">Avoid violating</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.2.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.3.2.1.1.2.1"><span class="ltx_text" id="S4.T2.3.3.2.1.1.2.1.1" style="font-size:80%;">legal requirements</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.2.2"><span class="ltx_text" id="S4.T2.3.3.2.2.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.2.3"><span class="ltx_text" id="S4.T2.3.3.2.3.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.2.4"><span class="ltx_text" id="S4.T2.3.3.2.4.1" style="font-size:80%;">27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.2.5"><span class="ltx_text" id="S4.T2.3.3.2.5.1" style="font-size:80%;">2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.3.4.3.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.4.3.1.1">
<tr class="ltx_tr" id="S4.T2.3.4.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.4.3.1.1.1.1"><span class="ltx_text" id="S4.T2.3.4.3.1.1.1.1.1" style="font-size:80%;">Avoid reputational</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4.3.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.4.3.1.1.2.1"><span class="ltx_text" id="S4.T2.3.4.3.1.1.2.1.1" style="font-size:80%;">damages</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.2"><span class="ltx_text" id="S4.T2.3.4.3.2.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.3"><span class="ltx_text" id="S4.T2.3.4.3.3.1" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.4"><span class="ltx_text" id="S4.T2.3.4.3.4.1" style="font-size:80%;">21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.5"><span class="ltx_text" id="S4.T2.3.4.3.5.1" style="font-size:80%;">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.3.5.4.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.5.4.1.1">
<tr class="ltx_tr" id="S4.T2.3.5.4.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.5.4.1.1.1.1"><span class="ltx_text" id="S4.T2.3.5.4.1.1.1.1.1" style="font-size:80%;">Improve the overall</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.4.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.5.4.1.1.2.1"><span class="ltx_text" id="S4.T2.3.5.4.1.1.2.1.1" style="font-size:80%;">quality</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.5.4.2"><span class="ltx_text" id="S4.T2.3.5.4.2.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.5.4.3"><span class="ltx_text" id="S4.T2.3.5.4.3.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.5.4.4"><span class="ltx_text" id="S4.T2.3.5.4.4.1" style="font-size:80%;">26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.5.4.5"><span class="ltx_text" id="S4.T2.3.5.4.5.1" style="font-size:80%;">2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T2.3.6.5.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.6.5.1.1">
<tr class="ltx_tr" id="S4.T2.3.6.5.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.6.5.1.1.1.1"><span class="ltx_text" id="S4.T2.3.6.5.1.1.1.1.1" style="font-size:80%;">Retain users/avoid</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.5.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.3.6.5.1.1.2.1"><span class="ltx_text" id="S4.T2.3.6.5.1.1.2.1.1" style="font-size:80%;">losing business</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.3.6.5.2"><span class="ltx_text" id="S4.T2.3.6.5.2.1" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.3.6.5.3"><span class="ltx_text" id="S4.T2.3.6.5.3.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.3.6.5.4"><span class="ltx_text" id="S4.T2.3.6.5.4.1" style="font-size:80%;">19</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.3.6.5.5"><span class="ltx_text" id="S4.T2.3.6.5.5.1" style="font-size:80%;">3</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Other important factors related to the reasons for caring about TAI emerged: i) ”<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.1">need to solve mission-critical tasks</span>”; ii) ”<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.2">[need to provide] models usable in real-world contexts</span>”, and this demonstrated that black box models are not allowed in some specific contexts; iii) ”<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.3">the robustness of the AI explanations themselves</span>”, which shows consciousness about the fact that all TAI aspects contribute to making the model more robust; and iv) ”<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.4">[need to] desire to commercially assemble AI systems to improve society</span>”.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Finally, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F1" title="Figure 1 ‣ 4.1. Preliminary concepts knowledge ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">1</span></a> shows that most of the participants address TAI principles during the <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Design</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.2">Development</span> SDLC phases. In contrast, very few participants reported that they had addressed TAI principles during the <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.3">Requirements Elicitation</span> and, especially, <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.4">Deploy</span> phases.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="592" id="S4.F1.g1" src="extracted/5598226/images/graph_9_12_2.png" width="1080"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>TAI principles addressed by SDLC phase.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Practices in Preventing untrustworthiness in AI</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the survey section ”3.<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Practices in preventing untrustworthiness in AI</span>”, as Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F2" title="Figure 2 ‣ 4.2. Practices in Preventing untrustworthiness in AI ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">2</span></a> shows, the most recurrent strategy employed by our participants to ensure trustworthiness is ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">algorithm that can best explain the decision</span>”. On the other hand, the least employed practice appears to be ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">inject malicious data points</span>”.
<span class="ltx_text" id="S4.SS2.p1.1.4" style="color:#000000;">In analyzing the disaggregated data, we found a statistically significant difference only in the responses related to the strategy “<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.4.1">Algorithm that can best explain the decision</span>” for the company size subgroup. Indeed, for medium-sized companies, we found more positive responses than for small and large enterprises. For medium ones, no negative answers were given and 63% of the participants chose ”Always”, reflecting the wide use of this strategy in medium-sized companies.</span></p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="828" id="S4.F2.g1" src="extracted/5598226/images/graph_13_4.png" width="1071"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Strategies employed to ensure trustworthiness in AI. N/A answers have been removed.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Some participants mentioned other strategies, such as ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">[conduct an] in-depth study of the state of the art [prior to start designing the system]</span>” and ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">[use the] post-processing phase […] to apply human-friendly deterministic rules to check whether a result is in line with the sense of the application domain</span>”.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Additionally, when we asked the participants to rate the utility of various hypothetical tools assuming their team had access to them, the participants rated as the most valuable the tool able to ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">[…] generate an explanation of a model after its creation […]</span>”. On the other hand, they rated as least useful the tool to ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">decide how much data you need for particular subgroups/subpopulations</span>”. These results are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F3" title="Figure 3 ‣ 4.2. Practices in Preventing untrustworthiness in AI ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="828" id="S4.F3.g1" src="extracted/5598226/images/graph_15_5.png" width="1071"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Perceived usefulness of hypothetical tools to prevent TAI issues. N/A answers have been removed.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Here too, some important insights emerged, including ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1">tools to improve software architectures</span>” and ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.2">[a tool for] referencing the best architecture (dependencies, docker files, instant compute, …) to perform the task with the lowest possible costs</span>”.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">The answers to this section show practitioners are prone to use techniques and tools to prevent trustworthiness issues, focusing mainly on ensuring <span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Transparency</span> (a.k.a. Explainability).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Practices in Discovering Untrustworthiness Issues</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In the survey section ”4.<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Practices in Discovering untrustworthiness in AI</span>”, we investigated which strategies participants mainly employed to discover TAI issues.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The data shows that the most used strategies are ”<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">Metrics/KPIs</span>”, ”<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">learn from user feedback</span>”, and ”<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.3">examine AI/ML model’s input features</span>” (see Fig.<a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F4" title="Figure 4 ‣ 4.3. Practices in Discovering Untrustworthiness Issues ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">4</span></a>).
<span class="ltx_text" id="S4.SS3.p2.1.4" style="color:#000000;">Examples of <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.4.1">Metrics/KPIs</span> related to fairness are, just to cite a few, Demographic Parity, Accuracy, F1-Score. Whereas, <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.4.2">user feedback</span> is intended as having a feedback form where the users can report misbehavior by the algorithm <cite class="ltx_cite ltx_citemacro_citep">(Gemini Mistake, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib22" title="">2024</a>)</cite></span>.
What stands out is that the strategy less employed by the participants is ”<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.5">generate specific adversarial/malicious samples</span>” (4/10 negative answers).</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="633" id="S4.F4.g1" src="extracted/5598226/images/graph_19_3.png" width="1088"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Strategies employed to discover untrustworthiness in AI.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Moreover, when asked for other (not mentioned) strategies they employ, one participant answered ”<span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.1">post processing studies to evaluate possible model ’discriminations’</span>”. As a free thought, another declared ”<span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.2">apply confidence criteria such that it is possible to measure how often the model fails to respond reliably</span>”.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">What emerges in this section is that participants employ both qualitative (<span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.1">e.g.</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.2">auditing tasks in the human labeling/scoring process</span>) and quantitative (<span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.3">e.g.</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.4">metrics and KPIs</span>) strategies.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Practices in Addressing Untrustworthiness Issues</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Regarding survey section ”5.<span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">Practices in Addressing Untrustworthiness in AI</span>”, participants reported that after finding a TAI issue only in 35% of the cases (12/34) the team addressed it directly, while 15% (5/34) of the participants stated that it was not addressed by them, but handled by a third party.
Worth noting is the fact that in 50% (17/34) of the cases participants reported that they did not fix the issue after finding it.
The reasons why participants did not solve the issue after finding it are asked in a subsequent question (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.T3" title="Table 3 ‣ 4.4. Practices in Addressing Untrustworthiness Issues ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">When participants addressed any issues found, they declared the most implemented strategies were ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.1">improving the quality of the dataset (e.g., removing spurious samples, paradoxical values)</span>” (8 answers) and ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.2">augmenting the dataset (e.g., with artiﬁcial, manually generated data points)</span>” (6 answers). On the other hand, the less implemented strategy was ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.3">searching for a tool which automates a specific trustworthiness issue-fixing process</span>” (1 answer).
One interviewee also mentioned that they usually approach explainability by ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.4">using [only] white box models</span>”. More details can be found in Figure A2 in the online appendix <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib7" title="">2024a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Regarding the reasons why participants
did not solve a TAI issue after finding it, Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.T3" title="Table 3 ‣ 4.4. Practices in Addressing Untrustworthiness Issues ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">3</span></a> shows the most frequent reasons are ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.1">the issue solution required too much time to be implemented</span>” (58.3%) and ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.2">the issue solution was likely to decrease the performance of the system (e.g., decreasing accuracy)</span>” (50%). On the other hand, none of the participants answered: ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.3">no one had an idea on how to solve the issue</span>”; this is a positive result since demonstrates practitioners are conscious of untrustworthiness problems and can formulate hypotheses on how to address them. During the interviews, one participant also mentioned ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.4">data availability</span>” as an impediment.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Reasons which prevented participants from addressing/ﬁxing AI trustworthiness issues. </figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.1.1" style="font-size:90%;">Impediment</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.2.1" style="font-size:90%;">Yes</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.3.1" style="font-size:90%;">No</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.4.1" style="font-size:90%;">N/A</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.2.1.1"><span class="ltx_text" id="S4.T3.3.2.1.1.1" style="font-size:90%;">No one had idea on how to solve the issue</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.1.2"><span class="ltx_text" id="S4.T3.3.2.1.2.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.1.3"><span class="ltx_text" id="S4.T3.3.2.1.3.1" style="font-size:90%;">9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.1.4"><span class="ltx_text" id="S4.T3.3.2.1.4.1" style="font-size:90%;">3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.2.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.3.3.2.1.1">
<tr class="ltx_tr" id="S4.T3.3.3.2.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.3.2.1.1.1.1"><span class="ltx_text" id="S4.T3.3.3.2.1.1.1.1.1" style="font-size:90%;">The issue solution required high human effort,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.2.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.3.2.1.1.2.1"><span class="ltx_text" id="S4.T3.3.3.2.1.1.2.1.1" style="font-size:90%;">which we could not afford</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.2.2"><span class="ltx_text" id="S4.T3.3.3.2.2.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.2.3"><span class="ltx_text" id="S4.T3.3.3.2.3.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.2.4"><span class="ltx_text" id="S4.T3.3.3.2.4.1" style="font-size:90%;">2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.3.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.3.4.3.1.1">
<tr class="ltx_tr" id="S4.T3.3.4.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.4.3.1.1.1.1"><span class="ltx_text" id="S4.T3.3.4.3.1.1.1.1.1" style="font-size:90%;">The issue solution was too expensive</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.4.3.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.4.3.1.1.2.1"><span class="ltx_text" id="S4.T3.3.4.3.1.1.2.1.1" style="font-size:90%;">(ﬁnancially) to address</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.3.2"><span class="ltx_text" id="S4.T3.3.4.3.2.1" style="font-size:90%;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.3.3"><span class="ltx_text" id="S4.T3.3.4.3.3.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.3.4"><span class="ltx_text" id="S4.T3.3.4.3.4.1" style="font-size:90%;">3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.5.4.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.3.5.4.1.1">
<tr class="ltx_tr" id="S4.T3.3.5.4.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.5.4.1.1.1.1"><span class="ltx_text" id="S4.T3.3.5.4.1.1.1.1.1" style="font-size:90%;">The issue solution required too much time to</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5.4.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.5.4.1.1.2.1"><span class="ltx_text" id="S4.T3.3.5.4.1.1.2.1.1" style="font-size:90%;">be implemented</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.5.4.2"><span class="ltx_text" id="S4.T3.3.5.4.2.1" style="font-size:90%;">7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.5.4.3"><span class="ltx_text" id="S4.T3.3.5.4.3.1" style="font-size:90%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.5.4.4"><span class="ltx_text" id="S4.T3.3.5.4.4.1" style="font-size:90%;">2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.6.5.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.3.6.5.1.1">
<tr class="ltx_tr" id="S4.T3.3.6.5.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.6.5.1.1.1.1"><span class="ltx_text" id="S4.T3.3.6.5.1.1.1.1.1" style="font-size:90%;">The issue solution was likely to decrease the</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6.5.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.6.5.1.1.2.1"><span class="ltx_text" id="S4.T3.3.6.5.1.1.2.1.1" style="font-size:90%;">performance of the system</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6.5.1.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.6.5.1.1.3.1">
<span class="ltx_text" id="S4.T3.3.6.5.1.1.3.1.1" style="font-size:90%;">(</span><span class="ltx_text ltx_font_italic" id="S4.T3.3.6.5.1.1.3.1.2" style="font-size:90%;">e.g.</span><span class="ltx_text" id="S4.T3.3.6.5.1.1.3.1.3" style="font-size:90%;">, decreasing accuracy)</span>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.6.5.2"><span class="ltx_text" id="S4.T3.3.6.5.2.1" style="font-size:90%;">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.6.5.3"><span class="ltx_text" id="S4.T3.3.6.5.3.1" style="font-size:90%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.6.5.4"><span class="ltx_text" id="S4.T3.3.6.5.4.1" style="font-size:90%;">3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T3.3.7.6.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.3.7.6.1.1">
<tr class="ltx_tr" id="S4.T3.3.7.6.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.7.6.1.1.1.1"><span class="ltx_text" id="S4.T3.3.7.6.1.1.1.1.1" style="font-size:90%;">There was not a tool which automated the</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7.6.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.7.6.1.1.2.1"><span class="ltx_text" id="S4.T3.3.7.6.1.1.2.1.1" style="font-size:90%;">fixing process</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.3.7.6.2"><span class="ltx_text" id="S4.T3.3.7.6.2.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.3.7.6.3"><span class="ltx_text" id="S4.T3.3.7.6.3.1" style="font-size:90%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.3.7.6.4"><span class="ltx_text" id="S4.T3.3.7.6.4.1" style="font-size:90%;">2</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Finally, when we again asked the participants to rate the utility of various hypothetical tools — assuming their team had access to them — the participants rated as the most valuable a tool able to ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.1">[…] help […] monitoring the AI model after its release to the public</span>”, followed by ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.2">best practices that can actively guide your team through the model’s SDLC</span>”, ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.3">tools to help the team in the data pre-processing steps (e.g., decide whether one needs to add/remove data points from your training set, and what kind of data you need to add/remove)</span>”, and ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.4">a knowledge book in which are mapped trustworthiness problems and […] solutions</span>”. On the other hand, they rated as least useful tools ”<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.5">[…] to help your team doing an ex-post TAI audit</span>” and tools able to <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.6">[…] help your team deciding which AI model best respects the TAI principles […]</span>. These results are graphically shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F5" title="Figure 5 ‣ 4.4. Practices in Addressing Untrustworthiness Issues ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="828" id="S4.F5.g1" src="extracted/5598226/images/graph_26_4.png" width="1072"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Perceived usefulness of hypothetical tools to address TAI issues. N/A answers have been removed.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Demographics and background information</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1"><span class="ltx_text" id="S4.SS5.p1.1.1" style="color:#000000;">In terms of demographics of the sample, the study participants’ gender is represented by 68% male, 21% female, 3% non-binary or gender diverse, and 9% preferred not to respond.</span>
Concerning academic qualifications, 56% of the participants have a master’s degree, 41% have earned a Ph.D., and 3% have completed their bachelor’s studies in computer science-related fields. The notable prevalence of advanced education within the respondent pool aligns with our expectations, reflecting the elevated cognitive expertise required by the complexities of this particular field.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">The vast majority of the participants are employed in medium-large companies, specifically, 32.3% (11/34) work for companies with less than 50 employees (small), 23.5%, (8/34) for companies between 50 and 500 employees (medium), while 44.1%, (15/34) are employed in large companies with more than 500 employees.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Participants have on average five years of experience in their role and two years of experience in the AI field.
</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">Regarding the technology area that best describes with which AI products/services the participants work, the four most prevalent are ”<span class="ltx_text ltx_font_italic" id="S4.SS5.p4.1.1">Decision support</span>” (15 answers), ”<span class="ltx_text ltx_font_italic" id="S4.SS5.p4.1.2">Natural Language Processing</span>” (13 answers), ”<span class="ltx_text ltx_font_italic" id="S4.SS5.p4.1.3">Computer Vision / Image Analysis</span>” (12 answers), and ”<span class="ltx_text ltx_font_italic" id="S4.SS5.p4.1.4">Recommender Systems</span>” (9 answers).</p>
</div>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">Finally, regarding the number of AI-enabled projects developed and deployed into a production environment, we observed that most of the participants (19/34, 56%) declared that just a small percentage of the developed projects — from 1 to 30% — are deployed in a production environment, while only 3% of the participants declared that most of the projects — from 90 to 100% — are deployed into a production environment. This reveals the fact that most of these types of projects are still in an experimental stage.</p>
</div>
<div class="ltx_para" id="S4.SS5.p6">
<p class="ltx_p" id="S4.SS5.p6.1"><span class="ltx_text" id="S4.SS5.p6.1.1" style="color:#000000;">Due to space constraints, we have not included tabular representation of demographics in the paper which are, however, all available in the online appendix <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib7" title="">2024a</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="footnote11.1.1.1" style="color:#000000;">11</span></span><span class="ltx_text" id="footnote11.5" style="color:#000000;">Appendix Table A4 (interviewees’ self-reported technology areas and team roles); Table A5 (participants count grouped by company size); Table A6 (participants count grouped by years of experience in their current role and in developing AI-enabled systems)</span></span></span></span></span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Summary of key findings</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">Here we summarize some key findings from our study.</p>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p2.1.1">F1</span>.
<span class="ltx_text" id="S4.SS6.p2.1.2" style="color:#000000;">The study reveals that participants care a lot about <span class="ltx_text ltx_font_bold" id="S4.SS6.p2.1.2.1">Privacy</span> and <span class="ltx_text ltx_font_bold" id="S4.SS6.p2.1.2.2">Transparency</span></span>. Indeed, among the most used strategies to ensure trustworthiness are ”<span class="ltx_text ltx_font_italic" id="S4.SS6.p2.1.3">post-hoc explainability</span>” and ”<span class="ltx_text ltx_font_italic" id="S4.SS6.p2.1.4">algorithm that can best explain the decision</span>” (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F2" title="Figure 2 ‣ 4.2. Practices in Preventing untrustworthiness in AI ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">2</span></a>). In addition, tools that ”<span class="ltx_text ltx_font_italic" id="S4.SS6.p2.1.5">generate an explanation</span>” and that help in deciding ”<span class="ltx_text ltx_font_italic" id="S4.SS6.p2.1.6">the more clear and explainable model</span>” are among the tools perceived as most useful (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F3" title="Figure 3 ‣ 4.2. Practices in Preventing untrustworthiness in AI ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS6.p3">
<p class="ltx_p" id="S4.SS6.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p3.1.1">F2</span>. Acting on the <span class="ltx_text ltx_font_bold" id="S4.SS6.p3.1.2">dataset</span> is one of the most used strategies to solve the found TAI issues. Indeed the most implemented strategies are ”<span class="ltx_text ltx_font_italic" id="S4.SS6.p3.1.3">improving the quality of the dataset (e.g., removing spurious samples, paradoxical values)</span>” and ”<span class="ltx_text ltx_font_italic" id="S4.SS6.p3.1.4">augmenting the dataset (e.g., with artiﬁcial, manually generated data points)</span>”.</p>
</div>
<div class="ltx_para" id="S4.SS6.p4">
<p class="ltx_p" id="S4.SS6.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p4.1.1">F3</span>. Business constraints — like the time required to implement the solution or the unacceptable performance drop — often represent <span class="ltx_text ltx_font_bold" id="S4.SS6.p4.1.2">impediments</span> to implementing trustworthy AI applications (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.T3" title="Table 3 ‣ 4.4. Practices in Addressing Untrustworthiness Issues ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS6.p5">
<p class="ltx_p" id="S4.SS6.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p5.1.1">F4</span>. Analyzing the <span class="ltx_text ltx_font_bold" id="S4.SS6.p5.1.2">tools</span> practitioners lack the most in addressing TAI issues, a need for tools for after-deploy <span class="ltx_text ltx_font_bold" id="S4.SS6.p5.1.3">monitoring</span> and <span class="ltx_text ltx_font_bold" id="S4.SS6.p5.1.4">best practices</span> and TAI <span class="ltx_text ltx_font_bold" id="S4.SS6.p5.1.5">knowledge books</span> that can actively guide a team through the SDLC emerges (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F5" title="Figure 5 ‣ 4.4. Practices in Addressing Untrustworthiness Issues ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS6.p6">
<p class="ltx_p" id="S4.SS6.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p6.1.1">F5</span>. Many TAI projects are developed but not deployed in a production environment, which reveals that in some cases practitioners are still experimenting with this field.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">RQ1. What vision/opinion do practitioners have about untrustworthiness problems and how often do they address them?</span></p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Our findings reveal that the most addressed principle is <span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Privacy</span>, probably because it is contained in various regulations that exist and must necessarily be complied with (<span class="ltx_text ltx_font_italic" id="S5.p2.1.2">e.g.</span>, in Europe the GDPR <cite class="ltx_cite ltx_citemacro_citep">(European Parliament and Council of the European Union, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib19" title="">5 04</a>)</cite>).
<span class="ltx_text ltx_font_bold" id="S5.p2.1.3">Transparency</span> is also often taken into consideration, probably because there are domains where it is a fundamental and unavoidable feature required by the law, such as in Healthcare and Financial Services.
On the other hand, the one less addressed is <span class="ltx_text ltx_font_bold" id="S5.p2.1.4">Fairness</span>, perhaps because there are still no clear and shared regulations for this dimension of trustworthiness and everything is left to the initiative and ethical values of those implementing these systems.
As a result, even large and well-established companies in the industry are often caught up in scandals that damage their reputation and show how even the most popular and widely used algorithms suffer from unfairness<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bloomberg.com/graphics/2023-generative-ai-bias/" title="">https://www.bloomberg.com/graphics/2023-generative-ai-bias/</a></span></span></span>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">It is notable that when we asked practitioners <span class="ltx_text ltx_font_italic" id="S5.p3.1.1">why their team cares about trustworthiness in AI</span>, the motivation ”<span class="ltx_text ltx_font_italic" id="S5.p3.1.2">retain users/avoid losing business</span>” found most disagreement among them. This may reveal that they believe that TAI issues do not lead to losing users and/or business. Moreover, during interviews, it emerged that addressing TAI is, in some cases, even mandatory and not an option.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Finally, our study suggests that TAI is mainly addressed in the early stages of the SDLC. While this is good — since the earlier certain decisions are made, the more effective they are in the design of the final model — this also reveals that practitioners are most likely not aware of tools and best practices to be used in the final stages of the lifecycle. In fact, <span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Deploy</span> is one of the least addressed phases. Indeed, one interviewee also mentioned the need for guidelines on the best cloud provider compliant with TAI practices. These elements allow us to infer that the choice of deployment infrastructure is often left to chance or, in the best case, to routines and/or trust in a specific cloud provider.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1">RQ2. What do practitioners do to address untrustworthiness problems?</span></p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">Based on the answers, it is clear that when practitioners want to solve TAI issues they mainly act on the dataset, as the most implemented strategies are related to ”improving the quality of the dataset” and ”augmenting it with artificial data points”.
As shown by Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F1" title="Figure 1 ‣ 4.1. Preliminary concepts knowledge ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F4" title="Figure 4 ‣ 4.3. Practices in Discovering Untrustworthiness Issues ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">4</span></a>, TAI is mainly addressed in <span class="ltx_text ltx_font_bold" id="S5.p6.1.1">Design</span> (<span class="ltx_text ltx_font_italic" id="S5.p6.1.2">e.g.</span>, examining model’s input features) and <span class="ltx_text ltx_font_bold" id="S5.p6.1.3">Development</span> phase, without disregarding the <span class="ltx_text ltx_font_bold" id="S5.p6.1.4">Monitoring</span> one (<span class="ltx_text ltx_font_italic" id="S5.p6.1.5">e.g.</span>, learning from user feedback).
Answers to question 22<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Q22: Which of the following strategies has your team evaluated, and which strategies were actually implemented?</span></span></span> sustain this trend and highlight that practitioners also give little consideration to the possibility of searching for tools that automate their manual activities, perhaps because they feel safer with manual analyses or because they are not familiar with automatic tools.
Finally, answers to question 24<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Q24: What prevented your team from addressing/ﬁxing these AI trustworthiness issues?</span></span></span> demonstrate that business constraints (<span class="ltx_text ltx_font_italic" id="S5.p6.1.6">e.g.</span>, time, money) often hinder the resolution of TAI issues: if solving a problem takes too long or may cause a slight performance degradation, practitioners tend to not address it.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1"><span class="ltx_text ltx_font_bold" id="S5.p7.1.1">RQ3. What do practitioners desire to better address untrustworthiness problems?</span></p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">What emerged from the data is that the tools most in demand are those that can generate an explanation of the model after its training.
However, literature provides several mature models to explain both traditional ML algorithms — see SHAP <cite class="ltx_cite ltx_citemacro_citep">(Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib36" title="">2017</a>)</cite> and LIME <cite class="ltx_cite ltx_citemacro_citep">(Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib41" title="">2016</a>)</cite> — and neural networks — such as <cite class="ltx_cite ltx_citemacro_citep">(Kenny et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib32" title="">2021</a>)</cite>.
Perhaps these answers could be due either to the unsuitability of such tools for specific issues to be solved or to scarce knowledge about explainability tools landscape, which would also explain the fact that <span class="ltx_text ltx_font_italic" id="S5.p8.1.1">post-hoc explainability</span> strategies are infrequently used (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F2" title="Figure 2 ‣ 4.2. Practices in Preventing untrustworthiness in AI ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">2</span></a>).
On the contrary, the tools deemed less useful are the ones that help decide how much data they need for particular subgroups/subpopulations, probably because there are other factors — not related to TAI requirements — that constrain practitioners while collecting and/or pre-processing the data. For instance, sometimes, collecting more data about a specific sub-group could be simply infeasible or very difficult.</p>
</div>
<div class="ltx_para" id="S5.p9">
<p class="ltx_p" id="S5.p9.1">Furthermore, it is evident that practitioners feel the need for tools to monitor the model after it has been released to the public, they especially express a need for guidelines and a knowledge base to help them in implementing TAI throughout the SDLC.
Moreover, practitioners believe a single tool <span class="ltx_text ltx_font_italic" id="S5.p9.1.1">to perform post-hoc analyses</span> to be of little use, probably because they feel that it is too late to worry about TAI once the system has been released on the market to end users.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Practical implications and recommendations</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Based on the previous discussion, we summarize some key practical implications and recommendations for the AI industry and the AI research community.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">P1.</span> Practitioners should take TAI principles into account throughout the entire SDLC and not just in the early stages, such as the <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.2">Design</span> phase, as shown by Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#S4.F1" title="Figure 1 ‣ 4.1. Preliminary concepts knowledge ‣ 4. Results and findings ‣ Trustworthy AI in practice: an analysis of practitioners’ needs and challenges"><span class="ltx_text ltx_ref_tag">1</span></a> and answers to question 18<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>Q18: Please indicate if you use one of the following strategies to discover trustworthiness issues.</span></span></span>. Even if valuable mitigation measures can be put in place early in SDLC, there is a huge amount of mitigations that can only be implemented in the latter stages of the SDLC. For instance, using a tool that helps to choose the best cloud provider to be TAI-compliant.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">P2.</span> Our study reveals that, on numerous occasions, even when TAI issues are identified, they are not addressed by practitioners — either directly or by third parties — due to business constraints such as limited time, financial constraints, or declining performance. Nevertheless, the oversight of these issues may result in significant economic and reputational consequences, incurring substantial costs for companies. Hence, it is imperative for the industry to commit to addressing TAI issues detected at various stages of the SDLC. In addition, such actions are crucial for compliance with emerging regulations, such as the AI Act <cite class="ltx_cite ltx_citemacro_citep">(European Union, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib20" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">P3.</span> As a general remark, there is a pressing need for <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.2">guidelines</span>, <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.3">knowledge bases</span>, and <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.4">tools</span> that can help practitioners implement TAI principles throughout the entire SDLC. They need guidance and practical advice on which tools to use at each stage of SDLC and to address which principles. Often, as pointed out in our study, although these tools exist, practitioners may not be aware of them. Moreover, as pointed out in our previous work <cite class="ltx_cite ltx_citemacro_citep">(Barletta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib9" title="">2023</a>)</cite>, there is a significant gap that should be filled between high-level AI ethics principles and low-level concrete practices for practitioners.
For this reason, as a research community, we should rethink how to design these guidelines and best practices, so that they are readily available and usable by professionals and provide actionable guidelines that can be put into practice while implementing trustworthy AI applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Threats to validity</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we discuss the threats to the validity <cite class="ltx_cite ltx_citemacro_citep">(Wohlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib49" title="">2012</a>)</cite> of our study.
We delineate the threats to validity and constraints on the outcomes of our study arising from the research methodology we employed.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_italic" id="S6.p2.1.1">Construct validity</span>
The thematic analysis was executed by two researchers, introducing the potential for subjective judgment. To address this concern, we implemented the negotiated agreement technique <cite class="ltx_cite ltx_citemacro_citep">(Spadini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib44" title="">2018</a>)</cite> between the first and second researcher, fostering consensus, which was achieved after a careful examination of 25 comments.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_italic" id="S6.p3.1.1">Internal validity</span>. Threats to internal validity pertain to unconsidered factors that might impact the variables and relationships under scrutiny. In our investigation, we conducted interviews with AI practitioners to gain insights into their perspectives on Trustworthy AI (TAI) issues. Each practitioner possesses a distinct background and ethical standpoint, potentially diverging from the practices of their peers. We sought to mitigate this issue by interviewing practitioners from diverse companies and different countries. Furthermore, interviewees’ viewpoints may be influenced by additional factors, such as existing literature on TAI, potentially leading to social desirability bias <cite class="ltx_cite ltx_citemacro_citep">(Furnham, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib21" title="">1986</a>)</cite>, or practices adopted in company projects they are involved in. To counteract this, we consistently reminded interviewees that the discussion focused specifically on the TAI issues they encounter in their daily work. At the conclusion of the interviews, we encouraged them to freely express their broader thoughts on TAI.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_italic" id="S6.p4.1.1">Generalizability – Transferability</span>. One notable threat lies in the generalizability of findings, as our sample, albeit diverse, may not fully represent the broader population of professionals engaged in AI development. To mitigate this threat, we solicited opinions from a heterogeneous participants sample: we took practitioners from corporates, characterized by different sizes
with different years of experience — both in their current role and in AI-enabled systems development;
with different genders, level of education, and job roles; working in diverse application domains and technology areas. All details can be found in the online appendix <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib7" title="">2024a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this study, we conducted think-aloud <span class="ltx_text ltx_font_bold" id="S7.p1.1.1">interviews</span> and a <span class="ltx_text ltx_font_bold" id="S7.p1.1.2">survey</span> with <span class="ltx_text ltx_font_bold" id="S7.p1.1.3">34</span> AI practitioners to explore how they handle and implement Trustworthy AI applications.
The study highlights how, among the TAI principles considered, practitioners mainly focus on <span class="ltx_text ltx_font_bold" id="S7.p1.1.4">Privacy</span> at the expense of <span class="ltx_text ltx_font_bold" id="S7.p1.1.5">Fairness</span>, even if many practitioners acknowledge to be aware of how important this last dimension is.
Noteworthy is the fact that half of the participants stated that they did not fix TAI issues after discovering them in their projects. Indeed only half of them declared to have addressed the issue, either by themselves (35%) or revolving to a third party (15%).
From the study it also emerged that TAI is mainly addressed in the initial phases of the SDLC (mainly during the <span class="ltx_text ltx_font_bold" id="S7.p1.1.6">Design</span> and <span class="ltx_text ltx_font_bold" id="S7.p1.1.7">Development</span>) and few practitioners declared to also address it in later stages, like <span class="ltx_text ltx_font_bold" id="S7.p1.1.8">Deploy</span>.
The strategies most employed to build TAI applications focus on data quality enhancement or choosing the most self-explanatory algorithm. Moreover, data and algorithm design are conducted by exploiting manual analysis, without using any automated tool. As one could expect, the most common cause of obstruction in implementing TAI is business constraints (<span class="ltx_text ltx_font_italic" id="S7.p1.1.9">e.g.</span>, time, money).</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Finally, we also identified that practitioners feel the need for tools to <span class="ltx_text ltx_font_bold" id="S7.p2.1.1">monitor</span> the model after production deployment, and
<span class="ltx_text ltx_font_bold" id="S7.p2.1.2">knowledge bases</span> and <span class="ltx_text ltx_font_bold" id="S7.p2.1.3">actionable guideline</span>s to help them implement trustworthiness throughout the entire SDLC.
While different frameworks, tools, and guidelines may exist <cite class="ltx_cite ltx_citemacro_citep">(Barletta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.12135v1#bib.bib9" title="">2023</a>)</cite>, these are either composed of high-level statements that are sometimes difficult to translate into concrete implementation strategies or may be unknown to AI practitioners (as shown by all the answers to the questions related to the need/usefulness of tools to address TAI issue). As future directions, we intend to complement and expand this work with more interviews with a larger sample of AI practitioners to investigate some of the critical points that emerged in this study more in-depth.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This study has been partially supported by the following projects: SSA (Secure Safe Apulia - Regional Security Center, Codice Progetto 6ESURE5) and KEIRETSU (Codice Progetto V9UFIL5) funded by ”Regolamento regionale della Puglia per gli aiuti in esenzione n. 17 del 30/09/2014; SERICS (PE00000014) under the MUR National Recovery and Resilience Plan funded by the European Union – NextGenerationEU.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bay (2018)</span>
<span class="ltx_bibblock">
2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Wagner, B. in Being Profiled: Cogitas Ergo Sum. 10 Years of ‘Profiling the European Citizen‘</em>.

</span>
<span class="ltx_bibblock">Amsterdam University Press.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mediarep.org/handle/doc/14277" title="">https://mediarep.org/handle/doc/14277</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">aequitas.dssg.io (2018)</span>
<span class="ltx_bibblock">
aequitas.dssg.io 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">aequitas.dssg.io</em>.

</span>
<span class="ltx_bibblock">
Retrieved Sep 20, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://aequitas.dssg.io/" title="">http://aequitas.dssg.io/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alvarado and Wærn (2018)</span>
<span class="ltx_bibblock">
O. Alvarado and A. Wærn. 2018.

</span>
<span class="ltx_bibblock">Towards Algorithmic Experience: Initial Efforts for Social Media Contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em> (2018).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:5053741" title="">https://api.semanticscholar.org/CorpusID:5053741</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bakeman and Quera (2011)</span>
<span class="ltx_bibblock">
Roger Bakeman and Vicenç Quera. 2011.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Observer Agreement and Cohen’s Kappa</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 57–71.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baldassarre et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
M.T. Baldassarre, D. Caivano, B. Fernandez Nieto, D. Gigante, and A. Ragone. 2023.

</span>
<span class="ltx_bibblock">The Social Impact of Generative AI: An Analysis on ChatGPT. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2023 ACM Conference on Information Technology for Social Good</em> (Lisbon, Portugal) <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(GoodIT ’23)</em>. Association for Computing Machinery, New York, NY, USA, 363–373.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baldassarre et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
M. T. Baldassarre, D. Gigante, M. Kalinowski, A. Ragone, and S. Tibidò. 2024a.

</span>
<span class="ltx_bibblock">Online appendix.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">https://figshare.com/s/099d7f7a0b2da08653e6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baldassarre et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
M. T. Baldassarre, D. Gigante, M. Kalinowski, A. Ragone, and S. Tibidò. 2024b.

</span>
<span class="ltx_bibblock">Survey link.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">https://forms.office.com/e/GVeeWf1Pqz.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barletta et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
V.S. Barletta, D. Caivano, D. Gigante, and A. Ragone. 2023.

</span>
<span class="ltx_bibblock">A Rapid Review of Responsible AI Frameworks: How to Guide the Development of Ethical AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering</em> (Oulu, Finland) <em class="ltx_emph ltx_font_italic" id="bib.bib9.4.2">(EASE ’23)</em>. Association for Computing Machinery, New York, NY, USA, 358–367.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3593434.3593478" title="">https://doi.org/10.1145/3593434.3593478</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benjamini and Hochberg (1995)</span>
<span class="ltx_bibblock">
Yoav Benjamini and Yosef Hochberg. 1995.

</span>
<span class="ltx_bibblock">Controlling the false discovery rate: a practical and powerful approach to multiple testing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Journal of the Royal statistical society: series B (Methodological)</em> 57, 1 (1995), 289–300.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bosch et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Nigel Bosch, Sidney K. D’Mello, R. Baker, Jaclyn L. Ocumpaugh, Valerie J. Shute, Matthew Ventura, Lubin Wang, and Weinan Zhao. 2016.

</span>
<span class="ltx_bibblock">Detecting Student Emotions in Computer-Enabled Classrooms. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">International Joint Conference on Artificial Intelligence</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:14637074" title="">https://api.semanticscholar.org/CorpusID:14637074</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun and Clarke (2006)</span>
<span class="ltx_bibblock">
V. Braun and V. Clarke. 2006.

</span>
<span class="ltx_bibblock">Using thematic analysis in psychology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Qualitative Research in Psychology</em> 3 (01 2006), 77–101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun and Clarke (2012)</span>
<span class="ltx_bibblock">
V. Braun and V. Clarke. 2012.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Thematic analysis.</em>
</span>
<span class="ltx_bibblock">57–71.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bucher (2017)</span>
<span class="ltx_bibblock">
Taina Bucher. 2017.

</span>
<span class="ltx_bibblock">The algorithmic imaginary: exploring the ordinary affects of Facebook algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Information, Communication &amp; Society</em> 20, 1 (2017), 30–44.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chouldechova (2016)</span>
<span class="ltx_bibblock">
Alexandra Chouldechova. 2016.

</span>
<span class="ltx_bibblock">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Big data</em> 5 2 (2016), 153–163.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cruzes and Dybå (2011)</span>
<span class="ltx_bibblock">
Daniela Cruzes and Tore Dybå. 2011.

</span>
<span class="ltx_bibblock">Recommended Steps for Thematic Synthesis in Software Engineering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">International Symposium on Empirical Software Engineering and Measurement</em>, 275 – 284.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ESEM.2011.36" title="">https://doi.org/10.1109/ESEM.2011.36</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">enisaml (2021)</span>
<span class="ltx_bibblock">
enisaml 2021.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Securing Machine Learning Algorithms</em>.

</span>
<span class="ltx_bibblock">
Retrieved Sep 20, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms" title="">https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esteva et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin M. Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun. 2017.

</span>
<span class="ltx_bibblock">Dermatologist-level classification of skin cancer with deep neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Nature</em> 542 (2017), 115–118.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Parliament and Council of the European Union (5 04)</span>
<span class="ltx_bibblock">
European Parliament and Council of the European Union. 2016-05-04.

</span>
<span class="ltx_bibblock">Regulation (EU) 2016/679 of the European Parliament and of the Council.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://data.europa.eu/eli/reg/2016/679/oj" title="">https://data.europa.eu/eli/reg/2016/679/oj</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Union (2023)</span>
<span class="ltx_bibblock">
European Union. 2023.

</span>
<span class="ltx_bibblock">AI Act.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence" title="">https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Furnham (1986)</span>
<span class="ltx_bibblock">
Adrian Furnham. 1986.

</span>
<span class="ltx_bibblock">Response bias, social desirability and dissimulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Personality and Individual Differences</em> 7 (1986), 385–400.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:144370219" title="">https://api.semanticscholar.org/CorpusID:144370219</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Mistake (2024)</span>
<span class="ltx_bibblock">
Gemini Mistake 2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Gemini image generation got it wrong. We’ll do better.</em>
</span>
<span class="ltx_bibblock">
Retrieved Mar 13, 2024 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="%7B%7Bhttps://blog.google/products/gemini/gemini-image-generation-issue%7D%7D" title="">{{https://blog.google/products/gemini/gemini-image-generation-issue}}</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">George and Mallery (2018)</span>
<span class="ltx_bibblock">
Darren George and Paul Mallery. 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">IBM SPSS Statistics 25 Step by Step: A Simple Guide and Reference</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greene et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Daniel Greene, Anna Lauren Hoffmann, and Luke Stark. 2019.

</span>
<span class="ltx_bibblock">Better, Nicer, Clearer, Fairer: A Critical Assessment of the Movement for Ethical Artificial Intelligence and Machine Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Hawaii International Conference on System Sciences</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heckathorn (2011)</span>
<span class="ltx_bibblock">
Douglas D. Heckathorn. 2011.

</span>
<span class="ltx_bibblock">Comment: Snowball versus Respondent-Driven Sampling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Sociological Methodology</em> 41 (2011), 355 – 366.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:117640237" title="">https://api.semanticscholar.org/CorpusID:117640237</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">High-Level Expert Group on AI (2018a) (AIHLEG)</span>
<span class="ltx_bibblock">High-Level Expert Group on AI (AIHLEG). 2018a.

</span>
<span class="ltx_bibblock">Ethics guidelines for trustworthy AI | Shaping Europe’s digital future.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai" title="">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">High-Level Expert Group on AI (2018b) (AIHLEG)</span>
<span class="ltx_bibblock">High-Level Expert Group on AI (AIHLEG). 2018b.

</span>
<span class="ltx_bibblock">High-Level Expert Group on AI (AIHLEG).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://digital-strategy.ec.europa.eu/en/policies/expert-group-ai" title="">https://digital-strategy.ec.europa.eu/en/policies/expert-group-ai</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HireVue.com (2019)</span>
<span class="ltx_bibblock">
HireVue.com 2019.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">HireVue.com</em>.

</span>
<span class="ltx_bibblock">
Retrieved Sep 20, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.hirevue.com/" title="">https://www.hirevue.com/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holborn Law LLC (2018)</span>
<span class="ltx_bibblock">
Holborn Law LLC. 2018.

</span>
<span class="ltx_bibblock">Advisory Council on the Ethical Use of Artificial Intelligence and Data in Singapore.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cms-holbornasia.law/en/sgh/publication/singapore-to-form-advisory-council-for-ethical-use-of-ai" title="">https://www.cms-holbornasia.law/en/sgh/publication/singapore-to-form-advisory-council-for-ethical-use-of-ai</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holstein et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Kenneth Holstein, Bruce M. McLaren, and Vincent Aleven. 2018.

</span>
<span class="ltx_bibblock">Student Learning Benefits of a Mixed-Reality Teacher Awareness Tool in AI-Enhanced Classrooms. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">International Conference on Artificial Intelligence in Education</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jobin et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
A. Jobin, M. Ienca, and E. Vayena. 2019.

</span>
<span class="ltx_bibblock">The global landscape of AI ethics guidelines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Nature Machine Intelligence</em> 1, 9 (01 Sep 2019), 389–399.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s42256-019-0088-2" title="">https://doi.org/10.1038/s42256-019-0088-2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenny et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Eoin M. Kenny, Courtney Ford, Molly Quinn, and Mark T. Keane. 2021.

</span>
<span class="ltx_bibblock">Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Artificial Intelligence</em> 294 (2021), 103459.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kupis et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Julia Kupis, Sydney Johnson, Gregory Hallihan, and Dana Lee Olstad. 2019.

</span>
<span class="ltx_bibblock">Assessing the Usability of the Automated Self-Administered Dietary Assessment Tool (ASA24) among Low-Income Adults.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Nutrients</em> 11, 1 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Singh (2021)</span>
<span class="ltx_bibblock">
Michelle Seng Ah Lee and Jat Singh. 2021.

</span>
<span class="ltx_bibblock">The Landscape and Gaps in Open Source Fairness Toolkits. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (Yokohama, Japan) <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">(CHI ’21)</em>. Association for Computing Machinery, New York, NY, USA, Article 699, 13 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lum and Isaac (2016)</span>
<span class="ltx_bibblock">
Kristian Lum and William Isaac. 2016.

</span>
<span class="ltx_bibblock">To Predict and Serve?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Significance</em> 13 (10 2016), 14–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lundberg and Lee (2017)</span>
<span class="ltx_bibblock">
Scott M Lundberg and Su-In Lee. 2017.

</span>
<span class="ltx_bibblock">A Unified Approach to Interpreting Model Predictions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems 30</em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 4765–4774.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NASA Artificial Intelligence Group (2018)</span>
<span class="ltx_bibblock">
NASA Artificial Intelligence Group. 2018.

</span>
<span class="ltx_bibblock">NASA Artificial Intelligence Group.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.jpl.nasa.gov/" title="">https://ai.jpl.nasa.gov/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pardo et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
César Pardo, Francisco Pino, Félix García, Francisco Romero Romero, Mario Piattini, and Maria Teresa Baldassarre. 2011.

</span>
<span class="ltx_bibblock">HProcessTOOL: A support tool in the harmonization of multiple reference models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</em> 6786 LNCS, PART 5 (2011), 370 – 382.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-642-21934-4_30" title="">https://doi.org/10.1007/978-3-642-21934-4_30</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">pymetrics.ai (2018)</span>
<span class="ltx_bibblock">
pymetrics.ai 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">pymetrics.ai</em>.

</span>
<span class="ltx_bibblock">
Retrieved Sep 20, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pymetrics.ai//" title="">https://www.pymetrics.ai//</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rader and Gray (2015)</span>
<span class="ltx_bibblock">
Emilee J. Rader and Rebecca M Gray. 2015.

</span>
<span class="ltx_bibblock">Understanding User Beliefs About Algorithmic Curation in the Facebook News Feed.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016.

</span>
<span class="ltx_bibblock">”Why Should I Trust You?”: Explaining the Predictions of Any Classifier. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016</em>. 1135–1144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richardson et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Brianna Richardson, Jean I. Garcia-Gathright, Samuel F. Way, Jennifer Thom-Santelli, and Henriette Cramer. 2021.

</span>
<span class="ltx_bibblock">Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Someren et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (1994)</span>
<span class="ltx_bibblock">
Maarten Someren, Yvonne Barnard, and Jacobijn Sandberg. 1994.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">The Think Aloud Method - A Practical Guide to Modelling CognitiveProcesses</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spadini et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Davide Spadini, Maurício Aniche, Margaret-Anne Storey, Magiel Bruntink, and Alberto Bacchelli. 2018.

</span>
<span class="ltx_bibblock">When Testing Meets Code Review: Why and How Developers Review Tests. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the 40th International Conference on Software Engineering</em> (Gothenburg, Sweden) <em class="ltx_emph ltx_font_italic" id="bib.bib44.4.2">(ICSE ’18)</em>. Association for Computing Machinery, New York, NY, USA, 677–687.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">UK AI Council (2019)</span>
<span class="ltx_bibblock">
UK AI Council. 2019.

</span>
<span class="ltx_bibblock">UK Government.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.gov.uk/government/groups/ai-council" title="">https://www.gov.uk/government/groups/ai-council</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">UNESCO Ad Hoc Expert Group (2021) (AHEG)</span>
<span class="ltx_bibblock">UNESCO Ad Hoc Expert Group (AHEG). 2021.

</span>
<span class="ltx_bibblock">Recommendation on the Ethics of Artificial Intelligence.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.unesco.org/en/artificial-intelligence/recommendation-ethics" title="">https://www.unesco.org/en/artificial-intelligence/recommendation-ethics</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veale et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Michael Veale, Max Van Kleek, and Reuben Binns. 2018.

</span>
<span class="ltx_bibblock">Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em> (Montreal QC, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib47.4.2">(CHI ’18)</em>. Association for Computing Machinery, New York, NY, USA, 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">whatiftool (2018)</span>
<span class="ltx_bibblock">
whatiftool 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">The What-If Tool: Code-free probing of machine learning models</em>.

</span>
<span class="ltx_bibblock">
Retrieved Sep 20, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html" title="">https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wohlin et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and Anders Wesslén. 2012.

</span>
<span class="ltx_bibblock">Experimentation in software engineering. Springer Science &amp; Business Media.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhengxin et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fang Zhengxin, Yuan Yi, Zhang Jingyu, Liu Yue, Mu Yuechen, Luan Qinghua, Xu Xiwei, Wang Jeff, Wang Chen, Zhang Shuai, and Chen Shiping. 2023.

</span>
<span class="ltx_bibblock">MLOps Spanning Whole Machine Learning Life Cycle: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">ArXiv</em> abs/2304.07296 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May 15 13:00:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
