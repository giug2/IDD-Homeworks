<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model</title>
<!--Generated on Mon Sep 23 07:55:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14784v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S1" title="In SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S2" title="In SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3" title="In SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">SAMEdge Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3.SS1" title="In III SAMEdge Architecture ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">SAM Workload Analysis and Partition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3.SS2" title="In III SAMEdge Architecture ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">The SAMEdge Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3.SS3" title="In III SAMEdge Architecture ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">High Accuracy SAM with Communication and Computation Resource Constraints</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3.SS3.SSS1" title="In III-C High Accuracy SAM with Communication and Computation Resource Constraints ‚Ä£ III SAMEdge Architecture ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span>Visual Prompts Transformation Problem</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3.SS3.SSS2" title="In III-C High Accuracy SAM with Communication and Computation Resource Constraints ‚Ä£ III SAMEdge Architecture ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span>Image Encoding Workload Partition Problem</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4" title="In SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">A Case Study on Visual Tour Guide</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS1" title="In IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Implementation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS1.SSS1" title="In IV-A Implementation ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Prototype implementation:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS1.SSS2" title="In IV-A Implementation ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>VTG Implementation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS2" title="In IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Evaluations</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS2.SSS1" title="In IV-B Evaluations ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span>Evaluation Setup</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS3" title="In IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Experiment Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS3.SSS1" title="In IV-C Experiment Results ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>1 </span>Improvement of Analytics Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS3.SSS2" title="In IV-C Experiment Results ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>2 </span>Improvement of Analytics Accuracy Impacted by Network Bandwidth</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.SS3.SSS3" title="In IV-C Experiment Results ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>3 </span>Case study of SAMEdge on the KITTI dataset.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S5" title="In SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Rui Lu1,
Siping Shi1,
Yanting Liu2,
Dan Wang1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
<br class="ltx_break"/>Department of Computing, The Hong Kong Polytechnic University
<br class="ltx_break"/>Email:1{csrlu, cssshi, csdwang}@comp.polyu.edu.hk, 2ulysses.liu@connect.polyu.hk

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">As artificial intelligence continues to evolve, it is increasingly capable of handling a wide range of video analytics tasks with merely one large model. One of the key foundation technologies is the Segment Anything Model (SAM), which allows the video analytics tasks to be determined on the fly according to the input prompts from the user. However, achieving real-time response in video analytics applications is crucial for user experiences due to the limited communication and computation resources on the edge, especially with SAM, where users may continuously interact by adding or adjusting prompts.</p>
<p class="ltx_p" id="id2.id2">In this paper, we propose SAMEdge, a novel edge-cloud computing architecture designed to support SAM computations for edge users. SAMEdge integrates new modules on the edge and the cloud to maximize analytics accuracy under visual prompts and image prompts input with latency constraints. It addresses resource challenges associated with prompt encoding and image encoding by offering a visual prompt transformation algorithm for visual prompts and efficient workload partitioning for image encoding.
SAMEdge is implemented by extending the open-source SAM project from Meta AI. We demonstrate the practical application of SAMEdge through a case study on a Visual Tour Guide application. Our evaluation indicates that SAMEdge significantly enhances the accuracy of the video analytics application under distinct network bandwidths across various prompts.</p>
</div>
<figure class="ltx_figure ltx_minipage ltx_align_center ltx_align_top" id="S0.F1.fig1" style="width:411.9pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S0.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="166" id="S0.F1.sf1.g1" src="extracted/5870326/fig/sam_1.png" width="131"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S0.F1.sf1.3.2" style="font-size:80%;">Model training</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S0.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="166" id="S0.F1.sf2.g1" src="extracted/5870326/fig/sam_2.png" width="253"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S0.F1.sf2.3.2" style="font-size:80%;">Model inference with offline image encoding</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S0.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="166" id="S0.F1.sf3.g1" src="extracted/5870326/fig/sam_3.png" width="245"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.sf3.2.1.1" style="font-size:80%;">(c)</span> </span><span class="ltx_text" id="S0.F1.sf3.3.2" style="font-size:80%;">Model inference with online image encoding</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The SAM computing architecture: (a) Model training, (b) Pre-recorded videos and online prompts; offline image encoding (c) Online videos and online prompts; online image encoding</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Currently, artificial intelligence (AI) is evolving towards artificial general intelligence (AGI), which refers to the ability of an AI model to perform a wide range of tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib1" title="">1</a>]</cite>. One of the key foundation technologies is the segment anything model (SAM), introduced by Meta AI Research¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib2" title="">2</a>]</cite>, considered the "GPT Moment" in the realm of computer vision¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib3" title="">3</a>]</cite>. It can segment any nominated objects on a certain image by giving a <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">prompt</span>, i.e., the instructions of a specific downstream task. Typical prompts include <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">visual prompts</span>, i.e., points, boxes, and scribbles; and <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">image prompts</span>, i.e., reference images. This advanced scheme allows the video tasks to be determined on-the-fly according to the input prompts from users.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">One of the typical applications is <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Media Analytics Services (MAS):</span> A media analytics service provider allows media analytics on a video stream of interest (e.g., the BBC animal series). Users can give prompts on the tasks that they want to perform out of this video stream. For example, a user may want to find the predator animals of Africa. He can either view a few frames and input visual prompts or he can upload image prompts of predators (e.g., lions, leopards). The system will return the frames with predators from the video stream of interest. Downstream tasks such as predictor counting can also be developed. Note that in conventional systems, a specific neural network (NN) model of predator detection needs to be trained (and a set of labeled images dedicated to this task should be constructed).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Existing SAM architectures deployed on edges straightforwardly have two major challenges, including communication and computation resource constraints on edge devices.
Communication resource consumption occurs in video and embedding transmission. Existing architectures either pre-encoding the third-party videos into embeddings on the cloud and transmit them to edge, like MAS, or users upload the video they captured to the cloud and get embedding returns. These transmissions will cost significant network bandwidth requirements. For example, the Meta AI SAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib2" title="">2</a>]</cite> requires 3.7 seconds to upload a 1080p image and 1.2 seconds to download its embedding under 20 Mbps bandwidth, posing challenges for real-time response in edge user scenarios.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Computation resource consumption occurs during the encoding of prompts and images when going through <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">prompt encoding</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">image encoding</span> to generate prompt embeddings and video embeddings, respectively. Those embeddings are then used in <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">embedding decoding</span> to generate analytics results. The new computing workloads arising are those associated with and incurred by prompts. There are new computational challenges when they are executed in the edge: (1) there can be multiple visual prompts; and decoding these prompts one-by-one leads to significant resource consumption; (2) image prompt encoding and image encoding request for workload offloading to the cloud; and these two modules can have contentions on the computing and communication resources.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we introduce SAMEdge, a novel edge-cloud computing architecture designed to support the Segment Anything Model (SAM) computations for edge users. SAMEdge extends the capabilities of Meta AI SAM by incorporating new modules on the edge and the cloud. Our work involves the development of innovative algorithms, specifically targeting visual prompt transformation and workload partitioning. The primary objective of these algorithms is to optimize analytics accuracy with visual and image prompt input under resources and latency constraints.
To validate our approach, we implement a prototype that seamlessly integrates our research findings into the open-source SAM system developed by Meta AI. Furthermore, we showcase the practical application of SAMEdge by creating a Visual Tour Guide (VTG) application, serving as a visual co-pilot for tourists.
Our experiments utilize Nvidia Jetson Nano as a testbed, and we conduct a comprehensive performance evaluation of SAMEdge against three comparison methods: a conventional SAM approach, a knowledge distillation SAM method, and a super-resolution method. The results reveal that SAMEdge significantly enhances video analytics accuracy, achieving up to a 4.64 times and 1.81 times improvement with visual and image prompt inputs, respectively.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, the contributions of this paper are:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We develop a new edge-cloud computing architecture SAMEdge, with new modules on the edge and the cloud to support SAM computing for edge users.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We develop new algorithms, specifically, new visual prompt transformation algorithms and new workload partitioning algorithms, with the objective of optimizing the analytics accuracy of SAMEdge.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We implement a prototype to integrate our research results and show how the SAMEdge architecture can be landed into practice through a case study on a visual tour guide application. Our SAMEdge system can efficiently adapt to prompt dynamics based on network changes.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Architecture</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Video analytics systems refer to the systems that infer important events in video streams. These systems feed video frames into an NN model that is well-trained for a specific task (e.g., vehicle detection) and conduct model inferences.
However, with the recent revolution of large foundation model technologies, a SAM architecture allows video tasks to be determined on the fly according to users‚Äô input prompts.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In a video analytics application, achieving real-time responses is crucial for user experiences, especially in SAM when users want to interact by adding or adjusting prompts. However, the limited resources on the edge make uploading all videos to the cloud impractical due to considerable bandwidth demands, especially in dynamic and complex wireless network environments. For example, transmitting a 1080p image to the cloud for image encoding and receiving embedding returns requires 4.9 seconds.
Additionally, video transmission raises concerns about privacy leakage risks. Complete offloading SAM on edge is also challenging. For instance, SAM demands significantly more computational resources, around 10 times more than a task-specific model like YOLOv8 designed for object detection.
One viable approach is the adoption of edge-cloud video analytics systems. In this paradigm, the edge device executes partial analytics processes, offering benefits such as enhanced privacy control, real-time responsiveness, and reduced communication overhead.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">We comment that there are architectures for deploying SAM on resource-constraint edge devices. Existing architectures based on super-resolution¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib4" title="">4</a>]</cite>, NN feature selection¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib5" title="">5</a>]</cite>, etc., are employed to decrease the transmission data size to accommodate the constraint and dynamic communication resources on the edge. Others, like NN model compression technologies¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib6" title="">6</a>]</cite>, reduce the NN size by model quantization, model pruning, knowledge distillation, etc., to adapt to the limited edge computational resource.
SAMEdge differs since it does not modify the weights or network structure of the SAM. SAMEdge dynamically offloads the workload of conventional, well-trained SAM according to the computation and communication resources of edge devices. SAMEdge is developed based on the Meta AI SAM project, the first to propose a promptable segmentation system with zero-shot generalization. SAMEdge differs from Meta AI SAM in that SAMEdge supports additional prompt categories with the new capability of workload offloading management.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">SAMEdge Architecture</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">SAM Workload Analysis and Partition</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To investigate resource bottlenecks in SAM, we first analyze the workload of the conventional SAM architecture. A typical <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">SAM computing architecture</span> is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S0.F1.fig1" title="Figure 1 ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">1</span></a> (e.g., the SAM project of Meta AI). Three NN models are pre-trained in a SAM system (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S0.F1.fig1" title="Figure 1 ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">1</span></a> (a)), an image encoder (i.e., the LVM), a prompt encoder, and an embedding decoder. Depending on whether the video stream of interest is pre-recorded or not, there are two cases: (1) pre-recorded (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S0.F1.fig1" title="Figure 1 ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">1</span></a> (b)); for example, the video stream comes from a third-party provider (e.g., media analytics services (MAS)): the image encoder will offline take the video stream of interest (e.g., the BBC animal series) and LVM as inputs and output an embedding of this video stream. In runtime, users input prompts to the prompt encoder, which outputs prompt embedding. The embedding decoder will take the prompt embedding and the video embedding to output analytics results; and (2) on-the-fly (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S0.F1.fig1" title="Figure 1 ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">1</span></a> (c)): for example, the video stream comes from a user (e.g., a video tour guide (VTG)): the user inputs both the prompts and the video stream of interest. The image encoder will online output video embedding.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We conducted a brief analysis of the computation resource requirements within each NN model in the SAM structure. Notably, the resource requirements are approximately ten times different for the visual prompt encoder model compared to the embedding decoder model. And when image prompts and video encoding are on edge, their resource consumption dominates.
This result reveals that: 1) Each visual prompt input in SAM necessitates a single embedding decoder for decoding. Given that the embedding decoder incurs ten times more resources than the visual prompt encoder, the bottleneck in resource consumption arises from the embedding decoder. Managing prompts judiciously becomes crucial, offering potential reductions in the number of embedding decoder executions.
2) When utilizing images as prompts and capturing video to obtain video encoding at the edge, the primary contributors to resource consumption are the image encoder and image prompt encoder. It becomes necessary to implement a joint workload partition between the edge and the cloud that involves the image prompt encoder and the image encoder.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="320" id="S3.F2.g1" src="x1.png" width="394"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The architecture of SAMEdge.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">The SAMEdge Architecture</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Since the conventional SAM architecture (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S0.F1.fig1" title="Figure 1 ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">1</span></a>) is not developed for the edge. If bringing such an architecture to the edge directly, it cannot effectively work in a real-time and resource-constrained manner. We develop a new edge-cloud computing architecture SAMEdge (Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3.F2" title="Figure 2 ‚Ä£ III-A SAM Workload Analysis and Partition ‚Ä£ III SAMEdge Architecture ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">2</span></a>). SAMEdge is logically divided into the edge side and the cloud side. SAMEdge has two additional modules:
(1) a <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">visual prompt transformation module</span> to combine and convert visual prompts; and (2) a <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">resource-aware workload partition module</span> to split the image prompt encoder and image encoder.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Specifically, users initiate SAMEdge by inputting their videos and prompts, including images and visual prompts, into the edge device. Visual prompts undergo initial processing in the Visual Prompt Transformation Module, resulting in transformed visual prompts. Subsequently, the Visual Prompt Encoder refines these prompts, generating visual prompt embeddings.
Simultaneously, custom video frames and image prompts are directed to the Image Encoder and Image Prompt Encoder, respectively, inside the Resource-Aware Workload Partition Module. The partitioning controller of this module dynamically determines splitting points based on real-time edge resource availability.
These splitting points dictate the workload division, with a portion processed locally on the edge and the remainder offloaded to a powerful computing cloud server. The encoder models will exit early at the splitting points and transmit the intermediate results to the computing cloud for further processing.
The cloud continues executing subsequent layers and returns the computed video embeddings and image prompts embeddings to the edge. All the embedding, i.e., video embeddings, image prompts embeddings, and visual prompt embeddings, will feed into the Embedding Decoder and obtain analytics results.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">High Accuracy SAM with Communication and Computation Resource Constraints</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">To design SAMEdge that supports SAM on edge devices with constraint communication and computation resources for edge users, the objective is to maximize the video analytics accuracy under those resource and latency constraints. Specifically, we define
<span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">SAMEdge Resource Optimization (SERO) Problem:</span>
Given video frames and prompts, well-trained NN models of SAM, the computation resources at the edge and communication resources between the edge and the cloud, and a maximal latency, determine the resolutions of the video frames and the prompt images, the workload offloading on edge, and the data transmission to the cloud, so that the latency is guaranteed and the analytics accuracy is maximized.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The SERO problem exhibits a Knapsack structure, where the maximum latency constraints, layers executed on the edge, computation time on the edge, and communication time for a given SAM model can be similarly considered as the capacity of the knapsack, the weight of the item, and the value of the item, respectively. SERO is equivalent to a Knapsack problem which has been proved as NP-hard, making it impractical to find a globally optimal solution within polynomial time.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">To address the SERO problem, we divided it into two subproblems based on distinct bottlenecks arising from different categories of prompts, as detailed in Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S3" title="III SAMEdge Architecture ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">III</span></a>(A): <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.1">Visual Prompts Transformation Problem</span> and <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.2">Image Encoding Workload Partition Problem</span>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.5.1.1">III-C</span>1 </span>Visual Prompts Transformation Problem</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Prompts are assumed to be transformed into a new set with fewer prompts for conciseness through two operations: 1) combining visual prompts to reduce redundancy, and 2) converting prompt types (e.g., from points to boxes). This reduction in prompts not only decreases the number of prompts but also lowers the latency of the embedding decoder, with an impact on the precision of prompts, affecting analytics accuracy.
So Visual Prompts Transformation Problem is defined as:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">Given a set of visual prompts, a set of visual prompt transformation operations (boxes, scribbles, etc.), and a latency constraint, determine a visual prompt transformation strategy to select and convert a new set of visual prompts, so that the latency is guaranteed and the analytics accuracy is maximized.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.2"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.2.1">Visual Prompts Transformation Algorithm</span>:
We propose an information-theoretical approach to address this problem. Intuitively, different prompts contain different amounts of information on the user‚Äôs "instructions", which can be redundant, urging us to find a way to extract them maximally.
Our method utilizes information entropy, a measure of information in a message, to assess NN performance, which has been applied in areas such as Explainable AI and Transfer Learning.
We design an information-based metric, <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p3.2.2">visual prompt contribution score</span>, to measure the contribution of each visual prompt to the task, using mutual information.
More information in a prompt leads to better analytics accuracy. Let <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.1.m1.1"><semantics id="S3.SS3.SSS1.p3.1.m1.1a"><mi id="S3.SS3.SSS1.p3.1.m1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.1.m1.1b"><ci id="S3.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.1.m1.1d">italic_X</annotation></semantics></math> be the input prompt, and <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.2.m2.1"><semantics id="S3.SS3.SSS1.p3.2.m2.1a"><mi id="S3.SS3.SSS1.p3.2.m2.1.1" xref="S3.SS3.SSS1.p3.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.2.m2.1b"><ci id="S3.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p3.2.m2.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.2.m2.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.2.m2.1d">italic_Y</annotation></semantics></math> be the predicted output for the task. The visual prompt contribution score can be represented as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="C(X,Y)=I(X;Y)," class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml">C</mi><mo id="S3.E1.m1.5.5.1.1.2.1" xref="S3.E1.m1.5.5.1.1.2.1.cmml">‚Å¢</mo><mrow id="S3.E1.m1.5.5.1.1.2.3.2" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml"><mo id="S3.E1.m1.5.5.1.1.2.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">X</mi><mo id="S3.E1.m1.5.5.1.1.2.3.2.2" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Y</mi><mo id="S3.E1.m1.5.5.1.1.2.3.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2" xref="S3.E1.m1.5.5.1.1.3.2.cmml">I</mi><mo id="S3.E1.m1.5.5.1.1.3.1" xref="S3.E1.m1.5.5.1.1.3.1.cmml">‚Å¢</mo><mrow id="S3.E1.m1.5.5.1.1.3.3.2" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml"><mo id="S3.E1.m1.5.5.1.1.3.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">X</mi><mo id="S3.E1.m1.5.5.1.1.3.3.2.2" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml">;</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">Y</mi><mo id="S3.E1.m1.5.5.1.1.3.3.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"></eq><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><times id="S3.E1.m1.5.5.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1"></times><ci id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2">ùê∂</ci><interval closure="open" id="S3.E1.m1.5.5.1.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùëã</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ùëå</ci></interval></apply><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><times id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.1"></times><ci id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2">ùêº</ci><list id="S3.E1.m1.5.5.1.1.3.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ùëã</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ùëå</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">C(X,Y)=I(X;Y),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_C ( italic_X , italic_Y ) = italic_I ( italic_X ; italic_Y ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p3.6">where <math alttext="I(X;Y)=H(X)+H(Y)-H(X,Y)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.3.m1.6"><semantics id="S3.SS3.SSS1.p3.3.m1.6a"><mrow id="S3.SS3.SSS1.p3.3.m1.6.7" xref="S3.SS3.SSS1.p3.3.m1.6.7.cmml"><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.cmml"><mi id="S3.SS3.SSS1.p3.3.m1.6.7.2.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.2.cmml">I</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.2.1" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.1.cmml">‚Å¢</mo><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.2.3.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.3.1.cmml"><mo id="S3.SS3.SSS1.p3.3.m1.6.7.2.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS1.p3.3.m1.1.1" xref="S3.SS3.SSS1.p3.3.m1.1.1.cmml">X</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.2.3.2.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.3.1.cmml">;</mo><mi id="S3.SS3.SSS1.p3.3.m1.2.2" xref="S3.SS3.SSS1.p3.3.m1.2.2.cmml">Y</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.2.3.2.3" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p3.3.m1.6.7.1" xref="S3.SS3.SSS1.p3.3.m1.6.7.1.cmml">=</mo><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.cmml"><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.cmml"><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.cmml"><mi id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.2.cmml">H</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.1" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.1.cmml">‚Å¢</mo><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.3.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.cmml"><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.cmml">(</mo><mi id="S3.SS3.SSS1.p3.3.m1.3.3" xref="S3.SS3.SSS1.p3.3.m1.3.3.cmml">X</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.3.2.2" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.1" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.1.cmml">+</mo><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.cmml"><mi id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.2.cmml">H</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.1" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.1.cmml">‚Å¢</mo><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.3.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.cmml"><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.cmml">(</mo><mi id="S3.SS3.SSS1.p3.3.m1.4.4" xref="S3.SS3.SSS1.p3.3.m1.4.4.cmml">Y</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.3.2.2" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.1" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.1.cmml">‚àí</mo><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3.3" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.cmml"><mi id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.2.cmml">H</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.1" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.1.cmml">‚Å¢</mo><mrow id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.1.cmml"><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.1.cmml">(</mo><mi id="S3.SS3.SSS1.p3.3.m1.5.5" xref="S3.SS3.SSS1.p3.3.m1.5.5.cmml">X</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.2.2" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p3.3.m1.6.6" xref="S3.SS3.SSS1.p3.3.m1.6.6.cmml">Y</mi><mo id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.2.3" stretchy="false" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.3.m1.6b"><apply id="S3.SS3.SSS1.p3.3.m1.6.7.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7"><eq id="S3.SS3.SSS1.p3.3.m1.6.7.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.1"></eq><apply id="S3.SS3.SSS1.p3.3.m1.6.7.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.2"><times id="S3.SS3.SSS1.p3.3.m1.6.7.2.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.1"></times><ci id="S3.SS3.SSS1.p3.3.m1.6.7.2.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.2">ùêº</ci><list id="S3.SS3.SSS1.p3.3.m1.6.7.2.3.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.2.3.2"><ci id="S3.SS3.SSS1.p3.3.m1.1.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.1.1">ùëã</ci><ci id="S3.SS3.SSS1.p3.3.m1.2.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.2.2">ùëå</ci></list></apply><apply id="S3.SS3.SSS1.p3.3.m1.6.7.3.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3"><minus id="S3.SS3.SSS1.p3.3.m1.6.7.3.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.1"></minus><apply id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2"><plus id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.1"></plus><apply id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2"><times id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.1"></times><ci id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.2.2">ùêª</ci><ci id="S3.SS3.SSS1.p3.3.m1.3.3.cmml" xref="S3.SS3.SSS1.p3.3.m1.3.3">ùëã</ci></apply><apply id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3"><times id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.1"></times><ci id="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.2.3.2">ùêª</ci><ci id="S3.SS3.SSS1.p3.3.m1.4.4.cmml" xref="S3.SS3.SSS1.p3.3.m1.4.4">ùëå</ci></apply></apply><apply id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3"><times id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.1"></times><ci id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.2.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.2">ùêª</ci><interval closure="open" id="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.1.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.7.3.3.3.2"><ci id="S3.SS3.SSS1.p3.3.m1.5.5.cmml" xref="S3.SS3.SSS1.p3.3.m1.5.5">ùëã</ci><ci id="S3.SS3.SSS1.p3.3.m1.6.6.cmml" xref="S3.SS3.SSS1.p3.3.m1.6.6">ùëå</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.3.m1.6c">I(X;Y)=H(X)+H(Y)-H(X,Y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.3.m1.6d">italic_I ( italic_X ; italic_Y ) = italic_H ( italic_X ) + italic_H ( italic_Y ) - italic_H ( italic_X , italic_Y )</annotation></semantics></math> is the mutual information between <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.4.m2.1"><semantics id="S3.SS3.SSS1.p3.4.m2.1a"><mi id="S3.SS3.SSS1.p3.4.m2.1.1" xref="S3.SS3.SSS1.p3.4.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.4.m2.1b"><ci id="S3.SS3.SSS1.p3.4.m2.1.1.cmml" xref="S3.SS3.SSS1.p3.4.m2.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.4.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.4.m2.1d">italic_X</annotation></semantics></math> and <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.5.m3.1"><semantics id="S3.SS3.SSS1.p3.5.m3.1a"><mi id="S3.SS3.SSS1.p3.5.m3.1.1" xref="S3.SS3.SSS1.p3.5.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.5.m3.1b"><ci id="S3.SS3.SSS1.p3.5.m3.1.1.cmml" xref="S3.SS3.SSS1.p3.5.m3.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.5.m3.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.5.m3.1d">italic_Y</annotation></semantics></math>, and <math alttext="H" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.6.m4.1"><semantics id="S3.SS3.SSS1.p3.6.m4.1a"><mi id="S3.SS3.SSS1.p3.6.m4.1.1" xref="S3.SS3.SSS1.p3.6.m4.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.6.m4.1b"><ci id="S3.SS3.SSS1.p3.6.m4.1.1.cmml" xref="S3.SS3.SSS1.p3.6.m4.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.6.m4.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.6.m4.1d">italic_H</annotation></semantics></math> is the entropy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p4">
<p class="ltx_p" id="S3.SS3.SSS1.p4.1">The visual prompt contribution score forms the basis for transforming visual prompts, i.e., combining or converting them. With numerous prompt transformation options, the strategy search space is extensive.
To balance the exploration and exploitation, we develop an offline configuration and online adaption approach:
In the offline stage, we first apply a random prompt transformation to search for the optimal transformation strategy. We then configure the mapping profiles among the transformation strategies, the analytics accuracy, and the latency.
During the online stage, we employ a greedy algorithm. This algorithm selects the transformation strategy with the highest ratio of visual prompt contribution scores, ensuring it adheres to the latency constraint.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.5.1.1">III-C</span>2 </span>Image Encoding Workload Partition Problem</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Given video frames and image prompts, the image encoder and the image prompt encoder NN models, the computation and communication resources at the edge, and a maximum latency constraint, determine the splitting points of the image encoder and the image prompt encoder NN models, the resolutions of the video frames and the prompt images, so that the latency is guaranteed and the analytics accuracy is maximized.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p2.1.1">Image Encoding Workload Partition Algorithm:</span>
We observe that the video embedding and the image prompt embedding output by the image encoder and the image prompt encoder should be fed into the embedding decoder at the same time for the decoding process.
As the computation of the NN models is progressive, we first construct a directed acyclic graph (DAG) where the vertex represents one NN layer.
Each layer is inherently indivisible and requires processing either on the edge side or the cloud side. To encapsulate the communication and dependency relationships among layers, a virtual entry vertex and an exit vertex are added, symbolizing the starting and ending points of the DNN, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">We then develop a model partition algorithm on this DAG, where we consider the computation and communication resources and resolutions of images.
Specifically, our objective is to identify a set of vertices, which constitutes a subset of DNN layers. The removal of this set results in the remaining graph splitting into two disconnected components. This problem can be transformed into a <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p3.1.1">minimum weighted s-t cut</span> problem¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib8" title="">8</a>]</cite>. Since researchers have extensively explored this problem, numerous existing solutions are available. We here employ the Boykov algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib9" title="">9</a>]</cite> to address the model partition problem with the computational complexity of <math alttext="O((m+n)n^{2})" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p3.1.m1.1"><semantics id="S3.SS3.SSS2.p3.1.m1.1a"><mrow id="S3.SS3.SSS2.p3.1.m1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.p3.1.m1.1.1.3" xref="S3.SS3.SSS2.p3.1.m1.1.1.3.cmml">O</mi><mo id="S3.SS3.SSS2.p3.1.m1.1.1.2" xref="S3.SS3.SSS2.p3.1.m1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.SS3.SSS2.p3.1.m1.1.1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.cmml"><mrow id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.2.cmml">m</mi><mo id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.3.cmml">n</mi></mrow><mo id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.2" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.2.cmml">‚Å¢</mo><msup id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.2" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.2.cmml">n</mi><mn id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.3" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.3.cmml">2</mn></msup></mrow><mo id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p3.1.m1.1b"><apply id="S3.SS3.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1"><times id="S3.SS3.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.2"></times><ci id="S3.SS3.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.3">ùëÇ</ci><apply id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1"><times id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.2"></times><apply id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1"><plus id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.1"></plus><ci id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.2">ùëö</ci><ci id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.1.1.1.3">ùëõ</ci></apply><apply id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.2">ùëõ</ci><cn id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p3.1.m1.1c">O((m+n)n^{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p3.1.m1.1d">italic_O ( ( italic_m + italic_n ) italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">A Case Study on Visual Tour Guide</span>
</h2>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="155" id="S4.F3.g1" src="x2.png" width="394"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>SAMEdge implementation based on SAM.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.1" style="width:206.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="343" id="S4.F5.1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of analytics accuracy performance under different datasets.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F5.2" style="width:206.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="343" id="S4.F5.2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of analytics accuracy impacted by diverse network bandwidth on KITTI.</figcaption>
</figure>
</div>
</div>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Implementation</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>Prototype implementation:</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We implement a SAMEdge prototype by extending from Meta AI SAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">Briefing of the Meta SAM Architecture.</span> Conventional SAM has three major modules: (1) <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p2.1.2">AppContext</span> for prompt encoding, (2) <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p2.1.3">ImgtoEmb</span>, which has an image encoder model for video embedding, (3) <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p2.1.4">EmbDec</span>, which aggregates the prompt embedding and video embedding through an embedding decoder to output analytics results. The Meta AI SAM presumably runs in the cloud.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">SAMEdge Module Implementation.</span> We develop SAMEdge, an edge-cloud SAM, by extending SAM with four modules on the edge and two on the cloud:
(1) a <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.2">SAMEdge_Context_Mgmt</span> module that inherits <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.3">AppContext</span>;
(2) a new <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.4">SAMEdge_VP_Trans</span> module for visual prompt transformation;
(3) a <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.5">SAMEdge_ImgEnc</span> module that inherits <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.6">ImgtoEmb</span> with a new <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.7">Model_Partition</span> function to allow an early-exit of the encoder NN model at the split layer;
(4) a <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.8">SAMEdge_Dec</span> module that inherits <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.9">EmbDec</span>;
(5) a <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.10">SAMEdge_Cloud_Reg</span> module to register the edge;
and (6) a <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p3.1.11">SAMEdge_Cloud_Offloading</span> module to perform SAM inference in the cloud.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>VTG Implementation</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">We implement VTG on SAMEdge. We use pre-train SAM of Meta AI and a TinyViT model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib10" title="">10</a>]</cite> to serve as an image prompt encoder. Models are pre-trained by SA-1B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib2" title="">2</a>]</cite>. We fine-tune them by the street view dataset KITTI¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib11" title="">11</a>]</cite> and LIV360¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib12" title="">12</a>]</cite>.
In a VTG application, the prompts can change according to the user preferences and the environment. There are prompt design and engineering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib13" title="">13</a>]</cite> on how prompts can accurately reflect user preferences. In this paper, we assume that there is a set of prompts¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib14" title="">14</a>]</cite> of users for evaluation. The system adapts to prompt changes and dynamically adjusts resource consumption on edges.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Evaluations</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.5.1.1">IV-B</span>1 </span>Evaluation Setup</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We evaluate the performance of SAMEdge VTG with an Nvidia Jetson Nano, a widely used edge AI device with a 128-core Maxwell GPU and a Quad-core ARM A57 CPU running the Ubuntu system.
A workstation as the cloud server equips dual powerful NVIDIA RTX 4090 GPUs, and an Intel i9 CPU, providing exceptional NN computation capabilities.
We use two street-view benchmark datasets for model fine-tuning and inference:
1) KITTI¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib11" title="">11</a>]</cite>, contains over 32,000 images of street views,
and 2) LIV360¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib12" title="">12</a>]</cite>, contains more than 30,000 frames of 10,000 labeled objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">Evaluation Criteria</span>: We evaluate the accuracy and the delay performance of the video analytic application supported by SAMEdge.
For image prompts, we use Intersection over Union (IoU) to measure the accuracy of VTG performance, i.e., <math alttext="IoU=\textit{Area of Overlap}/\textit{Area of Union}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.1.m1.1"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mrow id="S4.SS2.SSS1.p2.1.m1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.cmml"><mrow id="S4.SS2.SSS1.p2.1.m1.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml"><mi id="S4.SS2.SSS1.p2.1.m1.1.1.2.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.2.cmml">I</mi><mo id="S4.SS2.SSS1.p2.1.m1.1.1.2.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.1.cmml">‚Å¢</mo><mi id="S4.SS2.SSS1.p2.1.m1.1.1.2.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.3.cmml">o</mi><mo id="S4.SS2.SSS1.p2.1.m1.1.1.2.1a" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.1.cmml">‚Å¢</mo><mi id="S4.SS2.SSS1.p2.1.m1.1.1.2.4" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.4.cmml">U</mi></mrow><mo id="S4.SS2.SSS1.p2.1.m1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS2.SSS1.p2.1.m1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS1.p2.1.m1.1.1.3.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.2a.cmml">Area of Overlap</mtext><mo id="S4.SS2.SSS1.p2.1.m1.1.1.3.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.1.cmml">/</mo><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS1.p2.1.m1.1.1.3.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.3a.cmml">Area of Union</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><apply id="S4.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1"><eq id="S4.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1"></eq><apply id="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.2"><times id="S4.SS2.SSS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.1"></times><ci id="S4.SS2.SSS1.p2.1.m1.1.1.2.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.2">ùêº</ci><ci id="S4.SS2.SSS1.p2.1.m1.1.1.2.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.3">ùëú</ci><ci id="S4.SS2.SSS1.p2.1.m1.1.1.2.4.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.4">ùëà</ci></apply><apply id="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3"><divide id="S4.SS2.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.1"></divide><ci id="S4.SS2.SSS1.p2.1.m1.1.1.3.2a.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.2">Area of Overlap</mtext></ci><ci id="S4.SS2.SSS1.p2.1.m1.1.1.3.3a.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.3"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.3">Area of Union</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">IoU=\textit{Area of Overlap}/\textit{Area of Union}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.1.m1.1d">italic_I italic_o italic_U = Area of Overlap / Area of Union</annotation></semantics></math>.
For visual prompts, we use the Number of Clicks (NoC)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib14" title="">14</a>]</cite> metric to evaluate interactive VTG performance, which measures the number of clicks of visual prompts needed to achieve a certain Intersection over Union (IoU), i.e., 90
%. The lower value of NoC or higher value of IoU represents the higher accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">Baselines for Comparison</span>:
We compare SAMEdge with three existing architectures for comparison.</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Vanilla only processes visual prompts and decoders on edge devices, where the image prompts and video frames are sent to the workstation for image encoding as the conventional Meta Segment Anything Model Project¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Model Compression Scheme (MCS)
applies a lightweight version of SAM, Mobile_SAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib6" title="">6</a>]</cite>, through knowledge distillation. It only has 1/60 parameters of the original SAM that can deploy on edge devices directly and ignore the influence of dynamic bandwidth.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Super-resolution Scheme (SRS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib4" title="">4</a>]</cite>
applies a Super-resolution approach on the cloud and reduces the image resolution during transmission from the edge to the cloud according to the bandwidth. The cloud will convert the low-resolution images into high ones through the super-resolution approach for encoding.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="154" id="S4.F6.sf1.g1" src="extracted/5870326/fig/fig6a.png" width="219"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S4.F6.sf1.3.2" style="font-size:80%;">Visual Prompts</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="156" id="S4.F6.sf2.g1" src="extracted/5870326/fig/fig6b.png" width="225"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S4.F6.sf2.3.2" style="font-size:80%;">Image Prompts</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization of SAMEdge under visual prompt and image prompt in KITTI dataset of VTG Application.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Experiment Results</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS1.5.1.1">IV-C</span>1 </span>Improvement of Analytics Accuracy</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We evaluate the analytics accuracy performance of SAMEdge, compared to baselines on two datasets.
Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.F5" title="Figure 5 ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the accuracy of (a) visual prompts and (b) image prompts measured by NoC and IoU, respectively.
Taking the KITTI dataset as an example, SAMEdge achieves 1.87 NoC and 0.797 IoU, outperforming Vanilla with 4.64 times NoC and 1.81 times IoU. This improvement stems from the fact that Vanilla transmits low-quality videos and prompts to the cloud to reduce latency, thereby compromising analytics accuracy.
MCS achieves 4.85 NoC and 0.655 IoU, exhibiting 2.59 times higher NoC and 17.8% IoU reduction compared to SAMEdge. This reduction is attributed to model compression techniques, which not only reduce computation requirements but also degrade analytics accuracy performance.
SRS achieves 3.78 NoC and 0.694 IoU. This is because SRS reduces transmission data by downsampling resolution and then recovers it using super-resolution. However, the loss in super-resolution recovery results in significant accuracy drops. Similar results are observed in LIV360.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">We further analyze the analytics performance of SAMEdge and SAMEdge-WOT in LIV360, where SAMEdge-WOT does not apply the visual transformation algorithm. SAMEdge achieves 2.77 NoC in visual prompts, while SAMEdge-WOT achieves a better value of 1.44. This difference arises because SAMEdge applies visual prompt transformation algorithms and combines prompts to reduce latency, about 68%, with the cost of accuracy degradation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS2.5.1.1">IV-C</span>2 </span>Improvement of Analytics Accuracy Impacted by Network Bandwidth</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">To investigate the analytics performance across different network bandwidths, we simulate five network bandwidth traces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#bib.bib15" title="">15</a>]</cite>, including <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p1.1.1">wired cabled</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p1.1.2">5G</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p1.1.3">4G-LTE</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p1.1.4">802.11g</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p1.1.5">3G</span>, on KITTI. The average bandwidths are about {1000, 100, 50, 20, 3}Mbps, respectively. Baselines never exceed the maximum latency through their adjustable mechanisms, e.g., decline image resolutions.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">The results are depicted in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.F5" title="Figure 5 ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">5</span></a>. When the baselines and SAMEdge are provided with sufficient network bandwidth, such as with wired cabled connections, all achieve relatively high analytics accuracy under both visual and image prompts except MCS. MCS consistently performs regardless of changes in bandwidth, as it computes entirely locally, disregarding dynamic bandwidth variations. Vanilla exhibits rapid degradation with decreasing bandwidth, declining from 1.72 to 11.4 on NoC and from 0.826 to 0.411 on IoU. SRS experiences a slower decline in analytics compared to Vanilla, achieving 8.02 NoC and 0.521 IoU under the lowest bandwidth. This is because SRS reduces communication resource consumption when uploading images and videos to the cloud but still loses information during downsampling on the edge, which is challenging to recover via super-resolution on the cloud. In contrast, SAMEdge maintains high analytics accuracy across all bandwidths, experiencing only a +1.27 NoC and -0.112 IoU degradation. This is because SAMEdge only transforms embeddings to the cloud for partial inference instead of all videos and images, requiring minimal bandwidth resources. Furthermore, SAMEdge addresses the SERO problem by dynamically selecting the splitting position of the NN model constraint to maximum latency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS3.5.1.1">IV-C</span>3 </span>Case study of SAMEdge on the KITTI dataset.</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">We display the visualization of SAMEdge on dataset KITTI as shown in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.F6" title="Figure 6 ‚Ä£ IV-B1 Evaluation Setup ‚Ä£ IV-B Evaluations ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">6</span></a>. In Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.F6" title="Figure 6 ‚Ä£ IV-B1 Evaluation Setup ‚Ä£ IV-B Evaluations ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">6</span></a>(a), the visual prompts are points clicked by the user to choose the "building" target. Only two points are needed to mark the target with 90.4% accuracy. In Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.14784v1#S4.F6" title="Figure 6 ‚Ä£ IV-B1 Evaluation Setup ‚Ä£ IV-B Evaluations ‚Ä£ IV A Case Study on Visual Tour Guide ‚Ä£ SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model"><span class="ltx_text ltx_ref_tag">6</span></a>(b), we input a "building" cartoon image as a reference to find similar targets, and targets are marked with 92.1% accuracy. These validate that our proposed architecture and algorithm of SAMEdge are feasible and effective in practice.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this article, we proposed SAMEdge, a novel edge-cloud computing architecture designed to support SAM computations for edge users under resources and latency constraints.
SAMEdge fundamentally divides the workload of the conventional SAM into the edge and the cloud. We model the SERO problem and present a prompt transformation algorithm and a workload partitioning algorithm designed to maximize overall analytics accuracy within the constraints of computation and communication resources under the maximal latency on the edge device.
We implement a SAMEdge prototype that integrates Meta AI and features a Visual Tour Guide application. The results demonstrate that SAMEdge significantly enhances video analytics accuracy under dynamic network bandwidth.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">BIOGRAPHIES</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text ltx_font_bold" id="Sx1.p1.1.1">Rui Lu</span> received the B.S. degree from the Southern University of Science and Technology in 2019. He is currently a Ph.D. candidate at the Department of Computing of The Hong Kong Polytechnic University. His research interests include edge computing and video analytics.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx1.p2.1.1">Siping Shi</span> received the Ph.D. degree in computer science from The Hong Kong Polytechnic University in 2023. She is currently a Postdoc at the Department of Computing at the Hong Kong Polytechnic University. Her research interests include federated learning and analytics, edge computing, and privacy-preserving machine learning systems.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1"><span class="ltx_text ltx_font_bold" id="Sx1.p3.1.1">Yanting Liu</span> received the B.S. degree from The Hong Kong Polytechnic University in 2022. He is currently a Ph.D. student at the Department of Computing of The Hong Kong Polytechnic University. His research interests include networking, video streaming and analytics.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1"><span class="ltx_text ltx_font_bold" id="Sx1.p4.1.1">Dan Wang</span> received the Ph.D. degree in computer science from Simon Fraser University, Canada in 2007. He is a Professor of Department of Computing, The Hong Kong Polytechnic University. He was a TPC co-Chair of IEEE/ACM IWQoS 2020 and a TPC co-Chair of ACM e-Energy 2020. He is a senior member of the IEEE.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
I.¬†Al¬†Ridhawi, S.¬†Otoum, M.¬†Aloqaily, and A.¬†Boukerche, ‚ÄúGeneralizing ai: Challenges and opportunities for plug and play ai solutions,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">IEEE Network</span>, vol.¬†35, no.¬†1, pp.¬†372‚Äì379, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
‚ÄúSegment Anything Model (SAM): a new AI model from Meta AI that can "cut out" any object, in any image.‚Äù <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://segment-anything.com/" title="">https://segment-anything.com/</a>.

</span>
<span class="ltx_bibblock">Accessed Jan., 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
C.¬†Zhang, L.¬†Liu, Y.¬†Cui, G.¬†Huang, W.¬†Lin, Y.¬†Yang, and Y.¬†Hu, ‚ÄúA comprehensive survey on segment anything model for vision and beyond,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint:2305.08196</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y.¬†Zhang, Y.¬†Zhang, Y.¬†Wu, Y.¬†Tao, K.¬†Bian, P.¬†Zhou, L.¬†Song, and H.¬†Tuo, ‚ÄúImproving quality of experience by adaptive video streaming with super-resolution,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proc. of IEEE INFOCOM‚Äô20</span>, Virtual Conference, Jul. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.¬†Hu, R.¬†Lu, and D.¬†Wang, ‚ÄúFeva: A federated video analytics architecture for networked smart cameras,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">IEEE Network</span>, vol.¬†35, no.¬†6, pp.¬†163‚Äì170, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C.¬†Zhang, D.¬†Han, Y.¬†Qiao, J.¬†U. Kim, S.-H. Bae, S.¬†Lee, and C.¬†S. Hong, ‚ÄúFaster segment anything: Towards lightweight sam for mobile applications,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint:2306.14289</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L.-K. Huang, J.¬†Huang, Y.¬†Rong, Q.¬†Yang, and Y.¬†Wei, ‚ÄúFrustratingly easy transferability estimation,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proc. of ICML‚Äô22</span>, Baltimore, MD, Jul. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
C.¬†Hu, W.¬†Bao, D.¬†Wang, and F.¬†Liu, ‚ÄúDynamic adaptive dnn surgery for inference acceleration on the edge,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proc. of IEEE INFOCOM‚Äô19</span>, Paris, France, Apr. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y.¬†Boykov and V.¬†Kolmogorov, ‚ÄúAn experimental comparison of min-cut/max-flow algorithms for energy minimization in vision,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, vol.¬†26, no.¬†9, pp.¬†1124‚Äì1137, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K.¬†Wu, J.¬†Zhang, H.¬†Peng, M.¬†Liu, B.¬†Xiao, J.¬†Fu, and L.¬†Yuan, ‚ÄúTinyvit: Fast pretraining distillation for small vision transformers,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proc. of ECCV‚Äô22</span>, pp.¬†68‚Äì85, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M.¬†Menze and A.¬†Geiger, ‚ÄúObject scene flow for autonomous vehicles,‚Äù in <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proc. of IEEE CVPR‚Äô15</span>, (Boston, MA, USA), Jun. 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
G.¬†Palmer, M.¬†Green, E.¬†Boyland, Y.¬†S.¬†R. Vasconcelos, R.¬†Savani, and A.¬†Singleton, ‚ÄúA deep learning approach to identify unhealthy advertisements in street view images,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Scientific reports</span>, vol.¬†11, no.¬†1, p.¬†4884, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y.¬†Liu, H.¬†Du, D.¬†Niyato, J.¬†Kang, S.¬†Cui, X.¬†Shen, and P.¬†Zhang, ‚ÄúOptimizing mobile-edge ai-generated everything (aigx) services by prompt engineering: Fundamental, framework, and case study,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE Network</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X.¬†Zou, J.¬†Yang, H.¬†Zhang, F.¬†Li, L.¬†Li, J.¬†Gao, and Y.¬†J. Lee, ‚ÄúSegment everything everywhere all at once,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint:2304.06718</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X.¬†Wang, T.¬†Kwon, Y.¬†Choi, M.¬†Chen, and Y.¬†Zhang, ‚ÄúCharacterizing the gaming traffic of world of warcraft: From game scenarios to network access technologies,‚Äù <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">IEEE Network</span>, vol.¬†26, no.¬†1, pp.¬†27‚Äì34, 2012.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 07:55:04 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
