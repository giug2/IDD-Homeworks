<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study</title>
<!--Generated on Sat Sep 14 07:17:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Generative artificial intelligence,  large language model,  data center networking,  edge computing,  sustainability.
" lang="en" name="keywords"/>
<base href="/html/2409.09343v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S1" title="In Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2" title="In Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Preliminaries: Data Center Networking, Discriminative AI, and Generative AI</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS1" title="In II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Basics of Data Center Networking (DCN)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS2" title="In II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Issues of DCN and Solutions from DAI</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS2.SSS1" title="In II-B Issues of DCN and Solutions from DAI ‣ II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>Topology and Routing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS2.SSS2" title="In II-B Issues of DCN and Solutions from DAI ‣ II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>Load Balancing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS2.SSS3" title="In II-B Issues of DCN and Solutions from DAI ‣ II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>3 </span>Traffic Management</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS2.SSS4" title="In II-B Issues of DCN and Solutions from DAI ‣ II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>4 </span>Fault Tolerance and Resiliency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS3" title="In II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Existing Challenges</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS4" title="In II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">GenAI and LLM: New Opportunities</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS4.SSS1" title="In II-D GenAI and LLM: New Opportunities ‣ II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>1 </span>Learning Objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.SS4.SSS2" title="In II-D GenAI and LLM: New Opportunities ‣ II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>2 </span>Model Architecture</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S3" title="In Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Enhancing Data Center Networking with GenAI and LLM</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S3.SS1" title="In III Enhancing Data Center Networking with GenAI and LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Data Augmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S3.SS2" title="In III Enhancing Data Center Networking with GenAI and LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">DCN Process Automation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S3.SS3" title="In III Enhancing Data Center Networking with GenAI and LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Anomaly Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S3.SS4" title="In III Enhancing Data Center Networking with GenAI and LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Quality of Service Modeling</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4" title="In Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Transforming DCNs for GenAI/LLM</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.SS1" title="In IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Characterization of GenAI/LLM Workload</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.SS2" title="In IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">DCNs in the GenAI/LLM Era</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S5" title="In Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Case Study: Towards Full-Lifecycle DCN Digital Twin via GenAI</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S5.SS1" title="In V Case Study: Towards Full-Lifecycle DCN Digital Twin via GenAI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Problem Statement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S5.SS2" title="In V Case Study: Towards Full-Lifecycle DCN Digital Twin via GenAI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Proposed Framework</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S5.SS2.SSS1" title="In V-B Proposed Framework ‣ V Case Study: Towards Full-Lifecycle DCN Digital Twin via GenAI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>1 </span>Automatic Optimization Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S5.SS2.SSS2" title="In V-B Proposed Framework ‣ V Case Study: Towards Full-Lifecycle DCN Digital Twin via GenAI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>2 </span>Diffusion-empowered Optimization Solving</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S5.SS3" title="In V Case Study: Towards Full-Lifecycle DCN Digital Twin via GenAI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Evaluation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S6" title="In Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Future Research Directions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S7" title="In Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yinqiu Liu,
Hongyang Du,
Dusit Niyato, ,
<br class="ltx_break"/>Jiawen Kang,
Zehui Xiong,
Yonggang Wen, ,
and
Dong In Kim
</span><span class="ltx_author_notes">Y. Liu, D. Niyato, and Y. Wen are with the College of Computing and Data Science, Nanyang Technological University, Singapore (e-mail: yinqiu001@e.ntu.edu.sg, dniyato@ntu.edu.sg, and ygwen@ntu.edu.sg).H. Du is with the Department of Electrical and Electronic Engineering, University of Hong Kong, Pok Fu Lam, Hong Kong (e-mail: duhy@eee.hku.hk).J. Kang is with the School of Automation, Guangdong University of Technology, China (e-mail: kavinkang@gdut.edu.cn).Z. Xiong is with the Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Singapore (e-mail: zehui_xiong@sutd.edu.sg).D. Kim is with the College of Information and Communication Engineering, Sungkyunkwan University, South Korea (Email: dongin@skku.edu).
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Generative AI (GenAI), exemplified by Large Language Models (LLMs) such as OpenAI’s ChatGPT, is revolutionizing various fields.
Central to this transformation is Data Center Networking (DCN), which not only provides the computational power necessary for GenAI training and inference but also delivers GenAI-driven services to users.
This article examines an interplay between GenAI and DCNs, highlighting their symbiotic relationship and mutual advancements.
We begin by reviewing current challenges within DCNs and discuss how GenAI contributes to enhancing DCN capabilities through innovations, such as data augmentation, process automation, and domain transfer.
We then focus on analyzing the distinctive characteristics of GenAI workloads on DCNs, gaining insights that catalyze the evolution of DCNs to more effectively support GenAI and LLMs.
Moreover, to illustrate the seamless integration of GenAI with DCNs, we present a case study on full-lifecycle DCN digital twins.
In this study, we employ LLMs equipped with Retrieval Augmented Generation (RAG) to formulate optimization problems for DCNs and adopt Diffusion-Deep Reinforcement Learning (DRL) for optimizing the RAG knowledge placement strategy.
This approach not only demonstrates the application of advanced GenAI methods within DCNs but also positions the digital twin as a pivotal GenAI service operating on DCNs.
We anticipate that this article can promote further research into enhancing the virtuous interaction between GenAI and DCNs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Generative artificial intelligence, large language model, data center networking, edge computing, sustainability.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">From 2022, the transformative power of Generative AI (GenAI) has been demonstrated by the phenomenal success of Large Language Models (LLMs) like OpenAI’s ChatGPT.
Unlike Discriminative AI (DAI) models that learn the boundaries among classes, GenAI models are adept at representing the distribution of sample data, enabling them to produce realistic digital content <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib1" title="">1</a>]</cite>.
Nonetheless, to effectively capture complex patterns of real-world samples, GenAI models generally feature complicated architectures with huge sizes.
Consequently, GenAI operations demand significant computational resources, necessitating robust infrastructural support.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Simultaneously, the field of Data Center Networking (DCN) has undergone substantial development to meet the burgeoning requirements of advanced applications represented by GenAI.
For instance, NVIDIA presented the Blackwell DCN architecture oriented to LLMs, with optimized tensor cores to accelerate Transformer and 130TB/s switches to accelerate data synchronization during the GenAI training<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.nvidia.com/en-us/data-center</span></span></span>.
Moreover, AI advancements enable DCNs to transcend traditional roles as mere data conduits, efficiently orchestrating data storage, transportation, and computing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib2" title="">2</a>]</cite>.
For example, Huawei presented iManager<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://carrier.huawei.com/en/products/fixed-network</span></span></span>, which utilizes AI models to manage DCN power allocation and is estimated to improve the resource utilization rate of its facilities by 20%.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The interplay between GenAI/LLMs and DCNs is multifaceted and deeply symbiotic, as advancements in one field drive progress and enable new capabilities in the other.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">GenAI Enhances DCNs</span>: GenAI significantly outperforms DAI in enhancing DCN capabilities, owing to its inherent capability in distribution representation. For instance, in load balancing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib3" title="">3</a>]</cite>, GenAI can enrich the training dataset and simulate diverse DCN states. Additionally, GenAI can make accurate predictions in dynamic environments by learning from data distributions. It can even adaptively transfer and fine-tune knowledge across different domains. These advanced capabilities are not available with DAI.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">DCNs Supports GenAI</span>: DCNs are the cradle of GenAI. Taking ChatGPT as an example, its training was conducted on a super DCN comprising tens of thousands of servers. To meet the escalating demands from GenAI, future DCNs should be specifically designed to accommodate the unique characteristics of GenAI workloads <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite>. For instance, considering that the pre-training stage of GenAI is resource-intensive and time-consuming, the load balancing mechanism should be optimized to efficiently distribute tasks across servers, thereby reducing the service latency during high-demand periods.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="524" id="S1.F1.g1" src="x1.png" width="772"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of DAI and GenAI applications in DCN. The middle part illustrates the representative data center layout, with the server room, management center, and power room. Numerous servers can be connected in the three-tier, fat-tree, or DCell manners, the so-called networking layer. Finally, DAI and GenAI in the AI layer collaborate to realize efficient and secure DCN operations.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We notice that some preliminary work regarding this topic has been done.
For instance, Li <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib5" title="">5</a>]</cite> presented ChatTwin, which leverages GPT-4 to automatically configure DCNs according to the operator’s functional requirements.
Hu <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite> traced the LLM running on DCNs for six months, characterizing the LLM workload and presenting a series of mechanisms to ensure efficient LLM training.
Nonetheless, an interplay between GenAI and DCN has not yet been thoroughly explored in the academic realm, with several unsolved questions, e.g., “<span class="ltx_text ltx_font_italic" id="S1.p4.1.3">which DCN tasks can be effectively solved by GenAI and why</span>” and “<span class="ltx_text ltx_font_italic" id="S1.p4.1.4">how DCNs evolve to efficiently support GenAI/LLM workloads</span>”.
This paper first reviews the DAI-based methods to solve DCN issues, as well as the challenges.
Then, we analyze new possibilities brought by GenAI and review the existing works leveraging GenAI/LLM to enhance DCNs.
In addition to how GenAI enhances DCNs, we intend to explore how DCNs evolve to better support GenAI.
Hence, we analyze the characterization of GenAI/LLM workloads on DCNs.
Based on the acquired insights, we further review the DCN designs catering to GenAI/LLM.
Finally, we perform a case study to showcase the virtuous interaction between GenAI and DCNs.
The main contributions of our paper can be summarized as follows.</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.1">To the best of our knowledge, this is the first work exploring the interplay between GenAI and DCNs.</span> First, we analyze the existing challenges of DAI-empowered DCN and discuss why and how GenAI can bring new possibilities. Moreover, we review the existing works of GenAI for optimizing DCNs, including data augmentation, anomaly detection, and process automation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">From the opposite perspective, we characterize the pattern of GenAI/LLM workload on DCNs. Based on the insights from characterizations, we review the mechanisms that have been proposed to optimize the running of GenAI/LLM on DCNs. Then, we look forward to the development direction of DCNs in the GenAI era.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">We present a case study on full-lifecycle DCN digital twins. The proposal can automatically optimize DCNs via LLM with Retrieval Augmented Generation (RAG) and solve optimization problems via diffusion-Deep Reinforcement Learning (DRL). We evaluate our proposal by minimizing the retrieval latencies. Moreover, our digital twin is hosted by DCNs, revealing the virtuous interaction between GenAI and DCNs.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Preliminaries: Data Center Networking, Discriminative AI, and Generative AI</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Basics of Data Center Networking (DCN)</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">DCN refers to the integration of hardware and software infrastructure that facilitates the interconnection and management of numerous heterogeneous servers within a data center to realize efficient communication and high-quality services <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib6" title="">6</a>]</cite>.
This setup hinges on three essential components, i.e., hardware, software, and protocols.
Specifically, hardware involves various networking devices, such as switches, routers, and firewalls.
Software is deployed on the DCN hardware, realizing efficient network management, protection, and service provisioning.
Finally, protocols define the standards for heterogeneous DCN hardware/software regarding data transportation, storage, etc.
As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">1</span></a>, configuring these components in different ways leads to diverse DCN architectures, such as three-tier, fat tree, and DCell <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Issues of DCN and Solutions from DAI</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">To manage large-scale DCNs that accommodate dense devices, a series of mechanisms should be carefully designed and developed.
In this part, we review the major technical issues of DCNs, as well as the solutions based on DAI models.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.5.1.1">II-B</span>1 </span>Topology and Routing</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The topology of DCNs indicates how servers are interconnected, impacting the latency and reliability of data transmission.
Then, efficient routing protocols can be deployed to find the optimal paths for data packets to travel from source to destination.
Optimizing topology and routing is crucial for minimizing latency, managing bandwidth more effectively, and ensuring resilience against failures.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">By harnessing DAI, network operators can precisely model and forecast network behaviors under various load conditions, enabling them to proactively adjust topologies and enhance the overall network performance. For instance, Xie <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p2.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib7" title="">7</a>]</cite> presented Topology2Vec, which first encodes topological features by graphs and trains a DAI model to optimize controller placement in complicated DCNs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.5.1.1">II-B</span>2 </span>Load Balancing</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Load balancing aims to distribute the workload across numerous DCN servers to ensure optimal resource utilization and prevent the impacts of stragglers. Moreover, available resources should be placed and scheduled in a flexible way to fit the workload pattern.
DAI, especially DRL, significantly revolutionizes load balancing in DCNs. Specifically, the efficacy corresponding to each allocation scheme can be modeled by rewards. Afterward, the optimal scheme can be gradually approached through a Markov process, following the DRL paradigm. For instance, Liu <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib3" title="">3</a>]</cite> leveraged Deep Q-learning (DQN) to optimize data placements in DCNs and minimize the data movement latency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS3.5.1.1">II-B</span>3 </span>Traffic Management</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Traffic management in DCNs involves predicting, controlling, and optimizing both east-west and north-south data flows. Effective traffic management facilitates DCNs to avoid congestion, reduce latency, and improve Quality of Service (QoS). Considering the complex temporal-spatial dependencies existing in network traffic, researchers utilize diverse Deep Neural Networks (DNNs) to infer the relationship between network factors and traffic performance, thus supporting accurate traffic pattern predictions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS4.5.1.1">II-B</span>4 </span>Fault Tolerance and Resiliency</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">Resiliency in DCNs is crucial for maintaining high availability and continuous service delivery.
It involves mechanisms such as anomaly detection and predictive maintenance, which predict and prevent equipment failures before they occur.
Additionally, disaster recovery protocols allowing rapid response and restoration in the event of a failure are required.
DAI is widely adopted to predict anomalies and failures.
For instance, Ilager et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib9" title="">9</a>]</cite> trained a multiple Multi-Layer Perception (MLP) model to predict the temperature of DCN servers under different workload conditions, thereby facilitating proactive maintenance.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p2">
<p class="ltx_p" id="S2.SS2.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS4.p2.1.1">Perspectives</span>: The above discussion demonstrates that DAI is usually applied in DCNs in two ways.
First, by inferring the relationship between inputs and outputs, DAI exhibits strong predictive ability, enabling it to forecast workload, traffic patterns, and even potential failures accurately.
Second, DAI, especially DRL, excels in complex decision-making processes, thus greatly facilitating tasks such as workload allocation, routing optimization, and the development of advanced defense mechanisms.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Existing Challenges</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Despite the above progress, several challenges persist where DAI cannot effectively aid DCNs. Alternatively, GenAI can potentially offer solutions:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Insufficient Training Data</span>: Given the topological complexity of DCNs, DNNs require substantial scale to represent and learn complicated environmental features. Training large DNNs necessitates vast amounts of precisely labeled data. Nonetheless, there are often challenges in ensuring sufficient training data for DCNs. First, the high cost of labeling massive DCN data, coupled with the rapid changes in network conditions, makes it difficult to gather robust datasets. Additionally, the security-sensitive nature of data in DCNs can restrict the availability of real-world data for training purposes.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Limited Versatility</span>: The applications of DAI models are limited to making predictions/decisions. Specifically, each DAI model is dedicated to performing a specific function. This one-to-one paradigm limits the seamless integration of AI technology in DCNs. Moreover, numerous advanced DCN tasks are generative rather than predictive, such as process automation and responding to unseen issues, which are beyond DAI’s capabilities.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Weak Adaptability and Generalization</span>: Another major challenge is regarding the flexibility of DAI models across heterogeneous DCN environments. Each data center has unique configurations, traffic patterns, and security requirements. DAI models trained on one set of network conditions may not perform well in other settings without extensive customization and tuning, making it difficult to adapt to diverse infrastructures. Even though some techniques, such as transfer learning, can help, they are not versatile, complex to employ, or require resource-intensive re-training.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">GenAI and LLM: New Opportunities</span>
</h3>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS1.5.1.1">II-D</span>1 </span>Learning Objective</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.4">Recall that DAI is trained to make predictions and classifications.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">1</span></a>, for inputs <math alttext="X" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.1.m1.1"><semantics id="S2.SS4.SSS1.p1.1.m1.1a"><mi id="S2.SS4.SSS1.p1.1.m1.1.1" xref="S2.SS4.SSS1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.1.m1.1b"><ci id="S2.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.1.m1.1d">italic_X</annotation></semantics></math> and outputs <math alttext="Y" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.2.m2.1"><semantics id="S2.SS4.SSS1.p1.2.m2.1a"><mi id="S2.SS4.SSS1.p1.2.m2.1.1" xref="S2.SS4.SSS1.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.2.m2.1b"><ci id="S2.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.2.m2.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.2.m2.1d">italic_Y</annotation></semantics></math>, the learning objective of DAI is to acquire the conditional probability <math alttext="P(Y|X)" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.3.m3.1"><semantics id="S2.SS4.SSS1.p1.3.m3.1a"><mrow id="S2.SS4.SSS1.p1.3.m3.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.cmml"><mi id="S2.SS4.SSS1.p1.3.m3.1.1.3" xref="S2.SS4.SSS1.p1.3.m3.1.1.3.cmml">P</mi><mo id="S2.SS4.SSS1.p1.3.m3.1.1.2" xref="S2.SS4.SSS1.p1.3.m3.1.1.2.cmml">⁢</mo><mrow id="S2.SS4.SSS1.p1.3.m3.1.1.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.cmml"><mo id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.2" stretchy="false" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.2" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.3" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.3.cmml">X</mi></mrow><mo id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.3" stretchy="false" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.3.m3.1b"><apply id="S2.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1"><times id="S2.SS4.SSS1.p1.3.m3.1.1.2.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.2"></times><ci id="S2.SS4.SSS1.p1.3.m3.1.1.3.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.3">𝑃</ci><apply id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1"><csymbol cd="latexml" id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.2">𝑌</ci><ci id="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.1.1.3">𝑋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.3.m3.1c">P(Y|X)</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.3.m3.1d">italic_P ( italic_Y | italic_X )</annotation></semantics></math>.
In contrast, GenAI models aim to obtain the joint probability <math alttext="P(X,Y)" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.4.m4.2"><semantics id="S2.SS4.SSS1.p1.4.m4.2a"><mrow id="S2.SS4.SSS1.p1.4.m4.2.3" xref="S2.SS4.SSS1.p1.4.m4.2.3.cmml"><mi id="S2.SS4.SSS1.p1.4.m4.2.3.2" xref="S2.SS4.SSS1.p1.4.m4.2.3.2.cmml">P</mi><mo id="S2.SS4.SSS1.p1.4.m4.2.3.1" xref="S2.SS4.SSS1.p1.4.m4.2.3.1.cmml">⁢</mo><mrow id="S2.SS4.SSS1.p1.4.m4.2.3.3.2" xref="S2.SS4.SSS1.p1.4.m4.2.3.3.1.cmml"><mo id="S2.SS4.SSS1.p1.4.m4.2.3.3.2.1" stretchy="false" xref="S2.SS4.SSS1.p1.4.m4.2.3.3.1.cmml">(</mo><mi id="S2.SS4.SSS1.p1.4.m4.1.1" xref="S2.SS4.SSS1.p1.4.m4.1.1.cmml">X</mi><mo id="S2.SS4.SSS1.p1.4.m4.2.3.3.2.2" xref="S2.SS4.SSS1.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S2.SS4.SSS1.p1.4.m4.2.2" xref="S2.SS4.SSS1.p1.4.m4.2.2.cmml">Y</mi><mo id="S2.SS4.SSS1.p1.4.m4.2.3.3.2.3" stretchy="false" xref="S2.SS4.SSS1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.4.m4.2b"><apply id="S2.SS4.SSS1.p1.4.m4.2.3.cmml" xref="S2.SS4.SSS1.p1.4.m4.2.3"><times id="S2.SS4.SSS1.p1.4.m4.2.3.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.2.3.1"></times><ci id="S2.SS4.SSS1.p1.4.m4.2.3.2.cmml" xref="S2.SS4.SSS1.p1.4.m4.2.3.2">𝑃</ci><interval closure="open" id="S2.SS4.SSS1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.2.3.3.2"><ci id="S2.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1">𝑋</ci><ci id="S2.SS4.SSS1.p1.4.m4.2.2.cmml" xref="S2.SS4.SSS1.p1.4.m4.2.2">𝑌</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.4.m4.2c">P(X,Y)</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.4.m4.2d">italic_P ( italic_X , italic_Y )</annotation></semantics></math>.
Such a learning objective enables GenAI models to capture intricate details about the underlying input distribution and latent features.
In contrast, the objective of DAI is typically more focused and direct, i.e., mapping input features directly to outputs.
Consequently, DAI is optimal for tasks such as classification but may not capture the broader underlying data characteristics that are not directly relevant to the specific task.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS2.5.1.1">II-D</span>2 </span>Model Architecture</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">Notable GenAI models include Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), diffusion models, and transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib1" title="">1</a>]</cite>.
Specifically, VAEs operate through an encoder-decoder structure. The encoder compresses input data into a latent space representation, capturing essential data features, while the decoder leans to reconstruct the data back to its original form.
Afterward, new instances can be generated by sampling and decoding different points from the latent space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib1" title="">1</a>]</cite>.
Likewise, GANs consist of two competing models: a generator that creates realistic data and a discriminator that evaluates whether the output is synthetic.
This architecture trains through a zero-sum game where the generator improves its ability to produce increasingly realistic outputs as the discriminator becomes better at distinguishing real from synthetic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib1" title="">1</a>]</cite>.
In contrast, inspired by non-equilibrium thermodynamics, diffusion models operate by a forward diffusion process that gradually disturbs data samples into random noise, during which a denoising network is trained.
Then, a denoising process is performed to generate new instances that mirror the original distribution through learned noise reduction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.1">LLMs, exemplified by the Generative Pre-trained Transformer (GPT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib1" title="">1</a>]</cite>, are a subset of GenAI specifically designed for understanding and generating natural language.
As such, LLMs can accomplish tasks such as summarization, translation, and question-answering.
Central to their architecture is the transformer model, which utilizes self-attention mechanisms to process sequential data, such as text.
Moreover, to enhance the capability of understanding long inputs and generating complicated answers, LLMs typically incorporate numerous transformer layers.
For instance, GPT-3 accommodates 96 transformer layers, each of which contains 96 attention heads, resulting in a large mode size with 175 billion parameters<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Data available on: https://lambdalabs.com/blog/demystifying-gpt-3</span></span></span>.
Empowered by ever-enlarging models, alongside technical advancements such as cross-modal attention and mixture-of-experts architectures, LLMs are extending their capabilities beyond text generation.
Nowadays, multimodal LLMs, such as GPT-4, can effectively understand and generate images, videos, and even 3D objects.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="357" id="S2.F2.g1" src="x2.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustration of GenAI and LLM’s applications in DCNs. The number of peer-reviewed publications regarding GenAI and LLM per year is shown on the left-hand side (the publication data was collected from IEEE Xplore in Sept. 2024).</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Enhancing Data Center Networking with GenAI and LLM</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we review existing works that apply GenAI and LLMs to enhance DCNs (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S2.F2" title="Figure 2 ‣ II-D2 Model Architecture ‣ II-D GenAI and LLM: New Opportunities ‣ II Preliminaries: Data Center Networking, Discriminative AI, and Generative AI ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Data Augmentation</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">As a representative generative task, data augmentation in DCNs can be efficiently supported by GenAI.
For instance, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib10" title="">10</a>]</cite> presented a GAN-based approach to generate DCN workloads, thereby enriching the datasets to train task scheduling models.
Traditionally, researchers assume that task arrivals in DCNs follow specific distributions (e.g., <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.2.1">Erlang</span> or <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.2.2">Normal</span>) and fit models to these assumptions to estimate parameters.
However, this strategy requires prior knowledge about the workload pattern, which might not always be available or accurate from a practical perspective.
Moreover, such strategies can only capture simple patterns, whereas current DCN workloads exhibit more complex distributions that are influenced by a variety of unpredictable factors, e.g., instant peak during rush hours or special events.
Hence, the authors developed a GAN model to synthesize realistic workload data from samples.
The authors evaluated the validity of synthesized data using square error, which is calculated by generating a multi-bin histogram from the generated data and the original data.
The results show that the GAN-based method can maintain an error of less than 3.558<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mo id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><times id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">×</annotation></semantics></math>10<sup class="ltx_sup" id="S3.SS1.p1.2.3"><span class="ltx_text ltx_font_italic" id="S3.SS1.p1.2.3.1">-4</span></sup>, with much lower time and costs than the conventional methods.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Similarly, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib11" title="">11</a>]</cite> leveraged GANs to address data imbalance in fault diagnosis systems for DCNs, a common challenge due to the rarity of fault instances compared to normal operational data.
The GAN effectively balanced the training dataset by synthesizing fault data to supplement the limited real fault examples, enhancing the model’s ability to recognize and diagnose faults.
The experimental results demonstrate that balanced datasets can reduce the miss alert rate from 10.7% to 0.5%.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">DCN Process Automation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Building large-scale DCNs is a multifaceted challenge that encompasses server arrangements, network connectivity, cooling systems, and more, all tailored to specific site conditions and services.
To facilitate the deployment and operation of DCNs, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib5" title="">5</a>]</cite> presented ChatTwin, an LLM-empowered conversational system, to generate DCN configurations automatically.
Specifically, leveraging the advanced natural language understanding capability of ChatGPT-4, ChatTwin first analyzes user requirements through a segment-and-generate approach.
For example, with the user input, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">“I need a data hall room with one Air Conditioner Unit (ACU) and eight racks. Each rack contains three servers,”</span> ChatTwin parses and segments this input into discernible units: one ACU, eight racks, and three servers per rack.
This segmentation involves identifying key terms and their associated quantities.
Afterward, ChatTwin proceeds to fill out the corresponding DCN configuration files to define each unit.
Experimental results demonstrate that such an LLM-empowered method can achieve a successful DCN construction rate of 87% while saving considerable time.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Apart from constructions, a variety of DCN processes can be automated by LLM in a similar way.
For instance, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite>, the authors employed GPT-4 to automatically compress and analyze DCN operation logs, thus finding implicit anomalies.
The <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">Uptime Institute Data Center Resiliency Survey 2023<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote4.1.1.1">4</span></span><span class="ltx_text ltx_font_upright" id="footnote4.5">The Survey is available at: https://uptimeinstitute.com/resources/research-and-reports/uptime-institute-global-data-center-survey-results-2023</span></span></span></span></span> reveals that 39% of data center operators have experienced a serious outage because of human error, of which 50% were the result of a failure to follow the correct procedures.
In this case, leveraging LLMs or foundation models can significantly mitigate these issues by automating procedural compliance and decision-making processes in DCNs.
Additionally, LLMs can train personnel by providing real-time feedback, reducing the risk of procedural deviations that lead to serious outages.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Anomaly Detection</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Given the stringent requirements of DCNs for reliability, anomalies should be detected and tackled promptly.
For instance, sudden, unexpected increases in network traffic that deviate significantly from normal patterns might be indicative of denial-of-service attacks.
GenAI has been widely adopted for anomaly detection in the DCN context.
The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib12" title="">12</a>]</cite> proposed an innovative approach to DCN anomaly detection via autoencoders.
The autoencoder is trained with only normal traffic data and is expected to recover any given input as close as possible to the learned normal patterns.
Therefore, an input instance can be classified as an attack if its reconstruction error is larger than a predefined threshold; otherwise, it is regarded as normal.
We can observe that the strong distribution representation capability enables GenAI models to support not only generation but also classification tasks.
Furthermore, GenAI’s adaptability is showcased through its ability to construct diverse conditions, thus benchmarking the robustness of DCNs facing various anomalies.
For instance, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib13" title="">13</a>]</cite> developed a conditional GAN, utilizing a Gaussian noise with adjustable standard deviation to indicate how anomalies are desired in the generated traffic patterns.
Afterward, various conditions can be synthesized, which ensures that the DCN can be rigorously tested against a spectrum of potential issues.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Quality of Service Modeling</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Apart from classification, GenAI models can facilitate predictive tasks in DCN.
The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib14" title="">14</a>]</cite> developed a deep generative network for DCNs named Deep-Q, leveraging a VAE to predict the QoS under different traffic load conditions.
Intuitively, DAI models can solve this problem by learning the mapping relationship between input and output and predicting the exact QoS value under the given traffic load.
In contrast, GenAI generates QoS distributions, which more accurately reflect DCN realities, as many unseen factors, such as packet loss and queue dynamics, make QoS measures follow a complex distribution rather than deterministic.
Experimental results proved that Deep-Q achieves 3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mo id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><times id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">×</annotation></semantics></math> higher QoS prediction accuracy than traditional methods.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">Perspectives</span>: Our review illustrates that GenAI can not only independently address DCN tasks such as process automation, but also effectively enhance existing DAI methods. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">1</span></a>, GenAI can expand DAI training data, help DAI extract information from complex environments, dynamically select DAI models, etc. These collaborative paradigms need further research.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Transforming DCNs for GenAI/LLM</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we present how DCNs evolve to efficiently support GenAI and LLM.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="S4.F3.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Top: The illustration of GenAI/LLM lifecycle. We can observe that most of the issues happen in the pre-training stage. Bottom: The centralized and decentralized DCNs.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Characterization of GenAI/LLM Workload</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">GenAI/LLM in DCNs follows a distinct lifecycle with three key stages.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.F3" title="Figure 3 ‣ IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">3</span></a>(top), pre-training is initially performed, where GenAI models are trained on expansive datasets in a self-supervised manner to accumulate generalizable knowledge and construct a broad capability to handle diverse tasks (e.g., conversation and text-to-image generation).
Afterward, the GenAI models undergo task-oriented fine-tuning, usually trained on specific labeled datasets via supervised or reinforcement learning.
This fine-tuning will align the GenAI models with the particular requirements of certain downstream tasks.
Note that such alignment can also be realized by prompt engineering, i.e., users craft the prompts for instructing GenAI/LLM during the inference stage.
Finally, well-trained models are deployed to perform GenAI inferences and accomplish user demands.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">According to a six-month trace of LLM workload in practical DCNs, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite> stated that the discrepancies between GenAI/LLMs and prior task-specific deep learning workload exist in the following three aspects.</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Imbalanced Resource Usage</span>: The resource usage of LLMs exhibits a notable imbalance. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.F3" title="Figure 3 ‣ IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">3</span></a>(top), pre-training tasks, though they represent merely 3.2% of the total job count, disproportionately consume 94.0% of all GPU resources. In contrast, generative inferences, which make up 92.9% of job activities, utilize only 0.8% of the available resources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite>. Secondly, there is an uneven allocation among the infrastructure components. Specifically, while CPU, host memory, and network resources are often underutilized, GPU resources are consistently at high utilization levels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Bursty Traffic Patterns</span>: Traditional cloud computing tasks typically handle millions of flows with traffic utilization averaging below 20%, exhibiting a steady and gradual pattern that changes on an hourly basis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib15" title="">15</a>]</cite>. In contrast, LLM training involves infrequent but intense bursty flows. The Network Interface Card (NIC) sporadically handles substantial data volumes, quickly reaching full network capacity and sustaining this level for several seconds to tens of seconds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib15" title="">15</a>]</cite>. This pattern stems from the necessity for gradient synchronization during LLM training. Each training iteration requires synchronization of data across multiple GPU groups during the backward phase, resulting in sharp traffic spikes.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Sensitive to Failures</span>: According to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite>, nearly 40% LLM pre-training jobs fail, mainly caused by infrastructure failures, GPU overheating, and connection errors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib15" title="">15</a>]</cite>. In contrast, inference jobs seldom encounter failures since they only call the well-trained generative models. Given the long period of pre-training, failure recovery is extremely costly. Nowadays, the GenAI/LLM training typically leverages checkpoints to recover from failures. Nonetheless, generating a checkpoint requires substantial storage (e.g., 30GB per GPU in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib15" title="">15</a>]</cite>) and incurs high overhead (e.g., 100s in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib15" title="">15</a>]</cite>).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">DCNs in the GenAI/LLM Era</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Facing the unique workload patterns of GenAI/LLM in DCNs, researchers are exploring mechanisms to optimize data center architectures, ensuring more efficient and robust operations.
First, federated learning can be leveraged to overcome the tremendous resource overhead of GenAI/LLM pre-training.
By organizing geographically distributed DCNs and scheduling their computing and data resources, the pre-training workload can be balanced, avoiding assigning excessive burdens to one certain server.
To enhance the robustness of DCNs against bursty traffics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib15" title="">15</a>]</cite>, the authors presented the dual Top-of-Rack (ToR) architecture, which significantly reduces failure rates of GenAI/LLM pre-training tasks by eliminating dependencies on direct links between switches.
Furthermore, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite> presented fault-tolerant pretraining solutions that enhance reliability through asynchronous checkpoints and LLM-assisted automated diagnosis.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S4.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A: The automatic optimization formulation. B: Diffusion-DRL for optimization solving. C &amp; D: The comparison of optimization formulation between the backbone GPT-4 with the proposed digital twin.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Apart from building efficient GenAI/LLM models, the sustainability of DCNs also attracts great attention, due to the considerable resource consumption of pre-training.
For instance, POLCA is a power oversubscription framework that capitalizes on the unused power headroom by allowing 30% more server deployments within existing power infrastructures.
From the task perspective, hybrid DCN leverages a cost-based scheduling framework to dynamically assign LLM tasks to the most suitable hardware.
It optimizes task allocation by deciding whether tasks should be processed on energy-efficient processors or high-performance GPUs, guided by the size of each task’s input and output tokens.
Such a workload-aware approach decreases GPU energy consumption by 7.5% compared to a workload-unaware baseline.
Furthermore, cloud-edge collaboration is proposed to realize energy-efficient GenAI/LLM.
By integrating edge databases, the generated content/response can be cached, significantly reducing the frequency and volume of data queries needing cloud processing.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Perspectives</span>: Looking ahead, we believe that DCNs will evolve in two directions (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.F3" title="Figure 3 ‣ IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">3</span></a>(bottom)). First, centralized DCNs will continue to optimize storage, communication, and computing efficiency (such as by reducing transmission and memory read/write latency) to improve parallelism and performance, thereby supporting large generative models. Meanwhile, decentralized DCNs based on distributed or federated learning will be further developed to integrate massive resources and data over the network.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Case Study: Towards Full-Lifecycle DCN Digital Twin via GenAI</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we perform a case study using LLM to implement the digital twin that covers the entire DCN lifecycle.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Problem Statement</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">From the operator’s perspective, the complete DCN lifecycle consists of configuration, optimization, and management phases.
First, configuration refers to designing DCNs, setting the number of servers, the layout of racks, and connections <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib5" title="">5</a>]</cite>.
Optimization entails improving strategies for routing, resource allocation, load balancing, etc.
Finally, management focuses on ensuring the DCN operations, including anomaly detection, fault diagnosis, and recovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite>.
<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">Consequently, a full-lifecycle digital twin should realize the end-to-end automation of all these stages.</span>
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib5" title="">5</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib4" title="">4</a>]</cite>, researchers demonstrated the effectiveness of LLMs in designing and managing DCNs, respectively.
Hence, this case study addresses the research gap toward full-lifecycle DCN digital twins by exploring automatic DCN optimization.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Proposed Framework</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Our framework consists of two modules, intending to make DCN optimization easier and better, respectively.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS1.5.1.1">V-B</span>1 </span>Automatic Optimization Formulation</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">DCN operators need to formulate optimization problems manually, which involves gathering the relevant factors and modeling their mutual effects on the objective via appropriate formulas.
However, this process is time-consuming and heavily relies on the operator’s experience.
Moreover, given that DCN environments exhibit great heterogeneity, human-crafted formulations may fail to reflect the complicated but implicit mutual effects of factors precisely.
To this end, we present the automatic DCN optimization formulation leveraging an LLM with RAG.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.F4" title="Figure 4 ‣ IV-B DCNs in the GenAI/LLM Era ‣ IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">4</span></a>-A, the LLM is first fed with DCN-related documents (academic papers, standards, etc.), thereby increasing its expertise in DCN-related optimizations, such as load-balancing and routing.
Specifically, textual documents are vectorized into word embeddings, which are split into chunks and saved in the DCN.
Then, operators can formulate the optimization problem in a conversational way.
In detail, the operator’s description of the DCN state, optimization objective, and key factors are vectorized.
Afterward, the digital twin calculates the semantic distance between inputs and all preserved chunks.
The most relevant chunks will be fetched and serve as external knowledge, with which LLM is further employed to generate coherent responses that promote the process of problem formulation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS2.5.1.1">V-B</span>2 </span>Diffusion-empowered Optimization Solving</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">To solve the formulated optimization problems, we leverage the DRL paradigm and adopt Diffusion-DRL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#bib.bib1" title="">1</a>]</cite>.
Particularly, it follows the actor-citric architecture while building the actor network via diffusion.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.F4" title="Figure 4 ‣ IV-B DCNs in the GenAI/LLM Era ‣ IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">4</span></a>-B, in each training round, the decision variables are initialized randomly and sent to the denoising network.
Following the diffusion principle, the noisy decision variables are de-noised through a Markov denoising process.
The final decision variables are then put into the specific DCN environment and acquire the rewards, which are designed as the Q-value and guide the refinement of diffusion’s parameters.
The state, action, and reward are defined as follows.</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">State</span>: State describes the DCN environment, including the values of all the factors that affect the reward calculation.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Action</span>: The decision variable that should be optimized.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Reward</span>: The efficiency of applying the learned action in the current state, indicating the desirability of the actions.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Evaluation</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To evaluate the effectiveness of the proposed digital twin, we first leverage it to model a representative DCN optimization problem, i.e., data placement.
Recall that each DCN contains numerous interconnected servers.
The server selection when writing data to DCN significantly affects the service latency (e.g., the RAG latency).
To this end, we send baseline GPT-4 and our digital twin with the same prompt, which describes the objective of minimizing RAG latency by optimizing the placement of knowledge chunks.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.F4" title="Figure 4 ‣ IV-B DCNs in the GenAI/LLM Era ‣ IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">4</span></a>-C, the baseline GPT-4’s output is general and superficial.
Although it successfully models the objective of latency minimization, various factors that may affect the performance are ignored.
In contrast, contributed to RAG, our digital twin routes to the most relevant documents via semantic router (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S4.F4" title="Figure 4 ‣ IV-B DCNs in the GenAI/LLM Era ‣ IV Transforming DCNs for GenAI/LLM ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">4</span></a>-D).
Then, it fetches the key expertise, fuses it with the pre-trained knowledge of LLMs, and generates the final problem formulation.
We can observe that both the knowledge writing and the following reading operations are considered.
Moreover, the objective is defined as the weighted sum of the reciprocals of read &amp; write latencies, thus reflecting the pursuit of minimizing RAG latency.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Then, we explore the efficiency of the adopted Diffusion-DRL in solving optimization problems.
Particularly, we implement two default strategies, i.e., the knowledge is randomly placed (named <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">Random</span>), and the knowledge is placed in the serve with the lowest reading latency (named <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">Greedy</span>).
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09343v1#S6.F5" title="Figure 5 ‣ VI Future Research Directions ‣ Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study"><span class="ltx_text ltx_ref_tag">5</span></a>, the random policy leads to dramatic performance fluctuation, which may cause stragglers.
Aiming at minimizing the reading latency, the greedy policy achieves higher rewards and a higher level of stability.
Finally, the proposed Diffusion-DRL approach outperforms the baselines and can efficiently reduce the retrieval latency, thus facilitating the DCN digital twin to manage large-scale knowledge.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Future Research Directions</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we outline three main future directions for deepening the interplay between GenAI/LLM and DCN.</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">LLM-enhanced DCN Optimization</span>: Apart from diffusion models that enhance the exploration capability of DRL, LLM also facilitates DCN decision-making. For instance, LLM can serve as automatic agents and provide human-like feedback to each action, thus improving the human-centric DCN performance.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">Embodied DCN Digital Twin</span>: LLM is the building block for constructing embodied DCN digital twins. First, its multimodal understanding capability enables the digital twin to perceive complex physical environments. Moreover, LLMs’ large size allows life-long continuous learning, allowing detail twins to keep reinforcing their policies by fine-tuning.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">DCN Security in the LLM Era</span>: Future research should focus on developing robust mechanisms to safeguard against adversarial attacks tailored to exploit the vulnerabilities of LLM-driven systems, such as the poisoning attacks toward training data and deepfake.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S6.F5.g1" src="x5.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The training curve and rewards of random, greedy, and the proposal knowledge placement policies.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This article has explored the interplay between GenAI and DCN.
From the perspective of how GenAI/LLM enhances DCNs, we have introduced the existing challenges in the DCN field and reviewed the literature about how GenAI/LLM offers new possibilities.
Then, shifting the viewpoint to how DCN evolved to serve GenAI/LLM, we analyzed the characteristics of GenAI/LLM workloads on DCNs as well as the corresponding requirements.
Finally, we have performed a case study on developing full-lifecycle DCN digital twins.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Liu, H. Du, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, and A. Jamalipour, “Deep generative model and its applications in efficient wireless network management: A tutorial and case study,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IEEE Wireless Communications</em>, pp. 1–9, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L. Gu, J. Hu, D. Zeng, S. Guo, and H. Jin, “Service function chain deployment and network flow scheduling in geo-distributed data centers,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Transactions on Network Science and Engineering</em>, vol. 7, no. 4, pp. 2587–2597, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K. Liu, J. Peng, J. Wang, B. Yu, Z. Liao, Z. Huang, and J. Pan, “A learning-based data placement framework for low latency in data center networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on Cloud Computing</em>, vol. 10, no. 1, pp. 146–157, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Q. Hu, Z. Ye, Z. Wang, G. Wang, M. Zhang, Q. Chen, P. Sun, D. Lin, X. Wang, Y. Luo, Y. Wen, and T. Zhang, “Characterization of large language model development in the datacenter,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">USENIX Symposium on Networked Systems Design and Implementation</em>, 2024, p. 709–729.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Li, R. Wang, X. Zhou, Z. Zhu, Y. Wen, and R. Tan, “Chattwin: Toward automated digital twin generation for data center via large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation</em>, 2023, p. 208–211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Zhang, F. R. Yu, S. Wang, T. Huang, Z. Liu, and Y. Liu, “Load balancing in data center networks: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Communications Surveys &amp; Tutorials</em>, vol. 20, no. 3, pp. 2324–2352, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Z. Xie, L. Hu, K. Zhao, F. Wang, and J. Pang, “Topology2vec: Topology representation learning for data center networking,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE Access</em>, vol. 6, pp. 33 840–33 848, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Yu, H. Yang, K. K. Nguyen, J. Zhang, and M. Cheriet, “Burst traffic scheduling for hybrid e/o switching dcn: An error feedback spiking neural network approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Network and Service Management</em>, vol. 18, no. 1, pp. 882–893, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Ilager, K. Ramamohanarao, and R. Buyya, “Thermal prediction for efficient energy management of clouds using machine learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Parallel and Distributed Systems</em>, vol. 32, no. 5, pp. 1044–1056, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. R. Haverkort, F. Finkbeiner, and P.-T. de Boer, “Machine learning data center workloads using generative adversarial networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">SIGMETRICS Perform. Eval. Rev.</em>, vol. 48, no. 2, p. 21–23, nov 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Du, K. Chen, S. Chen, J. He, X. Zhu, and X. Jin, “Deep learning gan-based data generation and fault diagnosis in the data center hvac system,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Energy and Buildings</em>, vol. 289, p. 113072, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Torabi, S. L. Mirtaheri, and S. Greco, “Practical autoencoder based anomaly detection by using vector reconstruction error,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Cybersecurity</em>, vol. 6, no. 1, pp. 1–13, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Pérez, P. Arroba, and J. Moya, “Data augmentation through multivariate scenario forecasting in data centers using generative adversarial networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Applied Intelligence</em>, vol. 53, pp. 1469–1486, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Xiao, D. He, and Z. Gong, “Deep-q: Traffic-driven qos inference using deep generative network,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2018 Workshop on Network Meets AI &amp; ML</em>, 2018, p. 67–73.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. Qian, Y. Xi, J. Cao, J. Gao, Y. Xu, Y. Guan, B. Fu, X. Shi, F. Zhu, R. Miao, C. Wang, P. Wang, P. Zhang, X. Zeng, E. Ruan, Z. Yao, E. Zhai, and D. Cai, “Alibaba hpn: A data center network for large language model training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ACM SIGCOMM</em>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 14 07:17:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
