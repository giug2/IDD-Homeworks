<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.08247] PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese</title><meta property="og:description" content="We present in this paper a novel scheme for multimodal learning named the Parallel Attention mechanism. In addition, to take into account the advantages of grammar and context in Vietnamese, we propose the Hierarchical…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.08247">

<!--Generated on Wed Feb 28 17:43:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Information Fusion,  Visual Question Answering,  Attention,  MultiModal Learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nghia Hieu Nguyen<sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_italic">1,2,3</span></sup>, Kiet Van Nguyen<sup id="id8.8.id2" class="ltx_sup"><span id="id8.8.id2.1" class="ltx_text ltx_font_italic">1,2,4</span></sup> 
<br class="ltx_break"><sup id="id9.9.id3" class="ltx_sup"><span id="id9.9.id3.1" class="ltx_text ltx_font_italic">1</span></sup>Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam 
<br class="ltx_break"><sup id="id10.10.id4" class="ltx_sup"><span id="id10.10.id4.1" class="ltx_text ltx_font_italic">2</span></sup>Vietnam National University, Ho Chi Minh City, Vietnam 
<br class="ltx_break">Email: <sup id="id11.11.id5" class="ltx_sup"><span id="id11.11.id5.1" class="ltx_text ltx_font_italic">3</span></sup>19520178@gm.uit.edu.vn, <sup id="id12.12.id6" class="ltx_sup"><span id="id12.12.id6.1" class="ltx_text ltx_font_italic">4</span></sup>kietnv@uit.edu.vn
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">We present in this paper a novel scheme for multimodal learning named the Parallel Attention mechanism. In addition, to take into account the advantages of grammar and context in Vietnamese, we propose the Hierarchical Linguistic Features Extractor instead of using an LSTM network to extract linguistic features. Based on these two novel modules, we introduce the Parallel Attention Transformer (PAT), achieving the best accuracy compared to all baselines on the benchmark ViVQA dataset and other SOTA methods including SAAA and MCAN.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Information Fusion, Visual Question Answering, Attention, MultiModal Learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multimodal learning recently attracted lots of attention from the research community because of its exciting and challenging requirements. Multimodal learning aims to explore how to extract and fuse multimodal information effectively. Typical tasks of multimodal learning can be listed as Visual Question Answering (VQA) where a machine is required to answer a given question based on visual information from a given image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, Image Captioning (IC) where a machine is required to generate natural language captions that describe the content of the given image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, or Visual Grounding where a machine is required to draw bounding boxes on images that indicate objects mentioned in a given query using natural language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Most attention concentrates on the multimodal tasks relevant to visual-textual information, particularly the VQA task. Current approaches on VQA treat this task as an answers classification task. This guide the development of VQA methods focusing on studying the most effective scheme to fuse information from the given image and question in order to select the best accurate candidate among a given set of answers. According to the survey study of Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, based on the way of performing attention, VQA methods can be grouped into two types: single-hop attention methods and multi-hop attention methods. On large benchmark VQA for English, various works show that single-hop attention methods do not achieve good results compared to multi-hop attention methods.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present a new multi-hop attention method for fusing information from images. Our experimental results prove that single-hop attention methods find difficulty when they tackle the VQA even on a small-size dataset as ViVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related works</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">VQA datasets</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Antol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> first introduced the VQA task by releasing the VQAv1 dataset. This dataset includes 254,721 images with 764,163 questions and 4,598,610 answers. Most of the attention is drawn to the VQAv1 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and many attention mechanisms were proposed that still affect the mindset of design for later methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> such as Co-Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and Stacked Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Results of former studies on the VQAv1 dataset achieved pretty good results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> by treating the VQA task as answer selection over a defined set of candidates or answer classification. However, as other classification tasks, answer imbalance in the VQAv1 dataset forms a novel problem that was indicated by Goyal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Goyal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proved that former VQA methods obtained good results on the VQAv1 dataset as they suffered from the language prior methods. Particularly, when being given a question, former VQA methods recognize its pattern and select the most apparent answer belonging to that pattern as the candidate, despite the visual information of the images.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">To overcome the language prior phenomenon, Goyal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> balanced the VQAv1 datasets and then proposed the VQAv2 dataset. Goyal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> constructed lots of experiments and showed that former VQA methods did not perform well as they had behaved. The VQAv2 dataset contains 204,721 images with 1,105,904 questions and 11,059,040 answers, which becomes the largest benchmark for the VQA task in English.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Recent studies constructed VQA datasets that required reading comprehension of VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Moreover, to develop a VQA system that can use incorporate knowledge while answering the given questions, lots of datasets were released <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. On the other side, former VQA methods were designed to select answers rather than forming sentences to answer as humans. From that on, there are works conducted the open-ended VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to research the answer-generation methods instead of answer-selection ones.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">In Vietnamese, the first VQA dataset was introduced by Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. This dataset was constructed based on the COCO-QA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> using a semi-automatic method. Recently, Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> introduced the multilingual VQA dataset, the UIT-EVJVQA dataset, in three languages Vietnamese, English, and Japanese. This dataset is the first open-ended VQA dataset that includes Vietnamese. In addition, Nghia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> presented a Vietnamese open-ended VQA dataset consisting of 11,000+ images associated with 37,000+ question-answer pairs (QAs).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">VQA methods</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Former VQA methods were designed based on the attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. One well-known baseline on the VQAv1 dataset is the Hierarchical Co-Attention Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> which used the Convolutional Neural Network (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> to extract the n-gram features from questions and used the co-attention to perform attention mechanism over questions and images. Later studies based on this co-attention proposed various methods such as ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, or LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Another strong baseline on the VQAv1 dataset proposed by Kazemi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> introduces the Stack Attention. This kind of attention stacks the visual features and linguistic features together and then yielded the attention map over the two kinds of features. Later work proposed methods based on Stack Attention but using transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> such as VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>,
Unicoder-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, Uniter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, X-LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, Pixel-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, or VLMo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Our proposed method</span>
</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2307.08247/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overall structure of the PAT method.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Inspired by the success of the transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and the study of Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we propose a novel scheme of attention, Parallel Attention, that is a kind of multi-hop attention and differs from recent methods. Moreover, to leverage the linguistic features of Vietnamese, we provide Parallel Attention with the hierarchical feature extractor for questions and hence propose a novel method, the Parallel Attention Transformer (PAT). Our experiments prove that this hierarchical extractor is indeed necessary.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The PAT method includes four main components: the Hierarchical Linguistic Feature Extractor, the Image Embedding module, the Parallel Attention module, and the Answer Selector (Figure <a href="#S3.F1" title="Figure 1 ‣ III Our proposed method ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The detailed architecture of our method is detailed as follows.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Hierarchical Linguistic Feature Extractor</span>
</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.08247/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Hierarchical Linguistic Features Extractor.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We apply a pre-trained word embedding for Vietnamese to extract the linguistic features of questions. As each token of questions after being passed through the pre-trained word embeddings they are mapped to respective embedded vectors. Accordingly extracted features using word embedding are the unigram features. We aim to make our method have the ability to fully catch the linguistic context of the sentence, so we propose to construct the n-gram linguistic features based on the unigram features (Figure <a href="#S3.F2" title="Figure 2 ‣ III-A Hierarchical Linguistic Feature Extractor ‣ III Our proposed method ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Particularly, we use a 1D convolutional neural network (CNN) with a kernel of size 1, 2, 3, and 4 to extract the unigram, bigram, trigram, and 4-gram of the initial unigram features, respectively. We note that as the initial unigram features of pre-trained word embedding might not be in the same latent space of the model, so we use a 1D CNN with the kernel of size 1 to project these features into latent space. Our ablation study will prove that this 1D CNN is important to improve the accuracy of our proposed method. The four n-gram features finally are summed to yield the hierarchical linguistic features for questions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Image Embedding module</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Inspired by the study of Anderson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we perform the bottom-attention mechanism on the visual features. Particularly, we used the VinVL pre-trained image models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to achieve the region features from images. The VinVL pre-trained model was trained on large-scale datasets of vision-language tasks and they used detected tags of objects together with the ROI features of Faster-RCNN-based models hence their visual features are rich in visual aspect as well as linguistic aspect, and Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> proved that VinVL outperformed previous pre-trained image models on various tasks. The visual features are projected into the latent space of the model before being passed to the next components by using a fully connected layer.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Parallel Attention module.</span>
</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2307.08247/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="102" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A Parallel Attention module.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As various VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, our proposed method has an encoder module containing encoder layers to perform the attention mechanisms. In particular, the Parallel Attention module has four components. Each component has an attention layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and a Position-wise Feed Forward layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">The attention layer is the multi-head attention module proposed by Vaswani et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Given a query <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">Q</annotation></semantics></math>, key <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">K</annotation></semantics></math>, and value <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">V</annotation></semantics></math> vector, the attention map is specified as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="A=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><mi id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml">A</mi><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><mi id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.2.3.3" xref="S3.E1.m1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1a" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.2.3.4" xref="S3.E1.m1.1.2.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1b" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.2.3.5" xref="S3.E1.m1.1.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1c" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.2.3.6" xref="S3.E1.m1.1.2.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1d" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.2.3.7" xref="S3.E1.m1.1.2.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1e" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.2.3.8" xref="S3.E1.m1.1.2.3.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.3.1f" xref="S3.E1.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E1.m1.1.2.3.9.2" xref="S3.E1.m1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.2.3.9.2.1" xref="S3.E1.m1.1.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">​</mo><msup id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2" xref="S3.E1.m1.1.1.2.3.2.cmml">K</mi><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">d</mi><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo stretchy="false" id="S3.E1.m1.1.2.3.9.2.2" xref="S3.E1.m1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><ci id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2">𝐴</ci><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><times id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3.1"></times><ci id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2">𝑠</ci><ci id="S3.E1.m1.1.2.3.3.cmml" xref="S3.E1.m1.1.2.3.3">𝑜</ci><ci id="S3.E1.m1.1.2.3.4.cmml" xref="S3.E1.m1.1.2.3.4">𝑓</ci><ci id="S3.E1.m1.1.2.3.5.cmml" xref="S3.E1.m1.1.2.3.5">𝑡</ci><ci id="S3.E1.m1.1.2.3.6.cmml" xref="S3.E1.m1.1.2.3.6">𝑚</ci><ci id="S3.E1.m1.1.2.3.7.cmml" xref="S3.E1.m1.1.2.3.7">𝑎</ci><ci id="S3.E1.m1.1.2.3.8.cmml" xref="S3.E1.m1.1.2.3.8">𝑥</ci><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.2.3.9.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.2.3.9.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><times id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝑄</ci><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3">superscript</csymbol><ci id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3.2">𝐾</ci><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><root id="S3.E1.m1.1.1.3a.cmml" xref="S3.E1.m1.1.1.3"></root><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝑑</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">A=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.7" class="ltx_p">where <math id="S3.SS3.p2.4.m1.1" class="ltx_Math" alttext="d_{k}" display="inline"><semantics id="S3.SS3.p2.4.m1.1a"><msub id="S3.SS3.p2.4.m1.1.1" xref="S3.SS3.p2.4.m1.1.1.cmml"><mi id="S3.SS3.p2.4.m1.1.1.2" xref="S3.SS3.p2.4.m1.1.1.2.cmml">d</mi><mi id="S3.SS3.p2.4.m1.1.1.3" xref="S3.SS3.p2.4.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m1.1b"><apply id="S3.SS3.p2.4.m1.1.1.cmml" xref="S3.SS3.p2.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m1.1.1.1.cmml" xref="S3.SS3.p2.4.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m1.1.1.2.cmml" xref="S3.SS3.p2.4.m1.1.1.2">𝑑</ci><ci id="S3.SS3.p2.4.m1.1.1.3.cmml" xref="S3.SS3.p2.4.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m1.1c">d_{k}</annotation></semantics></math> is the dimension of the value vector and we assume that <math id="S3.SS3.p2.5.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS3.p2.5.m2.1a"><mi id="S3.SS3.p2.5.m2.1.1" xref="S3.SS3.p2.5.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m2.1b"><ci id="S3.SS3.p2.5.m2.1.1.cmml" xref="S3.SS3.p2.5.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m2.1c">Q</annotation></semantics></math>, <math id="S3.SS3.p2.6.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.6.m3.1a"><mi id="S3.SS3.p2.6.m3.1.1" xref="S3.SS3.p2.6.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m3.1b"><ci id="S3.SS3.p2.6.m3.1.1.cmml" xref="S3.SS3.p2.6.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m3.1c">K</annotation></semantics></math>, and <math id="S3.SS3.p2.7.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p2.7.m4.1a"><mi id="S3.SS3.p2.7.m4.1.1" xref="S3.SS3.p2.7.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m4.1b"><ci id="S3.SS3.p2.7.m4.1.1.cmml" xref="S3.SS3.p2.7.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m4.1c">V</annotation></semantics></math> have the same dimension. After obtaining the attention map, the encoded features are determined as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="Y=AV" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">Y</mi><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝑌</ci><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><times id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝐴</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">Y=AV</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.2" class="ltx_p">In the Parallel Attention module, the first two components are used to perform cross-and-parallel attention: vision over language and language over vision, respectively, by changing the query, key, and value role of visual features and linguistic features. The last second components are used to perform self-and-parallel attention: vision over itself and language over itself by defining the key, query, and value are all visual features or linguistic features (Figure <a href="#S3.F3" title="Figure 3 ‣ III-C Parallel Attention module. ‣ III Our proposed method ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Finally, the visual features <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="x_{v}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝑥</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">x_{v}</annotation></semantics></math> and linguistic features <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="x_{l}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝑥</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">x_{l}</annotation></semantics></math> are produced that have advantage information for selecting an accurate candidate among defined answers.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Answer Selector</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.3" class="ltx_p">The Answer Selector module is designed to fuse the information of visual features <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="x_{v}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝑥</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">x_{v}</annotation></semantics></math> and linguistic features <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="x_{l}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝑥</ci><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">x_{l}</annotation></semantics></math> that produce the fused features <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="x_{f}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">𝑥</ci><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">x_{f}</annotation></semantics></math>. The fused features are then projected into the vocab space. Finally, we obtained the probabilistic vector that indicates the most candidate as an answer. We follow the Attribute Reduction and Classifier of MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> method to design the Answer Selector.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.2" class="ltx_p">In particular, the Answer Selector module includes two phases: attributes reduction and Candidate Selection (in the context of the study of Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, this phase is named Answer Classifier). Given <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="x_{v}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">𝑥</ci><ci id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">x_{v}</annotation></semantics></math> and <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="x_{f}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">𝑥</ci><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">x_{f}</annotation></semantics></math> obtained from the Parallel Attention layers, we use the MLP layer with the softmax function to re-weight these features:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="attr_{v}=softmax(MLP(x_{v}))" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.1a" xref="S3.E3.m1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.4" xref="S3.E3.m1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.1b" xref="S3.E3.m1.1.1.3.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.5" xref="S3.E3.m1.1.1.3.5.cmml"><mi id="S3.E3.m1.1.1.3.5.2" xref="S3.E3.m1.1.1.3.5.2.cmml">r</mi><mi id="S3.E3.m1.1.1.3.5.3" xref="S3.E3.m1.1.1.3.5.3.cmml">v</mi></msub></mrow><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.4" xref="S3.E3.m1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2a" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.5" xref="S3.E3.m1.1.1.1.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2b" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.6" xref="S3.E3.m1.1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2c" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.7" xref="S3.E3.m1.1.1.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2d" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.8" xref="S3.E3.m1.1.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2e" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.9" xref="S3.E3.m1.1.1.1.9.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2f" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2a" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.5" xref="S3.E3.m1.1.1.1.1.1.1.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2b" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml">v</mi></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><times id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑎</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑡</ci><ci id="S3.E3.m1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.3.4">𝑡</ci><apply id="S3.E3.m1.1.1.3.5.cmml" xref="S3.E3.m1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.5.1.cmml" xref="S3.E3.m1.1.1.3.5">subscript</csymbol><ci id="S3.E3.m1.1.1.3.5.2.cmml" xref="S3.E3.m1.1.1.3.5.2">𝑟</ci><ci id="S3.E3.m1.1.1.3.5.3.cmml" xref="S3.E3.m1.1.1.3.5.3">𝑣</ci></apply></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">𝑠</ci><ci id="S3.E3.m1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.4">𝑜</ci><ci id="S3.E3.m1.1.1.1.5.cmml" xref="S3.E3.m1.1.1.1.5">𝑓</ci><ci id="S3.E3.m1.1.1.1.6.cmml" xref="S3.E3.m1.1.1.1.6">𝑡</ci><ci id="S3.E3.m1.1.1.1.7.cmml" xref="S3.E3.m1.1.1.1.7">𝑚</ci><ci id="S3.E3.m1.1.1.1.8.cmml" xref="S3.E3.m1.1.1.1.8">𝑎</ci><ci id="S3.E3.m1.1.1.1.9.cmml" xref="S3.E3.m1.1.1.1.9">𝑥</ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">𝑀</ci><ci id="S3.E3.m1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.4">𝐿</ci><ci id="S3.E3.m1.1.1.1.1.1.1.5.cmml" xref="S3.E3.m1.1.1.1.1.1.1.5">𝑃</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">attr_{v}=softmax(MLP(x_{v}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="attr_{l}=softmax(MLP(x_{l}))" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1a" xref="S3.E4.m1.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.4" xref="S3.E4.m1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1b" xref="S3.E4.m1.1.1.3.1.cmml">​</mo><msub id="S3.E4.m1.1.1.3.5" xref="S3.E4.m1.1.1.3.5.cmml"><mi id="S3.E4.m1.1.1.3.5.2" xref="S3.E4.m1.1.1.3.5.2.cmml">r</mi><mi id="S3.E4.m1.1.1.3.5.3" xref="S3.E4.m1.1.1.3.5.3.cmml">l</mi></msub></mrow><mo id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.4" xref="S3.E4.m1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2a" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.5" xref="S3.E4.m1.1.1.1.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2b" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.6" xref="S3.E4.m1.1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2c" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.7" xref="S3.E4.m1.1.1.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2d" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.8" xref="S3.E4.m1.1.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2e" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.9" xref="S3.E4.m1.1.1.1.9.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2f" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2a" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.1.5" xref="S3.E4.m1.1.1.1.1.1.1.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2b" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><times id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></times><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝑎</ci><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">𝑡</ci><ci id="S3.E4.m1.1.1.3.4.cmml" xref="S3.E4.m1.1.1.3.4">𝑡</ci><apply id="S3.E4.m1.1.1.3.5.cmml" xref="S3.E4.m1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.5.1.cmml" xref="S3.E4.m1.1.1.3.5">subscript</csymbol><ci id="S3.E4.m1.1.1.3.5.2.cmml" xref="S3.E4.m1.1.1.3.5.2">𝑟</ci><ci id="S3.E4.m1.1.1.3.5.3.cmml" xref="S3.E4.m1.1.1.3.5.3">𝑙</ci></apply></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">𝑠</ci><ci id="S3.E4.m1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.4">𝑜</ci><ci id="S3.E4.m1.1.1.1.5.cmml" xref="S3.E4.m1.1.1.1.5">𝑓</ci><ci id="S3.E4.m1.1.1.1.6.cmml" xref="S3.E4.m1.1.1.1.6">𝑡</ci><ci id="S3.E4.m1.1.1.1.7.cmml" xref="S3.E4.m1.1.1.1.7">𝑚</ci><ci id="S3.E4.m1.1.1.1.8.cmml" xref="S3.E4.m1.1.1.1.8">𝑎</ci><ci id="S3.E4.m1.1.1.1.9.cmml" xref="S3.E4.m1.1.1.1.9">𝑥</ci><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3">𝑀</ci><ci id="S3.E4.m1.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.1.1.4">𝐿</ci><ci id="S3.E4.m1.1.1.1.1.1.1.5.cmml" xref="S3.E4.m1.1.1.1.1.1.1.5">𝑃</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">attr_{l}=softmax(MLP(x_{l}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.2" class="ltx_p">Then the reduced attributes are applied to denoise and combine the visual features <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="x_{v}" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><msub id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mi id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS4.p4.1.m1.1.1.3" xref="S3.SS4.p4.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2">𝑥</ci><ci id="S3.SS4.p4.1.m1.1.1.3.cmml" xref="S3.SS4.p4.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">x_{v}</annotation></semantics></math> and linguistic features <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="x_{l}" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><msub id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml"><mi id="S3.SS4.p4.2.m2.1.1.2" xref="S3.SS4.p4.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS4.p4.2.m2.1.1.3" xref="S3.SS4.p4.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><apply id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.1.2.cmml" xref="S3.SS4.p4.2.m2.1.1.2">𝑥</ci><ci id="S3.SS4.p4.2.m2.1.1.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">x_{l}</annotation></semantics></math>:</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="x_{v}=sum(x_{v}*attr_{v})" display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml">x</mi><mi id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml">v</mi></msub><mo id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml">​</mo><mi id="S3.E5.m1.1.1.1.4" xref="S3.E5.m1.1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.2a" xref="S3.E5.m1.1.1.1.2.cmml">​</mo><mi id="S3.E5.m1.1.1.1.5" xref="S3.E5.m1.1.1.1.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.2b" xref="S3.E5.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml"><msub id="S3.E5.m1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.2.2.2.cmml">x</mi><mi id="S3.E5.m1.1.1.1.1.1.1.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.1.1.1.1.1.1.2.1" xref="S3.E5.m1.1.1.1.1.1.1.2.1.cmml">∗</mo><mi id="S3.E5.m1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.2.3.cmml">a</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.1a" xref="S3.E5.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E5.m1.1.1.1.1.1.1.4" xref="S3.E5.m1.1.1.1.1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.1b" xref="S3.E5.m1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E5.m1.1.1.1.1.1.1.5" xref="S3.E5.m1.1.1.1.1.1.1.5.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.5.2" xref="S3.E5.m1.1.1.1.1.1.1.5.2.cmml">r</mi><mi id="S3.E5.m1.1.1.1.1.1.1.5.3" xref="S3.E5.m1.1.1.1.1.1.1.5.3.cmml">v</mi></msub></mrow><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"></eq><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">𝑥</ci><ci id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3">𝑣</ci></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><times id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"></times><ci id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3">𝑠</ci><ci id="S3.E5.m1.1.1.1.4.cmml" xref="S3.E5.m1.1.1.1.4">𝑢</ci><ci id="S3.E5.m1.1.1.1.5.cmml" xref="S3.E5.m1.1.1.1.5">𝑚</ci><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1"></times><apply id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2"><times id="S3.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.1"></times><apply id="S3.E5.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2.2">𝑥</ci><ci id="S3.E5.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3">𝑣</ci></apply><ci id="S3.E5.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.3">𝑎</ci></apply><ci id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">𝑡</ci><ci id="S3.E5.m1.1.1.1.1.1.1.4.cmml" xref="S3.E5.m1.1.1.1.1.1.1.4">𝑡</ci><apply id="S3.E5.m1.1.1.1.1.1.1.5.cmml" xref="S3.E5.m1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.5.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.5.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.5.2">𝑟</ci><ci id="S3.E5.m1.1.1.1.1.1.1.5.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.5.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">x_{v}=sum(x_{v}*attr_{v})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="x_{l}=sum(x_{l}*attr_{l})" display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><msub id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.3.2" xref="S3.E6.m1.1.1.3.2.cmml">x</mi><mi id="S3.E6.m1.1.1.3.3" xref="S3.E6.m1.1.1.3.3.cmml">l</mi></msub><mo id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml">​</mo><mi id="S3.E6.m1.1.1.1.4" xref="S3.E6.m1.1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.2a" xref="S3.E6.m1.1.1.1.2.cmml">​</mo><mi id="S3.E6.m1.1.1.1.5" xref="S3.E6.m1.1.1.1.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.2b" xref="S3.E6.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml"><msub id="S3.E6.m1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2.cmml">x</mi><mi id="S3.E6.m1.1.1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.1.1.2.2.3.cmml">l</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.1.1.1.1.1.1.2.1" xref="S3.E6.m1.1.1.1.1.1.1.2.1.cmml">∗</mo><mi id="S3.E6.m1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">a</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1a" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.1.1.4" xref="S3.E6.m1.1.1.1.1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1b" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.1.1.5" xref="S3.E6.m1.1.1.1.1.1.1.5.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.5.2" xref="S3.E6.m1.1.1.1.1.1.1.5.2.cmml">r</mi><mi id="S3.E6.m1.1.1.1.1.1.1.5.3" xref="S3.E6.m1.1.1.1.1.1.1.5.3.cmml">l</mi></msub></mrow><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"></eq><apply id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.3.2">𝑥</ci><ci id="S3.E6.m1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.3.3">𝑙</ci></apply><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><times id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"></times><ci id="S3.E6.m1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.3">𝑠</ci><ci id="S3.E6.m1.1.1.1.4.cmml" xref="S3.E6.m1.1.1.1.4">𝑢</ci><ci id="S3.E6.m1.1.1.1.5.cmml" xref="S3.E6.m1.1.1.1.5">𝑚</ci><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"></times><apply id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2"><times id="S3.E6.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.1"></times><apply id="S3.E6.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.2">𝑥</ci><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2.3">𝑙</ci></apply><ci id="S3.E6.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.3">𝑎</ci></apply><ci id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">𝑡</ci><ci id="S3.E6.m1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.4">𝑡</ci><apply id="S3.E6.m1.1.1.1.1.1.1.5.cmml" xref="S3.E6.m1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.5.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.5.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.5.2">𝑟</ci><ci id="S3.E6.m1.1.1.1.1.1.1.5.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.5.3">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">x_{l}=sum(x_{l}*attr_{l})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p6.1" class="ltx_p">where <math id="S3.SS4.p6.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.SS4.p6.1.m1.1a"><mo id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><times id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">*</annotation></semantics></math> indicates the element-wise product.</p>
</div>
<div id="S3.SS4.p7" class="ltx_para">
<p id="S3.SS4.p7.3" class="ltx_p">Finally, the fused features <math id="S3.SS4.p7.1.m1.1" class="ltx_Math" alttext="x_{f}" display="inline"><semantics id="S3.SS4.p7.1.m1.1a"><msub id="S3.SS4.p7.1.m1.1.1" xref="S3.SS4.p7.1.m1.1.1.cmml"><mi id="S3.SS4.p7.1.m1.1.1.2" xref="S3.SS4.p7.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS4.p7.1.m1.1.1.3" xref="S3.SS4.p7.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.1.m1.1b"><apply id="S3.SS4.p7.1.m1.1.1.cmml" xref="S3.SS4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p7.1.m1.1.1.1.cmml" xref="S3.SS4.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p7.1.m1.1.1.2.cmml" xref="S3.SS4.p7.1.m1.1.1.2">𝑥</ci><ci id="S3.SS4.p7.1.m1.1.1.3.cmml" xref="S3.SS4.p7.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.1.m1.1c">x_{f}</annotation></semantics></math> are obtained by summing the <math id="S3.SS4.p7.2.m2.1" class="ltx_Math" alttext="x_{v}" display="inline"><semantics id="S3.SS4.p7.2.m2.1a"><msub id="S3.SS4.p7.2.m2.1.1" xref="S3.SS4.p7.2.m2.1.1.cmml"><mi id="S3.SS4.p7.2.m2.1.1.2" xref="S3.SS4.p7.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS4.p7.2.m2.1.1.3" xref="S3.SS4.p7.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.2.m2.1b"><apply id="S3.SS4.p7.2.m2.1.1.cmml" xref="S3.SS4.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p7.2.m2.1.1.1.cmml" xref="S3.SS4.p7.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p7.2.m2.1.1.2.cmml" xref="S3.SS4.p7.2.m2.1.1.2">𝑥</ci><ci id="S3.SS4.p7.2.m2.1.1.3.cmml" xref="S3.SS4.p7.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.2.m2.1c">x_{v}</annotation></semantics></math> and <math id="S3.SS4.p7.3.m3.1" class="ltx_Math" alttext="x_{l}" display="inline"><semantics id="S3.SS4.p7.3.m3.1a"><msub id="S3.SS4.p7.3.m3.1.1" xref="S3.SS4.p7.3.m3.1.1.cmml"><mi id="S3.SS4.p7.3.m3.1.1.2" xref="S3.SS4.p7.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS4.p7.3.m3.1.1.3" xref="S3.SS4.p7.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.3.m3.1b"><apply id="S3.SS4.p7.3.m3.1.1.cmml" xref="S3.SS4.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p7.3.m3.1.1.1.cmml" xref="S3.SS4.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p7.3.m3.1.1.2.cmml" xref="S3.SS4.p7.3.m3.1.1.2">𝑥</ci><ci id="S3.SS4.p7.3.m3.1.1.3.cmml" xref="S3.SS4.p7.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.3.m3.1c">x_{l}</annotation></semantics></math>:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="x_{f}=W_{v}x_{v}+W_{l}x_{l}" display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><msub id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml"><mi id="S3.E7.m1.1.1.2.2" xref="S3.E7.m1.1.1.2.2.cmml">x</mi><mi id="S3.E7.m1.1.1.2.3" xref="S3.E7.m1.1.1.2.3.cmml">f</mi></msub><mo id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mrow id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><msub id="S3.E7.m1.1.1.3.2.2" xref="S3.E7.m1.1.1.3.2.2.cmml"><mi id="S3.E7.m1.1.1.3.2.2.2" xref="S3.E7.m1.1.1.3.2.2.2.cmml">W</mi><mi id="S3.E7.m1.1.1.3.2.2.3" xref="S3.E7.m1.1.1.3.2.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.3.2.1" xref="S3.E7.m1.1.1.3.2.1.cmml">​</mo><msub id="S3.E7.m1.1.1.3.2.3" xref="S3.E7.m1.1.1.3.2.3.cmml"><mi id="S3.E7.m1.1.1.3.2.3.2" xref="S3.E7.m1.1.1.3.2.3.2.cmml">x</mi><mi id="S3.E7.m1.1.1.3.2.3.3" xref="S3.E7.m1.1.1.3.2.3.3.cmml">v</mi></msub></mrow><mo id="S3.E7.m1.1.1.3.1" xref="S3.E7.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E7.m1.1.1.3.3" xref="S3.E7.m1.1.1.3.3.cmml"><msub id="S3.E7.m1.1.1.3.3.2" xref="S3.E7.m1.1.1.3.3.2.cmml"><mi id="S3.E7.m1.1.1.3.3.2.2" xref="S3.E7.m1.1.1.3.3.2.2.cmml">W</mi><mi id="S3.E7.m1.1.1.3.3.2.3" xref="S3.E7.m1.1.1.3.3.2.3.cmml">l</mi></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.3.3.1" xref="S3.E7.m1.1.1.3.3.1.cmml">​</mo><msub id="S3.E7.m1.1.1.3.3.3" xref="S3.E7.m1.1.1.3.3.3.cmml"><mi id="S3.E7.m1.1.1.3.3.3.2" xref="S3.E7.m1.1.1.3.3.3.2.cmml">x</mi><mi id="S3.E7.m1.1.1.3.3.3.3" xref="S3.E7.m1.1.1.3.3.3.3.cmml">l</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"></eq><apply id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.2">subscript</csymbol><ci id="S3.E7.m1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.2.2">𝑥</ci><ci id="S3.E7.m1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.2.3">𝑓</ci></apply><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><plus id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3.1"></plus><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><times id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2.1"></times><apply id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.2.1.cmml" xref="S3.E7.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2.2">𝑊</ci><ci id="S3.E7.m1.1.1.3.2.2.3.cmml" xref="S3.E7.m1.1.1.3.2.2.3">𝑣</ci></apply><apply id="S3.E7.m1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.3.1.cmml" xref="S3.E7.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.3.2.cmml" xref="S3.E7.m1.1.1.3.2.3.2">𝑥</ci><ci id="S3.E7.m1.1.1.3.2.3.3.cmml" xref="S3.E7.m1.1.1.3.2.3.3">𝑣</ci></apply></apply><apply id="S3.E7.m1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.3.3"><times id="S3.E7.m1.1.1.3.3.1.cmml" xref="S3.E7.m1.1.1.3.3.1"></times><apply id="S3.E7.m1.1.1.3.3.2.cmml" xref="S3.E7.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.3.2.1.cmml" xref="S3.E7.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.3.2.2.cmml" xref="S3.E7.m1.1.1.3.3.2.2">𝑊</ci><ci id="S3.E7.m1.1.1.3.3.2.3.cmml" xref="S3.E7.m1.1.1.3.3.2.3">𝑙</ci></apply><apply id="S3.E7.m1.1.1.3.3.3.cmml" xref="S3.E7.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.3.3.1.cmml" xref="S3.E7.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E7.m1.1.1.3.3.3.2.cmml" xref="S3.E7.m1.1.1.3.3.3.2">𝑥</ci><ci id="S3.E7.m1.1.1.3.3.3.3.cmml" xref="S3.E7.m1.1.1.3.3.3.3">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">x_{f}=W_{v}x_{v}+W_{l}x_{l}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p8" class="ltx_para">
<p id="S3.SS4.p8.2" class="ltx_p">The selected candidate <math id="S3.SS4.p8.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS4.p8.1.m1.1a"><mi id="S3.SS4.p8.1.m1.1.1" xref="S3.SS4.p8.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.1.m1.1b"><ci id="S3.SS4.p8.1.m1.1.1.cmml" xref="S3.SS4.p8.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.1.m1.1c">c</annotation></semantics></math> is determined based on the fused features <math id="S3.SS4.p8.2.m2.1" class="ltx_Math" alttext="x_{f}" display="inline"><semantics id="S3.SS4.p8.2.m2.1a"><msub id="S3.SS4.p8.2.m2.1.1" xref="S3.SS4.p8.2.m2.1.1.cmml"><mi id="S3.SS4.p8.2.m2.1.1.2" xref="S3.SS4.p8.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS4.p8.2.m2.1.1.3" xref="S3.SS4.p8.2.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.2.m2.1b"><apply id="S3.SS4.p8.2.m2.1.1.cmml" xref="S3.SS4.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p8.2.m2.1.1.1.cmml" xref="S3.SS4.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p8.2.m2.1.1.2.cmml" xref="S3.SS4.p8.2.m2.1.1.2">𝑥</ci><ci id="S3.SS4.p8.2.m2.1.1.3.cmml" xref="S3.SS4.p8.2.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.2.m2.1c">x_{f}</annotation></semantics></math>:</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.1" class="ltx_Math" alttext="c=max(W_{vocab}x_{f})" display="block"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><mi id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3.cmml">c</mi><mo id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml">=</mo><mrow id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.3" xref="S3.E8.m1.1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.2" xref="S3.E8.m1.1.1.1.2.cmml">​</mo><mi id="S3.E8.m1.1.1.1.4" xref="S3.E8.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.2a" xref="S3.E8.m1.1.1.1.2.cmml">​</mo><mi id="S3.E8.m1.1.1.1.5" xref="S3.E8.m1.1.1.1.5.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.2b" xref="S3.E8.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E8.m1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E8.m1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><msub id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.2.2" xref="S3.E8.m1.1.1.1.1.1.1.2.2.cmml">W</mi><mrow id="S3.E8.m1.1.1.1.1.1.1.2.3" xref="S3.E8.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.2.3.2" xref="S3.E8.m1.1.1.1.1.1.1.2.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.2.3.1" xref="S3.E8.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E8.m1.1.1.1.1.1.1.2.3.3" xref="S3.E8.m1.1.1.1.1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.2.3.1a" xref="S3.E8.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E8.m1.1.1.1.1.1.1.2.3.4" xref="S3.E8.m1.1.1.1.1.1.1.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.2.3.1b" xref="S3.E8.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E8.m1.1.1.1.1.1.1.2.3.5" xref="S3.E8.m1.1.1.1.1.1.1.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.2.3.1c" xref="S3.E8.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E8.m1.1.1.1.1.1.1.2.3.6" xref="S3.E8.m1.1.1.1.1.1.1.2.3.6.cmml">b</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S3.E8.m1.1.1.1.1.1.1.3.3" xref="S3.E8.m1.1.1.1.1.1.1.3.3.cmml">f</mi></msub></mrow><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><eq id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2"></eq><ci id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3">𝑐</ci><apply id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"><times id="S3.E8.m1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.2"></times><ci id="S3.E8.m1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.3">𝑚</ci><ci id="S3.E8.m1.1.1.1.4.cmml" xref="S3.E8.m1.1.1.1.4">𝑎</ci><ci id="S3.E8.m1.1.1.1.5.cmml" xref="S3.E8.m1.1.1.1.5">𝑥</ci><apply id="S3.E8.m1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1"><times id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1"></times><apply id="S3.E8.m1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.2">𝑊</ci><apply id="S3.E8.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.3"><times id="S3.E8.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E8.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.3.2">𝑣</ci><ci id="S3.E8.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.3.3">𝑜</ci><ci id="S3.E8.m1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.3.4">𝑐</ci><ci id="S3.E8.m1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.3.5">𝑎</ci><ci id="S3.E8.m1.1.1.1.1.1.1.2.3.6.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2.3.6">𝑏</ci></apply></apply><apply id="S3.E8.m1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.3">𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">c=max(W_{vocab}x_{f})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental results</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this paper, we propose the Hierarchical Linguistic Feature Extractor to leverage the advantages of grammar and context in Vietnamese. Accordingly, we conduct experiments on the ViVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> which is the first visual question answering dataset for Vietnamese.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Evaluation Metrics</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We follow the study of Teney et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> that treats the VQA task as a classification task. From that on, we use the Accuracy metric or Exact Match (EM) metric defined by Antol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to measure the ability of VQA methods in our experiments. Particularly, the EM metric is determined as:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<table id="S4.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E9.m1.1" class="ltx_Math" alttext="EM=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{m}\sum_{j=1}^{m}\left(1-\alpha_{ij}\right)\right)" display="block"><semantics id="S4.E9.m1.1a"><mrow id="S4.E9.m1.1.1" xref="S4.E9.m1.1.1.cmml"><mrow id="S4.E9.m1.1.1.3" xref="S4.E9.m1.1.1.3.cmml"><mi id="S4.E9.m1.1.1.3.2" xref="S4.E9.m1.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E9.m1.1.1.3.1" xref="S4.E9.m1.1.1.3.1.cmml">​</mo><mi id="S4.E9.m1.1.1.3.3" xref="S4.E9.m1.1.1.3.3.cmml">M</mi></mrow><mo id="S4.E9.m1.1.1.2" xref="S4.E9.m1.1.1.2.cmml">=</mo><mrow id="S4.E9.m1.1.1.1" xref="S4.E9.m1.1.1.1.cmml"><mfrac id="S4.E9.m1.1.1.1.3" xref="S4.E9.m1.1.1.1.3.cmml"><mn id="S4.E9.m1.1.1.1.3.2" xref="S4.E9.m1.1.1.1.3.2.cmml">1</mn><mi id="S4.E9.m1.1.1.1.3.3" xref="S4.E9.m1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E9.m1.1.1.1.2" xref="S4.E9.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E9.m1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.cmml"><munderover id="S4.E9.m1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E9.m1.1.1.1.1.2.2.2" xref="S4.E9.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E9.m1.1.1.1.1.2.2.3" xref="S4.E9.m1.1.1.1.1.2.2.3.cmml"><mi id="S4.E9.m1.1.1.1.1.2.2.3.2" xref="S4.E9.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E9.m1.1.1.1.1.2.2.3.1" xref="S4.E9.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E9.m1.1.1.1.1.2.2.3.3" xref="S4.E9.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E9.m1.1.1.1.1.2.3" xref="S4.E9.m1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S4.E9.m1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.E9.m1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E9.m1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.cmml"><mfrac id="S4.E9.m1.1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.3.cmml"><mn id="S4.E9.m1.1.1.1.1.1.1.1.3.2" xref="S4.E9.m1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S4.E9.m1.1.1.1.1.1.1.1.3.3" xref="S4.E9.m1.1.1.1.1.1.1.1.3.3.cmml">m</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E9.m1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml"><munderover id="S4.E9.m1.1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.3.cmml">m</mi></munderover><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">α</mi><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">j</mi></mrow></msub></mrow><mo id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E9.m1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E9.m1.1b"><apply id="S4.E9.m1.1.1.cmml" xref="S4.E9.m1.1.1"><eq id="S4.E9.m1.1.1.2.cmml" xref="S4.E9.m1.1.1.2"></eq><apply id="S4.E9.m1.1.1.3.cmml" xref="S4.E9.m1.1.1.3"><times id="S4.E9.m1.1.1.3.1.cmml" xref="S4.E9.m1.1.1.3.1"></times><ci id="S4.E9.m1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.3.2">𝐸</ci><ci id="S4.E9.m1.1.1.3.3.cmml" xref="S4.E9.m1.1.1.3.3">𝑀</ci></apply><apply id="S4.E9.m1.1.1.1.cmml" xref="S4.E9.m1.1.1.1"><times id="S4.E9.m1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.2"></times><apply id="S4.E9.m1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.3"><divide id="S4.E9.m1.1.1.1.3.1.cmml" xref="S4.E9.m1.1.1.1.3"></divide><cn type="integer" id="S4.E9.m1.1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.1.3.2">1</cn><ci id="S4.E9.m1.1.1.1.3.3.cmml" xref="S4.E9.m1.1.1.1.3.3">𝑛</ci></apply><apply id="S4.E9.m1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1"><apply id="S4.E9.m1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.2.1.cmml" xref="S4.E9.m1.1.1.1.1.2">superscript</csymbol><apply id="S4.E9.m1.1.1.1.1.2.2.cmml" xref="S4.E9.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.2.2.1.cmml" xref="S4.E9.m1.1.1.1.1.2">subscript</csymbol><sum id="S4.E9.m1.1.1.1.1.2.2.2.cmml" xref="S4.E9.m1.1.1.1.1.2.2.2"></sum><apply id="S4.E9.m1.1.1.1.1.2.2.3.cmml" xref="S4.E9.m1.1.1.1.1.2.2.3"><eq id="S4.E9.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E9.m1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E9.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E9.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S4.E9.m1.1.1.1.1.2.2.3.3.cmml" xref="S4.E9.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E9.m1.1.1.1.1.2.3.cmml" xref="S4.E9.m1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S4.E9.m1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1"><times id="S4.E9.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.2"></times><apply id="S4.E9.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.3"><divide id="S4.E9.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.3"></divide><cn type="integer" id="S4.E9.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.3.2">1</cn><ci id="S4.E9.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.3.3">𝑚</ci></apply><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1"><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3"><eq id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.2">𝑗</ci><cn type="integer" id="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.3">𝑚</ci></apply><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝛼</ci><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3"><times id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑖</ci><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E9.m1.1c">EM=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{m}\sum_{j=1}^{m}\left(1-\alpha_{ij}\right)\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<table id="S4.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E10.m1.2" class="ltx_Math" alttext="\alpha_{ij}=\begin{cases}0\Leftrightarrow\hat{a_{i}}=a_{ij}\\
1\text{ otherwise}\end{cases}" display="block"><semantics id="S4.E10.m1.2a"><mrow id="S4.E10.m1.2.3" xref="S4.E10.m1.2.3.cmml"><msub id="S4.E10.m1.2.3.2" xref="S4.E10.m1.2.3.2.cmml"><mi id="S4.E10.m1.2.3.2.2" xref="S4.E10.m1.2.3.2.2.cmml">α</mi><mrow id="S4.E10.m1.2.3.2.3" xref="S4.E10.m1.2.3.2.3.cmml"><mi id="S4.E10.m1.2.3.2.3.2" xref="S4.E10.m1.2.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.2.3.2.3.1" xref="S4.E10.m1.2.3.2.3.1.cmml">​</mo><mi id="S4.E10.m1.2.3.2.3.3" xref="S4.E10.m1.2.3.2.3.3.cmml">j</mi></mrow></msub><mo id="S4.E10.m1.2.3.1" xref="S4.E10.m1.2.3.1.cmml">=</mo><mrow id="S4.E10.m1.2.2" xref="S4.E10.m1.2.3.3.1.cmml"><mo id="S4.E10.m1.2.2.3" xref="S4.E10.m1.2.3.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E10.m1.2.2.2" xref="S4.E10.m1.2.3.3.1.cmml"><mtr id="S4.E10.m1.2.2.2a" xref="S4.E10.m1.2.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E10.m1.2.2.2b" xref="S4.E10.m1.2.3.3.1.cmml"><mrow id="S4.E10.m1.1.1.1.1.1.1" xref="S4.E10.m1.1.1.1.1.1.1.cmml"><mn id="S4.E10.m1.1.1.1.1.1.1.2" xref="S4.E10.m1.1.1.1.1.1.1.2.cmml">0</mn><mo stretchy="false" id="S4.E10.m1.1.1.1.1.1.1.1" xref="S4.E10.m1.1.1.1.1.1.1.1.cmml">⇔</mo><mrow id="S4.E10.m1.1.1.1.1.1.1.3" xref="S4.E10.m1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S4.E10.m1.1.1.1.1.1.1.3.2" xref="S4.E10.m1.1.1.1.1.1.1.3.2.cmml"><msub id="S4.E10.m1.1.1.1.1.1.1.3.2.2" xref="S4.E10.m1.1.1.1.1.1.1.3.2.2.cmml"><mi id="S4.E10.m1.1.1.1.1.1.1.3.2.2.2" xref="S4.E10.m1.1.1.1.1.1.1.3.2.2.2.cmml">a</mi><mi id="S4.E10.m1.1.1.1.1.1.1.3.2.2.3" xref="S4.E10.m1.1.1.1.1.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S4.E10.m1.1.1.1.1.1.1.3.2.1" xref="S4.E10.m1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mo id="S4.E10.m1.1.1.1.1.1.1.3.1" xref="S4.E10.m1.1.1.1.1.1.1.3.1.cmml">=</mo><msub id="S4.E10.m1.1.1.1.1.1.1.3.3" xref="S4.E10.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E10.m1.1.1.1.1.1.1.3.3.2" xref="S4.E10.m1.1.1.1.1.1.1.3.3.2.cmml">a</mi><mrow id="S4.E10.m1.1.1.1.1.1.1.3.3.3" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S4.E10.m1.1.1.1.1.1.1.3.3.3.2" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.1.1.1.1.1.1.3.3.3.1" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E10.m1.1.1.1.1.1.1.3.3.3.3" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3.3.cmml">j</mi></mrow></msub></mrow></mrow></mtd><mtd id="S4.E10.m1.2.2.2c" xref="S4.E10.m1.2.3.3.1.1.cmml"></mtd></mtr><mtr id="S4.E10.m1.2.2.2d" xref="S4.E10.m1.2.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E10.m1.2.2.2e" xref="S4.E10.m1.2.3.3.1.cmml"><mrow id="S4.E10.m1.2.2.2.2.1.1" xref="S4.E10.m1.2.2.2.2.1.1.cmml"><mn id="S4.E10.m1.2.2.2.2.1.1.2" xref="S4.E10.m1.2.2.2.2.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.E10.m1.2.2.2.2.1.1.1" xref="S4.E10.m1.2.2.2.2.1.1.1.cmml">​</mo><mtext id="S4.E10.m1.2.2.2.2.1.1.3" xref="S4.E10.m1.2.2.2.2.1.1.3a.cmml"> otherwise</mtext></mrow></mtd><mtd id="S4.E10.m1.2.2.2f" xref="S4.E10.m1.2.3.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E10.m1.2b"><apply id="S4.E10.m1.2.3.cmml" xref="S4.E10.m1.2.3"><eq id="S4.E10.m1.2.3.1.cmml" xref="S4.E10.m1.2.3.1"></eq><apply id="S4.E10.m1.2.3.2.cmml" xref="S4.E10.m1.2.3.2"><csymbol cd="ambiguous" id="S4.E10.m1.2.3.2.1.cmml" xref="S4.E10.m1.2.3.2">subscript</csymbol><ci id="S4.E10.m1.2.3.2.2.cmml" xref="S4.E10.m1.2.3.2.2">𝛼</ci><apply id="S4.E10.m1.2.3.2.3.cmml" xref="S4.E10.m1.2.3.2.3"><times id="S4.E10.m1.2.3.2.3.1.cmml" xref="S4.E10.m1.2.3.2.3.1"></times><ci id="S4.E10.m1.2.3.2.3.2.cmml" xref="S4.E10.m1.2.3.2.3.2">𝑖</ci><ci id="S4.E10.m1.2.3.2.3.3.cmml" xref="S4.E10.m1.2.3.2.3.3">𝑗</ci></apply></apply><apply id="S4.E10.m1.2.3.3.1.cmml" xref="S4.E10.m1.2.2"><csymbol cd="latexml" id="S4.E10.m1.2.3.3.1.1.cmml" xref="S4.E10.m1.2.2.3">cases</csymbol><apply id="S4.E10.m1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1"><ci id="S4.E10.m1.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.1">⇔</ci><cn type="integer" id="S4.E10.m1.1.1.1.1.1.1.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.2">0</cn><apply id="S4.E10.m1.1.1.1.1.1.1.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3"><eq id="S4.E10.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.1"></eq><apply id="S4.E10.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2"><ci id="S4.E10.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2.1">^</ci><apply id="S4.E10.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S4.E10.m1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2.2.2">𝑎</ci><ci id="S4.E10.m1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.2.2.3">𝑖</ci></apply></apply><apply id="S4.E10.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E10.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S4.E10.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3.2">𝑎</ci><apply id="S4.E10.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3"><times id="S4.E10.m1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3.1"></times><ci id="S4.E10.m1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3.2">𝑖</ci><ci id="S4.E10.m1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S4.E10.m1.1.1.1.1.1.1.3.3.3.3">𝑗</ci></apply></apply></apply></apply><ci id="S4.E10.m1.2.3.3.1.3a.cmml" xref="S4.E10.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S4.E10.m1.2.3.3.1.3.cmml" xref="S4.E10.m1.2.2.3">otherwise</mtext></ci><apply id="S4.E10.m1.2.2.2.2.1.1.cmml" xref="S4.E10.m1.2.2.2.2.1.1"><times id="S4.E10.m1.2.2.2.2.1.1.1.cmml" xref="S4.E10.m1.2.2.2.2.1.1.1"></times><cn type="integer" id="S4.E10.m1.2.2.2.2.1.1.2.cmml" xref="S4.E10.m1.2.2.2.2.1.1.2">1</cn><ci id="S4.E10.m1.2.2.2.2.1.1.3a.cmml" xref="S4.E10.m1.2.2.2.2.1.1.3"><mtext id="S4.E10.m1.2.2.2.2.1.1.3.cmml" xref="S4.E10.m1.2.2.2.2.1.1.3"> otherwise</mtext></ci></apply><ci id="S4.E10.m1.2.3.3.1.5a.cmml" xref="S4.E10.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S4.E10.m1.2.3.3.1.5.cmml" xref="S4.E10.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E10.m1.2c">\alpha_{ij}=\begin{cases}0\Leftrightarrow\hat{a_{i}}=a_{ij}\\
1\text{ otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p3.8" class="ltx_p">where <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">n</annotation></semantics></math> is the total number of questions in whole dataset, <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">m</annotation></semantics></math> is the total number of answers of given question <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">i</annotation></semantics></math>, <math id="S4.SS2.p3.4.m4.1" class="ltx_Math" alttext="\hat{a_{i}}" display="inline"><semantics id="S4.SS2.p3.4.m4.1a"><mover accent="true" id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml"><msub id="S4.SS2.p3.4.m4.1.1.2" xref="S4.SS2.p3.4.m4.1.1.2.cmml"><mi id="S4.SS2.p3.4.m4.1.1.2.2" xref="S4.SS2.p3.4.m4.1.1.2.2.cmml">a</mi><mi id="S4.SS2.p3.4.m4.1.1.2.3" xref="S4.SS2.p3.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.p3.4.m4.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1"><ci id="S4.SS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1">^</ci><apply id="S4.SS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.2.1.cmml" xref="S4.SS2.p3.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.4.m4.1.1.2.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2.2">𝑎</ci><ci id="S4.SS2.p3.4.m4.1.1.2.3.cmml" xref="S4.SS2.p3.4.m4.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">\hat{a_{i}}</annotation></semantics></math> is the predicted answer for question <math id="S4.SS2.p3.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p3.5.m5.1a"><mi id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><ci id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">i</annotation></semantics></math>, <math id="S4.SS2.p3.6.m6.1" class="ltx_Math" alttext="a_{ij}" display="inline"><semantics id="S4.SS2.p3.6.m6.1a"><msub id="S4.SS2.p3.6.m6.1.1" xref="S4.SS2.p3.6.m6.1.1.cmml"><mi id="S4.SS2.p3.6.m6.1.1.2" xref="S4.SS2.p3.6.m6.1.1.2.cmml">a</mi><mrow id="S4.SS2.p3.6.m6.1.1.3" xref="S4.SS2.p3.6.m6.1.1.3.cmml"><mi id="S4.SS2.p3.6.m6.1.1.3.2" xref="S4.SS2.p3.6.m6.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.6.m6.1.1.3.1" xref="S4.SS2.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p3.6.m6.1.1.3.3" xref="S4.SS2.p3.6.m6.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><apply id="S4.SS2.p3.6.m6.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.6.m6.1.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p3.6.m6.1.1.2.cmml" xref="S4.SS2.p3.6.m6.1.1.2">𝑎</ci><apply id="S4.SS2.p3.6.m6.1.1.3.cmml" xref="S4.SS2.p3.6.m6.1.1.3"><times id="S4.SS2.p3.6.m6.1.1.3.1.cmml" xref="S4.SS2.p3.6.m6.1.1.3.1"></times><ci id="S4.SS2.p3.6.m6.1.1.3.2.cmml" xref="S4.SS2.p3.6.m6.1.1.3.2">𝑖</ci><ci id="S4.SS2.p3.6.m6.1.1.3.3.cmml" xref="S4.SS2.p3.6.m6.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">a_{ij}</annotation></semantics></math> is the <math id="S4.SS2.p3.7.m7.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS2.p3.7.m7.1a"><mi id="S4.SS2.p3.7.m7.1.1" xref="S4.SS2.p3.7.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m7.1b"><ci id="S4.SS2.p3.7.m7.1.1.cmml" xref="S4.SS2.p3.7.m7.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m7.1c">j</annotation></semantics></math>th ground truth answer for question <math id="S4.SS2.p3.8.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p3.8.m8.1a"><mi id="S4.SS2.p3.8.m8.1.1" xref="S4.SS2.p3.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m8.1b"><ci id="S4.SS2.p3.8.m8.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m8.1c">i</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Baselines</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We compare our proposed PAT method with all models implemented in the previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. In addition, we re-implemented the two baselines on VQAv1 and VQAv2 datasets, which are SAAA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> methods, respectively.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Configuration</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">All experiments in this paper used the VinVL pre-trained image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to extract region features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and grid features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Both SAAA and MCAN as well as PAT use FastText <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> as pre-trained word embeddings to extract features of questions. All implemented experiments were performed on an A100 GPU, with batch size 64 and the learning rate fixed at 0.01. We used Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> as the optimization method. The detailed configuration for each method is listed as follows:</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS1.4.1.1" class="ltx_text">IV-D</span>1 </span>SAAA (Show, Asked, Attend, and Answer)</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">We followed the configuration of SAAA that made this model obtain the best results on VQAv1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In particular, the LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> layer of SAAA has 1024 as its hidden dimension, and the attention size is 512. In the Classifier module of SAAA, features are mapped into 1024-dimensional space before being projected into the vocab space. In our implementation, we used VinVL instead of ResNet152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to achieve the grid features.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS2.4.1.1" class="ltx_text">IV-D</span>2 </span>MCAN (Deep Modular Co-Attention Network)</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">We followed the best configuration of MCAN reported in the study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. In particular, we used 6 layers for the Co-Attention module. The multi-head attention modules of MCAN have 512 as their hidden size. We used VinVL to extract region features instead of Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS3.4.1.1" class="ltx_text">IV-D</span>3 </span>PAT</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">The Hierarchical Linguistic Feature Extractor contains 4 CNN layers with respectively 1, 2, 3, and 4 as their kernel size to extract unigram, bigram, trigram, and 4-gram features. The Parallel Attention module contains 4 layers. All attention modules of each layer in the Parallel Attention module have 512 as their hidden dimension. We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to use GeLU as an activation function instead of ReLU as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Experimental results on the ViVQA dataset. Note that (*) indicates results from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">EM</span></td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LSTM + W2V (*)</th>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">0.3228</td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LSTM + FastText (*)</th>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_right">0.3299</td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LSTM + ELMO (*)</th>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_right">0.3154</td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LTSM + PhoW2Vec (*)</th>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_right">0.3385</td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Bi-LSTM + W2V (*)</th>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_right">0.3125</td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Bi-LSTM + FastText (*)</th>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_right">0.3348</td>
</tr>
<tr id="S4.T1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Bi-LSTM + ELMO (*)</th>
<td id="S4.T1.1.8.8.2" class="ltx_td ltx_align_right">0.3203</td>
</tr>
<tr id="S4.T1.1.9.9" class="ltx_tr">
<th id="S4.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Bi-LTSM + PhoW2Vec (*)</th>
<td id="S4.T1.1.9.9.2" class="ltx_td ltx_align_right">0.3397</td>
</tr>
<tr id="S4.T1.1.10.10" class="ltx_tr">
<th id="S4.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Hierarchical Co-Attention + LSTM (*)</th>
<td id="S4.T1.1.10.10.2" class="ltx_td ltx_align_right">0.3496</td>
</tr>
<tr id="S4.T1.1.11.11" class="ltx_tr">
<th id="S4.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SAAA</th>
<td id="S4.T1.1.11.11.2" class="ltx_td ltx_align_right ltx_border_t">0.5415</td>
</tr>
<tr id="S4.T1.1.12.12" class="ltx_tr">
<th id="S4.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MCAN</th>
<td id="S4.T1.1.12.12.2" class="ltx_td ltx_align_left">0.5711</td>
</tr>
<tr id="S4.T1.1.13.13" class="ltx_tr">
<th id="S4.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T1.1.13.13.1.1" class="ltx_text ltx_font_bold">PAT (ours)</span></th>
<td id="S4.T1.1.13.13.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T1.1.13.13.2.1" class="ltx_text ltx_font_bold">0.6055</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">As indicated in Table <a href="#S4.T1" title="TABLE I ‣ IV-E Results ‣ IV Experimental results ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, SAAA and MCAN achieved significantly better results compared to all implementations of Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Straightforward structures such as the combination of pre-trained word embeddings and LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> do not tackle effectively such complicated tasks as VQA, while deeper and ingeniously designed methods such as SAAA and MCAN took over the ViVQA dataset better.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Especially, our proposed method, PAT, obtained the best results while leaving other methods a far distance. Particularly, PAT achieved approximately 6% better than SAAA and approximately 3% better than MCAN despite these two methods are the SOTA methods on the VQAv1 and VQAv2 that were not pre-trained on large-scale datasets.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.4.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.5.2" class="ltx_text ltx_font_italic">Ablation study</span>
</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Ablation study for PAT method.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">EM</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PAT w/o Hier.</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">0.5868</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PAT w LSTM</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_right">0.5981</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.1.4.3.1.1" class="ltx_text ltx_font_bold">PAT</span></th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_right ltx_border_b"><span id="S4.T2.1.4.3.2.1" class="ltx_text ltx_font_bold">0.6055</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">we conduct an ablation study to comprehensively discover how our two proposed modules, Hierarchical Linguistic Feature Extractor, and Parallel Attention module, contribute to the overall result of the PAT. Results are shown in Table <a href="#S4.T2" title="TABLE II ‣ IV-F Ablation study ‣ IV Experimental results ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">According to Table <a href="#S4.T2" title="TABLE II ‣ IV-F Ablation study ‣ IV Experimental results ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, the PAT which does not use LSTM or Hierarchical Linguistic Features Extractor to extract features of questions obtained lower accuracy. When equipped with LSTM or Hierarchical Linguistic Extractor, PAT achieved better results. Especially it achieved the best results when using the Hierarchical Linguistic Extractor. This result proves that the Hierarchical Linguistic Feature Extractor leverages the grammar dependency as well as the context of Vietnamese better than a simple LSTM network.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Ablation study for PAT that use 1-size kernel CNN to extract unigram features</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">EM</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PAT w/o 1-size kernel CNN</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">0.5848</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">PAT w 1-size kernel CNN</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_right ltx_border_b">0.6055</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">Moreover, as stated in Section <a href="#S3.SS1" title="III-A Hierarchical Linguistic Feature Extractor ‣ III Our proposed method ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, the Hierarchical Linguistic Feature Extractor uses CNN to extract up to 4-gram features, including the unigram features. This is necessary as we assume the 1-size kernel CNN used to extract unigram is used to project the pre-trained word embedding features into the latent space of PAT where it finds easier to fuse information with features from images. In Table <a href="#S4.T3" title="TABLE III ‣ IV-F Ablation study ‣ IV Experimental results ‣ PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we proved our assumption where PAT which uses an additional 1-size kernel CNN has a better result than one using unigram features extracted from the pre-trained word embedding.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and future works</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we present the PAT, which achieved the best performance on the benchmark ViVQA dataset. Our ablation study showed that the proposed Hierarchical Linguistic Feature Extractor performed better than LSTM when extracting features from questions.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In future works, we continue to investigate the impact of using Large Language Models (LLMs) on the results of VQA methods, as well as find the most effective multimodal structure that yields the best accuracy on the ViVQA dataset. In addition, our proposed method can be evaluated on two benchmarks datasets: EVJVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and OpenViVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span>, pages 6077–6086, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision (ICCV)</span>, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. Bao, W. Wang, L. Dong, Q. Liu, O. K. Mohammed, K. Aggarwal, S. Som, and
F. Wei.

</span>
<span class="ltx_bibblock">Vlmo: Unified vision-language pre-training with
mixture-of-modality-experts.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2111.02358</span>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov.

</span>
<span class="ltx_bibblock">Enriching word vectors with subword information.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>,
5:135–146, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX</span>, pages 104–120.
Springer, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Cho, J. Lu, D. Schwenk, H. Hajishirzi, and A. Kembhavi.

</span>
<span class="ltx_bibblock">X-lxmert: Paint, caption and answer questions with multi-modal
transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.11278</span>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/1810.04805, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Neural Comput.</span>, 9(8):1735–1780, nov 1997.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu.

</span>
<span class="ltx_bibblock">Pixel-bert: Aligning image pixels with text by deep multi-modal
transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.00849</span>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Jiang, I. Misra, M. Rohrbach, E. Learned-Miller, and X. Chen.

</span>
<span class="ltx_bibblock">In defense of grid features for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Kantharaj, X. Do, R. T. K. Leong, J. Q. Tan, E. Hoque, and S. R. Joty.

</span>
<span class="ltx_bibblock">Opencqa: Open-ended question answering with charts.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2210.06628, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
V. Kazemi and A. Elqursh.

</span>
<span class="ltx_bibblock">Show, ask, attend, and answer: A strong baseline for visual question
answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.03162</span>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
W. Kim, B. Son, and I. Kim.

</span>
<span class="ltx_bibblock">Vilt: Vision-and-language transformer without convolution or region
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
5583–5594. PMLR, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1412.6980, 2014.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
G. Li, N. Duan, Y. Fang, M. Gong, and D. Jiang.

</span>
<span class="ltx_bibblock">Unicoder-vl: A universal encoder for vision and language by
cross-modal pre-training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume 34, pages 11336–11344, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for vision and language.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.03557</span>, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13</span>, pages 740–755.
Springer, 2014.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Lu, D. Batra, D. Parikh, and S. Lee.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi.

</span>
<span class="ltx_bibblock">Unified-io: A unified model for vision, language, and multi-modal
tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2206.08916</span>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 29, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/cvf conference on computer vision and
pattern recognition</span>, pages 3195–3204, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Mathew, D. Karatzas, and C. Jawahar.

</span>
<span class="ltx_bibblock">Docvqa: A dataset for vqa on document images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision (WACV)</span>, pages 2200–2209, January 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty.

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">ICDAR</span>, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
N. H. Nguyen, D. T. Vo, K. Van Nguyen, and N. L.-T. Nguyen.

</span>
<span class="ltx_bibblock">Openvivqa: Task, dataset, and multimodal fusion models for visual
question answering in vietnamese.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.04183</span>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
N. L.-T. Nguyen, N. H. Nguyen, D. T. Vo, K. Q. Tran, and K. Van Nguyen.

</span>
<span class="ltx_bibblock">Vlsp 2022–evjvqa challenge: Multilingual visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.11752</span>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. O’Shea and R. Nash.

</span>
<span class="ltx_bibblock">An introduction to convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.08458</span>, 2015.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and
M. Rohrbach.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pages 8317–8326, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai.

</span>
<span class="ltx_bibblock">Vl-bert: Pre-training of generic visual-linguistic representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.08530</span>, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
H. Tan and M. Bansal.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.07490</span>, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R. Tanaka, K. Nishida, and S. Yoshida.

</span>
<span class="ltx_bibblock">Visualmrc: Machine reading comprehension on document images.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>,
35(15):13878–13888, May 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
D. Teney, P. Anderson, X. He, and A. Van Den Hengel.

</span>
<span class="ltx_bibblock">Tips and tricks for visual question answering: Learnings from the
2017 challenge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4223–4232, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
K. Q. Tran, A. T. Nguyen, A. T.-H. Le, and K. V. Nguyen.

</span>
<span class="ltx_bibblock">Vivqa: Vietnamese visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of the 35th Pacific Asia Conference on Language,
Information and Computation</span>, pages 546–554, Shanghai, China, 11 2021.
Association for Computational Lingustics.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg.

</span>
<span class="ltx_bibblock">Modeling context in referring expressions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span>,
pages 69–85. Springer, 2016.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pages 6281–6290, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
D. Zhang, R. Cao, and S. Wu.

</span>
<span class="ltx_bibblock">Information fusion in visual question answering: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Information Fusion</span>, 52:268–280, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting visual representations in vision-language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 5579–5588, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.08246" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.08247" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.08247">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.08247" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.08248" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 17:43:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
