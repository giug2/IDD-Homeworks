<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence</title>
<!--Generated on Tue Sep 10 16:25:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Autonomous Vehicles,  Explainable AI,  AI Errors" lang="en" name="keywords"/>
<base href="/html/2409.05731v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S1" title="In What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S2" title="In What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S2.SS1" title="In 2. Related Work ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>AV Trust and Explainability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S2.SS2" title="In 2. Related Work ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>AI Errors</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3" title="In What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.SS1" title="In 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.SS2" title="In 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Simulated Driving Scenarios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.SS3" title="In 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>AV Explanations and Errors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.SS4" title="In 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Measures</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.SS4.SSS1" title="In 3.4. Measures ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Main Outcomes: Scenario Ratings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.SS4.SSS2" title="In 3.4. Measures ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Context Descriptors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.SS4.SSS3" title="In 3.4. Measures ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Additional Outcomes</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4" title="In What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS1" title="In 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS1.SSS1" title="In 4.1. Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Summary of Main Outcomes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS1.SSS2" title="In 4.1. Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Overall Impact of Errors on Main Outcomes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS1.SSS3" title="In 4.1. Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Potential Harm of ‘What’-type Errors</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS2" title="In 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Impact of Driving Context: Perceived Harm and Difficulty of Driving</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS2.SSS1" title="In 4.2. Impact of Driving Context: Perceived Harm and Difficulty of Driving ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Relationship Between Harm and Difficulty</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS2.SSS2" title="In 4.2. Impact of Driving Context: Perceived Harm and Difficulty of Driving ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Impact of Harm and Difficulty On Main Outcome Ratings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS2.SSS3" title="In 4.2. Impact of Driving Context: Perceived Harm and Difficulty of Driving ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Impact Of Harm and Difficulty On Differences Between Error Conditions (Interaction Effects)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS3" title="In 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Trust and Expertise</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS3.SSS1" title="In 4.3. Trust and Expertise ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Impact Of Exposure To Errors On Trust</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS3.SSS2" title="In 4.3. Trust and Expertise ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Correlations of Initial Trust and Subjective Expertise with Outcomes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS4" title="In 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Self-Reported Rationale For Trust and Reliance Decisions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS4.SSS1" title="In 4.4. Self-Reported Rationale For Trust and Reliance Decisions ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Factors Contributing to Rating Decisions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.SS4.SSS2" title="In 4.4. Self-Reported Rationale For Trust and Reliance Decisions ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Feedback from Participants</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S5" title="In What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S5.SS1" title="In 5. Discussion ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Implications For Autonomous Vehicle Design and Research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S5.SS2" title="In 5. Discussion ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Limitations and Future Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S6" title="In What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Robert Kaufman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rokaufma@ucsd.edu">rokaufma@ucsd.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of California, San Diego</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">9500 Gilman Dr</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">La Jolla</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4">California</span><span class="ltx_text ltx_affiliation_country" id="id5.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id6.6.id6">92093</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aaron Broukhim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aabroukh@ucsd.edu">aabroukh@ucsd.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">University of California, San Diego</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">La Jolla</span><span class="ltx_text ltx_affiliation_state" id="id9.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id10.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Kirsh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kirsh@ucsd.edu">kirsh@ucsd.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">University of California, San Diego</span><span class="ltx_text ltx_affiliation_city" id="id12.2.id2">La Jolla</span><span class="ltx_text ltx_affiliation_state" id="id13.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id14.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nadir Weibel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:weibel@ucsd.edu">weibel@ucsd.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id15.1.id1">University of California, San Diego</span><span class="ltx_text ltx_affiliation_city" id="id16.2.id2">La Jolla</span><span class="ltx_text ltx_affiliation_state" id="id17.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id18.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">Explanations for autonomous vehicle (AV) decisions may build trust, however, explanations can contain errors. In a simulated driving study (n = 232), we tested how AV explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) affected a passenger’s comfort in relying on an AV, preference for control, confidence in the AV’s ability, and explanation satisfaction. Errors negatively affected all outcomes. Surprisingly, despite identical driving, explanation errors reduced ratings of the AV’s driving ability. Severity and potential harm amplified the negative impact of errors. Contextual harm and driving difficulty directly impacted outcome ratings and influenced the relationship between errors and outcomes. Prior trust and expertise were positively associated with outcome ratings. Results emphasize the need for accurate, contextually adaptive, and personalized AV explanations to foster trust, reliance, satisfaction, and confidence. We conclude with design, research, and deployment recommendations for trustworthy AV explanation systems.</p>
</div>
<div class="ltx_keywords">Autonomous Vehicles, Explainable AI, AI Errors
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Empirical studies in HCI</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing User studies</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Autonomous vehicles offer a wide range of potential societal benefits, including reduced driving infractions, traffic volume, environmental impact, and passenger stress <cite class="ltx_cite ltx_citemacro_citep">(Fagnant and Kockelman, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib22" title="">2015</a>)</cite>. Despite these benefits, it is a well-documented problem that adoption is limited by a lack of trust in how vehicles make decisions <cite class="ltx_cite ltx_citemacro_citep">(Kenesei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib39" title="">2022</a>)</cite>. This is not a problem specifically with autonomous vehicles, but a widespread issue across many types of AI-based systems <cite class="ltx_cite ltx_citemacro_citep">(Bedué and Fritzsche, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib6" title="">2022</a>)</cite>. System transparency via explainable AI (XAI) has been proposed as a means to mitigate concerns with AI-based systems like AVs, offering users a look “under the hood” of black-box AI models so they understand what the system is doing and why  <cite class="ltx_cite ltx_citemacro_citep">(Gunning and Aha, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib26" title="">2019</a>; Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib52" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, although potentially increasing trust, explanation of AI interactions in the real world can contain errors. Prior work has shown that when autonomous vehicles exhibit <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">driving</em> errors, passenger trust and willingness to rely on the vehicle can deteriorate quickly <cite class="ltx_cite ltx_citemacro_citep">(Seet et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib64" title="">2020</a>; Kaplan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib34" title="">2023</a>)</cite>. Far less is known about the impact of <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">explanation</em> errors <cite class="ltx_cite ltx_citemacro_citep">(Cabitza et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib9" title="">2024</a>; Kenny et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib40" title="">2021</a>)</cite>. Particularly for safety-critical systems like autonomous vehicles – that may rely on explanations and other in-vehicle communications to elicit trust comfort, and safe reliance with users – knowing the consequences of errors is pivotal to safe deployment. Understanding these consequences is essential for user reliance because, without this knowledge, AV systems cannot be deployed safely or ethically <cite class="ltx_cite ltx_citemacro_citep">(Martinho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib51" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In addition, human-AV interactions do not exist in a vacuum: they are sensitive to the contextual demands of the external driving environment <cite class="ltx_cite ltx_citemacro_citep">(Hoff and Bashir, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib28" title="">2015</a>)</cite>. It is important to consider the driving context in which explanation errors occur, as prior work shows that context may significantly influence how people interact with AI-based systems <cite class="ltx_cite ltx_citemacro_citep">(Lim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib45" title="">2009</a>; Schilit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib61" title="">1994</a>)</cite>, including autonomous vehicles <cite class="ltx_cite ltx_citemacro_citep">(Capallera et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib10" title="">2022</a>; De Salis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib16" title="">2020</a>)</cite>. The complexity of a driving situation and perceived risk of harm have been singled out as particular factors of interest <cite class="ltx_cite ltx_citemacro_citep">(Ha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib27" title="">2020</a>; Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib37" title="">2024b</a>)</cite>, as these might drive a person’s explanatory needs and reliance behaviors.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Finally, recent work has posited that AV explanations should be tailored to meet the specific needs of the people interacting with the system <cite class="ltx_cite ltx_citemacro_citep">(Ma and Feng, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib50" title="">2023</a>; Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib37" title="">2024b</a>)</cite>. It is well known that people may interact differently with AI-based systems <cite class="ltx_cite ltx_citemacro_citep">(Schneider and Handali, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib62" title="">2019</a>)</cite> based on the individual characteristics or prior experiences they may have <cite class="ltx_cite ltx_citemacro_citep">(Ayoub et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib5" title="">2021b</a>)</cite>. Particularly well studied are domain expertise <cite class="ltx_cite ltx_citemacro_citep">(Araujo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib2" title="">2020</a>; Pazzani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib58" title="">2022</a>; Kaufman and Kirsh, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib38" title="">2022</a>)</cite> and initial (dispositional) trust <cite class="ltx_cite ltx_citemacro_citep">(Hoff and Bashir, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib28" title="">2015</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In the present study, we examine the impact of explanation errors across a variety of realistic, simulated driving scenarios. To deepen our investigation and understand the significance of the <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">type</span> of error presented, participants were shown AV explanations at three distinct accuracy levels. We measured the effect of explanation errors on four main outcomes: comfort relying on the AV, preference for control, confidence in the AV’s driving ability, and explanation satisfaction. We include measures of scenario context – perceived harm and driving difficulty – to assess their impact on our driving outcomes, including how they may moderate the relationship between explanation errors and user perceptions. To explore the influence of individual differences, we also included measures of trust and expertise to determine if they predict study outcomes.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our findings reveal that explanation errors, contextual characteristics, and personal traits significantly impact how a person may think, feel, and behave towards AVs. Explanation errors negatively affected all outcomes, with impacts proportional to the magnitude and negative implications of the error. Harm and driving difficulty directly impacted outcomes as well as moderated the relationship between errors and outcomes, though in opposing ways. Overall, harm was generally seen as more important and more negative than difficulty. Participants with higher expertise tended to trust AVs more, and these each correlated with more positive outcome ratings in turn.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In sum, we contribute:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A study on the impact of explanation errors based on information type (‘why’ and ’what + why’).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">An assessment of how contextual harm and driving difficulty, initial trust, and expertise impact study outcomes.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A set of future design and research implications based on the present findings.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Results highlight the critical need for accurate and contextually adaptive explanations for autonomous vehicles to enhance user trust, reliance, satisfaction, and confidence. Recognizing the implications of explanation errors is vital for advancing AV research and guiding design teams to make informed decisions. This understanding lays the groundwork for developing context-aware designs, personalized explanation interfaces, and establishing ethical or regulatory guidelines to ensure the deployment of safe and trustworthy explainable AI (XAI) systems for autonomous vehicles.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>AV Trust and Explainability</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">Human-Centered Explainable AI</span> – Continuous development of explainable AI (XAI) has brought major progress in the quest for trust through AI transparency <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib52" title="">2019</a>)</cite>. Approaches to XAI vary, often limited by the availability of the model <cite class="ltx_cite ltx_citemacro_citep">(Simonyan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib66" title="">2013</a>; Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib59" title="">2016</a>)</cite>. Recent work found that – even when an explanation is given – trust and engagement with the system may not improve unless the <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">right information</em> is given in the <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.3">right way</em>, at the <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.4">right time</em> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib69" title="">2019</a>; Liao and Varshney, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib44" title="">2021</a>)</cite>. Several studies and theory pieces have demonstrated the value of human-interpretable explanations on user understanding and trust  <cite class="ltx_cite ltx_citemacro_citep">(Holzinger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib29" title="">2019</a>; Soltani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib67" title="">2022</a>)</cite>. In particular, explanations that are modeled off those given by human experts have suggested as a means to increase understanding for experts and novices alike <cite class="ltx_cite ltx_citemacro_citep">(Pazzani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib58" title="">2022</a>; Kaufman and Kirsh, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib38" title="">2022</a>)</cite>. There have likewise been calls for the need for explanations to be sensitive to particular user characteristics (such as personal traits or experiences)  <cite class="ltx_cite ltx_citemacro_citep">(Ehsan and Riedl, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib20" title="">2020</a>; Ehsan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib19" title="">2021</a>; Kaufman and Kirsh, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib36" title="">2023</a>)</cite> or context of use (including the specific use environment and goals of a user) <cite class="ltx_cite ltx_citemacro_citep">(Schilit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib61" title="">1994</a>; Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib37" title="">2024b</a>)</cite>. Recent discussion on XAI usefulness had proposed that explanations should move beyond simply conveying algorithmic mechanisms for decision-making and instead supporting user hypothesis generation <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib53" title="">2023</a>)</cite>. Despite the large amount of research on explanation design, little has been done to understand what happens when these explanations fail. The study we present here is a first step towards filling this knowledge gap, using autonomous vehicles as a specific domain of interest.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Interface Modalities</span> – Research on in-vehicle interfaces for explanation, such as visual heads-up-displays (HUDs) <cite class="ltx_cite ltx_citemacro_citep">(Currano et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib14" title="">2021</a>; Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib11" title="">2016</a>; Schartmüller et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib60" title="">2019</a>)</cite>, audio interaction <cite class="ltx_cite ltx_citemacro_citep">(Mok et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib54" title="">2015</a>; Jeon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib33" title="">2009</a>; Löcken et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib48" title="">2017</a>)</cite>, and even haptic feedback for drivers <cite class="ltx_cite ltx_citemacro_citep">(Di Campli San Vito et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib17" title="">2020</a>)</cite> has shown that there are a variety of ways explanations can support users, each dependent on the use-case and context of interest. The benefits and drawbacks of different modalities often relate to the complex process of transferring sufficient knowledge to accomplish a task (such as to build understanding through system transparency) without creating too much cognitive load <cite class="ltx_cite ltx_citemacro_citep">(Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib35" title="">2024a</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib41" title="">2023</a>; Colley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib13" title="">2021</a>)</cite>. In the present study, we leverage video and audio explanations for our study on the impact of errors, as these are common and efficient methods to transfer information to a user without overwhelm. We present both modalities of information at the same time to increase the accessibility of our study and ensure there is successful transfer of information.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Explanation Content</span> – Choosing the appropriate content for an AI explanation is crucial for enhancing rider trust and reliance <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib52" title="">2019</a>)</cite>. Recent work as focused on providing a description of <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.2">what</em> a vehicle is doing and <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.3">why</em> a vehicle is doing it, as these enable a user to create a momentary evaluation of the AV’s behavior in terms of reliance <cite class="ltx_cite ltx_citemacro_citep">(Hoff and Bashir, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib28" title="">2015</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Koo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib42" title="">2015</a>)</cite> present ’how’, ‘why’, and a combination of both in various simulated autonomous driving scenarios to assess driver attitudes and safety performance. They found improved safety when both ’how’ and ‘why’ were presented to drivers, but a preference for ‘why’ explanations alone. <cite class="ltx_cite ltx_citemacro_citet">Kaufman et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib35" title="">2024a</a>)</cite> leveraged auditory and visual explanations to teach humans to be better drivers via an ’AI coach’, highlighting the additive value of <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.4">what</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.5">why</em>-type information for transferring knowledge, adding that <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.6">too much</span> information can cause overwhelm. They emphasize the need for explanations to strike a balance between complexity and comprehensiveness. The impact of <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.7">what</em> (similar to Koo’s <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.8">how</em>) and <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.9">why</em>-type explanation <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.10">errors</em> – explored in the present study – remains unknown.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">Factors Impacting Trust</span> – Knowing what factors may impact a person’s trust and reliance decisions is vitally important to designing XAI explanations to support them. Hoff and Bashir’s theoretical model of trust in automation highlights the importance of three distinct yet interdependent facets of trust: dispositional (e.g. personality or cultural attitudes), situational (e.g. based on a particular context of use), and learned trust (e.g. based on a present evaluation of system performance) <cite class="ltx_cite ltx_citemacro_citep">(Hoff and Bashir, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib28" title="">2015</a>)</cite>. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Kaufman et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib37" title="">2024b</a>)</cite> developed a framework to understand situational awareness in joint action between humans and AV, a key focus of explainable AI transparency in safety-critical situations like driving. They describe how communications like AV explanations can enable human-AV teamwork to achieve particular goals like safe and trustworthy driving. Factors of interest include external driving conditions, human traits and abilities, and communication preferences and goals – all of which are crucial in managing driving difficulty and reducing the risk of harm. Using these system-based models as a backdrop, AI explanation designers can form hypotheses on how to build more trustworthy systems. In the present study, we build off these models by investigating specific driving context factors and personal traits which may impact a person’s reliance judgments and, in the case of context, how much an error matters.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p5.1.1">Knowledge Gap</span> – Indeed, explanation interfaces have been implemented in mainstream deployed autonomous vehicles, such as communication interfaces by Tesla <cite class="ltx_cite ltx_citemacro_citep">(Inc., <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib32" title="">[n. d.]</a>)</cite> and Waymo <cite class="ltx_cite ltx_citemacro_citep">(LLC, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib47" title="">[n. d.]</a>)</cite>. Despite these efforts, we still know very little on the impact of AV explanation errors on how a person will trust and interact with an AV. We know even less about how errors may depend on contextual factors like driving difficulty or harm <cite class="ltx_cite ltx_citemacro_citep">(Capallera et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib10" title="">2022</a>; De Salis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib16" title="">2020</a>; Ha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib27" title="">2020</a>)</cite>. With this work, we seek to understand how contextual harm and difficulty of driving may affect study outcomes, as well as contribute to the body of literature on how personal traits predict a person’s interactions with an AV.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>AI Errors</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Widespread integration of AI systems into everyday tasks has demonstrated huge benefits to productivity and optimization in many domains <cite class="ltx_cite ltx_citemacro_citep">(Fauzi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib23" title="">2023</a>)</cite>. However, concerns over AI errors remain a major point of contention, particularly as systems proliferate. Examples of errors with real-world consequences include language models’ propensity to hallucinate <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib70" title="">2024</a>)</cite> and algorithmic bias that favors men over women used in the hiring process <cite class="ltx_cite ltx_citemacro_citep">(Dastin, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib15" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">General AV Errors</span> – Autonomous vehicle driving errors have important real-world consequences, which may include physical harm or even fatalities <cite class="ltx_cite ltx_citemacro_citep">(Favarò et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib24" title="">2017</a>)</cite>. Even in cases where AV driving outperforms humans <cite class="ltx_cite ltx_citemacro_citep">(Schwall et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib63" title="">2020</a>)</cite>, concerns over vehicle errors can be a major hindrance to adoption and use <cite class="ltx_cite ltx_citemacro_citep">(Choi and Ji, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib12" title="">2015</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Luo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib49" title="">2020</a>)</cite> shows that errors caused by an AV had a more significant negative impact on user trust than external errors, such as those caused by other drivers or road conditions. Declines in trust may be difficult to recover from and have long-lasting effect <cite class="ltx_cite ltx_citemacro_citep">(Seet et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib64" title="">2020</a>)</cite>. Explanations are no panacea: trust is difficult to achieve when the system itself performs poorly, even when explanations meant to elicit trust are presented <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib34" title="">2023</a>)</cite>. In this paper, we seek to connect prior work showing the dire impact of driving errors <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib49" title="">2020</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib71" title="">2022</a>)</cite> to broader XAI literature investigating user perceptions of AI explanations <cite class="ltx_cite ltx_citemacro_citep">(Lebovitz, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib43" title="">2019</a>; Cabitza et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib9" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Explanation Errors</span> – Some prior work has investigated the impact of <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.2">explanation</em> errors for autonomous systems. In a study on “white box” XAI, <cite class="ltx_cite ltx_citemacro_citet">Cabitza et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib9" title="">2024</a>)</cite> found that non-expert users tend to not catch explanations errors and believe the system even when explanations were wrong, attributing the phenomena to the Halo Effect found in social psychology where people assumed correctness of the system without verifying accuracy. Over- or under-reliance is a problem as people learn to calibrate their interactions with AI systems <cite class="ltx_cite ltx_citemacro_citep">(Endsley, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib21" title="">2018</a>)</cite>. Conflicting results suggest that explanations may help reduce over-reliance in some cases <cite class="ltx_cite ltx_citemacro_citep">(Vasconcelos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib68" title="">2023</a>)</cite>, but increase over-reliance in others <cite class="ltx_cite ltx_citemacro_citep">(Kenny et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib40" title="">2021</a>)</cite>. Other research has shown that the influence of explanations on reliance may be based on the systems’ performance itself <cite class="ltx_cite ltx_citemacro_citep">(Papenmeier et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib57" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">Knowledge Gap</span> – Though several prior studies have investigated the consequences of accurate AV explanations, the impact of explanation errors on reliance behavior and related outcomes remains unexplored. Of particular interest to us are cases when the autonomous vehicle’s driving performs properly, as this allows us to separate the impact of driving performance from the impact of explanation performance. This has yest to be explored, and we address this important knowledge gap.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We conducted an online experiment using realistic, simulated driving scenarios of an autonomous vehicle driving through various environments. These ranged from rural to urban driving and from routine navigational challenges like driving around construction cones to challenging situations like avoiding collisions with erratic drivers. During the experiment, participants viewed videos of the scenarios and, after each video, provided ratings related to trust, reliance, satisfaction, and evaluation of the AV.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To test the impact of explanation errors, participants were shown three versions of each scenario. Those with an “accurate” explanation were correctly told ‘what’ the AV was doing and ‘why’ it was doing it. Errors were introduced via a “low” error condition, where the AV correctly explained ‘what’ it was doing but incorrectly explained ‘why’, and a “high” error condition, where both ‘what’ and ‘why’ explanations were incorrect. In all three versions of each scenario, the AV drove identically, so only the explanations differed. The AV drove accurately and lawfully in all cases. Participants were randomly shown videos from 9 of 27 possible scenarios, providing ratings after each video, making 27 total video ratings per participant (9 scenarios X 3 accuracy levels). Scenarios were presented in random order. The 27 possible scenarios are described in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.T1" title="Table 1 ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Scenario Descriptions</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1.1">Scenario</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.1.2.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1.1.1">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.1.1">
<span class="ltx_p" id="S3.T1.1.2.1.1.1.1" style="width:43.4pt;">10s1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.2.1">
<span class="ltx_p" id="S3.T1.1.2.1.2.1.1" style="width:390.3pt;">Left turn in busy intersection, navigating around object in road.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.1.1">
<span class="ltx_p" id="S3.T1.1.3.2.1.1.1" style="width:43.4pt;">10s2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.2.1">
<span class="ltx_p" id="S3.T1.1.3.2.2.1.1" style="width:390.3pt;">Slowing to avoid collision between two vehicles ahead of the ego vehicle.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.1.1">
<span class="ltx_p" id="S3.T1.1.4.3.1.1.1" style="width:43.4pt;">10s4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.2.1">
<span class="ltx_p" id="S3.T1.1.4.3.2.1.1" style="width:390.3pt;">Parallel parking between two cars on side of busy road.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.1.1">
<span class="ltx_p" id="S3.T1.1.5.4.1.1.1" style="width:43.4pt;">10s5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.2.1">
<span class="ltx_p" id="S3.T1.1.5.4.2.1.1" style="width:390.3pt;">Slowing to avoid being sideswiped during turn by a vehicle that crosses into ego vehicle’s lane.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.1.1">
<span class="ltx_p" id="S3.T1.1.6.5.1.1.1" style="width:43.4pt;">10s6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.2.1">
<span class="ltx_p" id="S3.T1.1.6.5.2.1.1" style="width:390.3pt;">Sudden stop mid-intersection by large lead vehicle in adverse weather.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.1.1">
<span class="ltx_p" id="S3.T1.1.7.6.1.1.1" style="width:43.4pt;">10s7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.2.1">
<span class="ltx_p" id="S3.T1.1.7.6.2.1.1" style="width:390.3pt;">Left turn quickly followed by right merge for quick right turn in urban environment in adverse weather.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.1.1">
<span class="ltx_p" id="S3.T1.1.8.7.1.1.1" style="width:43.4pt;">10s8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.2.1">
<span class="ltx_p" id="S3.T1.1.8.7.2.1.1" style="width:390.3pt;">Ego vehicle slows for pedestrian crossing road in non-crosswalk area.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.1.1">
<span class="ltx_p" id="S3.T1.1.9.8.1.1.1" style="width:43.4pt;">4s1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.2.1">
<span class="ltx_p" id="S3.T1.1.9.8.2.1.1" style="width:390.3pt;">Reversing in parking lot.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.1.1">
<span class="ltx_p" id="S3.T1.1.10.9.1.1.1" style="width:43.4pt;">4s2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.2.1">
<span class="ltx_p" id="S3.T1.1.10.9.2.1.1" style="width:390.3pt;">Overtaking cyclist in suburban environment.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.11.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.1.1">
<span class="ltx_p" id="S3.T1.1.11.10.1.1.1" style="width:43.4pt;">4s3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.2.1">
<span class="ltx_p" id="S3.T1.1.11.10.2.1.1" style="width:390.3pt;">Ego vehicle must stop quickly for a pedestrian who jumps into the road.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.12.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.1.1">
<span class="ltx_p" id="S3.T1.1.12.11.1.1.1" style="width:43.4pt;">4s4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.2.1">
<span class="ltx_p" id="S3.T1.1.12.11.2.1.1" style="width:390.3pt;">Merging from far right lane to far left lane on highway to avoid emergency vehicles / car accident.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.13.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.1.1">
<span class="ltx_p" id="S3.T1.1.13.12.1.1.1" style="width:43.4pt;">4s5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.2.1">
<span class="ltx_p" id="S3.T1.1.13.12.2.1.1" style="width:390.3pt;">Ego vehicle must avoid object that falls off of lead vehicle on highway at night.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.14.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.14.13.1.1">
<span class="ltx_p" id="S3.T1.1.14.13.1.1.1" style="width:43.4pt;">4s6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.14.13.2.1">
<span class="ltx_p" id="S3.T1.1.14.13.2.1.1" style="width:390.3pt;">Ego vehicle hydroplanes on wet road on highway, and needs to maintain control.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.15.14.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.15.14.1.1">
<span class="ltx_p" id="S3.T1.1.15.14.1.1.1" style="width:43.4pt;">4s7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.15.14.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.15.14.2.1">
<span class="ltx_p" id="S3.T1.1.15.14.2.1.1" style="width:390.3pt;">Ego vehicle needs to pull over for flat tire at high speed.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.16.15.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.16.15.1.1">
<span class="ltx_p" id="S3.T1.1.16.15.1.1.1" style="width:43.4pt;">4s8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.16.15.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.16.15.2.1">
<span class="ltx_p" id="S3.T1.1.16.15.2.1.1" style="width:390.3pt;">Ego vehicle merges left to enter a busy highway.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.17.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.17.16.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.17.16.1.1">
<span class="ltx_p" id="S3.T1.1.17.16.1.1.1" style="width:43.4pt;">5s2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.17.16.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.17.16.2.1">
<span class="ltx_p" id="S3.T1.1.17.16.2.1.1" style="width:390.3pt;">Ego vehicle makes blind turn in intersection due to obstructed view. <span class="ltx_text ltx_font_bold" id="S3.T1.1.17.16.2.1.1.1">[removed]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.18.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.18.17.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.18.17.1.1">
<span class="ltx_p" id="S3.T1.1.18.17.1.1.1" style="width:43.4pt;">5s3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.18.17.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.18.17.2.1">
<span class="ltx_p" id="S3.T1.1.18.17.2.1.1" style="width:390.3pt;">Ego vehicle turning left and avoids a collision with another vehicle who ran a red light (T-bone).</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.19.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.19.18.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.19.18.1.1">
<span class="ltx_p" id="S3.T1.1.19.18.1.1.1" style="width:43.4pt;">5s4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.19.18.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.19.18.2.1">
<span class="ltx_p" id="S3.T1.1.19.18.2.1.1" style="width:390.3pt;">Car in parallel lane merges into ego vehicle’s path.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.20.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.20.19.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.20.19.1.1">
<span class="ltx_p" id="S3.T1.1.20.19.1.1.1" style="width:43.4pt;">5s5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.20.19.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.20.19.2.1">
<span class="ltx_p" id="S3.T1.1.20.19.2.1.1" style="width:390.3pt;">Ego vehicle needs to navigate around a stopped cyclist. <span class="ltx_text ltx_font_bold" id="S3.T1.1.20.19.2.1.1.1">[removed]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.21.20">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.21.20.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.21.20.1.1">
<span class="ltx_p" id="S3.T1.1.21.20.1.1.1" style="width:43.4pt;">5s6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.21.20.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.21.20.2.1">
<span class="ltx_p" id="S3.T1.1.21.20.2.1.1" style="width:390.3pt;">Ego vehicle navigates construction zone at night.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.22.21">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.22.21.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.22.21.1.1">
<span class="ltx_p" id="S3.T1.1.22.21.1.1.1" style="width:43.4pt;">7s1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.22.21.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.22.21.2.1">
<span class="ltx_p" id="S3.T1.1.22.21.2.1.1" style="width:390.3pt;">Hidden stop sign at night.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.23.22">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.23.22.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.23.22.1.1">
<span class="ltx_p" id="S3.T1.1.23.22.1.1.1" style="width:43.4pt;">7s3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.23.22.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.23.22.2.1">
<span class="ltx_p" id="S3.T1.1.23.22.2.1.1" style="width:390.3pt;">Lead vehicle quickly decelerates (brake check)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.24.23">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.24.23.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.24.23.1.1">
<span class="ltx_p" id="S3.T1.1.24.23.1.1.1" style="width:43.4pt;">7s4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.24.23.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.24.23.2.1">
<span class="ltx_p" id="S3.T1.1.24.23.2.1.1" style="width:390.3pt;">Ego vehicle crosses the midline to overtake a slow lead vehicle.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.25.24">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.25.24.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.25.24.1.1">
<span class="ltx_p" id="S3.T1.1.25.24.1.1.1" style="width:43.4pt;">7s5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.25.24.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.25.24.2.1">
<span class="ltx_p" id="S3.T1.1.25.24.2.1.1" style="width:390.3pt;">Ego vehicle needs to slow quickly from high speed for a stopped cyclist around a turn. <span class="ltx_text ltx_font_bold" id="S3.T1.1.25.24.2.1.1.1">[removed]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.26.25">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.26.25.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.26.25.1.1">
<span class="ltx_p" id="S3.T1.1.26.25.1.1.1" style="width:43.4pt;">7s6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.26.25.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.26.25.2.1">
<span class="ltx_p" id="S3.T1.1.26.25.2.1.1" style="width:390.3pt;">Vehicle failure (flat tire) in small parking lot.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.27.26">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.27.26.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.27.26.1.1">
<span class="ltx_p" id="S3.T1.1.27.26.1.1.1" style="width:43.4pt;">xs2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.27.26.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.27.26.2.1">
<span class="ltx_p" id="S3.T1.1.27.26.2.1.1" style="width:390.3pt;">Ego vehicle waits for child to cross crosswalk before turning right at stop sign.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.28.27">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.1.28.27.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.28.27.1.1">
<span class="ltx_p" id="S3.T1.1.28.27.1.1.1" style="width:43.4pt;">xs3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.1.28.27.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.28.27.2.1">
<span class="ltx_p" id="S3.T1.1.28.27.2.1.1" style="width:390.3pt;">Ego vehicle stops during right turn to avoid collision with vehicle turning right from incorrect (left) lane.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We evaluated changes across four major outcomes of interest, with two additional descriptive outcomes, making a total of six ratings per scenario video. After each video, participants rated their: (1) comfort relying on the AV, (2) preference to take control, (3) satisfaction with the explanation, and (4) confidence in the AV’s driving ability. We hypothesized that ratings may be context-dependent. As such, we also collected two ratings describing the driving context: (5) perceived harm and (6) perceived difficulty of driving in each scenario. These context descriptor variables were collected after the accurate explanation videos only. Outside of the rating task, participants answered questions about their trust in AVs, expertise, and demographics.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="322" id="S3.F1.g1" src="extracted/5846160/examplespread.png" width="568"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Example images from four driving scenarios: (1) the AV slows for a pedestrian crossing the road on a foggy day in the city, (2) the AV is cut off by a vehicle turning right from the left lane, (3) the AV merges around a slow lead vehicle in a forested area, (4) the AV moves past a cyclist on a suburban street. In the bottom right corner of each scenario, written AV explanations appear on the vehicle’s dashboard display.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F1.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F1.2">Example images from four driving scenarios: (1) the AV slows for a pedestrian crossing the road on a foggy day in the city, (2) the AV is cut off by a vehicle turning right from the left lane, (3) the AV merges around a slow lead vehicle in a forested area, (4) the AV moves past a cyclist on a suburban street. In the bottom right corner of each scenario, written AV explanations appear on the vehicle’s dashboard display.</p>
</div>
</div>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Participants</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">A total of 232 participants spread throughout the United States participated in the study and completed all study procedures. Participants were recruited from the general population via existing participant email lists and via SONA,<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.sona-systems.com</span></span></span> an undergraduate study pool system where students are granted study credit for participation. The mean age of the study sample was 23.6 (SD = 11.3), with ages ranging from 18 to 85 years. The sample was 73.7% female.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Simulated Driving Scenarios</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">All simulated driving videos were custom-made by the research team using DReye VR <cite class="ltx_cite ltx_citemacro_citep">(Silvera et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib65" title="">2022</a>)</cite>, a tool for creating realistic driving scenarios using the open-source driving simulator CARLA <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib18" title="">2017</a>)</cite>. Scenario design was inspired by the National Highway Traffic Safety Administration (NHTSA) list of common driving situations that result in vehicle crashes <cite class="ltx_cite ltx_citemacro_citep">(Najm et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib55" title="">2007</a>)</cite>. Examples include unexpected pedestrians in the road, collision avoidance from erratic other drivers, construction and emergency vehicle zones, parallel parking and reversing from park, navigating around stopped or slowed vehicles or cyclists, and dealing with flat tires or hydroplaning. Videos lasted between 10 and 30 seconds each (Examples in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.F1" title="Figure 1 ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Data from a total of 27 scenarios (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.T1" title="Table 1 ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">1</span></a>) was collected during the study; three scenarios were removed during analysis due to the AV’s driving determined to be imperfect (going above the speed limit, crossing over the center line during a turn, and improper yielding), making a total of 24 included in the final analysis. Data was cleaned prior to analysis to ensure that all videos were viewed in full.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>AV Explanations and Errors</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">During all driving scenario videos, explanations of the AV’s behavior were provided to participants at the time of AV action. Explanations were presented simultaneously in visual and auditory form to reflect current trends in AV explanation research <cite class="ltx_cite ltx_citemacro_citep">(Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib35" title="">2024a</a>)</cite>, as well as to provide better accessibility to participants. Specifically, explanations were presented visually in written English on the vehicle’s dashboard display and in auditory fashion via spoken English produced by Amazon Polly Text-To-Speech <cite class="ltx_cite ltx_citemacro_citep">(Inc., <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib31" title="">2023</a>)</cite>. For an example of the visual presentation of explanations, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.F1" title="Figure 1 ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Explanations provide information on ‘what’ action the AV is doing (e.g. <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.1">braking</span>) as well as a local explanation for ‘why’ the vehicle is doing it (e.g. <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2">to avoid a collision with a merging vehicle</span>). The importance of ‘what’ and ‘why’-type information has been studied in past experimental work on explanations of driving behavior by <cite class="ltx_cite ltx_citemacro_citet">Kaufman et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib35" title="">2024a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Koo et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib42" title="">2015</a>)</cite>. Frameworks by <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib69" title="">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Lim et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib46" title="">2019</a>)</cite> in cognitive science and <cite class="ltx_cite ltx_citemacro_citet">Miller (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib52" title="">2019</a>)</cite> in philosophy emphasize that both ‘what’ and ‘why’ information are important for transparency, trust, and user understanding when interacting with AI-based systems like AVs. The work presented here examines the impact of both ‘why’ errors alone, and ’why and what’ errors combined, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Explanation errors were presented via three conditions differing in the accuracy of the explanation provided. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S3.T2" title="Table 2 ‣ 3.3. AV Explanations and Errors ‣ 3. Method ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">2</span></a> shows a breakdown of the conditions and provides an example taken from one of the scenarios used in the study. Comparing the “accurate” to “low” group shows the impact of errors related to the AV’s rationale for behavior (i.e. ‘why’). Comparing the “low” to “high” group shows the impact of adding errors related to the AV’s description of its own action (i.e. adding ‘what’ to the ‘why’ error). It is important to note that the AV’s actual driving behavior remained consistent, accurate, and lawful across conditions; only the explanation itself changed. Participants rated videos for 9 of 27 possible scenarios – seeing one video for each of the three conditions per scenario – presented in random order. This makes a total of 27 total video ratings per participant (9 scenarios X 3 accuracy levels).</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">As part of a secondary analysis, we further categorized mistakes in the “high” error condition – where the AV is providing an incorrect description of what it is doing – based on the potential harm that <em class="ltx_emph ltx_font_italic" id="S3.SS3.p4.1.1">would</em> result should the AV have acted on the mistaken ‘what’ description. For example, if the AV mistakenly says it is going “left” when really it is going straight, we hypothesized that the impact of this error would be greater if going left would result in an accident, as opposed to simply a wrong turn. To explore if this difference in pragmatics does impact our results, we run a secondary analysis within just the ’high’ condition to test for impact of <em class="ltx_emph ltx_font_italic" id="S3.SS3.p4.1.2">potential</em> harm.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Experimental Conditions. Participants saw videos across each condition, providing ratings for each.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.1" style="width:65.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1.1.1">Error Condition</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.2.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1.1.1">Explanation Description</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.1.3.1.1" style="width:173.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1.1.1">Example</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.2.1.1.1.1" style="width:65.0pt;">Accurate</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.2.1.2.1.1" style="width:130.1pt;">Correct ‘what’ and correct ‘why’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.3.1">
<span class="ltx_p" id="S3.T2.1.2.1.3.1.1" style="width:173.4pt;"><span class="ltx_text ltx_font_italic" id="S3.T2.1.2.1.3.1.1.1">“Braking, a pedestrian is crossing the road.”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.1.1">
<span class="ltx_p" id="S3.T2.1.3.2.1.1.1" style="width:65.0pt;">Low</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.2.1">
<span class="ltx_p" id="S3.T2.1.3.2.2.1.1" style="width:130.1pt;">Correct ‘what’ and incorrect ‘why’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.3.1">
<span class="ltx_p" id="S3.T2.1.3.2.3.1.1" style="width:173.4pt;"><span class="ltx_text ltx_font_italic" id="S3.T2.1.3.2.3.1.1.1">“Braking, a cyclist is crossing the road.”</span>      (When the obstacle is actually a pedestrian)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.1.1">
<span class="ltx_p" id="S3.T2.1.4.3.1.1.1" style="width:65.0pt;">High</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.2.1">
<span class="ltx_p" id="S3.T2.1.4.3.2.1.1" style="width:130.1pt;">Incorrect ‘what’ and incorrect ‘why’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T2.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.3.1">
<span class="ltx_p" id="S3.T2.1.4.3.3.1.1" style="width:173.4pt;"><span class="ltx_text ltx_font_italic" id="S3.T2.1.4.3.3.1.1.1">“Merging right, a cyclist is crossing the road.”</span> (When the vehicle is slowing, not merging, and the obstacle is actually a pedestrian)</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Measures</h3>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>Main Outcomes: Scenario Ratings</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">These measures were taken after each video to provide insight into the impact of the explanation errors.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.I1.i1.p1.1.1">Comfort</span><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.2"> relying on AV</span> (proxy for trust). “How comfortable would you feel relying on this AV in this specific situation?” (0-10 rating scale)</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.I1.i2.p1.1.1">Reliance</span><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.2"> on AV</span> (preference to take control). “If this specific situation were to happen in the real world, would you prefer to rely on an AV or take control yourself?” (Binary choice: ’Rely on AV’ or ’Take control myself’). To aid comparison, reliance data was transformed from 0-1 to 0-10 to be on the same scale as the other variables. This does not impact how the data is interpreted.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.I1.i3.p1.1.1">Satisfaction</span><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.2"> with explanation.</span> “How satisfied are you with the AV’s explanation?” (0-10 rating scale)</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.I1.i4.p1.1.1">Confidence</span><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.2"> in AV driving ability.</span> “Please rate your confidence in the AV’s driving ability.” (0-10 rating scale). Note: the actual driving performance never changed.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Context Descriptors</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">These measures were taken for each scenario after the ”accurate” condition video only to provide insight into the impact of driving context on main outcomes.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.I2.i1.p1.1.1">Harm</span><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.2"> of Driving Situation.</span> “In the real world, how would you rate the risk of harm of this specific driving situation?” (0-10 rating scale)</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.I2.i2.p1.1.1">Difficulty</span><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.2"> of Driving.</span> “In the real world, how would you rate the difficulty of driving in this specific situation?” (0-10 rating scale)</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3. </span>Additional Outcomes</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">Additional outcomes further contextualize our main experimental findings. These were measured before and/or after the rating task.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i1.p1.1.1">Expertise</span>. Expertise was measured before the rating task via three self-rated questions related to a person’s knowledge and understanding of AVs. (5-point likert scale from Strongly Disagree to Strongly Agree)</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i2.p1.1.1">Trust</span>. Trust was measured before and after the rating task via four questions related to adaptability, safety, overt trust, and willing to recommend a friend to ride in an AV. (5-point likert scale from Strongly Disagree to Strongly Agree)</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i3.p1.1.1">Explicit Factors Contributing to Reliance Decisions</span>. After the task, participants rated the relative impact of seven aspects of the driving and explanation experience on their reliance decision-making. (5-point likert scale from Not At All to Very Much). Factors included: potential harm, driving difficulty, accuracy of ‘what’ and ‘why’ information, AV driving performance, prior knowledge, and previous scenarios viewed.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Summary of Main Outcomes</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Across our four main outcomes, segmenting the data by explanation error condition level gives us an initial impression on the impact of our experimental manipulation. We find the highest scores for comfort relying on the AV, reliance preference, satisfaction with the AV’s explanation, and confidence in the AV’s driving ability in the Accurate condition, followed by the Low condition and then the High condition (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.T3" title="Table 3 ‣ 4.1.1. Summary of Main Outcomes ‣ 4.1. Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">3</span></a>). Visualizing main outcomes by scenario, we find that the effect of condition was very consistent across scenarios (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.F2" title="Figure 2 ‣ 4.1.1. Summary of Main Outcomes ‣ 4.1. Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">Even when explanations were accurate, participants’ overall comfort relying on the AV (comfort), and their preference to rely on the AV (reliance), were middling to low, reflecting the overall reluctance to trust AVs found in past human-AV interaction research by <cite class="ltx_cite ltx_citemacro_citet">Kenesei et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib39" title="">2022</a>)</cite>. Participants were more positive about the AV explanations provided to them in the study, however, this satisfaction deteriorated quickly when errors were introduced. Despite the AV’s driving performance – and therefore, demonstrated ability – remaining consistent across all conditions, <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p2.1.1">impressions</em> of the AV’s driving ability worsened as AV explanation errors were introduced.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="447" id="S4.F2.g1" src="extracted/5846160/individualscenarios.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Mean Outcome by Error Condition Across All Scenarios. Plot A shows mean scores for Comfort Relying on AV, Plot B shows mean scores for Reliance Preference (Binary, scaled to 1-10), Plot C shows mean scores for Satisfaction with AV Explanation, and Plot D shows mean scores for Confidence in AV Driving Ability. Results consistently demonstrated the trend of Accurate ¿ Low ¿ High.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F2.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F2.2">Four visualizations, one for each major outcomes by error condition, shown for all 24 scenarios, depicting the mean outcome score by error condition across. Plot A shows mean scores for Comfort Relying on AV, Plot B shows mean scores for Reliance Preference (Binary, scaled to 1-10), Plot C shows mean scores for Satisfaction with AV Explanation, and Plot D shows mean scores for Confidence in AV Driving Ability. Results consistently demonstrated the trend of Accurate ¿ Low ¿ High.</p>
</div>
</div>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Means and Standard Errors of Main Outcomes By Error Condition. As errors increased, outcome ratings decreased.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.1"></td>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.2.1">
<span class="ltx_p" id="S4.T3.1.1.1.2.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1.1.1">Accurate</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.3.1">
<span class="ltx_p" id="S4.T3.1.1.1.3.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1.1.1">Low</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.4.1">
<span class="ltx_p" id="S4.T3.1.1.1.4.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1.1.1">High</span></span>
</span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.1.1">
<span class="ltx_p" id="S4.T3.1.2.2.1.1.1" style="width:95.4pt;">Comfort Relying on AV</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.2.1">
<span class="ltx_p" id="S4.T3.1.2.2.2.1.1" style="width:43.4pt;">4.59 (0.06)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.3.1">
<span class="ltx_p" id="S4.T3.1.2.2.3.1.1" style="width:43.4pt;">3.48 (0.06)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.2.4.1">
<span class="ltx_p" id="S4.T3.1.2.2.4.1.1" style="width:43.4pt;">2.71 (0.06)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.1.1">
<span class="ltx_p" id="S4.T3.1.3.3.1.1.1" style="width:95.4pt;">Reliance Decision</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.2.1">
<span class="ltx_p" id="S4.T3.1.3.3.2.1.1" style="width:43.4pt;">3.65 (0.11)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.3.1">
<span class="ltx_p" id="S4.T3.1.3.3.3.1.1" style="width:43.4pt;">2.43 (0.10)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.3.4.1">
<span class="ltx_p" id="S4.T3.1.3.3.4.1.1" style="width:43.4pt;">1.60 (0.09)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.1.1">
<span class="ltx_p" id="S4.T3.1.4.4.1.1.1" style="width:95.4pt;">Satisfaction w/ Expl.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.2.1">
<span class="ltx_p" id="S4.T3.1.4.4.2.1.1" style="width:43.4pt;">6.38 (0.06)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.3.1">
<span class="ltx_p" id="S4.T3.1.4.4.3.1.1" style="width:43.4pt;">3.13 (0.06)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.4.4.1">
<span class="ltx_p" id="S4.T3.1.4.4.4.1.1" style="width:43.4pt;">2.20 (0.06)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.5.1.1">
<span class="ltx_p" id="S4.T3.1.5.5.1.1.1" style="width:95.4pt;">Confidence in Driving</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.5.2.1">
<span class="ltx_p" id="S4.T3.1.5.5.2.1.1" style="width:43.4pt;">5.16 (0.06)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.5.3.1">
<span class="ltx_p" id="S4.T3.1.5.5.3.1.1" style="width:43.4pt;">3.68 (0.06)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.5.4.1">
<span class="ltx_p" id="S4.T3.1.5.5.4.1.1" style="width:43.4pt;">2.84 (0.06)</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Overall Impact of Errors on Main Outcomes</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Linear mixed-effects (LME) models were used to measure the effect of errors on each major outcome: comfort relying on the AV, reliance decision, satisfaction with explanation, and confidence in the AV’s driving ability. Though LME models produce similar results to mixed-model ANOVAs, they offer greater flexibility for repeated measures experiments. They also incorporate random effects, which help reduce the likelihood of a Type 1 error <cite class="ltx_cite ltx_citemacro_citep">(Boisgontier and Cheval, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib8" title="">2016</a>)</cite>. Models were made with fixed effects for error condition and random effects for the specific scenario and individual participant. The fixed effects allow us to compare differences between study conditions, while the random effects allow us to control for differences by scenario and individual participant.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">Individual comparisons between each condition (Accurate-Low, Accurate-High, Low-High) show highly significant effects (p ¡ 0.001) across all four outcome measures (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.T4" title="Table 4 ‣ 4.1.2. Overall Impact of Errors on Main Outcomes ‣ 4.1. Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">4</span></a>), implying error condition significantly impacted outcomes. The results for comfort relying on the AV, reliance decision, and satisfaction with the explanation follow the expected trend: we find that outcome scores decrease as error level increases. Surprisingly, we found the same effect on a person’s evaluation of the AV’s driving ability, where confidence scores also decrease as error level increases. This is unexpected, given that the driving performance shown in the videos were identical and only the explanations changed. The implication is that there is a cross-over effect between a person’s evaluation of the explanation and the person’s evaluation of the vehicle’s driving, <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p2.1.1">despite</em> evidence suggesting that the driving performance is consistently high quality.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Comparing Error Conditions for Main Outcomes (LME Models). These show highly significant outcome score differences between each error level.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.3.4.1">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T4.3.4.1.1"></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T4.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.1.2.1">Acc.-Low</span></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T4.3.4.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.1.3.1">Acc.-High</span></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T4.3.4.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.1.4.1">Low-High</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.4"></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.1.1.1.1">
<span class="ltx_p" id="S4.T4.1.1.1.1.1" style="width:21.7pt;"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><mi id="S4.T4.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.1.m1.1d">italic_β</annotation></semantics></math></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.5.1">
<span class="ltx_p" id="S4.T4.3.3.5.1.1" style="width:21.7pt;">df</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.6.1">
<span class="ltx_p" id="S4.T4.3.3.6.1.1" style="width:21.7pt;">t</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.3.3.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.7.1">
<span class="ltx_p" id="S4.T4.3.3.7.1.1" style="width:8.7pt;">sig.</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.2.2.2.1">
<span class="ltx_p" id="S4.T4.2.2.2.1.1" style="width:21.7pt;"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.1.m1.1"><semantics id="S4.T4.2.2.2.1.1.m1.1a"><mi id="S4.T4.2.2.2.1.1.m1.1.1" xref="S4.T4.2.2.2.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.1.m1.1b"><ci id="S4.T4.2.2.2.1.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.1.m1.1d">italic_β</annotation></semantics></math></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.8.1">
<span class="ltx_p" id="S4.T4.3.3.8.1.1" style="width:21.7pt;">df</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.9.1">
<span class="ltx_p" id="S4.T4.3.3.9.1.1" style="width:21.7pt;">t</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.3.3.10">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.10.1">
<span class="ltx_p" id="S4.T4.3.3.10.1.1" style="width:8.7pt;">sig.</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.3.1">
<span class="ltx_p" id="S4.T4.3.3.3.1.1" style="width:21.7pt;"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.1.m1.1"><semantics id="S4.T4.3.3.3.1.1.m1.1a"><mi id="S4.T4.3.3.3.1.1.m1.1.1" xref="S4.T4.3.3.3.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.1.m1.1b"><ci id="S4.T4.3.3.3.1.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.1.m1.1d">italic_β</annotation></semantics></math></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.11">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.11.1">
<span class="ltx_p" id="S4.T4.3.3.11.1.1" style="width:21.7pt;">df</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.12">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.12.1">
<span class="ltx_p" id="S4.T4.3.3.12.1.1" style="width:21.7pt;">t</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.13">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.3.13.1">
<span class="ltx_p" id="S4.T4.3.3.13.1.1" style="width:8.7pt;">sig.</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.3.5.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.1.1">
<span class="ltx_p" id="S4.T4.3.5.1.1.1.1" style="width:86.7pt;">Comfort Relying on AV</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.2.1">
<span class="ltx_p" id="S4.T4.3.5.1.2.1.1" style="width:21.7pt;">-1.08</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.3.1">
<span class="ltx_p" id="S4.T4.3.5.1.3.1.1" style="width:21.7pt;">5114</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.4.1">
<span class="ltx_p" id="S4.T4.3.5.1.4.1.1" style="width:21.7pt;">-17.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T4.3.5.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.5.1">
<span class="ltx_p" id="S4.T4.3.5.1.5.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.6.1">
<span class="ltx_p" id="S4.T4.3.5.1.6.1.1" style="width:21.7pt;">-1.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.7.1">
<span class="ltx_p" id="S4.T4.3.5.1.7.1.1" style="width:21.7pt;">5115</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.8.1">
<span class="ltx_p" id="S4.T4.3.5.1.8.1.1" style="width:21.7pt;">-28.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T4.3.5.1.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.9.1">
<span class="ltx_p" id="S4.T4.3.5.1.9.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.10">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.10.1">
<span class="ltx_p" id="S4.T4.3.5.1.10.1.1" style="width:21.7pt;">-0.75</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.11">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.11.1">
<span class="ltx_p" id="S4.T4.3.5.1.11.1.1" style="width:21.7pt;">5110</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.12">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.12.1">
<span class="ltx_p" id="S4.T4.3.5.1.12.1.1" style="width:21.7pt;">-11.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.3.5.1.13">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.5.1.13.1">
<span class="ltx_p" id="S4.T4.3.5.1.13.1.1" style="width:8.7pt;">***</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.6.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.1.1">
<span class="ltx_p" id="S4.T4.3.6.2.1.1.1" style="width:86.7pt;">Reliance Decision</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.2.1">
<span class="ltx_p" id="S4.T4.3.6.2.2.1.1" style="width:21.7pt;">-1.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.3.1">
<span class="ltx_p" id="S4.T4.3.6.2.3.1.1" style="width:21.7pt;">5114</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.4.1">
<span class="ltx_p" id="S4.T4.3.6.2.4.1.1" style="width:21.7pt;">-10.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T4.3.6.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.5.1">
<span class="ltx_p" id="S4.T4.3.6.2.5.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.6.1">
<span class="ltx_p" id="S4.T4.3.6.2.6.1.1" style="width:21.7pt;">-2.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.7.1">
<span class="ltx_p" id="S4.T4.3.6.2.7.1.1" style="width:21.7pt;">5115</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.8.1">
<span class="ltx_p" id="S4.T4.3.6.2.8.1.1" style="width:21.7pt;">-17.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T4.3.6.2.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.9.1">
<span class="ltx_p" id="S4.T4.3.6.2.9.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.10">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.10.1">
<span class="ltx_p" id="S4.T4.3.6.2.10.1.1" style="width:21.7pt;">-0.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.11">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.11.1">
<span class="ltx_p" id="S4.T4.3.6.2.11.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.12">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.12.1">
<span class="ltx_p" id="S4.T4.3.6.2.12.1.1" style="width:21.7pt;">-6.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.6.2.13">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.6.2.13.1">
<span class="ltx_p" id="S4.T4.3.6.2.13.1.1" style="width:8.7pt;">***</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.7.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.1.1">
<span class="ltx_p" id="S4.T4.3.7.3.1.1.1" style="width:86.7pt;">Satisfaction w/ Expl.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.2.1">
<span class="ltx_p" id="S4.T4.3.7.3.2.1.1" style="width:21.7pt;">-3.24</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.3.1">
<span class="ltx_p" id="S4.T4.3.7.3.3.1.1" style="width:21.7pt;">5116</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.4.1">
<span class="ltx_p" id="S4.T4.3.7.3.4.1.1" style="width:21.7pt;">-45.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T4.3.7.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.5.1">
<span class="ltx_p" id="S4.T4.3.7.3.5.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.6.1">
<span class="ltx_p" id="S4.T4.3.7.3.6.1.1" style="width:21.7pt;">-4.16</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.7.1">
<span class="ltx_p" id="S4.T4.3.7.3.7.1.1" style="width:21.7pt;">5118</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.8.1">
<span class="ltx_p" id="S4.T4.3.7.3.8.1.1" style="width:21.7pt;">-57.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T4.3.7.3.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.9.1">
<span class="ltx_p" id="S4.T4.3.7.3.9.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.10">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.10.1">
<span class="ltx_p" id="S4.T4.3.7.3.10.1.1" style="width:21.7pt;">-0.93</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.11">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.11.1">
<span class="ltx_p" id="S4.T4.3.7.3.11.1.1" style="width:21.7pt;">5111</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.12">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.12.1">
<span class="ltx_p" id="S4.T4.3.7.3.12.1.1" style="width:21.7pt;">-12.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.7.3.13">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.7.3.13.1">
<span class="ltx_p" id="S4.T4.3.7.3.13.1.1" style="width:8.7pt;">***</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.8.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.1.1">
<span class="ltx_p" id="S4.T4.3.8.4.1.1.1" style="width:86.7pt;">Confidence in Driving</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.2.1">
<span class="ltx_p" id="S4.T4.3.8.4.2.1.1" style="width:21.7pt;">-1.46</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.3.1">
<span class="ltx_p" id="S4.T4.3.8.4.3.1.1" style="width:21.7pt;">5114</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.4.1">
<span class="ltx_p" id="S4.T4.3.8.4.4.1.1" style="width:21.7pt;">-25.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T4.3.8.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.5.1">
<span class="ltx_p" id="S4.T4.3.8.4.5.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.6.1">
<span class="ltx_p" id="S4.T4.3.8.4.6.1.1" style="width:21.7pt;">-2.29</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.7.1">
<span class="ltx_p" id="S4.T4.3.8.4.7.1.1" style="width:21.7pt;">5114</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.8.1">
<span class="ltx_p" id="S4.T4.3.8.4.8.1.1" style="width:21.7pt;">-38.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T4.3.8.4.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.9.1">
<span class="ltx_p" id="S4.T4.3.8.4.9.1.1" style="width:8.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.10">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.10.1">
<span class="ltx_p" id="S4.T4.3.8.4.10.1.1" style="width:21.7pt;">-0.83</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.11">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.11.1">
<span class="ltx_p" id="S4.T4.3.8.4.11.1.1" style="width:21.7pt;">5111</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.12">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.12.1">
<span class="ltx_p" id="S4.T4.3.8.4.12.1.1" style="width:21.7pt;">-13.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.3.8.4.13">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.3.8.4.13.1">
<span class="ltx_p" id="S4.T4.3.8.4.13.1.1" style="width:8.7pt;">***</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.9.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="13" id="S4.T4.3.9.5.1"><span class="ltx_text ltx_font_italic" id="S4.T4.3.9.5.1.1">Sig. Codes: ‘***’ p ¡ 0.001 — ‘**’ p ¡ 0.01 — ‘*’ p ¡ 0.05 — ‘.’ trending p ¡ 0.1</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Potential Harm of ‘What’-type Errors</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In the high error condition group, what-type errors are incorrect descriptions of what the AV is doing. To understand the impact of the <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p1.1.1">potential</em> harm of these errors, we conducted a secondary analysis comparing errors that would result in an accident if acted upon by the AV versus those that would not. To test this case, we use LME models with fixed effects for the categorization of potential harm (0 or 1) and random effects for driving scenario and individual differences by participant. This analysis is conducted only on data from the “high” condition in isolation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.3">We find significant effects for satisfaction with an explanation (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p2.1.m1.1"><semantics id="S4.SS1.SSS3.p2.1.m1.1a"><mi id="S4.SS1.SSS3.p2.1.m1.1.1" xref="S4.SS1.SSS3.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.1.m1.1b"><ci id="S4.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p2.1.m1.1d">italic_β</annotation></semantics></math> = -0.46, t(22) = -2.6, p ¡ .05) and confidence in the AV’s driving ability (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p2.2.m2.1"><semantics id="S4.SS1.SSS3.p2.2.m2.1a"><mi id="S4.SS1.SSS3.p2.2.m2.1.1" xref="S4.SS1.SSS3.p2.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.2.m2.1b"><ci id="S4.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p2.2.m2.1d">italic_β</annotation></semantics></math> = -0.45, t(22) = -2.7, p ¡ .05). Comfort relying on the AV showed results trending towards significance (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p2.3.m3.1"><semantics id="S4.SS1.SSS3.p2.3.m3.1a"><mi id="S4.SS1.SSS3.p2.3.m3.1.1" xref="S4.SS1.SSS3.p2.3.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.3.m3.1b"><ci id="S4.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p2.3.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.3.m3.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p2.3.m3.1d">italic_β</annotation></semantics></math> = -0.38, t(22) = -1.9, p = 0.07). No significant results were found for the reliance preference measure. These results imply that the content of the error – in this case, the potential harm that could result <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS3.p2.3.1">from</em> the error – may impact the effect of the error on outcomes. Specifically, we find evidence that higher gravity errors may have a greater negative impact on some outcomes.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Impact of Driving Context: Perceived Harm and Difficulty of Driving</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Relationship Between Harm and Difficulty</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">By examining the impact of harm and difficulty on outcome ratings, we can derive an understanding of how these contextual factors influenced our results. Unsurprisingly, we found a strong, positive relationship between the perceived harm and the difficulty of driving in a particular scenario (r = 0.69, p ¡ .001; Adj R<sup class="ltx_sup" id="S4.SS2.SSS1.p1.1.1">2</sup> = .48). Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.F3" title="Figure 3 ‣ 4.2.1. Relationship Between Harm and Difficulty ‣ 4.2. Impact of Driving Context: Perceived Harm and Difficulty of Driving ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">3</span></a> shows difficulty and harm ratings by scenario.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="311" id="S4.F3.g1" src="extracted/5846160/individualdifficultyharm.png" width="419"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Mean Harm and Difficulty Across All Scenarios. Results show harm and difficulty are generally correlated, but differ depending on the specific scenario. We find that these contextual characteristics directly impacted outcome ratings as well as influenced the relationship between errors and outcomes.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F3.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F3.2">A visualization of the mean Harm and Difficulty scores for all 24 scenarios, depicting the mean Harm and Difficulty ratings for each scenario. Results show harm and difficulty are generally correlated, but differ depending on the specific scenario. We find that these contextual characteristics directly impacted outcome ratings as well as influenced the relationship between errors and outcomes.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Impact of Harm and Difficulty On Main Outcome Ratings</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.5">We assess the impact of harm and difficulty on main outcome ratings using LME models with added fixed effects for the average harm and difficulty of scenarios. We maintain the fixed effect of outcome condition as well as random effects for scenario and participant. Using this model, main effects for difficulty and harm give us an understanding of how these factors impacted outcome judgments <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.5.1">independent</em> of error condition. Estimates (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.1.m1.1"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mi id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><ci id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.1.m1.1d">italic_β</annotation></semantics></math>) are given relative to the “accurate” condition. We find that harm significantly impacted comfort relying on the AV (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.2.m2.1"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mi id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><ci id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.2.m2.1d">italic_β</annotation></semantics></math> = -0.43, t(38) = -3.7, p ¡ .001), reliance decision (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.3.m3.1"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mi id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><ci id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.3.m3.1d">italic_β</annotation></semantics></math> = -0.74, t(62) = -4.4, p ¡ .001), and confidence in the AV’s driving ability (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.4.m4.1"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><mi id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><ci id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.4.m4.1d">italic_β</annotation></semantics></math> = -0.22, t(41) = -2.2, p ¡ .05). No significant effects were found between harm and explanation satisfaction. We found that difficulty impacted only reliance decision (<math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.5.m5.1"><semantics id="S4.SS2.SSS2.p1.5.m5.1a"><mi id="S4.SS2.SSS2.p1.5.m5.1.1" xref="S4.SS2.SSS2.p1.5.m5.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.5.m5.1b"><ci id="S4.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.5.m5.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.5.m5.1d">italic_β</annotation></semantics></math> = 0.60, t(61) = 3.0, p ¡ .01). Interestingly, this was in the opposite direction as harm, where greater difficulty was associated with increased reliance.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">The implication is that contextual factors like harm and difficulty affect interaction with an AV system regardless of error condition, with harm potentially being the more influential factor. Specifically, higher harm is generally associated with lower comfort, reliance, and confidence, while higher difficulty is generally associated with higher reliance ratings. There does not appear to be an overall association between harm or difficulty and explanation satisfaction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Impact Of Harm and Difficulty On Differences Between Error Conditions (Interaction Effects)</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Using a similar LME model, we can assess if difficulty or harm impacted the relationship between error level and outcomes by looking at interaction effects. Specifically, we examine if the <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.1">differences</em> found between each error condition are predicted by a scenario’s average difficulty or harm. In this case, we use the accurate group as a common reference (model intercept) and then compare if the changes of each main outcome from accurate to low and accurate to high vary with respect to difficulty and harm. We find significant interaction effects for several of our outcomes, implying that contextual characteristics like harm and difficulty may be moderating how much of an impact an error may have. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.T5" title="Table 5 ‣ 4.2.3. Impact Of Harm and Difficulty On Differences Between Error Conditions (Interaction Effects) ‣ 4.2. Impact of Driving Context: Perceived Harm and Difficulty of Driving ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">5</span></a> shows interaction effects for harm, and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.T6" title="Table 6 ‣ 4.2.3. Impact Of Harm and Difficulty On Differences Between Error Conditions (Interaction Effects) ‣ 4.2. Impact of Driving Context: Perceived Harm and Difficulty of Driving ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">6</span></a> shows interaction effects for difficulty.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.1.1">Comfort Relying on AV</span> – Starting with comfort relying on the AV, we find that scenario difficulty has more of an impact on comfort in the low error condition than in the accurate condition. Specifically, when difficulty increased, comfort decreased significantly more in the low condition compared to the accurate condition. A similar but opposite effect was found with harm: in the low error condition, comfort increases significantly more with higher harm than in the accurate condition. The implication is that comfort judgments may be more sensitive to the difficulty and harm of the scenario when there are some errors (low condition) compared to when there are no errors (accurate). We do not see difficulty or harm as more impactful on comfort judgments in the high error condition compared to the accurate condition. This implies that these judgments of comfort are likely based on the high magnitude of error (main effect) for the high group, as opposed to being influenced by the difficulty or harm of the driving context (interaction effect) in these cases.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p3.1.1">Reliance Decision</span> – The case is similar for reliance decision, though the effects are trending towards significance as opposed to being statistically significant. We find that scenario difficulty has more of an impact on reliance in the low error condition than it did in the accurate condition. Specifically, when difficulty increased, reliance decreased more in the low condition compared to the accurate condition. For harm in the low error condition, higher harm increases reliance significantly more than in the accurate condition. The implication is that reliance judgments may be more sensitive to the difficulty and harm of the scenario when there are some errors (low condition) compared to when there are no errors (accurate). As with comfort, we do not find difficulty or harm as more impactful on reliance in the high error condition compared to the accurate condition. This implies that judgments of reliance when errors are high are likely based on the error itself rather than difficulty or harm.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p4.1.1">Satisfaction w/ Expl.</span> – The interaction effects change for satisfaction with an explanation. We do not see difficulty or harm as more impactful on satisfaction in the low error condition compared to the accurate condition, implying that differences in judgments of the explanation are likely based on the explanation quality itself without being influenced by the difficulty or harm of the situation in these cases. The same effect is seen when comparing differences between the accurate and high condition groups for harm. Surprisingly, we find that difficulty differentially impacted explanation satisfaction when in the high condition compared to the accurate condition. Specifically, when difficulty increased, satisfaction increased significantly more when in the high condition than when in the accurate condition. These results together imply that, in most cases, judgments of an explanation are likely based on the explanation quality itself, however, when explanation quality is exceptionally poor and driving is difficult, participants may actually be more forgiving with their judgments.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p5">
<p class="ltx_p" id="S4.SS2.SSS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p5.1.1">Confidence in Driving</span> – Lastly, for confidence in the AV’s driving ability, we find a similar trend as comfort and reliance decision. We find that scenario difficulty has more of an impact on confidence in the low error condition than it did in the accurate condition. When difficulty increased, confidence decreased more in the low condition compared to the accurate condition (trending towards significance). For harm in the low error condition, higher harm increases confidence significantly more than in the accurate condition. The implication is that confidence judgments may be more sensitive to the difficulty and harm of the scenario when there are some errors (low condition) compared to when there are no errors (accurate). As with comfort and reliance, we do not find difficulty or harm as more impactful on confidence in the high error condition compared to the accurate condition. This implies that judgments of confidence are more likely based on the main effect of error itself, rather than difficulty or harm.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Impact of Harm on Error Condition Differences (LME Models). These show if Harm influences the <span class="ltx_text ltx_font_italic" id="S4.T5.4.1">impact</span> of an error. We find significant effects for several of our outcomes, implying that Harm may moderate how much of an impact an error may have.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.2.3.1">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T5.2.3.1.1"></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T5.2.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.3.1.2.1">Acc.-Low</span></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T5.2.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.3.1.3.1">Acc.-High</span></th>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.3"></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.1.1.1.1">
<span class="ltx_p" id="S4.T5.1.1.1.1.1" style="width:21.7pt;"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T5.1.1.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><mi id="S4.T5.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.1.1.m1.1d">italic_β</annotation></semantics></math></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.2.4.1">
<span class="ltx_p" id="S4.T5.2.2.4.1.1" style="width:21.7pt;">df</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.2.5.1">
<span class="ltx_p" id="S4.T5.2.2.5.1.1" style="width:21.7pt;">t</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.2.6.1">
<span class="ltx_p" id="S4.T5.2.2.6.1.1" style="width:21.7pt;">sig.</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.2.2.1">
<span class="ltx_p" id="S4.T5.2.2.2.1.1" style="width:21.7pt;"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T5.2.2.2.1.1.m1.1"><semantics id="S4.T5.2.2.2.1.1.m1.1a"><mi id="S4.T5.2.2.2.1.1.m1.1.1" xref="S4.T5.2.2.2.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.1.1.m1.1b"><ci id="S4.T5.2.2.2.1.1.m1.1.1.cmml" xref="S4.T5.2.2.2.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.1.1.m1.1d">italic_β</annotation></semantics></math></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.2.7.1">
<span class="ltx_p" id="S4.T5.2.2.7.1.1" style="width:21.7pt;">df</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.2.8.1">
<span class="ltx_p" id="S4.T5.2.2.8.1.1" style="width:21.7pt;">t</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.2.9.1">
<span class="ltx_p" id="S4.T5.2.2.9.1.1" style="width:21.7pt;">sig.</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.2.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.1.1">
<span class="ltx_p" id="S4.T5.2.4.1.1.1.1" style="width:121.4pt;">Comfort Relying on AV</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.2.1">
<span class="ltx_p" id="S4.T5.2.4.1.2.1.1" style="width:21.7pt;">0.43</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.3.1">
<span class="ltx_p" id="S4.T5.2.4.1.3.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.4.1">
<span class="ltx_p" id="S4.T5.2.4.1.4.1.1" style="width:21.7pt;">4.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T5.2.4.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.5.1">
<span class="ltx_p" id="S4.T5.2.4.1.5.1.1" style="width:21.7pt;">***</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.6.1">
<span class="ltx_p" id="S4.T5.2.4.1.6.1.1" style="width:21.7pt;">0.10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.7.1">
<span class="ltx_p" id="S4.T5.2.4.1.7.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.8.1">
<span class="ltx_p" id="S4.T5.2.4.1.8.1.1" style="width:21.7pt;">0.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.2.4.1.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.4.1.9.1">
<span class="ltx_p" id="S4.T5.2.4.1.9.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.1.1">
<span class="ltx_p" id="S4.T5.2.5.2.1.1.1" style="width:121.4pt;">Reliance Decision</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.2.1">
<span class="ltx_p" id="S4.T5.2.5.2.2.1.1" style="width:21.7pt;">0.35</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.3.1">
<span class="ltx_p" id="S4.T5.2.5.2.3.1.1" style="width:21.7pt;">5106</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.4.1">
<span class="ltx_p" id="S4.T5.2.5.2.4.1.1" style="width:21.7pt;">1.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T5.2.5.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.5.1">
<span class="ltx_p" id="S4.T5.2.5.2.5.1.1" style="width:21.7pt;">.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.6.1">
<span class="ltx_p" id="S4.T5.2.5.2.6.1.1" style="width:21.7pt;">0.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.7.1">
<span class="ltx_p" id="S4.T5.2.5.2.7.1.1" style="width:21.7pt;">5106</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.8.1">
<span class="ltx_p" id="S4.T5.2.5.2.8.1.1" style="width:21.7pt;">1.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.5.2.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.5.2.9.1">
<span class="ltx_p" id="S4.T5.2.5.2.9.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.6.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.1.1">
<span class="ltx_p" id="S4.T5.2.6.3.1.1.1" style="width:121.4pt;">Satisfaction w/ Expl.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.2.1">
<span class="ltx_p" id="S4.T5.2.6.3.2.1.1" style="width:21.7pt;">0.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.3.1">
<span class="ltx_p" id="S4.T5.2.6.3.3.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.4.1">
<span class="ltx_p" id="S4.T5.2.6.3.4.1.1" style="width:21.7pt;">1.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T5.2.6.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.5.1">
<span class="ltx_p" id="S4.T5.2.6.3.5.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.6.1">
<span class="ltx_p" id="S4.T5.2.6.3.6.1.1" style="width:21.7pt;">-0.15</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.7.1">
<span class="ltx_p" id="S4.T5.2.6.3.7.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.8.1">
<span class="ltx_p" id="S4.T5.2.6.3.8.1.1" style="width:21.7pt;">-1.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.6.3.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.6.3.9.1">
<span class="ltx_p" id="S4.T5.2.6.3.9.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.7.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.1.1">
<span class="ltx_p" id="S4.T5.2.7.4.1.1.1" style="width:121.4pt;">Confidence in Driving</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.2.1">
<span class="ltx_p" id="S4.T5.2.7.4.2.1.1" style="width:21.7pt;">0.27</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.3.1">
<span class="ltx_p" id="S4.T5.2.7.4.3.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.4.1">
<span class="ltx_p" id="S4.T5.2.7.4.4.1.1" style="width:21.7pt;">2.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T5.2.7.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.5.1">
<span class="ltx_p" id="S4.T5.2.7.4.5.1.1" style="width:21.7pt;">**</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.6.1">
<span class="ltx_p" id="S4.T5.2.7.4.6.1.1" style="width:21.7pt;">-0.01</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.7.1">
<span class="ltx_p" id="S4.T5.2.7.4.7.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.8.1">
<span class="ltx_p" id="S4.T5.2.7.4.8.1.1" style="width:21.7pt;">-0.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.2.7.4.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.2.7.4.9.1">
<span class="ltx_p" id="S4.T5.2.7.4.9.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.8.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="9" id="S4.T5.2.8.5.1"><span class="ltx_text ltx_font_italic" id="S4.T5.2.8.5.1.1">Sig. Codes: ‘***’ p ¡ 0.001 — ‘**’ p ¡ 0.01 — ‘*’ p ¡ 0.05 — ‘.’ trending p ¡ 0.1</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6. </span>Impact of Difficulty on Error Condition Differences (LME Models). These show if Difficulty influences the <span class="ltx_text ltx_font_italic" id="S4.T6.4.1">impact</span> of an error. We find significant effects for several of our outcomes, implying that Difficulty may moderate how much of an impact an error may have.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T6.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.2.3.1">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T6.2.3.1.1"></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T6.2.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.2.3.1.2.1">Acc.-Low</span></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T6.2.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T6.2.3.1.3.1">Acc.-High</span></th>
</tr>
<tr class="ltx_tr" id="S4.T6.2.2">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.3"></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.1.1.1.1">
<span class="ltx_p" id="S4.T6.1.1.1.1.1" style="width:21.7pt;"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T6.1.1.1.1.1.m1.1"><semantics id="S4.T6.1.1.1.1.1.m1.1a"><mi id="S4.T6.1.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.T6.1.1.1.1.1.m1.1d">italic_β</annotation></semantics></math></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.2.4.1">
<span class="ltx_p" id="S4.T6.2.2.4.1.1" style="width:21.7pt;">df</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.2.5.1">
<span class="ltx_p" id="S4.T6.2.2.5.1.1" style="width:21.7pt;">t</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T6.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.2.6.1">
<span class="ltx_p" id="S4.T6.2.2.6.1.1" style="width:21.7pt;">sig.</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.2.2.1">
<span class="ltx_p" id="S4.T6.2.2.2.1.1" style="width:21.7pt;"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T6.2.2.2.1.1.m1.1"><semantics id="S4.T6.2.2.2.1.1.m1.1a"><mi id="S4.T6.2.2.2.1.1.m1.1.1" xref="S4.T6.2.2.2.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.1.1.m1.1b"><ci id="S4.T6.2.2.2.1.1.m1.1.1.cmml" xref="S4.T6.2.2.2.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.2.2.1.1.m1.1d">italic_β</annotation></semantics></math></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.2.7.1">
<span class="ltx_p" id="S4.T6.2.2.7.1.1" style="width:21.7pt;">df</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.2.8.1">
<span class="ltx_p" id="S4.T6.2.2.8.1.1" style="width:21.7pt;">t</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.2.9.1">
<span class="ltx_p" id="S4.T6.2.2.9.1.1" style="width:21.7pt;">sig.</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.2.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.1.1">
<span class="ltx_p" id="S4.T6.2.4.1.1.1.1" style="width:121.4pt;">Comfort Relying on AV</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.2.1">
<span class="ltx_p" id="S4.T6.2.4.1.2.1.1" style="width:21.7pt;">-0.26</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.3.1">
<span class="ltx_p" id="S4.T6.2.4.1.3.1.1" style="width:21.7pt;">5109</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.4.1">
<span class="ltx_p" id="S4.T6.2.4.1.4.1.1" style="width:21.7pt;">-2.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T6.2.4.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.5.1">
<span class="ltx_p" id="S4.T6.2.4.1.5.1.1" style="width:21.7pt;">*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.6.1">
<span class="ltx_p" id="S4.T6.2.4.1.6.1.1" style="width:21.7pt;">0.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.7.1">
<span class="ltx_p" id="S4.T6.2.4.1.7.1.1" style="width:21.7pt;">5109</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.8.1">
<span class="ltx_p" id="S4.T6.2.4.1.8.1.1" style="width:21.7pt;">0.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T6.2.4.1.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.4.1.9.1">
<span class="ltx_p" id="S4.T6.2.4.1.9.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.1.1">
<span class="ltx_p" id="S4.T6.2.5.2.1.1.1" style="width:121.4pt;">Reliance Decision</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.2.1">
<span class="ltx_p" id="S4.T6.2.5.2.2.1.1" style="width:21.7pt;">-0.42</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.3.1">
<span class="ltx_p" id="S4.T6.2.5.2.3.1.1" style="width:21.7pt;">5107</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.4.1">
<span class="ltx_p" id="S4.T6.2.5.2.4.1.1" style="width:21.7pt;">-1.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T6.2.5.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.5.1">
<span class="ltx_p" id="S4.T6.2.5.2.5.1.1" style="width:21.7pt;">.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.6.1">
<span class="ltx_p" id="S4.T6.2.5.2.6.1.1" style="width:21.7pt;">-0.16</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.7.1">
<span class="ltx_p" id="S4.T6.2.5.2.7.1.1" style="width:21.7pt;">5107</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.8.1">
<span class="ltx_p" id="S4.T6.2.5.2.8.1.1" style="width:21.7pt;">-0.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.5.2.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.5.2.9.1">
<span class="ltx_p" id="S4.T6.2.5.2.9.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.6.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.1.1">
<span class="ltx_p" id="S4.T6.2.6.3.1.1.1" style="width:121.4pt;">Satisfaction w/ Expl.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.2.1">
<span class="ltx_p" id="S4.T6.2.6.3.2.1.1" style="width:21.7pt;">-0.08</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.3.1">
<span class="ltx_p" id="S4.T6.2.6.3.3.1.1" style="width:21.7pt;">5109</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.4.1">
<span class="ltx_p" id="S4.T6.2.6.3.4.1.1" style="width:21.7pt;">-0.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T6.2.6.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.5.1">
<span class="ltx_p" id="S4.T6.2.6.3.5.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.6.1">
<span class="ltx_p" id="S4.T6.2.6.3.6.1.1" style="width:21.7pt;">0.30</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.7.1">
<span class="ltx_p" id="S4.T6.2.6.3.7.1.1" style="width:21.7pt;">5109</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.8.1">
<span class="ltx_p" id="S4.T6.2.6.3.8.1.1" style="width:21.7pt;">2.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.6.3.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.6.3.9.1">
<span class="ltx_p" id="S4.T6.2.6.3.9.1.1" style="width:21.7pt;">*</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.7.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.1.1">
<span class="ltx_p" id="S4.T6.2.7.4.1.1.1" style="width:121.4pt;">Confidence in Driving</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.2.1">
<span class="ltx_p" id="S4.T6.2.7.4.2.1.1" style="width:21.7pt;">-0.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.3.1">
<span class="ltx_p" id="S4.T6.2.7.4.3.1.1" style="width:21.7pt;">5108</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.4.1">
<span class="ltx_p" id="S4.T6.2.7.4.4.1.1" style="width:21.7pt;">-1.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T6.2.7.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.5.1">
<span class="ltx_p" id="S4.T6.2.7.4.5.1.1" style="width:21.7pt;">.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.6.1">
<span class="ltx_p" id="S4.T6.2.7.4.6.1.1" style="width:21.7pt;">0.04</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.7.1">
<span class="ltx_p" id="S4.T6.2.7.4.7.1.1" style="width:21.7pt;">5109</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.8">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.8.1">
<span class="ltx_p" id="S4.T6.2.7.4.8.1.1" style="width:21.7pt;">0.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T6.2.7.4.9">
<span class="ltx_inline-block ltx_align_top" id="S4.T6.2.7.4.9.1">
<span class="ltx_p" id="S4.T6.2.7.4.9.1.1" style="width:21.7pt;">NS</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.8.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="9" id="S4.T6.2.8.5.1"><span class="ltx_text ltx_font_italic" id="S4.T6.2.8.5.1.1">Sig. Codes: ‘***’ p ¡ 0.001 — ‘**’ p ¡ 0.01 — ‘*’ p ¡ 0.05 — ‘.’ trending p ¡ 0.1</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Trust and Expertise</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Impact Of Exposure To Errors On Trust</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We measured participant trust before and after the experiment was completed to see if exposure to repeated AV explanation mistakes affected AV trust levels as a disposition. In general, using a paired-samples t-test, we did not find that exposure to errors in the experiment impacted trust. Only one result was significant pre-post experiment: participants felt that they “understand why an autonomous vehicle makes decisions” worse after the experiment as compared to before (p ¡ 0.001).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Correlations of Initial Trust and Subjective Expertise with Outcomes</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Calculating correlations between initial trust and self-reported expertise level and a participants’ average outcome ratings allows us to assess if trust or expertise can predict how a person will evaluate the AV. We found a strong positive correlation between initial trust and expertise, (r = 0.58, p ¡ 0.001), implying that those who know more about AVs trust them more.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">We found moderate positive correlations between expertise and comfort (r = 0.38, p ¡ 0.001), satisfaction (r = 0.35, p ¡ 0.001), and confidence (r = 0.36, p ¡ 0.001), meaning that those with more expertise reported more positively to these outcomes on average. We found high correlations between initial trust and comfort (r = 0.52, p ¡ 0.001) as well as confidence (r = 0.51, p ¡ 0.001). We found moderate correlations between initial trust and reliance (r = 0.30, p ¡ 0.001) as well as satisfaction (r = 0.49, p ¡ 0.001). These effects are consistent with linear mixed-effects models looking at the main effects of expertise and initial trust on outcomes. Together, these results imply that participants with higher initial trust or expertise tended to rate outcomes higher on average.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Self-Reported Rationale For Trust and Reliance Decisions</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Factors Contributing to Rating Decisions</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">We explicitly inquired upon the basis of reliance decisions by asking participants to rate the relative importance of several factors on their reliance ratings (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#S4.F4" title="Figure 4 ‣ 4.4.1. Factors Contributing to Rating Decisions ‣ 4.4. Self-Reported Rationale For Trust and Reliance Decisions ‣ 4. Results ‣ What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"><span class="ltx_text ltx_ref_tag">4</span></a>). This was done after the rating task concluded. As a whole, we found that the accuracy of the explanation on ‘what’ the AV is doing was the most important consideration, followed by the explanation accuracy of ‘why’ the AV is doing it, the harm of the situation, the AV’s driving ability, and the difficulty of the driving situation. Prior scenes viewed and prior knowledge did not have much reported impact, though this is unsurprising, as these implicit judgments may be difficult for participants to self-describe. Many of these differences were significant based on ANOVA. Notably, harm was significantly more important than difficulty, while explanation accuracy (both ‘what’ and ‘why’ components) were significantly more important than AV driving ability. This latter effect may be in part due to the salience of these factors based on the study’s manipulation. We did not see significant differences between the importance of ‘what’ and ‘why’ accuracy.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="314" id="S4.F4.g1" src="extracted/5846160/post_eval_plot.png" width="419"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Relative Importance of Factors on Reliance Decisions. This shows the mean importance rating of each factor with respect to a person’s reliance decision. Results show many of the factors of interest in this study were of high importance.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F4.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F4.2">A visualization of relative importance (mean rating) of seven factors on reliance decisions. Results show many of the factors of interest in this study were of high importance.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Feedback from Participants</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Participants were given the opportunity to express their general views on autonomous vehicles on how explanation errors impacted their decision-making specifically. Unsurprisingly, many participants reflected general concerns with AVs, including doubt that they can <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.1">“adapt to any circumstance … [causing] a bigger accident.”</span> One participant commented that <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.2">“after a lifetime of driving myself, I’m not sure if I feel comfortable giving over control to a computer.”</span> Desire for control was brought up multiple times. The common sentiment was that giving control to AVs is considered risky.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">Regarding exposure to explanation errors, one participant commented that they were <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.1">“less confident in the reliability of [autonomous] vehicles”</span> after exposure to errors, while another commented that when explanations <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.2">“were wrong, [it made] my confidence in the system shaky.”</span> These comments reiterate our general study findings on the negative impact of errors on reliance, as well as the holistic judgment of an AV’s ability in general. Some participants broke their decision-making down in terms of individual factor priority. For instance, <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.1.3">“if the AV described the action it was taking incorrectly I was a lot less confident and consistently chose to take control myself … if the reason was incorrect, it still impacted my confidence level but not as much.”</span> This reflects the general trend found in our individual factor analysis suggesting ‘what’ information may be more important than ‘why’ rationale, even if this trend was not found to be statically significant.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p3">
<p class="ltx_p" id="S4.SS4.SSS2.p3.1">Regarding the impact of context, a participant noted, <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p3.1.1">“I chose to take control myself in higher risk situations even if the AV did a good job navigating.”</span> This supports this study’s overall findings on the importance of contextual factors like harm on decision-making.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This study aimed to assess the impact of autonomous vehicle (AV) explanation errors and driving context characteristics on participant comfort relying on an AV, preference to rely on the AV instead of taking control themselves, satisfaction with the explanation, and confidence in the AV’s driving ability. Using a mixed-methods approach with a heterogeneous sample of participants (n = 232), we found that explanation errors and contextual characteristics like driving difficulty and perceived harm have a large impact on how a person may think, feel, and behave towards AVs. Echoing prior work by <cite class="ltx_cite ltx_citemacro_citet">Nourani et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib56" title="">2020</a>)</cite>, important consideration must also be given to personal factors like initial trust and expertise, which may further impact how a person interacts with the system. Our results provide insight into how to design effective human-AV interactions and interactions with AI-based systems more generally. We discuss key findings and their implications for future AV design in these contexts.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Autonomous vehicle (AV) explanation errors had a detrimental effect on all outcomes.</span> Our results suggest that errors significantly reduced participant comfort relying on an AV, preference to rely on the AV instead of taking control themselves, and satisfaction with the explanation. These effects are unsurprising, as we would expect outcomes related to a person’s reliance decision or satisfaction with an explanation to reflect the system’s performance <cite class="ltx_cite ltx_citemacro_citep">(Hoff and Bashir, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib28" title="">2015</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">We were surprised, however, to find crossover effects between the <em class="ltx_emph ltx_font_italic" id="S5.p3.1.1">explanatory</em> performance of the AV and a person’s confidence in the AV’s <em class="ltx_emph ltx_font_italic" id="S5.p3.1.2">driving ability</em>, given the AV’s actual demonstrated driving performance remained consistent across all conditions. This crossover effect alludes to the mental model of potential AV users, where evaluation of the explanatory performance of the vehicle and the driving performance of the vehicle are connected. Though connecting explanatory and driving performance may be an intuitive and correct assumption, empirically observing this crossover provides evidence supporting the importance of high quality explanatory interfaces, particularly within safety-critical systems. For the case of AVs, this means that – even if the AV’s driving performance is perfect – if the explanations produced by the AV are not accurate as well, people may still refuse to adopt AV technology. This insight likely generalizes to explanatory communications for other AI-based systems.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">The negative impact of errors increased with error magnitude and the potential harm of the error.</span> Expanding on the effect of errors observed in our study, we find that the impact was commensurate with the magnitude of the errors presented. In the study presented here, ‘what’ and ’why’ errors (high condition) in combination had worse outcomes than ‘why’-only errors (low condition). On the surface, this is unsurprising, as more errors demonstrates lower system performance, and these results could simply be the cumulative effects of seeing errors on two parts of the explanation instead of just one.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">There remains a high probability that ‘what’ and ‘why’ information errors play distinct roles in the evaluation of the system’s performance, however. This would align with past work showing how description of action and description of justification may differentially impact the way a person interacts with autonomous systems <cite class="ltx_cite ltx_citemacro_citep">(Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib42" title="">2015</a>; Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib35" title="">2024a</a>; Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib52" title="">2019</a>)</cite>. When asked explicitly about the factors that contributed to their reliance decisions, participants reported that explanation accuracy (both ‘what’ and ‘why’ components) were both important. Qualitative assessment of participant feedback, however, suggests that ‘what’ information may be more important for reliance outcomes than ‘why’ rationale. This may be due to the potential harm that improper action (‘what’) can have on driving safety, in contrast to improper justification (‘why’). Concretely, a car incorrectly turning right into traffic can cause physical harm, but incorrectly justifying <em class="ltx_emph ltx_font_italic" id="S5.p5.1.1">why</em> it is turning right can not. We note that <em class="ltx_emph ltx_font_italic" id="S5.p5.1.2">conclusively</em> differentiating the impact of ‘what’ from ‘why’ errors independently will be left for future work.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">Supporting the hypothesis that the <em class="ltx_emph ltx_font_italic" id="S5.p6.1.1">implications</em> of an error matter in addition to the mere <em class="ltx_emph ltx_font_italic" id="S5.p6.1.2">presence</em> of an error, we find that ‘what’ errors – with more dire implications for potential harm – had a greater negative impact on comfort, confidence, and explanation satisfaction. Specifically, in cases when ‘what’ errors would have resulted in vehicle crashes if the AV had acted upon them, people were far less comfortable relying on the AV, had less confidence in the AV’s driving, and preferred the explanation less. We posit that the only reason we did not see differences in reliance preference as well was because the presence of an error altogether already reduced reliance preference to near-minimum levels.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">Together, these results suggest that the implications of an explanation error in terms of what they may mean about the system’s performance and consequences of harm, affect how a person thinks, feels, and behaves with an AV. Implications may be implicitly calculated based on the amount and type of error, as well as how these intersect with the external driving context.</p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1"><span class="ltx_text ltx_font_bold" id="S5.p8.1.1">Even with accurate explanations, Comfort, Reliance Preference, and Confidence in the AV’s driving ability were fairly low.</span> In the present study, we were primarily interested in the relative differences in outcome ratings based on errors, contextual factors, or individual differences like trust or expertise level. We note, however, that even in cases when the explanation presented was accurate, comfort, reliance, and confidence scores still hovered near the middle of the provided scale. Though the middling scores may be reflective of the overall challenging nature of the driving situations presented in the study, even the most straightforward of the scenarios presented did not have a reliance preference score greater than mid-range. These results reflect the trend found in a plethora of prior work supporting the premise that people do not generally trust or wish to adopt autonomous vehicles.</p>
</div>
<div class="ltx_para" id="S5.p9">
<p class="ltx_p" id="S5.p9.1"><span class="ltx_text ltx_font_bold" id="S5.p9.1.1">Contextual factors like the difficulty of driving in a specific situation and the perceived harm of that situation directly affect how people think, feel, and prefer to behave with AVs.</span> We found evidence that driving difficulty and perceived harm directly influenced outcomes regardless of error condition. Specifically, higher contextual harm was associated with lower comfort, reliance, and confidence ratings, while higher difficulty was associated with higher reliance ratings. We did not find direct associations between harm or difficulty and explanation satisfaction.</p>
</div>
<div class="ltx_para" id="S5.p10">
<p class="ltx_p" id="S5.p10.1">We attribute the negative impact of harm to the common concern that AVs may malfunction, and a malfunction in a more harmful driving situation may have more severe implications. This reflects the general sentiment that many people think <em class="ltx_emph ltx_font_italic" id="S5.p10.1.1">they</em> can drive better than an AV in most situations. The seemingly positive impact of difficulty on reliance may reflect that, in very high difficulty situations, people lack the confidence that they could perform better than the AV and, as such, prefer to rely on the AV in these cases. Indeed, the driving demonstrated by the AV in even the most difficult driving situations presented in the study was high quality.</p>
</div>
<div class="ltx_para" id="S5.p11">
<p class="ltx_p" id="S5.p11.1">Explicitly, participants reported contextual harm as significantly more important than driving difficulty for their reliance decision. From a theoretical lens, a person’s concern with difficult driving may stem from the increased likelihood of harm in challenging situations, particularly when they may have concerns with an AV’s ability to perform. This leads us to hypothesize that an overall evaluation of potential for harm is the underlying factor influencing reliance decisions overall. By this lens, difficulty and AV driving performance matter <em class="ltx_emph ltx_font_italic" id="S5.p11.1.1">because</em> harm matters. Thus, we hypothesize that harm itself may be the underlying driver of reliance decisions overall.</p>
</div>
<div class="ltx_para" id="S5.p12">
<p class="ltx_p" id="S5.p12.1">A person’s decision to rely on an AV (and other outcomes) may be a function of the demonstrated performance of the vehicle – compounded by an implicit evaluation of the overall harm of the situation (in part as a function of difficulty) – weighted against that person’s confidence in their own ability to perform better. This is similar to the model proposed by <cite class="ltx_cite ltx_citemacro_citet">Hoff and Bashir (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib28" title="">2015</a>)</cite>, with added details on the nested relationship between driving difficulty and harm.</p>
</div>
<div class="ltx_para" id="S5.p13">
<p class="ltx_p" id="S5.p13.1">The general implication of these findings is that there is a complex relationship between contextual factors like harm and difficulty, and outcomes like comfort, reliance, and confidence. It is clear that models of AV behavior incorporating internal and external characteristics of the driving situation can bring further insight into how people will interact with AVs <cite class="ltx_cite ltx_citemacro_citep">(Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib37" title="">2024b</a>)</cite>, and future research should examine how these can be best supported by explanatory communication.</p>
</div>
<div class="ltx_para" id="S5.p14">
<p class="ltx_p" id="S5.p14.1"><span class="ltx_text ltx_font_bold" id="S5.p14.1.1">Contextual factors moderate the relationship between errors and outcomes in complex, sometimes counter-intuitive ways.</span> By examining the interaction between contextual characteristics and the effects of error conditions, we found that – in some cases – the driving context may influence the <em class="ltx_emph ltx_font_italic" id="S5.p14.1.2">amount</em> of impact an error has. The general trend we observed for comfort relying on the AV, reliance preference, and confidence in the AV’s driving ability was that when difficulty increased, outcome scores decreased <em class="ltx_emph ltx_font_italic" id="S5.p14.1.3">more</em> in the low error condition compared to the accurate condition, with no differences seen between the accurate and high conditions. The same but opposite effect was found for harm: higher harm increased outcomes effect in the low condition more than the accurate condition, with no difference between the accurate and high condition.</p>
</div>
<div class="ltx_para" id="S5.p15">
<p class="ltx_p" id="S5.p15.1">The lack of difference found in the way difficulty and harm moderate the relationship between the accurate and high conditions implies that differences in these judgments are likely based solely on the error level as opposed to difficulty or harm of the driving context. Concretely, the effect of error likely overrides the effect of context when errors are at the extremes. When errors are in the middle – such as for the low error condition – the impact of the error appears to be more strongly moderated by the context. This may be because the ramifications of ‘why’-type errors are not strong enough to drive effects fully based on the presence of the error, and instead the amount of change that the error can cause is based on how dire the implications may be in a particular context. The direction of effect, in this case, is similar to the main effects explained previously.</p>
</div>
<div class="ltx_para" id="S5.p16">
<p class="ltx_p" id="S5.p16.1">For explanation satisfaction, we do not see difficulty or harm as more impactful on satisfaction in the low error condition, if we compare this to the accurate condition. This implies that differences in satisfaction are likely based on the explanation quality itself in these cases. The same effect is seen between the accurate and high condition for harm, but not for difficulty. Surprisingly, we found that when difficulty increased, satisfaction increased significantly more when in the high condition than when in the accurate condition. Together, these results imply that, in most cases, judgments of an explanation are based primarily on the explanation quality itself, however, when explanation quality is especially poor and driving is difficult, participants may actually be more forgiving of the AV.</p>
</div>
<div class="ltx_para" id="S5.p17">
<p class="ltx_p" id="S5.p17.1">Taken together, results from this analysis on interaction effects illustrate a nuanced and often complex relationship between driving difficulty, perceived harm and error condition when predicting our main outcomes. This nuance alludes to a prioritization of how different aspects of the driving situation – including harm, difficulty, and explanation errors – combine to form judgments of comfort in the AV, reliance preference, satisfaction with an explanation, and confidence in an AV’s driving ability.</p>
</div>
<div class="ltx_para" id="S5.p18">
<p class="ltx_p" id="S5.p18.1"><span class="ltx_text ltx_font_bold" id="S5.p18.1.1">Trust and expertise influenced how people think, feel, and behave towards the AV.</span> As discussed, how a person thinks and behaves towards AVs is not just a function of their external environment. We find evidence that internal characteristics, such as a participant’s trust and expertise, may also have played a role in the study’s results. Participants with higher initial trust or expertise tended to rate study outcomes higher on average. We also found that those with higher AV expertise trusted AVs more in general. As with the function on driving reliance described previously, these findings also align with Hoff and Bashir’s model of trust with AI systems, where a person’s prior experience impacts their reliance with the system <cite class="ltx_cite ltx_citemacro_citep">(Hoff and Bashir, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib28" title="">2015</a>)</cite>. Those with higher expertise may also have other underlying characteristics, like an affinity for new technologies, which may have influenced their trust. We attribute the finding that priors were not reported as explicitly important for reliance decisions in our post-evaluation to the common difficulty articulating the impact of implicit dispositions.</p>
</div>
<div class="ltx_para" id="S5.p19">
<p class="ltx_p" id="S5.p19.1">Though trust may be influential, we did not find evidence that exposure to AV mistakes due to the study’s manipulation had a large effect on a person’s overall trust level. A single exception was that participants reported less understanding of why AVs make decisions once the study concluded. This makes sense given the inconsistent explanations for behavior they had received, which may have impacted a person’s mental model of AV decision-making.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Implications For Autonomous Vehicle Design and Research</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Our results have important implications for future AV design and research. Understanding the consequences of AV explanation errors and contextual characteristics like driving difficulty and perceived harm are necessary first steps towards designing vehicles which may be more trustworthy, reliable, and satisfactory for people interacting with the AV. In this section, we will discuss how our study’s insights can be used to design more trustworthy AVs.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">The foremost implication of this work is to emphasize the importance of designing systems that can produce accurate explanations for AV decisions. This implication is not too surprising, as why would designers ever <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.1">intentionally</em> produce inaccurate explanations? It becomes more meaningful, however, when testing explanatory systems before deployment. Our results indicate that even with a well-functioning driving system, if explanations are not of high quality, people still won’t want to use the AV.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">The crossover between a person’s evaluation of <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.1.1">explanation</em> performance and <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.1.2">driving</em> performance may be of particular interest to AV designers. If this crossover is a concern – as it should be in the case of deployment – the most obvious solution would be to work on improving explanations so that they are on par with driving ability. In the meantime, specific UI features may be implemented to help users separate their evaluations of explanations from driving performance, such as by presenting distinct indicators for each system reassuring a rider that even if the explanation is messed up, the car can still drive sufficiently. This may be challenging or misleading, however, particularly if the explanatory system and driving system are indeed connected.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Though not tested in the present study, our results also have implications for <em class="ltx_emph ltx_font_italic" id="S5.SS1.p4.1.1">conditionally</em> automated vehicles – those which may require human takeover in difficult or computationally complex driving situations (SAE Level 3) <cite class="ltx_cite ltx_citemacro_citep">(Inagaki and Sheridan, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib30" title="">2019</a>; Ayoub et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib4" title="">2021a</a>)</cite>. There is the possibility that explanation errors produced in conditional contexts may result in inappropriate behaviors by human passengers. If users lose trust in the AV due to explanation errors, for example, they may react unpredictably or take over control at inappropriate times, potentially leading to accidents.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">For both the fully autonomous and conditionally autonomous cases, our results indicate that explanatory systems should be deployed with confidence in their accuracy or potentially not at all. Future work can estimate the exact outcome differences between no explanation and inaccurate explanations, but combining the results of our study and prior findings on the importance of AI explainability by <cite class="ltx_cite ltx_citemacro_citet">Gunning and Aha (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib26" title="">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Miller (<a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib52" title="">2019</a>)</cite> suggests that <em class="ltx_emph ltx_font_italic" id="S5.SS1.p5.1.1">accurate</em> explanations should be opted for whenever possible, and deploying untested or inconsistent systems can be disastrous.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">In terms of design priority, our work suggests that if explanation designers are going to focus on improving a single aspect of the explanation, they should optimize for accurate ‘what’ information before ‘why’ justification. Though both may be necessary in the long run, providing a correct description of behavior may produce a higher value return on effort for teams looking to find a place to start. Just as they have for differentiating the effects of accurate ‘what’ and ‘why’ explanations of driving behavior, future work should delineate the potentially different impacts ‘why’ and ‘what’ <em class="ltx_emph ltx_font_italic" id="S5.SS1.p6.1.1">errors</em> may have from each other in isolation. This can allow more detailed and conclusive conclusions on their relative importance to outcomes such as those in the present study.</p>
</div>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1">It is important to note that, while explanation satisfaction scores were generally higher than other outcome scores in the accurate condition, explanations were still nowhere near ceiling level. This means that there are likely ways in which our explanations could have improved. Prior work has emphasized outcome differences based on informational content, modality (such as including visualizations or haptic feedback), timing, or interactivity <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib52" title="">2019</a>; Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib35" title="">2024a</a>; Avetisyan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib3" title="">2022</a>; Goldman and Bustin, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib25" title="">2022</a>)</cite>. In this regard, future AV research and design teams should continue iterating on these aspects of explanation design and human-machine interface (HMI) development in order to fine-tune explainable systems.</p>
</div>
<div class="ltx_para" id="S5.SS1.p8">
<p class="ltx_p" id="S5.SS1.p8.1">Another major implication of this work is to orient AV designers to the potentially different outcomes that may result from interactions by people with different traits or while operating in driving environments. Specifically, context-aware or personalized explanations may be necessary in cases where the driving difficulty or perceived harm are classified as greater, or for people with little expertise or initial trust. It is likely that designers will need to focus on limitations on cognitive resources like attention and load, particularly for high difficulty scenarios. Other personal traits like risk preferences or personality may also be a basis for personalization <cite class="ltx_cite ltx_citemacro_citep">(Böckle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib7" title="">2021</a>)</cite>. Segmenting populations or isolating particular driving conditions to test future explanation designs would be an effective method to figure out how to meet the differing needs of diverse user groups <cite class="ltx_cite ltx_citemacro_citep">(Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05731v2#bib.bib37" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p9">
<p class="ltx_p" id="S5.SS1.p9.1">Taken in full, our results provide insight into the impact of explanation errors, and provide direction for future AV researchers and designers to improve human-AV communication. Given the serious negative repercussions that may result from errors, our results also provide a foundation for the creation of ethical or regulatory guidelines which dictate the testing and accuracy of AV explanations before they can be deployed to real-world consumers.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Limitations and Future Study</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">As with any study, this research is not without limitations. First, though study findings were generally robust and fit within logical narratives, there is the possibility that findings resulting from online data collection and based on simulations may not generalize to real-world attitudes or behavior. A large effort was put into making driving simulations as realistic as possible, however, we could not test the effect of explanation errors on real roads out of concern for participant safety. Future work should seek to test the impact of errors in a real-world environment. A similar concern may be found for study outcomes: though results on reported reliance or comfort may be clear in a research context, there is no guarantee that explicit declarations of thought or behavior will remain consistent when immersed in a real-world driving context with real consequences of bodily or financial harm. This is a concern for all studies which rely on survey measures or explicit participant statements as a proxy for real-world behavior. It is possible that seeing each scenario three times (with different explanations) may have impacted the ratings provided. This could be mitigated in the future by showing participants only one video per scenario, randomized by error condition. Finally, we examined proximal explanations for action (<span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">“braking”</span>) and cause of action (<span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">“… a pedestrian is crossing the road.”</span>) presented in written fashion and auditory fashion. Future work can examine explanations of distal causes (“pedestrian in the road … because there is an obstacle on the sidewalk”) which could provide additional context for <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.3">why</em> a driving situation is happening in the first place. These could be explained using potentially different modalities of presentation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In a simulated driving study with 232 participants we tested how autonomous vehicle (AV) explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) impact four driving perception and behavior-related outcomes: comfort relying on an AV, preference to rely on the AV instead of taking control themselves, satisfaction with the explanation, and confidence in the AV’s driving ability.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our results indicate that explanation errors, contextual characteristics, and personal traits have a large impact on how a person may think, feel, and behave towards AVs. Explanation errors negatively affected all outcomes. Surprisingly, this included reduced ratings of the AV’s driving ability, despite driving performance remaining constant. The negative impact of errors increased with error magnitude and the potential harm of the error, providing evidence that <em class="ltx_emph ltx_font_italic" id="S6.p2.1.1">implications</em> of an error matter in addition to the mere <em class="ltx_emph ltx_font_italic" id="S6.p2.1.2">presence</em> of an error. Harm and driving difficulty directly impacted outcomes as well as moderated the relationship between errors and outcomes. In general, harm was associated with lower comfort, reliance, and confidence ratings, while driving difficulty was associated with higher reliance ratings. Overall harm was the more important contextual factor of consideration. In terms of a decision function for AV reliance, perceived harm – influenced by driving difficulty – may underlie the evaluation between a person’s confidence in the AV’s performance compared to their own ability. We found that individuals with higher expertise tended to trust AVs more, and these each correlated with more positive outcome ratings in turn.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Overall, our results emphasize the need for accurate and contextually adaptive AV explanations to foster trust, reliance, satisfaction, and confidence. Understanding the ramifications of explanation errors can help future AV research and design teams prioritize design and better understand the impacts of their design choices. They also provide a foundation for context-aware design, personalized explanation interfaces, and potential ethical or regulatory guidelines for the deployment of explainable AI (XAI) systems for autonomous vehicles that are safe and trustworthy.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We would like to thank S. Sapre, R. Bhide, C. Lee, J. Molina, and E. Lee for simulator development and testing.

</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Araujo et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Theo Araujo, Natali Helberger, Sanne Kruikemeier, and Claes H De Vreese. 2020.

</span>
<span class="ltx_bibblock">In AI we trust? Perceptions about automated decision-making by artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">AI &amp; society</em> 35, 3 (2020), 611–623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Avetisyan et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Lilit Avetisyan, Jackie Ayoub, and Feng Zhou. 2022.

</span>
<span class="ltx_bibblock">Investigating explanations in conditional and highly automated driving: The effects of situation awareness and modality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Transportation research part F: traffic psychology and behaviour</em> 89 (2022), 456–466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ayoub et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jackie Ayoub, Lilit Avetisyan, Mustapha Makki, and Feng Zhou. 2021a.

</span>
<span class="ltx_bibblock">An investigation of drivers’ dynamic situational trust in conditionally automated driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">IEEE Transactions on Human-Machine Systems</em> 52, 3 (2021), 501–511.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ayoub et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Jackie Ayoub, X Jessie Yang, and Feng Zhou. 2021b.

</span>
<span class="ltx_bibblock">Modeling dispositional and initial learned trust in automated vehicles with predictability and explainability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Transportation research part F: traffic psychology and behaviour</em> 77 (2021), 102–116.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bedué and Fritzsche (2022)</span>
<span class="ltx_bibblock">
Patrick Bedué and Albrecht Fritzsche. 2022.

</span>
<span class="ltx_bibblock">Can we trust AI? an empirical investigation of trust requirements and guide to successful AI adoption.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Journal of Enterprise Information Management</em> 35, 2 (2022), 530–549.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Böckle et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Martin Böckle, Kwaku Yeboah-Antwi, and Iana Kouris. 2021.

</span>
<span class="ltx_bibblock">Can you trust the black box? The effect of personality traits on trust in AI-enabled user interfaces. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">International Conference on Human-Computer Interaction</em>. Springer, 3–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boisgontier and Cheval (2016)</span>
<span class="ltx_bibblock">
Matthieu P Boisgontier and Boris Cheval. 2016.

</span>
<span class="ltx_bibblock">The anova to mixed model transition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Neuroscience &amp; Biobehavioral Reviews</em> 68 (2016), 1004–1005.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cabitza et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Federico Cabitza, Caterina Fregosi, Andrea Campagner, and Chiara Natali. 2024.

</span>
<span class="ltx_bibblock">Explanations Considered Harmful: The Impact of Misleading Explanations on Accuracy in Hybrid Human-AI Decision Making. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">World Conference on Explainable Artificial Intelligence</em>. Springer, 255–269.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Capallera et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Marine Capallera, Leonardo Angelini, Quentin Meteier, Omar Abou Khaled, and Elena Mugellini. 2022.

</span>
<span class="ltx_bibblock">Human-vehicle interaction to support driver’s situation awareness in automated vehicles: a systematic review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">IEEE Transactions on intelligent vehicles</em> 8, 3 (2022), 2551–2567.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Chun-Cheng Chang, Jaka Sodnik, and Linda Ng Boyle. 2016.

</span>
<span class="ltx_bibblock">Don’t speak and drive: cognitive workload of in-vehicle speech interactions. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Adjunct Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications</em>. 99–104.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi and Ji (2015)</span>
<span class="ltx_bibblock">
Jong Kyu Choi and Yong Gu Ji. 2015.

</span>
<span class="ltx_bibblock">Investigating the importance of trust on adopting an autonomous vehicle.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Journal of Human-Computer Interaction</em> 31, 10 (2015), 692–702.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Colley et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mark Colley, Benjamin Eder, Jan Ole Rixen, and Enrico Rukzio. 2021.

</span>
<span class="ltx_bibblock">Effects of semantic segmentation visualization on trust, situation awareness, and cognitive load in highly automated vehicles. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 2021 CHI conference on human factors in computing systems</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Currano et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Rebecca Currano, So Yeon Park, Dylan James Moore, Kent Lyons, and David Sirkin. 2021.

</span>
<span class="ltx_bibblock">Little road driving hud: Heads-up display complexity influences drivers’ perceptions of automated vehicles. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 2021 CHI conference on human factors in computing systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dastin (2018)</span>
<span class="ltx_bibblock">
Jeffrey Dastin. 2018.

</span>
<span class="ltx_bibblock">Amazon scraps secret AI recruiting tool that showed bias against women.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Reuters</em> (October 10 2018).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG" title="">https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Salis et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Emmanuel De Salis, Marine Capallera, Quentin Meteier, Leonardo Angelini, Omar Abou Khaled, Elena Mugellini, Marino Widmer, and Stefano Carrino. 2020.

</span>
<span class="ltx_bibblock">Designing an AI-companion to support the driver in highly autonomous cars. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Human-Computer Interaction. Multimodal and Natural Interaction: Thematic Area, HCI 2020, Held as Part of the 22nd International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part II 22</em>. Springer, 335–349.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Campli San Vito et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrizia Di Campli San Vito, Edward Brown, Stephen Brewster, Frank Pollick, Simon Thompson, Lee Skrypchuk, and Alexandros Mouzakitis. 2020.

</span>
<span class="ltx_bibblock">Haptic feedback for the transfer of control in autonomous vehicles. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications</em>. 34–37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017.

</span>
<span class="ltx_bibblock">CARLA: An open urban driving simulator. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Conference on robot learning</em>. PMLR, 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ehsan et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Upol Ehsan, Samir Passi, Q Vera Liao, Larry Chan, I Lee, Michael Muller, Mark O Riedl, et al<span class="ltx_text" id="bib.bib19.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">The who in explainable ai: How ai background shapes perceptions of ai explanations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.4.1">arXiv preprint arXiv:2107.13509</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ehsan and Riedl (2020)</span>
<span class="ltx_bibblock">
Upol Ehsan and Mark O Riedl. 2020.

</span>
<span class="ltx_bibblock">Human-centered explainable ai: Towards a reflective sociotechnical approach. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">HCI International 2020-Late Breaking Papers: Multimodality and Intelligence: 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings 22</em>. Springer, 449–466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Endsley (2018)</span>
<span class="ltx_bibblock">
Mica R Endsley. 2018.

</span>
<span class="ltx_bibblock">Situation awareness in future autonomous vehicles: Beware of the unexpected. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Congress of the International Ergonomics Association</em>. Springer, 303–309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fagnant and Kockelman (2015)</span>
<span class="ltx_bibblock">
Daniel J Fagnant and Kara Kockelman. 2015.

</span>
<span class="ltx_bibblock">Preparing a nation for autonomous vehicles: opportunities, barriers and policy recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Transportation Research Part A: Policy and Practice</em> 77 (2015), 167–181.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fauzi et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fauzi Fauzi, Laros Tuhuteru, Ferdinandus Sampe, Abu Muna Almaududi Ausat, and Heliza Rahmania Hatta. 2023.

</span>
<span class="ltx_bibblock">Analysing the role of ChatGPT in improving student productivity in higher education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Journal on Education</em> 5, 4 (2023), 14886–14891.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Favarò et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Francesca M Favarò, Nazanin Nader, Sky O Eurich, Michelle Tripp, and Naresh Varadaraju. 2017.

</span>
<span class="ltx_bibblock">Examining accident reports involving autonomous vehicles in California.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">PLoS one</em> 12, 9 (2017), e0184952.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldman and Bustin (2022)</span>
<span class="ltx_bibblock">
Claudia V Goldman and Ronit Bustin. 2022.

</span>
<span class="ltx_bibblock">Trusting explainable autonomous driving: Simulated studies. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">2022 IEEE Intelligent Vehicles Symposium (IV)</em>. IEEE, 1255–1260.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunning and Aha (2019)</span>
<span class="ltx_bibblock">
David Gunning and David Aha. 2019.

</span>
<span class="ltx_bibblock">DARPA’s explainable artificial intelligence (XAI) program.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">AI magazine</em> 40, 2 (2019), 44–58.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Taehyun Ha, Sangyeon Kim, Donghak Seo, and Sangwon Lee. 2020.

</span>
<span class="ltx_bibblock">Effects of explanation types and perceived risk on trust in autonomous vehicles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Transportation research part F: traffic psychology and behaviour</em> 73 (2020), 271–280.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoff and Bashir (2015)</span>
<span class="ltx_bibblock">
Kevin Anthony Hoff and Masooda Bashir. 2015.

</span>
<span class="ltx_bibblock">Trust in automation: Integrating empirical evidence on factors that influence trust.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Human factors</em> 57, 3 (2015), 407–434.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holzinger et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Andreas Holzinger, Georg Langs, Helmut Denk, Kurt Zatloukal, and Heimo Müller. 2019.

</span>
<span class="ltx_bibblock">Causability and explainability of artificial intelligence in medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em> 9, 4 (2019), e1312.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inagaki and Sheridan (2019)</span>
<span class="ltx_bibblock">
Toshiyuki Inagaki and Thomas B Sheridan. 2019.

</span>
<span class="ltx_bibblock">A critique of the SAE conditional driving automation definition, and analyses of options for improvement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Cognition, technology &amp; work</em> 21 (2019), 569–578.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inc. (2023)</span>
<span class="ltx_bibblock">
Amazon Web Services Inc. 2023.

</span>
<span class="ltx_bibblock">AWS Amazon Polly.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/polly/" title="">https://aws.amazon.com/polly/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inc. ([n. d.])</span>
<span class="ltx_bibblock">
Tesla Inc. [n. d.].

</span>
<span class="ltx_bibblock">Tesla Model Y Owner’s Manual.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tesla.com/ownersmanual/modely/en_us/GUID-2CB60804-9CEA-4F4B-8B04-09B991368DC5.html" title="">https://www.tesla.com/ownersmanual/modely/en_us/GUID-2CB60804-9CEA-4F4B-8B04-09B991368DC5.html</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-07-22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeon et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Myounghoon Jeon, Benjamin K Davison, Michael A Nees, Jeff Wilson, and Bruce N Walker. 2009.

</span>
<span class="ltx_bibblock">Enhanced auditory menu cues improve dual task performance and are preferred with in-vehicle technologies. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 1st international conference on automotive user interfaces and interactive vehicular applications</em>. 91–98.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alexandra D Kaplan, Theresa T Kessler, J Christopher Brill, and Peter A Hancock. 2023.

</span>
<span class="ltx_bibblock">Trust in artificial intelligence: Meta-analytic findings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Human factors</em> 65, 2 (2023), 337–359.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaufman et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Robert Kaufman, Jean Costa, and Everlyne Kimani. 2024a.

</span>
<span class="ltx_bibblock">Effects of multimodal explanations for autonomous driving on driving performance, cognitive load, expertise, confidence, and trust.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Scientific reports</em> 14, 1 (2024), 13061.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaufman and Kirsh (2023)</span>
<span class="ltx_bibblock">
Robert Kaufman and David Kirsh. 2023.

</span>
<span class="ltx_bibblock">Explainable AI And Visual Reasoning: Insights From Radiology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2304.03318</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaufman et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Robert Kaufman, David Kirsh, and Nadir Weibel. 2024b.

</span>
<span class="ltx_bibblock">Developing Situational Awareness for Joint Action with Autonomous Vehicles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">arXiv preprint arXiv:2404.11800</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaufman and Kirsh (2022)</span>
<span class="ltx_bibblock">
Robert A Kaufman and David Kirsh. 2022.

</span>
<span class="ltx_bibblock">Cognitive Differences in Human and AI Explanation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the Annual Meeting of the Cognitive Science Society</em>, Vol. 44.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenesei et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zsófia Kenesei, Katalin Ásványi, László Kökény, Melinda Jászberényi, Márk Miskolczi, Tamás Gyulavári, and Jhanghiz Syahrivar. 2022.

</span>
<span class="ltx_bibblock">Trust and perceived risk: How different manifestations affect the adoption of autonomous vehicles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Transportation research part A: policy and practice</em> 164 (2022), 379–393.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenny et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Eoin M Kenny, Courtney Ford, Molly Quinn, and Mark T Keane. 2021.

</span>
<span class="ltx_bibblock">Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Artificial Intelligence</em> 294 (2021), 103459.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gwangbin Kim, Dohyeon Yeo, Taewoo Jo, Daniela Rus, and SeungJun Kim. 2023.

</span>
<span class="ltx_bibblock">What and When to Explain? On-road Evaluation of Explanations in Highly Automated Vehicles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 7, 3 (2023), 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koo et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Jeamin Koo, Jungsuk Kwac, Wendy Ju, Martin Steinert, Larry Leifer, and Clifford Nass. 2015.

</span>
<span class="ltx_bibblock">Why did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">International Journal on Interactive Design and Manufacturing (IJIDeM)</em> 9 (2015), 269–275.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lebovitz (2019)</span>
<span class="ltx_bibblock">
Sarah Lebovitz. 2019.

</span>
<span class="ltx_bibblock">Diagnostic doubt and artificial intelligence: An inductive field study of radiology work.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">International Conference on Information Systems</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao and Varshney (2021)</span>
<span class="ltx_bibblock">
Q Vera Liao and Kush R Varshney. 2021.

</span>
<span class="ltx_bibblock">Human-centered explainable ai (xai): From algorithms to user experiences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2110.10790</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Brian Y Lim, Anind K Dey, and Daniel Avrahami. 2009.

</span>
<span class="ltx_bibblock">Why and why not explanations improve the intelligibility of context-aware intelligent systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the SIGCHI conference on human factors in computing systems</em>. 2119–2128.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Brian Y Lim, Qian Yang, Ashraf M Abdul, and Danding Wang. 2019.

</span>
<span class="ltx_bibblock">Why these explanations? Selecting intelligibility types for explanation goals.. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">IUI Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LLC ([n. d.])</span>
<span class="ltx_bibblock">
Waymo LLC. [n. d.].

</span>
<span class="ltx_bibblock">Waymo One.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://waymo.com/waymo-one/" title="">https://waymo.com/waymo-one/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-07-22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Löcken et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Andreas Löcken, Shadan Sadeghian Borojeni, Heiko Müller, Thomas M Gable, Stefano Triberti, Cyriel Diels, Christiane Glatz, Ignacio Alvarez, Lewis Chuang, and Susanne Boll. 2017.

</span>
<span class="ltx_bibblock">Towards adaptive ambient in-vehicle displays and interactions: Insights and design guidelines from the 2015 AutomotiveUI dedicated workshop.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Automotive User Interfaces: Creating Interactive Experiences in the Car</em> (2017), 325–348.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ruikun Luo, Jian Chu, and X Jessie Yang. 2020.

</span>
<span class="ltx_bibblock">Trust dynamics in human-AV (automated vehicle) interaction. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Extended abstracts of the 2020 CHI conference on human factors in computing systems</em>. 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma and Feng (2023)</span>
<span class="ltx_bibblock">
Jun Ma and Xuejing Feng. 2023.

</span>
<span class="ltx_bibblock">Analysing the Effects of Scenario-Based Explanations on Automated Vehicle HMIs from Objective and Subjective Perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Sustainability</em> 16, 1 (2023), 63.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinho et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Andreia Martinho, Nils Herber, Maarten Kroesen, and Caspar Chorus. 2021.

</span>
<span class="ltx_bibblock">Ethical issues in focus by the autonomous vehicles industry.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">Transport reviews</em> 41, 5 (2021), 556–577.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (2019)</span>
<span class="ltx_bibblock">
Tim Miller. 2019.

</span>
<span class="ltx_bibblock">Explanation in artificial intelligence: Insights from the social sciences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Artificial intelligence</em> 267 (2019), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (2023)</span>
<span class="ltx_bibblock">
Tim Miller. 2023.

</span>
<span class="ltx_bibblock">Explainable ai is dead, long live explainable ai! hypothesis-driven decision support using evaluative ai. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 2023 ACM conference on fairness, accountability, and transparency</em>. 333–342.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mok et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Brian Ka-Jun Mok, David Sirkin, Srinath Sibi, David Bryan Miller, and Wendy Ju. 2015.

</span>
<span class="ltx_bibblock">Understanding driver-automated vehicle interactions through Wizard of Oz design improvisation. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Driving Assessment Conference</em>, Vol. 8. University of Iowa.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Najm et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Wassim G Najm, John D Smith, Mikio Yanagisawa, et al<span class="ltx_text" id="bib.bib55.3.1">.</span> 2007.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.4.1">Pre-crash scenario typology for crash avoidance research</em>.

</span>
<span class="ltx_bibblock">Technical Report. United States. Department of Transportation. National Highway Traffic Safety ….

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nourani et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mahsan Nourani, Joanie King, and Eric Ragan. 2020.

</span>
<span class="ltx_bibblock">The role of domain expertise in user trust and the impact of first impressions with intelligent systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</em>, Vol. 8. 112–121.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papenmeier et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Andrea Papenmeier, Dagmar Kern, Gwenn Englebienne, and Christin Seifert. 2022.

</span>
<span class="ltx_bibblock">It’s complicated: The relationship between user trust, model accuracy and explanations in AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">ACM Transactions on Computer-Human Interaction (TOCHI)</em> 29, 4 (2022), 1–33.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pazzani et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Michael Pazzani, Severine Soltani, Robert Kaufman, Samson Qian, and Albert Hsiao. 2022.

</span>
<span class="ltx_bibblock">Expert-informed, user-centric explanations for machine learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 36. 12280–12286.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016.

</span>
<span class="ltx_bibblock">” Why should i trust you?” Explaining the predictions of any classifier. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>. 1135–1144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schartmüller et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Clemens Schartmüller, Klemens Weigl, Philipp Wintersberger, Andreas Riener, and Marco Steinhauser. 2019.

</span>
<span class="ltx_bibblock">Text comprehension: Heads-up vs. auditory displays: Implications for a productive work environment in sae level 3 automated vehicles. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proceedings of the 11th International Conference on Automotive User Interfaces and Interactive Vehicular Applications</em>. 342–354.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schilit et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (1994)</span>
<span class="ltx_bibblock">
Bill Schilit, Norman Adams, and Roy Want. 1994.

</span>
<span class="ltx_bibblock">Context-aware computing applications. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">1994 first workshop on mobile computing systems and applications</em>. IEEE, 85–90.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schneider and Handali (2019)</span>
<span class="ltx_bibblock">
Johanes Schneider and Joshua Handali. 2019.

</span>
<span class="ltx_bibblock">Personalized explanation in machine learning: A conceptualization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:1901.00770</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwall et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Matthew Schwall, Tom Daniel, Trent Victor, Francesca Favaro, and Henning Hohnhold. 2020.

</span>
<span class="ltx_bibblock">Waymo public road safety performance data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv preprint arXiv:2011.00038</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seet et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Manuel Seet, Jonathan Harvy, Rohit Bose, Andrei Dragomir, Anastasios Bezerianos, and Nitish Thakor. 2020.

</span>
<span class="ltx_bibblock">Differential impact of autonomous vehicle malfunctions on human trust.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">IEEE transactions on intelligent transportation systems</em> 23, 1 (2020), 548–557.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silvera et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gustavo Silvera, Abhijat Biswas, and Henny Admoni. 2022.

</span>
<span class="ltx_bibblock">DReye VR: democratizing virtual reality driving simulation for behavioural &amp; interaction research. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>. IEEE, 639–643.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013.

</span>
<span class="ltx_bibblock">Deep inside convolutional networks: Visualising image classification models and saliency maps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">arXiv preprint arXiv:1312.6034</em> (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soltani et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Severine Soltani, Robert A Kaufman, and Michael J Pazzani. 2022.

</span>
<span class="ltx_bibblock">User-Centric Enhancements to Explainable AI Algorithms for Image Classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Proceedings of the Annual Meeting of the Cognitive Science Society</em>, Vol. 44.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vasconcelos et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-McLaughlin, Tobias Gerstenberg, Michael S Bernstein, and Ranjay Krishna. 2023.

</span>
<span class="ltx_bibblock">Explanations can reduce overreliance on ai systems during decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 7, CSCW1 (2023), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019.

</span>
<span class="ltx_bibblock">Designing theory-driven user-centric explainable AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024.

</span>
<span class="ltx_bibblock">Hallucination is inevitable: An innate limitation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">arXiv preprint arXiv:2401.11817</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhengming Zhang, Renran Tian, and Vincent G Duffy. 2022.

</span>
<span class="ltx_bibblock">Trust in automated vehicle: A meta-analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Human-Automation Interaction: Transportation</em>. Springer, 221–234.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 16:25:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
