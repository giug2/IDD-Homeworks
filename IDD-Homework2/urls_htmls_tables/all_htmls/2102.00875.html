<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2102.00875] Scaling Federated Learning for Fine-tuning of Large Language Models</title><meta property="og:description" content="Federated learning (FL) is a promising approach to distributed compute, as well as distributed data, and provides a level of privacy and compliance to legal frameworks. This makes FL attractive for both consumer and he…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scaling Federated Learning for Fine-tuning of Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Scaling Federated Learning for Fine-tuning of Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2102.00875">

<!--Generated on Mon Feb 26 20:05:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on 2020‐02‐01.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Scaling Federated Learning for Fine-tuning of Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Agrin Hilmkil
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Peltarion, <a target="_blank" href="https://peltarion.com" title="" class="ltx_ref ltx_href">peltarion.com</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastian Callh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Peltarion, <a target="_blank" href="https://peltarion.com" title="" class="ltx_ref ltx_href">peltarion.com</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matteo Barbieri
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Peltarion, <a target="_blank" href="https://peltarion.com" title="" class="ltx_ref ltx_href">peltarion.com</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Leon René Sütfeld
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">RISE Research Institutes of Sweden, <a target="_blank" href="https://ri.se" title="" class="ltx_ref ltx_href">ri.se</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Edvin Listo Zec
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">RISE Research Institutes of Sweden, <a target="_blank" href="https://ri.se" title="" class="ltx_ref ltx_href">ri.se</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Olof Mogren
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">RISE Research Institutes of Sweden, <a target="_blank" href="https://ri.se" title="" class="ltx_ref ltx_href">ri.se</a>
</span></span></span>
</div>
<div class="ltx_dates">(2020-02-01)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning (FL) is a promising approach to distributed compute, as well as distributed data, and provides a level of privacy and compliance to legal frameworks. This makes FL attractive for both consumer and healthcare applications. While the area is actively being explored, few studies have examined FL in the context of larger language models and there is a lack of comprehensive reviews of robustness across tasks, architectures, numbers of clients, and other relevant factors. In this paper, we explore the fine-tuning of Transformer-based language models in a federated learning setting. We evaluate three popular BERT-variants of different sizes (BERT, ALBERT, and DistilBERT) on a number of text classification tasks such as sentiment analysis and author identification. We perform an extensive sweep over the number of clients, ranging up to 32, to evaluate the impact of distributed compute on task performance in the federated averaging setting. While our findings suggest that the large sizes of the evaluated models are not generally prohibitive to federated training, we found that the different models handle federated averaging to a varying degree. Most notably, DistilBERT converges significantly slower with larger numbers of clients, and under some circumstances, even collapses to chance level performance. Investigating this issue presents an interesting perspective for future research.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Transformer-based architectures such as BERT have recently lead to breakthroughs in a variety of language-related tasks, such as document classification, sentiment analysis, question answering, and various forms of text-mining <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib23" title="" class="ltx_ref">2017</a>; Devlin et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>; Adhikari et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; Sun et al., <a href="#bib.bib21" title="" class="ltx_ref">2019a</a>; Yang et al., <a href="#bib.bib25" title="" class="ltx_ref">2019</a>; Lee et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. These models create semantic representations of text, which can subsequently be used in many downstream tasks <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>. The training process for Transformers typically includes two phases: During pre-training, the model learns to extract semantic representations from large, task-independent corpora. The pre-training is followed by task-specific fine-tuning on a separate dataset to optimize model performance further.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we study the effects of fine-tuning Transformer-based architectures in a federated learning (FL) setting. In FL, models are trained in a decentralized fashion on a number of local compute instances, called clients, and intermittently aggregated and synchronized via a central server. As such, FL is a solution for distributed compute, as well as distributed data, and provides a level of privacy with regards to the sharing of personal or otherwise sensitive data. Model aggregation is commonly performed via averaging of the weights of the individual client models, called Federated Averaging (<span id="S1.p2.1.1" class="ltx_text ltx_font_smallcaps">FedAvg</span>) <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib16" title="" class="ltx_ref">2017a</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Depending on the application, the number of clients in an FL setting can differ wildly. In instances where smartphones are used as clients, their number can reach into the millions <cite class="ltx_cite ltx_citemacro_citep">(Hard et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, whereas settings with higher compute requirements and more data per client will often range between a handful and a few dozens of clients. Here, we focus on the latter, as training large language models requires a lot of compute. A potential application of this is the medical field, in which automated analyses of electronic health records yield enormous potential for diagnostics and treatment-related insights <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al., <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Our contribution:</h4>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">We provide a comprehensive overview of the applicability of the federated learning setting to large language models. To this end, we work with a fixed computation budget for each task, and use a fixed total amount of data while varying the number of clients between which the data is split up. This way, we isolate the effects of distributing data over several clients for distributed compute. We leave comparisons with a fixed amount of data per client and varying non-i.i.d. data distributions between clients for future work. The main contributions of this paper are the following: (1) We provide a comparison of three popular Transformer-based language models in the federated learning setting, using the IMDB, Yelp F, and AG News datasets. (2) We analyze how the number of clients impacts task performance across tasks and model architectures.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Federated optimization was first introduced by <cite class="ltx_cite ltx_citemacro_cite">Konečnỳ et al. (<a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>. The key challenges in this paradigm are communication efficiency when learning from many clients, privacy concerns with respect to leakage of client data, and variability in data distributions between clients (non-i.i.d. setting). <span id="S2.p1.1.1" class="ltx_text ltx_font_smallcaps">FedAvg</span> <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib16" title="" class="ltx_ref">2017a</a>)</cite> solves the federated optimization problem by building a global model based on local stochastic gradient descent updates and has been shown to work on non-i.i.d. data. Since then, many adaptations have arisen <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>); Mohri et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>); Karimireddy et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Guha et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> proposes a one-shot FL algorithm, learning a global model efficiently in just one communication round. <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Hsu et al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Listo Zec et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> study effects of <span id="S2.p1.1.2" class="ltx_text ltx_font_smallcaps">FedAvg</span> and non-i.i.d. client data. <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib17" title="" class="ltx_ref">2017b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Hard et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> train large recurrent language models with user-level differential privacy guarantees and for mobile keyboard prediction, respectively. <cite class="ltx_cite ltx_citemacro_citet">Ge et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> use federated learning for named entity recognition with heterogeneous medical data.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Regarding model size, most architectures used in FL to date are relatively small (e.g., CIFG for mobile keyboard prediction: 1.4M parameters <cite class="ltx_cite ltx_citemacro_cite">Hard et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>), compared to BERT-based language models with hundreds of millions of parameters. How these very large models behave under <span id="S2.p2.1.1" class="ltx_text ltx_font_smallcaps">FedAvg</span> remains underexplored. To the best of our knowledge, <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Liu and Miller (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> are the first ones to train large Transformer models in a federated setting. <cite class="ltx_cite ltx_citemacro_citet">Liu and Miller (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> trained BERT on a medical corpus and showed that both pre-training and fine-tuning could be done in a federated manner with only minor declines in task performance. Nonetheless, the study is mainly a proof-of-concept and does not explore many of the factors that can be expected in real-world scenarios. For instance, the authors only used five clients, and evaluated them only on i.i.d. data. <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> introduces FedDF, an ensemble distillation algorithm for model fusion. The authors train a central model through unlabeled data on the client models outputs, and perform fine-tuning on a pre-trained DistilBERT <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> in a federated setting as a baseline. To the best of our knowledge, no systematic variation of the number of clients and other relevant factors has previously been explored in this context.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Federated learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.9" class="ltx_p">Federated learning aims to solve the optimization problem</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\min_{\theta\in\mathbb{R}^{d}}\frac{1}{K}\sum_{k=1}^{K}F_{k}(\theta)," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><munder id="S3.E1.m1.2.2.1.1.2.1" xref="S3.E1.m1.2.2.1.1.2.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.2" xref="S3.E1.m1.2.2.1.1.2.1.2.cmml">min</mi><mrow id="S3.E1.m1.2.2.1.1.2.1.3" xref="S3.E1.m1.2.2.1.1.2.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.3.2" xref="S3.E1.m1.2.2.1.1.2.1.3.2.cmml">θ</mi><mo id="S3.E1.m1.2.2.1.1.2.1.3.1" xref="S3.E1.m1.2.2.1.1.2.1.3.1.cmml">∈</mo><msup id="S3.E1.m1.2.2.1.1.2.1.3.3" xref="S3.E1.m1.2.2.1.1.2.1.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.3.3.2" xref="S3.E1.m1.2.2.1.1.2.1.3.3.2.cmml">ℝ</mi><mi id="S3.E1.m1.2.2.1.1.2.1.3.3.3" xref="S3.E1.m1.2.2.1.1.2.1.3.3.3.cmml">d</mi></msup></mrow></munder><mo lspace="0.167em" id="S3.E1.m1.2.2.1.1.2a" xref="S3.E1.m1.2.2.1.1.2.cmml">⁡</mo><mfrac id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.2.cmml"><mn id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.cmml">1</mn><mi id="S3.E1.m1.2.2.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">K</mi></mfrac></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><munderover id="S3.E1.m1.2.2.1.1.3.1" xref="S3.E1.m1.2.2.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.2.2.1.1.3.1.2.2" xref="S3.E1.m1.2.2.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.2.2.1.1.3.1.2.3" xref="S3.E1.m1.2.2.1.1.3.1.2.3.cmml"><mi id="S3.E1.m1.2.2.1.1.3.1.2.3.2" xref="S3.E1.m1.2.2.1.1.3.1.2.3.2.cmml">k</mi><mo id="S3.E1.m1.2.2.1.1.3.1.2.3.1" xref="S3.E1.m1.2.2.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.2.1.1.3.1.2.3.3" xref="S3.E1.m1.2.2.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.2.1.1.3.1.3" xref="S3.E1.m1.2.2.1.1.3.1.3.cmml">K</mi></munderover><mrow id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml"><msub id="S3.E1.m1.2.2.1.1.3.2.2" xref="S3.E1.m1.2.2.1.1.3.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.3.2.2.2" xref="S3.E1.m1.2.2.1.1.3.2.2.2.cmml">F</mi><mi id="S3.E1.m1.2.2.1.1.3.2.2.3" xref="S3.E1.m1.2.2.1.1.3.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.3.2.1" xref="S3.E1.m1.2.2.1.1.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.3.2.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.2.3.2.1" xref="S3.E1.m1.2.2.1.1.3.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.2.3.2.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><times id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"></times><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><apply id="S3.E1.m1.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1">subscript</csymbol><min id="S3.E1.m1.2.2.1.1.2.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.2"></min><apply id="S3.E1.m1.2.2.1.1.2.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3"><in id="S3.E1.m1.2.2.1.1.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.1"></in><ci id="S3.E1.m1.2.2.1.1.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.2">𝜃</ci><apply id="S3.E1.m1.2.2.1.1.2.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.3">superscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.3.2">ℝ</ci><ci id="S3.E1.m1.2.2.1.1.2.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.3.3">𝑑</ci></apply></apply></apply><apply id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2"><divide id="S3.E1.m1.2.2.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2"></divide><cn type="integer" id="S3.E1.m1.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2">1</cn><ci id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3">𝐾</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><apply id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.3.1.2.cmml" xref="S3.E1.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.2.2.1.1.3.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.1.2.2"></sum><apply id="S3.E1.m1.2.2.1.1.3.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.3.1.2.3"><eq id="S3.E1.m1.2.2.1.1.3.1.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1.2.3.1"></eq><ci id="S3.E1.m1.2.2.1.1.3.1.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.3.1.2.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.2.2.1.1.3.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3.1.3">𝐾</ci></apply><apply id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2"><times id="S3.E1.m1.2.2.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.2.1"></times><apply id="S3.E1.m1.2.2.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2.2">𝐹</ci><ci id="S3.E1.m1.2.2.1.1.3.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2.3">𝑘</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\min_{\theta\in\mathbb{R}^{d}}\frac{1}{K}\sum_{k=1}^{K}F_{k}(\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.8" class="ltx_p">where <math id="S3.SS1.p1.1.m1.4" class="ltx_Math" alttext="F_{k}(\theta)=\mathbb{E}_{x\sim\mathcal{D}_{k}}\left[\ell_{k}(\theta;x)\right]" display="inline"><semantics id="S3.SS1.p1.1.m1.4a"><mrow id="S3.SS1.p1.1.m1.4.4" xref="S3.SS1.p1.1.m1.4.4.cmml"><mrow id="S3.SS1.p1.1.m1.4.4.3" xref="S3.SS1.p1.1.m1.4.4.3.cmml"><msub id="S3.SS1.p1.1.m1.4.4.3.2" xref="S3.SS1.p1.1.m1.4.4.3.2.cmml"><mi id="S3.SS1.p1.1.m1.4.4.3.2.2" xref="S3.SS1.p1.1.m1.4.4.3.2.2.cmml">F</mi><mi id="S3.SS1.p1.1.m1.4.4.3.2.3" xref="S3.SS1.p1.1.m1.4.4.3.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.4.4.3.1" xref="S3.SS1.p1.1.m1.4.4.3.1.cmml">​</mo><mrow id="S3.SS1.p1.1.m1.4.4.3.3.2" xref="S3.SS1.p1.1.m1.4.4.3.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.4.4.3.3.2.1" xref="S3.SS1.p1.1.m1.4.4.3.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.4.4.3.3.2.2" xref="S3.SS1.p1.1.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.1.m1.4.4.2" xref="S3.SS1.p1.1.m1.4.4.2.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.4.4.1" xref="S3.SS1.p1.1.m1.4.4.1.cmml"><msub id="S3.SS1.p1.1.m1.4.4.1.3" xref="S3.SS1.p1.1.m1.4.4.1.3.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.3.2" xref="S3.SS1.p1.1.m1.4.4.1.3.2.cmml">𝔼</mi><mrow id="S3.SS1.p1.1.m1.4.4.1.3.3" xref="S3.SS1.p1.1.m1.4.4.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.3.3.2" xref="S3.SS1.p1.1.m1.4.4.1.3.3.2.cmml">x</mi><mo id="S3.SS1.p1.1.m1.4.4.1.3.3.1" xref="S3.SS1.p1.1.m1.4.4.1.3.3.1.cmml">∼</mo><msub id="S3.SS1.p1.1.m1.4.4.1.3.3.3" xref="S3.SS1.p1.1.m1.4.4.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.4.4.1.3.3.3.2" xref="S3.SS1.p1.1.m1.4.4.1.3.3.3.2.cmml">𝒟</mi><mi id="S3.SS1.p1.1.m1.4.4.1.3.3.3.3" xref="S3.SS1.p1.1.m1.4.4.1.3.3.3.3.cmml">k</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.4.4.1.2" xref="S3.SS1.p1.1.m1.4.4.1.2.cmml">​</mo><mrow id="S3.SS1.p1.1.m1.4.4.1.1.1" xref="S3.SS1.p1.1.m1.4.4.1.1.2.cmml"><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.2" xref="S3.SS1.p1.1.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.p1.1.m1.4.4.1.1.1.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.2.cmml">ℓ</mi><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.cmml">​</mo><mrow id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml">(</mo><mi id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">θ</mi><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml">;</mo><mi id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3.cmml">x</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.3" xref="S3.SS1.p1.1.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.4b"><apply id="S3.SS1.p1.1.m1.4.4.cmml" xref="S3.SS1.p1.1.m1.4.4"><eq id="S3.SS1.p1.1.m1.4.4.2.cmml" xref="S3.SS1.p1.1.m1.4.4.2"></eq><apply id="S3.SS1.p1.1.m1.4.4.3.cmml" xref="S3.SS1.p1.1.m1.4.4.3"><times id="S3.SS1.p1.1.m1.4.4.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.3.1"></times><apply id="S3.SS1.p1.1.m1.4.4.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.3.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.3.2.1.cmml" xref="S3.SS1.p1.1.m1.4.4.3.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.3.2.2.cmml" xref="S3.SS1.p1.1.m1.4.4.3.2.2">𝐹</ci><ci id="S3.SS1.p1.1.m1.4.4.3.2.3.cmml" xref="S3.SS1.p1.1.m1.4.4.3.2.3">𝑘</ci></apply><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝜃</ci></apply><apply id="S3.SS1.p1.1.m1.4.4.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1"><times id="S3.SS1.p1.1.m1.4.4.1.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.2"></times><apply id="S3.SS1.p1.1.m1.4.4.1.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.2">𝔼</ci><apply id="S3.SS1.p1.1.m1.4.4.1.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.3"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.4.4.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.3.1">similar-to</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.3.2">𝑥</ci><apply id="S3.SS1.p1.1.m1.4.4.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.3.3.2">𝒟</ci><ci id="S3.SS1.p1.1.m1.4.4.1.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.3.3.3.3">𝑘</ci></apply></apply></apply><apply id="S3.SS1.p1.1.m1.4.4.1.1.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.4.4.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1"><times id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.1"></times><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.2">ℓ</ci><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.3">𝑘</ci></apply><list id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2"><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">𝜃</ci><ci id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3">𝑥</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.4c">F_{k}(\theta)=\mathbb{E}_{x\sim\mathcal{D}_{k}}\left[\ell_{k}(\theta;x)\right]</annotation></semantics></math> is the expected loss on client <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">k</annotation></semantics></math> and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝒟</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathcal{D}_{k}</annotation></semantics></math> is the data distribution of client <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">k</annotation></semantics></math>. In <span id="S3.SS1.p1.8.1" class="ltx_text ltx_font_smallcaps">FedAvg</span>, a global model <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="f_{\theta}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑓</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">f_{\theta}</annotation></semantics></math> is initialized on a central server and distributed to all <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">K</annotation></semantics></math> clients, each of which then trains its individual copy of the network using SGD for <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">E</annotation></semantics></math> local epochs with local batch size <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">B</annotation></semantics></math>.
The clients’ updated parameters are then averaged on the central server, weighted by the local data size at each client. The averaged model is distributed to the clients again, and the process is repeated for a defined number of communication rounds.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We implement <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_smallcaps">FedAvg</span> using distributed PyTorch <cite class="ltx_cite ltx_citemacro_cite">Paszke et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. For each experiment we start from a pre-trained model, and fine-tune it with federated averaging on the current task.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We include BERT with 110M parameters, 12 layers <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>, ALBERT with 11M parameters, 12 layers <cite class="ltx_cite ltx_citemacro_cite">Lan et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> and DistilBERT with 65M parameters, 6 layers <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. This allows us to study the effect that both the parameter count and the number of layers have on <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">FedAvg</span>. All models are the corresponding base models pre-trained on (cased) English. In particular, it should be noted that while the models have similar architectures, they have some key differences. ALBERT introduces factorized embedding parameterization and cross-layer parameter sharing, while the DistilBERT model is a student network trained with knowledge distillation from BERT. We use the weights and implementations of the models available in the Huggingface Transformers library <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Datasets</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We performed experiments on three standard datasets
to assess the performance of the proposed approach on different tasks.
All of them pose classification problems with a different number of target categories and dataset sizes. For each dataset, we use the test set specified by the source.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">IMDB.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">The Large Movie Review Dataset <cite class="ltx_cite ltx_citemacro_cite">Maas et al. (<a href="#bib.bib15" title="" class="ltx_ref">2011</a>)</cite> contains of a collection of 50,000 movie reviews and their associated binary sentiment polarity labels (either “positive” or “negative”), which is used to train a sentiment classifier.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Yelp F.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">This dataset <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> contains reviews of local businesses and their associated rating (1-5). The task is posed as a text classification task, from the review text to its associated rating.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">AG News.</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.1" class="ltx_p">The AG’s corpus of news articles<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html</a></span></span></span> consists of over one million news articles gathered from more than 2,000 news sources, divided into a number of categories depending on their content. We used the common subset <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> of the whole dataset, consisting of a total of 120,000 samples equally divided in four categories.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Experiments and hyperparameters</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We construct several experiments to evaluate how well Federated Learning scales to an exponentially increasing number of clients. In all experiments, the respective dataset is evenly partitioned into a number of subsets equal to the number of clients. Data points are uniformly sampled on each client (i.i.d.) like <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib16" title="" class="ltx_ref">2017a</a>)</cite>. We do not perform any hyperparameter tuning, and instead, keep all other hyperparameters constant for an unbiased comparison. As baselines we run for each task and BERT-variant a non-federated scenario (clients = 1) with the same configuration.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.2" class="ltx_p">We run the baselines for a fixed number of rounds based on our compute budget. The test set performance for the baselines are then compared against varying number of participating clients at the same number of rounds. Finally, since runs with a larger number of clients converge more slowly, we allow those runs to continue to a second threshold and report the number of rounds required to reach <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mn id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">90</mn><mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">90\%</annotation></semantics></math> of the baseline performance, similar to <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib16" title="" class="ltx_ref">2017a</a>)</cite>. Runs not reaching <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mrow id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mn id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">90</mn><mo id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="latexml" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">90\%</annotation></semantics></math> of the baseline performance within the second threshold are reported as failures.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.6" class="ltx_p">We run the baseline for 100 rounds for both IMDB and AG News while setting the second threshold to 200 rounds. However, we only run Yelp F baselines for 50 rounds due to its large size and set the second threshold at 100 rounds. Like <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, we avoid momentum, weight decay, and dynamic learning rates for simplicity. Instead, all experiments are performed with SGD. Based on <cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a href="#bib.bib22" title="" class="ltx_ref">2019b</a>)</cite> we choose the constant learning rate <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="2\cdot 10^{-5}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mn id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">⋅</mo><msup id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml"><mn id="S3.SS4.p3.1.m1.1.1.3.2" xref="S3.SS4.p3.1.m1.1.1.3.2.cmml">10</mn><mrow id="S3.SS4.p3.1.m1.1.1.3.3" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml"><mo id="S3.SS4.p3.1.m1.1.1.3.3a" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS4.p3.1.m1.1.1.3.3.2" xref="S3.SS4.p3.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><ci id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1">⋅</ci><cn type="integer" id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">2</cn><apply id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS4.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.2">10</cn><apply id="S3.SS4.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3"><minus id="S3.SS4.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.SS4.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">2\cdot 10^{-5}</annotation></semantics></math>, maximum sequence length <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mn id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><cn type="integer" id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">128</annotation></semantics></math> and batch size (<math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">B</annotation></semantics></math>) of <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mn id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><cn type="integer" id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">32</annotation></semantics></math>. Furthermore, the number of local epochs (<math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">E</annotation></semantics></math>) is set to <math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><mn id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><cn type="integer" id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">2</annotation></semantics></math> per round.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fixed compute budget</h3>

<figure id="S4.F1" class="ltx_figure"><img src="/html/2102.00875/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Accuracy at a fixed compute budget of 100 rounds for AG, IMDB, and 50 rounds for Yelp F. The expected accuracy of a random classifier for each task has been highlighted in the dashed line. Higher is better.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In Figure <a href="#S4.F1" title="Figure 1 ‣ 4.1 Fixed compute budget ‣ 4 Results ‣ Scaling Federated Learning for Fine-tuning of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we study the effect of increasing the number of clients. It shows the final accuracy after 100 rounds IMDB and AG News, and 50 rounds of the much larger Yelp F., with an exponentially increasing number of clients. Both ALBERT and BERT are well behaved and exhibit a gradual decrease with an increasing number of clients. However, DistilBERT shows a much steeper decline when moving past 4 clients for all datasets, down to the random classifier baseline (IMDB, Yelp F).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Rounds until target performance</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2102.00875/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Number of training rounds required to reach <math id="S4.F2.2.m1.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S4.F2.2.m1.1b"><mrow id="S4.F2.2.m1.1.1" xref="S4.F2.2.m1.1.1.cmml"><mn id="S4.F2.2.m1.1.1.2" xref="S4.F2.2.m1.1.1.2.cmml">90</mn><mo id="S4.F2.2.m1.1.1.1" xref="S4.F2.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><apply id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1"><csymbol cd="latexml" id="S4.F2.2.m1.1.1.1.cmml" xref="S4.F2.2.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.F2.2.m1.1.1.2.cmml" xref="S4.F2.2.m1.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">90\%</annotation></semantics></math> of the non-federated baseline accuracy. Omittions occur when the target is not reached in 100 (Yelp F) or 200 rounds (AG News, IMDB). Lower is better.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Examining the number of rounds necessary to achieve 90% of the non-federated baseline accuracy (Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Rounds until target performance ‣ 4 Results ‣ Scaling Federated Learning for Fine-tuning of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) yields a similar observation. While all models perform worse with more clients, ALBERT and BERT mostly reach the target accuracy within the allocated number of rounds until 32 clients are used. DistilBERT on the other is unable to reach the target accuracy at 16 clients for Yelp F, and as low as 4 clients for IMDB.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Dynamics of fine-tuning</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2102.00875/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="510" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Test accuracy (higher is better) over communication rounds for our scenarios. The random classifier baseline is shown as a dashed line.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The test accuracy during fine-tuning (Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3 Dynamics of fine-tuning ‣ 4 Results ‣ Scaling Federated Learning for Fine-tuning of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) allows a more complete understanding of how well <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">FedAvg</span> scales for language model fine-tuning. While some scenarios (e.g. Yelp F. with BERT) show a gradual degradation with the number of clients, others configurations are more adversely affected by the increasing number of clients. In some instances the accuracy stays constant over a large period, sometimes even at the random classifier baseline for the whole (DistilBERT on IMDB) or part (DistilBERT on AG News) of the experiment when the number of clients is high.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we have evaluated the performance of Transformer-based language models fine-tuned in a federated setting.
While BERT and ALBERT seem to learn each task quickly (Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3 Dynamics of fine-tuning ‣ 4 Results ‣ Scaling Federated Learning for Fine-tuning of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), DistilBERT has a much slower learning progression in the federated setup. A possible explanation is the process of distillation during pre-training, but further research is needed to fully understand why this happens. We demonstrated that BERT and ALBERT scale well up to 32 clients with no sharp decline in performance (Figure <a href="#S4.F1" title="Figure 1 ‣ 4.1 Fixed compute budget ‣ 4 Results ‣ Scaling Federated Learning for Fine-tuning of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), but found DistilBERT to struggle at 16 clients in the Yelp F and AG News tasks, and with as low as 4 clients in the IMDB task, with a substantial drop in performance compared to the baseline. Furthermore, DistilBERT takes more rounds to achieve the same performance. These results demonstrate that we have obtained a higher communication efficiency for BERT and ALBERT as compared to DistilBERT. Further work is required to get a good picture of exactly what affects the communication efficiency for federated learning of Transformer-based language models.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Conversely, the sudden drop in performance in some scenarios indicates that FL can be sensitive to the number of clients. The cause for the instability has not been fully determined. It may be related to both smaller partitions and contradicting models. This highlights the importance of evaluating FL with a varying number of clients at these scales.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">For all three models, the performance decline from the baseline is steeper for IMDB compared to the other tasks. This may be related to the variability in the movie review data, adding to a larger inter-client difference in data distribution when data is put into smaller partitions, resulting in a larger difference between the client models taking part in the federated averaging.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In conclusion, we have demonstrated the applicability of the federated learning paradigm and evaluated it on a number of Transformer-based models up to 32 clients. Our findings show that the relatively large sizes of these models are not prohibitive for federated learning.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work was funded by VINNOVA (grant <span id="S6.p1.1.1" class="ltx_text">2019-05156</span>). We would also like to thank <span id="S6.p1.1.2" class="ltx_text">AI Sweden</span> and CGit for providing us with compute resources.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adhikari et al. (2019)</span>
<span class="ltx_bibblock">
Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and Jimmy Lin. 2019.

</span>
<span class="ltx_bibblock">Docbert: Bert for document classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.08398</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclweb.org/anthology/papers/N/N19/N19-1423/" title="" class="ltx_ref ltx_href">Bert:
Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">NAACL-HLT (1)</em>, pages 4171–4186.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2020)</span>
<span class="ltx_bibblock">
Suyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2020.

</span>
<span class="ltx_bibblock">Fedner: Medical named entity recognition with federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.09288</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guha et al. (2019)</span>
<span class="ltx_bibblock">
Neel Guha, Ameet Talwalkar, and Virginia Smith. 2019.

</span>
<span class="ltx_bibblock">One-shot federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.11175</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1811.03604" title="" class="ltx_ref ltx_href">Federated learning for
mobile keyboard prediction</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1811.03604.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2019)</span>
<span class="ltx_bibblock">
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. 2019.

</span>
<span class="ltx_bibblock">Measuring the effects of non-identical data distribution for
federated visual classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.06335</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. (2019)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi,
Sebastian U Stich, and Ananda Theertha Suresh. 2019.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for on-device federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.06378</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. (2015)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, Brendan McMahan, and Daniel Ramage. 2015.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed optimization beyond the
datacenter.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.03575</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. (2020)</span>
<span class="ltx_bibblock">
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
Radu Soricut. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=H1eA7AEtvS" title="" class="ltx_ref ltx_href">Albert: A lite
bert for self-supervised learning of language representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2020)</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2020.

</span>
<span class="ltx_bibblock">Biobert: a pre-trained biomedical language representation model for
biomedical text mining.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, 36(4):1234–1240.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. 2019.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10497</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.07242</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Listo Zec et al. (2020)</span>
<span class="ltx_bibblock">
Edvin Listo Zec, Olof Mogren, John Martinsson, Leon René Sütfeld, and Daniel
Gillblad. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2010.02056" title="" class="ltx_ref ltx_href">Federated learning using a
mixture of experts</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Miller (2020)</span>
<span class="ltx_bibblock">
Dianbo Liu and Tim Miller. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2002.08562" title="" class="ltx_ref ltx_href">Federated pretraining and
fine tuning of bert using clinical notes from multiple silos</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maas et al. (2011)</span>
<span class="ltx_bibblock">
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and
Christopher Potts. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.aclweb.org/anthology/P11-1015" title="" class="ltx_ref ltx_href">Learning word
vectors for sentiment analysis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies</em>, pages 142–150,
Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017a)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas. 2017a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_href">Communication-Efficient Learning of Deep Networks from Decentralized
Data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em>, volume 54, pages 1273–1282, Fort
Lauderdale, FL, USA. PMLR.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017b)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang.
2017b.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06963</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri et al. (2019)</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v97/mohri19a.html" title="" class="ltx_ref ltx_href">Agnostic
federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, volume 97, pages
4615–4625, Long Beach, California, USA. PMLR.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="" class="ltx_ref ltx_href">Pytorch: An imperative style, high-performance deep learning library</a>.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 32</em>, pages 8024–8035. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1910.01108" title="" class="ltx_ref ltx_href">Distilbert, a distilled
version of bert: smaller, faster, cheaper and lighter</a>.

</span>
<span class="ltx_bibblock">In EMC<sup id="bib.bib20.2.1" class="ltx_sup">2</sup> Workshop at NeurIPS 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2019a)</span>
<span class="ltx_bibblock">
Chi Sun, Luyao Huang, and Xipeng Qiu. 2019a.

</span>
<span class="ltx_bibblock">Utilizing bert for aspect-based sentiment analysis via constructing
auxiliary sentence.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.09588</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2019b)</span>
<span class="ltx_bibblock">
Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019b.

</span>
<span class="ltx_bibblock">How to fine-tune bert for text classification?

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Chinese Computational Linguistics</em>, pages 194–206, Cham.
Springer International Publishing.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
5998–6008.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander M. Rush. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1910.03771.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li,
and Jimmy Lin. 2019.

</span>
<span class="ltx_bibblock">End-to-end open-domain question answering with bertserini.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.01718</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2018)</span>
<span class="ltx_bibblock">
Zexian Zeng, Yu Deng, Xiaoyu Li, Tristan Naumann, and Yuan Luo. 2018.

</span>
<span class="ltx_bibblock">Natural language processing for ehr-based computational phenotyping.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM transactions on computational biology and
bioinformatics</em>, 16(1):139–153.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2015)</span>
<span class="ltx_bibblock">
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" title="" class="ltx_ref ltx_href">Character-level convolutional networks for text classification</a>.

</span>
<span class="ltx_bibblock">In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 28</em>, pages
649–657. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2102.00874" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2102.00875" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2102.00875">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2102.00875" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2102.00876" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 20:05:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
