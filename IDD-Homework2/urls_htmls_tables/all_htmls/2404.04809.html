<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language</title>
<!--Generated on Sun Apr  7 05:05:17 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.04809v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S1" title="1. Introduction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S2" title="2. The Mambai Language ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>The Mambai Language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3" title="3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology for Data Extraction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.SS1" title="3.1. Materials ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Materials</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.SS2" title="3.2. OCR Process ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>OCR Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.SS3" title="3.3. Text Corpora ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Text Corpora</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.SS3.SSS1" title="3.3.1. Dictionary extraction ‣ 3.3. Text Corpora ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Dictionary extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.SS3.SSS2" title="3.3.2. Parallel sentence extraction ‣ 3.3. Text Corpora ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Parallel sentence extraction</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4" title="4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Mambai Translation through Retrieval-Augmented LLM Prompting</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.SS1" title="4.1. Rationale ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Rationale</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.SS2" title="4.2. Methodology ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.SS2.SSS1" title="4.2.1. Data setup ‣ 4.2. Methodology ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Data setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.SS2.SSS2" title="4.2.2. Prompt ‣ 4.2. Methodology ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.SS2.SSS3" title="4.2.3. Models ‣ 4.2. Methodology ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.SS3" title="4.3. Translation Results ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Translation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.SS4" title="4.4. Error analysis ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Error analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S5" title="5. Related Work ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2404.04809v1 [cs.CL] 07 Apr 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This study explores the use of large language models (LLMs) for translating English into Mambai, a low-resource Austronesian language spoken in Timor-Leste, with approximately 200,000 native speakers. Leveraging a novel corpus derived from a Mambai language manual and additional sentences translated by a native speaker, we examine the efficacy of few-shot LLM prompting for machine translation (MT) in this low-resource context. Our methodology involves the strategic selection of parallel sentences and dictionary entries for prompting, aiming to enhance translation accuracy, using open-source and proprietary LLMs (LlaMa 2 70b, Mixtral 8x7B, GPT-4). We find that including dictionary entries in prompts and a mix of sentences retrieved through TF-IDF and semantic embeddings significantly improves translation quality. However, our findings reveal stark disparities in translation performance across test sets, with BLEU scores reaching as high as 21.2 on materials from the language manual, in contrast to a maximum of 4.4 on a test set provided by a native speaker. These results underscore the importance of diverse and representative corpora in assessing MT for low-resource languages. Our research provides insights into few-shot LLM prompting for low-resource MT, and makes available an initial corpus for the Mambai language.

<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id1.id1.1">Keywords: </span>low-resource languages, austronesian language, large language models, prompting, dictionary, parallel data</p>
</div>
<span class="ltx_ERROR undefined" id="id1">\NAT@set@cites</span>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text" id="p1.1.1"></span></p>
</div>
<div class="ltx_para ltx_align_center" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1" style="font-size:144%;">Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language</span></p>
</div>
<div class="ltx_para ltx_align_center" id="p3">
<table class="ltx_tabular ltx_align_top" id="p3.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="p3.3.3">
<td class="ltx_td ltx_align_center" id="p3.3.3.3"><span class="ltx_text ltx_font_bold" id="p3.3.3.3.3" style="font-size:120%;">Raphaël Merx<math alttext="{}^{\textrm{\textctc}}" class="ltx_Math" display="inline" id="p3.1.1.1.1.m1.1"><semantics id="p3.1.1.1.1.m1.1a"><msup id="p3.1.1.1.1.m1.1.1" xref="p3.1.1.1.1.m1.1.1.cmml"><mi id="p3.1.1.1.1.m1.1.1a" xref="p3.1.1.1.1.m1.1.1.cmml"></mi><mtext class="undefined" id="p3.1.1.1.1.m1.1.1.1" mathsize="171%" mathvariant="bold" xref="p3.1.1.1.1.m1.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.1.1.1.1.m1.1.1.1.1nest">\textctc</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.1.1.1.1.m1.1b"><apply id="p3.1.1.1.1.m1.1.1.cmml" xref="p3.1.1.1.1.m1.1.1"><ci id="p3.1.1.1.1.m1.1.1.1b.cmml" xref="p3.1.1.1.1.m1.1.1.1"><mtext class="undefined" id="p3.1.1.1.1.m1.1.1.1.cmml" mathvariant="bold" xref="p3.1.1.1.1.m1.1.1.1"><span class="ltx_ERROR undefined" id="p3.1.1.1.1.m1.1.1.1.1anest">\textctc</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.1.1.1.1.m1.1c">{}^{\textrm{\textctc}}</annotation><annotation encoding="application/x-llamapun" id="p3.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math>        Aso Mahmudi<math alttext="{}^{\textrm{\textltailm}}" class="ltx_Math" display="inline" id="p3.2.2.2.2.m2.1"><semantics id="p3.2.2.2.2.m2.1a"><msup id="p3.2.2.2.2.m2.1.1" xref="p3.2.2.2.2.m2.1.1.cmml"><mi id="p3.2.2.2.2.m2.1.1a" xref="p3.2.2.2.2.m2.1.1.cmml"></mi><mtext class="undefined" id="p3.2.2.2.2.m2.1.1.1" mathsize="171%" mathvariant="bold" xref="p3.2.2.2.2.m2.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.2.2.2.2.m2.1.1.1.1nest">\textltailm</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.2.2.2.2.m2.1b"><apply id="p3.2.2.2.2.m2.1.1.cmml" xref="p3.2.2.2.2.m2.1.1"><ci id="p3.2.2.2.2.m2.1.1.1b.cmml" xref="p3.2.2.2.2.m2.1.1.1"><mtext class="undefined" id="p3.2.2.2.2.m2.1.1.1.cmml" mathvariant="bold" xref="p3.2.2.2.2.m2.1.1.1"><span class="ltx_ERROR undefined" id="p3.2.2.2.2.m2.1.1.1.1anest">\textltailm</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.2.2.2.2.m2.1c">{}^{\textrm{\textltailm}}</annotation><annotation encoding="application/x-llamapun" id="p3.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math>       Katrina Langford<math alttext="{}^{\textrm{\textturnt}}" class="ltx_Math" display="inline" id="p3.3.3.3.3.m3.1"><semantics id="p3.3.3.3.3.m3.1a"><msup id="p3.3.3.3.3.m3.1.1" xref="p3.3.3.3.3.m3.1.1.cmml"><mi id="p3.3.3.3.3.m3.1.1a" xref="p3.3.3.3.3.m3.1.1.cmml"></mi><mtext class="undefined" id="p3.3.3.3.3.m3.1.1.1" mathsize="171%" mathvariant="bold" xref="p3.3.3.3.3.m3.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.3.3.3.3.m3.1.1.1.1nest">\textturnt</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.3.3.3.3.m3.1b"><apply id="p3.3.3.3.3.m3.1.1.cmml" xref="p3.3.3.3.3.m3.1.1"><ci id="p3.3.3.3.3.m3.1.1.1b.cmml" xref="p3.3.3.3.3.m3.1.1.1"><mtext class="undefined" id="p3.3.3.3.3.m3.1.1.1.cmml" mathvariant="bold" xref="p3.3.3.3.3.m3.1.1.1"><span class="ltx_ERROR undefined" id="p3.3.3.3.3.m3.1.1.1.1anest">\textturnt</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.3.3.3.3.m3.1c">{}^{\textrm{\textturnt}}</annotation><annotation encoding="application/x-llamapun" id="p3.3.3.3.3.m3.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="p3.5.5">
<td class="ltx_td ltx_align_center" id="p3.5.5.2"><span class="ltx_text ltx_font_bold" id="p3.5.5.2.2" style="font-size:120%;">Leo Alberto de Araujo<math alttext="{}^{\textrm{\textesh}}" class="ltx_Math" display="inline" id="p3.4.4.1.1.m1.1"><semantics id="p3.4.4.1.1.m1.1a"><msup id="p3.4.4.1.1.m1.1.1" xref="p3.4.4.1.1.m1.1.1.cmml"><mi id="p3.4.4.1.1.m1.1.1a" xref="p3.4.4.1.1.m1.1.1.cmml"></mi><mtext class="undefined" id="p3.4.4.1.1.m1.1.1.1" mathsize="171%" mathvariant="bold" xref="p3.4.4.1.1.m1.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.4.4.1.1.m1.1.1.1.1nest">\textesh</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.4.4.1.1.m1.1b"><apply id="p3.4.4.1.1.m1.1.1.cmml" xref="p3.4.4.1.1.m1.1.1"><ci id="p3.4.4.1.1.m1.1.1.1b.cmml" xref="p3.4.4.1.1.m1.1.1.1"><mtext class="undefined" id="p3.4.4.1.1.m1.1.1.1.cmml" mathvariant="bold" xref="p3.4.4.1.1.m1.1.1.1"><span class="ltx_ERROR undefined" id="p3.4.4.1.1.m1.1.1.1.1anest">\textesh</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.4.4.1.1.m1.1c">{}^{\textrm{\textesh}}</annotation><annotation encoding="application/x-llamapun" id="p3.4.4.1.1.m1.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math>     Ekaterina Vylomova<math alttext="{}^{\textrm{\textltailm}}" class="ltx_Math" display="inline" id="p3.5.5.2.2.m2.1"><semantics id="p3.5.5.2.2.m2.1a"><msup id="p3.5.5.2.2.m2.1.1" xref="p3.5.5.2.2.m2.1.1.cmml"><mi id="p3.5.5.2.2.m2.1.1a" xref="p3.5.5.2.2.m2.1.1.cmml"></mi><mtext class="undefined" id="p3.5.5.2.2.m2.1.1.1" mathsize="171%" mathvariant="bold" xref="p3.5.5.2.2.m2.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.5.5.2.2.m2.1.1.1.1nest">\textltailm</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.5.5.2.2.m2.1b"><apply id="p3.5.5.2.2.m2.1.1.cmml" xref="p3.5.5.2.2.m2.1.1"><ci id="p3.5.5.2.2.m2.1.1.1b.cmml" xref="p3.5.5.2.2.m2.1.1.1"><mtext class="undefined" id="p3.5.5.2.2.m2.1.1.1.cmml" mathvariant="bold" xref="p3.5.5.2.2.m2.1.1.1"><span class="ltx_ERROR undefined" id="p3.5.5.2.2.m2.1.1.1.1anest">\textltailm</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.5.5.2.2.m2.1c">{}^{\textrm{\textltailm}}</annotation><annotation encoding="application/x-llamapun" id="p3.5.5.2.2.m2.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="p3.9.9">
<td class="ltx_td ltx_align_center" id="p3.9.9.4">
<math alttext="{}^{\textrm{\textctc}}" class="ltx_Math" display="inline" id="p3.6.6.1.m1.1"><semantics id="p3.6.6.1.m1.1a"><msup id="p3.6.6.1.m1.1.1" xref="p3.6.6.1.m1.1.1.cmml"><mi id="p3.6.6.1.m1.1.1a" xref="p3.6.6.1.m1.1.1.cmml"></mi><mtext class="undefined" id="p3.6.6.1.m1.1.1.1" mathsize="142%" xref="p3.6.6.1.m1.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.6.6.1.m1.1.1.1.1nest">\textctc</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.6.6.1.m1.1b"><apply id="p3.6.6.1.m1.1.1.cmml" xref="p3.6.6.1.m1.1.1"><ci id="p3.6.6.1.m1.1.1.1b.cmml" xref="p3.6.6.1.m1.1.1.1"><mtext class="undefined" id="p3.6.6.1.m1.1.1.1.cmml" xref="p3.6.6.1.m1.1.1.1"><span class="ltx_ERROR undefined" id="p3.6.6.1.m1.1.1.1.1anest">\textctc</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.6.6.1.m1.1c">{}^{\textrm{\textctc}}</annotation><annotation encoding="application/x-llamapun" id="p3.6.6.1.m1.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math>Catalpa International    <math alttext="{}^{\textrm{\textltailm}}" class="ltx_Math" display="inline" id="p3.7.7.2.m2.1"><semantics id="p3.7.7.2.m2.1a"><msup id="p3.7.7.2.m2.1.1" xref="p3.7.7.2.m2.1.1.cmml"><mi id="p3.7.7.2.m2.1.1a" xref="p3.7.7.2.m2.1.1.cmml"></mi><mtext class="undefined" id="p3.7.7.2.m2.1.1.1" mathsize="142%" xref="p3.7.7.2.m2.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.7.7.2.m2.1.1.1.1nest">\textltailm</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.7.7.2.m2.1b"><apply id="p3.7.7.2.m2.1.1.cmml" xref="p3.7.7.2.m2.1.1"><ci id="p3.7.7.2.m2.1.1.1b.cmml" xref="p3.7.7.2.m2.1.1.1"><mtext class="undefined" id="p3.7.7.2.m2.1.1.1.cmml" xref="p3.7.7.2.m2.1.1.1"><span class="ltx_ERROR undefined" id="p3.7.7.2.m2.1.1.1.1anest">\textltailm</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.7.7.2.m2.1c">{}^{\textrm{\textltailm}}</annotation><annotation encoding="application/x-llamapun" id="p3.7.7.2.m2.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math>The University of Melbourne    <math alttext="{}^{\textrm{\textturnt}}" class="ltx_Math" display="inline" id="p3.8.8.3.m3.1"><semantics id="p3.8.8.3.m3.1a"><msup id="p3.8.8.3.m3.1.1" xref="p3.8.8.3.m3.1.1.cmml"><mi id="p3.8.8.3.m3.1.1a" xref="p3.8.8.3.m3.1.1.cmml"></mi><mtext class="undefined" id="p3.8.8.3.m3.1.1.1" mathsize="142%" xref="p3.8.8.3.m3.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.8.8.3.m3.1.1.1.1nest">\textturnt</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.8.8.3.m3.1b"><apply id="p3.8.8.3.m3.1.1.cmml" xref="p3.8.8.3.m3.1.1"><ci id="p3.8.8.3.m3.1.1.1b.cmml" xref="p3.8.8.3.m3.1.1.1"><mtext class="undefined" id="p3.8.8.3.m3.1.1.1.cmml" xref="p3.8.8.3.m3.1.1.1"><span class="ltx_ERROR undefined" id="p3.8.8.3.m3.1.1.1.1anest">\textturnt</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.8.8.3.m3.1c">{}^{\textrm{\textturnt}}</annotation><annotation encoding="application/x-llamapun" id="p3.8.8.3.m3.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math>Timorlink    <math alttext="{}^{\textrm{\textesh}}" class="ltx_Math" display="inline" id="p3.9.9.4.m4.1"><semantics id="p3.9.9.4.m4.1a"><msup id="p3.9.9.4.m4.1.1" xref="p3.9.9.4.m4.1.1.cmml"><mi id="p3.9.9.4.m4.1.1a" xref="p3.9.9.4.m4.1.1.cmml"></mi><mtext class="undefined" id="p3.9.9.4.m4.1.1.1" mathsize="142%" xref="p3.9.9.4.m4.1.1.1b.cmml"><span class="ltx_ERROR undefined" id="p3.9.9.4.m4.1.1.1.1nest">\textesh</span></mtext></msup><annotation-xml encoding="MathML-Content" id="p3.9.9.4.m4.1b"><apply id="p3.9.9.4.m4.1.1.cmml" xref="p3.9.9.4.m4.1.1"><ci id="p3.9.9.4.m4.1.1.1b.cmml" xref="p3.9.9.4.m4.1.1.1"><mtext class="undefined" id="p3.9.9.4.m4.1.1.1.cmml" xref="p3.9.9.4.m4.1.1.1"><span class="ltx_ERROR undefined" id="p3.9.9.4.m4.1.1.1.1anest">\textesh</span></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.9.9.4.m4.1c">{}^{\textrm{\textesh}}</annotation><annotation encoding="application/x-llamapun" id="p3.9.9.4.m4.1d">start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT</annotation></semantics></math>Seminario Menor Balide Dili</td>
</tr>
<tr class="ltx_tr" id="p3.9.10.1">
<td class="ltx_td ltx_align_center" id="p3.9.10.1.1">
<span class="ltx_text ltx_font_typewriter" id="p3.9.10.1.1.1">raphael.merx@gmail.com</span>  
<span class="ltx_text ltx_font_typewriter" id="p3.9.10.1.1.2">timorlink@hotmail.com</span>  
<span class="ltx_text ltx_font_typewriter" id="p3.9.10.1.1.3">amahmudi@student.unimelb.edu.au</span>
</td>
</tr>
<tr class="ltx_tr" id="p3.9.11.2">
<td class="ltx_td ltx_align_center" id="p3.9.11.2.1">
<span class="ltx_text ltx_font_typewriter" id="p3.9.11.2.1.1">leonberto372@gmail.com</span>  
<span class="ltx_text ltx_font_typewriter" id="p3.9.11.2.1.2">vylomovae@unimelb.edu.au</span>
</td>
</tr>
</tbody>
</table>
<br class="ltx_break"/>
<p class="ltx_p" id="p3.10"><span class="ltx_text ltx_font_italic" id="p3.10.1">Abstract content</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLM) have shown remarkable abilities to perform natural language processing (NLP) tasks they were not explicitly trained for, including named entity recognition <cite class="ltx_cite ltx_citemacro_cite">Mehta and Varma (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib29" title="">2023</a>)</cite>, text classification <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib36" title="">2023</a>)</cite>, text summarisation <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib51" title="">2023b</a>)</cite>, and machine translation <cite class="ltx_cite ltx_citemacro_cite">(Hendy et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib20" title="">2023</a>; Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib25" title="">2023</a>; Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib6" title="">2022</a>, MT)</cite>. LLMs can be competitive with traditional encoder-decoder MT models for high-resource languages, but lag behind traditional MT models when translating to and from low-resource languages <cite class="ltx_cite ltx_citemacro_cite">Robinson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib34" title="">2023</a>); Hendy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib20" title="">2023</a>); Garcia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib15" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While LLMs can achieve moderately high translation accuracy through zero-shot prompting <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib47" title="">2021</a>)</cite>, few-shot prompting can improve translation accuracy <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib50" title="">2023a</a>)</cite>. Research on the selection of example sentences for use in LLM prompts found that examples close to the source text do not always result in better translation than random examples <cite class="ltx_cite ltx_citemacro_cite">Vilar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib45" title="">2023</a>)</cite>, but that in-domain examples can improve accuracy for technical domains <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib2" title="">2023</a>)</cite>. In particular, for English to Kinyarwanda MT, <cite class="ltx_cite ltx_citemacro_citet">Moslem et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib30" title="">2023</a>)</cite> finds an improvement of 11 ChrF points when using in-domain examples instead of random ones.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Using domain adaptation as an analogy, in this paper we explore whether LLMs can be prompted to translate <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">into</em> a very low-resource language, through careful selection of sentences and words close to the source text for use in prompting. We work with the Mambai language, a primarily oral language from Timor-Leste with around 200,000 native speakers <cite class="ltx_cite ltx_citemacro_cite">Timor-Leste General Directorate of Statistics (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib41" title="">2015</a>)</cite>. We source prompt examples exclusively from <cite class="ltx_cite ltx_citemacro_citet">Hull (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#biba.bib2" title="">2001</a>)</cite>, a language manual which includes parallel English-Mambai sentences and a bilingual word dictionary. We evaluate machine translation quality on both a random subset of sentences from the manual, and on a small corpus of translations collected from a native Mambai speaker.
</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We find that translation accuracy varies a lot depending on (1) the test set used for evaluation, (2) LLM used for translation, and (3) examples included in the prompt. While 10-shot translation yields BLEU score as high as 23.5 for the test sentences sampled from the language manual used in prompting (with GPT-4 and a mix of sentences retrieved through semantic embeddings and TF-IDF in the prompt), BLEU drops below 5 across all experimental setups for test sentences outside of this domain (novel sentences collected from a native speaker).<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We release the code for extracting the language manual data and for using this data to construct a few-shot prompt given a sentence to translate, as well as the corpus of sentences translated by the paper’s author, in <a class="ltx_ref ltx_href" href="https://github.com/raphaelmerx/mambai" title="">https://github.com/raphaelmerx/mambai</a>. The language manual data is available upon request.</span></span></span></p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our findings highlight the risks of relying on a single source when evaluating MT for low-resource languages, especially for languages like Mambai that do not have a standardised vocabulary, orthography, or syntax, where a single corpus can have substantial influence on NLP experiments, despite not always being representative of the language’s variations.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   The Mambai Language</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Timor-Leste (also known as East Timor) is a half-island nation in South-East Asia, with a population of 1.3 million as of 2022 <cite class="ltx_cite ltx_citemacro_cite">Timor-Leste General Directorate of Statistics (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib42" title="">2022</a>)</cite>. While its official languages are Portuguese and Tetun Dili <cite class="ltx_cite ltx_citemacro_cite">(Government of Timor-Leste, <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib17" title="">2002</a>, also spelled Tetum)</cite>, the country has over 30 indigenous languages, from both the Austronesian and Papuan language families <cite class="ltx_cite ltx_citemacro_cite">Kingsbury (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib24" title="">2010</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Mambai (also spelled Mambae) is the country’s second most common mother tongue after Tetun, with around 200,000 native speakers <cite class="ltx_cite ltx_citemacro_cite">Timor-Leste General Directorate of Statistics (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib41" title="">2015</a>)</cite>. An Austronesian language, it is mostly spoken in the Ermera, Aileu, Manufahi, and Ainaro municipalities <cite class="ltx_cite ltx_citemacro_cite">Berlie (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib5" title="">2008</a>)</cite>, and does not have a standardised orthography <cite class="ltx_cite ltx_citemacro_citep">(Hull, <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#biba.bib2" title="">2001</a>)</cite>. It has three distinct varieties, and this article will focus on the southern variety, spoken primarily in the Ainaro, Same, and Hatu-Builico administrative posts <cite class="ltx_cite ltx_citemacro_cite">Fogaça (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib13" title="">2013</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Translating to Mambai can bring valuable material closer to Mambai-speaking communities. For example, the Government of Timor-Leste has a mother tongue education program named EMULI, which found that students who were taught in their mother tongue have a higher level in reading comprehension and mathematics than students taught in Portuguese. This program leverages translated material for the curriculum <cite class="ltx_cite ltx_citemacro_cite">Gusmão (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib18" title="">2023</a>); Walter (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib46" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Unfortunately, in the taxonomy of <cite class="ltx_cite ltx_citemacro_citet">Joshi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib22" title="">2020</a>)</cite>, Mambai would be assigned class 0, "The Left-Behinds", i.e. “languages that have been and are still ignored in the aspect of language technologies”. A search for Mambai sentences on OPUS <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib40" title="">2009</a>)</cite> returns only 36 sentences, all from Tatoeba.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://tatoeba.org/" title="">https://tatoeba.org/</a></span></span></span> To our knowledge, the only NLP tools that claim to support Mambai are language identification models GlotLID <cite class="ltx_cite ltx_citemacro_cite">Kargaran et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib23" title="">2023</a>)</cite> and MMS <cite class="ltx_cite ltx_citemacro_cite">Pratap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib32" title="">2023</a>)</cite>. Mambai does not appear on popular datasets for low-resource languages such as MT560 <cite class="ltx_cite ltx_citemacro_citep">(Gowda et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#biba.bib1" title="">2021</a>)</cite> or FLORES-200 evaluation benchmark <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#biba.bib3" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.F1.1" style="width:455.2pt;height:141.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.6pt,17.8pt) scale(0.798148725995781,0.798148725995781) ;"><svg class="ltx_picture" height="243.7" id="S2.F1.1.pic1" overflow="visible" version="1.1" width="775.39"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,243.7) matrix(1 0 0 -1 0 0) translate(54.1,0) translate(0,131.23)"><g fill="#B3B3FF"><path d="M -53.82 -9.45 h 107.65 v 18.91 h -107.65 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -49.21 -4.84)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Scan &amp; OCR</span></span>
</span></span></foreignobject></g><path d="M 215.55 9.45 L 118.97 9.45 C 115.91 9.45 113.43 6.98 113.43 3.92 L 113.43 -3.92 C 113.43 -6.98 115.91 -9.45 118.97 -9.45 L 215.55 -9.45 C 218.6 -9.45 221.08 -6.98 221.08 -3.92 L 221.08 3.92 C 221.08 6.98 218.6 9.45 215.55 9.45 Z M 113.43 -9.45" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.04 -4.84)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Word document</span></span>
</span></span></foreignobject></g><path d="M 215.55 104.58 L 118.97 104.58 C 115.91 104.58 113.43 102.1 113.43 99.04 L 113.43 74.6 C 113.43 71.54 115.91 69.06 118.97 69.06 L 215.55 69.06 C 218.6 69.06 221.08 71.54 221.08 74.6 L 221.08 99.04 C 221.08 102.1 218.6 104.58 215.55 104.58 Z M 113.43 69.06" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.04 90.28)"><foreignobject height="26.29" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Word files containing dictionaries</span></span>
</span></span></foreignobject></g><path d="M 215.55 -69.06 L 118.97 -69.06 C 115.91 -69.06 113.43 -71.54 113.43 -74.6 L 113.43 -108.56 C 113.43 -111.61 115.91 -114.09 118.97 -114.09 L 215.55 -114.09 C 218.6 -114.09 221.08 -111.61 221.08 -108.56 L 221.08 -74.6 C 221.08 -71.54 218.6 -69.06 215.55 -69.06 Z M 113.43 -114.09" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.04 -83.36)"><foreignobject height="35.8" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Word file containing parallel sentences</span></span>
</span></span></foreignobject></g><g fill="#B3B3FF"><path d="M 280.69 61.62 h 107.65 v 50.4 h -107.65 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 285.3 97.72)"><foreignobject height="41.18" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Extract dictionaries in json format using regex</span></span>
</span></span></foreignobject></g><g fill="#B3B3FF"><path d="M 280.69 -130.95 h 107.65 v 78.75 h -107.65 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 285.3 -66.5)"><foreignobject height="69.53" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Extract Mambai and English sentences in separate files using font weight hints</span></span>
</span></span></foreignobject></g><g fill="#FFB3B3"><path d="M 534.07 12.08 L 530.1 16.05 L 526.13 20.02 L 426.42 20.02 L 422.45 16.05 L 418.49 12.08 L 418.49 -12.08 L 422.45 -16.05 L 426.42 -20.02 L 526.13 -20.02 L 530.1 -16.05 L 534.07 -12.08 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 427.07 1.75)"><foreignobject height="22.88" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Hunalign: align sentences</span></span>
</span></span></foreignobject></g><path d="M 715.42 112.19 L 618.84 112.19 C 615.78 112.19 613.3 109.71 613.3 106.65 L 613.3 66.99 C 613.3 63.93 615.78 61.45 618.84 61.45 L 715.42 61.45 C 718.47 61.45 720.95 63.93 720.95 66.99 L 720.95 106.65 C 720.95 109.71 718.47 112.19 715.42 112.19 Z M 613.3 61.45" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 617.91 97.89)"><foreignobject height="41.51" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Dictionaries for English-Mambai and Mambai-English</span></span>
</span></span></foreignobject></g><path d="M 715.48 22.51 L 618.9 22.51 C 615.84 22.51 613.36 20.04 613.36 16.98 L 613.36 -16.98 C 613.36 -20.04 615.84 -22.51 618.9 -22.51 L 715.48 -22.51 C 718.53 -22.51 721.01 -20.04 721.01 -16.98 L 721.01 16.98 C 721.01 20.04 718.53 22.51 715.48 22.51 Z M 613.36 -22.51" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 617.98 8.22)"><foreignobject height="35.8" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S2.F1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S2.F1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.F1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S2.F1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S2.F1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Parallel aligned English-Mambai sentences</span></span>
</span></span></foreignobject></g><path d="M 54.1 0 L 106.79 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(1.0 0.0 0.0 1.0 106.79 0)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 167.26 9.73 L 167.26 62.42" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(0.0 1.0 -1.0 0.0 167.26 62.42)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 167.26 -9.73 L 167.26 -62.42" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(0.0 -1.0 1.0 0.0 167.26 -62.42)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 221.36 86.82 L 274.05 86.82" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(1.0 0.0 0.0 1.0 274.05 86.82)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 221.36 -91.58 L 274.05 -91.58" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(1.0 0.0 0.0 1.0 274.05 -91.58)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 388.62 86.82 L 476.28 86.82 L 476.28 26.66" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(0.0 -1.0 1.0 0.0 476.28 26.66)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 388.62 -91.58 L 476.28 -91.58 L 476.28 -26.66" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(0.0 1.0 -1.0 0.0 476.28 -26.66)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 534.35 0 L 606.72 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(1.0 0.0 0.0 1.0 606.72 0)"><path d="M 5.26 0 C 4.61 0.16 1.77 1.04 0 2.01 L 0 -2.01 C 1.77 -1.04 4.61 -0.16 5.26 0 Z"></path></g><path d="M 388.62 86.82 L 608.04 86.82" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 608.04 86.82)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our process for extracting dictionaries and a parallel corpus from the Mambai Language Manual</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Methodology for Data Extraction</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As the language does not have any resources in a machine-readable format, we start by digitising the available materials. The general process of data extraction is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S2.F1" title="Figure 1 ‣ 2. The Mambai Language ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="589" id="S3.F2.g1" src="extracted/5518521/Abbyy-Mambai.png" width="582"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Mambai configuration in ABBYY FineReader 15.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   Materials</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our primary data source is a Mambai Language Manual <cite class="ltx_cite ltx_citemacro_cite">Hull (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#biba.bib2" title="">2001</a>)</cite> that aims to teach the basics of Mambai to foreign speakers, following the Ainaro variety. This 109-page long document includes a pronunciation guide, a grammar, a phrase book, and bilingual dictionaries (English-Mambai and Mambai-English).<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The author of this book gave his consent to us using it as material, and we acknowledge him as the holder of copyright protecting this intellectual property.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To test generalisation of our results, we collaborated with a native Mambai speaker who translated a small corpus of 50 English sentences to Mambai. Since Mambai has no formalised orthography, we tried to keep orthography close to that used in the manual, however we did not aim to produce the same syntactic structures as the manual.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   OCR Process</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">For the Mambai Language Manual, which we received in paper format, we followed the following OCR process:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The book was scanned using an optical zoom camera, which reduces the radial distortion effect and improves the OCR quality;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The open-source ScanTailor software<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href" href="https://scantailor.org/" title="">https://scantailor.org/</a></span></span></span> was employed to semi-automatically deskew images and make them flat black and white;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">In the proprietary software ABBYY FineReader 15,<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_href" href="https://pdf.abbyy.com/" title="">https://pdf.abbyy.com/</a></span></span></span> we set up a language alphabet, taking into account the characters utilised in each book, with Indonesian (also an Austronesian language) serving as the fallback language, as illustrated on Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.F2" title="Figure 2 ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">2</span></a>. The result of the OCR process was saved in a Word document, preserving font formatting;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">We then manually separated the extracted data into three collections:
</p>
<ol class="ltx_enumerate" id="S3.I1.i4.I1">
<li class="ltx_item" id="S3.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S3.I1.i4.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i4.I1.i1.p1.1">the section of the manual that contains parallel sentences (14,347 words),</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="S3.I1.i4.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i4.I1.i2.p1.1">the section that contains the English to Mambai dictionary (4,023 words),</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="S3.I1.i4.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i4.I1.i3.p1.1">the section of the manual that contains the Mambai to English word dictionary (4,522 words).</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.3.   Text Corpora</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In this subsection, we present the process of our corpus construction: using the Word documents produced in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.SS2" title="3.2. OCR Process ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we create English-Mambai bilingual dictionaries in JSON format and a corpus of parallel English-Mambai sentences.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.3.1.   Dictionary extraction</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">For dictionary files, we mined triplets (entry, part of speech, translation) through the following process:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">using the python-docx library,<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_href" href="https://python-docx.readthedocs.io/" title="">https://python-docx.readthedocs.io/</a></span></span></span> read the file by preserving font weight, and identify text in bold as the dictionary entry;</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">use a regular expression to match the part of speech, if any;</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">use the rest of the text as value corresponding to the entry;</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">if one entry had multiple translations, denormalise them by splitting with “;” and “,”.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">This process outputs dictionaries in JSON format, one for the English to Mambai direction (1,790 entries), and one for the Mambai to English direction (1,592 entries). Where present, each entry also contains part of speech information, e.g.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS1.p3.1">
{
  ’entry’: ’beik’,
  ’translation’: ’silly’,
  ’part_of_speech’: ’adj.’
}
</pre>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.3.2.   Parallel sentence extraction</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Since no embedding models or MT systems support Mambai, we were precluded from relying on sentence embeddings <cite class="ltx_cite ltx_citemacro_cite">Thompson and Koehn (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib39" title="">2019</a>)</cite> or back-translations <cite class="ltx_cite ltx_citemacro_cite">Sennrich and Volk (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib35" title="">2011</a>)</cite> to mine parallel sentences from extracted documents. Instead, we rely on a combination of Gale-Church sentence-length information <cite class="ltx_cite ltx_citemacro_cite">Gale and Church (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib14" title="">1993</a>)</cite> and lexical similarity through the Hunalign<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_href" href="https://github.com/danielvarga/hunalign" title="">https://github.com/danielvarga/hunalign</a></span></span></span> sentence aligner <cite class="ltx_cite ltx_citemacro_cite">Varga et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib44" title="">2007</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">We identify Mambai sentences from their bold font-weight, English sentences from their normal font-weight, and section delimiters through text in upper case. For each section, we put the set of Mambai and English sentences in separate text files, which are fed to Hunalign, along with the bilingual dictionary extracted in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S3.SS3.SSS1" title="3.3.1. Dictionary extraction ‣ 3.3. Text Corpora ‣ 3. Methodology for Data Extraction ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>. Hunalign outputs a series of tab-delimited aligned sentence pairs, with an alignment score for each pair. After manual review of a subset of 100 sentences, we find that setting a score threshold of 0.2 corresponds to keeping a high number of well-aligned sentences, while removing poorly aligned ones. After filtering out sentence pairs below this threshold, we land on 1,187 parallel sentences extracted from this phrase book, from a total of 1,275 potential bitexts.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">Since sentences come from a language education manual, they tend to be relatively short, with an average of 5.05 words per sentence in Mambai, and 5.66 words per sentence in English. Some sentences have alternative words in parentheses, which we leave in place, e.g.:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS2.p4.1">
{
 ’Mambai’:
  "Balb ps masmidar lao xa (kaf).",
 ’English’:
  "Don’t put sugar in my tea (coffee)."
}
</pre>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Mambai Translation through Retrieval-Augmented LLM Prompting</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">After all required data is ready, we now turn to the machine translation part.
The general process for translation is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.F3" title="Figure 3 ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F3.1" style="width:455.2pt;height:128.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.7pt,18.1pt) scale(0.77872563362977,0.77872563362977) ;"><svg class="ltx_picture" height="226.42" id="S4.F3.1.pic1" overflow="visible" version="1.1" width="795.08"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,226.42) matrix(1 0 0 -1 0 0) translate(54.1,0) translate(0,193.33)"><path d="M 48.29 16.55 L -48.29 16.55 C -51.35 16.55 -53.82 14.07 -53.82 11.01 L -53.82 -11.01 C -53.82 -14.07 -51.35 -16.55 -48.29 -16.55 L 48.29 -16.55 C 51.35 -16.55 53.82 -14.07 53.82 -11.01 L 53.82 11.01 C 53.82 14.07 51.35 16.55 48.29 16.55 Z M -53.82 -16.55" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -49.21 2.25)"><foreignobject height="23.87" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Example sentences from manual</span></span>
</span></span></foreignobject></g><path d="M 48.29 -64.34 L -48.29 -64.34 C -51.35 -64.34 -53.82 -66.82 -53.82 -69.88 L -53.82 -91.9 C -53.82 -94.96 -51.35 -97.44 -48.29 -97.44 L 48.29 -97.44 C 51.35 -97.44 53.82 -94.96 53.82 -91.9 L 53.82 -69.88 C 53.82 -66.82 51.35 -64.34 48.29 -64.34 Z M -53.82 -97.44" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -49.21 -78.64)"><foreignobject height="23.87" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Input: English sentence from test set</span></span>
</span></span></foreignobject></g><path d="M 48.29 -145.23 L -48.29 -145.23 C -51.35 -145.23 -53.82 -147.71 -53.82 -150.77 L -53.82 -172.79 C -53.82 -175.85 -51.35 -178.33 -48.29 -178.33 L 48.29 -178.33 C 51.35 -178.33 53.82 -175.85 53.82 -172.79 L 53.82 -150.77 C 53.82 -147.71 51.35 -145.23 48.29 -145.23 Z M -53.82 -178.33" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -49.21 -159.53)"><foreignobject height="23.87" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Dictionary entries from manual</span></span>
</span></span></foreignobject></g><g fill="#B3B3FF"><path d="M 113.43 -32.81 h 107.65 v 65.62 h -107.65 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.04 18.51)"><foreignobject height="56.4" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Selection of example sentences where source is close to input</span></span>
</span></span></foreignobject></g><g fill="#B3B3FF"><path d="M 113.43 -193.05 h 107.65 v 62.55 h -107.65 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.04 -144.81)"><foreignobject height="53.32" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Filter dictionary entries for words that appear in input sentences</span></span>
</span></span></foreignobject></g><g fill="#B3B3FF"><path d="M 251.23 -113.7 h 107.65 v 65.62 h -107.65 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 255.84 -62.38)"><foreignobject height="56.4" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Prompt construction from example sentences and dictionary entries</span></span>
</span></span></foreignobject></g><g fill="#FFB3B3"><path d="M 553.76 -75.4 L 549.79 -71.44 L 545.82 -67.47 L 446.11 -67.47 L 442.14 -71.44 L 438.17 -75.4 L 438.17 -86.38 L 442.14 -90.34 L 446.11 -94.31 L 545.82 -94.31 L 549.79 -90.34 L 553.76 -86.38 Z"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 446.75 -85.73)"><foreignobject height="8.51" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">LLM</span></span>
</span></span></foreignobject></g><path d="M 735.16 -56.9 L 638.58 -56.9 C 635.53 -56.9 633.05 -59.38 633.05 -62.43 L 633.05 -99.35 C 633.05 -102.4 635.53 -104.88 638.58 -104.88 L 735.16 -104.88 C 738.22 -104.88 740.7 -102.4 740.7 -99.35 L 740.7 -62.43 C 740.7 -59.38 738.22 -56.9 735.16 -56.9 Z M 633.05 -104.88" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 637.66 -71.2)"><foreignobject height="38.76" overflow="visible" transform="matrix(1 0 0 -1 0 15.22)" width="98.43"><span class="ltx_inline-para ltx_minipage ltx_align_top" id="S4.F3.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1" style="width:71.1pt;">
<span class="ltx_para" id="S4.F3.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F3.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1.1"></span>
<span class="ltx_p" id="S4.F3.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text" id="S4.F3.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.p1.2.1" style="font-size:90%;">Output: translated sentence in Mambai</span></span>
</span></span></foreignobject></g><path d="M 54.1 0 L 108.18 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 108.18 0)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 54.1 -80.89 L 167.26 -80.89 L 167.26 -38.07" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 167.26 -38.07)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 54.1 -80.89 L 167.26 -80.89 L 167.26 -125.25" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 167.26 -125.25)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 54.1 -161.78 L 108.18 -161.78" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 108.18 -161.78)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 221.36 0 L 305.05 0 L 305.05 -42.82" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 305.05 -42.82)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 221.36 -161.78 L 305.05 -161.78 L 305.05 -118.96" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 305.05 -118.96)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 359.15 -80.89 L 432.91 -80.89" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 432.91 -80.89)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 554.03 -80.89 L 627.79 -80.89" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 627.79 -80.89)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of our process for translating English sentences to Mambai using both dictionary entries and sentence pairs in few-shot LLM prompting.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   Rationale</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Adelani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib1" title="">2022</a>)</cite> found that a couple thousand high-quality sentences can substantially increase low-resource MT performance, giving us hope that a language manual with a similar order of magnitude of data could be enough to produce moderate-quality translations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Working with LLM prompting gives us a flexible format to incorporate both the parallel sentence corpus and the dictionary entries. Further, having access to a phrase book offers substantial domain coverage, in comparison with corpora purely from the religious domain, which are often the only option for low-resource languages <cite class="ltx_cite ltx_citemacro_cite">Haddow et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib19" title="">2022</a>); Walter (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib46" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Here we work on English to Mambai translation, aiming to address the following research questions:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Given an English sentence, how can a corpus of bilingual sentences, and a bilingual word dictionary, be incorporated in an LLM prompt to maximise translation accuracy?</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Which LLMs (open-source or proprietary) show the best results for translating into a low-resource language, and what is the observed variance between them?</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">How does translation accuracy vary across test sets?</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.   Methodology</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.2.1.   Data setup</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Our bilingual corpus of 1,187 parallel Mambai-English sentences is randomly split into 119 (10%) sentences used for testing translation, and 1,068 (90%) sentences for potential use in the prompt, after retrieval selection. Since our objective is to translate full sentences, not individual words, all 1,790 words in the Mambai dictionary are used in prompting.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">We also assess translation system quality by providing a different test corpus of 50 sentences translated from English to Mambai by a native speaker of Mambai. This small corpus has relatively simple but slightly longer sentences, with 9 words per sentence on average. The English source sentences were designed to cover a broad range of domains, such as daily life activities, education, health and well-being, family relationships, religion, politics, weather, employment, food and agriculture, technology, personal characteristics, and Timor-Leste specific historical events.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">By using the two test sets, we aim to evaluate robustness to variance between domains, as well as estimate risks of overfitting that come from using a test corpus that comes from the same material as the data for prompting. Expected variance between test sets comes from their different authors, their different years of publication (2001 vs 2024), and potentially by them covering different domains.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.2.2.   Prompt</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">We make use of the best performing prompt template from <cite class="ltx_cite ltx_citemacro_citet">Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib31" title="">2023</a>)</cite>, to which we add dictionary entries for words found in the sentence, landing on the following prompt template:</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="S4.SS2.SSS2.p2.1">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,WW91IGFyZSBhIHRyYW5zbGF0b3IgZm9yIHRoZSBNYW1iYWkgbGFuZ3VhZ2UsIG9yaWdpbmFsbHkgZnJvbSBUaW1vci1MZXN0ZS4KCiMgRXhhbXBsZSBzZW50ZW5jZXMKCkVuZ2xpc2g6IHtTZW50X2VuZ18xfQpNYW1iYWk6IHtTZW50X21nbV8xfQoKRW5nbGlzaDogLi4uCk1hbWJhaTogLi4uCgojIERpY3Rpb25hcnkgZW50cmllcwoKRW5nbGlzaDoge1dvcmRfZW5nXzF9Ck1hbWJhaToge1dvcmRfbWdtXzF9CgpFbmdsaXNoOiAuLi4KTWFtYmFpOiAuLi4KClBsZWFzZSBwcm92aWRlIHRoZSB0cmFuc2xhdGlvbiBmb3IgdGhlIGZvbGxvd2luZyBzZW50ZW5jZS4gRG8gbm90IHByb3ZpZGUgYW55IGV4cGxhbmF0aW9ucyBvciB0ZXh0IGFwYXJ0IGZyb20gdGhlIHRyYW5zbGF0aW9uLgoKRW5nbGlzaDoge2lucHV0fQpNYW1iYWk6">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.5">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.7">translator</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.9">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.11">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.13">Mambai</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.15">language</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.16">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.18">originally</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.20">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.22">Timor</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.23">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.24">Leste</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.25">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx3.1">#</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.3">Example</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.5">sentences</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.1">English</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.5">Sent_eng_1</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.1">Mambai</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.5">Sent_mgm_1</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.1">English</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.4">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.1">Mambai</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.4">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx11.1">#</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.3">Dictionary</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.5">entries</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.1">English</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.5">Word_eng_1</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.1">Mambai</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5">Word_mgm_1</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.1">English</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.4">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.1">Mambai</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.4">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.1">Please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.3">provide</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.5">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.7">translation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.9">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.11">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.13">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.15">sentence</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.16">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.18">Do</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.20">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.22">provide</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.24">any</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.26">explanations</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.28">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.30">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.32">apart</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.34">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.36">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.37"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.38">translation</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.39">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx20">
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.1">English</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.5">input</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.1">Mambai</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.2">:</span>
</div>
</div>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.3"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T1.2.2.3.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1"><math alttext="N_{\text{TFIDF}}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml">N</mi><mtext id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3a.cmml">𝐓𝐅𝐈𝐃𝐅</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">𝑁</ci><ci id="S4.T1.1.1.1.m1.1.1.3a.cmml" xref="S4.T1.1.1.1.m1.1.1.3"><mtext id="S4.T1.1.1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.T1.1.1.1.m1.1.1.3">𝐓𝐅𝐈𝐃𝐅</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">N_{\text{TFIDF}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">italic_N start_POSTSUBSCRIPT TFIDF end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2"><math alttext="N_{\text{embed}}" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><msub id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml"><mi id="S4.T1.2.2.2.m1.1.1.2" xref="S4.T1.2.2.2.m1.1.1.2.cmml">N</mi><mtext id="S4.T1.2.2.2.m1.1.1.3" xref="S4.T1.2.2.2.m1.1.1.3a.cmml">𝐞𝐦𝐛𝐞𝐝</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><apply id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.m1.1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.2.2.2.m1.1.1.2.cmml" xref="S4.T1.2.2.2.m1.1.1.2">𝑁</ci><ci id="S4.T1.2.2.2.m1.1.1.3a.cmml" xref="S4.T1.2.2.2.m1.1.1.3"><mtext id="S4.T1.2.2.2.m1.1.1.3.cmml" mathsize="70%" xref="S4.T1.2.2.2.m1.1.1.3">𝐞𝐦𝐛𝐞𝐝</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">N_{\text{embed}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">italic_N start_POSTSUBSCRIPT embed end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T1.2.2.4.1">UseDict</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.5.1">BLEU</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.6.1">ChrF</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.7.1">ChrF++</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.3.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T1.2.3.1.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.3.1.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.3.1.2">0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.3.1.3">0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.3.1.4">FALSE</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.3.1.5">3.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.3.1.6">22.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.3.1.7">19.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.2">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.4.2.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.4.2.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.2.2">0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.2.3">0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.2.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.2.5">6.9</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.2.6">25.3</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.2.7">24.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5.3">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.5.3.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.5.3.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.3.2">10</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.3.3">0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.3.4">FALSE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.3.5">16.1</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.3.6">40.3</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.3.7">39.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6.4">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.6.4.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.6.4.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.4.2">10</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.4.3">0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.4.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.4.5">20.9</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.4.6">41.8</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.4.7">41.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7.5">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.7.5.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.7.5.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.5.2">0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.5.3">10</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.5.4">FALSE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.5.5">16.8</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.5.6">38.2</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.5.7">37.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.8.6">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.8.6.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.8.6.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.6.2">0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.6.3">10</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.6.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.6.5">18.3</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.6.6">39.6</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.6.7">39.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.9.7">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.9.7.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.9.7.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.9.7.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.9.7.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.9.7.4">FALSE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.9.7.5">17.7</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.9.7.6">40.4</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.9.7.7">39.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.10.8">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.10.8.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.10.8.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.10.8.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.10.8.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.10.8.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.10.8.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.10.8.5.1">21.2</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.10.8.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.10.8.6.1">41.8</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.10.8.7"><span class="ltx_text ltx_font_bold" id="S4.T1.2.10.8.7.1">41.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.11.9">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.11.9.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.11.9.1.1">Mixtral 8x7B</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.11.9.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.11.9.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.11.9.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.11.9.5">9.0</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.11.9.6">30.9</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.11.9.7">30.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.12.10">
<td class="ltx_td ltx_align_justify" id="S4.T1.2.12.10.1">
<p class="ltx_p ltx_align_top" id="S4.T1.2.12.10.1.1">LlaMa 70b</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.12.10.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.12.10.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.12.10.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.12.10.5">12.3</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.12.10.6">32.3</td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.12.10.7">31.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experiment results for test set from the language manual. <math alttext="N_{\text{TFIDF}}" class="ltx_Math" display="inline" id="S4.T1.5.m1.1"><semantics id="S4.T1.5.m1.1b"><msub id="S4.T1.5.m1.1.1" xref="S4.T1.5.m1.1.1.cmml"><mi id="S4.T1.5.m1.1.1.2" xref="S4.T1.5.m1.1.1.2.cmml">N</mi><mtext id="S4.T1.5.m1.1.1.3" xref="S4.T1.5.m1.1.1.3a.cmml">TFIDF</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.5.m1.1c"><apply id="S4.T1.5.m1.1.1.cmml" xref="S4.T1.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.5.m1.1.1.1.cmml" xref="S4.T1.5.m1.1.1">subscript</csymbol><ci id="S4.T1.5.m1.1.1.2.cmml" xref="S4.T1.5.m1.1.1.2">𝑁</ci><ci id="S4.T1.5.m1.1.1.3a.cmml" xref="S4.T1.5.m1.1.1.3"><mtext id="S4.T1.5.m1.1.1.3.cmml" mathsize="70%" xref="S4.T1.5.m1.1.1.3">TFIDF</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.m1.1d">N_{\text{TFIDF}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.m1.1e">italic_N start_POSTSUBSCRIPT TFIDF end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="N_{\text{embed}}" class="ltx_Math" display="inline" id="S4.T1.6.m2.1"><semantics id="S4.T1.6.m2.1b"><msub id="S4.T1.6.m2.1.1" xref="S4.T1.6.m2.1.1.cmml"><mi id="S4.T1.6.m2.1.1.2" xref="S4.T1.6.m2.1.1.2.cmml">N</mi><mtext id="S4.T1.6.m2.1.1.3" xref="S4.T1.6.m2.1.1.3a.cmml">embed</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.6.m2.1c"><apply id="S4.T1.6.m2.1.1.cmml" xref="S4.T1.6.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.6.m2.1.1.1.cmml" xref="S4.T1.6.m2.1.1">subscript</csymbol><ci id="S4.T1.6.m2.1.1.2.cmml" xref="S4.T1.6.m2.1.1.2">𝑁</ci><ci id="S4.T1.6.m2.1.1.3a.cmml" xref="S4.T1.6.m2.1.1.3"><mtext id="S4.T1.6.m2.1.1.3.cmml" mathsize="70%" xref="S4.T1.6.m2.1.1.3">embed</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.m2.1d">N_{\text{embed}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.m2.1e">italic_N start_POSTSUBSCRIPT embed end_POSTSUBSCRIPT</annotation></semantics></math> represent the number of sentence pairs retrieved through TF-IDF and semantic embeddings, respectively. <span class="ltx_text ltx_font_typewriter" id="S4.T1.8.1">UseDict</span> indicates whether dictionary entries are included in the prompt. While different hyperparameter combinations were tested for all models, we only report on the best configuration for the less performant models (Mistral 8x7B and LlaMa 70b).</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.2.3.   Models</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">We experiment with three models: <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p1.1.1">Mixtral</span> as it is the open-source model with the highest MT-bench score <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib21" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p1.1.2">LlaMa 70b</span> <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib43" title="">2023</a>)</cite> as it has a permissive license and has shown high zero-shot translation performance <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib48" title="">2024a</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p1.1.3">GPT-4</span>, which, despite being proprietary, has very high zero-shot translation performance <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib48" title="">2024a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">For each model, we experiment with the following setups:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I2.i1.p1.1.1">UseDict</span> (either <span class="ltx_text ltx_font_typewriter" id="S4.I2.i1.p1.1.2">True</span> or <span class="ltx_text ltx_font_typewriter" id="S4.I2.i1.p1.1.3">False</span>): For each word that appears in the source language input (English), if this word is present in the English-Mambai dictionary, we include its dictionary translation in the prompt;</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.2"><math alttext="N_{\text{TFIDF}}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.1.m1.1"><semantics id="S4.I2.i2.p1.1.m1.1a"><msub id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml"><mi id="S4.I2.i2.p1.1.m1.1.1.2" xref="S4.I2.i2.p1.1.m1.1.1.2.cmml">N</mi><mtext id="S4.I2.i2.p1.1.m1.1.1.3" xref="S4.I2.i2.p1.1.m1.1.1.3a.cmml">TFIDF</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><apply id="S4.I2.i2.p1.1.m1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I2.i2.p1.1.m1.1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.I2.i2.p1.1.m1.1.1.2.cmml" xref="S4.I2.i2.p1.1.m1.1.1.2">𝑁</ci><ci id="S4.I2.i2.p1.1.m1.1.1.3a.cmml" xref="S4.I2.i2.p1.1.m1.1.1.3"><mtext id="S4.I2.i2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.I2.i2.p1.1.m1.1.1.3">TFIDF</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">N_{\text{TFIDF}}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.1.m1.1d">italic_N start_POSTSUBSCRIPT TFIDF end_POSTSUBSCRIPT</annotation></semantics></math>: Number of sentence pairs retrieved through TF-IDF, where the English sentences are ranked according to TF-IDF similarity to the input. The rationale here is that less frequent words can be harder to translate, therefore should be surfaced in the prompt more often. <math alttext="N_{\text{TFIDF}}\in\{0,5,10\}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.2.m2.3"><semantics id="S4.I2.i2.p1.2.m2.3a"><mrow id="S4.I2.i2.p1.2.m2.3.4" xref="S4.I2.i2.p1.2.m2.3.4.cmml"><msub id="S4.I2.i2.p1.2.m2.3.4.2" xref="S4.I2.i2.p1.2.m2.3.4.2.cmml"><mi id="S4.I2.i2.p1.2.m2.3.4.2.2" xref="S4.I2.i2.p1.2.m2.3.4.2.2.cmml">N</mi><mtext id="S4.I2.i2.p1.2.m2.3.4.2.3" xref="S4.I2.i2.p1.2.m2.3.4.2.3a.cmml">TFIDF</mtext></msub><mo id="S4.I2.i2.p1.2.m2.3.4.1" xref="S4.I2.i2.p1.2.m2.3.4.1.cmml">∈</mo><mrow id="S4.I2.i2.p1.2.m2.3.4.3.2" xref="S4.I2.i2.p1.2.m2.3.4.3.1.cmml"><mo id="S4.I2.i2.p1.2.m2.3.4.3.2.1" stretchy="false" xref="S4.I2.i2.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S4.I2.i2.p1.2.m2.1.1" xref="S4.I2.i2.p1.2.m2.1.1.cmml">0</mn><mo id="S4.I2.i2.p1.2.m2.3.4.3.2.2" xref="S4.I2.i2.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S4.I2.i2.p1.2.m2.2.2" xref="S4.I2.i2.p1.2.m2.2.2.cmml">5</mn><mo id="S4.I2.i2.p1.2.m2.3.4.3.2.3" xref="S4.I2.i2.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S4.I2.i2.p1.2.m2.3.3" xref="S4.I2.i2.p1.2.m2.3.3.cmml">10</mn><mo id="S4.I2.i2.p1.2.m2.3.4.3.2.4" stretchy="false" xref="S4.I2.i2.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.2.m2.3b"><apply id="S4.I2.i2.p1.2.m2.3.4.cmml" xref="S4.I2.i2.p1.2.m2.3.4"><in id="S4.I2.i2.p1.2.m2.3.4.1.cmml" xref="S4.I2.i2.p1.2.m2.3.4.1"></in><apply id="S4.I2.i2.p1.2.m2.3.4.2.cmml" xref="S4.I2.i2.p1.2.m2.3.4.2"><csymbol cd="ambiguous" id="S4.I2.i2.p1.2.m2.3.4.2.1.cmml" xref="S4.I2.i2.p1.2.m2.3.4.2">subscript</csymbol><ci id="S4.I2.i2.p1.2.m2.3.4.2.2.cmml" xref="S4.I2.i2.p1.2.m2.3.4.2.2">𝑁</ci><ci id="S4.I2.i2.p1.2.m2.3.4.2.3a.cmml" xref="S4.I2.i2.p1.2.m2.3.4.2.3"><mtext id="S4.I2.i2.p1.2.m2.3.4.2.3.cmml" mathsize="70%" xref="S4.I2.i2.p1.2.m2.3.4.2.3">TFIDF</mtext></ci></apply><set id="S4.I2.i2.p1.2.m2.3.4.3.1.cmml" xref="S4.I2.i2.p1.2.m2.3.4.3.2"><cn id="S4.I2.i2.p1.2.m2.1.1.cmml" type="integer" xref="S4.I2.i2.p1.2.m2.1.1">0</cn><cn id="S4.I2.i2.p1.2.m2.2.2.cmml" type="integer" xref="S4.I2.i2.p1.2.m2.2.2">5</cn><cn id="S4.I2.i2.p1.2.m2.3.3.cmml" type="integer" xref="S4.I2.i2.p1.2.m2.3.3">10</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.2.m2.3c">N_{\text{TFIDF}}\in\{0,5,10\}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.2.m2.3d">italic_N start_POSTSUBSCRIPT TFIDF end_POSTSUBSCRIPT ∈ { 0 , 5 , 10 }</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.2"><math alttext="N_{\text{embed}}" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1"><semantics id="S4.I2.i3.p1.1.m1.1a"><msub id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml"><mi id="S4.I2.i3.p1.1.m1.1.1.2" xref="S4.I2.i3.p1.1.m1.1.1.2.cmml">N</mi><mtext id="S4.I2.i3.p1.1.m1.1.1.3" xref="S4.I2.i3.p1.1.m1.1.1.3a.cmml">embed</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><apply id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I2.i3.p1.1.m1.1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.I2.i3.p1.1.m1.1.1.2.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2">𝑁</ci><ci id="S4.I2.i3.p1.1.m1.1.1.3a.cmml" xref="S4.I2.i3.p1.1.m1.1.1.3"><mtext id="S4.I2.i3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.I2.i3.p1.1.m1.1.1.3">embed</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">N_{\text{embed}}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.1.m1.1d">italic_N start_POSTSUBSCRIPT embed end_POSTSUBSCRIPT</annotation></semantics></math>: Number of sentence pairs retrieved through LASER semantic embeddings <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib43" title="">2023</a>)</cite>, where the English sentences in training set are first ranked using cosine similarity to the input. <math alttext="N_{\text{embed}}\in\{0,5,10\}" class="ltx_Math" display="inline" id="S4.I2.i3.p1.2.m2.3"><semantics id="S4.I2.i3.p1.2.m2.3a"><mrow id="S4.I2.i3.p1.2.m2.3.4" xref="S4.I2.i3.p1.2.m2.3.4.cmml"><msub id="S4.I2.i3.p1.2.m2.3.4.2" xref="S4.I2.i3.p1.2.m2.3.4.2.cmml"><mi id="S4.I2.i3.p1.2.m2.3.4.2.2" xref="S4.I2.i3.p1.2.m2.3.4.2.2.cmml">N</mi><mtext id="S4.I2.i3.p1.2.m2.3.4.2.3" xref="S4.I2.i3.p1.2.m2.3.4.2.3a.cmml">embed</mtext></msub><mo id="S4.I2.i3.p1.2.m2.3.4.1" xref="S4.I2.i3.p1.2.m2.3.4.1.cmml">∈</mo><mrow id="S4.I2.i3.p1.2.m2.3.4.3.2" xref="S4.I2.i3.p1.2.m2.3.4.3.1.cmml"><mo id="S4.I2.i3.p1.2.m2.3.4.3.2.1" stretchy="false" xref="S4.I2.i3.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S4.I2.i3.p1.2.m2.1.1" xref="S4.I2.i3.p1.2.m2.1.1.cmml">0</mn><mo id="S4.I2.i3.p1.2.m2.3.4.3.2.2" xref="S4.I2.i3.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S4.I2.i3.p1.2.m2.2.2" xref="S4.I2.i3.p1.2.m2.2.2.cmml">5</mn><mo id="S4.I2.i3.p1.2.m2.3.4.3.2.3" xref="S4.I2.i3.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S4.I2.i3.p1.2.m2.3.3" xref="S4.I2.i3.p1.2.m2.3.3.cmml">10</mn><mo id="S4.I2.i3.p1.2.m2.3.4.3.2.4" stretchy="false" xref="S4.I2.i3.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.2.m2.3b"><apply id="S4.I2.i3.p1.2.m2.3.4.cmml" xref="S4.I2.i3.p1.2.m2.3.4"><in id="S4.I2.i3.p1.2.m2.3.4.1.cmml" xref="S4.I2.i3.p1.2.m2.3.4.1"></in><apply id="S4.I2.i3.p1.2.m2.3.4.2.cmml" xref="S4.I2.i3.p1.2.m2.3.4.2"><csymbol cd="ambiguous" id="S4.I2.i3.p1.2.m2.3.4.2.1.cmml" xref="S4.I2.i3.p1.2.m2.3.4.2">subscript</csymbol><ci id="S4.I2.i3.p1.2.m2.3.4.2.2.cmml" xref="S4.I2.i3.p1.2.m2.3.4.2.2">𝑁</ci><ci id="S4.I2.i3.p1.2.m2.3.4.2.3a.cmml" xref="S4.I2.i3.p1.2.m2.3.4.2.3"><mtext id="S4.I2.i3.p1.2.m2.3.4.2.3.cmml" mathsize="70%" xref="S4.I2.i3.p1.2.m2.3.4.2.3">embed</mtext></ci></apply><set id="S4.I2.i3.p1.2.m2.3.4.3.1.cmml" xref="S4.I2.i3.p1.2.m2.3.4.3.2"><cn id="S4.I2.i3.p1.2.m2.1.1.cmml" type="integer" xref="S4.I2.i3.p1.2.m2.1.1">0</cn><cn id="S4.I2.i3.p1.2.m2.2.2.cmml" type="integer" xref="S4.I2.i3.p1.2.m2.2.2">5</cn><cn id="S4.I2.i3.p1.2.m2.3.3.cmml" type="integer" xref="S4.I2.i3.p1.2.m2.3.3">10</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.2.m2.3c">N_{\text{embed}}\in\{0,5,10\}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.2.m2.3d">italic_N start_POSTSUBSCRIPT embed end_POSTSUBSCRIPT ∈ { 0 , 5 , 10 }</annotation></semantics></math>, similar to <cite class="ltx_cite ltx_citemacro_citep">Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib50" title="">2023a</a>; Vilar et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib45" title="">2023</a>; Hendy et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib20" title="">2023</a></cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">For each combination of the above features, we measure the BLEU and Chrf++ scores on both test sets, one from the language manual, and one manually translated by a native speaker.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.3.   Translation Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Our experiment results for test sentences from the manual are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.T1" title="Table 1 ‣ 4.2.2. Prompt ‣ 4.2. Methodology ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">1</span></a>, and Table <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.T2" title="Table 2 ‣ 4.3. Translation Results ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">2</span></a> provides the results for the test set collected from a native speaker.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">To summarise, we make the following observations:</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">(1) Translation accuracy varies widely between both test sets</span>. While we get an accuracy of up to 23.5 BLEU (41.9 ChrF++) for the test set that comes from the language manual, we could not reach a BLEU higher than 4.4 (33.1 ChrF++) for the test set from the native speaker. More analysis is needed to understand this discrepancy, but it sends a strong signal about the risks of overfitting by using a test set that comes from the same material as the examples used in prompting. In particular, we think our result might partially invalidate <cite class="ltx_cite ltx_citemacro_cite">Tanzer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib37" title="">2024</a>)</cite>, which similarly attempts to translate into a very low-resource language using prompting from a single grammar book, but used exclusively sentences from the grammar book in the test set.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">(2) Dictionary entries help improve translation quality</span>. When including dictionary entries in the prompt, filtering on words that appear in the source text, we found that translation quality improved significantly. This is true across all experiments when keeping other hyperparameters constant, with an average improvement of 3.25 BLEU points and 2.7 ChrF++ points.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.1.1">(3) A blend of sentences retrieved through semantic embeddings and through TF-IDF yields the highest translation accuracy</span>. When working with a random split of sentences from the language manual in particular, a blend of 5 sentences retrieved through TF-IDF and 5 sentences retrieved through semantic embeddings outperforms 10 sentences retrieved exclusively through one of these features. This holds true for all three LLMs tested in this project.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.2"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.2.1">(4) GPT-4 consistently outperforms other LLMs</span>. GPT-4 yields both the highest translation score overall, and the higher translation score for every single experiment, when compared with LlaMa 70b and Mixtral 8x7B while keeping <math alttext="N_{\text{TFIDF}}" class="ltx_Math" display="inline" id="S4.SS3.p6.1.m1.1"><semantics id="S4.SS3.p6.1.m1.1a"><msub id="S4.SS3.p6.1.m1.1.1" xref="S4.SS3.p6.1.m1.1.1.cmml"><mi id="S4.SS3.p6.1.m1.1.1.2" xref="S4.SS3.p6.1.m1.1.1.2.cmml">N</mi><mtext id="S4.SS3.p6.1.m1.1.1.3" xref="S4.SS3.p6.1.m1.1.1.3a.cmml">TFIDF</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.1.m1.1b"><apply id="S4.SS3.p6.1.m1.1.1.cmml" xref="S4.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p6.1.m1.1.1.1.cmml" xref="S4.SS3.p6.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p6.1.m1.1.1.2.cmml" xref="S4.SS3.p6.1.m1.1.1.2">𝑁</ci><ci id="S4.SS3.p6.1.m1.1.1.3a.cmml" xref="S4.SS3.p6.1.m1.1.1.3"><mtext id="S4.SS3.p6.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p6.1.m1.1.1.3">TFIDF</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.1.m1.1c">N_{\text{TFIDF}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p6.1.m1.1d">italic_N start_POSTSUBSCRIPT TFIDF end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="N_{\text{embed}}" class="ltx_Math" display="inline" id="S4.SS3.p6.2.m2.1"><semantics id="S4.SS3.p6.2.m2.1a"><msub id="S4.SS3.p6.2.m2.1.1" xref="S4.SS3.p6.2.m2.1.1.cmml"><mi id="S4.SS3.p6.2.m2.1.1.2" xref="S4.SS3.p6.2.m2.1.1.2.cmml">N</mi><mtext id="S4.SS3.p6.2.m2.1.1.3" xref="S4.SS3.p6.2.m2.1.1.3a.cmml">embed</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.2.m2.1b"><apply id="S4.SS3.p6.2.m2.1.1.cmml" xref="S4.SS3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p6.2.m2.1.1.1.cmml" xref="S4.SS3.p6.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p6.2.m2.1.1.2.cmml" xref="S4.SS3.p6.2.m2.1.1.2">𝑁</ci><ci id="S4.SS3.p6.2.m2.1.1.3a.cmml" xref="S4.SS3.p6.2.m2.1.1.3"><mtext id="S4.SS3.p6.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p6.2.m2.1.1.3">embed</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.2.m2.1c">N_{\text{embed}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p6.2.m2.1d">italic_N start_POSTSUBSCRIPT embed end_POSTSUBSCRIPT</annotation></semantics></math> constant.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.3"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T2.2.2.3.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1"><math alttext="N_{\text{TFIDF}}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml">N</mi><mtext id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3a.cmml">𝐓𝐅𝐈𝐃𝐅</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">𝑁</ci><ci id="S4.T2.1.1.1.m1.1.1.3a.cmml" xref="S4.T2.1.1.1.m1.1.1.3"><mtext id="S4.T2.1.1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.T2.1.1.1.m1.1.1.3">𝐓𝐅𝐈𝐃𝐅</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">N_{\text{TFIDF}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">italic_N start_POSTSUBSCRIPT TFIDF end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2"><math alttext="N_{\text{embed}}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><msub id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.m1.1.1.2.cmml">N</mi><mtext id="S4.T2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.m1.1.1.3a.cmml">𝐞𝐦𝐛𝐞𝐝</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.m1.1.1.2">𝑁</ci><ci id="S4.T2.2.2.2.m1.1.1.3a.cmml" xref="S4.T2.2.2.2.m1.1.1.3"><mtext id="S4.T2.2.2.2.m1.1.1.3.cmml" mathsize="70%" xref="S4.T2.2.2.2.m1.1.1.3">𝐞𝐦𝐛𝐞𝐝</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">N_{\text{embed}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">italic_N start_POSTSUBSCRIPT embed end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.2.2.4.1">UseDict</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.5.1">BLEU</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.6.1">ChrF</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.7.1">ChrF++</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.3.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.2.3.1.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.3.1.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.3.1.2">0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.3.1.3">0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.3.1.4">TRUE</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.3.1.5">3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.3.1.6">30.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.3.1.7">27.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.2">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.4.2.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.4.2.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.2.2">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.2.3">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.2.4">FALSE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.2.5">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.2.6">30.8</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.2.7">26.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.3">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.5.3.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.5.3.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.5.3.2">10</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.5.3.3">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.5.3.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.5.3.5">4</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.5.3.6">36.9</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.5.3.7">33.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.4">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.6.4.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.6.4.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.4.2">10</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.4.3">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.4.4">FALSE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.4.5">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.4.6">33.4</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.4.7">29.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.5">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.7.5.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.7.5.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.7.5.2">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.7.5.3">10</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.7.5.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.7.5.5">3.4</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.7.5.6">34.5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.7.5.7">31.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8.6">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.8.6.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.8.6.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.8.6.2">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.8.6.3">10</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.8.6.4">FALSE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.8.6.5">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.8.6.6">31.4</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.8.6.7">27.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.9.7">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.9.7.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.9.7.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.9.7.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.9.7.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.9.7.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.9.7.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.9.7.5.1">4.4</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.9.7.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.9.7.6.1">35.9</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.9.7.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.9.7.7.1">33</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.10.8">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.10.8.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.10.8.1.1">gpt-4-turbo</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.8.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.8.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.8.4">FALSE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.8.5">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.8.6">33.7</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.8.7">29.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.11.9">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.11.9.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.11.9.1.1">Mixtral 8x7B</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.11.9.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.11.9.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.11.9.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.11.9.5">3.5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.11.9.6">26.8</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.11.9.7">24.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.12.10">
<td class="ltx_td ltx_align_justify" id="S4.T2.2.12.10.1">
<p class="ltx_p ltx_align_top" id="S4.T2.2.12.10.1.1">LlaMa 70b</p>
</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.12.10.2">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.12.10.3">5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.12.10.4">TRUE</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.12.10.5">0</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.12.10.6">27.7</td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.12.10.7">24.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experiment results for the minicorpus of translations collected from a native Mambai speaker. <math alttext="N_{\text{TFIDF}}" class="ltx_Math" display="inline" id="S4.T2.5.m1.1"><semantics id="S4.T2.5.m1.1b"><msub id="S4.T2.5.m1.1.1" xref="S4.T2.5.m1.1.1.cmml"><mi id="S4.T2.5.m1.1.1.2" xref="S4.T2.5.m1.1.1.2.cmml">N</mi><mtext id="S4.T2.5.m1.1.1.3" xref="S4.T2.5.m1.1.1.3a.cmml">TFIDF</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.5.m1.1c"><apply id="S4.T2.5.m1.1.1.cmml" xref="S4.T2.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.5.m1.1.1.1.cmml" xref="S4.T2.5.m1.1.1">subscript</csymbol><ci id="S4.T2.5.m1.1.1.2.cmml" xref="S4.T2.5.m1.1.1.2">𝑁</ci><ci id="S4.T2.5.m1.1.1.3a.cmml" xref="S4.T2.5.m1.1.1.3"><mtext id="S4.T2.5.m1.1.1.3.cmml" mathsize="70%" xref="S4.T2.5.m1.1.1.3">TFIDF</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.m1.1d">N_{\text{TFIDF}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.m1.1e">italic_N start_POSTSUBSCRIPT TFIDF end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="N_{\text{embed}}" class="ltx_Math" display="inline" id="S4.T2.6.m2.1"><semantics id="S4.T2.6.m2.1b"><msub id="S4.T2.6.m2.1.1" xref="S4.T2.6.m2.1.1.cmml"><mi id="S4.T2.6.m2.1.1.2" xref="S4.T2.6.m2.1.1.2.cmml">N</mi><mtext id="S4.T2.6.m2.1.1.3" xref="S4.T2.6.m2.1.1.3a.cmml">embed</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.6.m2.1c"><apply id="S4.T2.6.m2.1.1.cmml" xref="S4.T2.6.m2.1.1"><csymbol cd="ambiguous" id="S4.T2.6.m2.1.1.1.cmml" xref="S4.T2.6.m2.1.1">subscript</csymbol><ci id="S4.T2.6.m2.1.1.2.cmml" xref="S4.T2.6.m2.1.1.2">𝑁</ci><ci id="S4.T2.6.m2.1.1.3a.cmml" xref="S4.T2.6.m2.1.1.3"><mtext id="S4.T2.6.m2.1.1.3.cmml" mathsize="70%" xref="S4.T2.6.m2.1.1.3">embed</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m2.1d">N_{\text{embed}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.m2.1e">italic_N start_POSTSUBSCRIPT embed end_POSTSUBSCRIPT</annotation></semantics></math> represent the number of sentence pairs retrieved through TF-IDF and semantic embeddings, respectively. <span class="ltx_text ltx_font_typewriter" id="S4.T2.8.1">UseDict</span> indicates whether dictionary entries are included in the prompt. While different hyperparameter combinations were tested for all models, we only report on the best configuration for the less performant models (Mistral 8x7B and LlaMa 70b).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.4.   Error analysis</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">We find that the large gap in performance across test sets is mostly due to differences in translation output, rather than differences in the source English text (Table <a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#S4.T3" title="Table 3 ‣ 4.4. Error analysis ‣ 4. Mambai Translation through Retrieval-Augmented LLM Prompting ‣ Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language"><span class="ltx_text ltx_ref_tag">3</span></a>):</p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Using TF-IDF representations of English sentences, we computed the cosine similarity in the whole training set and the two tests sets, resulting in 0.021 for the manual test set and 0.017 for the native speaker test set, a relatively small difference. For the Mambai target reference, however, we get a 0.027 and 0.012 for the manual and native speaker’s test sets, respectively, a much larger difference.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">LASER Semantic similarity between each test set and the training set are roughly equivalent at 0.42 and 0.40 for the manual and native speaker’s test sets, respectively, on the English source side.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Similarity</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.2.1.1">ManualTest x Train</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2">eng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3">TF-IDF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4">0.021</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.3.2.1">NativeTest x Train</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2">eng</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3">TF-IDF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4">0.017</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.3.1">ManualTest x Train</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.2">mgm</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.3">TF-IDF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.4">0.027</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.4.1">NativeTest x Train</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.2">mgm</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.3">TF-IDF</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.4">0.012</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.6.5.1">ManualTest x Train</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.2">eng</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.3">Semantic</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.4">0.42</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.1.7.6.1">NativeTest x Train</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.6.2">eng</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.6.3">Semantic</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.6.4">0.40</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Similarity scores using TF-IDF cosine similarity and LASER semantic cosine similarity between the two test sets and the training set for English (source, eng) and Mambai (target, mgm) sentences.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Through manual review of the translation differences in both test sets, we further identify the following potential causes for the large discrepancy in translation quality metrics:</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.1.1">(1) Literal vs figurative translation</span>: As sentences in the language manual are made for learning, they tend to use more literal translations, which correspond to what LLMs produce. On the other hand, our test set translated by a native speaker often uses more idiosyncratic translation, further away from words used in from the source input.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">(2) Language variation</span>: The Mambai language has changed since 2001, when the Mambai Language Manual was published. In particular, we noted more usage of Portuguese and Tetun Dili words in our test set reference sentences, which might indicate that Mambai speakers mix more Tetun Dili and Portuguese in their Mambai since the two languages were chosen as official in the 2002 Constitution <cite class="ltx_cite ltx_citemacro_cite">Government of Timor-Leste (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib17" title="">2002</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p5.1.1">(3) Spelling</span>: Despite trying to stay close to spelling used in the Mambai Language Manual, we found that our test set at times uses different spelling than the language manual (e.g. less hyphenation, some letters missing). This reinforces our view that oral languages like Mambai are better covered by speech datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Traditionally, neural MT systems are trained on parallel corpora of aligned sentence pairs <cite class="ltx_cite ltx_citemacro_cite">Duong (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib10" title="">2017</a>)</cite>. Low-resource languages tend to have orders of magnitude less sentences available than higher-resource languages <cite class="ltx_cite ltx_citemacro_cite">Arivazhagan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib4" title="">2019</a>)</cite>. To compensate for this lack of data, previous research found that low-resource MT accuracy can be improved through leveraging multilingual translation models that include better-resourced but related languages <cite class="ltx_cite ltx_citemacro_cite">Arivazhagan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib4" title="">2019</a>); Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib12" title="">2020</a>); Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib38" title="">2022</a>)</cite>. Other techniques include pre-training on monolingual data <cite class="ltx_cite ltx_citemacro_cite">Lample et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib27" title="">2018</a>)</cite>, the incorporation of audio data that shares an embedding space with text data <cite class="ltx_cite ltx_citemacro_cite">Communication et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib8" title="">2023</a>)</cite>, and the generation of synthetic parallel sentences <cite class="ltx_cite ltx_citemacro_cite">Edunov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib11" title="">2018</a>)</cite>, including by leveraging bilingual dictionaries <cite class="ltx_cite ltx_citemacro_cite">Duan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib9" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In parallel, large language models have shown increased ability to translate, at times surpassing specialised encoder-decoder MT systems <cite class="ltx_cite ltx_citemacro_cite">Robinson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib34" title="">2023</a>)</cite>. Finding the right prompt recipe for increased MT accuracy using LLMs has been a topic of research <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib50" title="">2023a</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib28" title="">2022</a>)</cite>, with findings that few-shot prompting often improves MT accuracy <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib50" title="">2023a</a>)</cite>, and that the type of sentences used as few-shot examples can have a large influence on accuracy <cite class="ltx_cite ltx_citemacro_cite">Moslem et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib30" title="">2023</a>)</cite>. Dynamic adaptation of the prompt by retrieving example sentences that are close to the input text <cite class="ltx_cite ltx_citemacro_cite">Kumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib26" title="">2023</a>)</cite>, or dictionary entries for words that appear in the source <cite class="ltx_cite ltx_citemacro_cite">Ghazvininejad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib16" title="">2023</a>)</cite> can further improve MT accuracy.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The applicability of common LLM prompting techniques when translating into very low-resource languages is unclear, given these languages might not be represented at all during LLM pretraining. <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib37" title="">2024</a>)</cite> partially addresses this issue by focusing on MT between English and Kalamang, an endangered Papuan language, using a single grammar book. Experimenting with different models (Claude 2, LlaMa, gpt-3.5, gpt-4), and different prompt setups (injecting sentences close to the input, dictionary entries, and the grammar explanations found in the book), they achieve up to 45.8 ChrF on the English to Kalamang direction. However, they work with a test set that is a random subset of sentences found in the book, raising issues around the applicability of their results to text translated by a different author, or to domains not covered in the grammar book.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Recognising the potential of LLMs for MT, and the importance of in-context examples used in prompting, our work experiments with retrieval-augmented LLM prompting for translation into a low-resource language. We test translation quality on both a subset of sentences coming from the language manual used as corpus, and a test set specially translated by a native Mambai speaker for this project.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx1.1.1" style="font-size:120%;">Conclusion</span></h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">In this paper, we introduced a novel corpus for the Mambai language, a language with around 200,000 native speakers that had virtually no NLP resources. Our corpus includes bilingual dictionaries in both directions for English-Mambai, a set of 1,187 parallel sentences from a language manual published in 2001, and a set of 50 parallel sentences translated by a native Mambai speaker. Our experiments on few-shot LLM prompting for English to Mambai translation showed that moderate MT quality can be achieved for test sentences very close to the original corpus, but MT quality decreases significantly for sentences that come from a separate corpus, thus highlighting the need for using test sets that do not come from the same material as original examples used in prompting. We think LLMs offer a flexible approach for integrating scarce resources in different formats (dictionary entries, parallel sentences), and few-shot prompting shows potential in improving low-resource MT using general purpose LLMs.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx2.1.1" style="font-size:120%;">Limitations</span></h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The sentences used in both training set (from the Mambai Language Manual) and test sets tend to be rather short and simple, which raises questions around translation quality for longer sentences, or for technical domains that get little coverage in our corpus (e.g. health or legal text).</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">Mambai has no standard orthography. Even though the native Mambai speaker we collaborated with tried to follow spelling close to that used in the language manual, we expect that variances in spelling still negatively impacted the test BLEU score. This stresses the need for heightened focus on audio for primarily spoken languages like Mambai <cite class="ltx_cite ltx_citemacro_cite">Chrupała (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib7" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">While we were able to gather a test set from a native Mambai speaker, they did not evaluate translation quality for MT-translated text; instead we relied solely on automated MT metrics. While BLEU tends to be a reliable measure of MT quality for morphologically simple languages like Mambai <cite class="ltx_cite ltx_citemacro_cite">Reiter (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib33" title="">2018</a>)</cite>, we would have preferred to dig deeper into the shortcomings of our LLM-generated translations.</p>
</div>
<div class="ltx_para" id="Sx2.p4">
<p class="ltx_p" id="Sx2.p4.1">Lastly, Mambai has a simple grammar and morphology, which might make it particularly prone to MT quality improvement using few-shot prompting. Therefore, our results might not translate well on more morphologically complex languages.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx3.1.1" style="font-size:120%;">Future Work</span></h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">This work focused solely on Mambai, without leveraging resources from related languages that have more resources, such as Tetun Dili, Portuguese, or Indonesian. In future work, we would like to investigate the addition of Tetun Dili sentences to the prompt, especially for domain-specific text that might be very poorly covered by our small Mambai corpus, but that could be covered by a larger Tetun Dili corpus.
</p>
</div>
<div class="ltx_para" id="Sx3.p2">
<p class="ltx_p" id="Sx3.p2.1">In terms of finding the right recipe for prompting, future endeavours could use a more systematic approach, similar to <cite class="ltx_cite ltx_citemacro_citet">Kumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib26" title="">2023</a>)</cite> which uses a regression model for example selection. Additionally, more retrieval techniques could be tested, e.g. bag of words, or even ChrF similarity between the input and English source side.</p>
</div>
<div class="ltx_para" id="Sx3.p3">
<p class="ltx_p" id="Sx3.p3.1">In this paper, we used general purpose LLMs that likely saw little to no Mambai text during pretraining. We think future work could experiment with continuous pretraining on Mambai, or languages related to Mambai, before prompting, similar to approaches in <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib49" title="">2024b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Alves et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.04809v1#bib.bib3" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx4.1.1" style="font-size:120%;">Acknowledgements</span></h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">We thank Pr. Geoffrey Hull (Macquarie University), author of the Mambai Language Manual, for authorising usage of his work as part of this research. Pr. Hull remains the holder of copyright protecting this intellectual property.</p>
</div>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx5.1.1" style="font-size:120%;">References</span></h2>
<span class="ltx_ERROR undefined" id="Sx5.2">\c@NAT@ctr</span>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2022)</span>
<span class="ltx_bibblock">
David Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel
Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen
Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong,
Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre
Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair
Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed
Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,
Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste
Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris
Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula,
and Sam Manthalu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.naacl-main.223" title="">A few
thousand translations go a long way! leveraging pre-trained models for
African news translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 3053–3070, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2023)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan
Ghazvininejad. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.564" title="">In-context
examples selection for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 8857–8873, Toronto, Canada. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves et al. (2024)</span>
<span class="ltx_bibblock">
Duarte M. Alves, José Pombal, Nuno M. Guerreiro, Pedro H. Martins, João
Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta
Agrawal, Pierre Colombo, José G. C. de Souza, and André F. T. Martins.
2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.17733" title="">Tower: An open multilingual
large language model for translation-related tasks</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arivazhagan et al. (2019)</span>
<span class="ltx_bibblock">
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson,
Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang
Macherey, Zhifeng Chen, and Yonghui Wu. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1907.05019" title="">Massively multilingual
neural machine translation in the wild: Findings and challenges</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berlie (2008)</span>
<span class="ltx_bibblock">
Berlie. 2008.

</span>
<span class="ltx_bibblock">Notes on east timor: Languages and education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Asian Journal of Social Science</em>, 36(3-4):629–637.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai,
Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2204.02311" title="">Palm: Scaling language
modeling with pathways</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chrupała (2023)</span>
<span class="ltx_bibblock">
Grzegorz Chrupała. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.495" title="">Putting
natural in natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 7820–7827, Toronto, Canada. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication et al. (2023)</span>
<span class="ltx_bibblock">
Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli,
David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong,
Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht,
Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek,
Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis,
Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes,
Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi,
Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan
Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna
Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin
Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur
Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee,
Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah
Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler
Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.11596" title="">Seamlessm4t: Massively
multilingual &amp; multimodal machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al. (2020)</span>
<span class="ltx_bibblock">
Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min Zhang, Boxing Chen, Weihua Luo,
and Yue Zhang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2007.02671" title="">Bilingual dictionary based
neural machine translation without using parallel sentences</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duong (2017)</span>
<span class="ltx_bibblock">
Long Duong. 2017.

</span>
<span class="ltx_bibblock">Natural language processing for resource-poor languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">University of Melbourne</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et al. (2018)</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1808.09381" title="">Understanding
back-translation at scale</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2020)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,
Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov,
Edouard Grave, Michael Auli, and Armand Joulin. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2010.11125" title="">Beyond english-centric
multilingual machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fogaça (2013)</span>
<span class="ltx_bibblock">
Helem Andressa de Oliveira Fogaça. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://web.archive.org/web/20221115061452/http://repositorio.unb.br/bitstream/10482/12959/1/2013_HelemAndressaOliveiraFogaca.pdf" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1.1">Estudo fonético e fonológico do Mambae de Same: uma
língua de Timor-Leste</em></a>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, University of Brasilia.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gale and Church (1993)</span>
<span class="ltx_bibblock">
William A. Gale and Kenneth W. Church. 1993.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/J93-1004" title="">A program for aligning
sentences in bilingual corpora</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Computational Linguistics</em>, 19(1):75–102.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia et al. (2023)</span>
<span class="ltx_bibblock">
Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun,
Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.01398" title="">The unreasonable
effectiveness of few-shot learning for machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghazvininejad et al. (2023)</span>
<span class="ltx_bibblock">
Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.07856" title="">Dictionary-based
phrase-level prompting of large language models for machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Government of Timor-Leste (2002)</span>
<span class="ltx_bibblock">
Government of Timor-Leste. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://timor-leste.gov.tl/wp-content/uploads/2010/03/Constitution_RDTL_ENG.pdf" title="">Constitution of the democratic republic of timor-leste</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gusmão (2023)</span>
<span class="ltx_bibblock">
Kirsty Sword Gusmão. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.kirstyswordgusmao.org/post/the-key-to-quality-inclusive-education-in-timor-leste-s-third-decade-as-an-independent-nation" title="">The Key to Quality Inclusive Education in Timor-Leste’s
Third Decade as an Independent Nation</a>.

</span>
<span class="ltx_bibblock">[Accessed 27-02-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haddow et al. (2022)</span>
<span class="ltx_bibblock">
Barry Haddow, Rachel Bawden, Antonio Valerio Miceli Barone, Jindřich
Helcl, and Alexandra Birch. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/coli_a_00446" title="">Survey of low-resource
machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Computational Linguistics</em>, 48(3):673–732.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy et al. (2023)</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu
Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.09210" title="">How good are gpt models at
machine translation? a comprehensive evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou
Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock,
Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile
Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.
2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.04088" title="">Mixtral of experts</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2020)</span>
<span class="ltx_bibblock">
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit
Choudhury. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.560" title="">The state and
fate of linguistic diversity and inclusion in the NLP world</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 6282–6293, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kargaran et al. (2023)</span>
<span class="ltx_bibblock">
Amir Hossein Kargaran, Ayyoob Imani, François Yvon, and Hinrich
Schütze. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=dl4e3EBz5j" title="">GlotLID:
Language identification for low-resource languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">The 2023 Conference on Empirical Methods in Natural Language
Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingsbury (2010)</span>
<span class="ltx_bibblock">
Damien Kingsbury. 2010.

</span>
<span class="ltx_bibblock">National identity in timor-leste: challenges and opportunities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">South East Asia Research</em>, 18(1):133–159.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2023)</span>
<span class="ltx_bibblock">
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton
Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda,
Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof
Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa,
Martin Popel, Maja Popović, and Mariya Shmatova. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.1" title="">Findings of the
2023 conference on machine translation (WMT23): LLMs are here but not
quite there yet</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Eighth Conference on Machine
Translation</em>, pages 1–42, Singapore. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al. (2023)</span>
<span class="ltx_bibblock">
Aswanth Kumar, Ratish Puduppully, Raj Dabre, and Anoop Kunchukuttan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14105" title="">Ctqscorer: Combining
multiple features for in-context example selection for machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample et al. (2018)</span>
<span class="ltx_bibblock">
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato.
2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1711.00043" title="">Unsupervised machine
translation using monolingual corpora only</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Yafu Li, Yongjing Yin, Jing Li, and Yue Zhang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-acl.203" title="">Prompt-driven neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Findings of the Association for Computational Linguistics:
ACL 2022</em>, pages 2579–2590, Dublin, Ireland. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta and Varma (2023)</span>
<span class="ltx_bibblock">
Rahul Mehta and Vasudeva Varma. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.semeval-1.62" title="">LLM-RM at
SemEval-2023 task 2: Multilingual complex NER using XLM-RoBERTa</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 17th International Workshop on Semantic
Evaluation (SemEval-2023)</em>, pages 453–456, Toronto, Canada. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem et al. (2023)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.13294" title="">Adaptive machine translation
with large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin
Ouyang, and Dacheng Tao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.373" title="">Towards
making the most of ChatGPT for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2023</em>, pages 5622–5633, Singapore. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al. (2023)</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani
Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei
Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, and Michael
Auli. 2023.

</span>
<span class="ltx_bibblock">Scaling speech technology to 1,000+ languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reiter (2018)</span>
<span class="ltx_bibblock">
Ehud Reiter. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/coli_a_00322" title="">A Structured Review of
the Validity of BLEU</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Computational Linguistics</em>, 44(3):393–401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson et al. (2023)</span>
<span class="ltx_bibblock">
Nathaniel Robinson, Perez Ogayo, David R. Mortensen, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.40" title="">ChatGPT MT:
Competitive for high- (but not low-) resource languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the Eighth Conference on Machine
Translation</em>, pages 392–418, Singapore. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich and Volk (2011)</span>
<span class="ltx_bibblock">
Rico Sennrich and Martin Volk. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W11-4624" title="">Iterative, MT-based
sentence alignment of parallel texts</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 18th Nordic Conference of Computational
Linguistics (NODALIDA 2011)</em>, pages 175–182, Riga, Latvia. Northern
European Association for Language Technology (NEALT).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and
Guoyin Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.603" title="">Text
classification via large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2023</em>, pages 8990–9005, Singapore. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanzer et al. (2024)</span>
<span class="ltx_bibblock">
Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke
Melas-Kyriazi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=tbVWug9f2h" title="">A benchmark for
learning to translate a new language from one grammar book</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">The Twelfth International Conference on Learning
Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,
Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi
Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John
Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,
Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov,
Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn,
Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and
Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.04672" title="">No language left behind:
Scaling human-centered machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson and Koehn (2019)</span>
<span class="ltx_bibblock">
Brian Thompson and Philipp Koehn. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/d19-1136" title="">Vecalign: Improved
sentence alignment in linear time and space</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2009)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2009.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">News from OPUS - A Collection of Multilingual Parallel Corpora
with Tools and Interfaces</em>, volume V, pages 237–248. John Benjamins,
Amsterdam/Philadelphia.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Timor-Leste General Directorate of Statistics (2015)</span>
<span class="ltx_bibblock">
Timor-Leste General Directorate of Statistics. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://inetl-ip.gov.tl/2023/03/09/census-2015-priority-table-population-by-language/" title="">2015 population and housing census</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Timor-Leste General Directorate of Statistics (2022)</span>
<span class="ltx_bibblock">
Timor-Leste General Directorate of Statistics. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://inetl-ip.gov.tl/2023/05/30/population-timor-leste-census-2022/" title="">2022 population and housing census</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.13971" title="">Llama: Open and efficient
foundation language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varga et al. (2007)</span>
<span class="ltx_bibblock">
Dániel Varga, Péter Halácsy, András Kornai, Viktor Nagy, László Németh,
and Viktor Trón. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1075/cilt.292.32var" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1.1">Parallel
corpora for medium density languages</em></a>, page 247–258. John Benjamins
Publishing Company.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et al. (2023)</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and
George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.859" title="">Prompting
PaLM for translation: Assessing strategies and performance</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 15406–15427,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Walter (2016)</span>
<span class="ltx_bibblock">
Dr. Stephen L. Walter. 2016.

</span>
<span class="ltx_bibblock">The embli endline evaluation study.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Shuo Wang, Zhaopeng Tu, Zhixing Tan, Wenxuan Wang, Maosong Sun, and Yang Liu.
2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2106.13627" title="">Language models are good
translators</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024a)</span>
<span class="ltx_bibblock">
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla.
2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=farT6XXntP" title="">A paradigm shift
in machine translation: Boosting translation performance of large language
models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">The Twelfth International Conference on Learning
Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024b)</span>
<span class="ltx_bibblock">
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van
Durme, Kenton Murray, and Young Jin Kim. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.08417" title="">Contrastive preference
optimization: Pushing the boundaries of llm performance in machine
translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/zhang23m.html" title="">Prompting
large language model for machine translation: A case study</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 40th International Conference on Machine
Learning</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib50.2.2">Proceedings of Machine Learning Research</em>,
pages 41092–41110. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.714" title="">SummIt: Iterative text summarization via ChatGPT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2023</em>, pages 10644–10657, Singapore. Association for Computational
Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx6.1.1" style="font-size:120%;">Language Resource References</span></h2>
<span class="ltx_ERROR undefined" id="Sx6.2">\c@NAT@ctr</span>
</section>
<section class="ltx_bibliography" id="biba">
<h2 class="ltx_title ltx_title_bibliography"> </h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="biba.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gowda et al. (2021)</span>
<span class="ltx_bibblock">
Gowda, Thamme and Zhang, Zhao and Mattmann, Chris and May, Jonathan. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-demo.37" title=""><em class="ltx_emph ltx_font_italic" id="biba.bib1.1.1.1">Many-to-English Machine Translation Tools, Data, and Pretrained
Models</em></a>.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="biba.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hull (2001)</span>
<span class="ltx_bibblock">
Hull, Geoffrey. 2001.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://catalogue.nla.gov.au/catalog/2115553" title=""><em class="ltx_emph ltx_font_italic" id="biba.bib2.1.1.1">Mambai
Language Manual: Ainaro Dialect</em></a>.

</span>
<span class="ltx_bibblock">Sebastião Aparício da Silva Project.

</span>
</li>
<li class="ltx_bibitem" id="biba.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha
Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and
Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang
and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and
Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley
Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau
Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey
Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco
Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and
Safiyyah Saleem and Holger Schwenk and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.04672" title=""><em class="ltx_emph ltx_font_italic" id="biba.bib3.1.1.1">No Language Left
Behind: Scaling Human-Centered Machine Translation</em></a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Apr  7 05:05:17 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
