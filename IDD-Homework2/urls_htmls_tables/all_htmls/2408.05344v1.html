<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations</title>
<!--Generated on Fri Aug  9 21:18:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="coding assistant,  large language model,  context window,  code generation,  evaluation" lang="en" name="keywords"/>
<base href="/html/2408.05344v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#S1" title="In AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#S2" title="In AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Context Engine</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#S3" title="In AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Hartman, Rishabh Mehrotra, Hitesh Sagtani, Dominic Cooney, Rafal Gajdulewicz, Beyang Liu, Julie Tibshirani, Quinn Slack
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jan.hartman,rishabh.mehrotra@sourcegraph.com">jan.hartman,rishabh.mehrotra@sourcegraph.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Sourcegraph</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">San Francisco</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id5.id1">In this work, we discuss a recently popular type of recommender system: an LLM-based coding assistant. Connecting the task of providing code recommendations in multiple formats to traditional RecSys challenges, we outline several similarities and differences due to domain specifics. We emphasize the importance of providing relevant context to an LLM for this use case and discuss lessons learned from context enhancements &amp; offline and online evaluation of such AI-assisted coding systems.</p>
</div>
<div class="ltx_keywords">coding assistant, large language model, context window, code generation, evaluation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>18th ACM Conference on Recommender Systems; October 14–18, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>18th ACM Conference on Recommender Systems (RecSys ’24), October 14–18, 2024, Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3640457.3688060</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0505-2/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Software and its engineering Software creation and management</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Language models</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language processing</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent advancements in the large language model (LLM) area have made it feasible to use such models to assist with software development. Popular models like the GPT family <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib2" title="">2023</a>)</cite> or Claude <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib3" title="">2024</a>)</cite> have demonstrated good results on coding benchmarks such as HumanEval <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib5" title="">2021</a>)</cite>. There is also a number of LLMs fine-tuned for code generation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib5" title="">2021</a>)</cite>
with many such models having publicly available weights <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib10" title="">2023</a>; Roziere et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib12" title="">2023</a>)</cite>. These models are commonly used in coding assistant applications to improve productivity <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib8" title="">2023</a>)</cite> by providing code recommendations in use cases such as autocomplete or chat <cite class="ltx_cite ltx_citemacro_citep">(Ross et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib11" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">A key part of any coding assistant is understanding the user’s codebase as this enables the assistant to provide reliable recommendations. However, a key problem here is that the underlying LLMs likely are not trained on the user’s code (typically due to costs, UX issues, and privacy concerns), meaning that out of the box, their responses will be far from optimal. Furthermore, lacking repository-specific code context, the generated code often suffers from hallucinations.
The common solution to this problem is providing relevant context to the model inside the prompt.
This involves finding sections of text or code that are relevant to the user’s query and stitching together a prompt to enable in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib7" title="">2022</a>)</cite> for the LLM, dramatically increasing the quality of the responses <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib4" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, we cannot simply fit all relevant snippets into the prompt as the context window size in LLMs is limited for practical reasons. Despite many recent advancements <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib6" title="">2023</a>)</cite> in this direction, some even allowing sizes up to 1 million tokens <cite class="ltx_cite ltx_citemacro_citep">(Team et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib13" title="">2023</a>)</cite>, extending the context window is not a one-size-fits-all solution – it makes inference far more costly and with large codebases, we still might not be able to fit all relevant items. This means that our approach has to consider both recall and precision and be able to pick only a few most relevant context items.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the remainder of this work, we discuss the importance of the context-picking engine inside a coding assistant, outline some key evaluation differences compared to a traditional RecSys, and provide some practical considerations gathered while working on a real-life coding assistant – Sourcegraph’s Cody<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cody.dev" title="">https://cody.dev</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Context Engine</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The goal of the context engine inside a coding assistant is similar to candidate generation in a traditional recommender system: given the state of the user’s workspace and a query, find N most relevant sections of text or code (we call these context items). The number of context items depends heavily on the use case as the latency can be a critical part of the recommendations – an autocomplete suggestion needs to be generated much faster than a chat response. Like in most RecSys, we also distinguish between two stages in this process: retrieving context items and ranking them.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The number of sources of context we can use in a coding assistant is vast: local and remote code, source control history, code review tools, editor state, terminal, documentation, chats, internal Wikis, ticketing systems, observability dashboards, etc.
Unlike a traditional RecSys, items from these are not in a centralized source of data and we often do not have access to them ahead of time, thus diminishing our ability to e.g. create indexes or precompute useful metrics.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">For searching through the space of context items, we can use techniques like similarity-based matching, keyword search, semantic search (embedding-powered retrieval augmented generation  <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib9" title="">2020</a>)</cite>), code graph analysis, etc. In this stage, approximate approaches are preferred for performance reasons and we are optimizing for recall – that is, we care more about retrieving all relevant items rather than retrieving only relevant items. Our limitations here are latency and the token budget.
An interesting observation is that context retrieval sources should be complementary – that is, we
want them to retrieve distinct sets of relevant items. Theoretically, we should be able to achieve this by utilizing techniques like keyword and semantic search as they use different matching strategies.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The second stage of the context engine consists of ranking the context items. Here, our use case again differs from traditional RecSys: we do not present the items we are ranking to the user, the goal of ranking here is only to pick the most relevant items to insert into the LLM prompt.
This means we cannot get user feedback as labels, which complicates evaluation – we discuss more in the next section. Furthermore, we do not take into account the positions of the ranked items. We only want to pick items above a certain threshold. The threshold can be e.g. a relevance score or a token budget. Given a good ranking model, this stage optimizes for precision, i.e. keeping only relevant items.
With our setting, we can make a pointwise ranker model work well: we can simply train it to predict whether a given context item is relevant to the user’s query.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Evaluation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Evaluating an LLM-based coding assistant can present several difficulties. The biggest issue is the large online-offline discrepancy as the two settings are significantly different.
It is important to note that we cannot rely on being able to easily store the complete state of the workspace at request time due to several factors – most prominently, technological issues and privacy concerns. Many of the context item sources also only exist on the user’s device and can be ephemeral. Being unable to log and use this data for offline evaluations makes it very difficult to bridge the online-offline gap. In the online environment, we can receive user feedback – we know if the user accepted an autocomplete suggestion or if they marked the chat response as useful / not useful. These can be used to judge online A/B tests.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The second issue is a significant lack of labeled data, especially with regard to the context engine. A dataset here might be a set of queries against codebases with the labels being sets of relevant snippets of text or code. Even manual annotation is no easy task: to get correct answers to queries like “Where is XYZ implemented?” in real codebases, we require an expert annotator, meaning that this task cannot be easily crowdsourced.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Lastly, we have to take into account that the coding assistant has several components – we can try to evaluate either each component (retrieval, ranking, LLM) separately or everything end-to-end.
For retrieval, we have created an internal, crowdsourced dataset of queries and have found that can provide great insights into different retrieval strategies.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">To build and evaluate the model used for ranking, we share learnings from various datasets we created that enabled us to perform offline evaluations of various coding assistant features: code completions, code edits, unit test generation, and open-ended chat.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">The end-to-end case is by far the closest to the online setting.
Of course, the main goal of the coding assistant is to provide high-quality recommendations. End-to-end evaluation means we have to judge the utility to the user, which differs based on the task.
Here, one of the advantages of working with code is that we can utilize domain specifics to make sure that the recommendations provide value. For autocompletions, this can involve syntactic (e.g. does it parse?) and semantic (e.g. do the types match?) checks over the resulting code segment to prevent nonsensical outputs.
When generating unit tests, we can follow similar logic, including checking if the tested function was called, if it was called with the correct arguments, and actually running the test. If the user poses a question relating to existing functionality in the codebase, we can scan the resulting code segment to make sure the generated code symbols exist.
For general chat responses, we can utilize techniques such as LLM judging <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.05344v1#bib.bib14" title="">2024</a>)</cite> to evaluate their quality.
An especially positive side of these checks is that certain ones can be used as guardrails in the online setting: if the assistant generates a recommendation, it makes sense to check if it fulfills the intended purpose before displaying it to the user. With techniques such as these, we can curtail LLM hallucinations and improve the quality of the assistant.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Presenter bio</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Jan Hartman is a machine learning engineer at Sourcegraph, where he works on Cody’s context engine. His work is focused on evaluating new sources of context, i.e. ways for Cody to understand codebases. Before joining Sourcegraph, he worked on high-throughput, low-latency ML pipelines at a large scale in adtech. Honors MSc degree in Computer &amp; Data Science from the University of Ljubljana. Open-source contributor. Research interests include deep learning, neural network embeddings, and model compression.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We would like to thank the entire Sourcegraph Cody team for their contributions.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al<span class="ltx_text" id="bib.bib2.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">arXiv preprint arXiv:2303.08774</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic. 2024.

</span>
<span class="ltx_bibblock">Claude 3 model family.

</span>
<span class="ltx_bibblock">(2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf" title="">https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span class="ltx_text" id="bib.bib4.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.4.1">Advances in neural information processing systems</em> 33 (2020), 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al<span class="ltx_text" id="bib.bib5.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">arXiv preprint arXiv:2107.03374</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023.

</span>
<span class="ltx_bibblock">Extending context window of large language models via positional interpolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">arXiv preprint arXiv:2306.15595</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.

</span>
<span class="ltx_bibblock">A survey on in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">arXiv preprint arXiv:2301.00234</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M. Zhang. 2023.

</span>
<span class="ltx_bibblock">Large Language Models for Software Engineering: Survey and Open Problems. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)</em>. IEEE Computer Society, Los Alamitos, CA, USA, 31–53.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICSE-FoSE59343.2023.00008" title="">https://doi.org/10.1109/ICSE-FoSE59343.2023.00008</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">Advances in Neural Information Processing Systems</em> 33 (2020), 9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al<span class="ltx_text" id="bib.bib10.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Starcoder: may the source be with you!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">arXiv preprint arXiv:2305.06161</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ross et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D. Weisz. 2023.

</span>
<span class="ltx_bibblock">The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 28th International Conference on Intelligent User Interfaces</em> (Sydney, NSW, Australia) <em class="ltx_emph ltx_font_italic" id="bib.bib11.4.2">(IUI ’23)</em>. Association for Computing Machinery, New York, NY, USA, 491–514.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3581641.3584037" title="">https://doi.org/10.1145/3581641.3584037</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roziere et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">arXiv preprint arXiv:2308.12950</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al<span class="ltx_text" id="bib.bib13.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.4.1">arXiv preprint arXiv:2312.11805</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al<span class="ltx_text" id="bib.bib14.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug  9 21:18:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
