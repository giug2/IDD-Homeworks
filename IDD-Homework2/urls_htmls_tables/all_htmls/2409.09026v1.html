<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks</title>
<!--Generated on Fri Sep 13 15:33:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09026v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S1" title="In Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S2" title="In Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title">Artist Similarity with Graph Neural Networks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title">Neural Embeddings for Recommender Tasks.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S3" title="In Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Neural Audio Embeddings for Artist Relationships</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S3.SS1" title="In 3 Neural Audio Embeddings for Artist Relationships ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S3.SS2" title="In 3 Neural Audio Embeddings for Artist Relationships ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S3.SS2.SSS0.Px1" title="In 3.2 Results ‣ 3 Neural Audio Embeddings for Artist Relationships ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title">Limitations</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S4" title="In Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\copyrightclause</span>
<p class="ltx_p" id="p1.2">Copyright for this paper by its authors.
Use permitted under Creative Commons License Attribution 4.0
International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\conference</span>
<p class="ltx_p" id="p2.2">The 2nd Music Recommender Workshop (@RecSys),
October 14, 2024, Bari, Italy</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">[orcid=0009-0004-1509-174X,
email=fgroetschla@ethz.ch
]</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">[orcid=0009-0002-5264-162X,
email=lucastr@ethz.ch
]</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">[orcid=0009-0009-5953-7842,
email=lanzendoerfer@ethz.ch
]</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">[orcid=0000-0002-6339-3134,
email=wattenhofer@ethz.ch
]</p>
</div>
<h1 class="ltx_title ltx_title_document">Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Florian Grötschla
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">ETH Zurich, Switzerland
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luca Strässle
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luca A. Lanzendörfer
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roger Wattenhofer
</span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Music recommender systems frequently utilize network-based models to capture relationships between music pieces, artists, and users. Although these relationships provide valuable insights for predictions, new music pieces or artists often face the cold-start problem due to insufficient initial information. To address this, one can extract content-based information directly from the music to enhance collaborative-filtering-based methods. While previous approaches have relied on hand-crafted audio features for this purpose, we explore the use of contrastively pretrained neural audio embedding models, which offer a richer and more nuanced representation of music. Our experiments demonstrate that neural embeddings, particularly those generated with the Contrastive Language-Audio Pretraining (CLAP) model, present a promising approach to enhancing music recommendation tasks within graph-based frameworks.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Music recommendation <span class="ltx_ERROR undefined" id="id2.id1">\sep</span>graph neural network <span class="ltx_ERROR undefined" id="id3.id2">\sep</span>contrastive learning

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Music and artist recommendations have become a cornerstone of streaming services, profoundly influencing how users discover and engage with music. Algorithmically generated playlists, tailored to individual tastes, are integral to the listening experience, enabling users to find music that suits their mood and environment, as well as discover new artists. For artists, inclusion in these playlists can significantly boost their listener base, while exclusion poses challenges for discovery. Music recommendation systems can be broadly categorized into collaborative filtering-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib1" title="">1</a>]</cite> and content-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib2" title="">2</a>]</cite>. Collaborative filtering leverages relational data, capturing relationships between artists or tracks from manually curated similarities, tags, and user listening behavior. Content-based approaches utilize descriptive data to encapsulate the essence of an artist’s music, representing attributes like melody, harmony, and rhythm. Hybrid recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib4" title="">4</a>]</cite> combine both types of data to enhance recommendation quality.
In recent years, contrastive learning approaches have gained traction for their effectiveness in representing various types of data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib6" title="">6</a>]</cite>. One such model, Contrastive Language-Audio Pretraining (CLAP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib7" title="">7</a>]</cite>, maps text and audio into a joint multi-modal space, offering a novel method for representing music. Our work explores the utility of CLAP representations as descriptive data in music recommendation systems.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As a proof-of-concept, we examine a graph-based artist-relationship prediction task, where additional musical information has previously enhanced model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib8" title="">8</a>]</cite>. The goal is to predict relationships between previously unseen artists using the attached information. By varying this information and incorporating CLAP embeddings, we evaluate its utility in a controlled environment and benchmark the effectiveness of different representations.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Artist Similarity with Graph Neural Networks.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Graph Neural Networks (GNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib9" title="">9</a>]</cite> extend deep learning techniques to graph-structured data, addressing the limitations of traditional neural networks that require structured inputs. GNNs operate on graphs defined by nodes and edges, leveraging message passing to aggregate and update node information based on their neighbors. This approach has shown success in tasks such as node classification, edge prediction, and graph classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib10" title="">10</a>]</cite>. GNNs lend themselves to music recommender tasks as they can encode the structural, relational information together with additional features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">The study by <cite class="ltx_cite ltx_citemacro_citet">Korzeniowski et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib8" title="">8</a>]</cite> introduces the OLGA dataset, which includes artist relations from AllMusic<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.allmusic.com/" title="">https://www.allmusic.com/</a></span></span></span> and audio features from AcousticBrainz <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib13" title="">13</a>]</cite>. Their GNN architecture combines graph convolution layers with fully connected layers and was trained with a triplet loss. Performance evaluations on an artist similarity task demonstrated that incorporating graph layers and meaningful artist features significantly improved prediction accuracy over using deep neural networks alone.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Neural Embeddings for Recommender Tasks.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Various methods have been explored for music similarity detection. Previous approaches used a graph autoencoder to learn latent representations in an artist graph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib14" title="">14</a>]</cite>, or leveraging a Siamese DCNN model for feature extraction and genre classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib15" title="">15</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Oramas et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib16" title="">16</a>]</cite> use CNNs to extract music information, which, in contrast to our work, can not benefit from contrastive pertaining. Furthermore, hybrid recommendation systems using GNNs have been applied in other domains, such as predicting anime recommendations by combining user-anime interaction graphs with BERT embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">Contrastive Language-Audio Pretraining (CLAP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib7" title="">7</a>]</cite> learns the (dis)similarity between audio and text through contrastive learning, mapping both modalities into a joint multimodal space. Through the contrastive learning approach, even the audio embeddings alone maintain semantic information, making it suitable for tasks such as music recommendation and artist similarity.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Neural Audio Embeddings for Artist Relationships</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We investigate an established artist similarity task similar to the OLGA dataset to evaluate the effectiveness of neural audio embeddings over classical audio features in music recommendation tasks. This dataset comprises a large graph of artists, and the performance of our model is assessed based on its ability to predict new relationships between previously unseen artists, represented as nodes within the graph. Each node is annotated with features extracted from the music produced by the respective artist.
Previous research demonstrated that incorporating musical information significantly improves model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib8" title="">8</a>]</cite>. We extend this analysis by extracting CLAP embeddings from the music and comparing their effectiveness against other feature sets. Our goal is to determine if CLAP embeddings provide better representations.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our setup is inspired by the approach of <cite class="ltx_cite ltx_citemacro_citet">Korzeniowski et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib8" title="">8</a>]</cite> on OLGA, where artists are represented as connected nodes based on their relationships described in AllMusic. Following the same methodology, we create an updated version of the original dataset. This allows us to ensure that the song for which we extract features from AcousticBrainz is consistent with the song for which we create CLAP embeddings.
We start with the same set of artists and collect additional information during preprocessing, specifically the categorical features for moods and themes of an artist, which we use during evaluation. Low-level music features for songs were retrieved from AcousticBrainz, and CLAP embeddings were computed using the LAION CLAP model from tracks on YouTube.
In contrast to the original OLGA dataset, we only use one song per artist and do not aggregate the features over multiple songs. Due to constantly changing information on AllMusic, some artists without connections to other artists or missing matches on MusicBrainz or AcousticBrainz had to be dropped. Overall, this reduced the total number of artists from 17,673 in the original to 16,864 in our version. We reuse the split allocation of the OLGA dataset, which is possible since every artist in our dataset is present in the OLGA dataset as well. This resulted in 13,489 artists in the training, 1,679 artists in the validation, and 1,696 artists in the test split.
We utilize the same loss functions and GNN backbone as proposed by <cite class="ltx_cite ltx_citemacro_citet">Korzeniowski et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib8" title="">8</a>]</cite>, but with a uniform sampling based on triplets instead of distance-weighted sampling. More specifically, we employed the triplet loss, finding that using both endpoints as anchors performed better than randomly selecting one endpoint. Euclidean distance was used for the loss, and the Normalized Discounted Cumulative Gain (NDCG) serves for the evaluation.
For the graph neural network layers, we experimented with <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.1">SAGE</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib18" title="">18</a>]</cite>, <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.2">GatedGCN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib19" title="">19</a>]</cite>, and <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.3">GIN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#bib.bib20" title="">20</a>]</cite>, with <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.4">SAGE</span> demonstrating the best performance.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We vary two primary aspects in our experiments: the number of graph layers and the node features. The number of graph layers ranges from zero to four and is varied to assess the contribution that the graph topology can make to the task. With zero graph layers, the architecture only utilizes an MLP to make predictions and does not consider the graph topology, thus serving as a baseline for models that use GNN layers. As the number of graph layers increases, nodes can aggregate information from a larger neighborhood, enhancing the model’s capacity to learn from the graph structure. For node features, we use random features as a baseline and experimented with AcousticBrainz features, CLAP features, and Moods-Themes features. We also test combinations of these non-random features.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Results</h3>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="498" id="S3.F1.sf1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F1.sf1.3.2" style="font-size:90%;">Comparison of CLAP features with Random, Moods-Themes, and AcousticBrainz features. CLAP outperforms all other features when used with enough layers.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="498" id="S3.F1.sf2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F1.sf2.3.2" style="font-size:90%;">Comparison of various feature combinations. With fewer layers, feature combinations perform better than single features, whereas they perform on par for more layers.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Comparison of input features used for the artist relationship prediction task. We report the mean performance and indicate the standard deviation over three seeds for each configuration, testing all setups with 0 to 4 GNN layers. The 0-layer configuration serves as the baseline, where no message-passing is performed, and only the input features are used to predict node pairs. </span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S3.F1.sf1" title="In Figure 1 ‣ 3.2 Results ‣ 3 Neural Audio Embeddings for Artist Relationships ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_tag">1(a)</span></a> compares the performance of models using random features, AcousticBrainz features, Moods-Themes features, and CLAP features. The baseline model, which does not utilize any graph convolution layers, performs significantly worse than models incorporating graph topology information. Performance generally improves with the addition of more graph layers. Random features consistently underperform, while CLAP features show better results with increased layers in comparison to the others.
Moods-Themes features perform well without graph layers but only achieve results similar to random features with four layers, indicating that the information they provide can be compensated by knowledge of the neighborhood around an artist.
Based on these findings, we conclude that CLAP embeddings are effective in enhancing music recommendation tasks and provide information that is missing in other features.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We further compare combinations of CLAP embeddings with other features to assess their effectiveness. Our analysis in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S3.F1.sf2" title="In Figure 1 ‣ 3.2 Results ‣ 3 Neural Audio Embeddings for Artist Relationships ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_tag">1(b)</span></a> reveals that for lower layer numbers, the combination of features can greatly increase performance in comparison to single features (as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09026v1#S3.F1.sf1" title="In Figure 1 ‣ 3.2 Results ‣ 3 Neural Audio Embeddings for Artist Relationships ‣ Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks"><span class="ltx_text ltx_ref_tag">1(a)</span></a>). For more layers, the tested feature combinations approach the performance of the model that only uses CLAP features. This could mean that the other features do not provide much additional value for the task or that the information gained from the graph topology is sufficient to compensate for it.
Overall, feature combinations that include CLAP perform better, while we can see a clear increase of AcousticBrainz + Moods-Themes over the single feature baselines.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Limitations</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Our experimental evaluation has two main limitations: the potential for model architecture improvements and the limited representation of artists using only one song.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p2.1">First, regarding model architecture, there is room for enhancement through more advanced techniques, such as distance-weighted sampling, more sophisticated GNN layers, or Graph Transformers. We anticipate these improvements would likely lead to better overall performance. However, our conclusions primarily focus on the relative performance gains of different feature sets. We believe these relative differences would remain consistent even with improved models and training techniques, though absolute performance might increase.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p3.1">Second, we only use a single song to represent each artist. This approach could introduce variability based on the choice of the song, potentially affecting the performance of the features. A more comprehensive representation involving multiple songs per artist could provide a more robust understanding, but this would require careful consideration of how to aggregate these song embeddings. Additionally, there is potential for exploring different versions of CLAP or other audio embedding models. Nevertheless, the fact that we achieved consistent performance gains even with just one song per artist demonstrates the effectiveness of CLAP embeddings as a viable approach for music recommendation, which was the primary objective of this study.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this work, we explored the use of CLAP embeddings as descriptive data for music recommendation systems. Our experiments focused on a graph-based artist-relationship prediction task, comparing the effectiveness of various feature representations, including AcousticBrainz, CLAP, and a combination of both. Our results indicate that models incorporating CLAP embeddings significantly outperform those using traditional features, particularly as the number of graph convolutional layers increases. This highlights the potential of CLAP embeddings to capture rich and relevant information about music, thereby enhancing the performance of music recommendation systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarwar et al. [2001]</span>
<span class="ltx_bibblock">
B. Sarwar, G. Karypis, J. Konstan, J. Riedl,

</span>
<span class="ltx_bibblock">Item-based collaborative filtering recommendation algorithms,

</span>
<span class="ltx_bibblock">in: Proceedings of the 10th international conference on World Wide Web, 2001, pp. 285–295.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pazzani and Billsus [2007]</span>
<span class="ltx_bibblock">
M. J. Pazzani, D. Billsus,

</span>
<span class="ltx_bibblock">Content-based recommendation systems,

</span>
<span class="ltx_bibblock">in: The adaptive web: methods and strategies of web personalization, Springer, 2007, pp. 325–341.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burke [2002]</span>
<span class="ltx_bibblock">
R. Burke,

</span>
<span class="ltx_bibblock">Hybrid recommender systems: Survey and experiments,

</span>
<span class="ltx_bibblock">User modeling and user-adapted interaction 12 (2002) 331–370.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adomavicius and Tuzhilin [2005]</span>
<span class="ltx_bibblock">
G. Adomavicius, A. Tuzhilin,

</span>
<span class="ltx_bibblock">Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions,

</span>
<span class="ltx_bibblock">IEEE transactions on knowledge and data engineering 17 (2005) 734–749.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
T. Chen, S. Kornblith, M. Norouzi, G. Hinton,

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations,

</span>
<span class="ltx_bibblock">in: International conference on machine learning, PMLR, 2020, pp. 1597–1607.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al.,

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision,

</span>
<span class="ltx_bibblock">in: International conference on machine learning, PMLR, 2021, pp. 8748–8763.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, S. Dubnov,

</span>
<span class="ltx_bibblock">Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,

</span>
<span class="ltx_bibblock">in: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korzeniowski et al. [2021]</span>
<span class="ltx_bibblock">
F. Korzeniowski, S. Oramas, F. Gouyon,

</span>
<span class="ltx_bibblock">Artist similarity with graph neural networks,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2107.14541 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scarselli et al. [2008]</span>
<span class="ltx_bibblock">
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, G. Monfardini,

</span>
<span class="ltx_bibblock">The graph neural network model,

</span>
<span class="ltx_bibblock">IEEE transactions on neural networks 20 (2008) 61–80.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2020]</span>
<span class="ltx_bibblock">
Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip,

</span>
<span class="ltx_bibblock">A comprehensive survey on graph neural networks,

</span>
<span class="ltx_bibblock">IEEE transactions on neural networks and learning systems 32 (2020) 4–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oramas et al. [2016]</span>
<span class="ltx_bibblock">
S. Oramas, V. C. Ostuni, T. D. Noia, X. Serra, E. D. Sciascio,

</span>
<span class="ltx_bibblock">Sound and music recommendation with knowledge graphs,

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology (TIST) 8 (2016) 1–21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng et al. [2022]</span>
<span class="ltx_bibblock">
H. Weng, J. Chen, D. Wang, X. Zhang, D. Yu,

</span>
<span class="ltx_bibblock">Graph-based attentive sequential model with metadata for music recommendation,

</span>
<span class="ltx_bibblock">IEEE Access 10 (2022) 108226–108240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogdanov et al. [2019]</span>
<span class="ltx_bibblock">
D. Bogdanov, A. Porter, H. Schreiber, J. Urbano, S. Oramas,

</span>
<span class="ltx_bibblock">The acousticbrainz genre dataset: Multi-source, multi-level, multi-label, and large-scale,

</span>
<span class="ltx_bibblock">in: Proceedings of the 20th Conference of the International Society for Music Information Retrieval (ISMIR 2019): 2019 Nov 4-8; Delft, The Netherlands.[Canada]: ISMIR; 2019., International Society for Music Information Retrieval (ISMIR), 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salha-Galvan et al. [2021]</span>
<span class="ltx_bibblock">
G. Salha-Galvan, R. Hennequin, B. Chapus, V.-A. Tran, M. Vazirgiannis,

</span>
<span class="ltx_bibblock">Cold start similar artists ranking with gravity-inspired graph autoencoders,

</span>
<span class="ltx_bibblock">in: Proceedings of the 15th ACM Conference on Recommender Systems, 2021, pp. 443–452.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2017]</span>
<span class="ltx_bibblock">
J. Park, J. Lee, J. Park, J.-W. Ha, J. Nam,

</span>
<span class="ltx_bibblock">Representation learning of music using artist labels,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1710.06648 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oramas et al. [2017]</span>
<span class="ltx_bibblock">
S. Oramas, O. Nieto, M. Sordo, X. Serra,

</span>
<span class="ltx_bibblock">A deep multimodal approach for cold-start music recommendation,

</span>
<span class="ltx_bibblock">in: Proceedings of the 2nd workshop on deep learning for recommender systems, 2017, pp. 32–37.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Javaji and Sarode [2023]</span>
<span class="ltx_bibblock">
S. R. Javaji, K. Sarode,

</span>
<span class="ltx_bibblock">Hybrid recommendation system using graph neural network and bert embeddings,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.04878 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton et al. [2017]</span>
<span class="ltx_bibblock">
W. Hamilton, Z. Ying, J. Leskovec,

</span>
<span class="ltx_bibblock">Inductive representation learning on large graphs,

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 30 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwivedi et al. [2023]</span>
<span class="ltx_bibblock">
V. P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y. Bengio, X. Bresson,

</span>
<span class="ltx_bibblock">Benchmarking graph neural networks,

</span>
<span class="ltx_bibblock">Journal of Machine Learning Research 24 (2023) 1–48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2018]</span>
<span class="ltx_bibblock">
K. Xu, W. Hu, J. Leskovec, S. Jegelka,

</span>
<span class="ltx_bibblock">How powerful are graph neural networks?,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1810.00826 (2018).

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 13 15:33:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
