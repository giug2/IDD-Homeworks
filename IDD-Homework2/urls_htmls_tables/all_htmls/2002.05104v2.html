<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2002.05104] Component Analysis for Visual Question Answering Architectures</title><meta property="og:description" content="Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Component Analysis for Visual Question Answering Architectures">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Component Analysis for Visual Question Answering Architectures">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2002.05104">

<!--Generated on Sun Mar 17 18:55:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Visual Question Answering,  Computer Vision,  Natural Language Processing.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Component Analysis for 
<br class="ltx_break">Visual Question Answering Architectures
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Camila Kolling, Jônatas Wehrmann, and Rodrigo C. Barros
<br class="ltx_break">Machine Intelligence and Robotics Research Group
<br class="ltx_break">School of Technology, Pontifícia Universidade Católica do Rio Grande do Sul
<br class="ltx_break">Av. Ipiranga, 6681, 90619-900, Porto Alegre, RS, Brazil
<br class="ltx_break">Email: {camila.kolling,jonatas.wehrmann}@edu.pucrs.br, rodrigo.barros@pucrs.br
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). This system takes an image and a free-form, open-ended natural-language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Visual Question Answering, Computer Vision, Natural Language Processing.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent research advances in Computer Vision (CV) and Natural Language Processing (NLP) introduced several tasks that are quite challenging to be solved, the so-called AI-complete problems. Most of those tasks require systems that understand information from multiple sources, i.e., semantics from visual and textual data, in order to provide some kind of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">reasoning</span>. For instance, image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> presents itself as a hard task to solve, though it is actually challenging to quantitatively evaluate models on that task, and that recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> have raised questions on its AI-completeness.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> task was introduced as an attempt to solve that issue: to be an actual AI-complete problem whose performance is easy to evaluate. It requires a system that receives as input an image and a free-form, open-ended, natural-language question to produce a natural-language answer as the output <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. It is a multidisciplinary topic that is gaining popularity by encompassing CV and NLP into a single architecture, what is usually regarded as a multimodal model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. There are many real-world applications for models trained for Visual Question Answering, such as automatic surveillance video queries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and visually-impaired aiding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Models trained for VQA are required to understand the semantics from images while finding relationships with the asked question. Therefore, those models must present a deep understanding of the image to properly perform inference and produce a reasonable answer to the visual question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In addition, it is much easier to evaluate this task since there is a finite set of possible answers for each image-question pair.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Traditionally, VQA approaches comprise three major steps: (i) representation learning of the image and the question; (ii) projection of a single multimodal representation through fusion and attention modules that are capable of leveraging both visual and textual information; and (iii) the generation of the natural language answer to the question at hand.
This task often requires sophisticated models that are able to understand a question expressed in text, identify relevant elements of the image, and evaluate how these two inputs correlate.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Given the current interest of the scientific community in VQA, many recent advances try to improve individual components such as the image encoder, the question representation, or the fusion and attention strategies to better leverage both information sources.
With so many approaches currently being introduced at the same time, it becomes unclear the real contribution and importance of each component within the proposed models. Thus, the main goal of this work is to understand the impact of each component on a proposed baseline architecture, which draws inspiration from the pioneer VQA model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Each component within that architecture is then systematically tested, allowing us to understand its impact on the system’s final performance through a thorough set of experiments and ablation analysis.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2002.05104/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="300" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Baseline architecture proposed for the experimental setup.</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">More specifically, we observe the impact of: (i) pre-trained word embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, recurrent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and transformer-based sentence encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as question representation strategies; (ii) distinct convolutional neural networks used for visual feature extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>; and (iii) standard fusion strategies, as well as the importance of two main attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. We notice that even using a relatively simple baseline architecture, our best models are competitive to the (maybe overly-complex) state-of-the-art models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Given the experimental nature of this work, we have trained over 130 neural network models, accounting for more than 600 GPU processing hours. We expect our findings to be useful as guidelines for training novel VQA models, and that they serve as a basis for the development of future architectures that seek to maximize predictive performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The task of VAQ has gained attention since Antol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> presented a large-scale dataset with open-ended questions. Many of the developed VQA models employ a very similar architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>: they represent images with features from pre-trained convolutional neural networks; they use word embeddings or recurrent neural networks to represent questions and/or answers; and they combine those features in a classification model over possible answers.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Despite their wide adoption, RNN-based models suffer from their limited representation power <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Some recent approaches have investigated the application of the Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to tasks that incorporate visual and textual knowledge, as image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Attention-based methods are also being continuously investigated since they enable reasoning by focusing on relevant objects or regions in original input features. They allow models to pay attention on important parts of visual or textual inputs at each step of a task. Visual attention models focus on small regions within an image to extract important features. A number of methods have adopted visual attention to benefit visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Recently, dynamic memory networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> integrate an attention mechanism with a memory module, and multimodal bilinear pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> is exploited to expressively combine multimodal features and predict attention over the image. These methods commonly employ visual attention to find critical regions, but textual attention has been rarely incorporated into VQA systems.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">While all the aforementioned approaches have exploited those kind of mechanisms, in this paper we study the impact of such choices specifically for the task of VQA, and create a simple yet effective model. Burns et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> conducted experiments comparing different word embeddings, language models, and embedding augmentation steps on five multimodal tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. While their work focuses on textual experiments, our experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. To the best of our knowledge, this is the first paper that provides a comprehensive analysis on the impact of each major component within a VQA architecture.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Impact of VQA Components</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we first introduce the baseline approach, with default image and text encoders, alongside a pre-defined fusion strategy. That base approach is inspired by the pioneer of Antol et al. on VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. To understand the importance of each component, we update the base architecture according to each component we are investigating.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In our baseline model we replace the VGG network from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> by a Faster RCNN pre-trained in the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. The default text encoding is given by the last hidden-state of a Bidirectional LSTM network, instead of the concatenation of the last hidden-state and memory cell used in the original work. Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the proposed baseline architecture, which is subdivided into three major segments: independent feature extraction from (1) images and (2) questions, as well as (3) the fusion mechanism responsible to learn cross-modal features.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.2" class="ltx_p">The default text encoder (denoted by the pink rectangle in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) employed in this work comprises a randomly initialized word-embedding module that takes a tokenized question and returns a continuum vector for each token. Those vectors are used to feed an LSTM network. The last hidden-state is used as the question encoding, which is projected with a linear layer into a <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">d</annotation></semantics></math>-dimensional space so it can be fused along to the visual features. As the default option for the LSTM network, we use a single layer with <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.p3.2.m2.1a"><mn id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><cn type="integer" id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">2048</annotation></semantics></math> hidden units. Given that this text encoding approach is fully trainable, we hereby name it <span id="S3.p3.2.1" class="ltx_text ltx_font_smallcaps">Learnable Word Embedding</span> (<span id="S3.p3.2.2" class="ltx_text ltx_font_smallcaps">LWE</span>).</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">For the question encoding, we explore pre-trained and randomly initialized word-embeddings in various settings, including Word2Vec (W2V) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. We also explore the use of hidden-states of Skip-Thoughts Vector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as replacements for word-embeddings and sentence encoding approaches.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Regarding the visual feature extraction (depicted as the green rectangle in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), we decided to use the pre-computed features proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Such an architecture employs a ResNet-152 with a Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> fine-tuned on the Visual Genome dataset. We opted for this approach due to the fact that using pre-computed features is far more computationally efficient, allowing us to train several models with distinct configurations. Moreover, several recent approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> employ that same strategy as well, making it easier to provide fair comparison to the state-of-the-art approaches.
In this study we
perform experiments with two additional networks widely used for the task at hand, namely VGG-16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and ReSNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Given the multimodal nature of the problem we are dealing with, it is quite challenging to train proper image and question encoders so as to capture relevant semantic information from both of them. Nevertheless, another essential aspect of the architecture is the component that merges them altogether, allowing for the model to generate answers based on both information sources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. The process of multimodal fusion consists itself in a research area with many approaches being recently proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.
The fusion module receives the extracted image and query features, and provides multimodal features that theoretically present information that allows the system to answer to the visual question. There are many fusion strategies that can either assume quite simple forms, such as vector multiplication or concatenation, or be really complex, involving multilayered neural networks, tensor decomposition, and bi-linear pooling, just to name a few.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.8" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, we adopt the element-wise vector multiplication (also referred as Hadamard product) as the default fusion strategy. This approach requires the feature representations to be fused to have the same dimensionality. Therefore, we project them using a fully-connected layer to reduce their dimension from <math id="S3.p7.1.m1.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.p7.1.m1.1a"><mn id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><cn type="integer" id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">2048</annotation></semantics></math> to <math id="S3.p7.2.m2.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.p7.2.m2.1a"><mn id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><cn type="integer" id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">1024</annotation></semantics></math>. After being fused together, the multimodal features are finally passed through a fully-connected layer that provides scores (<span id="S3.p7.8.1" class="ltx_text ltx_font_italic">logits</span>) further converted into probabilities via a softmax function (<math id="S3.p7.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.p7.3.m3.1a"><mi id="S3.p7.3.m3.1.1" xref="S3.p7.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.p7.3.m3.1b"><ci id="S3.p7.3.m3.1.1.cmml" xref="S3.p7.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.3.m3.1c">S</annotation></semantics></math>). We want to maximize the probability <math id="S3.p7.4.m4.1" class="ltx_math_unparsed" alttext="P(Y=y|X=x,Q=q)" display="inline"><semantics id="S3.p7.4.m4.1a"><mrow id="S3.p7.4.m4.1b"><mi id="S3.p7.4.m4.1.1">P</mi><mrow id="S3.p7.4.m4.1.2"><mo stretchy="false" id="S3.p7.4.m4.1.2.1">(</mo><mi id="S3.p7.4.m4.1.2.2">Y</mi><mo id="S3.p7.4.m4.1.2.3">=</mo><mi id="S3.p7.4.m4.1.2.4">y</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.p7.4.m4.1.2.5">|</mo><mi id="S3.p7.4.m4.1.2.6">X</mi><mo id="S3.p7.4.m4.1.2.7">=</mo><mi id="S3.p7.4.m4.1.2.8">x</mi><mo id="S3.p7.4.m4.1.2.9">,</mo><mi id="S3.p7.4.m4.1.2.10">Q</mi><mo id="S3.p7.4.m4.1.2.11">=</mo><mi id="S3.p7.4.m4.1.2.12">q</mi><mo stretchy="false" id="S3.p7.4.m4.1.2.13">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.p7.4.m4.1c">P(Y=y|X=x,Q=q)</annotation></semantics></math> of the correct answer <math id="S3.p7.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.p7.5.m5.1a"><mi id="S3.p7.5.m5.1.1" xref="S3.p7.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p7.5.m5.1b"><ci id="S3.p7.5.m5.1.1.cmml" xref="S3.p7.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.5.m5.1c">y</annotation></semantics></math> given the image <math id="S3.p7.6.m6.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.p7.6.m6.1a"><mi id="S3.p7.6.m6.1.1" xref="S3.p7.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p7.6.m6.1b"><ci id="S3.p7.6.m6.1.1.cmml" xref="S3.p7.6.m6.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.6.m6.1c">X</annotation></semantics></math> and the provided question <math id="S3.p7.7.m7.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p7.7.m7.1a"><mi id="S3.p7.7.m7.1.1" xref="S3.p7.7.m7.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p7.7.m7.1b"><ci id="S3.p7.7.m7.1.1.cmml" xref="S3.p7.7.m7.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.7.m7.1c">Q</annotation></semantics></math>. Our models are trained to choose within a set comprised by the <math id="S3.p7.8.m8.1" class="ltx_Math" alttext="3000" display="inline"><semantics id="S3.p7.8.m8.1a"><mn id="S3.p7.8.m8.1.1" xref="S3.p7.8.m8.1.1.cmml">3000</mn><annotation-xml encoding="MathML-Content" id="S3.p7.8.m8.1b"><cn type="integer" id="S3.p7.8.m8.1.1.cmml" xref="S3.p7.8.m8.1.1">3000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.8.m8.1c">3000</annotation></semantics></math> most frequent answers extracted from both training and validation sets of the VQA v<span id="S3.p7.8.2" class="ltx_text ltx_font_italic">2.0</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental Setup</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For conducting this study we decided to use the VQA v<span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">2.0</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. It is one of the largest and most frequently used datasets for training and evaluation of models in this task, being the official dataset used in yearly challenges hosted by mainstream computer vision venues <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>VQA Challenge: https://visualqa.org/challenge.html</span></span></span>. This dataset enhances the original one <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> by alleviating bias problems within the data and increasing the original number of instances.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">VQA v<span id="S4.SS1.p2.2.1" class="ltx_text ltx_font_italic">2.0</span> contains over <math id="S4.SS1.p2.1.m1.2" class="ltx_Math" alttext="200,000" display="inline"><semantics id="S4.SS1.p2.1.m1.2a"><mrow id="S4.SS1.p2.1.m1.2.3.2" xref="S4.SS1.p2.1.m1.2.3.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">200</mn><mo id="S4.SS1.p2.1.m1.2.3.2.1" xref="S4.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p2.1.m1.2.2" xref="S4.SS1.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.2b"><list id="S4.SS1.p2.1.m1.2.3.1.cmml" xref="S4.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">200</cn><cn type="integer" id="S4.SS1.p2.1.m1.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.2c">200,000</annotation></semantics></math> images from MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, over 1 million questions and <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\approx 11" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml"></mi><mo id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">≈</mo><mn id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><approx id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></approx><csymbol cd="latexml" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\approx 11</annotation></semantics></math> million answers. In addition, it has at least two questions per image, which prevents the model from answering the question without considering the input image.
We follow VQA v<span id="S4.SS1.p2.2.2" class="ltx_text ltx_font_italic">2.0</span> standards and adopt the official provided splits allowing for fair comparison with other approaches. The splits we use are Validation, Test-Dev, Test-Standard.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.5" class="ltx_p">In this work, results of the ablation experiments are reported on the Validation set, which is the default option used for this kind of experiment. In some experiments we also report the training set accuracy to verify evidence of overfitting due to excessive model complexity. Training data has a total of <math id="S4.SS1.p3.1.m1.2" class="ltx_Math" alttext="443,757" display="inline"><semantics id="S4.SS1.p3.1.m1.2a"><mrow id="S4.SS1.p3.1.m1.2.3.2" xref="S4.SS1.p3.1.m1.2.3.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">443</mn><mo id="S4.SS1.p3.1.m1.2.3.2.1" xref="S4.SS1.p3.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.1.m1.2.2" xref="S4.SS1.p3.1.m1.2.2.cmml">757</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.2b"><list id="S4.SS1.p3.1.m1.2.3.1.cmml" xref="S4.SS1.p3.1.m1.2.3.2"><cn type="integer" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">443</cn><cn type="integer" id="S4.SS1.p3.1.m1.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2">757</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.2c">443,757</annotation></semantics></math> questions labeled with <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mn id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><cn type="integer" id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">4</annotation></semantics></math> million answers, while the Test-Dev has a total of <math id="S4.SS1.p3.3.m3.2" class="ltx_Math" alttext="214,354" display="inline"><semantics id="S4.SS1.p3.3.m3.2a"><mrow id="S4.SS1.p3.3.m3.2.3.2" xref="S4.SS1.p3.3.m3.2.3.1.cmml"><mn id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">214</mn><mo id="S4.SS1.p3.3.m3.2.3.2.1" xref="S4.SS1.p3.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.2.2" xref="S4.SS1.p3.3.m3.2.2.cmml">354</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.2b"><list id="S4.SS1.p3.3.m3.2.3.1.cmml" xref="S4.SS1.p3.3.m3.2.3.2"><cn type="integer" id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">214</cn><cn type="integer" id="S4.SS1.p3.3.m3.2.2.cmml" xref="S4.SS1.p3.3.m3.2.2">354</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.2c">214,354</annotation></semantics></math> questions. Note that the validation size is about 4-fold larger than ImageNet’s, which contains about <math id="S4.SS1.p3.4.m4.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S4.SS1.p3.4.m4.2a"><mrow id="S4.SS1.p3.4.m4.2.3.2" xref="S4.SS1.p3.4.m4.2.3.1.cmml"><mn id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">50</mn><mo id="S4.SS1.p3.4.m4.2.3.2.1" xref="S4.SS1.p3.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.4.m4.2.2" xref="S4.SS1.p3.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.2b"><list id="S4.SS1.p3.4.m4.2.3.1.cmml" xref="S4.SS1.p3.4.m4.2.3.2"><cn type="integer" id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">50</cn><cn type="integer" id="S4.SS1.p3.4.m4.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.2c">50,000</annotation></semantics></math> samples. Therefore, one must keep in mind that even small performance gaps might indicate quite significant results improvement. For instance, 1% accuracy gains depict <math id="S4.SS1.p3.5.m5.2" class="ltx_Math" alttext="\approx 2,000" display="inline"><semantics id="S4.SS1.p3.5.m5.2a"><mrow id="S4.SS1.p3.5.m5.2.3" xref="S4.SS1.p3.5.m5.2.3.cmml"><mi id="S4.SS1.p3.5.m5.2.3.2" xref="S4.SS1.p3.5.m5.2.3.2.cmml"></mi><mo id="S4.SS1.p3.5.m5.2.3.1" xref="S4.SS1.p3.5.m5.2.3.1.cmml">≈</mo><mrow id="S4.SS1.p3.5.m5.2.3.3.2" xref="S4.SS1.p3.5.m5.2.3.3.1.cmml"><mn id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">2</mn><mo id="S4.SS1.p3.5.m5.2.3.3.2.1" xref="S4.SS1.p3.5.m5.2.3.3.1.cmml">,</mo><mn id="S4.SS1.p3.5.m5.2.2" xref="S4.SS1.p3.5.m5.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.2b"><apply id="S4.SS1.p3.5.m5.2.3.cmml" xref="S4.SS1.p3.5.m5.2.3"><approx id="S4.SS1.p3.5.m5.2.3.1.cmml" xref="S4.SS1.p3.5.m5.2.3.1"></approx><csymbol cd="latexml" id="S4.SS1.p3.5.m5.2.3.2.cmml" xref="S4.SS1.p3.5.m5.2.3.2">absent</csymbol><list id="S4.SS1.p3.5.m5.2.3.3.1.cmml" xref="S4.SS1.p3.5.m5.2.3.3.2"><cn type="integer" id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">2</cn><cn type="integer" id="S4.SS1.p3.5.m5.2.2.cmml" xref="S4.SS1.p3.5.m5.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.2c">\approx 2,000</annotation></semantics></math> additional instances being correctly classified. We submit the predictions of our best models to the online evaluation servers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> so as to obtain results for the Test-Standard split, allowing for a fair comparison to state-of-the-art approaches.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Evaluation Metric</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Free and open-ended questions result in a diverse set of possible answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For some questions, a simple <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">yes</span> or <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">no</span> answer may be sufficient. Other questions, however, may require more complex answers. In addition, it is worth noticing that multiple answers may be considered correct, such as <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">gray</span> and <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_italic">light gray</span>. Therefore, VQA v<span id="S4.SS2.p1.1.5" class="ltx_text ltx_font_italic">2.0</span> provides ten ground-truth answers for each question. These answers were collected from ten different randomly-chosen humans.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">The evaluation metric used to measure model performance in the open-ended Visual Question Answering task is a particular kind of accuracy. For each question in the input dataset, the model’s most likely response is compared to the ten possible answers provided by humans in the dataset associated with that question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and evaluated according to Equation <a href="#S4.E1" title="In IV-B Evaluation Metric ‣ IV Experimental Setup ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In this approach, the prediction is considered totally correct only if at least <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">3</annotation></semantics></math> out of <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">10</annotation></semantics></math> people provided that same answer.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2002.05104/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="300" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall validation accuracy improvement (<math id="S4.F2.2.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S4.F2.2.m1.1b"><mi mathvariant="normal" id="S4.F2.2.m1.1.1" xref="S4.F2.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><ci id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">\Delta</annotation></semantics></math>) over the baseline architecture. Models denoted with <span id="S4.F2.4.1" class="ltx_text ltx_font_italic">*</span> present fixed word-embedding representations, i.e., they are not updated via back-propagation.</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2002.05104/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overall accuracy <span id="S4.F3.6.1" class="ltx_text ltx_font_italic">vs.</span> number of parameters trade-off analysis. Circled markers denote two-layered RNNs. Number of parameters increases due to the number of hidden units <math id="S4.F3.3.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S4.F3.3.m1.1b"><mi id="S4.F3.3.m1.1.1" xref="S4.F3.3.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S4.F3.3.m1.1c"><ci id="S4.F3.3.m1.1.1.cmml" xref="S4.F3.3.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.3.m1.1d">H</annotation></semantics></math> within the RNN. In this experiment we vary <math id="S4.F3.4.m2.5" class="ltx_Math" alttext="H\in\{128,256,512,1024,2048\}" display="inline"><semantics id="S4.F3.4.m2.5b"><mrow id="S4.F3.4.m2.5.6" xref="S4.F3.4.m2.5.6.cmml"><mi id="S4.F3.4.m2.5.6.2" xref="S4.F3.4.m2.5.6.2.cmml">H</mi><mo id="S4.F3.4.m2.5.6.1" xref="S4.F3.4.m2.5.6.1.cmml">∈</mo><mrow id="S4.F3.4.m2.5.6.3.2" xref="S4.F3.4.m2.5.6.3.1.cmml"><mo stretchy="false" id="S4.F3.4.m2.5.6.3.2.1" xref="S4.F3.4.m2.5.6.3.1.cmml">{</mo><mn id="S4.F3.4.m2.1.1" xref="S4.F3.4.m2.1.1.cmml">128</mn><mo id="S4.F3.4.m2.5.6.3.2.2" xref="S4.F3.4.m2.5.6.3.1.cmml">,</mo><mn id="S4.F3.4.m2.2.2" xref="S4.F3.4.m2.2.2.cmml">256</mn><mo id="S4.F3.4.m2.5.6.3.2.3" xref="S4.F3.4.m2.5.6.3.1.cmml">,</mo><mn id="S4.F3.4.m2.3.3" xref="S4.F3.4.m2.3.3.cmml">512</mn><mo id="S4.F3.4.m2.5.6.3.2.4" xref="S4.F3.4.m2.5.6.3.1.cmml">,</mo><mn id="S4.F3.4.m2.4.4" xref="S4.F3.4.m2.4.4.cmml">1024</mn><mo id="S4.F3.4.m2.5.6.3.2.5" xref="S4.F3.4.m2.5.6.3.1.cmml">,</mo><mn id="S4.F3.4.m2.5.5" xref="S4.F3.4.m2.5.5.cmml">2048</mn><mo stretchy="false" id="S4.F3.4.m2.5.6.3.2.6" xref="S4.F3.4.m2.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.4.m2.5c"><apply id="S4.F3.4.m2.5.6.cmml" xref="S4.F3.4.m2.5.6"><in id="S4.F3.4.m2.5.6.1.cmml" xref="S4.F3.4.m2.5.6.1"></in><ci id="S4.F3.4.m2.5.6.2.cmml" xref="S4.F3.4.m2.5.6.2">𝐻</ci><set id="S4.F3.4.m2.5.6.3.1.cmml" xref="S4.F3.4.m2.5.6.3.2"><cn type="integer" id="S4.F3.4.m2.1.1.cmml" xref="S4.F3.4.m2.1.1">128</cn><cn type="integer" id="S4.F3.4.m2.2.2.cmml" xref="S4.F3.4.m2.2.2">256</cn><cn type="integer" id="S4.F3.4.m2.3.3.cmml" xref="S4.F3.4.m2.3.3">512</cn><cn type="integer" id="S4.F3.4.m2.4.4.cmml" xref="S4.F3.4.m2.4.4">1024</cn><cn type="integer" id="S4.F3.4.m2.5.5.cmml" xref="S4.F3.4.m2.5.5">2048</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.4.m2.5d">H\in\{128,256,512,1024,2048\}</annotation></semantics></math>.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.3" class="ltx_Math" alttext="\centering\scriptsize accuracy=\min{\bigg{(}\frac{\text{\# humans that provided that answer}}{3},1\bigg{)}}\@add@centering" display="block"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.4" xref="S4.E1.m1.3.4.cmml"><mrow id="S4.E1.m1.3.4.2" xref="S4.E1.m1.3.4.2.cmml"><mi mathsize="70%" id="S4.E1.m1.3.4.2.2" xref="S4.E1.m1.3.4.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1" xref="S4.E1.m1.3.4.2.1.cmml">​</mo><mi mathsize="70%" id="S4.E1.m1.3.4.2.3" xref="S4.E1.m1.3.4.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1a" xref="S4.E1.m1.3.4.2.1.cmml">​</mo><mi mathsize="70%" id="S4.E1.m1.3.4.2.4" xref="S4.E1.m1.3.4.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1b" xref="S4.E1.m1.3.4.2.1.cmml">​</mo><mi mathsize="70%" id="S4.E1.m1.3.4.2.5" xref="S4.E1.m1.3.4.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1c" xref="S4.E1.m1.3.4.2.1.cmml">​</mo><mi mathsize="70%" id="S4.E1.m1.3.4.2.6" xref="S4.E1.m1.3.4.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1d" xref="S4.E1.m1.3.4.2.1.cmml">​</mo><mi mathsize="70%" id="S4.E1.m1.3.4.2.7" xref="S4.E1.m1.3.4.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1e" xref="S4.E1.m1.3.4.2.1.cmml">​</mo><mi mathsize="70%" id="S4.E1.m1.3.4.2.8" xref="S4.E1.m1.3.4.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.4.2.1f" xref="S4.E1.m1.3.4.2.1.cmml">​</mo><mi mathsize="70%" id="S4.E1.m1.3.4.2.9" xref="S4.E1.m1.3.4.2.9.cmml">y</mi></mrow><mo mathsize="70%" id="S4.E1.m1.3.4.1" xref="S4.E1.m1.3.4.1.cmml">=</mo><mrow id="S4.E1.m1.3.4.3.2" xref="S4.E1.m1.3.4.3.1.cmml"><mi mathsize="70%" id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">min</mi><mo id="S4.E1.m1.3.4.3.2a" xref="S4.E1.m1.3.4.3.1.cmml">⁡</mo><mrow id="S4.E1.m1.3.4.3.2.1" xref="S4.E1.m1.3.4.3.1.cmml"><mo maxsize="210%" minsize="210%" id="S4.E1.m1.3.4.3.2.1.1" xref="S4.E1.m1.3.4.3.1.cmml">(</mo><mfrac id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><mtext mathsize="70%" id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2a.cmml"># humans that provided that answer</mtext><mn mathsize="70%" id="S4.E1.m1.2.2.3" xref="S4.E1.m1.2.2.3.cmml">3</mn></mfrac><mo mathsize="70%" id="S4.E1.m1.3.4.3.2.1.2" xref="S4.E1.m1.3.4.3.1.cmml">,</mo><mn mathsize="70%" id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">1</mn><mo maxsize="210%" minsize="210%" id="S4.E1.m1.3.4.3.2.1.3" xref="S4.E1.m1.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.4.cmml" xref="S4.E1.m1.3.4"><eq id="S4.E1.m1.3.4.1.cmml" xref="S4.E1.m1.3.4.1"></eq><apply id="S4.E1.m1.3.4.2.cmml" xref="S4.E1.m1.3.4.2"><times id="S4.E1.m1.3.4.2.1.cmml" xref="S4.E1.m1.3.4.2.1"></times><ci id="S4.E1.m1.3.4.2.2.cmml" xref="S4.E1.m1.3.4.2.2">𝑎</ci><ci id="S4.E1.m1.3.4.2.3.cmml" xref="S4.E1.m1.3.4.2.3">𝑐</ci><ci id="S4.E1.m1.3.4.2.4.cmml" xref="S4.E1.m1.3.4.2.4">𝑐</ci><ci id="S4.E1.m1.3.4.2.5.cmml" xref="S4.E1.m1.3.4.2.5">𝑢</ci><ci id="S4.E1.m1.3.4.2.6.cmml" xref="S4.E1.m1.3.4.2.6">𝑟</ci><ci id="S4.E1.m1.3.4.2.7.cmml" xref="S4.E1.m1.3.4.2.7">𝑎</ci><ci id="S4.E1.m1.3.4.2.8.cmml" xref="S4.E1.m1.3.4.2.8">𝑐</ci><ci id="S4.E1.m1.3.4.2.9.cmml" xref="S4.E1.m1.3.4.2.9">𝑦</ci></apply><apply id="S4.E1.m1.3.4.3.1.cmml" xref="S4.E1.m1.3.4.3.2"><min id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"></min><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><divide id="S4.E1.m1.2.2.1.cmml" xref="S4.E1.m1.2.2"></divide><ci id="S4.E1.m1.2.2.2a.cmml" xref="S4.E1.m1.2.2.2"><mtext mathsize="70%" id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"># humans that provided that answer</mtext></ci><cn type="integer" id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.3">3</cn></apply><cn type="integer" id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">\centering\scriptsize accuracy=\min{\bigg{(}\frac{\text{\# humans that provided that answer}}{3},1\bigg{)}}\@add@centering</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Hyper-parameters</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> we train our models in a classification-based manner, in which we minimize the cross-entropy loss calculated with an image-question-answer triplet sampled from the training set. We optimize the parameters of all VQA models using Adamax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> optimizer with a base learning rate of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="7\times 10^{-4}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"><mn id="S4.SS3.p1.1.m1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.p1.1.m1.1.1.3.3" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS3.p1.1.m1.1.1.3.3a" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p1.1.m1.1.1.3.3.2" xref="S4.SS3.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">7</cn><apply id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2">10</cn><apply id="S4.SS3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3"><minus id="S4.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">7\times 10^{-4}</annotation></semantics></math>, with exception of BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> in which we apply a 10-fold reduction as suggested in the original paper. We used a learning rate warm-up schedule in which we halve the base learning rate and linearly increase it until the fourth epoch where it reaches twice its base value. It remains the same until the tenth epoch, where we start applying a 25% decay every two epochs. Gradients are calculated using batch sizes of <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="integer" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">64</annotation></semantics></math> instances, and we train all models for 20 epochs.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experimental Analysis</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section we show the experimental analysis for each component in the baseline VQA model. We also provide a summary of our findings regarding the impact of each part. Finally, we train a model with all the components that provide top results and compare it against state-of-the-art approaches.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Text Encoder</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In our first experiment, we analyze the impact of different embeddings for the textual representation of the questions. To this end, we evaluate: (i) the impact of word-embeddings (pre-trained, or trained from scratch); and (ii) the role of the temporal encoding function, i.e., distinct RNN types, as well as pre-trained sentence encoders (e.g., Skip-Thoughts, BERT).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The word-embedding strategies we evaluate are <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_smallcaps">Learnable Word Embedding</span> (randomly initialized and trained from scratch), Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. We also use word-level representations from widely used sentence embeddings strategies, namely Skip-Thoughts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. To do so, we use the hidden-states from the Skip-thoughts GRU network, while for BERT we use the activations of the last layer as word-level information. Those vectors feed an RNN that encodes the temporal sequence into a single global vector.
Different types of RNNs are also investigated for encoding textual representation, including LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, Bidirectional LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and Bidirectional GRU. For bidirectional architectures we concatenate both forward and backward hidden-states so as to aggregate information from both directions. Those approaches are also compared to a linear strategy, where we use a fully-connected layer followed by a global average pooling on the temporal dimension. The linear strategy discards any order information so we can demonstrate the role of the recurrent network as a temporal encoder to improve model performance.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.2" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 ‣ IV-B Evaluation Metric ‣ IV Experimental Setup ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the performance variation of different types of word-embeddings, recurrent networks, initialization strategies, and the effect of fine-tuning the textual encoder.
Clearly, the linear layer is outperformed by any type of recurrent layer. When using Skip-Thoughts the difference reaches <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="2.22\%" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">2.22</mn><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">2.22</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">2.22\%</annotation></semantics></math>, which accounts for almost <math id="S5.SS1.p3.2.m2.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S5.SS1.p3.2.m2.2a"><mrow id="S5.SS1.p3.2.m2.2.3.2" xref="S5.SS1.p3.2.m2.2.3.1.cmml"><mn id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">5</mn><mo id="S5.SS1.p3.2.m2.2.3.2.1" xref="S5.SS1.p3.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS1.p3.2.m2.2.2" xref="S5.SS1.p3.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.2b"><list id="S5.SS1.p3.2.m2.2.3.1.cmml" xref="S5.SS1.p3.2.m2.2.3.2"><cn type="integer" id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">5</cn><cn type="integer" id="S5.SS1.p3.2.m2.2.2.cmml" xref="S5.SS1.p3.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.2c">5,000</annotation></semantics></math> instances that the linear model mistakenly labeled. The only case in which the linear approach performed well is when trained with BERT. That is expected since Transformer-based architectures employ several attention layers that present the advantage of achieving the total receptive field size in all layers. While doing so, BERT also encodes temporal information with special positional vectors that allow for learning temporal relations. Hence, it is easier for the model to encode order information within word-level vectors without using recurrent layers.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">For the Skip-Thoughts vector model, considering that its original architecture is based on GRUs, we evaluate both the randomly initialized and the pre-trained GRU of the original model, described as [GRU] and [GRU (skip)], respectively. We noticed that both options present virtually the same performance. In fact, GRU trained from scratch performed <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="0.13\%" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mrow id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml"><mn id="S5.SS1.p4.1.m1.1.1.2" xref="S5.SS1.p4.1.m1.1.1.2.cmml">0.13</mn><mo id="S5.SS1.p4.1.m1.1.1.1" xref="S5.SS1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><apply id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.p4.1.m1.1.1.2">0.13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">0.13\%</annotation></semantics></math> better than its pre-trained version.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Analyzing the results obtained with pre-trained word embeddings, it is clear that GloVe obtained consistently better results than the Word2Vec counterpart. We believe that GloVe vectors perform better given that they capture not only local context statistics as in Word2Vec, but they also incorporate global statistics such as co-occurrence of words.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">One can also observe that the use of different RNNs models inflicts minor effects on the results. It might be more advisable to use GRU networks since they halve the number of trainable parameters when compared to the LSTMs, albeit being faster and consistently presenting top results. Note also that the best results for Skip-Thoughts, Word2Vec, and GloVe were all quite similar, without any major variation regarding accuracy.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p">The best overall result is achieved when using BERT to extract the textual features. BERT versions using either the linear layer or the RNNs outperformed all other pre-trained embeddings and sentence encoders. In addition, the overall training accuracy for BERT models is not so high compared to all other approaches. That might be an indication that BERT models are less prone to overfit training data, and therefore present better generalization ability.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.3" class="ltx_p">Results make it clear that when using BERT, one must fine-tune it for achieving top performance. Figure <a href="#S4.F2" title="Figure 2 ‣ IV-B Evaluation Metric ‣ IV Experimental Setup ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that it is possible to achieve a <math id="S5.SS1.p8.1.m1.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S5.SS1.p8.1.m1.1a"><mrow id="S5.SS1.p8.1.m1.1.1" xref="S5.SS1.p8.1.m1.1.1.cmml"><mn id="S5.SS1.p8.1.m1.1.1.2" xref="S5.SS1.p8.1.m1.1.1.2.cmml">3</mn><mo id="S5.SS1.p8.1.m1.1.1.1" xref="S5.SS1.p8.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p8.1.m1.1b"><apply id="S5.SS1.p8.1.m1.1.1.cmml" xref="S5.SS1.p8.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p8.1.m1.1.1.1.cmml" xref="S5.SS1.p8.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p8.1.m1.1.1.2.cmml" xref="S5.SS1.p8.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p8.1.m1.1c">3\%</annotation></semantics></math> to <math id="S5.SS1.p8.2.m2.1" class="ltx_Math" alttext="4\%" display="inline"><semantics id="S5.SS1.p8.2.m2.1a"><mrow id="S5.SS1.p8.2.m2.1.1" xref="S5.SS1.p8.2.m2.1.1.cmml"><mn id="S5.SS1.p8.2.m2.1.1.2" xref="S5.SS1.p8.2.m2.1.1.2.cmml">4</mn><mo id="S5.SS1.p8.2.m2.1.1.1" xref="S5.SS1.p8.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p8.2.m2.1b"><apply id="S5.SS1.p8.2.m2.1.1.cmml" xref="S5.SS1.p8.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.p8.2.m2.1.1.1.cmml" xref="S5.SS1.p8.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p8.2.m2.1.1.2.cmml" xref="S5.SS1.p8.2.m2.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p8.2.m2.1c">4\%</annotation></semantics></math> accuracy improvement when updating BERT weights with <math id="S5.SS1.p8.3.m3.1" class="ltx_Math" alttext="1/10" display="inline"><semantics id="S5.SS1.p8.3.m3.1a"><mrow id="S5.SS1.p8.3.m3.1.1" xref="S5.SS1.p8.3.m3.1.1.cmml"><mn id="S5.SS1.p8.3.m3.1.1.2" xref="S5.SS1.p8.3.m3.1.1.2.cmml">1</mn><mo id="S5.SS1.p8.3.m3.1.1.1" xref="S5.SS1.p8.3.m3.1.1.1.cmml">/</mo><mn id="S5.SS1.p8.3.m3.1.1.3" xref="S5.SS1.p8.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p8.3.m3.1b"><apply id="S5.SS1.p8.3.m3.1.1.cmml" xref="S5.SS1.p8.3.m3.1.1"><divide id="S5.SS1.p8.3.m3.1.1.1.cmml" xref="S5.SS1.p8.3.m3.1.1.1"></divide><cn type="integer" id="S5.SS1.p8.3.m3.1.1.2.cmml" xref="S5.SS1.p8.3.m3.1.1.2">1</cn><cn type="integer" id="S5.SS1.p8.3.m3.1.1.3.cmml" xref="S5.SS1.p8.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p8.3.m3.1c">1/10</annotation></semantics></math> of the base learning rate. Moreover, Figure <a href="#S4.F3" title="Figure 3 ‣ IV-B Evaluation Metric ‣ IV Experimental Setup ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that the use of a pre-training strategy is helpful, once Skip-thoughts and BERT outperform trainable word-embeddings in most of the evaluated settings. Is also make clear that using a single-layered RNNs provide best results, and are far more efficient in terms of parameters.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Image Encoder</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Experiments in this section analyze the visual feature extraction layers. The baseline uses the Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> network, and we will also experiment with other pre-trained neural networks to encode image information so we can observe their impact on predictive performance. Additionally to Faster-RCNN, we experiment with two widely used networks for VQA, namely ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and VGG-16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Impact of the network used for visual feature extraction.</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.1.1" class="ltx_text ltx_font_italic">Embedding</span></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">RNN</th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Network</th>
<th id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Training</th>
<th id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Validation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T1.1.2.1.1.1" class="ltx_text">BERT</span></td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T1.1.2.1.2.1" class="ltx_text">GRU</span></td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Faster</td>
<td id="S5.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.34</td>
<td id="S5.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.2.1.5.1" class="ltx_text ltx_font_bold">58.88</span></td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">ResNet-101</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">76.14</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">56.09</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">VGG-16</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">65.59</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">53.49</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.3" class="ltx_p">Table <a href="#S5.T1" title="TABLE I ‣ V-B Image Encoder ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> illustrates the result of this experiment. Intuitively, visual features provide a larger impact on model’s performance. The accuracy difference between the best and the worst performing approaches is <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\approx 5\%" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">≈</mo><mrow id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml"><mn id="S5.SS2.p2.1.m1.1.1.3.2" xref="S5.SS2.p2.1.m1.1.1.3.2.cmml">5</mn><mo id="S5.SS2.p2.1.m1.1.1.3.1" xref="S5.SS2.p2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><approx id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">absent</csymbol><apply id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.3.1.cmml" xref="S5.SS2.p2.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS2.p2.1.m1.1.1.3.2.cmml" xref="S5.SS2.p2.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\approx 5\%</annotation></semantics></math>. That difference accounts for roughly <math id="S5.SS2.p2.2.m2.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S5.SS2.p2.2.m2.2a"><mrow id="S5.SS2.p2.2.m2.2.3.2" xref="S5.SS2.p2.2.m2.2.3.1.cmml"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">10</mn><mo id="S5.SS2.p2.2.m2.2.3.2.1" xref="S5.SS2.p2.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS2.p2.2.m2.2.2" xref="S5.SS2.p2.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.2b"><list id="S5.SS2.p2.2.m2.2.3.1.cmml" xref="S5.SS2.p2.2.m2.2.3.2"><cn type="integer" id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">10</cn><cn type="integer" id="S5.SS2.p2.2.m2.2.2.cmml" xref="S5.SS2.p2.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.2c">10,000</annotation></semantics></math> validation set instances.
VGG-16 visual features presented the worst accuracy, but that was expected since it is the oldest network used in this study. In addition, it is only sixteen layers deep, and it has been shown that the depth of the network is quite important to hierarchically encode complex structures. Moreover, VGG-16 architecture encodes all the information in a <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="4096" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mn id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><cn type="integer" id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">4096</annotation></semantics></math> dimensional vector that is extracted after the second fully-connected layer at the end. That vector encodes little to none spatial information, which makes it almost impossible for the network to answer questions on the spatial positioning of objects.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">ResNet-101 obtained intermediate results.
It is a much deeper network than VGG-16 and it achieves much better results on ImageNet, which shows the difference of the the learning capacity of both networks. ResNet-101 provides information encoded in <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mn id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><cn type="integer" id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">2048</annotation></semantics></math> dimensional vectors, extracted from the global average pooling layer, which also summarizes spatial information into a fixed-sized representation.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.2" class="ltx_p">The best result as a visual feature extractor was achieved by the Faster-RCNN fine-tuned on the Visual Genome dataset. Such a network employs a ResNet-152 as backbone for training an RPN-based object detector. In addition, given that it was fine-tuned on the Visual Genome dataset, it allows for the training of robust models suited for general feature extraction. Hence, differently from the previous ResNet and VGG approaches, the Faster-RCNN approach is trained to detect objects, and therefore one can use it to extract features from the most relevant image regions. Each region is encoded as a <math id="S5.SS2.p4.1.m1.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S5.SS2.p4.1.m1.1a"><mn id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><cn type="integer" id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">2048</annotation></semantics></math> dimensional vector. They contain rich information regarding regions and objects, since object detectors often operate over high-dimensional images, instead of resized ones (e.g., <math id="S5.SS2.p4.2.m2.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S5.SS2.p4.2.m2.1a"><mrow id="S5.SS2.p4.2.m2.1.1" xref="S5.SS2.p4.2.m2.1.1.cmml"><mn id="S5.SS2.p4.2.m2.1.1.2" xref="S5.SS2.p4.2.m2.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p4.2.m2.1.1.1" xref="S5.SS2.p4.2.m2.1.1.1.cmml">×</mo><mn id="S5.SS2.p4.2.m2.1.1.3" xref="S5.SS2.p4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.2.m2.1b"><apply id="S5.SS2.p4.2.m2.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1"><times id="S5.SS2.p4.2.m2.1.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1.1"></times><cn type="integer" id="S5.SS2.p4.2.m2.1.1.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2">256</cn><cn type="integer" id="S5.SS2.p4.2.m2.1.1.3.cmml" xref="S5.SS2.p4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.2.m2.1c">256\times 256</annotation></semantics></math>) as in typical classification networks. Hence, even after applying global pooling over regions, the network still has access to spatial information because of the pre-extracted regions of interest from each image.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Experiment using different fusion strategies.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">Embedding</span></th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">RNN</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Fusion</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Training</th>
<th id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Validation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<td id="S5.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T2.1.2.1.1.1" class="ltx_text">BERT</span></td>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T2.1.2.1.2.1" class="ltx_text">GRU</span></td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mult</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.28</td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.1.2.1.5.1" class="ltx_text ltx_font_bold">58.75</span></td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<td id="S5.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Concat</td>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">67.85</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">55.07</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<td id="S5.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Sum</td>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">68.21</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">54.93</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Fusion strategy</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In order to analyze the impact that the different fusion methods have on the network performance, three simple fusion mechanisms were analyzed: element-wise multiplication, concatenation, and summation of the textual and visual features.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The choice of the fusion component is essential in VQA architectures, since its output generates multi-modal features used for answering the given visual question. The resulting multi-modal vector is projected into a <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="3000" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mn id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">3000</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><cn type="integer" id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">3000</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">3000</annotation></semantics></math>-dimensional label space, which provides a probability distribution over each possible answer to the question at hand <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Table <a href="#S5.T2" title="TABLE II ‣ V-B Image Encoder ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents the experimental results with the fusion strategies. The best result is obtained using the element-wise multiplication. Such an approach functions as a filtering strategy that is able to scale down the importance of irrelevant dimensions from the visual-question feature vectors. In other words, vector dimensions with high cross-modal affinity will have their magnitudes increased, differently from the uncorrelated ones that will have their values reduced. Summation does provide the worst results overall, closely followed by the concatenation operator. Moreover, among all the fusion strategies used in this study, multiplication seems to ease the training process as it presents a much higher training set accuracy (<math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="\approx 11\%" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><mrow id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml"></mi><mo id="S5.SS3.p3.1.m1.1.1.1" xref="S5.SS3.p3.1.m1.1.1.1.cmml">≈</mo><mrow id="S5.SS3.p3.1.m1.1.1.3" xref="S5.SS3.p3.1.m1.1.1.3.cmml"><mn id="S5.SS3.p3.1.m1.1.1.3.2" xref="S5.SS3.p3.1.m1.1.1.3.2.cmml">11</mn><mo id="S5.SS3.p3.1.m1.1.1.3.1" xref="S5.SS3.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><approx id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2">absent</csymbol><apply id="S5.SS3.p3.1.m1.1.1.3.cmml" xref="S5.SS3.p3.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS3.p3.1.m1.1.1.3.1.cmml" xref="S5.SS3.p3.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS3.p3.1.m1.1.1.3.2.cmml" xref="S5.SS3.p3.1.m1.1.1.3.2">11</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">\approx 11\%</annotation></semantics></math> improvement) as well.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Experiment using different attention mechanisms.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T3.1.2.1.1.1" class="ltx_text ltx_font_italic">Embedding</span></th>
<th id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">RNN</th>
<th id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Attention</th>
<th id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Training</th>
<th id="S5.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Validation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.3.1" class="ltx_tr">
<td id="S5.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T3.1.3.1.1.1" class="ltx_text">BERT</span></td>
<td id="S5.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T3.1.3.1.2.1" class="ltx_text">GRU</span></td>
<td id="S5.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.20</td>
<td id="S5.T3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.75</td>
</tr>
<tr id="S5.T3.1.4.2" class="ltx_tr">
<td id="S5.T3.1.4.2.1" class="ltx_td ltx_align_center ltx_border_r">Co-Attention</td>
<td id="S5.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">71.10</td>
<td id="S5.T3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">58.54</td>
</tr>
<tr id="S5.T3.1.5.3" class="ltx_tr">
<td id="S5.T3.1.5.3.1" class="ltx_td ltx_align_center ltx_border_r">Co-Attention (L2 norm)</td>
<td id="S5.T3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">86.03</td>
<td id="S5.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">64.03</td>
</tr>
<tr id="S5.T3.1.6.4" class="ltx_tr">
<td id="S5.T3.1.6.4.1" class="ltx_td ltx_align_center ltx_border_r">Top Down</td>
<td id="S5.T3.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">82.64</td>
<td id="S5.T3.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">62.37</td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Top Down (<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\sigma=" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.m1.1.1.2.cmml">σ</mi><mo id="S5.T3.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.m1.1.1.1.cmml">=</mo><mi id="S5.T3.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"><eq id="S5.T3.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1.1"></eq><ci id="S5.T3.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.m1.1.1.2">𝜎</ci><csymbol cd="latexml" id="S5.T3.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\sigma=</annotation></semantics></math>ReLU)</td>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">87.02</td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.1.1.3.1" class="ltx_text ltx_font_bold">64.12</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Attention Mechanism</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Finally, we analyze the impact of different attention mechanisms, such as Top-Down Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and Co-Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. These mechanisms are used to provide distinct image representations according to the asked questions. Attention allows the model to focus on the most relevant visual information required to generate proper answers to the given questions. Hence, it is possible to generate several distinct representations of the same image, which also has a data augmentation effect.</p>
</div>
<section id="S5.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS1.4.1.1" class="ltx_text">V-D</span>1 </span>Top-Down Attention</h4>

<div id="S5.SS4.SSS1.p1" class="ltx_para">
<p id="S5.SS4.SSS1.p1.8" class="ltx_p">Top-down attention, as the name suggests, uses global features from questions to weight local visual information.
The global textual features <math id="S5.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{q}\in\mathbb{R}^{2048}" display="inline"><semantics id="S5.SS4.SSS1.p1.1.m1.1a"><mrow id="S5.SS4.SSS1.p1.1.m1.1.1" xref="S5.SS4.SSS1.p1.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p1.1.m1.1.1.2" xref="S5.SS4.SSS1.p1.1.m1.1.1.2.cmml">𝐪</mi><mo id="S5.SS4.SSS1.p1.1.m1.1.1.1" xref="S5.SS4.SSS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S5.SS4.SSS1.p1.1.m1.1.1.3" xref="S5.SS4.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S5.SS4.SSS1.p1.1.m1.1.1.3.2" xref="S5.SS4.SSS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S5.SS4.SSS1.p1.1.m1.1.1.3.3" xref="S5.SS4.SSS1.p1.1.m1.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.1.m1.1b"><apply id="S5.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1"><in id="S5.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.1"></in><ci id="S5.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.2">𝐪</ci><apply id="S5.SS4.SSS1.p1.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S5.SS4.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S5.SS4.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.1.m1.1c">\mathbf{q}\in\mathbb{R}^{2048}</annotation></semantics></math> are selected from the last internal state of the RNN, and the image features <math id="S5.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="V\in\mathbb{R}^{k\times 2048}" display="inline"><semantics id="S5.SS4.SSS1.p1.2.m2.1a"><mrow id="S5.SS4.SSS1.p1.2.m2.1.1" xref="S5.SS4.SSS1.p1.2.m2.1.1.cmml"><mi id="S5.SS4.SSS1.p1.2.m2.1.1.2" xref="S5.SS4.SSS1.p1.2.m2.1.1.2.cmml">V</mi><mo id="S5.SS4.SSS1.p1.2.m2.1.1.1" xref="S5.SS4.SSS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S5.SS4.SSS1.p1.2.m2.1.1.3" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S5.SS4.SSS1.p1.2.m2.1.1.3.2" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS4.SSS1.p1.2.m2.1.1.3.3" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3.cmml"><mi id="S5.SS4.SSS1.p1.2.m2.1.1.3.3.2" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p1.2.m2.1.1.3.3.1" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S5.SS4.SSS1.p1.2.m2.1.1.3.3.3" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3.3.cmml">2048</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.2.m2.1b"><apply id="S5.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1"><in id="S5.SS4.SSS1.p1.2.m2.1.1.1.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.1"></in><ci id="S5.SS4.SSS1.p1.2.m2.1.1.2.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.2">𝑉</ci><apply id="S5.SS4.SSS1.p1.2.m2.1.1.3.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S5.SS4.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S5.SS4.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3"><times id="S5.SS4.SSS1.p1.2.m2.1.1.3.3.1.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3.1"></times><ci id="S5.SS4.SSS1.p1.2.m2.1.1.3.3.2.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3.2">𝑘</ci><cn type="integer" id="S5.SS4.SSS1.p1.2.m2.1.1.3.3.3.cmml" xref="S5.SS4.SSS1.p1.2.m2.1.1.3.3.3">2048</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.2.m2.1c">V\in\mathbb{R}^{k\times 2048}</annotation></semantics></math> are extracted from the Faster-RCNN, where <math id="S5.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.SSS1.p1.3.m3.1a"><mi id="S5.SS4.SSS1.p1.3.m3.1.1" xref="S5.SS4.SSS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.3.m3.1b"><ci id="S5.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S5.SS4.SSS1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.3.m3.1c">k</annotation></semantics></math> represents the number of regions extracted from the image. In the present work we used <math id="S5.SS4.SSS1.p1.4.m4.1" class="ltx_Math" alttext="k=36" display="inline"><semantics id="S5.SS4.SSS1.p1.4.m4.1a"><mrow id="S5.SS4.SSS1.p1.4.m4.1.1" xref="S5.SS4.SSS1.p1.4.m4.1.1.cmml"><mi id="S5.SS4.SSS1.p1.4.m4.1.1.2" xref="S5.SS4.SSS1.p1.4.m4.1.1.2.cmml">k</mi><mo id="S5.SS4.SSS1.p1.4.m4.1.1.1" xref="S5.SS4.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS4.SSS1.p1.4.m4.1.1.3" xref="S5.SS4.SSS1.p1.4.m4.1.1.3.cmml">36</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.4.m4.1b"><apply id="S5.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S5.SS4.SSS1.p1.4.m4.1.1"><eq id="S5.SS4.SSS1.p1.4.m4.1.1.1.cmml" xref="S5.SS4.SSS1.p1.4.m4.1.1.1"></eq><ci id="S5.SS4.SSS1.p1.4.m4.1.1.2.cmml" xref="S5.SS4.SSS1.p1.4.m4.1.1.2">𝑘</ci><cn type="integer" id="S5.SS4.SSS1.p1.4.m4.1.1.3.cmml" xref="S5.SS4.SSS1.p1.4.m4.1.1.3">36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.4.m4.1c">k=36</annotation></semantics></math>.
The question features are linearly projected so as to reduce its dimension to <math id="S5.SS4.SSS1.p1.5.m5.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S5.SS4.SSS1.p1.5.m5.1a"><mn id="S5.SS4.SSS1.p1.5.m5.1.1" xref="S5.SS4.SSS1.p1.5.m5.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.5.m5.1b"><cn type="integer" id="S5.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S5.SS4.SSS1.p1.5.m5.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.5.m5.1c">512</annotation></semantics></math>, which is the size used in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Image features are concatenated with the textual features, generating a matrix <math id="S5.SS4.SSS1.p1.6.m6.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S5.SS4.SSS1.p1.6.m6.1a"><mi id="S5.SS4.SSS1.p1.6.m6.1.1" xref="S5.SS4.SSS1.p1.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.6.m6.1b"><ci id="S5.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S5.SS4.SSS1.p1.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.6.m6.1c">C</annotation></semantics></math> of dimensions <math id="S5.SS4.SSS1.p1.7.m7.1" class="ltx_Math" alttext="k\times 2560" display="inline"><semantics id="S5.SS4.SSS1.p1.7.m7.1a"><mrow id="S5.SS4.SSS1.p1.7.m7.1.1" xref="S5.SS4.SSS1.p1.7.m7.1.1.cmml"><mi id="S5.SS4.SSS1.p1.7.m7.1.1.2" xref="S5.SS4.SSS1.p1.7.m7.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p1.7.m7.1.1.1" xref="S5.SS4.SSS1.p1.7.m7.1.1.1.cmml">×</mo><mn id="S5.SS4.SSS1.p1.7.m7.1.1.3" xref="S5.SS4.SSS1.p1.7.m7.1.1.3.cmml">2560</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.7.m7.1b"><apply id="S5.SS4.SSS1.p1.7.m7.1.1.cmml" xref="S5.SS4.SSS1.p1.7.m7.1.1"><times id="S5.SS4.SSS1.p1.7.m7.1.1.1.cmml" xref="S5.SS4.SSS1.p1.7.m7.1.1.1"></times><ci id="S5.SS4.SSS1.p1.7.m7.1.1.2.cmml" xref="S5.SS4.SSS1.p1.7.m7.1.1.2">𝑘</ci><cn type="integer" id="S5.SS4.SSS1.p1.7.m7.1.1.3.cmml" xref="S5.SS4.SSS1.p1.7.m7.1.1.3">2560</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.7.m7.1c">k\times 2560</annotation></semantics></math>. Features resulting from that concatenation are first non-linearly projected with a trainable weight matrix <math id="S5.SS4.SSS1.p1.8.m8.1" class="ltx_Math" alttext="W_{1}^{2560\times 512}" display="inline"><semantics id="S5.SS4.SSS1.p1.8.m8.1a"><msubsup id="S5.SS4.SSS1.p1.8.m8.1.1" xref="S5.SS4.SSS1.p1.8.m8.1.1.cmml"><mi id="S5.SS4.SSS1.p1.8.m8.1.1.2.2" xref="S5.SS4.SSS1.p1.8.m8.1.1.2.2.cmml">W</mi><mn id="S5.SS4.SSS1.p1.8.m8.1.1.2.3" xref="S5.SS4.SSS1.p1.8.m8.1.1.2.3.cmml">1</mn><mrow id="S5.SS4.SSS1.p1.8.m8.1.1.3" xref="S5.SS4.SSS1.p1.8.m8.1.1.3.cmml"><mn id="S5.SS4.SSS1.p1.8.m8.1.1.3.2" xref="S5.SS4.SSS1.p1.8.m8.1.1.3.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p1.8.m8.1.1.3.1" xref="S5.SS4.SSS1.p1.8.m8.1.1.3.1.cmml">×</mo><mn id="S5.SS4.SSS1.p1.8.m8.1.1.3.3" xref="S5.SS4.SSS1.p1.8.m8.1.1.3.3.cmml">512</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.8.m8.1b"><apply id="S5.SS4.SSS1.p1.8.m8.1.1.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p1.8.m8.1.1.1.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1">superscript</csymbol><apply id="S5.SS4.SSS1.p1.8.m8.1.1.2.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p1.8.m8.1.1.2.1.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p1.8.m8.1.1.2.2.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1.2.2">𝑊</ci><cn type="integer" id="S5.SS4.SSS1.p1.8.m8.1.1.2.3.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1.2.3">1</cn></apply><apply id="S5.SS4.SSS1.p1.8.m8.1.1.3.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1.3"><times id="S5.SS4.SSS1.p1.8.m8.1.1.3.1.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1.3.1"></times><cn type="integer" id="S5.SS4.SSS1.p1.8.m8.1.1.3.2.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1.3.2">2560</cn><cn type="integer" id="S5.SS4.SSS1.p1.8.m8.1.1.3.3.cmml" xref="S5.SS4.SSS1.p1.8.m8.1.1.3.3">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.8.m8.1c">W_{1}^{2560\times 512}</annotation></semantics></math> generating a novel multimodal representation for each image region:</p>
</div>
<div id="S5.SS4.SSS1.p2" class="ltx_para">
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.1" class="ltx_Math" alttext="\hat{C}=\phi(CW_{1})" display="block"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml"><mover accent="true" id="S5.E2.m1.1.1.3" xref="S5.E2.m1.1.1.3.cmml"><mi id="S5.E2.m1.1.1.3.2" xref="S5.E2.m1.1.1.3.2.cmml">C</mi><mo id="S5.E2.m1.1.1.3.1" xref="S5.E2.m1.1.1.3.1.cmml">^</mo></mover><mo id="S5.E2.m1.1.1.2" xref="S5.E2.m1.1.1.2.cmml">=</mo><mrow id="S5.E2.m1.1.1.1" xref="S5.E2.m1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.3" xref="S5.E2.m1.1.1.1.3.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.2" xref="S5.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S5.E2.m1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E2.m1.1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E2.m1.1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S5.E2.m1.1.1.1.1.1.1.3" xref="S5.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.E2.m1.1.1.1.1.1.1.3.2" xref="S5.E2.m1.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S5.E2.m1.1.1.1.1.1.1.3.3" xref="S5.E2.m1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S5.E2.m1.1.1.1.1.1.3" xref="S5.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"><eq id="S5.E2.m1.1.1.2.cmml" xref="S5.E2.m1.1.1.2"></eq><apply id="S5.E2.m1.1.1.3.cmml" xref="S5.E2.m1.1.1.3"><ci id="S5.E2.m1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.3.1">^</ci><ci id="S5.E2.m1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.3.2">𝐶</ci></apply><apply id="S5.E2.m1.1.1.1.cmml" xref="S5.E2.m1.1.1.1"><times id="S5.E2.m1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.2"></times><ci id="S5.E2.m1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.3">italic-ϕ</ci><apply id="S5.E2.m1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1"><times id="S5.E2.m1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1.1.1"></times><ci id="S5.E2.m1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.1.1.2">𝐶</ci><apply id="S5.E2.m1.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S5.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">\hat{C}=\phi(CW_{1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S5.SS4.SSS1.p2.10" class="ltx_p">Therefore, such a layer learns image-question relations, generating <math id="S5.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="k\times 512" display="inline"><semantics id="S5.SS4.SSS1.p2.1.m1.1a"><mrow id="S5.SS4.SSS1.p2.1.m1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p2.1.m1.1.1.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p2.1.m1.1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS4.SSS1.p2.1.m1.1.1.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.1.m1.1b"><apply id="S5.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1"><times id="S5.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1"></times><ci id="S5.SS4.SSS1.p2.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S5.SS4.SSS1.p2.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.1.m1.1c">k\times 512</annotation></semantics></math> features that are transformed by an activation function <math id="S5.SS4.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S5.SS4.SSS1.p2.2.m2.1a"><mi id="S5.SS4.SSS1.p2.2.m2.1.1" xref="S5.SS4.SSS1.p2.2.m2.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.2.m2.1b"><ci id="S5.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p2.2.m2.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.2.m2.1c">\phi</annotation></semantics></math>. Often, <math id="S5.SS4.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S5.SS4.SSS1.p2.3.m3.1a"><mi id="S5.SS4.SSS1.p2.3.m3.1.1" xref="S5.SS4.SSS1.p2.3.m3.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.3.m3.1b"><ci id="S5.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.3.m3.1c">\phi</annotation></semantics></math> is ReLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, Tanh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, or Gated Tanh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. The latter employs both the logistic Sigmoid and Tanh, in a gating scheme <math id="S5.SS4.SSS1.p2.4.m4.2" class="ltx_Math" alttext="\sigma(x)\times\textsc{tanh}(x)" display="inline"><semantics id="S5.SS4.SSS1.p2.4.m4.2a"><mrow id="S5.SS4.SSS1.p2.4.m4.2.3" xref="S5.SS4.SSS1.p2.4.m4.2.3.cmml"><mrow id="S5.SS4.SSS1.p2.4.m4.2.3.2" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.cmml"><mrow id="S5.SS4.SSS1.p2.4.m4.2.3.2.2" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.cmml"><mi id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.2" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.1" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.1.cmml">​</mo><mrow id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.3.2" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.cmml"><mo stretchy="false" id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.3.2.1" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.cmml">(</mo><mi id="S5.SS4.SSS1.p2.4.m4.1.1" xref="S5.SS4.SSS1.p2.4.m4.1.1.cmml">x</mi><mo rspace="0.055em" stretchy="false" id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.3.2.2" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S5.SS4.SSS1.p2.4.m4.2.3.2.1" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.1.cmml">×</mo><mtext class="ltx_font_smallcaps" id="S5.SS4.SSS1.p2.4.m4.2.3.2.3" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.3a.cmml">tanh</mtext></mrow><mo lspace="0em" rspace="0em" id="S5.SS4.SSS1.p2.4.m4.2.3.1" xref="S5.SS4.SSS1.p2.4.m4.2.3.1.cmml">​</mo><mrow id="S5.SS4.SSS1.p2.4.m4.2.3.3.2" xref="S5.SS4.SSS1.p2.4.m4.2.3.cmml"><mo stretchy="false" id="S5.SS4.SSS1.p2.4.m4.2.3.3.2.1" xref="S5.SS4.SSS1.p2.4.m4.2.3.cmml">(</mo><mi id="S5.SS4.SSS1.p2.4.m4.2.2" xref="S5.SS4.SSS1.p2.4.m4.2.2.cmml">x</mi><mo stretchy="false" id="S5.SS4.SSS1.p2.4.m4.2.3.3.2.2" xref="S5.SS4.SSS1.p2.4.m4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.4.m4.2b"><apply id="S5.SS4.SSS1.p2.4.m4.2.3.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3"><times id="S5.SS4.SSS1.p2.4.m4.2.3.1.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.1"></times><apply id="S5.SS4.SSS1.p2.4.m4.2.3.2.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.2"><times id="S5.SS4.SSS1.p2.4.m4.2.3.2.1.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.1"></times><apply id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2"><times id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.1.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.1"></times><ci id="S5.SS4.SSS1.p2.4.m4.2.3.2.2.2.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.2.2">𝜎</ci><ci id="S5.SS4.SSS1.p2.4.m4.1.1.cmml" xref="S5.SS4.SSS1.p2.4.m4.1.1">𝑥</ci></apply><ci id="S5.SS4.SSS1.p2.4.m4.2.3.2.3a.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.3"><mtext class="ltx_font_smallcaps" id="S5.SS4.SSS1.p2.4.m4.2.3.2.3.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.3.2.3">tanh</mtext></ci></apply><ci id="S5.SS4.SSS1.p2.4.m4.2.2.cmml" xref="S5.SS4.SSS1.p2.4.m4.2.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.4.m4.2c">\sigma(x)\times\textsc{tanh}(x)</annotation></semantics></math>. A second fully-connected layer is employed to summarize the <math id="S5.SS4.SSS1.p2.5.m5.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S5.SS4.SSS1.p2.5.m5.1a"><mn id="S5.SS4.SSS1.p2.5.m5.1.1" xref="S5.SS4.SSS1.p2.5.m5.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.5.m5.1b"><cn type="integer" id="S5.SS4.SSS1.p2.5.m5.1.1.cmml" xref="S5.SS4.SSS1.p2.5.m5.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.5.m5.1c">512</annotation></semantics></math>-dimensional vectors into <math id="S5.SS4.SSS1.p2.6.m6.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S5.SS4.SSS1.p2.6.m6.1a"><mi id="S5.SS4.SSS1.p2.6.m6.1.1" xref="S5.SS4.SSS1.p2.6.m6.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.6.m6.1b"><ci id="S5.SS4.SSS1.p2.6.m6.1.1.cmml" xref="S5.SS4.SSS1.p2.6.m6.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.6.m6.1c">h</annotation></semantics></math> values per region (<math id="S5.SS4.SSS1.p2.7.m7.1" class="ltx_Math" alttext="k\times h" display="inline"><semantics id="S5.SS4.SSS1.p2.7.m7.1a"><mrow id="S5.SS4.SSS1.p2.7.m7.1.1" xref="S5.SS4.SSS1.p2.7.m7.1.1.cmml"><mi id="S5.SS4.SSS1.p2.7.m7.1.1.2" xref="S5.SS4.SSS1.p2.7.m7.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p2.7.m7.1.1.1" xref="S5.SS4.SSS1.p2.7.m7.1.1.1.cmml">×</mo><mi id="S5.SS4.SSS1.p2.7.m7.1.1.3" xref="S5.SS4.SSS1.p2.7.m7.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.7.m7.1b"><apply id="S5.SS4.SSS1.p2.7.m7.1.1.cmml" xref="S5.SS4.SSS1.p2.7.m7.1.1"><times id="S5.SS4.SSS1.p2.7.m7.1.1.1.cmml" xref="S5.SS4.SSS1.p2.7.m7.1.1.1"></times><ci id="S5.SS4.SSS1.p2.7.m7.1.1.2.cmml" xref="S5.SS4.SSS1.p2.7.m7.1.1.2">𝑘</ci><ci id="S5.SS4.SSS1.p2.7.m7.1.1.3.cmml" xref="S5.SS4.SSS1.p2.7.m7.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.7.m7.1c">k\times h</annotation></semantics></math>). It is usual to use a small value for <math id="S5.SS4.SSS1.p2.8.m8.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S5.SS4.SSS1.p2.8.m8.1a"><mi id="S5.SS4.SSS1.p2.8.m8.1.1" xref="S5.SS4.SSS1.p2.8.m8.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.8.m8.1b"><ci id="S5.SS4.SSS1.p2.8.m8.1.1.cmml" xref="S5.SS4.SSS1.p2.8.m8.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.8.m8.1c">h</annotation></semantics></math> such as <math id="S5.SS4.SSS1.p2.9.m9.2" class="ltx_Math" alttext="\{1,2\}" display="inline"><semantics id="S5.SS4.SSS1.p2.9.m9.2a"><mrow id="S5.SS4.SSS1.p2.9.m9.2.3.2" xref="S5.SS4.SSS1.p2.9.m9.2.3.1.cmml"><mo stretchy="false" id="S5.SS4.SSS1.p2.9.m9.2.3.2.1" xref="S5.SS4.SSS1.p2.9.m9.2.3.1.cmml">{</mo><mn id="S5.SS4.SSS1.p2.9.m9.1.1" xref="S5.SS4.SSS1.p2.9.m9.1.1.cmml">1</mn><mo id="S5.SS4.SSS1.p2.9.m9.2.3.2.2" xref="S5.SS4.SSS1.p2.9.m9.2.3.1.cmml">,</mo><mn id="S5.SS4.SSS1.p2.9.m9.2.2" xref="S5.SS4.SSS1.p2.9.m9.2.2.cmml">2</mn><mo stretchy="false" id="S5.SS4.SSS1.p2.9.m9.2.3.2.3" xref="S5.SS4.SSS1.p2.9.m9.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.9.m9.2b"><set id="S5.SS4.SSS1.p2.9.m9.2.3.1.cmml" xref="S5.SS4.SSS1.p2.9.m9.2.3.2"><cn type="integer" id="S5.SS4.SSS1.p2.9.m9.1.1.cmml" xref="S5.SS4.SSS1.p2.9.m9.1.1">1</cn><cn type="integer" id="S5.SS4.SSS1.p2.9.m9.2.2.cmml" xref="S5.SS4.SSS1.p2.9.m9.2.2">2</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.9.m9.2c">\{1,2\}</annotation></semantics></math>. The role of <math id="S5.SS4.SSS1.p2.10.m10.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S5.SS4.SSS1.p2.10.m10.1a"><mi id="S5.SS4.SSS1.p2.10.m10.1.1" xref="S5.SS4.SSS1.p2.10.m10.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.10.m10.1b"><ci id="S5.SS4.SSS1.p2.10.m10.1.1.cmml" xref="S5.SS4.SSS1.p2.10.m10.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.10.m10.1c">h</annotation></semantics></math> is to allow the model to produce distinct attention maps, which is useful for understanding complex sentences that require distinct viewpoints. Values produced by this layer are normalized with a <span id="S5.SS4.SSS1.p2.10.1" class="ltx_text ltx_font_smallcaps">softmax</span> function applied on the columns of the matrix, as follows.</p>
</div>
<div id="S5.SS4.SSS1.p3" class="ltx_para">
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.1" class="ltx_Math" alttext="A=\textsc{softmax}(\hat{C}W_{2})" display="block"><semantics id="S5.E3.m1.1a"><mrow id="S5.E3.m1.1.1" xref="S5.E3.m1.1.1.cmml"><mi id="S5.E3.m1.1.1.3" xref="S5.E3.m1.1.1.3.cmml">A</mi><mo id="S5.E3.m1.1.1.2" xref="S5.E3.m1.1.1.2.cmml">=</mo><mrow id="S5.E3.m1.1.1.1" xref="S5.E3.m1.1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S5.E3.m1.1.1.1.3" xref="S5.E3.m1.1.1.1.3a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.1.2" xref="S5.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S5.E3.m1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E3.m1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E3.m1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S5.E3.m1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.2.2" xref="S5.E3.m1.1.1.1.1.1.1.2.2.cmml">C</mi><mo id="S5.E3.m1.1.1.1.1.1.1.2.1" xref="S5.E3.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S5.E3.m1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.3.2" xref="S5.E3.m1.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S5.E3.m1.1.1.1.1.1.1.3.3" xref="S5.E3.m1.1.1.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S5.E3.m1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.1b"><apply id="S5.E3.m1.1.1.cmml" xref="S5.E3.m1.1.1"><eq id="S5.E3.m1.1.1.2.cmml" xref="S5.E3.m1.1.1.2"></eq><ci id="S5.E3.m1.1.1.3.cmml" xref="S5.E3.m1.1.1.3">𝐴</ci><apply id="S5.E3.m1.1.1.1.cmml" xref="S5.E3.m1.1.1.1"><times id="S5.E3.m1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.2"></times><ci id="S5.E3.m1.1.1.1.3a.cmml" xref="S5.E3.m1.1.1.1.3"><mtext class="ltx_font_smallcaps" id="S5.E3.m1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.3">softmax</mtext></ci><apply id="S5.E3.m1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1"><times id="S5.E3.m1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1"></times><apply id="S5.E3.m1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2"><ci id="S5.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S5.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2">𝐶</ci></apply><apply id="S5.E3.m1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S5.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.1c">A=\textsc{softmax}(\hat{C}W_{2})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS4.SSS1.p4" class="ltx_para">
<p id="S5.SS4.SSS1.p4.2" class="ltx_p">It generates an attention mask <math id="S5.SS4.SSS1.p4.1.m1.1" class="ltx_Math" alttext="A^{k\times h}" display="inline"><semantics id="S5.SS4.SSS1.p4.1.m1.1a"><msup id="S5.SS4.SSS1.p4.1.m1.1.1" xref="S5.SS4.SSS1.p4.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p4.1.m1.1.1.2" xref="S5.SS4.SSS1.p4.1.m1.1.1.2.cmml">A</mi><mrow id="S5.SS4.SSS1.p4.1.m1.1.1.3" xref="S5.SS4.SSS1.p4.1.m1.1.1.3.cmml"><mi id="S5.SS4.SSS1.p4.1.m1.1.1.3.2" xref="S5.SS4.SSS1.p4.1.m1.1.1.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p4.1.m1.1.1.3.1" xref="S5.SS4.SSS1.p4.1.m1.1.1.3.1.cmml">×</mo><mi id="S5.SS4.SSS1.p4.1.m1.1.1.3.3" xref="S5.SS4.SSS1.p4.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p4.1.m1.1b"><apply id="S5.SS4.SSS1.p4.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p4.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p4.1.m1.1.1">superscript</csymbol><ci id="S5.SS4.SSS1.p4.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p4.1.m1.1.1.2">𝐴</ci><apply id="S5.SS4.SSS1.p4.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p4.1.m1.1.1.3"><times id="S5.SS4.SSS1.p4.1.m1.1.1.3.1.cmml" xref="S5.SS4.SSS1.p4.1.m1.1.1.3.1"></times><ci id="S5.SS4.SSS1.p4.1.m1.1.1.3.2.cmml" xref="S5.SS4.SSS1.p4.1.m1.1.1.3.2">𝑘</ci><ci id="S5.SS4.SSS1.p4.1.m1.1.1.3.3.cmml" xref="S5.SS4.SSS1.p4.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p4.1.m1.1c">A^{k\times h}</annotation></semantics></math> used to weight image regions, producing the image vector <math id="S5.SS4.SSS1.p4.2.m2.1" class="ltx_Math" alttext="\hat{\mathbf{v}}" display="inline"><semantics id="S5.SS4.SSS1.p4.2.m2.1a"><mover accent="true" id="S5.SS4.SSS1.p4.2.m2.1.1" xref="S5.SS4.SSS1.p4.2.m2.1.1.cmml"><mi id="S5.SS4.SSS1.p4.2.m2.1.1.2" xref="S5.SS4.SSS1.p4.2.m2.1.1.2.cmml">𝐯</mi><mo id="S5.SS4.SSS1.p4.2.m2.1.1.1" xref="S5.SS4.SSS1.p4.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p4.2.m2.1b"><apply id="S5.SS4.SSS1.p4.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p4.2.m2.1.1"><ci id="S5.SS4.SSS1.p4.2.m2.1.1.1.cmml" xref="S5.SS4.SSS1.p4.2.m2.1.1.1">^</ci><ci id="S5.SS4.SSS1.p4.2.m2.1.1.2.cmml" xref="S5.SS4.SSS1.p4.2.m2.1.1.2">𝐯</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p4.2.m2.1c">\hat{\mathbf{v}}</annotation></semantics></math>, as shown in Equation <a href="#S5.E4" title="In V-D1 Top-Down Attention ‣ V-D Attention Mechanism ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S5.SS4.SSS1.p5" class="ltx_para">
<table id="S5.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E4.m1.1" class="ltx_math_unparsed" alttext="\hat{\mathbf{v}_{j}}=\sum_{i}^{k}V_{i,.}A_{ij}" display="block"><semantics id="S5.E4.m1.1a"><mrow id="S5.E4.m1.1.2"><mover accent="true" id="S5.E4.m1.1.2.2"><msub id="S5.E4.m1.1.2.2.2"><mi id="S5.E4.m1.1.2.2.2.2">𝐯</mi><mi id="S5.E4.m1.1.2.2.2.3">j</mi></msub><mo id="S5.E4.m1.1.2.2.1">^</mo></mover><mo rspace="0.111em" id="S5.E4.m1.1.2.1">=</mo><mrow id="S5.E4.m1.1.2.3"><munderover id="S5.E4.m1.1.2.3.1"><mo movablelimits="false" id="S5.E4.m1.1.2.3.1.2.2">∑</mo><mi id="S5.E4.m1.1.2.3.1.2.3">i</mi><mi id="S5.E4.m1.1.2.3.1.3">k</mi></munderover><mrow id="S5.E4.m1.1.2.3.2"><msub id="S5.E4.m1.1.2.3.2.2"><mi id="S5.E4.m1.1.2.3.2.2.2">V</mi><mrow id="S5.E4.m1.1.1.1"><mi id="S5.E4.m1.1.1.1.1">i</mi><mo id="S5.E4.m1.1.1.1.2">,</mo><mo lspace="0em" id="S5.E4.m1.1.1.1.3">.</mo></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.3.2.1">​</mo><msub id="S5.E4.m1.1.2.3.2.3"><mi id="S5.E4.m1.1.2.3.2.3.2">A</mi><mrow id="S5.E4.m1.1.2.3.2.3.3"><mi id="S5.E4.m1.1.2.3.2.3.3.2">i</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.3.2.3.3.1">​</mo><mi id="S5.E4.m1.1.2.3.2.3.3.3">j</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S5.E4.m1.1b">\hat{\mathbf{v}_{j}}=\sum_{i}^{k}V_{i,.}A_{ij}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS4.SSS1.p6" class="ltx_para">
<p id="S5.SS4.SSS1.p6.4" class="ltx_p">Note that when <math id="S5.SS4.SSS1.p6.1.m1.1" class="ltx_Math" alttext="h&gt;1" display="inline"><semantics id="S5.SS4.SSS1.p6.1.m1.1a"><mrow id="S5.SS4.SSS1.p6.1.m1.1.1" xref="S5.SS4.SSS1.p6.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p6.1.m1.1.1.2" xref="S5.SS4.SSS1.p6.1.m1.1.1.2.cmml">h</mi><mo id="S5.SS4.SSS1.p6.1.m1.1.1.1" xref="S5.SS4.SSS1.p6.1.m1.1.1.1.cmml">&gt;</mo><mn id="S5.SS4.SSS1.p6.1.m1.1.1.3" xref="S5.SS4.SSS1.p6.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p6.1.m1.1b"><apply id="S5.SS4.SSS1.p6.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p6.1.m1.1.1"><gt id="S5.SS4.SSS1.p6.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p6.1.m1.1.1.1"></gt><ci id="S5.SS4.SSS1.p6.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p6.1.m1.1.1.2">ℎ</ci><cn type="integer" id="S5.SS4.SSS1.p6.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p6.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p6.1.m1.1c">h&gt;1</annotation></semantics></math>, the dimensionality of the visual features increases <math id="S5.SS4.SSS1.p6.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S5.SS4.SSS1.p6.2.m2.1a"><mi id="S5.SS4.SSS1.p6.2.m2.1.1" xref="S5.SS4.SSS1.p6.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p6.2.m2.1b"><ci id="S5.SS4.SSS1.p6.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p6.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p6.2.m2.1c">h</annotation></semantics></math>-fold. Hence, <math id="S5.SS4.SSS1.p6.3.m3.1" class="ltx_Math" alttext="\hat{\mathbf{v}}^{h\times 2048}" display="inline"><semantics id="S5.SS4.SSS1.p6.3.m3.1a"><msup id="S5.SS4.SSS1.p6.3.m3.1.1" xref="S5.SS4.SSS1.p6.3.m3.1.1.cmml"><mover accent="true" id="S5.SS4.SSS1.p6.3.m3.1.1.2" xref="S5.SS4.SSS1.p6.3.m3.1.1.2.cmml"><mi id="S5.SS4.SSS1.p6.3.m3.1.1.2.2" xref="S5.SS4.SSS1.p6.3.m3.1.1.2.2.cmml">𝐯</mi><mo id="S5.SS4.SSS1.p6.3.m3.1.1.2.1" xref="S5.SS4.SSS1.p6.3.m3.1.1.2.1.cmml">^</mo></mover><mrow id="S5.SS4.SSS1.p6.3.m3.1.1.3" xref="S5.SS4.SSS1.p6.3.m3.1.1.3.cmml"><mi id="S5.SS4.SSS1.p6.3.m3.1.1.3.2" xref="S5.SS4.SSS1.p6.3.m3.1.1.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p6.3.m3.1.1.3.1" xref="S5.SS4.SSS1.p6.3.m3.1.1.3.1.cmml">×</mo><mn id="S5.SS4.SSS1.p6.3.m3.1.1.3.3" xref="S5.SS4.SSS1.p6.3.m3.1.1.3.3.cmml">2048</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p6.3.m3.1b"><apply id="S5.SS4.SSS1.p6.3.m3.1.1.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p6.3.m3.1.1.1.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1">superscript</csymbol><apply id="S5.SS4.SSS1.p6.3.m3.1.1.2.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1.2"><ci id="S5.SS4.SSS1.p6.3.m3.1.1.2.1.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1.2.1">^</ci><ci id="S5.SS4.SSS1.p6.3.m3.1.1.2.2.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1.2.2">𝐯</ci></apply><apply id="S5.SS4.SSS1.p6.3.m3.1.1.3.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1.3"><times id="S5.SS4.SSS1.p6.3.m3.1.1.3.1.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1.3.1"></times><ci id="S5.SS4.SSS1.p6.3.m3.1.1.3.2.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1.3.2">ℎ</ci><cn type="integer" id="S5.SS4.SSS1.p6.3.m3.1.1.3.3.cmml" xref="S5.SS4.SSS1.p6.3.m3.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p6.3.m3.1c">\hat{\mathbf{v}}^{h\times 2048}</annotation></semantics></math>, which we reshape to be a <math id="S5.SS4.SSS1.p6.4.m4.1" class="ltx_Math" alttext="(2048\times h)\times 1" display="inline"><semantics id="S5.SS4.SSS1.p6.4.m4.1a"><mrow id="S5.SS4.SSS1.p6.4.m4.1.1" xref="S5.SS4.SSS1.p6.4.m4.1.1.cmml"><mrow id="S5.SS4.SSS1.p6.4.m4.1.1.1.1" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.2" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.cmml"><mn id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.2" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.2.cmml">2048</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.1" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.1.cmml">×</mo><mi id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.3" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.3.cmml">h</mi></mrow><mo rspace="0.055em" stretchy="false" id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.3" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S5.SS4.SSS1.p6.4.m4.1.1.2" xref="S5.SS4.SSS1.p6.4.m4.1.1.2.cmml">×</mo><mn id="S5.SS4.SSS1.p6.4.m4.1.1.3" xref="S5.SS4.SSS1.p6.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p6.4.m4.1b"><apply id="S5.SS4.SSS1.p6.4.m4.1.1.cmml" xref="S5.SS4.SSS1.p6.4.m4.1.1"><times id="S5.SS4.SSS1.p6.4.m4.1.1.2.cmml" xref="S5.SS4.SSS1.p6.4.m4.1.1.2"></times><apply id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.cmml" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1"><times id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.1.cmml" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.1"></times><cn type="integer" id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.2.cmml" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.2">2048</cn><ci id="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.3.cmml" xref="S5.SS4.SSS1.p6.4.m4.1.1.1.1.1.3">ℎ</ci></apply><cn type="integer" id="S5.SS4.SSS1.p6.4.m4.1.1.3.cmml" xref="S5.SS4.SSS1.p6.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p6.4.m4.1c">(2048\times h)\times 1</annotation></semantics></math> vector, constitutes the final question-aware image representation.</p>
</div>
</section>
<section id="S5.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS2.4.1.1" class="ltx_text">V-D</span>2 </span>Co-Attention</h4>

<div id="S5.SS4.SSS2.p1" class="ltx_para">
<p id="S5.SS4.SSS2.p1.8" class="ltx_p">Unlike the Top-Down attention mechanism, Co-Attention is based on the computation of local similarities between all questions words and image regions. It expects two inputs: an image feature matrix <math id="S5.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="V^{k\times 2048}" display="inline"><semantics id="S5.SS4.SSS2.p1.1.m1.1a"><msup id="S5.SS4.SSS2.p1.1.m1.1.1" xref="S5.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S5.SS4.SSS2.p1.1.m1.1.1.2" xref="S5.SS4.SSS2.p1.1.m1.1.1.2.cmml">V</mi><mrow id="S5.SS4.SSS2.p1.1.m1.1.1.3" xref="S5.SS4.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S5.SS4.SSS2.p1.1.m1.1.1.3.2" xref="S5.SS4.SSS2.p1.1.m1.1.1.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS2.p1.1.m1.1.1.3.1" xref="S5.SS4.SSS2.p1.1.m1.1.1.3.1.cmml">×</mo><mn id="S5.SS4.SSS2.p1.1.m1.1.1.3.3" xref="S5.SS4.SSS2.p1.1.m1.1.1.3.3.cmml">2048</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.1.m1.1b"><apply id="S5.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1">superscript</csymbol><ci id="S5.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1.2">𝑉</ci><apply id="S5.SS4.SSS2.p1.1.m1.1.1.3.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1.3"><times id="S5.SS4.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1.3.1"></times><ci id="S5.SS4.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1.3.2">𝑘</ci><cn type="integer" id="S5.SS4.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.1.m1.1c">V^{k\times 2048}</annotation></semantics></math>, such that each image feature vector encodes an image region out of <math id="S5.SS4.SSS2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.SSS2.p1.2.m2.1a"><mi id="S5.SS4.SSS2.p1.2.m2.1.1" xref="S5.SS4.SSS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.2.m2.1b"><ci id="S5.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS4.SSS2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.2.m2.1c">k</annotation></semantics></math>; and a set of word-level features <math id="S5.SS4.SSS2.p1.3.m3.1" class="ltx_Math" alttext="Q^{n\times 2048}" display="inline"><semantics id="S5.SS4.SSS2.p1.3.m3.1a"><msup id="S5.SS4.SSS2.p1.3.m3.1.1" xref="S5.SS4.SSS2.p1.3.m3.1.1.cmml"><mi id="S5.SS4.SSS2.p1.3.m3.1.1.2" xref="S5.SS4.SSS2.p1.3.m3.1.1.2.cmml">Q</mi><mrow id="S5.SS4.SSS2.p1.3.m3.1.1.3" xref="S5.SS4.SSS2.p1.3.m3.1.1.3.cmml"><mi id="S5.SS4.SSS2.p1.3.m3.1.1.3.2" xref="S5.SS4.SSS2.p1.3.m3.1.1.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS2.p1.3.m3.1.1.3.1" xref="S5.SS4.SSS2.p1.3.m3.1.1.3.1.cmml">×</mo><mn id="S5.SS4.SSS2.p1.3.m3.1.1.3.3" xref="S5.SS4.SSS2.p1.3.m3.1.1.3.3.cmml">2048</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.3.m3.1b"><apply id="S5.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S5.SS4.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p1.3.m3.1.1.1.cmml" xref="S5.SS4.SSS2.p1.3.m3.1.1">superscript</csymbol><ci id="S5.SS4.SSS2.p1.3.m3.1.1.2.cmml" xref="S5.SS4.SSS2.p1.3.m3.1.1.2">𝑄</ci><apply id="S5.SS4.SSS2.p1.3.m3.1.1.3.cmml" xref="S5.SS4.SSS2.p1.3.m3.1.1.3"><times id="S5.SS4.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S5.SS4.SSS2.p1.3.m3.1.1.3.1"></times><ci id="S5.SS4.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S5.SS4.SSS2.p1.3.m3.1.1.3.2">𝑛</ci><cn type="integer" id="S5.SS4.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S5.SS4.SSS2.p1.3.m3.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.3.m3.1c">Q^{n\times 2048}</annotation></semantics></math>. Both <math id="S5.SS4.SSS2.p1.4.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S5.SS4.SSS2.p1.4.m4.1a"><mi id="S5.SS4.SSS2.p1.4.m4.1.1" xref="S5.SS4.SSS2.p1.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.4.m4.1b"><ci id="S5.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S5.SS4.SSS2.p1.4.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.4.m4.1c">V</annotation></semantics></math> and <math id="S5.SS4.SSS2.p1.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.SS4.SSS2.p1.5.m5.1a"><mi id="S5.SS4.SSS2.p1.5.m5.1.1" xref="S5.SS4.SSS2.p1.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.5.m5.1b"><ci id="S5.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S5.SS4.SSS2.p1.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.5.m5.1c">Q</annotation></semantics></math> are normalized to have unit <math id="S5.SS4.SSS2.p1.6.m6.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S5.SS4.SSS2.p1.6.m6.1a"><msub id="S5.SS4.SSS2.p1.6.m6.1.1" xref="S5.SS4.SSS2.p1.6.m6.1.1.cmml"><mi id="S5.SS4.SSS2.p1.6.m6.1.1.2" xref="S5.SS4.SSS2.p1.6.m6.1.1.2.cmml">L</mi><mn id="S5.SS4.SSS2.p1.6.m6.1.1.3" xref="S5.SS4.SSS2.p1.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.6.m6.1b"><apply id="S5.SS4.SSS2.p1.6.m6.1.1.cmml" xref="S5.SS4.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p1.6.m6.1.1.1.cmml" xref="S5.SS4.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p1.6.m6.1.1.2.cmml" xref="S5.SS4.SSS2.p1.6.m6.1.1.2">𝐿</ci><cn type="integer" id="S5.SS4.SSS2.p1.6.m6.1.1.3.cmml" xref="S5.SS4.SSS2.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.6.m6.1c">L_{2}</annotation></semantics></math> norm, so their multiplication <math id="S5.SS4.SSS2.p1.7.m7.1" class="ltx_Math" alttext="VQ^{T}" display="inline"><semantics id="S5.SS4.SSS2.p1.7.m7.1a"><mrow id="S5.SS4.SSS2.p1.7.m7.1.1" xref="S5.SS4.SSS2.p1.7.m7.1.1.cmml"><mi id="S5.SS4.SSS2.p1.7.m7.1.1.2" xref="S5.SS4.SSS2.p1.7.m7.1.1.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S5.SS4.SSS2.p1.7.m7.1.1.1" xref="S5.SS4.SSS2.p1.7.m7.1.1.1.cmml">​</mo><msup id="S5.SS4.SSS2.p1.7.m7.1.1.3" xref="S5.SS4.SSS2.p1.7.m7.1.1.3.cmml"><mi id="S5.SS4.SSS2.p1.7.m7.1.1.3.2" xref="S5.SS4.SSS2.p1.7.m7.1.1.3.2.cmml">Q</mi><mi id="S5.SS4.SSS2.p1.7.m7.1.1.3.3" xref="S5.SS4.SSS2.p1.7.m7.1.1.3.3.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.7.m7.1b"><apply id="S5.SS4.SSS2.p1.7.m7.1.1.cmml" xref="S5.SS4.SSS2.p1.7.m7.1.1"><times id="S5.SS4.SSS2.p1.7.m7.1.1.1.cmml" xref="S5.SS4.SSS2.p1.7.m7.1.1.1"></times><ci id="S5.SS4.SSS2.p1.7.m7.1.1.2.cmml" xref="S5.SS4.SSS2.p1.7.m7.1.1.2">𝑉</ci><apply id="S5.SS4.SSS2.p1.7.m7.1.1.3.cmml" xref="S5.SS4.SSS2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p1.7.m7.1.1.3.1.cmml" xref="S5.SS4.SSS2.p1.7.m7.1.1.3">superscript</csymbol><ci id="S5.SS4.SSS2.p1.7.m7.1.1.3.2.cmml" xref="S5.SS4.SSS2.p1.7.m7.1.1.3.2">𝑄</ci><ci id="S5.SS4.SSS2.p1.7.m7.1.1.3.3.cmml" xref="S5.SS4.SSS2.p1.7.m7.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.7.m7.1c">VQ^{T}</annotation></semantics></math> results in the cosine similarity matrix used as guidance for generating the filtered image features. A context feature matrix <math id="S5.SS4.SSS2.p1.8.m8.1" class="ltx_Math" alttext="C^{k\times 2048}" display="inline"><semantics id="S5.SS4.SSS2.p1.8.m8.1a"><msup id="S5.SS4.SSS2.p1.8.m8.1.1" xref="S5.SS4.SSS2.p1.8.m8.1.1.cmml"><mi id="S5.SS4.SSS2.p1.8.m8.1.1.2" xref="S5.SS4.SSS2.p1.8.m8.1.1.2.cmml">C</mi><mrow id="S5.SS4.SSS2.p1.8.m8.1.1.3" xref="S5.SS4.SSS2.p1.8.m8.1.1.3.cmml"><mi id="S5.SS4.SSS2.p1.8.m8.1.1.3.2" xref="S5.SS4.SSS2.p1.8.m8.1.1.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS2.p1.8.m8.1.1.3.1" xref="S5.SS4.SSS2.p1.8.m8.1.1.3.1.cmml">×</mo><mn id="S5.SS4.SSS2.p1.8.m8.1.1.3.3" xref="S5.SS4.SSS2.p1.8.m8.1.1.3.3.cmml">2048</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.8.m8.1b"><apply id="S5.SS4.SSS2.p1.8.m8.1.1.cmml" xref="S5.SS4.SSS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p1.8.m8.1.1.1.cmml" xref="S5.SS4.SSS2.p1.8.m8.1.1">superscript</csymbol><ci id="S5.SS4.SSS2.p1.8.m8.1.1.2.cmml" xref="S5.SS4.SSS2.p1.8.m8.1.1.2">𝐶</ci><apply id="S5.SS4.SSS2.p1.8.m8.1.1.3.cmml" xref="S5.SS4.SSS2.p1.8.m8.1.1.3"><times id="S5.SS4.SSS2.p1.8.m8.1.1.3.1.cmml" xref="S5.SS4.SSS2.p1.8.m8.1.1.3.1"></times><ci id="S5.SS4.SSS2.p1.8.m8.1.1.3.2.cmml" xref="S5.SS4.SSS2.p1.8.m8.1.1.3.2">𝑘</ci><cn type="integer" id="S5.SS4.SSS2.p1.8.m8.1.1.3.3.cmml" xref="S5.SS4.SSS2.p1.8.m8.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.8.m8.1c">C^{k\times 2048}</annotation></semantics></math> is given by:</p>
</div>
<div id="S5.SS4.SSS2.p2" class="ltx_para">
<table id="S5.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E5.m1.1" class="ltx_Math" alttext="C^{T}=Q^{T}(QV^{T})" display="block"><semantics id="S5.E5.m1.1a"><mrow id="S5.E5.m1.1.1" xref="S5.E5.m1.1.1.cmml"><msup id="S5.E5.m1.1.1.3" xref="S5.E5.m1.1.1.3.cmml"><mi id="S5.E5.m1.1.1.3.2" xref="S5.E5.m1.1.1.3.2.cmml">C</mi><mi id="S5.E5.m1.1.1.3.3" xref="S5.E5.m1.1.1.3.3.cmml">T</mi></msup><mo id="S5.E5.m1.1.1.2" xref="S5.E5.m1.1.1.2.cmml">=</mo><mrow id="S5.E5.m1.1.1.1" xref="S5.E5.m1.1.1.1.cmml"><msup id="S5.E5.m1.1.1.1.3" xref="S5.E5.m1.1.1.1.3.cmml"><mi id="S5.E5.m1.1.1.1.3.2" xref="S5.E5.m1.1.1.1.3.2.cmml">Q</mi><mi id="S5.E5.m1.1.1.1.3.3" xref="S5.E5.m1.1.1.1.3.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.1.1.2" xref="S5.E5.m1.1.1.1.2.cmml">​</mo><mrow id="S5.E5.m1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E5.m1.1.1.1.1.1.2" xref="S5.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E5.m1.1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.1.cmml"><mi id="S5.E5.m1.1.1.1.1.1.1.2" xref="S5.E5.m1.1.1.1.1.1.1.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.1.1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.1.1.cmml">​</mo><msup id="S5.E5.m1.1.1.1.1.1.1.3" xref="S5.E5.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.E5.m1.1.1.1.1.1.1.3.2" xref="S5.E5.m1.1.1.1.1.1.1.3.2.cmml">V</mi><mi id="S5.E5.m1.1.1.1.1.1.1.3.3" xref="S5.E5.m1.1.1.1.1.1.1.3.3.cmml">T</mi></msup></mrow><mo stretchy="false" id="S5.E5.m1.1.1.1.1.1.3" xref="S5.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E5.m1.1b"><apply id="S5.E5.m1.1.1.cmml" xref="S5.E5.m1.1.1"><eq id="S5.E5.m1.1.1.2.cmml" xref="S5.E5.m1.1.1.2"></eq><apply id="S5.E5.m1.1.1.3.cmml" xref="S5.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S5.E5.m1.1.1.3.1.cmml" xref="S5.E5.m1.1.1.3">superscript</csymbol><ci id="S5.E5.m1.1.1.3.2.cmml" xref="S5.E5.m1.1.1.3.2">𝐶</ci><ci id="S5.E5.m1.1.1.3.3.cmml" xref="S5.E5.m1.1.1.3.3">𝑇</ci></apply><apply id="S5.E5.m1.1.1.1.cmml" xref="S5.E5.m1.1.1.1"><times id="S5.E5.m1.1.1.1.2.cmml" xref="S5.E5.m1.1.1.1.2"></times><apply id="S5.E5.m1.1.1.1.3.cmml" xref="S5.E5.m1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E5.m1.1.1.1.3.1.cmml" xref="S5.E5.m1.1.1.1.3">superscript</csymbol><ci id="S5.E5.m1.1.1.1.3.2.cmml" xref="S5.E5.m1.1.1.1.3.2">𝑄</ci><ci id="S5.E5.m1.1.1.1.3.3.cmml" xref="S5.E5.m1.1.1.1.3.3">𝑇</ci></apply><apply id="S5.E5.m1.1.1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1.1.1"><times id="S5.E5.m1.1.1.1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1.1.1.1.1"></times><ci id="S5.E5.m1.1.1.1.1.1.1.2.cmml" xref="S5.E5.m1.1.1.1.1.1.1.2">𝑄</ci><apply id="S5.E5.m1.1.1.1.1.1.1.3.cmml" xref="S5.E5.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E5.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.E5.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S5.E5.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.E5.m1.1.1.1.1.1.1.3.2">𝑉</ci><ci id="S5.E5.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.E5.m1.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E5.m1.1c">C^{T}=Q^{T}(QV^{T})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS4.SSS2.p3" class="ltx_para">
<p id="S5.SS4.SSS2.p3.7" class="ltx_p">Finally, <math id="S5.SS4.SSS2.p3.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S5.SS4.SSS2.p3.1.m1.1a"><mi id="S5.SS4.SSS2.p3.1.m1.1.1" xref="S5.SS4.SSS2.p3.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.1.m1.1b"><ci id="S5.SS4.SSS2.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p3.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.1.m1.1c">C</annotation></semantics></math> is normalized with a <span id="S5.SS4.SSS2.p3.7.1" class="ltx_text ltx_markedasmath ltx_font_smallcaps">softmax</span> function, and the <math id="S5.SS4.SSS2.p3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.SSS2.p3.3.m3.1a"><mi id="S5.SS4.SSS2.p3.3.m3.1.1" xref="S5.SS4.SSS2.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.3.m3.1b"><ci id="S5.SS4.SSS2.p3.3.m3.1.1.cmml" xref="S5.SS4.SSS2.p3.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.3.m3.1c">k</annotation></semantics></math> regions are summed so as to generate a <math id="S5.SS4.SSS2.p3.4.m4.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S5.SS4.SSS2.p3.4.m4.1a"><mn id="S5.SS4.SSS2.p3.4.m4.1.1" xref="S5.SS4.SSS2.p3.4.m4.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.4.m4.1b"><cn type="integer" id="S5.SS4.SSS2.p3.4.m4.1.1.cmml" xref="S5.SS4.SSS2.p3.4.m4.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.4.m4.1c">1024</annotation></semantics></math>-sized vector <math id="S5.SS4.SSS2.p3.5.m5.1" class="ltx_Math" alttext="\hat{\mathbf{v}}" display="inline"><semantics id="S5.SS4.SSS2.p3.5.m5.1a"><mover accent="true" id="S5.SS4.SSS2.p3.5.m5.1.1" xref="S5.SS4.SSS2.p3.5.m5.1.1.cmml"><mi id="S5.SS4.SSS2.p3.5.m5.1.1.2" xref="S5.SS4.SSS2.p3.5.m5.1.1.2.cmml">𝐯</mi><mo id="S5.SS4.SSS2.p3.5.m5.1.1.1" xref="S5.SS4.SSS2.p3.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.5.m5.1b"><apply id="S5.SS4.SSS2.p3.5.m5.1.1.cmml" xref="S5.SS4.SSS2.p3.5.m5.1.1"><ci id="S5.SS4.SSS2.p3.5.m5.1.1.1.cmml" xref="S5.SS4.SSS2.p3.5.m5.1.1.1">^</ci><ci id="S5.SS4.SSS2.p3.5.m5.1.1.2.cmml" xref="S5.SS4.SSS2.p3.5.m5.1.1.2">𝐯</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.5.m5.1c">\hat{\mathbf{v}}</annotation></semantics></math> to represent relevant visual features <math id="S5.SS4.SSS2.p3.6.m6.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S5.SS4.SSS2.p3.6.m6.1a"><mi id="S5.SS4.SSS2.p3.6.m6.1.1" xref="S5.SS4.SSS2.p3.6.m6.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.6.m6.1b"><ci id="S5.SS4.SSS2.p3.6.m6.1.1.cmml" xref="S5.SS4.SSS2.p3.6.m6.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.6.m6.1c">V</annotation></semantics></math> based on question <math id="S5.SS4.SSS2.p3.7.m7.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.SS4.SSS2.p3.7.m7.1a"><mi id="S5.SS4.SSS2.p3.7.m7.1.1" xref="S5.SS4.SSS2.p3.7.m7.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.7.m7.1b"><ci id="S5.SS4.SSS2.p3.7.m7.1.1.cmml" xref="S5.SS4.SSS2.p3.7.m7.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.7.m7.1c">Q</annotation></semantics></math>:</p>
</div>
<div id="S5.SS4.SSS2.p4" class="ltx_para">
<table id="S5.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E6.m1.1" class="ltx_Math" alttext="\hat{\mathbf{v}}=\sum_{i}^{k}\textsc{softmax}(C)_{i}" display="block"><semantics id="S5.E6.m1.1a"><mrow id="S5.E6.m1.1.2" xref="S5.E6.m1.1.2.cmml"><mover accent="true" id="S5.E6.m1.1.2.2" xref="S5.E6.m1.1.2.2.cmml"><mi id="S5.E6.m1.1.2.2.2" xref="S5.E6.m1.1.2.2.2.cmml">𝐯</mi><mo id="S5.E6.m1.1.2.2.1" xref="S5.E6.m1.1.2.2.1.cmml">^</mo></mover><mo rspace="0.111em" id="S5.E6.m1.1.2.1" xref="S5.E6.m1.1.2.1.cmml">=</mo><mrow id="S5.E6.m1.1.2.3" xref="S5.E6.m1.1.2.3.cmml"><munderover id="S5.E6.m1.1.2.3.1" xref="S5.E6.m1.1.2.3.1.cmml"><mo movablelimits="false" id="S5.E6.m1.1.2.3.1.2.2" xref="S5.E6.m1.1.2.3.1.2.2.cmml">∑</mo><mi id="S5.E6.m1.1.2.3.1.2.3" xref="S5.E6.m1.1.2.3.1.2.3.cmml">i</mi><mi id="S5.E6.m1.1.2.3.1.3" xref="S5.E6.m1.1.2.3.1.3.cmml">k</mi></munderover><mrow id="S5.E6.m1.1.2.3.2" xref="S5.E6.m1.1.2.3.2.cmml"><mtext class="ltx_font_smallcaps" id="S5.E6.m1.1.2.3.2.2" xref="S5.E6.m1.1.2.3.2.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S5.E6.m1.1.2.3.2.1" xref="S5.E6.m1.1.2.3.2.1.cmml">​</mo><msub id="S5.E6.m1.1.2.3.2.3" xref="S5.E6.m1.1.2.3.2.3.cmml"><mrow id="S5.E6.m1.1.2.3.2.3.2.2" xref="S5.E6.m1.1.2.3.2.3.cmml"><mo stretchy="false" id="S5.E6.m1.1.2.3.2.3.2.2.1" xref="S5.E6.m1.1.2.3.2.3.cmml">(</mo><mi id="S5.E6.m1.1.1" xref="S5.E6.m1.1.1.cmml">C</mi><mo stretchy="false" id="S5.E6.m1.1.2.3.2.3.2.2.2" xref="S5.E6.m1.1.2.3.2.3.cmml">)</mo></mrow><mi id="S5.E6.m1.1.2.3.2.3.3" xref="S5.E6.m1.1.2.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E6.m1.1b"><apply id="S5.E6.m1.1.2.cmml" xref="S5.E6.m1.1.2"><eq id="S5.E6.m1.1.2.1.cmml" xref="S5.E6.m1.1.2.1"></eq><apply id="S5.E6.m1.1.2.2.cmml" xref="S5.E6.m1.1.2.2"><ci id="S5.E6.m1.1.2.2.1.cmml" xref="S5.E6.m1.1.2.2.1">^</ci><ci id="S5.E6.m1.1.2.2.2.cmml" xref="S5.E6.m1.1.2.2.2">𝐯</ci></apply><apply id="S5.E6.m1.1.2.3.cmml" xref="S5.E6.m1.1.2.3"><apply id="S5.E6.m1.1.2.3.1.cmml" xref="S5.E6.m1.1.2.3.1"><csymbol cd="ambiguous" id="S5.E6.m1.1.2.3.1.1.cmml" xref="S5.E6.m1.1.2.3.1">superscript</csymbol><apply id="S5.E6.m1.1.2.3.1.2.cmml" xref="S5.E6.m1.1.2.3.1"><csymbol cd="ambiguous" id="S5.E6.m1.1.2.3.1.2.1.cmml" xref="S5.E6.m1.1.2.3.1">subscript</csymbol><sum id="S5.E6.m1.1.2.3.1.2.2.cmml" xref="S5.E6.m1.1.2.3.1.2.2"></sum><ci id="S5.E6.m1.1.2.3.1.2.3.cmml" xref="S5.E6.m1.1.2.3.1.2.3">𝑖</ci></apply><ci id="S5.E6.m1.1.2.3.1.3.cmml" xref="S5.E6.m1.1.2.3.1.3">𝑘</ci></apply><apply id="S5.E6.m1.1.2.3.2.cmml" xref="S5.E6.m1.1.2.3.2"><times id="S5.E6.m1.1.2.3.2.1.cmml" xref="S5.E6.m1.1.2.3.2.1"></times><ci id="S5.E6.m1.1.2.3.2.2a.cmml" xref="S5.E6.m1.1.2.3.2.2"><mtext class="ltx_font_smallcaps" id="S5.E6.m1.1.2.3.2.2.cmml" xref="S5.E6.m1.1.2.3.2.2">softmax</mtext></ci><apply id="S5.E6.m1.1.2.3.2.3.cmml" xref="S5.E6.m1.1.2.3.2.3"><csymbol cd="ambiguous" id="S5.E6.m1.1.2.3.2.3.1.cmml" xref="S5.E6.m1.1.2.3.2.3">subscript</csymbol><ci id="S5.E6.m1.1.1.cmml" xref="S5.E6.m1.1.1">𝐶</ci><ci id="S5.E6.m1.1.2.3.2.3.3.cmml" xref="S5.E6.m1.1.2.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E6.m1.1c">\hat{\mathbf{v}}=\sum_{i}^{k}\textsc{softmax}(C)_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS4.SSS2.p5" class="ltx_para">
<p id="S5.SS4.SSS2.p5.1" class="ltx_p">Table <a href="#S5.T3" title="TABLE III ‣ V-C Fusion strategy ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> depicts the results obtained by adding the attention mechanisms to the baseline model. For these experiments we used only element-wise multiplication as fusion strategy, given that it presented the best performance in our previous experiments. We observe that attention is a crucial mechanism for VQA, leading to an <math id="S5.SS4.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\approx 6\%" display="inline"><semantics id="S5.SS4.SSS2.p5.1.m1.1a"><mrow id="S5.SS4.SSS2.p5.1.m1.1.1" xref="S5.SS4.SSS2.p5.1.m1.1.1.cmml"><mi id="S5.SS4.SSS2.p5.1.m1.1.1.2" xref="S5.SS4.SSS2.p5.1.m1.1.1.2.cmml"></mi><mo id="S5.SS4.SSS2.p5.1.m1.1.1.1" xref="S5.SS4.SSS2.p5.1.m1.1.1.1.cmml">≈</mo><mrow id="S5.SS4.SSS2.p5.1.m1.1.1.3" xref="S5.SS4.SSS2.p5.1.m1.1.1.3.cmml"><mn id="S5.SS4.SSS2.p5.1.m1.1.1.3.2" xref="S5.SS4.SSS2.p5.1.m1.1.1.3.2.cmml">6</mn><mo id="S5.SS4.SSS2.p5.1.m1.1.1.3.1" xref="S5.SS4.SSS2.p5.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p5.1.m1.1b"><apply id="S5.SS4.SSS2.p5.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p5.1.m1.1.1"><approx id="S5.SS4.SSS2.p5.1.m1.1.1.1.cmml" xref="S5.SS4.SSS2.p5.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S5.SS4.SSS2.p5.1.m1.1.1.2.cmml" xref="S5.SS4.SSS2.p5.1.m1.1.1.2">absent</csymbol><apply id="S5.SS4.SSS2.p5.1.m1.1.1.3.cmml" xref="S5.SS4.SSS2.p5.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS4.SSS2.p5.1.m1.1.1.3.1.cmml" xref="S5.SS4.SSS2.p5.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS4.SSS2.p5.1.m1.1.1.3.2.cmml" xref="S5.SS4.SSS2.p5.1.m1.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p5.1.m1.1c">\approx 6\%</annotation></semantics></math> accuracy improvement.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison of the models on VQA2 Test-Standard set. The models were trained on the union of VQA 2.0 trainval split and VisualGenome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> train split. <span id="S5.T4.5.1" class="ltx_text ltx_font_italic">All</span> is the overall OpenEnded accuracy (higher is better). <span id="S5.T4.6.2" class="ltx_text ltx_font_italic">Yes/No</span>, <span id="S5.T4.7.3" class="ltx_text ltx_font_italic">Numbers</span>, and <span id="S5.T4.8.4" class="ltx_text ltx_font_italic">Others</span> are subsets that correspond to answers types. * scores reported from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</figcaption>
<table id="S5.T4.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.9.1.1" class="ltx_tr">
<td id="S5.T4.9.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T4.9.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S5.T4.9.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">VQA2.0 Test-Dev</td>
<td id="S5.T4.9.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">VQA2.0 Test-Std</td>
</tr>
<tr id="S5.T4.9.2.2" class="ltx_tr">
<td id="S5.T4.9.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">All</td>
<td id="S5.T4.9.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes/No</td>
<td id="S5.T4.9.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Num.</td>
<td id="S5.T4.9.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Other</td>
<td id="S5.T4.9.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">All</td>
<td id="S5.T4.9.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes/No</td>
<td id="S5.T4.9.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Num.</td>
<td id="S5.T4.9.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Other</td>
</tr>
<tr id="S5.T4.9.3.3" class="ltx_tr">
<td id="S5.T4.9.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MCB* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S5.T4.9.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.9.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.9.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.9.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.9.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.27</td>
<td id="S5.T4.9.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.82</td>
<td id="S5.T4.9.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.28</td>
<td id="S5.T4.9.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.36</td>
</tr>
<tr id="S5.T4.9.4.4" class="ltx_tr">
<td id="S5.T4.9.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ReasonNet* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S5.T4.9.4.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.9.4.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.9.4.4.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.9.4.4.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.9.4.4.6" class="ltx_td ltx_align_center ltx_border_r">64.61</td>
<td id="S5.T4.9.4.4.7" class="ltx_td ltx_align_center ltx_border_r">78.86</td>
<td id="S5.T4.9.4.4.8" class="ltx_td ltx_align_center ltx_border_r">41.98</td>
<td id="S5.T4.9.4.4.9" class="ltx_td ltx_align_center ltx_border_r">57.39</td>
</tr>
<tr id="S5.T4.9.5.5" class="ltx_tr">
<td id="S5.T4.9.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Tips&amp;Tricks* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S5.T4.9.5.5.2" class="ltx_td ltx_align_center ltx_border_r">65.32</td>
<td id="S5.T4.9.5.5.3" class="ltx_td ltx_align_center ltx_border_r">81.82</td>
<td id="S5.T4.9.5.5.4" class="ltx_td ltx_align_center ltx_border_r">44.21</td>
<td id="S5.T4.9.5.5.5" class="ltx_td ltx_align_center ltx_border_r">56.05</td>
<td id="S5.T4.9.5.5.6" class="ltx_td ltx_align_center ltx_border_r">65.67</td>
<td id="S5.T4.9.5.5.7" class="ltx_td ltx_align_center ltx_border_r">82.20</td>
<td id="S5.T4.9.5.5.8" class="ltx_td ltx_align_center ltx_border_r">43.90</td>
<td id="S5.T4.9.5.5.9" class="ltx_td ltx_align_center ltx_border_r">56.26</td>
</tr>
<tr id="S5.T4.9.6.6" class="ltx_tr">
<td id="S5.T4.9.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">
<span id="S5.T4.9.6.6.1.1" class="ltx_text ltx_font_smallcaps">block</span>* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S5.T4.9.6.6.2" class="ltx_td ltx_align_center ltx_border_r">67.58</td>
<td id="S5.T4.9.6.6.3" class="ltx_td ltx_align_center ltx_border_r">83.60</td>
<td id="S5.T4.9.6.6.4" class="ltx_td ltx_align_center ltx_border_r">47.33</td>
<td id="S5.T4.9.6.6.5" class="ltx_td ltx_align_center ltx_border_r">58.51</td>
<td id="S5.T4.9.6.6.6" class="ltx_td ltx_align_center ltx_border_r">67.92</td>
<td id="S5.T4.9.6.6.7" class="ltx_td ltx_align_center ltx_border_r">83.98</td>
<td id="S5.T4.9.6.6.8" class="ltx_td ltx_align_center ltx_border_r">46.77</td>
<td id="S5.T4.9.6.6.9" class="ltx_td ltx_align_center ltx_border_r">58.79</td>
</tr>
<tr id="S5.T4.9.7.7" class="ltx_tr">
<td id="S5.T4.9.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">BERT-GRU-Faster-TopDown</td>
<td id="S5.T4.9.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.16</td>
<td id="S5.T4.9.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.76</td>
<td id="S5.T4.9.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.82</td>
<td id="S5.T4.9.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">57.23</td>
<td id="S5.T4.9.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.28</td>
<td id="S5.T4.9.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.75</td>
<td id="S5.T4.9.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.90</td>
<td id="S5.T4.9.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">57.20</td>
</tr>
<tr id="S5.T4.9.8.8" class="ltx_tr">
<td id="S5.T4.9.8.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">BERT-GRU-Faster-CoAttention</td>
<td id="S5.T4.9.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">67.18</td>
<td id="S5.T4.9.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">84.85</td>
<td id="S5.T4.9.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">45.92</td>
<td id="S5.T4.9.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">56.84</td>
<td id="S5.T4.9.8.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">67.39</td>
<td id="S5.T4.9.8.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">85.00</td>
<td id="S5.T4.9.8.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">46.20</td>
<td id="S5.T4.9.8.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">56.91</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Comparison of the models on VQA2 test-dev set. <span id="S5.T5.5.1" class="ltx_text ltx_font_italic">All</span> is the overall OpenEnded accuracy (higher is better). <span id="S5.T5.6.2" class="ltx_text ltx_font_italic">Yes/No</span>, <span id="S5.T5.7.3" class="ltx_text ltx_font_italic">Numbers</span>, and <span id="S5.T5.8.4" class="ltx_text ltx_font_italic">Others</span> are subsets that correspond to answers types. * scores reported from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</figcaption>
<table id="S5.T5.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.9.1.1" class="ltx_tr">
<th id="S5.T5.9.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T5.9.1.1.1.1" class="ltx_text">Model</span></th>
<td id="S5.T5.9.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">VQA2.0 Test-Dev</td>
</tr>
<tr id="S5.T5.9.2.2" class="ltx_tr">
<td id="S5.T5.9.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">All</td>
<td id="S5.T5.9.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes/No</td>
<td id="S5.T5.9.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Num.</td>
<td id="S5.T5.9.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Other</td>
</tr>
<tr id="S5.T5.9.3.3" class="ltx_tr">
<th id="S5.T5.9.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T5.9.3.3.1.1" class="ltx_text ltx_font_smallcaps">Deeper-lstm-q</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="S5.T5.9.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51.95</td>
<td id="S5.T5.9.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.42</td>
<td id="S5.T5.9.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.28</td>
<td id="S5.T5.9.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.64</td>
</tr>
<tr id="S5.T5.9.4.4" class="ltx_tr">
<th id="S5.T5.9.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">MCB* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<td id="S5.T5.9.4.4.2" class="ltx_td ltx_align_center ltx_border_r">61.23</td>
<td id="S5.T5.9.4.4.3" class="ltx_td ltx_align_center ltx_border_r">79.73</td>
<td id="S5.T5.9.4.4.4" class="ltx_td ltx_align_center ltx_border_r">39.13</td>
<td id="S5.T5.9.4.4.5" class="ltx_td ltx_align_center ltx_border_r">50.45</td>
</tr>
<tr id="S5.T5.9.5.5" class="ltx_tr">
<th id="S5.T5.9.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">
<span id="S5.T5.9.5.5.1.1" class="ltx_text ltx_font_smallcaps">block</span>* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<td id="S5.T5.9.5.5.2" class="ltx_td ltx_align_center ltx_border_r">66.41</td>
<td id="S5.T5.9.5.5.3" class="ltx_td ltx_align_center ltx_border_r">82.86</td>
<td id="S5.T5.9.5.5.4" class="ltx_td ltx_align_center ltx_border_r">44.76</td>
<td id="S5.T5.9.5.5.5" class="ltx_td ltx_align_center ltx_border_r">57.30</td>
</tr>
<tr id="S5.T5.9.6.6" class="ltx_tr">
<th id="S5.T5.9.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">BERT-GRU-Faster-CoAttention</th>
<td id="S5.T5.9.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.84</td>
<td id="S5.T5.9.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.66</td>
<td id="S5.T5.9.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.36</td>
<td id="S5.T5.9.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.50</td>
</tr>
<tr id="S5.T5.9.7.7" class="ltx_tr">
<th id="S5.T5.9.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">BERT-GRU-Faster-TopDown</th>
<td id="S5.T5.9.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">66.02</td>
<td id="S5.T5.9.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">83.72</td>
<td id="S5.T5.9.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">44.88</td>
<td id="S5.T5.9.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">55.77</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS4.SSS2.p6" class="ltx_para">
<p id="S5.SS4.SSS2.p6.2" class="ltx_p">The best performing attention approach was Top-Down attention with ReLU activation, followed closely by Co-Attention. We noticed that when using Gated Tanh within Top-Down attention, results degraded 2%. In addition, experiments show that <math id="S5.SS4.SSS2.p6.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S5.SS4.SSS2.p6.1.m1.1a"><msub id="S5.SS4.SSS2.p6.1.m1.1.1" xref="S5.SS4.SSS2.p6.1.m1.1.1.cmml"><mi id="S5.SS4.SSS2.p6.1.m1.1.1.2" xref="S5.SS4.SSS2.p6.1.m1.1.1.2.cmml">L</mi><mn id="S5.SS4.SSS2.p6.1.m1.1.1.3" xref="S5.SS4.SSS2.p6.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p6.1.m1.1b"><apply id="S5.SS4.SSS2.p6.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p6.1.m1.1.1.1.cmml" xref="S5.SS4.SSS2.p6.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p6.1.m1.1.1.2.cmml" xref="S5.SS4.SSS2.p6.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S5.SS4.SSS2.p6.1.m1.1.1.3.cmml" xref="S5.SS4.SSS2.p6.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p6.1.m1.1c">L_{2}</annotation></semantics></math> normalization is quite important in Co-Attention, providing an improvement of almost <math id="S5.SS4.SSS2.p6.2.m2.1" class="ltx_Math" alttext="6\%" display="inline"><semantics id="S5.SS4.SSS2.p6.2.m2.1a"><mrow id="S5.SS4.SSS2.p6.2.m2.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.cmml"><mn id="S5.SS4.SSS2.p6.2.m2.1.1.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.2.cmml">6</mn><mo id="S5.SS4.SSS2.p6.2.m2.1.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p6.2.m2.1b"><apply id="S5.SS4.SSS2.p6.2.m2.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1"><csymbol cd="latexml" id="S5.SS4.SSS2.p6.2.m2.1.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS4.SSS2.p6.2.m2.1.1.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p6.2.m2.1c">6\%</annotation></semantics></math>.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Findings Summary</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The experiments presented in Section <a href="#S5.SS1" title="V-A Text Encoder ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a> have shown that the best text encoder approach is fine-tuning a pre-trained BERT model with a GRU network trained from scratch.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In Section <a href="#S5.SS2" title="V-B Image Encoder ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a> we performed experiments for analyzing the impact of pre-trained networks to extract visual features, among them Faster-RCNN, ResNet-101, and VGG-16. The best result was using a Faster-RCNN, reaching a <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S6.p2.1.m1.1a"><mrow id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml"><mn id="S6.p2.1.m1.1.1.2" xref="S6.p2.1.m1.1.1.2.cmml">3</mn><mo id="S6.p2.1.m1.1.1.1" xref="S6.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><apply id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.p2.1.m1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.p2.1.m1.1.1.2.cmml" xref="S6.p2.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">3\%</annotation></semantics></math> improvement in the overall accuracy.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">We analyzed different ways to perform multimodal feature fusion in Section <a href="#S5.SS3" title="V-C Fusion strategy ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a>. In this sense, the fusion mechanism that obtained the best result was the element-wise product. It provides <math id="S6.p3.1.m1.1" class="ltx_Math" alttext="\approx 3\%" display="inline"><semantics id="S6.p3.1.m1.1a"><mrow id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml"><mi id="S6.p3.1.m1.1.1.2" xref="S6.p3.1.m1.1.1.2.cmml"></mi><mo id="S6.p3.1.m1.1.1.1" xref="S6.p3.1.m1.1.1.1.cmml">≈</mo><mrow id="S6.p3.1.m1.1.1.3" xref="S6.p3.1.m1.1.1.3.cmml"><mn id="S6.p3.1.m1.1.1.3.2" xref="S6.p3.1.m1.1.1.3.2.cmml">3</mn><mo id="S6.p3.1.m1.1.1.3.1" xref="S6.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><apply id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1"><approx id="S6.p3.1.m1.1.1.1.cmml" xref="S6.p3.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S6.p3.1.m1.1.1.2.cmml" xref="S6.p3.1.m1.1.1.2">absent</csymbol><apply id="S6.p3.1.m1.1.1.3.cmml" xref="S6.p3.1.m1.1.1.3"><csymbol cd="latexml" id="S6.p3.1.m1.1.1.3.1.cmml" xref="S6.p3.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S6.p3.1.m1.1.1.3.2.cmml" xref="S6.p3.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">\approx 3\%</annotation></semantics></math> higher overall accuracy when compared to the other fusion approaches.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Finally, in Section <a href="#S5.SS4" title="V-D Attention Mechanism ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-D</span></span></a> we have studied two main attention mechanisms and their variations. They aim to provide question-aware image representation by attending to the most important spatial features. The top performing mechanism is the Top-Down attention with the ReLU activation function, which provided an <math id="S6.p4.1.m1.1" class="ltx_Math" alttext="\approx 6\%" display="inline"><semantics id="S6.p4.1.m1.1a"><mrow id="S6.p4.1.m1.1.1" xref="S6.p4.1.m1.1.1.cmml"><mi id="S6.p4.1.m1.1.1.2" xref="S6.p4.1.m1.1.1.2.cmml"></mi><mo id="S6.p4.1.m1.1.1.1" xref="S6.p4.1.m1.1.1.1.cmml">≈</mo><mrow id="S6.p4.1.m1.1.1.3" xref="S6.p4.1.m1.1.1.3.cmml"><mn id="S6.p4.1.m1.1.1.3.2" xref="S6.p4.1.m1.1.1.3.2.cmml">6</mn><mo id="S6.p4.1.m1.1.1.3.1" xref="S6.p4.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.1b"><apply id="S6.p4.1.m1.1.1.cmml" xref="S6.p4.1.m1.1.1"><approx id="S6.p4.1.m1.1.1.1.cmml" xref="S6.p4.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S6.p4.1.m1.1.1.2.cmml" xref="S6.p4.1.m1.1.1.2">absent</csymbol><apply id="S6.p4.1.m1.1.1.3.cmml" xref="S6.p4.1.m1.1.1.3"><csymbol cd="latexml" id="S6.p4.1.m1.1.1.3.1.cmml" xref="S6.p4.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S6.p4.1.m1.1.1.3.2.cmml" xref="S6.p4.1.m1.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.1c">\approx 6\%</annotation></semantics></math> overall accuracy improvement when compared to the base architecture.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Comparison to state-of-the-art methods</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">After evaluating individually each component in a typical VQA architecture, our goal in this section is to compare the approach that combines the best performing components into a single model with the current state-of-the-art in VQA. Our comparison involves the following VQA models: <span id="S7.p1.1.1" class="ltx_text ltx_font_smallcaps">Deeper-lstm-q</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, ReasonNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, Tips&amp;Tricks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, and the recent <span id="S7.p1.1.2" class="ltx_text ltx_font_smallcaps">block</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Tables <a href="#S5.T4" title="TABLE IV ‣ V-D2 Co-Attention ‣ V-D Attention Mechanism ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and <a href="#S5.T5" title="TABLE V ‣ V-D2 Co-Attention ‣ V-D Attention Mechanism ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> show that our best architecture outperforms all competitors but <span id="S7.p2.1.1" class="ltx_text ltx_font_smallcaps">block</span>, in both Test-Standard (Table <a href="#S5.T4" title="TABLE IV ‣ V-D2 Co-Attention ‣ V-D Attention Mechanism ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>) and Test-Dev sets (Table <a href="#S5.T5" title="TABLE V ‣ V-D2 Co-Attention ‣ V-D Attention Mechanism ‣ V Experimental Analysis ‣ Component Analysis for Visual Question Answering Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>). Despite <span id="S7.p2.1.2" class="ltx_text ltx_font_smallcaps">block</span> presenting a marginal advantage in accuracy, we have shown in this paper that by carefully analyzing each individual component we are capable of generating a method, without any bells and whistles, that is on par with much more complex methods.
For instance, <span id="S7.p2.1.3" class="ltx_text ltx_font_smallcaps">block</span> and MCB require 18M and 32M parameters respectively for the fusion scheme alone, while our fusion approach is parameter-free.
Moreover, our model performs far better than <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, which are also arguably much more complex methods.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this study we observed the actual impact of several components within VQA models. We have shown that transformer-based encoders together with GRU models provide the best performance for question representation. Notably, we demonstrated that using pre-trained text representations provide consistent performance improvements across several hyper-parameter configurations. We have also shown that using an object detector fine-tuned with external data provides large improvements in accuracy. Our experiments have demonstrated that even simple fusion strategies can achieve performance on par with the state-of-the-art. Moreover, we have shown that attention mechanisms are paramount for learning top performing networks, once they allow producing question-aware image representations that are capable of encoding spatial relations. It became clear that Top-Down is the preferred attention method, given its results with ReLU activation. It is is now clear that some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant and can be removed altogether without harming accuracy.
For future work, we expect to expand this study in two main ways: (i) cover additional datasets, such as Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>; and (ii) study in an exhaustive fashion how distinct components interact with each other, instead of observing their impact alone on the classification performance.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nivel Superior – Brasil (CAPES) – Finance Code 001. We also would like to thank FAPERGS for funding this research.
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the graphics cards used for this research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for
visual recognition and description,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2015, pp. 2625–2634.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár,
J. Gao, X. He, M. Mitchell, J. C. Platt <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “From captions to
visual concepts and back,” in <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2015, pp. 1473–1482.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X. Chen and C. Lawrence Zitnick, “Mind’s eye: A recurrent visual
representation for image caption generation,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE conference on computer vision and pattern recognition</em>, 2015, pp.
2422–2431.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and
D. Parikh, “Vqa: Visual question answering,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE international conference on computer vision</em>, 2015, pp. 2425–2433.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Singh, V. Ying, and A. Nutkiewicz, “Attention on attention: Architectures
for visual question answering (vqa),” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1803.07724</em>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Wehrmann, C. Kolling, and R. C. Barros, “Adaptive cross-modal embeddings
for image-text alignment,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">AAAI 2020</em>, 2018, pp. 7718–7726.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Wehrmann, D. M. Souza, M. A. Lopes, and R. C. Barros, “Language-agnostic
visual-semantic embeddings,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ICCV 2019</em>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Tu, M. Meng, M. W. Lee, T. E. Choe, and S.-C. Zhu, “Joint video and text
parsing for understanding events and answering queries,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE
MultiMedia</em>, vol. 21, no. 2, pp. 42–70, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. Miller, R. Miller
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Vizwiz: nearly real-time answers to visual questions,” in
<em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 23nd ACM Symposium on User Interface Software and
Technology</em>, 2010, pp. 333–342.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
W. S. Lasecki, Y. Zhong, and J. P. Bigham, “Increasing the bandwidth of
crowdsourced visual question answering to better support blind users,” in
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th international ACM SIGACCESS conference on
Computers &amp; accessibility</em>.   ACM,
2014, pp. 263–264.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-guided
spatial attention for visual question answering,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">European
Conference on Computer Vision</em>.   Springer, 2016, pp. 451–466.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for word
representation,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP)</em>, 2014, pp. 1532–1543.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word
representations in vector space,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1301.3781</em>,
2013.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and
S. Fidler, “Skip-thought vectors,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, 2015, pp. 3294–3302.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep
bidirectional transformers for language understanding,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, 2015, pp. 91–99.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention for
image-text matching,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2018, pp. 201–216.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
“Bottom-up and top-down attention for image captioning and visual question
answering,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018, pp. 6077–6086.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, “Block: Bilinear
superdiagonal fusion for visual question answering and visual relationship
detection,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.00038</em>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
R. Cadene, H. Ben-Younes, M. Cord, and N. Thome, “Murel: Multimodal relational
reasoning for visual question answering,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 1989–1998.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach,
“Multimodal compact bilinear pooling for visual question answering and
visual grounding,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.01847</em>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear pooling,” in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016, pp. 317–326.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image
co-attention for visual question answering,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, 2016, pp.
289–297.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons: A neural-based
approach to answering questions about images,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015, pp.
1–9.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Mallya and S. Lazebnik, “Learning models for actions and person-object
interactions with transfer to question answering,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.   Springer, 2016, pp. 414–428.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C. Xiong, S. Merity, and R. Socher, “Dynamic memory networks for visual and
textual question answering,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2016, pp. 2397–2406.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “M2: Meshed-memory
transformer for image captioning,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.08226</em>,
2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Wehrmann and R. C. Barros, “Bidirectional retrieval made simple,” in
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">CVPR 2018</em>, 2018, pp. 7718–7726.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Wehrmann, C. Kolling, and R. C. Barros, “Fast and efficient text
classification with class-based embeddings,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IJCNN 2019</em>.   IEEE, 2017, pp. 2384–2391.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Wehrmann and R. C. Barros, “Convolutions through time for multi-label movie
genre classification,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">SAC 2017</em>, 2017, pp. 114–119.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in
<em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, 2017, pp. 5998–6008.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for
image question answering,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016, pp. 21–29.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
K. J. Shih, S. Singh, and D. Hoiem, “Where to look: Focus regions for visual
question answering,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016, pp. 4613–4621.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Z. Yu, J. Yu, C. Xiang, J. Fan, and D. Tao, “Beyond bilinear: Generalized
multimodal factorized high-order pooling for visual question answering,”
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>, vol. 29,
no. 12, pp. 5947–5959, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Burns, R. Tan, K. Saenko, S. Sclaroff, and B. A. Plummer, “Language
features matter: Effective language representations for vision-language
tasks,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.06327</em>, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Visual genome:
Connecting language and vision using crowdsourced dense image annotations,”
<em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, vol. 123, no. 1, pp. 32–73,
2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Bai, J. Fu, T. Zhao, and T. Mei, “Deep attention neural tensor network for
visual question answering,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference
on Computer Vision (ECCV)</em>, 2018, pp. 20–35.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
B. Duke and G. W. Taylor, “Generalized hadamard-product fusion operators for
visual question answering,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">2018 15th Conference on Computer and
Robot Vision (CRV)</em>.   IEEE, 2018, pp.
39–46.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
H. Ben-Younes, R. Cadene, M. Cord, and N. Thome, “Mutan: Multimodal tucker
fusion for visual question answering,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
international conference on computer vision</em>, 2017, pp. 2612–2620.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang, “Hadamard
product for low-rank bilinear pooling,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1610.04325</em>, 2016.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the v in
vqa matter: Elevating the role of image understanding in visual question
answering,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition</em>, 2017, pp. 6904–6913.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
D. Yadav, R. Jain, H. Agrawal, P. Chattopadhyay, T. Singh, A. Jain, S. B.
Singh, S. Lee, and D. Batra, “Evalai: Towards better evaluation systems for
ai agents,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.03570</em>, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Neural
computation</em>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,”
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, vol. 45, no. 11, pp.
2673–2681, 1997.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “<span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Learning phrase representations using
RNN encoder-decoder for statistical machine translation</span>,” <em id="bib.bib49.2.2" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1406.1078</em>, 2014.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
A. F. Agarap, “Deep learning using rectified linear units (relu),”
<em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.08375</em>, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, “Efficient
backprop,” in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Neural networks: Tricks of the trade</em>.   Springer, 2012, pp. 9–48.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
W. Xue and T. Li, “Aspect based sentiment analysis with gated convolutional
networks,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.07043</em>, 2018.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
I. Ilievski and J. Feng, “Multimodal learning and reasoning for visual
question answering,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">NIPS 2017</em>, 2017, pp. 551–562.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
D. Teney, P. Anderson, X. He, and A. van den Hengel, “Tips and tricks for
visual question answering: Learnings from the 2017 challenge,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">CVPR
2018</em>, 2018, pp. 4223–4232.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2002.05103" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2002.05104" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2002.05104">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2002.05104" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2002.05106" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 18:55:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
