<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.10505] Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model</title><meta property="og:description" content="While developing artificial intelligence (AI)-based algorithms to solve problems, the amount of data plays a pivotal role—large amount of data helps the researchers and engineers to develop robust AI algorithms. In the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.10505">

<!--Generated on Mon Mar 11 11:27:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amartya Bhattacharya
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science and Engineering
<br class="ltx_break">University of Calcutta
<br class="ltx_break">
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Manish Gawali
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Senior Data Scientist
<br class="ltx_break">DeepTek Inc
<br class="ltx_break">
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jitesh Seth
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Data Scientist
<br class="ltx_break">DeepTek Inc
<br class="ltx_break">
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Viraj Kulkarni
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Chief Data Scientist
<br class="ltx_break">DeepTek Inc
<br class="ltx_break">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">While developing artificial intelligence (AI)-based algorithms to solve problems, the amount of data plays a pivotal role—large amount of data helps the researchers and engineers to develop robust AI algorithms. In the case of building AI-based models for problems related to medical imaging, these data need to be transferred from the medical institutions where they were acquired to the organizations developing the algorithms. This movement of data involves time-consuming formalities like complying with HIPAA, GDPR, etc.There is also a risk of patients’ private data getting leaked, compromising their confidentiality. One solution to these problems is using the Federated Learning framework.</p>
<p id="id2.id2" class="ltx_p">Federated Learning (FL) helps AI models to generalize better and create a robust AI model by using data from different sources having different distributions and data characteristics without moving all the data to a central server. In our paper, we apply the FL framework for training a deep learning model to solve a binary classification problem of predicting the presence or absence of COVID-19. We took three different sources of data and trained individual models on each source. Then we trained an FL model on the complete data and compared all the model performances. We demonstrated that the FL model performs better than the individual models. Moreover, the FL model performed at par with the model trained on all the data combined at a central server. Thus Federated Learning leads to generalized AI models without the cost of data transfer and regulatory overhead.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2204.10505/assets/FederatedLearningFramework.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The Federated Learning Process: a) Initially a global model is sent to clients’ local servers b) The model gets trained on the local servers c) Model updates are then sent back to the global server d) The global model then utilizes the updates to build a better model and the process continues till we get a robust model</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">COVID-19 created a huge crisis in the healthcare sector, which was not equipped to handle the situation. With the increase in cases, it became difficult for radiologists to diagnose COVID-19 cases accurately and timely from the thousands of X-Ray reports. These situations suggested the need for automatic detection and classification of X-Ray images.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> . Thus researchers developed deep learning-based solutions to predict COVID-19 from X-Ray images<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The creation of these solutions required data to be transferred from medical institutions where it was acquired to organizations working on Computer Aided Detection(CAD) solutions. This process made the data vulnerable to exposure to several third-party individuals or organizations and getting misused.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Moreover, while creating AI models, generally the dataset used belongs to a single source and hence the data points are sampled from a distribution which might not be representative of the entire population. This could result in poor AI model performance when validating on a dataset with a different distribution from the training set.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Federated Learning (FL)
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is a solution to tackle both the problems mentioned above. Federated Learning is a distributed learning framework where models can be trained on the clients’ data on their own servers
without going through the complications of creating the model or distributing their data to any third-party organizations.
In FL the models are trained through multiple Federated rounds which results into a robust model being formed. A single FL round of training works by initially creating a model at a global server. The model is the pushed to all the clients’ servers. Since the data is present in the clients’ local server, the model gets trained at each of the local server. After training the updated models are then sent back to the global server where the Federated Averaging process<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is used to form a better model. In this way a robust model is built which results in better performance irrespective of the source of the test data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Thus, a robust solution is developed without the involvement of any data transfer.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To this date, the use of FL in the medical domain has been limited. But it has the potential to build AI models which can generalize well and reduce bias
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In this paper, we applied FL to train a Convolutional Neural Network(CNN) model to classify chest X-rays belonging to COVID-19 affected patients from others.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There is not much work on the application of FL in classification of chest X-Rays (CXRs) into COVID and non-COVID classes. Liu et. al 2020<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, built a novel model named Covid-Net that performed the task of detecting COVID-19 in CXRs with an accuracy of 92% with the model having a ResNet18 backbone, which proved to be better than some of the existing state-of-the-art CNN-based models in a federated architecture<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. They combined the data taken from different sources and then segregated it into multiple parts to maintain the non-IID characteristics. Feki et.al 2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> studied the performance of the Federated Learning model in classifying COVID-19 and non-COVID-19 chest X-Rays.They used a dataset containing 108 COVID-19 cases and then split them equally into 4 clients, but this meant that the data being used was still IID.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">We address these shortcomings by taking three different sources of data to maintain the non-IID nature.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> The data contains images belonging to two classes i.e COVID-19 present or absent. We trained an AI model on this data using the FLframework. The model is compared with each model trained on a single source and tested on other sources of data. Moreover, we also compared the model built using Federated Learning with a model trained by combining the data from all the sources.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In this paper we have trained five models. On each of the three client servers we trained individual models (Client1, Client2 and Client3). We also pooled all the data to train a centralised global model (the ”Combined model”). Then, we used the FL framework to train another global model (the ”FL model”). The performances of these models were evaluated on separate test sets from each client. We found that the FL model performed better than all the individual models and at par with the combined model in terms of the AUROC and AUPRC metrics.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Data and Methods</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Data</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We formulated the experiment by ensuring that the data used for each of the clients is non-Independent Identically Distributed(IID). In our experiments, data from three sources were used. The datasets had variations in the number of images per class i.e. COVID-19 and non-COVID-19. For the Client1, the COVID-19 chest X-ray data was collected from the SIRM website <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and the non-COVID-19 chest X-ray data was taken from the RSNA Pneumonia Detection challenge on Kaggle<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The data for Client 1 was downsampled such that it had a total of 719 images out of which 219 belonged to the COVID-19 class and the rest to the non-COVID-19 class. For Client2 the chest X-Ray images were collected from the Eurorad database from Kaggle Repository<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> . There were 2416 images out of which 1125 belonged to the COVID-19 class. Client3 data was collected from the IEEE data repository<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> having 287 images out of which 84 images belonged to the COVID-19 affected chest X-ray images. The data was then divided into the training, validation, and test datasets for each of the clients. The details about the split of the data for each of the clients are mentioned in TABLE <a href="#S3.T1" title="TABLE I ‣ III-A Data ‣ III Data and Methods ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. A large amount of variation was present in the data in terms of distribution as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ III-A Data ‣ III Data and Methods ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and the distribution of the pixel values per unit area has been given in <a href="#S3.F3" title="Figure 3 ‣ III-A Data ‣ III Data and Methods ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Client</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Data Sources</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Train</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Validation</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Test</th>
<th id="S3.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Client 1</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S3.T1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.2.1.2.1.1" class="ltx_tr">
<td id="S3.T1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Positive - SIRM</td>
</tr>
<tr id="S3.T1.1.2.1.2.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Negative - RSNA</td>
</tr>
</table>
</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">623</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S3.T1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">719</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Client 2</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Eurorad</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2330</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S3.T1.1.3.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2426</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Client 3</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">IEEE Data Repository</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">191</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">48</td>
<td id="S3.T1.1.4.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">287</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Total</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">3144</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">144</td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">144</td>
<td id="S3.T1.1.5.4.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">3432</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The source and the distribution of the data points (CXRs) after it is split into train, validation and test set </figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2204.10505/assets/DistributionofCXRs.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Client-wise distribution of CXRs of patients having or not having COVID-19.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2204.10505/assets/PixelDensity.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The frequency of pixel values in each dataset. All CXR images had 8-bit encoding and thus values range from 0-255. We clearly see how the data from each client is vastly different from each other.But it should be noted the minimum value of the pixel present was 0, but the curve was extrapolated in order to be converged with the axis. </figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Methods</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We used DenseNet121<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as the main neural network architecture. At first, initial models were built for each of the clients and trained on their data. Since it was a binary classification problem i.e non-COVID-19 and COVID-19, the final layer of the neural network model had a single neuron. We used Binary Cross-Entropy as the loss function. We used the Adam optimizer was used with the learning rate fixed to <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mo id="S3.SS2.p1.1.m1.1.1.3a" xref="S3.SS2.p1.1.m1.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">10</cn><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><minus id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">10^{-3}</annotation></semantics></math> since it was found to converge early. The combined model and the individual models were trained for 20 rounds. We set the number of epochs to 20 as we found out that the model converges within 20 rounds.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The FL model was trained using DenseNet121 as the base architecture. it was initialized using Imagenet weights. The global model was then transferred to be trained on each of the client’s datasets locally. In the training process, the local model was trained for 4 epochs and the weights corresponding to the least validation loss were stored.
These weights were sent to the global server and the global weights were calculated by averaging the local weights. The global FL model was then sent to all the clients for the next FL training round.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">This FL training round was repeated 5 times, thus the whole model underwent training for 20 epochs. We used threshold agnostic metrics like ROC-AUC, ROC-PRC scores for comparing the model performances as it is difficult to compare the model performance of different models using threshold dependent metrics like sensitivity, specificity, and F1-score. The results obtained have been discussed below.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2204.10505/assets/ROC-AUC.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Model Performances using the ROC-AUC scores. In general it can be seen from the plots that FL model performs well in classifying the test data for all the clients than the models that uses only single source of data.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2204.10505/assets/ROC-PRC.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Model Performances using the ROC-PRC scores. In general it can be seen from the plots that FL model performs well in classifying the test data for all the clients than the models that uses only single source of data.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_ll ltx_border_t" style="padding-bottom:2.15277pt;">Data</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">Client1</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">Client2</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">Client3</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">FL</th>
<th id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Combined</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_tt">Client1</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">99.96</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">88.85</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">86.44</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">99.61</td>
<td id="S4.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">99.86</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_t">Client2</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">37.85</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">93.94</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">77.94</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">92.31</td>
<td id="S4.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">99.92</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_ll ltx_border_t">Client3</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">43.85</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">84.57</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">97.73</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">88.31</td>
<td id="S4.T2.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">93.51</td>
</tr>
</tbody>
</table>
<br class="ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>ROC-AUC Score</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_ll ltx_border_t" style="padding-bottom:2.15277pt;">Data</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">Client1</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">Client2</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">Client3</th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">FL</th>
<th id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Combined</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_tt">Client1</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">99.01</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">80.03</td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">82.28</td>
<td id="S4.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">99.85</td>
<td id="S4.T3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">97.96</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_t">Client2</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">33.75</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">94.75</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">74.62</td>
<td id="S4.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">91.54</td>
<td id="S4.T3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">98.94</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_ll ltx_border_t">Client3</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">41.32</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">83.68</td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">95.67</td>
<td id="S4.T3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">88.57</td>
<td id="S4.T3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">92.86</td>
</tr>
</tbody>
</table>
<br class="ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>ROC-PRC Score</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The models trained with Client1 data, Client2 data, Client3 data, and the combined model were evaluated against the test data from each of the clients. The results were then compared against the results obtained by the FL Model. Figures <a href="#S4.F4" title="Figure 4 ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S4.F5" title="Figure 5 ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show the comparison of the ROC-AUC, ROC-PRC scores for each of the models.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">From the graph of the performance, it can be observed that each of the clients’ models performed well when evaluated against the test data belonging to the same source as the data with which it was trained. But they failed to perform as well when they were provided data from a different source. The same observation can be made for both the AUC-PRC table (TABLE <a href="#S4.T3" title="TABLE III ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) as well as the AUC-ROC table (TABLE <a href="#S4.T2" title="TABLE II ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>).</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Thus it can be concluded that the models, when trained from a single source of data get biased and don’t perform well on other sources of data.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">From Fig. <a href="#S4.F4" title="Figure 4 ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and as well as from TABLE <a href="#S4.T3" title="TABLE III ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and TABLE <a href="#S4.T2" title="TABLE II ‣ IV Results ‣ Application of Federated Learning in building a robust COVID-19 Chest X-ray classification Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, it can be seen that the model trained using the Federated Learning framework solves this issue and performs equally well on the test data coming from different sources. Unlike the individual models, it performs exceptionally well on the test data coming from different sources. Thus it can be inferred that by using the different sources of data and averaging the weights learned during the training process, a more robust model can be achieved.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Moreover, the process of developing a FL Deep Learning model did not require the data from all the sources to be sent for training. Instead, the model was sent to the clients to train the models locally. This suggests that the performance of the Federated Learning model is impressive as compared to the combined model. Although the combined model performs better than the Federated Model, all the data had to be sent to a central location to train it, which compromised the privacy issue as discussed above.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">AI models tend to get biased towards the training data and don’t perform well when evaluated against the data coming from different sources. This bias can be reduced if we utilize data from different sources for the training of a deep learning neural network model. In the case of medical imaging, the movement of data from medical institutions to organizations developing solutions is a time-consuming process.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Federated Learning is one of the solutions that try to solve both the problems by generalizing the model, by training the model parameters using the data from different sources. In this paper, we applied the Federated Learning-based framework to present a robust solution for classifying COVID and non-COVID chest X-ray images. We trained 5 different models to compare the results. Three of those models were built on the corresponding clients’ data, one was built using Federated Learning, and another one by combining all the data. From the results obtained, we could infer that the Federated Learning model performed better than the models built using the three clients’ data individually, thus generalizing well. Moreover, performed almost equally well as the model trained on the aggregated data set. This shows that in real-time, Federated Learning shows a path to solve both the privacy as well as generalization issues.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, S. Pati, A. Kotrotsou,
M. Milchenko, W. Xu, D. Marcus, R. R. Colen, <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Federated
learning in medicine: facilitating multi-institutional collaborations without
sharing patient data,” <span id="bib.bib1.2.2" class="ltx_text ltx_font_italic">Scientific reports</span>, vol. 10, no. 1, pp. 1–12,
2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y. Zhang, J. Chen, B. Liu, Y. Yang, H. Li, X. Zheng, X. Chen, T. Ren, and
N. Xiong, “Covid-19 public opinion and emotion monitoring system based on
time series thermal new word mining,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.11458</span>,
2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
and D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.05492</span>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
V. Kulkarni, M. Kulkarni, and A. Pant, “Survey of personalization techniques
for federated learning,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2020 Fourth World Conference on Smart
Trends in Systems, Security and Sustainability (WorldS4)</span>, pp. 794–797,
IEEE, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
L. Li, Y. Fan, M. Tse, and K.-Y. Lin, “A review of applications in federated
learning,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Computers &amp; Industrial Engineering</span>, vol. 149, p. 106854,
2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W. Zhang, T. Zhou, Q. Lu, X. Wang, C. Zhu, H. Sun, Z. Wang, S. K. Lo, and F.-Y.
Wang, “Dynamic-fusion-based federated learning for covid-19 detection,”
<span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, vol. 8, no. 21, pp. 15884–15891,
2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni, S. Bakas,
M. N. Galtier, B. A. Landman, K. Maier-Hein, <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">et al.</span>, “The future of
digital health with federated learning,” <span id="bib.bib7.2.2" class="ltx_text ltx_font_italic">NPJ digital medicine</span>, vol. 3,
no. 1, pp. 1–7, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B. Liu, B. Yan, Y. Zhou, Y. Yang, and Y. Zhang, “Experiments of federated
learning for covid-19 chest x-ray images,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2007.05592</span>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
I. Feki, S. Ammar, Y. Kessentini, and K. Muhammad, “Federated learning for
covid-19 screening from chest x-ray images,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Applied Soft Computing</span>,
vol. 106, p. 107330, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Gawali, C. Arvind, S. Suryavanshi, H. Madaan, A. Gaikwad, K. Bhanu Prakash,
V. Kulkarni, and A. Pant, “Comparison of privacy-preserving distributed deep
learning methods in healthcare,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Annual Conference on Medical Image
Understanding and Analysis</span>, pp. 457–471, Springer, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Rahman, A. Khandakar, Y. Qiblawey, A. Tahir, S. Kiranyaz, S. B. A. Kashem,
M. T. Islam, S. Al Maadeed, S. M. Zughaier, M. S. Khan, <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">et al.</span>,
“Exploring the effect of image enhancement techniques on covid-19 detection
using chest x-ray images,” <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Computers in biology and medicine</span>,
vol. 132, p. 104319, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “Chestx-ray8:
Hospital-scale chest x-ray database and benchmarks on weakly-supervised
classification and localization of common thorax diseases,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</span>, pp. 2097–2106, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. E. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A. Kadir, Z. B. Mahbub,
K. R. Islam, M. S. Khan, A. Iqbal, N. Al Emadi, <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Can ai help
in screening viral and covid-19 pneumonia?,” <span id="bib.bib13.2.2" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 8,
pp. 132665–132676, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Q. Duong, and M. Ghassemi,
“Covid-19 image data collection: Prospective predictions are the future,”
<span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.11988</span>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
convolutional networks,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</span>, pp. 4700–4708, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.10504" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.10505" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.10505">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.10505" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.10506" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 11:27:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
