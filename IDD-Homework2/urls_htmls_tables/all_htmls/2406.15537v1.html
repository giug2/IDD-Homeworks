<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity</title>
<!--Generated on Fri Jun 21 17:05:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.15537v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S1" title="In R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2" title="In R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Material and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2.SS1" title="In 2 Material and Methods ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2.SS2" title="In 2 Material and Methods ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Functional Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2.SS3" title="In 2 Material and Methods ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Music Feature Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2.SS4" title="In 2 Material and Methods ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Encoding Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2.SS5" title="In 2 Material and Methods ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Decoding Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2.SS6" title="In 2 Material and Methods ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3" title="In R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.SS1" title="In 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Encoding Models and Delineation of brain areas responsive to music</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.SS2" title="In 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Identification Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.SS3" title="In 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Genre Decoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.SS4" title="In 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Impact of Functional Alignment techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.SS5" title="In 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Decoding in Time</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4" title="In R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS1" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implications of Music Decoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS2" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Performance on Genre Decoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS3" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Identification of music-related brain regions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS4" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Impact on Musical Therapy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS5" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Deeper Investigation of Music and Emotions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS6" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Extension to Generative Music</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS7" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S4.SS8" title="In 4 Discussion ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S5" title="In R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matteo Ferrante 
<br class="ltx_break"/>Department of Biomedicine and Prevention
<br class="ltx_break"/>University of Rome Tor Vergata
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">matteo.ferrante@uniroma2.it</span>
<br class="ltx_break"/>&amp;Matteo Ciferri* 
<br class="ltx_break"/>Department of Biomedicine and Prevention
<br class="ltx_break"/>University of Rome Tor Vergata
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">matteo.ciferri@students.uniroma2.eu</span>
<br class="ltx_break"/>Nicola Toschi 
<br class="ltx_break"/>Department of Biomedicine and Prevention
<br class="ltx_break"/>University of Rome Tor Vergata
<br class="ltx_break"/>A.A. Martinos Center for Biomedical Imaging
<br class="ltx_break"/>Harvard Medical School/MGH, Boston (US)
<br class="ltx_break"/>
</span><span class="ltx_author_notes">These authors contributed equally to this work</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">Music is a universal phenomenon that profoundly influences human experiences across cultures. This study investigates whether music can be decoded from human brain activity measured with functional MRI (fMRI) during its perception. Leveraging recent advancements in extensive datasets and pre-trained computational models, we construct mappings between neural data and latent representations of musical stimuli. Our approach integrates functional and anatomical alignment techniques to facilitate cross-subject decoding, addressing the challenges posed by the low temporal resolution and signal-to-noise ratio (SNR) in fMRI data. Starting from the GTZan fMRI dataset, where five participants listened to 540 musical stimuli from 10 different genres while their brain activity was recorded, we used the CLAP (Contrastive Language-Audio Pretraining) model to extract latent representations of the musical stimuli and developed voxel-wise encoding models to identify brain regions responsive to these stimuli. By applying a threshold to the association between predicted and actual brain activity, we identified specific regions of interest (ROIs) which can be interpreted as key players in music processing. Our decoding pipeline, primarily retrieval-based, employs a linear map to project brain activity to the corresponding CLAP features. This enables us to predict and retrieve the musical stimuli most similar to those that originated the fMRI data. Our results demonstrate state-of-the-art identification accuracy, with our methods significantly outperforming existing approaches. Our findings suggest that neural-based music retrieval systems could enable personalized recommendations and therapeutic applications. Future work could use higher temporal resolution neuroimaging and generative models to improve decoding accuracy and explore the neural underpinnings of music perception and emotion.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Music universally permeates cultures, exerting a profound influence on the lives of those who perceive its harmonies and rhythms. Despite its pervasive role, the intricacies of how music impacts the human brain remain enigmatic. Music engages complex neurological pathways, triggering diverse emotional responses, evoking vivid episodic memories, and even interacting with various neurological disorders. These interactions suggest a deep and multifaceted relationship between music and brain function, warranting extensive scientific exploration <cite class="ltx_cite ltx_citemacro_citep">(Margulis et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib25" title="">2019</a>)</cite>. This study investigates the extent to which music can be decoded from human brain activity measured with functional MRI (fMRI).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Historically, the study of how the brain interprets and processes music has been a topic of classical inquiry within neuroscience <cite class="ltx_cite ltx_citemacro_citep">(Raglio et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib33" title="">2019</a>)</cite>. However, recent advancements have revolutionized this field, making it practicable to use AI to explore and decode brain patterns relative to a wide set of stimuli <cite class="ltx_cite ltx_citemacro_citep">(Oota et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib29" title="">2023</a>)</cite>. In this context, the emergence of extensive datasets coupled with robust, pre-trained computational models presents an unprecedented opportunity. These tools enable us to construct detailed mappings between neural data and the latent, compact representations of external stimuli, such as images <cite class="ltx_cite ltx_citemacro_citep">(Ferrante et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib15" title="">2023c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib13" title="">a</a>; Ozcelik and VanRullen, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib30" title="">2023</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib6" title="">2022</a>; Scotti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib34" title="">2023</a>)</cite>, videos <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib7" title="">2023</a>)</cite>, language <cite class="ltx_cite ltx_citemacro_citep">(Antonello et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib3" title="">2023</a>; Défossez et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib11" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib36" title="">Tang et al., </a>)</cite>, and notably, music <cite class="ltx_cite ltx_citemacro_citep">(Denk et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite>. These works propose several retrievals as well as generative pipelines to create a map between neural data and latent representations of external stimuli. The neural data is primarily measured via functional magnetic resonance imaging (fMRI), magnetoencephalography (MEG), or electroencephalography (EEG), and the latent representations are commonly obtained from large pretrained models. The estimated latent representations are further used for stimulus retrieval or conditioning of a generative model to generate e.g. images in vision decoding. Typically, these pipelines involve linear mappings between these two spaces (brain and latent representations of stimuli) and require subject-specific models, although some approaches to multisubject brain representations or alignment and nonlinear mappings exist <cite class="ltx_cite ltx_citemacro_citep">(Ferrante et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib14" title="">2023b</a>; Benchetrit et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib5" title="">2023</a>; Scotti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib35" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="830" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our pipeline. <span class="ltx_text ltx_font_bold" id="S1.F1.4.1">Top pane</span>: In the GTZan fMRI experiment, five participants were exposed to auditory stimuli that included multiple musical tracks while their brain activity was monitored via functional MRI. This setup captures the direct neural response to complex auditory inputs. In the <span class="ltx_text ltx_font_bold" id="S1.F1.5.2">middle pane</span>, our encoding pipeline is described: Starting from the music stimulus, we first obtain its latent representation using the CLAP model. Subsequently, we develop voxel-wise encoding models to map the brain’s response to these stimuli to this latent space. A threshold is then applied to the voxel-wise correlation between real and predicted brain activities to identify brain regions whose activity allows the best decoding of musical stimuli. These regions are considered as most responsive to music-related regions of interest (ROIs). The <span class="ltx_text ltx_font_bold" id="S1.F1.6.3">bottom pane</span> outlines our decoding pipeline, which is primarily retrieval-based. We train a model that inputs brain activity from the previously identified ROIs and predicts the corresponding CLAP features. Using these features, we then search within the CLAP latent space for the closest musical stimulus, selecting the nearest k (k=5) stimulus as our retrieved samples.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Understanding these complex relationships is both fascinating and informative, potentially offering insights into fundamental brain functions. For example, understanding the connection between music perception and neural responses could unlock novel avenues for diagnosing and treating neurological disorders. Moreover, it could enhance music therapy approaches, potentially leading to innovative treatments that harness the therapeutic properties of music <cite class="ltx_cite ltx_citemacro_citep">(Kamioka et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib19" title="">2014</a>; de Witte et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib8" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we aim to decode music from brain activity—a process that involves translating the neural signals evoked by music into a comprehensible format. This objective challenges us to retrieve complex auditory information encoded within the brain’s activity. In the case of fMRI, the primary challenge lies in decoding a signal of inherently higher frequency than the neural signal, <span class="ltx_text" id="S1.p4.1.1" style="color:#000000;">which is further confounded by the local variation in the brain of the Haemodynamic Response Function (HRF)</span>. Additional limitations include the constraints posed by small datasets typically comprising few subjects with intrinsic between-subject anatomical and functional differences.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges, we first constructed encoding models to identify brain regions responsive to musical stimuli. We then aggregated brain activity across subjects to facilitate a cross-subject decoding approach. This included aligning functional brain data and mapping the identified regions’ activity to the latent representations of music stimuli. These representations were derived using an open-source, multimodal pre-trained foundation model known as Contrastive Language-Audio Pretraining (CLAP) <cite class="ltx_cite ltx_citemacro_citep">(Elizalde et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib12" title="">2022</a>)</cite>. In the final stages of our study, we compared the representations of music estimated from brain data with their true counterparts, employing a selection criterion that identified the five closest matching representations as potential candidates for accurate decoding.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The studies most closely related to our research include <cite class="ltx_cite ltx_citemacro_cite">Bellier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib4" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Denk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Bellier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib4" title="">2023</a>)</cite> demonstrate that time-frequency decompositions can be effective representations for this type of task, and that they can be performed using both linear and nonlinear approaches to decode the auditory experience using invasive iEEG data.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Another pivotal study, <cite class="ltx_cite ltx_citemacro_citep">(Denk et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite>, shares similarities with our approach in that it addresses the challenges of retrieval-based as well as generative music decoding using the same fMRI dataset we employ here. However, unlike our methodology, <cite class="ltx_cite ltx_citemacro_cite">Denk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite> uses subject-specific decoding pipelines based on anatomical atlases and proprietary models like MuLAN and MusicLM <cite class="ltx_cite ltx_citemacro_citep">(Agostinelli et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib2" title="">2023</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib17" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In this paper, we advance the state of the art by designing a streamlined pipeline that leverages open-source models. Our approach begins by identifying brain regions whose activity can be reliably modelled using latent representations of audio stimuli. Subsequently, we use the brain activity from these regions to construct cross-subject decoding pipelines. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_tag">1</span></a> depicts our pipeline. In our work, we aspire to refine our understanding of how music is processed within the brain and to lay the groundwork for future explorations into the therapeutic potential of music in neurological settings.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Material and Methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we describe the proposed method and the data we used. The data are publicly available and can be requested at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openneuro.org/datasets/ds003720/versions/1.0.1" title="">https://openneuro.org/datasets/ds003720/versions/1.0.1</a>. All experiments and models were trained on a server equipped with four NVIDIA A100 GPU cards (80GB RAM each connected through NVLINK) and 2 TB of System RAM. Throughout this paper, we will use the terms "fMRI data" as "brain activity", "neural activity" or "neural representations" interchangeably. These terms all stand for the fMRI signal, averaged over the time-points related to a specific stimulus, i.e. a 3D map. Additionally, the terms "musical features" or "musical representations" always refer to the embedding of musical stimuli generated by the CLAP model.
Code is available at this repository: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/neoayanami/fmri-music-retrieve" title="">https://github.com/neoayanami/fmri-music-retrieve</a>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The GTZan fMRI dataset <cite class="ltx_cite ltx_citemacro_citep">(Nakai et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib28" title="">2023</a>)</cite> comprises functional magnetic resonance imaging (fMRI) data collected from five subjects ("sub-001" to "sub-005") while they listened to music stimuli drawn from 10 distinct genres. The experimental protocol included 18 fMRI acquisitions (i.e. "runs") per subject, consisting of 12 training runs and 6 test runs. Each run is also associated with detailed information about each stimulus, including onset time, genre type, track name, and start and end times of excerpts from the original music stimuli. All stimuli have a duration of 15 seconds, including 2 seconds of fade-in and fade-out (a total of 4 seconds). The data are provided in intensity normalized form, i.e. after root mean square (RMS) normalization. In the test run ensemble, each musical stimulus was administered four times and the brain activity averaged across identical stimuli. Data averaging improves the signal-to-noise ratio (SNR) and enhances the detection of consistent neural responses associated with the stimulus under investigation.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">After motion correction, we co-registered the fMRI data to the Montreal Neurological Institute (MNI) standard space using a T1w anatomical image as reference for each subject, and applied detrending and standardization at the run level. The final step involved "delaying" the brain activity by 3 Repetition Times (TR) (i.e. 4.5 s) in order to account for the peak of the hemodynamic response, and averaging the following 15 seconds to obtain a neural representation for each musical stimulus. Our final dataset is therefore composed of a total of 540 stimuli-processed fMRI pairs for each subject, divided into 480/60 train/test, as defined by the authors of the dataset. We used FSL <cite class="ltx_cite ltx_citemacro_citep">(Jenkinson et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib18" title="">2012</a>)</cite> for co-registration and the Nilearn python library <cite class="ltx_cite ltx_citemacro_citep">(Abraham et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib1" title="">2014</a>)</cite> to perform all other preprocessing steps.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Functional Alignment</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">To address the inherent variability in brain structure/function across different individuals, we explored three distinct methodologies for aggregating cross-subject data. These techniques aim to enhance the robustness and accuracy of decoding models by aligning and integrating neural data from multiple subjects. Each method offers a unique approach to the challenge of intersubject variability, a common hurdle in neuroimaging studies.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The first method we implemented was anatomical alignment, which uses standard brain atlases to align brain imaging data from different subjects based on their anatomical landmarks. By mapping each subject’s data to a common anatomical space, we can directly compare and combine data across individuals, despite differences in brain size, shape, or orientation. This method is widely used in neuroimaging as it facilitates the direct comparison of localized brain activity across subjects.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Moving beyond mere anatomical correspondence, our second method, functional alignment, aligns brain activity based on functional data. This technique involves matching brain regions that exhibit similar activity patterns during specific tasks or stimuli across different subjects. Unlike anatomical alignment, functional alignment accounts for individual variations in brain function topology that may not align with variations in physical brain structures, making it particularly advantageous for studies where functional responses to complex stimuli are the primary focus. To this end, we leveraged the "hyperalignment" strategy proposed by <cite class="ltx_cite ltx_citemacro_cite">Haxby et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib16" title="">2011</a>)</cite> based on Procrustes analysis.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Lastly, given recent literature <cite class="ltx_cite ltx_citemacro_citep">(Ferrante et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib14" title="">2023b</a>; Défossez et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib9" title="">2023</a>; Benchetrit et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib5" title="">2023</a>)</cite> which demonstrated that linear layers are a useful tool to align neural representations into a common space, we employed ridge regression to aggregate cross-subject brain data. This approach applies regularization to address multicollinearity in high-dimensional datasets, which is typical of fMRI data. By introducing a penalty term, ridge regression combines voxel-wise data from different subjects into a unified model while enhancing the stability and generalizability of our predictions. Each of these methods was tested for its potential to improve the accuracy of our decoding models, with the goal of establishing a reliable approach to interpreting complex brain data in a multi-subject context.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Music Feature Extraction</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Our brain engages with music in intricate, non-linear ways, forming representations that support our cognitive processes. This complexity suggests that a multimodal pre-trained model like CLAP, <cite class="ltx_cite ltx_citemacro_cite">Elizalde et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib12" title="">2022</a>)</cite>) may mimic some aspects of how our brains process music. Under this hypothesis, CLAP can transform musical stimuli into a vectorial representation that could present topological similarities similarity with the brain representations, allowing the identifications of simple mapping between the latent representations generated by CLAP and those generated by the human brain.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">CLAP is a multimodal neural network designed for contrastive learning in the realm of audio and text processing. It is trained on a diverse set of audio and text pairs, learning to align text and audio latent representations. The model employs the SWINTransformer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib24" title="">2021</a>)</cite> to extract audio features from log-Mel representations and the RoBERTa model <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib23" title="">2019</a>)</cite> to extract text representations, both projected into a shared latent space of identical dimensionality. The similarity between audio and text features is measured using cosine similarity.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S2.F2" title="Figure 2 ‣ 2.3 Music Feature Extraction ‣ 2 Material and Methods ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results of using t-Distributed Stochastic Neighbor Embedding (t-SNE, <cite class="ltx_cite ltx_citemacro_cite">van der Maaten and Hinton (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib37" title="">2008</a>)</cite>) to create a 2D visualization of the true music features overlayed on genre labels, offering a qualitative understanding of how the CLAP model’s representations are able to separate different genres.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="664" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Two-dimensional t-SNE representation of CLAP latent representations of music, coloured by different musical genres.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Encoding Models</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The primary goal of this part of our study was to identify brain regions responsive to musical stimuli by constructing voxel-wise encoding models. These models map the latent representations of musical stimuli onto voxel-wise brain activity. To assess the efficacy of each voxel’s model, we employed a cross-validation scheme, wherein the correlation between the predicted and real brain activities of each voxel was measured.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.4">Model training incorporated a hyperparameter search for the regularization parameter <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><mi id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><ci id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">italic_α</annotation></semantics></math>. We explored a range of <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS4.p2.2.m2.1"><semantics id="S2.SS4.p2.2.m2.1a"><mi id="S2.SS4.p2.2.m2.1.1" xref="S2.SS4.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.2.m2.1b"><ci id="S2.SS4.p2.2.m2.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.2.m2.1d">italic_α</annotation></semantics></math> values set on a logarithmic scale from <math alttext="10^{-2}" class="ltx_Math" display="inline" id="S2.SS4.p2.3.m3.1"><semantics id="S2.SS4.p2.3.m3.1a"><msup id="S2.SS4.p2.3.m3.1.1" xref="S2.SS4.p2.3.m3.1.1.cmml"><mn id="S2.SS4.p2.3.m3.1.1.2" xref="S2.SS4.p2.3.m3.1.1.2.cmml">10</mn><mrow id="S2.SS4.p2.3.m3.1.1.3" xref="S2.SS4.p2.3.m3.1.1.3.cmml"><mo id="S2.SS4.p2.3.m3.1.1.3a" xref="S2.SS4.p2.3.m3.1.1.3.cmml">−</mo><mn id="S2.SS4.p2.3.m3.1.1.3.2" xref="S2.SS4.p2.3.m3.1.1.3.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.3.m3.1b"><apply id="S2.SS4.p2.3.m3.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.3.m3.1.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1">superscript</csymbol><cn id="S2.SS4.p2.3.m3.1.1.2.cmml" type="integer" xref="S2.SS4.p2.3.m3.1.1.2">10</cn><apply id="S2.SS4.p2.3.m3.1.1.3.cmml" xref="S2.SS4.p2.3.m3.1.1.3"><minus id="S2.SS4.p2.3.m3.1.1.3.1.cmml" xref="S2.SS4.p2.3.m3.1.1.3"></minus><cn id="S2.SS4.p2.3.m3.1.1.3.2.cmml" type="integer" xref="S2.SS4.p2.3.m3.1.1.3.2">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.3.m3.1c">10^{-2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.3.m3.1d">10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="10^{3}" class="ltx_Math" display="inline" id="S2.SS4.p2.4.m4.1"><semantics id="S2.SS4.p2.4.m4.1a"><msup id="S2.SS4.p2.4.m4.1.1" xref="S2.SS4.p2.4.m4.1.1.cmml"><mn id="S2.SS4.p2.4.m4.1.1.2" xref="S2.SS4.p2.4.m4.1.1.2.cmml">10</mn><mn id="S2.SS4.p2.4.m4.1.1.3" xref="S2.SS4.p2.4.m4.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.4.m4.1b"><apply id="S2.SS4.p2.4.m4.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.4.m4.1.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1">superscript</csymbol><cn id="S2.SS4.p2.4.m4.1.1.2.cmml" type="integer" xref="S2.SS4.p2.4.m4.1.1.2">10</cn><cn id="S2.SS4.p2.4.m4.1.1.3.cmml" type="integer" xref="S2.SS4.p2.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.4.m4.1c">10^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.4.m4.1d">10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. Upon completing the model training, we established an empirical threshold for selection at a correlation of 0.1. This threshold was empirically chosen during preliminary explorations and was used to generate a mask of the brain regions. This mask delineates areas showing higher responsiveness to musical stimuli.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Decoding Model</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Following the identification of brain regions responsive to music, our next objective was to construct a common model that could map the brain activity from these regions to the latent representations of musical features. This model aims to facilitate a translation process where the neural responses could potentially be directly mapped into musical features, by creating a predictive model where the brain’s response could serve as a proxy for the music itself, also illustrating a direct link between neural activity and musical perception. To this end, we trained a Ridge regression with hyperparameter optimization between the aligned brain activity of all subjects in "music-responsive" brain regions. Successively, we then focused on optimizing the retrieval process within the testing dataset. For each predicted musical feature, we selected the top-k closest elements based on the lowest L2 (Euclidean) distance between predicted and true musical features in CLAP space. This approach forms the basis of a straightforward retrieval pipeline, where the model searches for and retrieves the most similar musical stimuli from the latent space, based on the neural activity they elicited.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Evaluation</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.7">In our study, we measured the identification accuracy as described in the Brain2Music framework <cite class="ltx_cite ltx_citemacro_citep">(Denk et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite>. Identification accuracy quantifies how accurately the predicted <math alttext="d" class="ltx_Math" display="inline" id="S2.SS6.p1.1.m1.1"><semantics id="S2.SS6.p1.1.m1.1a"><mi id="S2.SS6.p1.1.m1.1.1" xref="S2.SS6.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.1.m1.1b"><ci id="S2.SS6.p1.1.m1.1.1.cmml" xref="S2.SS6.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.1.m1.1d">italic_d</annotation></semantics></math>-dimensional features correspond to the target features by computing the Pearson correlation coefficient between each pair of predicted and target features. In our case, the features are the estimated and true CLAP features (last layer, dimensionality 512). The accuracy for each prediction is the proportion of correct identifications, where a correct identification occurs if the correlation (computed as above) for a given prediction is higher than the one for any other prediction. In detail, the metric is calculated as follows: first, construct a correlation matrix between the predicted and true embeddings. Each element of this matrix, <math alttext="C_{i,j}" class="ltx_Math" display="inline" id="S2.SS6.p1.2.m2.2"><semantics id="S2.SS6.p1.2.m2.2a"><msub id="S2.SS6.p1.2.m2.2.3" xref="S2.SS6.p1.2.m2.2.3.cmml"><mi id="S2.SS6.p1.2.m2.2.3.2" xref="S2.SS6.p1.2.m2.2.3.2.cmml">C</mi><mrow id="S2.SS6.p1.2.m2.2.2.2.4" xref="S2.SS6.p1.2.m2.2.2.2.3.cmml"><mi id="S2.SS6.p1.2.m2.1.1.1.1" xref="S2.SS6.p1.2.m2.1.1.1.1.cmml">i</mi><mo id="S2.SS6.p1.2.m2.2.2.2.4.1" xref="S2.SS6.p1.2.m2.2.2.2.3.cmml">,</mo><mi id="S2.SS6.p1.2.m2.2.2.2.2" xref="S2.SS6.p1.2.m2.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.2.m2.2b"><apply id="S2.SS6.p1.2.m2.2.3.cmml" xref="S2.SS6.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S2.SS6.p1.2.m2.2.3.1.cmml" xref="S2.SS6.p1.2.m2.2.3">subscript</csymbol><ci id="S2.SS6.p1.2.m2.2.3.2.cmml" xref="S2.SS6.p1.2.m2.2.3.2">𝐶</ci><list id="S2.SS6.p1.2.m2.2.2.2.3.cmml" xref="S2.SS6.p1.2.m2.2.2.2.4"><ci id="S2.SS6.p1.2.m2.1.1.1.1.cmml" xref="S2.SS6.p1.2.m2.1.1.1.1">𝑖</ci><ci id="S2.SS6.p1.2.m2.2.2.2.2.cmml" xref="S2.SS6.p1.2.m2.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.2.m2.2c">C_{i,j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.2.m2.2d">italic_C start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, represents the Pearson correlation coefficient between the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS6.p1.3.m3.1"><semantics id="S2.SS6.p1.3.m3.1a"><mi id="S2.SS6.p1.3.m3.1.1" xref="S2.SS6.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.3.m3.1b"><ci id="S2.SS6.p1.3.m3.1.1.cmml" xref="S2.SS6.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.3.m3.1d">italic_i</annotation></semantics></math>-th predicted embedding and the <math alttext="j" class="ltx_Math" display="inline" id="S2.SS6.p1.4.m4.1"><semantics id="S2.SS6.p1.4.m4.1a"><mi id="S2.SS6.p1.4.m4.1.1" xref="S2.SS6.p1.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.4.m4.1b"><ci id="S2.SS6.p1.4.m4.1.1.cmml" xref="S2.SS6.p1.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.4.m4.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.4.m4.1d">italic_j</annotation></semantics></math>-th target embedding. For each predicted embedding, determine whether the correlation with its corresponding target (diagonal element <math alttext="C_{i,i}" class="ltx_Math" display="inline" id="S2.SS6.p1.5.m5.2"><semantics id="S2.SS6.p1.5.m5.2a"><msub id="S2.SS6.p1.5.m5.2.3" xref="S2.SS6.p1.5.m5.2.3.cmml"><mi id="S2.SS6.p1.5.m5.2.3.2" xref="S2.SS6.p1.5.m5.2.3.2.cmml">C</mi><mrow id="S2.SS6.p1.5.m5.2.2.2.4" xref="S2.SS6.p1.5.m5.2.2.2.3.cmml"><mi id="S2.SS6.p1.5.m5.1.1.1.1" xref="S2.SS6.p1.5.m5.1.1.1.1.cmml">i</mi><mo id="S2.SS6.p1.5.m5.2.2.2.4.1" xref="S2.SS6.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="S2.SS6.p1.5.m5.2.2.2.2" xref="S2.SS6.p1.5.m5.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.5.m5.2b"><apply id="S2.SS6.p1.5.m5.2.3.cmml" xref="S2.SS6.p1.5.m5.2.3"><csymbol cd="ambiguous" id="S2.SS6.p1.5.m5.2.3.1.cmml" xref="S2.SS6.p1.5.m5.2.3">subscript</csymbol><ci id="S2.SS6.p1.5.m5.2.3.2.cmml" xref="S2.SS6.p1.5.m5.2.3.2">𝐶</ci><list id="S2.SS6.p1.5.m5.2.2.2.3.cmml" xref="S2.SS6.p1.5.m5.2.2.2.4"><ci id="S2.SS6.p1.5.m5.1.1.1.1.cmml" xref="S2.SS6.p1.5.m5.1.1.1.1">𝑖</ci><ci id="S2.SS6.p1.5.m5.2.2.2.2.cmml" xref="S2.SS6.p1.5.m5.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.5.m5.2c">C_{i,i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.5.m5.2d">italic_C start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) is greater than the correlations with all other targets (non-diagonal elements <math alttext="C_{i,j}" class="ltx_Math" display="inline" id="S2.SS6.p1.6.m6.2"><semantics id="S2.SS6.p1.6.m6.2a"><msub id="S2.SS6.p1.6.m6.2.3" xref="S2.SS6.p1.6.m6.2.3.cmml"><mi id="S2.SS6.p1.6.m6.2.3.2" xref="S2.SS6.p1.6.m6.2.3.2.cmml">C</mi><mrow id="S2.SS6.p1.6.m6.2.2.2.4" xref="S2.SS6.p1.6.m6.2.2.2.3.cmml"><mi id="S2.SS6.p1.6.m6.1.1.1.1" xref="S2.SS6.p1.6.m6.1.1.1.1.cmml">i</mi><mo id="S2.SS6.p1.6.m6.2.2.2.4.1" xref="S2.SS6.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S2.SS6.p1.6.m6.2.2.2.2" xref="S2.SS6.p1.6.m6.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.6.m6.2b"><apply id="S2.SS6.p1.6.m6.2.3.cmml" xref="S2.SS6.p1.6.m6.2.3"><csymbol cd="ambiguous" id="S2.SS6.p1.6.m6.2.3.1.cmml" xref="S2.SS6.p1.6.m6.2.3">subscript</csymbol><ci id="S2.SS6.p1.6.m6.2.3.2.cmml" xref="S2.SS6.p1.6.m6.2.3.2">𝐶</ci><list id="S2.SS6.p1.6.m6.2.2.2.3.cmml" xref="S2.SS6.p1.6.m6.2.2.2.4"><ci id="S2.SS6.p1.6.m6.1.1.1.1.cmml" xref="S2.SS6.p1.6.m6.1.1.1.1">𝑖</ci><ci id="S2.SS6.p1.6.m6.2.2.2.2.cmml" xref="S2.SS6.p1.6.m6.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.6.m6.2c">C_{i,j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.6.m6.2d">italic_C start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext="j\neq i" class="ltx_Math" display="inline" id="S2.SS6.p1.7.m7.1"><semantics id="S2.SS6.p1.7.m7.1a"><mrow id="S2.SS6.p1.7.m7.1.1" xref="S2.SS6.p1.7.m7.1.1.cmml"><mi id="S2.SS6.p1.7.m7.1.1.2" xref="S2.SS6.p1.7.m7.1.1.2.cmml">j</mi><mo id="S2.SS6.p1.7.m7.1.1.1" xref="S2.SS6.p1.7.m7.1.1.1.cmml">≠</mo><mi id="S2.SS6.p1.7.m7.1.1.3" xref="S2.SS6.p1.7.m7.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.7.m7.1b"><apply id="S2.SS6.p1.7.m7.1.1.cmml" xref="S2.SS6.p1.7.m7.1.1"><neq id="S2.SS6.p1.7.m7.1.1.1.cmml" xref="S2.SS6.p1.7.m7.1.1.1"></neq><ci id="S2.SS6.p1.7.m7.1.1.2.cmml" xref="S2.SS6.p1.7.m7.1.1.2">𝑗</ci><ci id="S2.SS6.p1.7.m7.1.1.3.cmml" xref="S2.SS6.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.7.m7.1c">j\neq i</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.7.m7.1d">italic_j ≠ italic_i</annotation></semantics></math>). The identification accuracy for each prediction is then calculated using an indicator function:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{id\_acc}_{i}=\frac{1}{n-1}\sum_{j=1}^{n}1\left[C_{i,i}&gt;C_{i,j}\right]" class="ltx_Math" display="block" id="S2.Ex1.m1.5"><semantics id="S2.Ex1.m1.5a"><mrow id="S2.Ex1.m1.5.5" xref="S2.Ex1.m1.5.5.cmml"><msub id="S2.Ex1.m1.5.5.3" xref="S2.Ex1.m1.5.5.3.cmml"><mtext id="S2.Ex1.m1.5.5.3.2" xref="S2.Ex1.m1.5.5.3.2a.cmml">id_acc</mtext><mi id="S2.Ex1.m1.5.5.3.3" xref="S2.Ex1.m1.5.5.3.3.cmml">i</mi></msub><mo id="S2.Ex1.m1.5.5.2" xref="S2.Ex1.m1.5.5.2.cmml">=</mo><mrow id="S2.Ex1.m1.5.5.1" xref="S2.Ex1.m1.5.5.1.cmml"><mfrac id="S2.Ex1.m1.5.5.1.3" xref="S2.Ex1.m1.5.5.1.3.cmml"><mn id="S2.Ex1.m1.5.5.1.3.2" xref="S2.Ex1.m1.5.5.1.3.2.cmml">1</mn><mrow id="S2.Ex1.m1.5.5.1.3.3" xref="S2.Ex1.m1.5.5.1.3.3.cmml"><mi id="S2.Ex1.m1.5.5.1.3.3.2" xref="S2.Ex1.m1.5.5.1.3.3.2.cmml">n</mi><mo id="S2.Ex1.m1.5.5.1.3.3.1" xref="S2.Ex1.m1.5.5.1.3.3.1.cmml">−</mo><mn id="S2.Ex1.m1.5.5.1.3.3.3" xref="S2.Ex1.m1.5.5.1.3.3.3.cmml">1</mn></mrow></mfrac><mo id="S2.Ex1.m1.5.5.1.2" xref="S2.Ex1.m1.5.5.1.2.cmml">⁢</mo><mrow id="S2.Ex1.m1.5.5.1.1" xref="S2.Ex1.m1.5.5.1.1.cmml"><munderover id="S2.Ex1.m1.5.5.1.1.2" xref="S2.Ex1.m1.5.5.1.1.2.cmml"><mo id="S2.Ex1.m1.5.5.1.1.2.2.2" movablelimits="false" xref="S2.Ex1.m1.5.5.1.1.2.2.2.cmml">∑</mo><mrow id="S2.Ex1.m1.5.5.1.1.2.2.3" xref="S2.Ex1.m1.5.5.1.1.2.2.3.cmml"><mi id="S2.Ex1.m1.5.5.1.1.2.2.3.2" xref="S2.Ex1.m1.5.5.1.1.2.2.3.2.cmml">j</mi><mo id="S2.Ex1.m1.5.5.1.1.2.2.3.1" xref="S2.Ex1.m1.5.5.1.1.2.2.3.1.cmml">=</mo><mn id="S2.Ex1.m1.5.5.1.1.2.2.3.3" xref="S2.Ex1.m1.5.5.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.Ex1.m1.5.5.1.1.2.3" xref="S2.Ex1.m1.5.5.1.1.2.3.cmml">n</mi></munderover><mrow id="S2.Ex1.m1.5.5.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.cmml"><mn id="S2.Ex1.m1.5.5.1.1.1.3" xref="S2.Ex1.m1.5.5.1.1.1.3.cmml">1</mn><mo id="S2.Ex1.m1.5.5.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.2.cmml">⁢</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.2.cmml"><mo id="S2.Ex1.m1.5.5.1.1.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.1.2.1.cmml">[</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.cmml"><msub id="S2.Ex1.m1.5.5.1.1.1.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.2.2.cmml">C</mi><mrow id="S2.Ex1.m1.2.2.2.4" xref="S2.Ex1.m1.2.2.2.3.cmml"><mi id="S2.Ex1.m1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.cmml">i</mi><mo id="S2.Ex1.m1.2.2.2.4.1" xref="S2.Ex1.m1.2.2.2.3.cmml">,</mo><mi id="S2.Ex1.m1.2.2.2.2" xref="S2.Ex1.m1.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.cmml">&gt;</mo><msub id="S2.Ex1.m1.5.5.1.1.1.1.1.1.3" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.3.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.3.2.cmml">C</mi><mrow id="S2.Ex1.m1.4.4.2.4" xref="S2.Ex1.m1.4.4.2.3.cmml"><mi id="S2.Ex1.m1.3.3.1.1" xref="S2.Ex1.m1.3.3.1.1.cmml">i</mi><mo id="S2.Ex1.m1.4.4.2.4.1" xref="S2.Ex1.m1.4.4.2.3.cmml">,</mo><mi id="S2.Ex1.m1.4.4.2.2" xref="S2.Ex1.m1.4.4.2.2.cmml">j</mi></mrow></msub></mrow><mo id="S2.Ex1.m1.5.5.1.1.1.1.1.3" xref="S2.Ex1.m1.5.5.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.5b"><apply id="S2.Ex1.m1.5.5.cmml" xref="S2.Ex1.m1.5.5"><eq id="S2.Ex1.m1.5.5.2.cmml" xref="S2.Ex1.m1.5.5.2"></eq><apply id="S2.Ex1.m1.5.5.3.cmml" xref="S2.Ex1.m1.5.5.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.3.1.cmml" xref="S2.Ex1.m1.5.5.3">subscript</csymbol><ci id="S2.Ex1.m1.5.5.3.2a.cmml" xref="S2.Ex1.m1.5.5.3.2"><mtext id="S2.Ex1.m1.5.5.3.2.cmml" xref="S2.Ex1.m1.5.5.3.2">id_acc</mtext></ci><ci id="S2.Ex1.m1.5.5.3.3.cmml" xref="S2.Ex1.m1.5.5.3.3">𝑖</ci></apply><apply id="S2.Ex1.m1.5.5.1.cmml" xref="S2.Ex1.m1.5.5.1"><times id="S2.Ex1.m1.5.5.1.2.cmml" xref="S2.Ex1.m1.5.5.1.2"></times><apply id="S2.Ex1.m1.5.5.1.3.cmml" xref="S2.Ex1.m1.5.5.1.3"><divide id="S2.Ex1.m1.5.5.1.3.1.cmml" xref="S2.Ex1.m1.5.5.1.3"></divide><cn id="S2.Ex1.m1.5.5.1.3.2.cmml" type="integer" xref="S2.Ex1.m1.5.5.1.3.2">1</cn><apply id="S2.Ex1.m1.5.5.1.3.3.cmml" xref="S2.Ex1.m1.5.5.1.3.3"><minus id="S2.Ex1.m1.5.5.1.3.3.1.cmml" xref="S2.Ex1.m1.5.5.1.3.3.1"></minus><ci id="S2.Ex1.m1.5.5.1.3.3.2.cmml" xref="S2.Ex1.m1.5.5.1.3.3.2">𝑛</ci><cn id="S2.Ex1.m1.5.5.1.3.3.3.cmml" type="integer" xref="S2.Ex1.m1.5.5.1.3.3.3">1</cn></apply></apply><apply id="S2.Ex1.m1.5.5.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1"><apply id="S2.Ex1.m1.5.5.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2">superscript</csymbol><apply id="S2.Ex1.m1.5.5.1.1.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.2.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2">subscript</csymbol><sum id="S2.Ex1.m1.5.5.1.1.2.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.2"></sum><apply id="S2.Ex1.m1.5.5.1.1.2.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.3"><eq id="S2.Ex1.m1.5.5.1.1.2.2.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.3.1"></eq><ci id="S2.Ex1.m1.5.5.1.1.2.2.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.3.2">𝑗</ci><cn id="S2.Ex1.m1.5.5.1.1.2.2.3.3.cmml" type="integer" xref="S2.Ex1.m1.5.5.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.Ex1.m1.5.5.1.1.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.2.3">𝑛</ci></apply><apply id="S2.Ex1.m1.5.5.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1"><times id="S2.Ex1.m1.5.5.1.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.2"></times><cn id="S2.Ex1.m1.5.5.1.1.1.3.cmml" type="integer" xref="S2.Ex1.m1.5.5.1.1.1.3">1</cn><apply id="S2.Ex1.m1.5.5.1.1.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.5.5.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1"><gt id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1"></gt><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.2.2">𝐶</ci><list id="S2.Ex1.m1.2.2.2.3.cmml" xref="S2.Ex1.m1.2.2.2.4"><ci id="S2.Ex1.m1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1">𝑖</ci><ci id="S2.Ex1.m1.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.2.2">𝑖</ci></list></apply><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.3.2">𝐶</ci><list id="S2.Ex1.m1.4.4.2.3.cmml" xref="S2.Ex1.m1.4.4.2.4"><ci id="S2.Ex1.m1.3.3.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1">𝑖</ci><ci id="S2.Ex1.m1.4.4.2.2.cmml" xref="S2.Ex1.m1.4.4.2.2">𝑗</ci></list></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.5c">\text{id\_acc}_{i}=\frac{1}{n-1}\sum_{j=1}^{n}1\left[C_{i,i}&gt;C_{i,j}\right]</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.5d">id_acc start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_n - 1 end_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT 1 [ italic_C start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT &gt; italic_C start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS6.p1.9">where <math alttext="1[\cdot]" class="ltx_Math" display="inline" id="S2.SS6.p1.8.m1.1"><semantics id="S2.SS6.p1.8.m1.1a"><mrow id="S2.SS6.p1.8.m1.1.2" xref="S2.SS6.p1.8.m1.1.2.cmml"><mn id="S2.SS6.p1.8.m1.1.2.2" xref="S2.SS6.p1.8.m1.1.2.2.cmml">1</mn><mo id="S2.SS6.p1.8.m1.1.2.1" xref="S2.SS6.p1.8.m1.1.2.1.cmml">⁢</mo><mrow id="S2.SS6.p1.8.m1.1.2.3.2" xref="S2.SS6.p1.8.m1.1.2.3.1.cmml"><mo id="S2.SS6.p1.8.m1.1.2.3.2.1" stretchy="false" xref="S2.SS6.p1.8.m1.1.2.3.1.1.cmml">[</mo><mo id="S2.SS6.p1.8.m1.1.1" lspace="0em" rspace="0em" xref="S2.SS6.p1.8.m1.1.1.cmml">⋅</mo><mo id="S2.SS6.p1.8.m1.1.2.3.2.2" stretchy="false" xref="S2.SS6.p1.8.m1.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.8.m1.1b"><apply id="S2.SS6.p1.8.m1.1.2.cmml" xref="S2.SS6.p1.8.m1.1.2"><times id="S2.SS6.p1.8.m1.1.2.1.cmml" xref="S2.SS6.p1.8.m1.1.2.1"></times><cn id="S2.SS6.p1.8.m1.1.2.2.cmml" type="integer" xref="S2.SS6.p1.8.m1.1.2.2">1</cn><apply id="S2.SS6.p1.8.m1.1.2.3.1.cmml" xref="S2.SS6.p1.8.m1.1.2.3.2"><csymbol cd="latexml" id="S2.SS6.p1.8.m1.1.2.3.1.1.cmml" xref="S2.SS6.p1.8.m1.1.2.3.2.1">delimited-[]</csymbol><ci id="S2.SS6.p1.8.m1.1.1.cmml" xref="S2.SS6.p1.8.m1.1.1">⋅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.8.m1.1c">1[\cdot]</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.8.m1.1d">1 [ ⋅ ]</annotation></semantics></math> is the indicator function that returns 1 if the condition is true and 0 otherwise. The formula ensures that each comparison excludes the self-comparison (<math alttext="j=i" class="ltx_Math" display="inline" id="S2.SS6.p1.9.m2.1"><semantics id="S2.SS6.p1.9.m2.1a"><mrow id="S2.SS6.p1.9.m2.1.1" xref="S2.SS6.p1.9.m2.1.1.cmml"><mi id="S2.SS6.p1.9.m2.1.1.2" xref="S2.SS6.p1.9.m2.1.1.2.cmml">j</mi><mo id="S2.SS6.p1.9.m2.1.1.1" xref="S2.SS6.p1.9.m2.1.1.1.cmml">=</mo><mi id="S2.SS6.p1.9.m2.1.1.3" xref="S2.SS6.p1.9.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS6.p1.9.m2.1b"><apply id="S2.SS6.p1.9.m2.1.1.cmml" xref="S2.SS6.p1.9.m2.1.1"><eq id="S2.SS6.p1.9.m2.1.1.1.cmml" xref="S2.SS6.p1.9.m2.1.1.1"></eq><ci id="S2.SS6.p1.9.m2.1.1.2.cmml" xref="S2.SS6.p1.9.m2.1.1.2">𝑗</ci><ci id="S2.SS6.p1.9.m2.1.1.3.cmml" xref="S2.SS6.p1.9.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.p1.9.m2.1c">j=i</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.p1.9.m2.1d">italic_j = italic_i</annotation></semantics></math>). The overall identification accuracy is the average across all predictions:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{id\_acc}=\frac{1}{n}\sum_{i=1}^{n}\text{id\_acc}_{i}" class="ltx_Math" display="block" id="S2.Ex2.m1.1"><semantics id="S2.Ex2.m1.1a"><mrow id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml"><mtext id="S2.Ex2.m1.1.1.2" xref="S2.Ex2.m1.1.1.2a.cmml">id_acc</mtext><mo id="S2.Ex2.m1.1.1.1" xref="S2.Ex2.m1.1.1.1.cmml">=</mo><mrow id="S2.Ex2.m1.1.1.3" xref="S2.Ex2.m1.1.1.3.cmml"><mfrac id="S2.Ex2.m1.1.1.3.2" xref="S2.Ex2.m1.1.1.3.2.cmml"><mn id="S2.Ex2.m1.1.1.3.2.2" xref="S2.Ex2.m1.1.1.3.2.2.cmml">1</mn><mi id="S2.Ex2.m1.1.1.3.2.3" xref="S2.Ex2.m1.1.1.3.2.3.cmml">n</mi></mfrac><mo id="S2.Ex2.m1.1.1.3.1" xref="S2.Ex2.m1.1.1.3.1.cmml">⁢</mo><mrow id="S2.Ex2.m1.1.1.3.3" xref="S2.Ex2.m1.1.1.3.3.cmml"><munderover id="S2.Ex2.m1.1.1.3.3.1" xref="S2.Ex2.m1.1.1.3.3.1.cmml"><mo id="S2.Ex2.m1.1.1.3.3.1.2.2" movablelimits="false" xref="S2.Ex2.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S2.Ex2.m1.1.1.3.3.1.2.3" xref="S2.Ex2.m1.1.1.3.3.1.2.3.cmml"><mi id="S2.Ex2.m1.1.1.3.3.1.2.3.2" xref="S2.Ex2.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S2.Ex2.m1.1.1.3.3.1.2.3.1" xref="S2.Ex2.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S2.Ex2.m1.1.1.3.3.1.2.3.3" xref="S2.Ex2.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.Ex2.m1.1.1.3.3.1.3" xref="S2.Ex2.m1.1.1.3.3.1.3.cmml">n</mi></munderover><msub id="S2.Ex2.m1.1.1.3.3.2" xref="S2.Ex2.m1.1.1.3.3.2.cmml"><mtext id="S2.Ex2.m1.1.1.3.3.2.2" xref="S2.Ex2.m1.1.1.3.3.2.2a.cmml">id_acc</mtext><mi id="S2.Ex2.m1.1.1.3.3.2.3" xref="S2.Ex2.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.1b"><apply id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1"><eq id="S2.Ex2.m1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1"></eq><ci id="S2.Ex2.m1.1.1.2a.cmml" xref="S2.Ex2.m1.1.1.2"><mtext id="S2.Ex2.m1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.2">id_acc</mtext></ci><apply id="S2.Ex2.m1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.3"><times id="S2.Ex2.m1.1.1.3.1.cmml" xref="S2.Ex2.m1.1.1.3.1"></times><apply id="S2.Ex2.m1.1.1.3.2.cmml" xref="S2.Ex2.m1.1.1.3.2"><divide id="S2.Ex2.m1.1.1.3.2.1.cmml" xref="S2.Ex2.m1.1.1.3.2"></divide><cn id="S2.Ex2.m1.1.1.3.2.2.cmml" type="integer" xref="S2.Ex2.m1.1.1.3.2.2">1</cn><ci id="S2.Ex2.m1.1.1.3.2.3.cmml" xref="S2.Ex2.m1.1.1.3.2.3">𝑛</ci></apply><apply id="S2.Ex2.m1.1.1.3.3.cmml" xref="S2.Ex2.m1.1.1.3.3"><apply id="S2.Ex2.m1.1.1.3.3.1.cmml" xref="S2.Ex2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.3.3.1.1.cmml" xref="S2.Ex2.m1.1.1.3.3.1">superscript</csymbol><apply id="S2.Ex2.m1.1.1.3.3.1.2.cmml" xref="S2.Ex2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.3.3.1.2.1.cmml" xref="S2.Ex2.m1.1.1.3.3.1">subscript</csymbol><sum id="S2.Ex2.m1.1.1.3.3.1.2.2.cmml" xref="S2.Ex2.m1.1.1.3.3.1.2.2"></sum><apply id="S2.Ex2.m1.1.1.3.3.1.2.3.cmml" xref="S2.Ex2.m1.1.1.3.3.1.2.3"><eq id="S2.Ex2.m1.1.1.3.3.1.2.3.1.cmml" xref="S2.Ex2.m1.1.1.3.3.1.2.3.1"></eq><ci id="S2.Ex2.m1.1.1.3.3.1.2.3.2.cmml" xref="S2.Ex2.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn id="S2.Ex2.m1.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S2.Ex2.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.Ex2.m1.1.1.3.3.1.3.cmml" xref="S2.Ex2.m1.1.1.3.3.1.3">𝑛</ci></apply><apply id="S2.Ex2.m1.1.1.3.3.2.cmml" xref="S2.Ex2.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.3.3.2.1.cmml" xref="S2.Ex2.m1.1.1.3.3.2">subscript</csymbol><ci id="S2.Ex2.m1.1.1.3.3.2.2a.cmml" xref="S2.Ex2.m1.1.1.3.3.2.2"><mtext id="S2.Ex2.m1.1.1.3.3.2.2.cmml" xref="S2.Ex2.m1.1.1.3.3.2.2">id_acc</mtext></ci><ci id="S2.Ex2.m1.1.1.3.3.2.3.cmml" xref="S2.Ex2.m1.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.1c">\text{id\_acc}=\frac{1}{n}\sum_{i=1}^{n}\text{id\_acc}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.1d">id_acc = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT id_acc start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS6.p2">
<p class="ltx_p" id="S2.SS6.p2.1">Identification accuracy is especially useful in scenarios where the data may lead to ambiguous interpretations, requiring robust model performance to correctly identify the underlying condition or stimulus. Following an intuitive explanation of identification accuracy provided in <cite class="ltx_cite ltx_citemacro_citep">(Denk et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite> adapted for our case: from a practical perspective, consider a model that achieves an identification accuracy of 90%. This implies that, on average, 10% of the predictions are incorrect, i.e. cases where another candidate (not the correct "target") corresponds to a higher correlation coefficient than the correct candidate. In a dataset containing 60 examples, this would mean that the correct music track, on average, is ranked sixth (10% of 60 equals 6) in terms of correlation, suggesting that five other music stimuli were mistakenly rated as more likely candidates as compared to the correct one.</p>
</div>
<div class="ltx_para" id="S2.SS6.p3">
<p class="ltx_p" id="S2.SS6.p3.1">For demonstration purposes, we provide qualitative examples of decoded music. These examples can be accessed at the provided URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mind2music.my.canva.site/decoding-music-from-brain-activity-exploring-the-neural-correlates-of-music-perception" title="">https://mind2music.my.canva.site/decoding-music-from-brain-activity-exploring-the-neural-correlates-of-music-perception</a>, where listeners can directly experience the output of our decoding process, offering an auditory validation of the model’s performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This study examined the effectiveness of various embedding models and functional alignment strategies in identifying and classifying musical genres based on brain activity data. The results highlight significant advancements in genre classification accuracy and provide insights into the spatial distribution of musically responsive brain regions.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Encoding Models and Delineation of brain areas responsive to music</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">By setting a threshold of 0.1 (see methods), the encoding models identified 833 voxels in total. This threshold was empirically determined to optimize the balance between sensitivity and specificity in our voxel selection procedure. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.F3" title="Figure 3 ‣ 3.1 Encoding Models and Delineation of brain areas responsive to music ‣ 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_tag">3</span></a> shows the distribution of the relevant voxels within anatomical brain space, which appear to co-localize within lateral and temporal regions.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="S3.F3.g1" src="extracted/5684084/brain_roi.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Regions of interest (ROIs) corresponding to musically responsive areas were identified by applying a threshold to the correlations between predicted and actual brain activity. This process was part of a cross-validation procedure used in the encoding models.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Identification Accuracy</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.T1" title="Table 1 ‣ 3.2 Identification Accuracy ‣ 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_tag">1</span></a>, our proposed methods with functional alignment techniques, denoted <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">linear</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">hyperalign</span>, demonstrated superior performance with identification accuracies of 0.9012 ± 0.01573 and 0.8805 ± 0.0231, respectively, outperforming other baselines and the anatomical alignment method. The linear alignment method, in particular, shows the highest performance, underscoring the efficacy of our linear modelling approach to achieve cross-subject music decoding from brain activity. This is in accordance with our previous observation in vision decoding <cite class="ltx_cite ltx_citemacro_citep">(Ferrante et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib14" title="">2023b</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of Test Identification Accuracy</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.7.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.7.8.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.7.8.1.1.1">Embedding</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.7.8.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.7.8.1.2.1">Test Identification Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.2">SoundStream-avg</th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.1.1.1"><math alttext="0.674\pm 0.016" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mn id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">0.674</mn><mo id="S3.T1.1.1.1.m1.1.1.1" xref="S3.T1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml">0.016</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.T1.1.1.1.m1.1.1.2.cmml" type="float" xref="S3.T1.1.1.1.m1.1.1.2">0.674</cn><cn id="S3.T1.1.1.1.m1.1.1.3.cmml" type="float" xref="S3.T1.1.1.1.m1.1.1.3">0.016</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">0.674\pm 0.016</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">0.674 ± 0.016</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.2.2">w2v-BERT-avg</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.2.1"><math alttext="0.837\pm 0.005" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1.1"><semantics id="S3.T1.2.2.1.m1.1a"><mrow id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml"><mn id="S3.T1.2.2.1.m1.1.1.2" xref="S3.T1.2.2.1.m1.1.1.2.cmml">0.837</mn><mo id="S3.T1.2.2.1.m1.1.1.1" xref="S3.T1.2.2.1.m1.1.1.1.cmml">±</mo><mn id="S3.T1.2.2.1.m1.1.1.3" xref="S3.T1.2.2.1.m1.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><apply id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.2.2.1.m1.1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.T1.2.2.1.m1.1.1.2.cmml" type="float" xref="S3.T1.2.2.1.m1.1.1.2">0.837</cn><cn id="S3.T1.2.2.1.m1.1.1.3.cmml" type="float" xref="S3.T1.2.2.1.m1.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">0.837\pm 0.005</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.m1.1d">0.837 ± 0.005</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.3.3.2">MuLan<sub class="ltx_sub" id="S3.T1.3.3.2.1">text</sub>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.3.1"><math alttext="0.817\pm 0.014" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.1"><semantics id="S3.T1.3.3.1.m1.1a"><mrow id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml"><mn id="S3.T1.3.3.1.m1.1.1.2" xref="S3.T1.3.3.1.m1.1.1.2.cmml">0.817</mn><mo id="S3.T1.3.3.1.m1.1.1.1" xref="S3.T1.3.3.1.m1.1.1.1.cmml">±</mo><mn id="S3.T1.3.3.1.m1.1.1.3" xref="S3.T1.3.3.1.m1.1.1.3.cmml">0.014</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><apply id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.T1.3.3.1.m1.1.1.2.cmml" type="float" xref="S3.T1.3.3.1.m1.1.1.2">0.817</cn><cn id="S3.T1.3.3.1.m1.1.1.3.cmml" type="float" xref="S3.T1.3.3.1.m1.1.1.3">0.014</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">0.817\pm 0.014</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.1d">0.817 ± 0.014</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.4.2">MuLan<sub class="ltx_sub" id="S3.T1.4.4.2.1">music</sub>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.4.4.1"><math alttext="0.876\pm 0.015" class="ltx_Math" display="inline" id="S3.T1.4.4.1.m1.1"><semantics id="S3.T1.4.4.1.m1.1a"><mrow id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml"><mn id="S3.T1.4.4.1.m1.1.1.2" xref="S3.T1.4.4.1.m1.1.1.2.cmml">0.876</mn><mo id="S3.T1.4.4.1.m1.1.1.1" xref="S3.T1.4.4.1.m1.1.1.1.cmml">±</mo><mn id="S3.T1.4.4.1.m1.1.1.3" xref="S3.T1.4.4.1.m1.1.1.3.cmml">0.015</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><apply id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.T1.4.4.1.m1.1.1.2.cmml" type="float" xref="S3.T1.4.4.1.m1.1.1.2">0.876</cn><cn id="S3.T1.4.4.1.m1.1.1.3.cmml" type="float" xref="S3.T1.4.4.1.m1.1.1.3">0.015</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">0.876\pm 0.015</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.1.m1.1d">0.876 ± 0.015</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.5.5.2">Ours - anatomical</th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.5.5.1"><math alttext="0.7746\pm 0.01551" class="ltx_Math" display="inline" id="S3.T1.5.5.1.m1.1"><semantics id="S3.T1.5.5.1.m1.1a"><mrow id="S3.T1.5.5.1.m1.1.1" xref="S3.T1.5.5.1.m1.1.1.cmml"><mn id="S3.T1.5.5.1.m1.1.1.2" xref="S3.T1.5.5.1.m1.1.1.2.cmml">0.7746</mn><mo id="S3.T1.5.5.1.m1.1.1.1" xref="S3.T1.5.5.1.m1.1.1.1.cmml">±</mo><mn id="S3.T1.5.5.1.m1.1.1.3" xref="S3.T1.5.5.1.m1.1.1.3.cmml">0.01551</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.1b"><apply id="S3.T1.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.T1.5.5.1.m1.1.1.2.cmml" type="float" xref="S3.T1.5.5.1.m1.1.1.2">0.7746</cn><cn id="S3.T1.5.5.1.m1.1.1.3.cmml" type="float" xref="S3.T1.5.5.1.m1.1.1.3">0.01551</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.1c">0.7746\pm 0.01551</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.1.m1.1d">0.7746 ± 0.01551</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.2"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.2.1">Ours - hyperalign</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.1"><math alttext="\mathbf{0.8805\pm 0.0231}" class="ltx_Math" display="inline" id="S3.T1.6.6.1.m1.1"><semantics id="S3.T1.6.6.1.m1.1a"><mrow id="S3.T1.6.6.1.m1.1.1" xref="S3.T1.6.6.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S3.T1.6.6.1.m1.1.1.2" mathvariant="bold" xref="S3.T1.6.6.1.m1.1.1.2.cmml">0.8805</mn><mo id="S3.T1.6.6.1.m1.1.1.1" xref="S3.T1.6.6.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" id="S3.T1.6.6.1.m1.1.1.3" mathvariant="bold" xref="S3.T1.6.6.1.m1.1.1.3.cmml">0.0231</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.1.m1.1b"><apply id="S3.T1.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.6.6.1.m1.1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.T1.6.6.1.m1.1.1.2.cmml" type="float" xref="S3.T1.6.6.1.m1.1.1.2">0.8805</cn><cn id="S3.T1.6.6.1.m1.1.1.3.cmml" type="float" xref="S3.T1.6.6.1.m1.1.1.3">0.0231</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.1.m1.1c">\mathbf{0.8805\pm 0.0231}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.1.m1.1d">bold_0.8805 ± bold_0.0231</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.7.7.2"><span class="ltx_text ltx_font_bold" id="S3.T1.7.7.2.1">Ours - linear</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.7.7.1"><math alttext="\mathbf{0.9012\pm 0.01573}" class="ltx_Math" display="inline" id="S3.T1.7.7.1.m1.1"><semantics id="S3.T1.7.7.1.m1.1a"><mrow id="S3.T1.7.7.1.m1.1.1" xref="S3.T1.7.7.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S3.T1.7.7.1.m1.1.1.2" mathvariant="bold" xref="S3.T1.7.7.1.m1.1.1.2.cmml">0.9012</mn><mo id="S3.T1.7.7.1.m1.1.1.1" xref="S3.T1.7.7.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" id="S3.T1.7.7.1.m1.1.1.3" mathvariant="bold" xref="S3.T1.7.7.1.m1.1.1.3.cmml">0.01573</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.1.m1.1b"><apply id="S3.T1.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.T1.7.7.1.m1.1.1.2.cmml" type="float" xref="S3.T1.7.7.1.m1.1.1.2">0.9012</cn><cn id="S3.T1.7.7.1.m1.1.1.3.cmml" type="float" xref="S3.T1.7.7.1.m1.1.1.3">0.01573</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.1.m1.1c">\mathbf{0.9012\pm 0.01573}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.1.m1.1d">bold_0.9012 ± bold_0.01573</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Genre Decoding</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="384" id="S3.F4.g1" src="extracted/5684084/confusion_matrix.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Confusion matrix showing our model’s accuracy (number of correct predictions over the number of total predictions) in classifying musical genres based on fMRI data from five participants. Diagonal elements represent correct predictions for each genre, while off-diagonal elements indicate misclassifications. Each genre has 30 music stimuli, evenly distributed across the subjects; a value of 30 in the main diagonal therefore represents 100% accuracy. The model performs well for classical, jazz, and pop genres, with minimal confusion, while disco and metal genres show higher misclassification rates, likely due to overlapping music features. The matrix highlights the effectiveness of the cross-subject decoding pipeline and areas for improvement.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The confusion matrix shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.F4" title="Figure 4 ‣ 3.3 Genre Decoding ‣ 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the model’s capability to classify musical genres based on brain activity, with a notable concentration of correct predictions along the diagonal. Classical and jazz genres showed high accuracy with minimal confusion, suggesting that they correspond to distinct neural representations. However, genres like metal and disco exhibited more confusion, potentially indicating less separability in the CLAP space. For example, the confusion between disco and metal may arise from similar rhythmic patterns or instrumentation that blur genre-specific boundaries in neural encoding. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.F5" title="Figure 5 ‣ 3.3 Genre Decoding ‣ 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_tag">5</span></a> shows the similarity between the retrieved music and the original genre stimulus, using time-frequency as visual aids. Within the retrieved cluster, the exact stimulus is found very often, emphasizing the effectiveness of the pipeline. Given feature overlap, it is common to encounter different genres in the retrieved group of music stimuli compared to the stimulus, although always within genres that exhibit shared acoustic patterns.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="619" id="S3.F5.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Time-frequency Decompositions (TFDs - used as illustrative visual aids to estimate similarity between audio data) of original musical stimuli (jazz and metal) and the stimuli decoded from the top-3 CLAP embeddings predicted using the Ridge regression decoding model. The left side displays the TFD of the original jazz stimulus, while the right side shows the TFDs of the original metal stimulus. Below each original stimulus, the top-3 predicted stimuli are shown. For the jazz stimulus, the predicted simuli were all identified as jazz. For the metal stimulus, the top-3 predictions included two metal and one rock embedding. This comparison highlights the model’s ability to accurately predict musical genres from brain activity, while also illustrating occasional genre misclassification, particularly in more complex or overlapping genre spaces.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Impact of Functional Alignment techniques</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The choice of functional alignment techniques significantly enhanced the identification accuracy compared to baselines that did not make use of alignment. This improvement indicates that aligning functional brain data across subjects, while preserving individual differences in brain anatomy, allows for more accurate generalizations when decoding music genres from brain activity when compared to single-subject modelling. The technique effectively harnesses shared information across different subjects, thereby boosting the overall model’s performance <cite class="ltx_cite ltx_citemacro_citep">(Denk et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Compared to existing studies, such as those using basic MuLan or SoundStream embeddings <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib17" title="">2022</a>; Denk et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib10" title="">2023</a>)</cite>, our method provides high performances in music track retrieval and genre classification accuracy. Previous studies often did not account for individual variations in brain anatomy and function as effectively, which our hyperalignment and linear methods address directly.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">The results from this study not only reinforce the utility of advanced machine learning techniques in neuroscience but also pave the way for more personalized and accurate interpretations of brain activity in response to complex stimuli like music. Future work could explore deeper neural network architectures or alternative machine learning models that might further refine the accuracy of musical genre classification from brain imaging data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Decoding in Time</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">In our main experiment, we averaged the 15 seconds of fMRI data for each musical stimulus. Another possible interesting research question is when, after the stimulus onset, a peak in performance for music decoding can be observed. To address this question, we evaluated the neural responses contained in each fMRI volume. This analysis relies on identical procedures as described above; however, instead of using averaged brain activity over 15s as input for the decoding model, instantaneous (i.s. sample-wise) brain activity is used, resulting in a decoding-in-time representation (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#S3.F6" title="Figure 6 ‣ 3.5 Decoding in Time ‣ 3 Results ‣ R&amp;B - Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity"><span class="ltx_text ltx_ref_tag">6</span></a>). By identifying the samples/time delays at which the identification accuracy is highest, this approach illustrates the specific temporal dynamics underlying music perception within the brain.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S3.F6.g1" src="x4.png" width="639"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Identification accuracy of the music decoding model over a time course of 18 seconds, skipping the first 4.5 s to account for HRF delay. The y-axis represents the identification accuracy, while the x-axis represents the time in seconds, where 0s indicates stimulus onset. There is an evident trend of increasing identification accuracy as time progresses, reaching a peak towards the later part of the time window. This indicates that the model’s ability to accurately decode musical genres from brain activity improves with longer exposure to the musical stimuli, suggesting that prolonged neural engagement with the music enhances the decoding performance.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The findings of this study provide compelling evidence that decoding music from cross-subject neural activity is not only feasible, but also remarkably accurate when appropriate computational approaches and neural data alignment techniques are employed and adapted. This opens up numerous possibilities for understanding the cognitive processing of music and its applications, ranging from therapeutic practices to advanced brain-computer interfaces.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implications of Music Decoding</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The successful decoding of music genres from brain activity suggests profound implications for cognitive neuroscience and psychological studies. By associating specific genres with distinct patterns of brain activation, researchers can further explore how these patterns correlate with cognitive functions, emotional states, and individual preferences. This understanding could eventually lead to personalized music interventions designed to manage various psychological conditions such as anxiety, depression, and stress. Further refinement of this process could lead to neural-guided recommendation systems, allowing individuals to receive personalized music suggestions based on neural similarities with music stimuli they enjoy or those that evoke specific emotions.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance on Genre Decoding</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our analysis achieved results in line with <cite class="ltx_cite ltx_citemacro_citep">(Nakai et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib27" title="">2022</a>)</cite>, further showing that certain genres like classical and jazz are more distinctly encoded in the brain, possibly due to their unique structural and rhythmic complexities which might engage specific neural pathways. However, the confusion between closely related genres like rock and metal highlights the challenges of distinguishing between potentially similar auditory stimuli and suggests a need for more refined modelling techniques that can capture subtle nuances in music perception.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Identification of music-related brain regions</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Our results identified key brain regions involved in music perception and processing. Specifically, we identified the superior temporal gyrus (STG) <cite class="ltx_cite ltx_citemacro_citep">(Yoo et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib40" title="">2016</a>)</cite>, primary auditory cortex <cite class="ltx_cite ltx_citemacro_citep">(Warren, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib39" title="">2008</a>)</cite>, planum temporale <cite class="ltx_cite ltx_citemacro_citep">(Warren and Griffiths, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib38" title="">2003</a>)</cite>, and potentially the inferior parietal lobule <cite class="ltx_cite ltx_citemacro_citep">(Yoo et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib40" title="">2016</a>)</cite>. These areas are essential for decoding various aspects of auditory and musical stimuli, contributing to our ability to perceive and appreciate music. The superior temporal gyrus (STG), which includes the primary auditory cortex, is crucial for processing auditory information such as pitch, rhythm, and timbre. The primary auditory cortex, located within the STG, plays a fundamental role in detecting and discriminating sound frequencies, allowing us to discern different notes and rhythms in music <cite class="ltx_cite ltx_citemacro_cite">Warren (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib39" title="">2008</a>)</cite>. This region’s function is vital for understanding melodies and the basic structural components of music. Adjacent to the primary auditory cortex is the planum temporale, a region involved in higher-order auditory processing <cite class="ltx_cite ltx_citemacro_cite">Warren and Griffiths (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib38" title="">2003</a>)</cite>. The planum temporale is asymmetrically larger in the left hemisphere, a feature associated with language dominance, but it also plays a significant role in music processing <cite class="ltx_cite ltx_citemacro_cite">Warren and Griffiths (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib38" title="">2003</a>)</cite>. This area is crucial for discerning complex auditory patterns and structures, such as harmonies and musical sequences. The ability of the planum temporale to process these intricate auditory stimuli contributes to our cognitive understanding of music and its structural components. In addition to the STG and planum temporale, the inferior parietal lobule is implicated in the integration of sensory information from various modalities <cite class="ltx_cite ltx_citemacro_citep">(Pando-Naude et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib31" title="">2021</a>)</cite>. This region contributes to spatial awareness of sounds, which is important for perceiving the spatial dynamics of music, such as the localization of instruments within a stereo field. The inferior parietal lobule also plays a role in attention and the processing of rhythmic elements, enhancing our ability to perceive musical tempo and timing <cite class="ltx_cite ltx_citemacro_cite">Pando-Naude et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib31" title="">2021</a>)</cite>. This integrative function is essential for experiencing music as a coherent and dynamic auditory event. Together, these regions form a network that facilitates different aspects of music perception. The superior temporal gyrus and primary auditory cortex are central to decoding the basic auditory properties of music <cite class="ltx_cite ltx_citemacro_cite">Yoo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib40" title="">2016</a>); Warren (<a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib39" title="">2008</a>)</cite>, while the planum temporale supports higher-order processing and pattern recognition. The inferior parietal lobule’s involvement in sensory integration and attention further enriches our ability to experience and appreciate the spatial and temporal dimensions of music. These interconnected brain regions work in concert to provide a comprehensive and nuanced understanding of music, enabling listeners to engage with its emotional and aesthetic qualities fully.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Impact on Musical Therapy</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">There are potential applications of this research in the field of musical therapy that could be significant. Making a step towards a better understanding of the neural underpinnings of how music influences emotion and cognition can aid in developing more effective therapeutic protocols. As highlighted in <cite class="ltx_cite ltx_citemacro_citep">(Raglio et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib32" title="">2016</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib33" title="">2019</a>)</cite>, music therapy has been shown to have beneficial effects on various patient outcomes. While still in early stages, genre-specific neural decoding could tailor these therapies to individual needs, enhancing their effectiveness. Music therapy has been utilized in various clinical settings, demonstrating positive outcomes in patients with conditions such as Alzheimer’s disease, stroke, and depression <cite class="ltx_cite ltx_citemacro_citep">(Kamioka et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib19" title="">2014</a>; de Witte et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib8" title="">2022</a>)</cite>. By decoding how different genres affect brain activity, therapists could potentially customize music interventions that align more closely with the neural and emotional states of individual patients. This personalized approach could maximize therapeutic benefits by targeting specific neural circuits involved in emotional regulation and cognitive function. Moreover, further research into the relationship between music and neural responses could contribute to the development of innovative treatment modalities. For instance, integrating neurofeedback mechanisms that respond to real-time neural data could enable dynamic adjustments in musical stimuli, optimizing therapeutic outcomes. This approach could be particularly effective in managing chronic pain, stress, and anxiety, where music’s role in altering brain states can be leveraged for long-term health benefits <cite class="ltx_cite ltx_citemacro_citep">(Koelsch, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib21" title="">2011</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib20" title="">2014</a>; Koelsch et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib22" title="">2006</a>)</cite>. Understanding the specific neural mechanisms involved in music perception and emotional processing also provides insights into broader applications in cognitive neuroscience. For example, exploring how music can enhance cognitive rehabilitation in post-stroke patients or improve social communication skills in individuals with autism spectrum disorder represents promising research avenues. The ability to decode and harness the power of music at a neural level opens up new possibilities for both clinical practice and scientific inquiry into the profound effects of music on the human brain <cite class="ltx_cite ltx_citemacro_citep">(Nakai et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib26" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib27" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Deeper Investigation of Music and Emotions</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Further research could benefit from exploring the intricate connections between music and emotions, a relationship well-documented in the studies by <cite class="ltx_cite ltx_citemacro_citep">(Koelsch et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib22" title="">2006</a>; Koelsch, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib21" title="">2011</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.15537v1#bib.bib20" title="">2014</a>)</cite>. By decoding the emotional content of music from brain activity, researchers could gain insights into the emotional processing in the brain, providing a clearer picture of the emotional impacts of music at a neurological level. Envisioning a significant advancement for the future, we could consider this type of research as the foundation for a neural recommendation system. This system could potentially offer personalized music track suggestions based on our emotional and neural states or even suggest music stimuli that could guide us toward new emotional experiences.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Extension to Generative Music</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">Looking forward, the decoding techniques used in this study could be extended to generative music systems, potentially leading to innovative applications in creating music from brain activity, including musical imagery.</p>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1">At the time of writing, the primary reason we are focusing on retrieval rather than generation is the low temporal resolution of fMRI acquisition. This limitation constrains the possibility of generating music online based on neural dynamics, which however might be achievable with other neural activity measures like iEEG or MEG. A particularly intriguing prospect is to replace the retrieval module with a generative stage, especially by combining music decoding with imagery. Imagine an artist entering the scanner and envisioning a music track to be decoded through this process. The resulting piece could be seen as a collaborative creation between the artist’s imagination and artificial intelligence, potentially giving rise to a new art form where learned musical priors are transformed and used by neural decoding models to produce unique artistic expressions. Such systems would not only deepen our understanding of the creative processes that underpin music generation but also open the door to innovative forms of artistic expression that are directly influenced by neural dynamics.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Limitations</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">Despite these advancements, several limitations remain. The neural signals used in this study are inherently noisy and are only a subsampled representation of brain activity, which limits the detail and accuracy of the music that can be reconstructed. Rhythmic elements, particularly those at fine temporal resolutions, remain challenging to decode accurately due to the limitations in the temporal resolution of fMRI technology. Moreover, the extensive scanning time required for collecting sufficient data is a practical limitation that could restrict the use of these techniques in everyday applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Future Work</h3>
<div class="ltx_para" id="S4.SS8.p1">
<p class="ltx_p" id="S4.SS8.p1.1">Future research could explore the use of alternative neuroimaging methods, such as electroencephalography (EEG) or intracranial EEG (iEEG), which offer higher temporal resolution and could potentially provide more detailed insights into the neural encoding of music. Additionally, the development of more sophisticated generative models that can better handle the complexity and variability of neural data represents a promising direction for both academic research and practical applications in neuromusicology.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This study demonstrates high identification accuracy in decoding music from cross-subject neural activity using a streamlined retrieval pipeline, setting a new benchmark in neuromusicology with significant implications for therapeutic and personalized music applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abraham et al. [2014]</span>
<span class="ltx_bibblock">
Alexandre Abraham, Fabian Pedregosa, Michael Eickenberg, Philippe Gervais, Andreas Mueller, Jean Kossaifi, Alexandre Gramfort, Bertrand Thirion, and Gael Varoquaux.

</span>
<span class="ltx_bibblock">Machine learning for neuroimaging with scikit-learn.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Frontiers in Neuroinformatics</em>, 8, 2014.

</span>
<span class="ltx_bibblock">ISSN 1662-5196.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3389/fninf.2014.00014</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.frontiersin.org/articles/10.3389/fninf.2014.00014" title="">https://www.frontiersin.org/articles/10.3389/fninf.2014.00014</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agostinelli et al. [2023]</span>
<span class="ltx_bibblock">
Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank.

</span>
<span class="ltx_bibblock">Musiclm: Generating music from text, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antonello et al. [2023]</span>
<span class="ltx_bibblock">
Richard Antonello, Aditya Vaidya, and Alexander G. Huth.

</span>
<span class="ltx_bibblock">Scaling laws for language encoding models in fmri, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellier et al. [2023]</span>
<span class="ltx_bibblock">
L. Bellier, A. Llorens, D. Marciano, A. Gunduz, G. Schalk, P. Brunner, et al.

</span>
<span class="ltx_bibblock">Music can be reconstructed from human auditory cortex activity using nonlinear decoding models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">PLoS Biology</em>, 21(8):e3002176, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1371/journal.pbio.3002176</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1371/journal.pbio.3002176" title="">https://doi.org/10.1371/journal.pbio.3002176</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benchetrit et al. [2023]</span>
<span class="ltx_bibblock">
Yohann Benchetrit, Hubert Banville, and Jean-Rémi King.

</span>
<span class="ltx_bibblock">Brain decoding: toward real-time reconstruction of visual perception, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022]</span>
<span class="ltx_bibblock">
Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou.

</span>
<span class="ltx_bibblock">Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
Zijiao Chen, Jiaxin Qing, and Juan Helen Zhou.

</span>
<span class="ltx_bibblock">Cinematic mindscapes: High-quality video reconstruction from brain activity, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Witte et al. [2022]</span>
<span class="ltx_bibblock">
Martina de Witte, Ana da Silva Pinho, Geert-Jan Stams, Xavier Moonen, Arjan E R Bos, and Susan van Hooren.

</span>
<span class="ltx_bibblock">Music therapy for stress reduction: a systematic review and meta-analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Health Psychol. Rev.</em>, 16(1):134–159, March 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Défossez et al. [2023]</span>
<span class="ltx_bibblock">
A. Défossez, C. Caucheteux, J. Rapin, et al.

</span>
<span class="ltx_bibblock">Decoding speech perception from non-invasive brain recordings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Nature Machine Intelligence</em>, 5:1097–1107, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s42256-023-00714-5</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denk et al. [2023]</span>
<span class="ltx_bibblock">
Timo I. Denk, Yu Takagi, Takuya Matsuyama, Andrea Agostinelli, Tomoya Nakai, Christian Frank, and Shinji Nishimoto.

</span>
<span class="ltx_bibblock">Brain2music: Reconstructing music from human brain activity, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Défossez et al. [2023]</span>
<span class="ltx_bibblock">
A. Défossez, C. Caucheteux, J. Rapin, et al.

</span>
<span class="ltx_bibblock">Decoding speech perception from non-invasive brain recordings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Nature Machine Intelligence</em>, 5:1097–1107, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s42256-023-00714-5</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s42256-023-00714-5" title="">https://doi.org/10.1038/s42256-023-00714-5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elizalde et al. [2022]</span>
<span class="ltx_bibblock">
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.

</span>
<span class="ltx_bibblock">Clap: Learning audio concepts from natural language supervision, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrante et al. [2023a]</span>
<span class="ltx_bibblock">
Matteo Ferrante, Tommaso Boccato, Furkan Ozcelik, Rufin VanRullen, and Nicola Toschi.

</span>
<span class="ltx_bibblock">Multimodal decoding of human brain activity into images and text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">UniReps: the First Workshop on Unifying Representations in Neural Models</em>, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rGCabZfV3d" title="">https://openreview.net/forum?id=rGCabZfV3d</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrante et al. [2023b]</span>
<span class="ltx_bibblock">
Matteo Ferrante, Tommaso Boccato, and Nicola Toschi.

</span>
<span class="ltx_bibblock">Through their eyes: multi-subject brain decoding with simple alignment techniques, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrante et al. [2023c]</span>
<span class="ltx_bibblock">
Matteo Ferrante, Tommaso Boccato, and Nicola Toschi.

</span>
<span class="ltx_bibblock">Semantic brain decoding: from fmri to conceptually similar image reconstruction of visual stimuli, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haxby et al. [2011]</span>
<span class="ltx_bibblock">
James V Haxby, J Swaroop Guntupalli, Andrew C Connolly, Yaroslav O Halchenko, Bryan R Conroy, M Ida Gobbini, Michael Hanke, and Peter J Ramadge.

</span>
<span class="ltx_bibblock">A common, high-dimensional model of the representational space in human ventral temporal cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Neuron</em>, 72(2):404–416, October 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2022]</span>
<span class="ltx_bibblock">
Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis.

</span>
<span class="ltx_bibblock">Mulan: A joint embedding of music audio and natural language, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jenkinson et al. [2012]</span>
<span class="ltx_bibblock">
M. Jenkinson, C. F. Beckmann, T. E. J. Behrens, M. W. Woolrich, and S. M. Smith.

</span>
<span class="ltx_bibblock">Fsl.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">NeuroImage</em>, 62(2):782–790, 2012.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.neuroimage.2011.09.015</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neuroimage.2011.09.015" title="">https://doi.org/10.1016/j.neuroimage.2011.09.015</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamioka et al. [2014]</span>
<span class="ltx_bibblock">
Hiroharu Kamioka, Kiichiro Tsutani, Minoru Yamada, Hyuntae Park, Hiroyasu Okuizumi, Koki Tsuruoka, Takuya Honda, Shinpei Okada, Sang-Jun Park, Jun Kitayuguchi, Takafumi Abe, Shuichi Handa, Takuya Oshio, and Yoshiteru Mutoh.

</span>
<span class="ltx_bibblock">Effectiveness of music therapy: a summary of systematic reviews based on randomized controlled trials of music interventions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Patient Prefer. Adherence</em>, 8:727–754, May 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koelsch [2014]</span>
<span class="ltx_bibblock">
S. Koelsch.

</span>
<span class="ltx_bibblock">Brain correlates of music-evoked emotions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Nature Reviews Neuroscience</em>, 15:170–180, 2014.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/nrn3666</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/nrn3666" title="">https://doi.org/10.1038/nrn3666</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koelsch [2011]</span>
<span class="ltx_bibblock">
Stefan Koelsch.

</span>
<span class="ltx_bibblock">Toward a neural basis of music perception - a review and updated model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Front. Psychol.</em>, 2:110, June 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koelsch et al. [2006]</span>
<span class="ltx_bibblock">
Stefan Koelsch, Thomas Fritz, D. Yves V Cramon, Karsten Müller, and Angela D. Friederici.

</span>
<span class="ltx_bibblock">Investigating emotion with music: an fMRI study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Human Brain Mapping</em>, 27(3):239–250, March 2006.

</span>
<span class="ltx_bibblock">ISSN 1065-9471.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1002/hbm.20180</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted windows, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Margulis et al. [2019]</span>
<span class="ltx_bibblock">
Elizabeth Hellmuth Margulis, Patrick C. M. Wong, Rhimmon Simchy-Gross, and J. Devin McAuley.

</span>
<span class="ltx_bibblock">What the music said: narrative listening across cultures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Palgrave Communications</em>, 5(1):146, Nov 2019.

</span>
<span class="ltx_bibblock">ISSN 2055-1045.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1057/s41599-019-0363-1</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1057/s41599-019-0363-1" title="">https://doi.org/10.1057/s41599-019-0363-1</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakai et al. [2021]</span>
<span class="ltx_bibblock">
Tomoya Nakai, Naoko Koide-Majima, and Shinji Nishimoto.

</span>
<span class="ltx_bibblock">Correspondence of categorical and feature-based representations of music in the human brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Brain and Behavior</em>, 11(1):e01936, 2021.

</span>
<span class="ltx_bibblock">ISSN 2162-3279.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1002/brb3.1936</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/brb3.1936" title="">https://onlinelibrary.wiley.com/doi/abs/10.1002/brb3.1936</a>.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/brb3.1936.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakai et al. [2022]</span>
<span class="ltx_bibblock">
Tomoya Nakai, Naoko Koide-Majima, and Shinji Nishimoto.

</span>
<span class="ltx_bibblock">Music genre neuroimaging dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Data in Brief</em>, 40:107675, 2022.

</span>
<span class="ltx_bibblock">ISSN 2352-3409.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.dib.2021.107675</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S2352340921009501" title="">https://www.sciencedirect.com/science/article/pii/S2352340921009501</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakai et al. [2023]</span>
<span class="ltx_bibblock">
Tomoya Nakai, Naoko Koide-Majima, and Shinji Nishimoto.

</span>
<span class="ltx_bibblock">"music genre fmri dataset", 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. [2023]</span>
<span class="ltx_bibblock">
Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, and Xavier Hinaut.

</span>
<span class="ltx_bibblock">Deep neural networks and brain alignment: Brain encoding and decoding (survey), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozcelik and VanRullen [2023]</span>
<span class="ltx_bibblock">
Furkan Ozcelik and Rufin VanRullen.

</span>
<span class="ltx_bibblock">Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pando-Naude et al. [2021]</span>
<span class="ltx_bibblock">
V. Pando-Naude, A. Patyczek, L. Bonetti, et al.

</span>
<span class="ltx_bibblock">An ale meta-analytic review of top-down and bottom-up processing of music in the brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Scientific Reports</em>, 11:20813, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41598-021-00139-3</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raglio et al. [2016]</span>
<span class="ltx_bibblock">
Alfredo Raglio, Caterina Galandra, Luisella Sibilla, Fabrizio Esposito, Francesca Gaeta, Francesco Di Salle, Luca Moro, Irene Carne, Stefano Bastianello, Maurizia Baldi, and Marcello Imbriani.

</span>
<span class="ltx_bibblock">Effects of active music therapy on the normal brain: fMRI based evidence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Brain Imaging and Behavior</em>, 10(1):182–186, March 2016.

</span>
<span class="ltx_bibblock">ISSN 1931-7565.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/s11682-015-9380-x</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raglio et al. [2019]</span>
<span class="ltx_bibblock">
Alfredo Raglio, Enrico Oddone, Lara Morotti, Yasmin Khreiwesh, Chiara Zuddas, Jessica Brusinelli, Chiara Imbriani, and Marcello Imbriani.

</span>
<span class="ltx_bibblock">Music in the workplace: A narrative literature review of intervention studies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Journal of Complementary &amp; Integrative Medicine</em>, pages /j/jcim.ahead–of–print/jcim–2017–0046/jcim–2017–0046.xml, October 2019.

</span>
<span class="ltx_bibblock">ISSN 1553-3840.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1515/jcim-2017-0046</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scotti et al. [2023]</span>
<span class="ltx_bibblock">
Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, and Tanishq Mathew Abraham.

</span>
<span class="ltx_bibblock">Reconstructing the mind’s eye: fmri-to-image with contrastive learning and diffusion priors, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scotti et al. [2024]</span>
<span class="ltx_bibblock">
Paul S. Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A. Norman, and Tanishq Mathew Abraham.

</span>
<span class="ltx_bibblock">Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G. Huth.

</span>
<span class="ltx_bibblock">Semantic reconstruction of continuous language from non-invasive brain recordings.

</span>
<span class="ltx_bibblock">26(5):858–866.

</span>
<span class="ltx_bibblock">ISSN 1546-1726.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41593-023-01304-9</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41593-023-01304-9" title="">https://www.nature.com/articles/s41593-023-01304-9</a>.

</span>
<span class="ltx_bibblock">Number: 5 Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Maaten and Hinton [2008]</span>
<span class="ltx_bibblock">
Laurens van der Maaten and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Visualizing data using t-sne.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Journal of Machine Learning Research</em>, 9(86):2579–2605, 2008.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v9/vandermaaten08a.html" title="">http://jmlr.org/papers/v9/vandermaaten08a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warren and Griffiths [2003]</span>
<span class="ltx_bibblock">
J D Warren and T D Griffiths.

</span>
<span class="ltx_bibblock">Distinct mechanisms for processing spatial sequences and pitch sequences in the human auditory brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">J. Neurosci.</em>, 23(13):5799–5804, July 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warren [2008]</span>
<span class="ltx_bibblock">
Jason Warren.

</span>
<span class="ltx_bibblock">How does the brain process music?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Clin. Med.</em>, 8(1):32–36, February 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo et al. [2016]</span>
<span class="ltx_bibblock">
Hyun-Joon Yoo, Hyun Im Moon, and Sung-Bom Pyun.

</span>
<span class="ltx_bibblock">Amusia after right temporoparietal lobe infarction: A case report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Ann. Rehabil. Med.</em>, 40(5):933–937, October 2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun 21 17:05:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
