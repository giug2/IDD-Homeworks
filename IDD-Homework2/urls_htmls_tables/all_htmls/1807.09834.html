<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1807.09834] Applying Domain Randomization to Synthetic Data for Object Category Detection</title><meta property="og:description" content="Recent advances in deep learning‚Äìbased object detection techniques have revolutionized their applicability in several fields.
However, since these methods rely on unwieldy and large amounts of data, a common practice i‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Applying Domain Randomization to Synthetic Data for Object Category Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Applying Domain Randomization to Synthetic Data for Object Category Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1807.09834">

<!--Generated on Sat Mar  9 05:36:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<section id="id10" class="ltx_glossary ltx_acronym ltx_list_acronym">
<dl id="id10.10" class="ltx_glossarylist">
<dt id="id1.1.id1" class="ltx_glossaryentry">COCO</dt>
<dd>Common Objects in COntext</dd>
<dt id="id2.2.id2" class="ltx_glossaryentry">CoM</dt>
<dd>Center of Mass</dd>
<dt id="id3.3.id3" class="ltx_glossaryentry">CNN</dt>
<dd>Convolutional Neural Network</dd>
<dt id="id4.4.id4" class="ltx_glossaryentry">PCA</dt>
<dd>Principal Component Analysis</dd>
<dt id="id5.5.id5" class="ltx_glossaryentry">SDF</dt>
<dd>Simulation Description Files</dd>
<dt id="id6.6.id6" class="ltx_glossaryentry">SSD</dt>
<dd>Single‚ÄìShot Detector</dd>
<dt id="id7.7.id7" class="ltx_glossaryentry">YOLO</dt>
<dd>You Only Look Once</dd>
<dt id="id8.8.id8" class="ltx_glossaryentry">IoU</dt>
<dd>Intersection over Union</dd>
<dt id="id9.9.id9" class="ltx_glossaryentry">AP</dt>
<dd>Average Precision</dd>
<dt id="id10.10.id10" class="ltx_glossaryentry">mAP</dt>
<dd>Mean Average Precision</dd>
</dl>
</section><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
Instituto Superior T√©cnico
<br class="ltx_break"><span id="id1.1" class="ltx_text ltx_font_typewriter">{jborrego,adehban,ruifigueiredo,plinio,alex,jasv}
<br class="ltx_break">@isr.tecnico.ulisboa.pt</span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Applying <em id="id13.id1" class="ltx_emph ltx_font_italic">Domain Randomization</em> to Synthetic Data for Object Category Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jo√£o Borrego<sup id="id11.1.1" class="ltx_sup"><math id="id11.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="id11.1.1.m1.1a"><mo id="id11.1.1.m1.1.1" xref="id11.1.1.m1.1.1.cmml">‚ãÜ</mo><annotation-xml encoding="MathML-Content" id="id11.1.1.m1.1b"><ci id="id11.1.1.m1.1.1.cmml" xref="id11.1.1.m1.1.1">‚ãÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="id11.1.1.m1.1c">\star</annotation></semantics></math></sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Atabak Dehban<sup id="id12.1.1" class="ltx_sup"><math id="id12.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="id12.1.1.m1.1a"><mo id="id12.1.1.m1.1.1" xref="id12.1.1.m1.1.1.cmml">‚ãÜ</mo><annotation-xml encoding="MathML-Content" id="id12.1.1.m1.1b"><ci id="id12.1.1.m1.1.1.cmml" xref="id12.1.1.m1.1.1">‚ãÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="id12.1.1.m1.1c">\star</annotation></semantics></math></sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rui Figueiredo
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Plinio Moreno
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexandre Bernardino
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jos√© Santos-Victor
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">Recent advances in deep learning‚Äìbased object detection techniques have revolutionized their applicability in several fields.
However, since these methods rely on unwieldy and large amounts of data, a common practice is to download models pre-trained on standard datasets and fine-tune them for specific application domains with a small set of domain relevant images.
In this work, we show that using synthetic datasets that are not necessarily photo-realistic can be a better alternative to simply fine-tune pre-trained networks.
Specifically, our results show an impressive <span id="id14.id1.1" class="ltx_text ltx_font_bold">25% improvement in the <a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Mean Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">mAP</span></abbr></a> metric</span> over a fine-tuning baseline when only about 200 labelled images are available to train.
Finally, an ablation study of our results is presented to delineate the individual contribution of different components in the randomization pipeline.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">0</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">0</sup><span class="ltx_note_type">footnotetext: </span>Authors contributed equally to this manuscript.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the availability of advanced object detectors¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, these systems and their variations have found many applications ranging from face detection¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, to medical applications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and to robotics¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, training these systems from scratch is still a challenge as these methods rely on the availability of large, annotated, and high quality datasets.
One common approach to circumvent this issue is to re-use detectors that were pre-trained on large and available datasets such as  <a href="#id1.1.id1"><span href="#id1.1.id1" title="Common Objects in COntext" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Common Objects in COntext</span></span></a> (<a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Common Objects in COntext" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">COCO</span></abbr></a>)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and ImageNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and later, apply some form of domain adaptation technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for the particular task at hand using a smaller, domain specific dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
This approach results in varying degrees of success¬†(refer to¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for a study on how knowledge can be transferred across different tasks).
This line of research has been accelerated, thanks to the availability of high quality open source implementations of <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">state‚Äìof‚Äìthe‚Äìart</em> object detectors¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">At the heart of these domain adaptation techniques, lies the implicit assumption that there exists some sort of underlying data structure that can be transferred across different domains.
However, this premise does not hold in many applications, specially when the target domain does not significantly overlap with the outdoor images that make up a large portion of both ImageNet and <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Common Objects in COntext" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">COCO</span></abbr></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1807.09834/assets/figures/set346.jpg" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="269" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
An example image from the test set, annotated by the object detector.
Annotations are red for boxes, blue for cylinders and green for sphere.
</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To overcome these challenges, in this work, we are investigating the usage of <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">domain randomization</em>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to facilitate the adaptation of an object detector, namely  <a href="#id6.6.id6"><span href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Single‚ÄìShot Detector</span></span></a> (<a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a>)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, to detect three classes of objects: cylinders, spheres, and boxes.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This task is accomplished using an open source plugin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> developed for Gazebo simulator¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
This plugin was selected as it streamlines the generation and rendering of different objects as long as their mesh description is available.
In addition, adding parametric classes of objects using this plugin is quite straightforward.
Finally, Gazebo is the current de facto standard for robotics which covers several physics engines, families of robots, different type of actuators<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://en.wikipedia.org/wiki/Robotics_simulator" title="" class="ltx_ref ltx_font_typewriter">https://en.wikipedia.org/wiki/Robotics_simulator</a> as of </span></span></span>. We believe that roboticists will build upon these features and implement <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">domain randomization</em> experiments where multiple robots may interact with several objects while learning new skills.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">According to our experiments, domain randomization can substantially increase the accuracy of object detectors at least in situations where only a relatively small domain‚Äìspecific dataset of annotated images is available.
Even though not completely generalizable, the fact that the synthetic dataset does not necessarily need to be photo-realistic helps to significantly lower the barrier in applying this technique in different applications.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The main contributions of this paper can be summarized as following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We have shown substantial improvements in the accuracy of <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> compared to the case where it was simply fine-tuned on a small, domain‚Äìspecific dataset;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We conducted a comprehensive study in order to determine the contribution of individual components of the pipeline and discuss the importance of viewpoint variations, different types of textures and number of available images for training;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We have made significant contributions to an open source Gazebo plugin, which has resulted in <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">doubling</em> the speed of scene generation pipeline, by effectively removing redundant object load times. These modifications have greatly facilitated the study of domain randomization in object category detection.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The rest of this paper is organized as follows: in section¬†<a href="#S2" title="2 Related Work ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we examine the related work on different domain adaptation techniques relevant to object detection, that have been studied in the literature.
In section¬†<a href="#S3" title="3 Methods ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we explain the setup of the experiments, as well as our contributions to the Gazebo plugin which has made this work possible.
For the sake of completeness, a brief overview of <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> is also provided.
Section¬†<a href="#S4" title="4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> discusses the results of using domain randomization on object detection and the significance of different components in the domain randomization pipeline.
It also benchmarks the importance of our contributions to the Gazebo plugin for scene generation.
Finally, we draw our conclusions and discuss promising future research directions in section¬†<a href="#S5" title="5 Conclusions and future directions ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Recent advances on deep learning and parallel computing have boosted research and many breakthroughs in machine learning and computer vision.
Being capable of learning the underlying highly nonlinear structure of high dimensional data, they have achieved <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">state‚Äìof‚Äìthe‚Äìart</span> performance in image classification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, detection¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and segmentation tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
However, supervised training of deep neural networks relies on the availability of large datasets, hand-labeled in a laborious and time consuming manner.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In this section we overview the main concepts and related work on automated, computer driven data augmentation techniques for computer vision applications.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Reality Gap</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The discrepancy between the real world and simulated, computer generated environments is often referred to as the reality gap.
There are two common approaches to bridge this disparity: either reducing the gap by attempting to increase the resemblance between the two domains or explore methods that are trained in a more generic domain, representative of both simulation and reality domains simultaneously.
To achieve the former, one may increase the accuracy of the simulators in an attempt to obtain high-fidelity results¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>; or use Generative Adversarial Networks (GANs) to turn simulated images more photo-realistic¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Both methods require great effort in the creation of systems which model complex physical phenomena to attain realistic simulation.
Our work focuses mainly on the second approach.
Instead of diminishing the reality gap in order to use traditional machine-learning methods, we analyze methods that are aware of this disparity.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Augmentation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">An alternative approach to obtain large amounts of annotated training data is to enrich a small dataset with new labelled elements.
In ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, the authors generate synthetic composite images for training neural networks for object detection.
They propose methods in which 2D cropped object images are superimposed into a real-world RGB-D scene.
Moreover, their proposal integrates scene contextual information in the data generation process.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">This work demonstrates that the performance of state-of-the-art object detectors performed better when trained with both synthetic and real data than with real data alone.
The data generation method is tested with two publicly available datasets, GMU-Kitchens¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and Washington RGB-D Scenes V2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Domain Randomization</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Rather than attempting to perfectly emulate reality, we may create models that strive to achieve robustness to high variability in the environment.
Domain randomization is a simple yet powerful technique for generating training data for machine-learning algorithms.
The goal is to synthetically generate or enhance the data, in order to introduce random variances in the environment properties <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">that are not essential to the learning task</em>.
This idea dates back to at least 1997¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, with Jakobi‚Äôs observation that evolved controllers exploit the unrealistic details of flawed simulators.
His work on evolutionary robotics studies the hypothesis that controllers can evolve to become more robust by introducing random noise in all the aspects of simulation which do not have a basis in reality, and only slightly randomizing the remaining which do.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">It is expected that given enough variability in the simulation, the transition between simulated and real domains is perceived by the model as a mere disturbance, to which it has became robust.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Concurrent to our work,¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> reports the effect of overlaying real textures on the accuracy of state-of-the-art object detectors in a single-class outdoor car detection scenario. In contrast, we report the impact of overlaying synthetically generated patterns with different characteristics and increasing complexity on the accuracy metrics in a multiple-class indoor detection of parametric shape primitives scenario.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In order to apply an object detector in a new domain, it is necessary to collect some training samples from the domain at hand.
Labelling data for object detection is harder than labelling it for object classification, as bounding box coordinates are needed in addition to target object‚Äôs identity, which adds to the importance of optimally benefiting from the available data.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">After data collection, a detector is selected, commonly based on a trade-off between speed and accuracy, and is fine-tuned using the available ‚Äútarget domain‚Äù data.
Our proposal is to use a synthetic dataset, with algorithmic variations in irrelevant aspects of objects of interest, instead of relying on pre-trained networks on datasets which share little resemblance to the task at hand.
This approach is further detailed in this section.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><a href="#id6.6.id6"><span href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Single‚ÄìShot Detector</span></span></a>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In all of our experiments, <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> was used as the base detector as it is one of the few detectors that can be applied in real-time while showing a decent accuracy. However, we expect our results to directly generalize to other deep learning based detectors.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The inner workings of <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> is briefly described here, however, readers should refer to the original publication¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for a comprehensive study of the detector.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">At the root of all deep learning based object detectors, there exists a base  <a href="#id3.3.id3"><span href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Convolutional Neural Network</span></span></a> (<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a>) which is used as feature extractors for further down-stream tasks, <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">i.e.</em>¬†bounding box generation and foreground/background recognition.
Similar to <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="You Only Look Once" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">YOLO</span></abbr></a> architecture¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> takes advantage from the concept of <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">priors</em> or <em id="S3.SS1.p3.1.3" class="ltx_emph ltx_font_italic">default boxes</em><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Called <em id="footnote2.1" class="ltx_emph ltx_font_italic">anchor box</em> in <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="You Only Look Once" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">YOLO</span></abbr></a>.</span></span></span> where each cell identifies itself as including an object or not, and where this object exists, relative to a default location.
However, unlike <a href="#id7.7.id7"><abbr href="#id7.7.id7" title="You Only Look Once" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">YOLO</span></abbr></a>, <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> does this at different layers of the base <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a>.
Since neurons in different layers of <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">CNNs</span></abbr></a> have different receptive fields in terms of size and aspect ratios, effectively, objects of various shapes can be detected.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">During training, if a ground truth bounding box matches a default box, <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">i.e.</em>¬†they have an  <a href="#id8.8.id8"><span href="#id8.8.id8" title="Intersection over Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Intersection over Union</span></span></a> (<a href="#id8.8.id8"><abbr href="#id8.8.id8" title="Intersection over Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IoU</span></abbr></a>) of more than <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mn id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><cn type="float" id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">0.5</annotation></semantics></math>, the parameters of how to move this box to perfectly match the ground truth are learned by minimizing an smooth L1 metric.
Hard negative mining is used to create a more balanced dataset between foreground and background boxes.
Finally, Non-Maximum Suppression (NMS) is used to determine the final location of the objects.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Unlike the original <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> architecture, we used MobileNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> as the base <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a> for feature extraction in all experiments.
MobileNet changes the connections in a conventional <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a> to drastically reduce its number of parameters, without having a significant toll on performance, relative to a comparable architecture.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Contributions to Gazebo plugin</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our contribution to the open-source Gazebo plugin for domain randomization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> consists of an optimization in the scene composition, which almost doubled performance.
Originally, each scene required parametric objects to be generated from a  <a href="#id5.5.id5"><span href="#id5.5.id5" title="Simulation Description Files" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Simulation Description Files</span></span></a> (<a href="#id5.5.id5"><abbr href="#id5.5.id5" title="Simulation Description Files" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SDF</span></abbr></a>)<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="http://sdformat.org/" title="" class="ltx_ref ltx_font_typewriter">http://sdformat.org/</a> as of </span></span></span> formatted string, which was altered during run-time in order to allow for different object dimensions and visuals.
Furthermore, objects were created and destroyed in between scenes.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Instead, we first spawn the maximum number of objects on scene of each type.
Then, in each scene we alter their visual properties from within the Gazebo engine, by for instance changing their scale and pose,
which results in a substantial performance boost.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">In addition, we improved the existing auxiliary texture generation module in order to exploit parallelism in Perlin noise generation, using OpenMP<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://www.openmp.org/" title="" class="ltx_ref ltx_font_typewriter">https://www.openmp.org/</a> as of </span></span></span> framework.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experiment design and setup</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We have conducted various experiments and tests to quantify the results of different scenarios.
Initially, two sets of 30<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">k</annotation></semantics></math> synthetic images are generated.
The modifications we mentioned in the previous subsection have greatly facilitated this process.
These two sets differed from one another by the degree in which the virtual camera in the scene has changed its location.
In the first set, the viewpoint was fixed, whereas in the other set, its location varied largely across the scene.
More details will be provided in section¬†<a href="#S4" title="4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Four types of textures were used in the generation of synthetic images, which have been employed in recent research applying domain randomization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
Specifically, these include flat colors, gradients of colors, chess patterns, and Perlin noise¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which can be seen in Figure¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.3 Experiment design and setup ‚Ä£ 3 Methods ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1807.09834/assets/figures/textures_1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="329" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Example synthetic scene employing all 4 texture patterns.
Labelled by the plug-in.
The ground has a flat color, box has gradient, cylinder has chess and sphere has Perlin noise.
</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">In addition, we have collected 250 real images in the lab, out of which 49 contain objects unseen in training, for the sole purpose of reporting final performance¬†(Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
The train, validation and test partitions of our real image dataset is specified in Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3.3 Experiment design and setup ‚Ä£ 3 Methods ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of real images in train, validation and test partitions.</figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.4" class="ltx_tr">
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Training</th>
<th id="S3.T1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Validation</th>
<th id="S3.T1.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Test</th>
<th id="S3.T1.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.5.1" class="ltx_tr">
<td id="S3.T1.4.5.1.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">175</td>
<td id="S3.T1.4.5.1.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">26</td>
<td id="S3.T1.4.5.1.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">49</td>
<td id="S3.T1.4.5.1.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">250</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">In this dataset, there was no consideration to explicitly keep the percentage of different classes balanced¬†(Table¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3.3 Experiment design and setup ‚Ä£ 3 Methods ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), as such, we have also reported precision-recall curves for each class.
Finally, all our reported metrics are calculated with an <a href="#id8.8.id8"><abbr href="#id8.8.id8" title="Intersection over Union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IoU</span></abbr></a> of <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mn id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><cn type="float" id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">0.5</annotation></semantics></math>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Percentage of different classes in the real dataset.</figcaption>
<table id="S3.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.5.5" class="ltx_tr">
<th id="S3.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Partition</th>
<th id="S3.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"># Box</th>
<th id="S3.T2.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"># Cylinder</th>
<th id="S3.T2.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"># Sphere</th>
<th id="S3.T2.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.5.6.1" class="ltx_tr">
<td id="S3.T2.5.6.1.1" class="ltx_td ltx_align_left ltx_border_t">Train set</td>
<td id="S3.T2.5.6.1.2" class="ltx_td ltx_align_left ltx_border_t">502 (63%)</td>
<td id="S3.T2.5.6.1.3" class="ltx_td ltx_align_left ltx_border_t">209 (26%)</td>
<td id="S3.T2.5.6.1.4" class="ltx_td ltx_align_left ltx_border_t">86 (11%)</td>
<td id="S3.T2.5.6.1.5" class="ltx_td ltx_align_left ltx_border_t">797</td>
</tr>
<tr id="S3.T2.5.7.2" class="ltx_tr">
<td id="S3.T2.5.7.2.1" class="ltx_td ltx_align_left ltx_border_b">Test set</td>
<td id="S3.T2.5.7.2.2" class="ltx_td ltx_align_left ltx_border_b">106 (40%)</td>
<td id="S3.T2.5.7.2.3" class="ltx_td ltx_align_left ltx_border_b">104 (40%)</td>
<td id="S3.T2.5.7.2.4" class="ltx_td ltx_align_left ltx_border_b">53 (20%)</td>
<td id="S3.T2.5.7.2.5" class="ltx_td ltx_align_left ltx_border_b">263</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">For baseline calculations, we have used <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a>, trained on <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Common Objects in COntext" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">COCO</span></abbr></a>, and fine-tuned it on the train set until the performance by validation set failed to improve.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">In other experiments, we have used MobileNet which was trained on ImageNet as the <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a> classifier of <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> and first fine-tuned it on synthetic datasets with bigger learning rates and later, in some experiments, fine-tuned again with smaller learning rates on the real dataset.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p">Finally, smaller synthetic datasets of <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="6k" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><mrow id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml"><mn id="S3.SS3.p7.1.m1.1.1.2" xref="S3.SS3.p7.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p7.1.m1.1.1.1" xref="S3.SS3.p7.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.1.m1.1.1.3" xref="S3.SS3.p7.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><apply id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"><times id="S3.SS3.p7.1.m1.1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p7.1.m1.1.1.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2">6</cn><ci id="S3.SS3.p7.1.m1.1.1.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">6k</annotation></semantics></math> images were generated, each with a type of texture missing, and an additional baseline for comparison, which includes every pattern type.
These datasets allowed us to study the contribution of each individual texture in the final performance, as well as performance comparison of the smaller synthetic datasets.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">All synthetic images have Full-HD (1920 <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><times id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\times</annotation></semantics></math> 1080) resolution and are encoded in <span id="S4.p1.2.1" class="ltx_text ltx_font_typewriter">JPEG</span> lossy format, to match the training images taken by Kinect v2.0 that were used in our experiments.
For training and testing, images are down-scaled to half these dimensions (960 <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p1.2.m2.1a"><mo id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><times id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\times</annotation></semantics></math> 540) which is the resolution employed for all test scenarios in our pipeline.
Examples of the real datasets can be seen in Fig.¬†<a href="#S4.F3" title="Figure 3 ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:208.1pt;">
<img src="/html/1807.09834/assets/x1.png" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="257" alt="Refer to caption">
<p id="S4.F3.1.1" class="ltx_p ltx_align_center"><span id="S4.F3.1.1.1" class="ltx_text ltx_font_bold">(a)</span> Training Set Examples;</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:208.1pt;">
<img src="/html/1807.09834/assets/x2.png" id="S4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="260" alt="Refer to caption">
<p id="S4.F3.2.1" class="ltx_p ltx_align_center"><span id="S4.F3.2.1.1" class="ltx_text ltx_font_bold">(b)</span> Test Set Examples;</p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Example images from real¬†<span id="S4.F3.5.1" class="ltx_text ltx_font_bold">(a)</span>¬†training and¬†<span id="S4.F3.6.2" class="ltx_text ltx_font_bold">(b)</span>¬†test sets, annotated with ground truth and detector outputs, respectively.
</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Networks were trained with mini-batches of size <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">8</annotation></semantics></math>, on a machine with two Nvidia Titan Xp GPUs, for a duration depending on the performance in a real image validation set.
We have only used horizontal flips and random crops, with parameters reported in original <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> paper, as the pre-processing step, since we are interested in studying the effects of synthetic data and not different pre-processings.
Finally, in compliance with the findings in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, all the weights of the network are being updated in our experiments.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Our code and dataset are currently hosted on GitHub<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/jsbruglie/tf-shape-detection" title="" class="ltx_ref ltx_font_typewriter">https://github.com/jsbruglie/tf-shape-detection</a>, as of </span></span></span> and our Laboratory‚Äôs webpage<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="http://vislab.isr.ist.utl.pt/datasets/#shapes2018" title="" class="ltx_ref ltx_font_typewriter">http://vislab.isr.ist.utl.pt/datasets/#shapes2018</a>, as of </span></span></span>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benchmarking contributions to Gazebo plugin</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the authors state that a dataset of 9.000 Full-HD (1920 <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation></semantics></math> 1080) images took roughly 3 hours to generate.
In a similar computer, we tested the plugin with our modifications and obtained almost double of the speed performance, generating 9.000 synthetic images in little over 1h30min, albeit resorting to a larger set of available random textures (a total of 60.000 textures, compared to the reported 20.000), which expectedly should have increased run-time.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Our novel approach allows us to alter the properties of the objects directly through the rendering engine API, which is much more efficient than spawning and removing objects with different features.
Specifically, objects are spawned below the ground plane and moved to desired location in the new scene.
By changing its scale vector we can effectively morph the object shape.
Finally, we load the random textures as Gazebo resources on launch, and can apply them directly, although they are only loaded into memory once they are required by the rendering engine.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Effects of domain randomization on object detection</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">In this subsection, we wish to quantify how much an object detector performance would improve due to the usage of synthetic data.
To this purpose, initially, we fine-tuned a <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a>, pre-trained on <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Common Objects in COntext" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">COCO</span></abbr></a> dataset with our real image dataset for 16.000 epochs, which we determined to be sufficient by evaluating the performance on our validation set.
We used a decaying learning rate <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\alpha_{0}=0.004" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><msub id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.cmml">Œ±</mi><mn id="S4.SS2.p1.1.m1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">0.004</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><apply id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2">ùõº</ci><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3">0</cn></apply><cn type="float" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">0.004</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\alpha_{0}=0.004</annotation></semantics></math>, with a decay factor <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="k=0.95" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">ùëò</ci><cn type="float" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">k=0.95</annotation></semantics></math> every <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="t=100k" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">t</mi><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml"><mn id="S4.SS2.p1.3.m3.1.1.3.2" xref="S4.SS2.p1.3.m3.1.1.3.2.cmml">100</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.3.1" xref="S4.SS2.p1.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p1.3.m3.1.1.3.3" xref="S4.SS2.p1.3.m3.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">ùë°</ci><apply id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3"><times id="S4.SS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.p1.3.m3.1.1.3.2">100</cn><ci id="S4.SS2.p1.3.m3.1.1.3.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3">ùëò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">t=100k</annotation></semantics></math> steps.
In the subsequent sections we refer to this network as baseline.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Afterwards, we trained a <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> with only its classifier pre-trained on ImageNet, using our two synthetic datasets of <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="30k" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">30</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">30</cn><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">30k</annotation></semantics></math> images each, as described in section¬†<a href="#S3.SS3" title="3.3 Experiment design and setup ‚Ä£ 3 Methods ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">Both of these datasets contain simulated tabletop scenarios with a random number of objects <math id="S4.SS2.p3.1.m1.2" class="ltx_Math" alttext="N\in[2,7]" display="inline"><semantics id="S4.SS2.p3.1.m1.2a"><mrow id="S4.SS2.p3.1.m1.2.3" xref="S4.SS2.p3.1.m1.2.3.cmml"><mi id="S4.SS2.p3.1.m1.2.3.2" xref="S4.SS2.p3.1.m1.2.3.2.cmml">N</mi><mo id="S4.SS2.p3.1.m1.2.3.1" xref="S4.SS2.p3.1.m1.2.3.1.cmml">‚àà</mo><mrow id="S4.SS2.p3.1.m1.2.3.3.2" xref="S4.SS2.p3.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS2.p3.1.m1.2.3.3.2.1" xref="S4.SS2.p3.1.m1.2.3.3.1.cmml">[</mo><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">2</mn><mo id="S4.SS2.p3.1.m1.2.3.3.2.2" xref="S4.SS2.p3.1.m1.2.3.3.1.cmml">,</mo><mn id="S4.SS2.p3.1.m1.2.2" xref="S4.SS2.p3.1.m1.2.2.cmml">7</mn><mo stretchy="false" id="S4.SS2.p3.1.m1.2.3.3.2.3" xref="S4.SS2.p3.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.2b"><apply id="S4.SS2.p3.1.m1.2.3.cmml" xref="S4.SS2.p3.1.m1.2.3"><in id="S4.SS2.p3.1.m1.2.3.1.cmml" xref="S4.SS2.p3.1.m1.2.3.1"></in><ci id="S4.SS2.p3.1.m1.2.3.2.cmml" xref="S4.SS2.p3.1.m1.2.3.2">ùëÅ</ci><interval closure="closed" id="S4.SS2.p3.1.m1.2.3.3.1.cmml" xref="S4.SS2.p3.1.m1.2.3.3.2"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">2</cn><cn type="integer" id="S4.SS2.p3.1.m1.2.2.cmml" xref="S4.SS2.p3.1.m1.2.2">7</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.2c">N\in[2,7]</annotation></semantics></math>, each in one of three classes: box, cylinder or sphere.
These objects are placed randomly on the ground plane in a <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">3</cn><cn type="integer" id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">3\times 3</annotation></semantics></math> grid, to avoid overlap.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">In the first dataset, the camera pose is randomly generated for each scene, such that it points to the center of the object grid.
This generally results in high variability in the output, which may improve generalization capabilities of the network at the expense of added difficulty to the learning task, as, for instance, it exhibits higher levels of occlusion.
In the second dataset, the camera is fixed overlooking the scene at a downward angle, which is closer to the scenario we considered in the real dataset.
Example scenes with viewpoint candidates for each dataset are shown in Figure¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F4.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x3.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_square" width="461" height="461" alt="Refer to caption">
<p id="S4.F4.1.1" class="ltx_p ltx_align_center"><span id="S4.F4.1.1.1" class="ltx_text ltx_font_bold">(a)</span> Moving Viewpoint;</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F4.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x4.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_square" width="461" height="461" alt="Refer to caption">
<p id="S4.F4.2.1" class="ltx_p ltx_align_center"><span id="S4.F4.2.1.1" class="ltx_text ltx_font_bold">(b)</span> Fixed Viewpoint;</p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Viewpoint candidates in synthetic scene generation.
Left: Viewpoint changes both position and rotation in between scenes.
Subfigure represents four possible camera poses.
Right: Viewpoint is static.
</figcaption>
</figure>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">In addition to the camera, the scene light source is always allowed to move in a manner akin to the camera, in the first dataset.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.3" class="ltx_p">Similar to the baseline, the networks were trained on these datasets for over 90 epochs, based on their performance on the validation set employing an exponentially decaying learning rate, starting at <math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="\alpha_{0}=8\times 10^{-3}" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mrow id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><msub id="S4.SS2.p6.1.m1.1.1.2" xref="S4.SS2.p6.1.m1.1.1.2.cmml"><mi id="S4.SS2.p6.1.m1.1.1.2.2" xref="S4.SS2.p6.1.m1.1.1.2.2.cmml">Œ±</mi><mn id="S4.SS2.p6.1.m1.1.1.2.3" xref="S4.SS2.p6.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S4.SS2.p6.1.m1.1.1.1" xref="S4.SS2.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS2.p6.1.m1.1.1.3" xref="S4.SS2.p6.1.m1.1.1.3.cmml"><mn id="S4.SS2.p6.1.m1.1.1.3.2" xref="S4.SS2.p6.1.m1.1.1.3.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p6.1.m1.1.1.3.1" xref="S4.SS2.p6.1.m1.1.1.3.1.cmml">√ó</mo><msup id="S4.SS2.p6.1.m1.1.1.3.3" xref="S4.SS2.p6.1.m1.1.1.3.3.cmml"><mn id="S4.SS2.p6.1.m1.1.1.3.3.2" xref="S4.SS2.p6.1.m1.1.1.3.3.2.cmml">10</mn><mrow id="S4.SS2.p6.1.m1.1.1.3.3.3" xref="S4.SS2.p6.1.m1.1.1.3.3.3.cmml"><mo id="S4.SS2.p6.1.m1.1.1.3.3.3a" xref="S4.SS2.p6.1.m1.1.1.3.3.3.cmml">‚àí</mo><mn id="S4.SS2.p6.1.m1.1.1.3.3.3.2" xref="S4.SS2.p6.1.m1.1.1.3.3.3.2.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><eq id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1.1"></eq><apply id="S4.SS2.p6.1.m1.1.1.2.cmml" xref="S4.SS2.p6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p6.1.m1.1.1.2.1.cmml" xref="S4.SS2.p6.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.p6.1.m1.1.1.2.2.cmml" xref="S4.SS2.p6.1.m1.1.1.2.2">ùõº</ci><cn type="integer" id="S4.SS2.p6.1.m1.1.1.2.3.cmml" xref="S4.SS2.p6.1.m1.1.1.2.3">0</cn></apply><apply id="S4.SS2.p6.1.m1.1.1.3.cmml" xref="S4.SS2.p6.1.m1.1.1.3"><times id="S4.SS2.p6.1.m1.1.1.3.1.cmml" xref="S4.SS2.p6.1.m1.1.1.3.1"></times><cn type="integer" id="S4.SS2.p6.1.m1.1.1.3.2.cmml" xref="S4.SS2.p6.1.m1.1.1.3.2">8</cn><apply id="S4.SS2.p6.1.m1.1.1.3.3.cmml" xref="S4.SS2.p6.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p6.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p6.1.m1.1.1.3.3">superscript</csymbol><cn type="integer" id="S4.SS2.p6.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p6.1.m1.1.1.3.3.2">10</cn><apply id="S4.SS2.p6.1.m1.1.1.3.3.3.cmml" xref="S4.SS2.p6.1.m1.1.1.3.3.3"><minus id="S4.SS2.p6.1.m1.1.1.3.3.3.1.cmml" xref="S4.SS2.p6.1.m1.1.1.3.3.3"></minus><cn type="integer" id="S4.SS2.p6.1.m1.1.1.3.3.3.2.cmml" xref="S4.SS2.p6.1.m1.1.1.3.3.3.2">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">\alpha_{0}=8\times 10^{-3}</annotation></semantics></math>, and a decay of <math id="S4.SS2.p6.2.m2.1" class="ltx_Math" alttext="k=0.95" display="inline"><semantics id="S4.SS2.p6.2.m2.1a"><mrow id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml"><mi id="S4.SS2.p6.2.m2.1.1.2" xref="S4.SS2.p6.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS2.p6.2.m2.1.1.1" xref="S4.SS2.p6.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p6.2.m2.1.1.3" xref="S4.SS2.p6.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1"><eq id="S4.SS2.p6.2.m2.1.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1.1"></eq><ci id="S4.SS2.p6.2.m2.1.1.2.cmml" xref="S4.SS2.p6.2.m2.1.1.2">ùëò</ci><cn type="float" id="S4.SS2.p6.2.m2.1.1.3.cmml" xref="S4.SS2.p6.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">k=0.95</annotation></semantics></math> every <math id="S4.SS2.p6.3.m3.1" class="ltx_Math" alttext="t=50k" display="inline"><semantics id="S4.SS2.p6.3.m3.1a"><mrow id="S4.SS2.p6.3.m3.1.1" xref="S4.SS2.p6.3.m3.1.1.cmml"><mi id="S4.SS2.p6.3.m3.1.1.2" xref="S4.SS2.p6.3.m3.1.1.2.cmml">t</mi><mo id="S4.SS2.p6.3.m3.1.1.1" xref="S4.SS2.p6.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS2.p6.3.m3.1.1.3" xref="S4.SS2.p6.3.m3.1.1.3.cmml"><mn id="S4.SS2.p6.3.m3.1.1.3.2" xref="S4.SS2.p6.3.m3.1.1.3.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p6.3.m3.1.1.3.1" xref="S4.SS2.p6.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p6.3.m3.1.1.3.3" xref="S4.SS2.p6.3.m3.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.3.m3.1b"><apply id="S4.SS2.p6.3.m3.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1"><eq id="S4.SS2.p6.3.m3.1.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1.1"></eq><ci id="S4.SS2.p6.3.m3.1.1.2.cmml" xref="S4.SS2.p6.3.m3.1.1.2">ùë°</ci><apply id="S4.SS2.p6.3.m3.1.1.3.cmml" xref="S4.SS2.p6.3.m3.1.1.3"><times id="S4.SS2.p6.3.m3.1.1.3.1.cmml" xref="S4.SS2.p6.3.m3.1.1.3.1"></times><cn type="integer" id="S4.SS2.p6.3.m3.1.1.3.2.cmml" xref="S4.SS2.p6.3.m3.1.1.3.2">50</cn><ci id="S4.SS2.p6.3.m3.1.1.3.3.cmml" xref="S4.SS2.p6.3.m3.1.1.3.3">ùëò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.3.m3.1c">t=50k</annotation></semantics></math> steps.
These networks were then directly applied to the test set¬†(which has real images) without any fine-tuning on our dataset of real object data, in order to quantify how much knowledge can be directly transferred from synthetic to real domain.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">Finally, these two detectors were fine-tuned on the real dataset for over 2200 epochs and with a fixed learning rate of <math id="S4.SS2.p7.1.m1.1" class="ltx_Math" alttext="\alpha=10^{-3}" display="inline"><semantics id="S4.SS2.p7.1.m1.1a"><mrow id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml"><mi id="S4.SS2.p7.1.m1.1.1.2" xref="S4.SS2.p7.1.m1.1.1.2.cmml">Œ±</mi><mo id="S4.SS2.p7.1.m1.1.1.1" xref="S4.SS2.p7.1.m1.1.1.1.cmml">=</mo><msup id="S4.SS2.p7.1.m1.1.1.3" xref="S4.SS2.p7.1.m1.1.1.3.cmml"><mn id="S4.SS2.p7.1.m1.1.1.3.2" xref="S4.SS2.p7.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p7.1.m1.1.1.3.3" xref="S4.SS2.p7.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.p7.1.m1.1.1.3.3a" xref="S4.SS2.p7.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S4.SS2.p7.1.m1.1.1.3.3.2" xref="S4.SS2.p7.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.1b"><apply id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1"><eq id="S4.SS2.p7.1.m1.1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1.1"></eq><ci id="S4.SS2.p7.1.m1.1.1.2.cmml" xref="S4.SS2.p7.1.m1.1.1.2">ùõº</ci><apply id="S4.SS2.p7.1.m1.1.1.3.cmml" xref="S4.SS2.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p7.1.m1.1.1.3.1.cmml" xref="S4.SS2.p7.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.p7.1.m1.1.1.3.2.cmml" xref="S4.SS2.p7.1.m1.1.1.3.2">10</cn><apply id="S4.SS2.p7.1.m1.1.1.3.3.cmml" xref="S4.SS2.p7.1.m1.1.1.3.3"><minus id="S4.SS2.p7.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p7.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p7.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p7.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.1c">\alpha=10^{-3}</annotation></semantics></math>.
The result of this analysis is depicted in Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and summarized in Table¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1807.09834/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Per class <a href="#id9.9.id9"><abbr href="#id9.9.id9" title="Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AP</span></abbr></a> and <a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Mean Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">mAP</span></abbr></a> of different detectors.
MV:¬†Moving Viewpoint;
FV:¬†Fixed Viewpoint;
Real:¬†fine-tuned on the real dataset
</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
<a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> performance on test set.
For abbreviations refer to Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</figcaption>
<table id="S4.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.5.5" class="ltx_tr">
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Run</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">mAP</th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP Box</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP Cylinder</th>
<th id="S4.T3.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP Sphere</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.6" class="ltx_tr">
<td id="S4.T3.6.6.1" class="ltx_td ltx_align_left ltx_border_t">COCO + Real</td>
<td id="S4.T3.6.6.2" class="ltx_td ltx_align_left ltx_border_t">0.6598</td>
<td id="S4.T3.6.6.3" class="ltx_td ltx_align_left ltx_border_t">0.7640</td>
<td id="S4.T3.6.6.4" class="ltx_td ltx_align_left ltx_border_t">0.5491</td>
<td id="S4.T3.6.6.5" class="ltx_td ltx_align_left ltx_border_t">0.6664</td>
</tr>
<tr id="S4.T3.6.7.1" class="ltx_tr">
<td id="S4.T3.6.7.1.1" class="ltx_td ltx_align_left">FV</td>
<td id="S4.T3.6.7.1.2" class="ltx_td ltx_align_left">0.5801</td>
<td id="S4.T3.6.7.1.3" class="ltx_td ltx_align_left">0.4190</td>
<td id="S4.T3.6.7.1.4" class="ltx_td ltx_align_left">0.4632</td>
<td id="S4.T3.6.7.1.5" class="ltx_td ltx_align_left">0.8581</td>
</tr>
<tr id="S4.T3.6.8.2" class="ltx_tr">
<td id="S4.T3.6.8.2.1" class="ltx_td ltx_align_left">MV</td>
<td id="S4.T3.6.8.2.2" class="ltx_td ltx_align_left">0.5804</td>
<td id="S4.T3.6.8.2.3" class="ltx_td ltx_align_left">0.5578</td>
<td id="S4.T3.6.8.2.4" class="ltx_td ltx_align_left">0.3230</td>
<td id="S4.T3.6.8.2.5" class="ltx_td ltx_align_left"><span id="S4.T3.6.8.2.5.1" class="ltx_text ltx_font_bold">0.8603</span></td>
</tr>
<tr id="S4.T3.6.9.3" class="ltx_tr">
<td id="S4.T3.6.9.3.1" class="ltx_td ltx_align_left">FV + Real</td>
<td id="S4.T3.6.9.3.2" class="ltx_td ltx_align_left"><span id="S4.T3.6.9.3.2.1" class="ltx_text ltx_font_bold">0.8319</span></td>
<td id="S4.T3.6.9.3.3" class="ltx_td ltx_align_left"><span id="S4.T3.6.9.3.3.1" class="ltx_text ltx_font_bold">0.8988</span></td>
<td id="S4.T3.6.9.3.4" class="ltx_td ltx_align_left"><span id="S4.T3.6.9.3.4.1" class="ltx_text ltx_font_bold">0.7573</span></td>
<td id="S4.T3.6.9.3.5" class="ltx_td ltx_align_left">0.8395</td>
</tr>
<tr id="S4.T3.6.10.4" class="ltx_tr">
<td id="S4.T3.6.10.4.1" class="ltx_td ltx_align_left ltx_border_b">MV + Real</td>
<td id="S4.T3.6.10.4.2" class="ltx_td ltx_align_left ltx_border_b">0.7480</td>
<td id="S4.T3.6.10.4.3" class="ltx_td ltx_align_left ltx_border_b">0.8896</td>
<td id="S4.T3.6.10.4.4" class="ltx_td ltx_align_left ltx_border_b">0.5954</td>
<td id="S4.T3.6.10.4.5" class="ltx_td ltx_align_left ltx_border_b">0.7591</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.2" class="ltx_p">The network trained on the dataset with no camera pose variation and fine-tuning on real data exhibits the best performance at <math id="S4.SS2.p8.1.m1.1" class="ltx_Math" alttext="0.83" display="inline"><semantics id="S4.SS2.p8.1.m1.1a"><mn id="S4.SS2.p8.1.m1.1.1" xref="S4.SS2.p8.1.m1.1.1.cmml">0.83</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.1.m1.1b"><cn type="float" id="S4.SS2.p8.1.m1.1.1.cmml" xref="S4.SS2.p8.1.m1.1.1">0.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.1.m1.1c">0.83</annotation></semantics></math> <a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Mean Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">mAP</span></abbr></a>, which corresponds to an improvement of <math id="S4.SS2.p8.2.m2.1" class="ltx_Math" alttext="\mathbf{26\%}" display="inline"><semantics id="S4.SS2.p8.2.m2.1a"><mrow id="S4.SS2.p8.2.m2.1.1" xref="S4.SS2.p8.2.m2.1.1.cmml"><mn id="S4.SS2.p8.2.m2.1.1.2" xref="S4.SS2.p8.2.m2.1.1.2.cmml">ùüêùüî</mn><mo id="S4.SS2.p8.2.m2.1.1.1" xref="S4.SS2.p8.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.2.m2.1b"><apply id="S4.SS2.p8.2.m2.1.1.cmml" xref="S4.SS2.p8.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p8.2.m2.1.1.1.cmml" xref="S4.SS2.p8.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p8.2.m2.1.1.2.cmml" xref="S4.SS2.p8.2.m2.1.1.2">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.2.m2.1c">\mathbf{26\%}</annotation></semantics></math> over baseline.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">Furthermore, we can observe that although the networks trained only on each of the synthetic datasets obtain similar <a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Mean Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">mAP</span></abbr></a> values in the test set (roughly <math id="S4.SS2.p9.1.m1.1" class="ltx_Math" alttext="0.58" display="inline"><semantics id="S4.SS2.p9.1.m1.1a"><mn id="S4.SS2.p9.1.m1.1.1" xref="S4.SS2.p9.1.m1.1.1.cmml">0.58</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.1.m1.1b"><cn type="float" id="S4.SS2.p9.1.m1.1.1.cmml" xref="S4.SS2.p9.1.m1.1.1">0.58</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.1.m1.1c">0.58</annotation></semantics></math>), their results differ greatly after fine-tuning on real images.
This observation suggests that the changes in camera pose, seen in the synthetic dataset have indeed hurt the performance, as our test set does not exhibit these variations.
However, it is expected that the network trained on the corresponding dataset is more robust and would perform better if it was tested against a dataset with varying camera/light positions.</p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p">Fig.¬†<a href="#S4.F6" title="Figure 6 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the precision-recall curves of different networks for each class.
Consistently, the networks trained on the fixed viewpoint dataset and fine-tuned on the real dataset out-perform other variations.
This trend is only less prominent in the case of <em id="S4.SS2.p10.1.1" class="ltx_emph ltx_font_italic">sphere</em> class, where, seemingly, due to the smaller examples of this class in the real dataset¬†(Table¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3.3 Experiment design and setup ‚Ä£ 3 Methods ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) the training benefits less from fine-tuning on the real dataset for some values of iso-f1 surfaces.
This observation is also visible in Table¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We hypothesize that more real sphere examples could help the detector in improving the  <a href="#id9.9.id9"><span href="#id9.9.id9" title="Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Average Precision</span></span></a> (<a href="#id9.9.id9"><abbr href="#id9.9.id9" title="Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AP</span></abbr></a>) score for spheres.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F6.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x6.png" id="S4.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F6.1.1" class="ltx_p ltx_align_center"><span id="S4.F6.1.1.1" class="ltx_text ltx_font_bold">(a)</span> Sphere;</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F6.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x7.png" id="S4.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F6.2.1" class="ltx_p ltx_align_center"><span id="S4.F6.2.1.1" class="ltx_text ltx_font_bold">(b)</span> Cylinder;</p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F6.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x8.png" id="S4.F6.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F6.3.1" class="ltx_p ltx_align_center"><span id="S4.F6.3.1.1" class="ltx_text ltx_font_bold">(c)</span> Box;</p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Precision-recall curves of different variants of the detectors.
For abbreviations refer to Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Individual contribution of texture patterns</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">A valid question in domain randomization research is the contribution of including various textures as well as the importance of sample sizes. To study this question, we have created smaller synthetic datasets with only <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="6k" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">6</cn><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">6k</annotation></semantics></math> images, where in each of them one specific texture is missing.
Similar to previous subsection, MobileNet pre-trained on ImageNet was selected as the classifier <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr></a>, but the detectors were instead trained on these smaller synthetic datasets and then, fine-tuned on the real dataset.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.4" class="ltx_p">The training of all networks on synthetic datasets lasted for 130 epochs, which was found to be the point where the <a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Mean Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">mAP</span></abbr></a> did not improve over the validation set, with an exponentially decaying learning rate starting at <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\alpha_{0}=0.004" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><msub id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2.2" xref="S4.SS3.p2.1.m1.1.1.2.2.cmml">Œ±</mi><mn id="S4.SS3.p2.1.m1.1.1.2.3" xref="S4.SS3.p2.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">0.004</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><eq id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></eq><apply id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.2.1.cmml" xref="S4.SS3.p2.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2.2">ùõº</ci><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.3.cmml" xref="S4.SS3.p2.1.m1.1.1.2.3">0</cn></apply><cn type="float" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">0.004</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\alpha_{0}=0.004</annotation></semantics></math>, <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="k=0.95" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><eq id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></eq><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">ùëò</ci><cn type="float" id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">k=0.95</annotation></semantics></math> and <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="t=50k" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">t</mi><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml"><mn id="S4.SS3.p2.3.m3.1.1.3.2" xref="S4.SS3.p2.3.m3.1.1.3.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m3.1.1.3.1" xref="S4.SS3.p2.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S4.SS3.p2.3.m3.1.1.3.3" xref="S4.SS3.p2.3.m3.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><eq id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"></eq><ci id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">ùë°</ci><apply id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3"><times id="S4.SS3.p2.3.m3.1.1.3.1.cmml" xref="S4.SS3.p2.3.m3.1.1.3.1"></times><cn type="integer" id="S4.SS3.p2.3.m3.1.1.3.2.cmml" xref="S4.SS3.p2.3.m3.1.1.3.2">50</cn><ci id="S4.SS3.p2.3.m3.1.1.3.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3.3">ùëò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">t=50k</annotation></semantics></math> steps.
Finally, these networks were fine-tuned with the real-image dataset for 1100 epochs, with a constant learning rate <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="\alpha=0.001" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mrow id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mi id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">Œ±</mi><mo id="S4.SS3.p2.4.m4.1.1.1" xref="S4.SS3.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.4.m4.1.1.3" xref="S4.SS3.p2.4.m4.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><eq id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1.1"></eq><ci id="S4.SS3.p2.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.m4.1.1.2">ùõº</ci><cn type="float" id="S4.SS3.p2.4.m4.1.1.3.cmml" xref="S4.SS3.p2.4.m4.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">\alpha=0.001</annotation></semantics></math>.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F7.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:199.5pt;">
<img src="/html/1807.09834/assets/x9.png" id="S4.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F7.1.1" class="ltx_p ltx_align_center"><span id="S4.F7.1.1.1" class="ltx_text ltx_font_bold">(a)</span> Before fine-tuning;</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F7.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:199.5pt;">
<img src="/html/1807.09834/assets/x10.png" id="S4.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F7.2.1" class="ltx_p ltx_align_center"><span id="S4.F7.2.1.1" class="ltx_text ltx_font_bold">(b)</span> After fine-tuning;</p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
Performance of <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> on test set during training on smaller datasets of 6k images, each missing a type of texture, with the exception of the baseline, prior and after fine-tuning on real image dataset (<span id="S4.F7.5.1" class="ltx_text ltx_font_bold">(a)</span>,<span id="S4.F7.6.2" class="ltx_text ltx_font_bold">(b)</span> respectively).
</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F8.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x11.png" id="S4.F8.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F8.1.1" class="ltx_p ltx_align_center"><span id="S4.F8.1.1.1" class="ltx_text ltx_font_bold">(a)</span> Before fine-tuning;</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F8.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x12.png" id="S4.F8.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F8.2.1" class="ltx_p ltx_align_center"><span id="S4.F8.2.1.1" class="ltx_text ltx_font_bold">(b)</span> After fine-tuning;</p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
Performance of <a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> on validation set during training on smaller datasets of 6k images, each missing a type of texture, with the exception of the baseline, prior and after fine-tuning on real image dataset (<span id="S4.F8.5.1" class="ltx_text ltx_font_bold">(a)</span>,¬†<span id="S4.F8.6.2" class="ltx_text ltx_font_bold">(b)</span> respectively).
</figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F9.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x13.png" id="S4.F9.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F9.1.1" class="ltx_p ltx_align_center"><span id="S4.F9.1.1.1" class="ltx_text ltx_font_bold">(a)</span> Sphere;</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F9.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x14.png" id="S4.F9.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F9.2.1" class="ltx_p ltx_align_center"><span id="S4.F9.2.1.1" class="ltx_text ltx_font_bold">(b)</span> Cylinder;</p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F9.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:173.4pt;">
<img src="/html/1807.09834/assets/x15.png" id="S4.F9.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<p id="S4.F9.3.1" class="ltx_p ltx_align_center"><span id="S4.F9.3.1.1" class="ltx_text ltx_font_bold">(c)</span> Box;</p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
Precision-recall curves of different variants of the detectors after fine-tuning on the real dataset.
For abbreviations refer to Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.2 Effects of domain randomization on object detection ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">The results of these experiments are reported in Fig.¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.3 Individual contribution of texture patterns ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and Table¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4.3 Individual contribution of texture patterns ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
By comparing figures <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">(a)</span> and <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_bold">(b)</span> in Fig.¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.3 Individual contribution of texture patterns ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> it is clear that all variations have benefited from fine-tuning with the real dataset.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">About the individual performances, generally speaking, by removing more and more complex textures¬†(flat to be the least complex and Perlin to be the most complex), the performance hurts, and we found Perlin noise to be a vital texture for object detection, while the flat texture has the least significance.
Consistent with this observation, according to Fig.¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.3 Individual contribution of texture patterns ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>¬†<span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">(b)</span>, the small dataset with all the textures cannot always compete with some of the datasets where a texture is missing.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">According to Figures¬†<a href="#S4.F8" title="Figure 8 ‚Ä£ 4.3 Individual contribution of texture patterns ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.3 Individual contribution of texture patterns ‚Ä£ 4 Experiments and results ‚Ä£ Applying Domain Randomization to Synthetic Data for Object Category Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>¬†<span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">(a)</span>, the detector trained on the small dataset with all texture classes, outperformed other variations on the <span id="S4.SS3.p5.1.2" class="ltx_text ltx_font_bold">validation set</span> during training, however, presumably due to smaller number of samples and simultaneously, so many texture classes, over-fitted to the objects in the train set and failed to generalize as well as others to the objects in the test set.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.3" class="ltx_p">Regarding the number of samples, our in house study with <math id="S4.SS3.p6.1.m1.1" class="ltx_Math" alttext="200k" display="inline"><semantics id="S4.SS3.p6.1.m1.1a"><mrow id="S4.SS3.p6.1.m1.1.1" xref="S4.SS3.p6.1.m1.1.1.cmml"><mn id="S4.SS3.p6.1.m1.1.1.2" xref="S4.SS3.p6.1.m1.1.1.2.cmml">200</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p6.1.m1.1.1.1" xref="S4.SS3.p6.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS3.p6.1.m1.1.1.3" xref="S4.SS3.p6.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.1.m1.1b"><apply id="S4.SS3.p6.1.m1.1.1.cmml" xref="S4.SS3.p6.1.m1.1.1"><times id="S4.SS3.p6.1.m1.1.1.1.cmml" xref="S4.SS3.p6.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p6.1.m1.1.1.2.cmml" xref="S4.SS3.p6.1.m1.1.1.2">200</cn><ci id="S4.SS3.p6.1.m1.1.1.3.cmml" xref="S4.SS3.p6.1.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.1.m1.1c">200k</annotation></semantics></math> synthetic images¬†(unreported) in line with the findings in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, suggests that more is not always better.
The network trained on our smaller dataset of only <math id="S4.SS3.p6.2.m2.1" class="ltx_Math" alttext="6k" display="inline"><semantics id="S4.SS3.p6.2.m2.1a"><mrow id="S4.SS3.p6.2.m2.1.1" xref="S4.SS3.p6.2.m2.1.1.cmml"><mn id="S4.SS3.p6.2.m2.1.1.2" xref="S4.SS3.p6.2.m2.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p6.2.m2.1.1.1" xref="S4.SS3.p6.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S4.SS3.p6.2.m2.1.1.3" xref="S4.SS3.p6.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.2.m2.1b"><apply id="S4.SS3.p6.2.m2.1.1.cmml" xref="S4.SS3.p6.2.m2.1.1"><times id="S4.SS3.p6.2.m2.1.1.1.cmml" xref="S4.SS3.p6.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.p6.2.m2.1.1.2.cmml" xref="S4.SS3.p6.2.m2.1.1.2">6</cn><ci id="S4.SS3.p6.2.m2.1.1.3.cmml" xref="S4.SS3.p6.2.m2.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.2.m2.1c">6k</annotation></semantics></math> images without the ‚Äúflat‚Äù texture has even slightly out-performed the network that was trained on <math id="S4.SS3.p6.3.m3.1" class="ltx_Math" alttext="30k" display="inline"><semantics id="S4.SS3.p6.3.m3.1a"><mrow id="S4.SS3.p6.3.m3.1.1" xref="S4.SS3.p6.3.m3.1.1.cmml"><mn id="S4.SS3.p6.3.m3.1.1.2" xref="S4.SS3.p6.3.m3.1.1.2.cmml">30</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p6.3.m3.1.1.1" xref="S4.SS3.p6.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S4.SS3.p6.3.m3.1.1.3" xref="S4.SS3.p6.3.m3.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.3.m3.1b"><apply id="S4.SS3.p6.3.m3.1.1.cmml" xref="S4.SS3.p6.3.m3.1.1"><times id="S4.SS3.p6.3.m3.1.1.1.cmml" xref="S4.SS3.p6.3.m3.1.1.1"></times><cn type="integer" id="S4.SS3.p6.3.m3.1.1.2.cmml" xref="S4.SS3.p6.3.m3.1.1.2">30</cn><ci id="S4.SS3.p6.3.m3.1.1.3.cmml" xref="S4.SS3.p6.3.m3.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.3.m3.1c">30k</annotation></semantics></math> synthetic images.
This result seems to be consistent for detectors with classifiers trained on real images, trained on synthetic data and then again fine-tuned with real samples.
After a fixed number of images, the <a href="#id10.10.id10"><abbr href="#id10.10.id10" title="Mean Average Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">mAP</span></abbr></a> performance oscillates for one or two percents.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><a href="#id6.6.id6"><abbr href="#id6.6.id6" title="Single‚ÄìShot Detector" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSD</span></abbr></a> performance on test set after train on each of the 6k sub‚Äìdatasets and fine-tuned on real images.</figcaption>
<table id="S4.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.5.5" class="ltx_tr">
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Training dataset</th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">mAP</th>
<th id="S4.T4.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP Box</th>
<th id="S4.T4.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP Cylinder</th>
<th id="S4.T4.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP Sphere</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.5.6.1" class="ltx_tr">
<td id="S4.T4.5.6.1.1" class="ltx_td ltx_align_left ltx_border_t">All</td>
<td id="S4.T4.5.6.1.2" class="ltx_td ltx_align_left ltx_border_t">0.7885</td>
<td id="S4.T4.5.6.1.3" class="ltx_td ltx_align_left ltx_border_t">0.8344</td>
<td id="S4.T4.5.6.1.4" class="ltx_td ltx_align_left ltx_border_t">0.6616</td>
<td id="S4.T4.5.6.1.5" class="ltx_td ltx_align_left ltx_border_t">0.8694</td>
</tr>
<tr id="S4.T4.5.7.2" class="ltx_tr">
<td id="S4.T4.5.7.2.1" class="ltx_td ltx_align_left">No Flat</td>
<td id="S4.T4.5.7.2.2" class="ltx_td ltx_align_left"><span id="S4.T4.5.7.2.2.1" class="ltx_text ltx_font_bold">0.8410</span></td>
<td id="S4.T4.5.7.2.3" class="ltx_td ltx_align_left"><span id="S4.T4.5.7.2.3.1" class="ltx_text ltx_font_bold">0.8775</span></td>
<td id="S4.T4.5.7.2.4" class="ltx_td ltx_align_left"><span id="S4.T4.5.7.2.4.1" class="ltx_text ltx_font_bold">0.7546</span></td>
<td id="S4.T4.5.7.2.5" class="ltx_td ltx_align_left"><span id="S4.T4.5.7.2.5.1" class="ltx_text ltx_font_bold">0.8910</span></td>
</tr>
<tr id="S4.T4.5.8.3" class="ltx_tr">
<td id="S4.T4.5.8.3.1" class="ltx_td ltx_align_left">No Chess</td>
<td id="S4.T4.5.8.3.2" class="ltx_td ltx_align_left">0.7925</td>
<td id="S4.T4.5.8.3.3" class="ltx_td ltx_align_left">0.8332</td>
<td id="S4.T4.5.8.3.4" class="ltx_td ltx_align_left">0.6958</td>
<td id="S4.T4.5.8.3.5" class="ltx_td ltx_align_left">0.8485</td>
</tr>
<tr id="S4.T4.5.9.4" class="ltx_tr">
<td id="S4.T4.5.9.4.1" class="ltx_td ltx_align_left">No Gradient</td>
<td id="S4.T4.5.9.4.2" class="ltx_td ltx_align_left">0.7668</td>
<td id="S4.T4.5.9.4.3" class="ltx_td ltx_align_left">0.8296</td>
<td id="S4.T4.5.9.4.4" class="ltx_td ltx_align_left">0.6172</td>
<td id="S4.T4.5.9.4.5" class="ltx_td ltx_align_left">0.8536</td>
</tr>
<tr id="S4.T4.5.10.5" class="ltx_tr">
<td id="S4.T4.5.10.5.1" class="ltx_td ltx_align_left ltx_border_b">No Perlin</td>
<td id="S4.T4.5.10.5.2" class="ltx_td ltx_align_left ltx_border_b">0.6901</td>
<td id="S4.T4.5.10.5.3" class="ltx_td ltx_align_left ltx_border_b">0.7764</td>
<td id="S4.T4.5.10.5.4" class="ltx_td ltx_align_left ltx_border_b">0.5058</td>
<td id="S4.T4.5.10.5.5" class="ltx_td ltx_align_left ltx_border_b">0.7880</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and future directions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work we have shown that multi-category object detection pipelines can significantly benefit from pre-training on synthetic non-photo-realistic datasets.
Our modifications to an open-source plugin have enabled us to rapidly test different variations in the synthetic data and assess the importance of various components such as texture complexity and sample size.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">According to our experiments, increasing texture complexity in the synthetic data should be compensated by larger number of samples, however, big gains in detector accuracy can be obtained with synthetic datasets that are orders of magnitude smaller than <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Common Objects in COntext" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">COCO</span></abbr></a> or ImageNet as long as a classifier trained on real datasets is being used.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Our modifications to the plugin for the synthetic data generation will facilitate the creation of scenes for other types of studies in <em id="S5.p3.1.1" class="ltx_emph ltx_font_italic">domain randomization</em>, such as the impact of clutter and the increasing number of new object classes.
The choice of Gazebo will facilitate the creation of scenes for deep learning experiments in robotics, such as object tracking and mobile manipulation.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In real scenarios where final performance metric is usually the most pertinent consideration, various data augmentation techniques such as color intensity distortions, random crops, etc. should be added to the training pipeline of domain randomization to improve the generalization capabilities of the detector at test time.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">We believe enriching the plugin with more texture categories and combinations of categories can significantly improve the synthetic data quality for domain randomization studies.
More specifically, currently no synthetic object can have more than one texture, where as in reality, e.g., a box can have different textures at each side.
Another limitation of this plugin is that it not possible to stack objects on top of one another, however, in our test scenarios many objects were placed on top of each other.
Removing these limitations can widen the applicability of the plugin in different domain randomization scenarios.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Finally, with the advance of deep instance segmentation methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, a similar study should be conducted to assess the applicability of domain randomization on object category segmentation.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is partially supported by the Portuguese Foundation for Science and Technology (FCT) project [UID/EEA/50009/2013].
Atabak Dehban and Rui Figueiredo are funded by FCT PhD grants PD/BD/105776/2014 and
PD/BD/105779/2014, respectively.
The Titan Xp GPUs used for this research were donated by the NVIDIA Corporation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Dai, J., Li, Y., He, K., Sun, J.:

</span>
<span class="ltx_bibblock">R-fcn: Object detection via region-based fully convolutional
networks.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2016)
379‚Äì387

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:

</span>
<span class="ltx_bibblock">SSD: Single shot multibox detector.

</span>
<span class="ltx_bibblock">In: European Conference on Computer Vision (ECCV), Springer (2016)
21‚Äì37

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Redmon, J., Farhadi, A.:

</span>
<span class="ltx_bibblock">YOLO9000: Better, faster, stronger.

</span>
<span class="ltx_bibblock">In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). (2017) 6517‚Äì6525

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.:

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards real-time object detection with region
proposal networks.

</span>
<span class="ltx_bibblock">In: Advances in neural information processing systems. (2015) 91‚Äì99

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Zhang, S., Zhu, X., Lei, Z., Shi, H., Wang, X., Li, S.Z.:

</span>
<span class="ltx_bibblock">S^ 3fd: Single shot scale-invariant face detector.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Computer Vision (ICCV). (2017)
192‚Äì201

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Zhu, W., Liu, C., Fan, W., Xie, X.:

</span>
<span class="ltx_bibblock">Deeplung: Deep 3d dual path nets for automated pulmonary nodule
detection and classification.

</span>
<span class="ltx_bibblock">In: IEEE Winter Conference on Applications of Computer Vision (WACV).
(2018) 673‚Äì681

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Maiettini, E., Pasquale, G., Rosasco, L., Natale, L.:

</span>
<span class="ltx_bibblock">Interactive data collection for deep learning object detectors on
humanoid robots.

</span>
<span class="ltx_bibblock">In: IEEE‚ÄìRAS International Conference on Humanoid Robotics
(Humanoids). (2017) 862‚Äì868

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Doll√°r, P., Zitnick, C.L.:

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In: European Conference on Computer Vision (ECCV), Springer (2014)
740‚Äì755

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et¬†al.:

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">115</span> (2015)
211‚Äì252

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Patel, V.M., Gopalan, R., Li, R., Chellappa, R.:

</span>
<span class="ltx_bibblock">Visual domain adaptation: A survey of recent advances.

</span>
<span class="ltx_bibblock">IEEE signal processing magazine <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">32</span> (2015) 53‚Äì69

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ferguson, M., Ak, R., Lee, Y.T.T., Law, K.H.:

</span>
<span class="ltx_bibblock">Automatic localization of casting defects with convolutional neural
networks.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Big Data. (2017) 1726‚Äì1735

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Maeda, H., Sekimoto, Y., Seto, T., Kashiyama, T., Omata, H.:

</span>
<span class="ltx_bibblock">Road damage detection using deep neural networks with images captured
through a smartphone.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1801.09454 (2018)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Van¬†Horn, G., Mac¬†Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H.,
Perona, P., Belongie, S.:

</span>
<span class="ltx_bibblock">The inaturalist species classification and detection dataset.

</span>
<span class="ltx_bibblock">(2018)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Zamir, A.R., Sax, A., Shen, W., Guibas, L., Malik, J., Savarese, S.:

</span>
<span class="ltx_bibblock">Taskonomy: Disentangling task transfer learning.

</span>
<span class="ltx_bibblock">In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). (2018) 3712‚Äì3722

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Girshick, R., Radosavovic, I., Gkioxari, G., Doll√°r, P., He, K.:

</span>
<span class="ltx_bibblock">Detectron.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron/" title="" class="ltx_ref">https://github.com/facebookresearch/detectron/</a> (2018)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer,
I., Wojna, Z., Song, Y., Guadarrama, S., et¬†al.:

</span>
<span class="ltx_bibblock">Speed/accuracy trade-offs for modern convolutional object detectors.

</span>
<span class="ltx_bibblock">In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). (2017) 3296‚Äì3297

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., Abbeel, P.:

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from
simulation to the real world.

</span>
<span class="ltx_bibblock">In: IEEE‚ÄìRSJ International Conference on Intelligent Robots and
Systems (IROS). (2017) 23‚Äì30

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Borrego, J., Figueiredo, R., Dehban, A., Moreno, P., Bernardino, A.,
Santos-Victor, J.:

</span>
<span class="ltx_bibblock">A generic visual perception domain randomisation framework for
gazebo.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Autonomous Robot Systems and
Competitions (ICARSC), IEEE (2018) 237‚Äì242

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Koenig, N., Howard, A.:

</span>
<span class="ltx_bibblock">Design and use paradigms for gazebo, an open-source multi-robot
simulator.

</span>
<span class="ltx_bibblock">In: IEEE‚ÄìRSJ International Conference on Intelligent Robots and
Systems (IROS). Volume¬†3. (2004) 2149‚Äì2154

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Rawat, W., Wang, Z.:

</span>
<span class="ltx_bibblock">Deep convolutional neural networks for image classification: A
comprehensive review.

</span>
<span class="ltx_bibblock">Neural computation <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">29</span> (2017) 2352‚Äì2449

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., Doll√°r, P., Girshick, R.:

</span>
<span class="ltx_bibblock">Mask R-CNN.

</span>
<span class="ltx_bibblock">In: Computer Vision (ICCV), 2017 IEEE International Conference on,
IEEE (2017) 2980‚Äì2988

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Johnson-Roberson, M., Barto, C., Mehta, R., Sridhar, S.N., Rosaen, K.,
Vasudevan, R.:

</span>
<span class="ltx_bibblock">Driving in the matrix: Can virtual worlds replace human-generated
annotations for real world tasks?

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Robotics and Automation (ICRA),
IEEE (2017) 746‚Äì753

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L., Farhadi,
A.:

</span>
<span class="ltx_bibblock">Target-driven visual navigation in indoor scenes using deep
reinforcement learning.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Robotics and Automation (ICRA),
IEEE (2017) 3357‚Äì3364

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.:

</span>
<span class="ltx_bibblock">Learning from simulated and unsupervised images through adversarial
training.

</span>
<span class="ltx_bibblock">In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). Volume¬†3. (2017) ¬†6

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Georgakis, G., Reza, M.A., Mousavian, A., Le, P.H., Ko≈°eck√°, J.:

</span>
<span class="ltx_bibblock">Multiview RGB-D dataset for object instance detection.

</span>
<span class="ltx_bibblock">In: International Conference on 3D Vision (3DV), IEEE
(2016) 426‚Äì434

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Georgakis, G., Reza, M.A., Mousavian, A., Le, P., Kosecka, J.:

</span>
<span class="ltx_bibblock">Multiview RGB-D dataset for object instance detection.

</span>
<span class="ltx_bibblock">CoRR (2016)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Lai, K., Bo, L., Fox, D.:

</span>
<span class="ltx_bibblock">Unsupervised feature learning for 3d scene labeling.

</span>
<span class="ltx_bibblock">In: 2014 IEEE International Conference on Robotics and Automation
(ICRA). (2014) 3050‚Äì3057

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Jakobi, N.:

</span>
<span class="ltx_bibblock">Evolutionary robotics and the radical envelope-of-noise hypothesis.

</span>
<span class="ltx_bibblock">Adaptive Behavior <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">6</span> (1997) 325‚Äì368

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Anil, C., To,
T., Cameracci, E., Boochoon, S., Birchfield, S.:

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops. (2018) 969‚Äì977

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
Andreetto, M., Adam, H.:

</span>
<span class="ltx_bibblock">Mobilenets: Efficient convolutional neural networks for mobile vision
applications.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1704.04861 (2017)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
James, S., Davison, A.J., Johns, E.:

</span>
<span class="ltx_bibblock">Transferring end-to-end visuomotor control from simulation to real
world for a multi-stage task.

</span>
<span class="ltx_bibblock">In: Conference on Robot Learning (CoRL). (2017) 334‚Äì343

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Perlin, K.:

</span>
<span class="ltx_bibblock">Improving noise.

</span>
<span class="ltx_bibblock">ACM Trans. Graph. <span id="bib.bib32.1.1" class="ltx_text ltx_font_bold">21</span> (2002) 681‚Äì682

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1807.09833" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1807.09834" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1807.09834">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1807.09834" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1807.09835" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 05:36:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
