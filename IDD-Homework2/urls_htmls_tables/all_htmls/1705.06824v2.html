<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1705.06824] Learning Convolutional Text Representations for Visual Question Answering</title><meta property="og:description" content="Visual question answering (VQA) is a recently proposed artificial
intelligence task that requires a deep understanding of both images and
texts. In deep learning, images are typically modeled through convolutional
neur…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning Convolutional Text Representations for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning Convolutional Text Representations for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1705.06824">

<!--Generated on Sat Mar 16 12:54:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %**** 110.tex Line 75 **** .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document" style="font-size:144%;">Learning Convolutional Text Representations for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhengyang Wang
</span><span class="ltx_author_notes">School of Electrical Engineering and Computer
Science at Washington State University. Email: zwang6@eecs.wsu.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuiwang Ji
</span><span class="ltx_author_notes">School of Electrical Engineering and Computer
Science at Washington State University. Email: sji@eecs.wsu.edu</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">Visual question answering (VQA) is a recently proposed artificial
intelligence task that requires a deep understanding of both images and
texts. In deep learning, images are typically modeled through convolutional
neural networks (CNNs) while texts are typically modeled through recurrent
neural networks (RNNs). In this work, we perform a detailed analysis on the
natural language questions in VQA, which raises a different need for text
representations as compared to other natural language processing tasks. Based
on the analysis, we propose to rely on CNNs for learning text
representations. By exploring various properties of CNNs specialized for text
data, we present our “CNN Inception + Gate” model for
text feature extraction in VQA. The experimental results show that simply
replacing RNNs with our CNN-based model improves question representations and thus the
overall accuracy of VQA models. In addition, our model has much fewer
parameters and the computation is much faster. We also prove that the text
representation requirement in VQA is more complicated
and comprehensive than that in conventional natural language processing
tasks. Shallow models like the fastText model, which can obtain comparable
results with deep learning models in simple tasks like text classification,
have poor performances in VQA.</span></p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Keywords</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Deep learning, visual question answering, convolutional neural networks, text representations</p>
</div>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction.</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> asks an
agent to generate an accurate answer to a natural language question that
queries an image (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This composite task involves a
variety of artificial intelligence fields, such as computer vision, natural
language processing, knowledge representation and reasoning. With the great
success of deep learning in these fields, an effective VQA agent can be built
with applications of artificial neural networks. A typical design is to use
an answer generator based on a joint representation of image and text
inputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. A considerable body of research has been
conducted on how to efficiently combine image and text
representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>,
while the fundamental question of learning these representations specifically
for VQA has not generated a lot of interests. In this work, we perform a
detailed analysis on text data in VQA and design text representation learning
methods that are appropriate for this task.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In VQA, the subtask of extracting visual information can be well addressed by
models commonly used in computer vision tasks like object
detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and image
classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, because
they share a similar requirement for image representations. Deep convolutional
neural networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have achieved significant
breakthroughs in computer vision and can be directly used in VQA. In natural
language processing, recurrent neural networks
(RNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> are widely used to learn text
representations in tasks like sentiment
classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>,
language modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and
machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Parallel to image representations, most previous VQA
models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
directly rely on RNNs to extract textual information. However, our detailed
analysis reveals some special properties of text data in VQA, which indicates
that RNNs may not be the best fit for learning text representations in VQA.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With the above analysis and insight, we propose to apply CNNs for learning
text representations in VQA. Our experiments show that a very simple
CNN-based model outperforms a RNN-based model that has much more
parameters, a result consistent with our analysis. We further
explore techniques from CNNs for images and make specialized improvements
to build more effective models. Different methods for text vectorization
are also tested and analyzed. Our best model yields a substantial
improvement as compared to VQA models with RNN-based text models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our analysis also demonstrates a higher requirement for text representations
in VQA than that in traditional text tasks. Recent study on text
classification showed that a shallow model named fastText
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> can achieve comparable accuracies with deep learning models with
much faster computation. It is speculated that simple text classification
only needs shallow representation power. To validate our analysis, we conduct
experiments on learning text representations using fastText in VQA and
observe a significant decrease in accuracy. As a result, employing deep
models to learn text representations is more appropriate.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1705.06824/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="223" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of VQA task.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background.</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we describe convolutional neural networks, recurrent
neural networks and a common design pattern of VQA models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Convolutional Neural Networks and Recurrent Neural Networks.</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Convolutional neural networks (CNNs), which apply convolutional kernels in
artificial neural networks, have outperformed many other methods in computer
vision tasks. Unlike in image data processing, where convolutional kernels
are hardwired to be primitive feature detectors, CNNs train the parameters of
kernels, deciding what kinds of features are important to specific tasks. By
stacking several convolution layers, CNNs extract a hierarchy of increasingly
high-level image features. These features are then used as inputs to a
classifier, a text generator, or a decoder, depending on tasks. CNNs are
considered as a natural choice for matrix data like images which have fixed sizes.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Recurrent neural networks (RNNs) are developed for processing sequential
data. In natural language processing, text data is naturally a type of
sequential data so that RNNs are widely used in text tasks. However, recent
studies have shown that applying CNNs on sequential data with appropriate
pooling layers is feasible and able to obtain comparable results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
In this work, we look into text data in VQA and propose a CNN-based model to
learn text representations based on our analysis and the characteristics of CNNs.
Details are discussed in Section <a href="#S3" title="3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visual Question Answering.</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Visual question answering (VQA) is considered as an advanced AI task, since
both visual and textual understanding and knowledge reasoning are needed in
VQA. Deep learning has shown its power in a variety of AI tasks. However,
training deep learning models demands a large amount of data. To this end,
various datasets aimed at VQA are collected and
published <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, a VQA dataset
(COCO-VQA) with a well-defined quantitative evaluation metric was made
available.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Most current VQA models share a similar design pattern. That is,
they consist of four basic components: an image feature extractor, a
text feature extractor, a feature combiner and a classifier. Image feature
extractors are usually pre-trained CNN-based models for image classification,
such as ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Better classification models yield better results when used in VQA models.
However, this is not the case on text side, as discussed in
Section <a href="#S3" title="3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Currently, most text feature extractors are RNNs like
LSTMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. To the best of our knowledge, only a very
simple CNN has been tried in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and, without careful
analysis and design, it only achieved similar performances as RNNs. This work
provides a wide exploration of CNN-based text feature extractors based on
detailed analysis and obtains considerably better results. For feature
combiners, most efforts in VQA research have been devoted to improving them
to get a better joint representation derived from image and text
representations. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> won the <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="2016" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mn id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">2016</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><cn type="integer" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">2016</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">2016</annotation></semantics></math> VQA challenge on
COCO-VQA by proposing the multimodal compact bilinear (MCB) pooling, which
was further improved in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In addition, the attention
mechanism has been proved to be effective as part of the combiner with its
ability to guide feature extractors to extract more related
information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Contrast to these
works, we address the more fundamental question of how to learn better text
representations specifically for VQA. Finally, as proposed in
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we can cast the VQA problem into a classification
problem, where the joint representation is used as the input to a classifier.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Text Representations in VQA.</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Analysis of Texts in VQA.</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">Natural language questions in VQA are different from other text data in
several aspects. First, people tend to ask short questions, according to
different VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. For example, the longest
question in the training set of COCO-VQA contains only <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">22</annotation></semantics></math> words, and the
average length is <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="6.2" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">6.2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="float" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">6.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">6.2</annotation></semantics></math>. Most questions have <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">4</annotation></semantics></math> to <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><cn type="integer" id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">10</annotation></semantics></math> words. Second, the
required level for text understanding in VQA differs from that in
conventional natural language processing tasks. For instance, in sentiment
analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the model only needs to tell whether
the sentiment is positive or negative. So it will focus on emotional words
but pay little attention to other contents. In VQA, however, in order to
answer a question, a comprehensive understanding is required since a question
can ask anything. As a result, text feature extractors in VQA should be more
powerful and collects comprehensive information from raw texts. Third,
questions are different from declarative sentences in terms of syntax. And in
VQA, words in a question are highly related to the contents of its
corresponding image.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Based on these properties, we argue that, as compared to RNNs, CNNs are the
better choice for text feature extraction in VQA. By analyzing how human
beings process questions, we observe that there are two keys in question
understanding: one is understanding the question type and the other is
catching objects mentioned in the question and the relationships among them.
In many cases, the question type, which is usually determined by the first a
few words, directly describes what the answer looks like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Answers to questions starting with “Is the”, “Is there”, “Do” are
typically “yes” or “no”. “What number” and “How many” questions must
have numbers as answers. Questions beginning with “What color”, “What
animal”, “What sport” and so on all explicitly indicate their answers’
categories. Meanwhile, objects and their relationships are usually nouns and
prepositional phrases, respectively. They provide guidance on locating
answer-related facts in the image, which is the fundamental
part of the attention mechanism in VQA models.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Now the task of text feature extraction becomes clear; that is, to obtain a
feature vector consisting of information about the question type and objects
being queried. To be more specific, the text representation is supposed to
extract what the starting words, nouns as well as prepositional phrases
represent. Considering words and phrases as features of text, a model
specializing on feature detection should be an appropriate choice. RNNs like LSTMs do not have explicit feature detection units. In
contrast to convolutional connections in CNNs, the connections within and
between units in RNNs are mostly fully-connected.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">To summarize, CNNs are conceptually more appropriate as text feature
extractors in VQA, which is also validated by our experiments. Additional
advantages provided by CNNs are fewer parameters and easy parallelization,
which accelerate training and testing and reduce the risk of
over-fitting.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transforming Text Data.</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">A challenge of applying CNNs on text data is how to convert raw texts in a
format that CNNs can take, as they are originally designed for fixed-size
matrix data like images. To apply CNNs on texts directly, we need to
represent text data in the same way as how image data are represented. An
image is typically stored as a <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">3</annotation></semantics></math>-dimensional tensor, where the three
dimensions correspond to height, width, and number of channels, respectively.
Each pixel of the image is represented as a <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">n</annotation></semantics></math>-component vector
corresponding to <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">n</annotation></semantics></math> channels.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Inspired by the bag-of-words model in natural language processing, a
vocabulary is first built. The vocabulary can be either word-based that
contains words appearing in the texts, or character-based, which is fixed for
a particular language. It is also reasonable for the vocabulary to include
punctuation as single words or characters. With the vocabulary, each sentence
can be transformed into an pseudo image whose height equals to <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">1</annotation></semantics></math> and width
is defined based on the vocabulary. For word-level representations, the width
is number of words in a sentence; for character-level representations, we
count the number of characters. For the third dimension, similar to pixels in
an image, if we can convert each word as a vector, the length of the vector
is the number of channels. The problem is then reduced to
word vectorization, which is usually done by one-hot vectorization.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.12" class="ltx_p">To make it concrete, we take the word-based vocabulary as an example, and the
character-based case can be easily generalized in Section <a href="#S3.SS3" title="3.3 Word-Based versus Character-Based Representations. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
Given a vocabulary <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">V</annotation></semantics></math>, each word can be represented as a one-hot vector;
namely a <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.2.2" xref="S3.SS2.p3.2.m2.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.1.2.2.1" xref="S3.SS2.p3.2.m2.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS2.p3.2.m2.1.2.2.2" xref="S3.SS2.p3.2.m2.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.2.2"><abs id="S3.SS2.p3.2.m2.1.2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.2.2.1"></abs><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">|V|</annotation></semantics></math>-component vector with one <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mn id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><cn type="integer" id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">1</annotation></semantics></math> at the position corresponding to
the index of the word in <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">V</annotation></semantics></math> and <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mn id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><cn type="integer" id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">0</cn></annotation-xml></semantics></math>s for other entries, where <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.2.2" xref="S3.SS2.p3.6.m6.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.2.1" xref="S3.SS2.p3.6.m6.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.2.2" xref="S3.SS2.p3.6.m6.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.2.1.cmml" xref="S3.SS2.p3.6.m6.1.2.2"><abs id="S3.SS2.p3.6.m6.1.2.1.1.cmml" xref="S3.SS2.p3.6.m6.1.2.2.1"></abs><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">|V|</annotation></semantics></math> is the
size of <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">V</annotation></semantics></math>. With one-hot vectorization, the number of channels becomes
<math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mrow id="S3.SS2.p3.8.m8.1.2.2" xref="S3.SS2.p3.8.m8.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.8.m8.1.2.2.1" xref="S3.SS2.p3.8.m8.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS2.p3.8.m8.1.2.2.2" xref="S3.SS2.p3.8.m8.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.2.1.cmml" xref="S3.SS2.p3.8.m8.1.2.2"><abs id="S3.SS2.p3.8.m8.1.2.1.1.cmml" xref="S3.SS2.p3.8.m8.1.2.2.1"></abs><ci id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">|V|</annotation></semantics></math>. As a result, a sentence with <math id="S3.SS2.p3.9.m9.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p3.9.m9.1a"><mi id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><ci id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">L</annotation></semantics></math> words is treated as a <math id="S3.SS2.p3.10.m10.1" class="ltx_Math" alttext="1\times L" display="inline"><semantics id="S3.SS2.p3.10.m10.1a"><mrow id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml"><mn id="S3.SS2.p3.10.m10.1.1.2" xref="S3.SS2.p3.10.m10.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.10.m10.1.1.1" xref="S3.SS2.p3.10.m10.1.1.1.cmml">×</mo><mi id="S3.SS2.p3.10.m10.1.1.3" xref="S3.SS2.p3.10.m10.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.1b"><apply id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1"><times id="S3.SS2.p3.10.m10.1.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1.1"></times><cn type="integer" id="S3.SS2.p3.10.m10.1.1.2.cmml" xref="S3.SS2.p3.10.m10.1.1.2">1</cn><ci id="S3.SS2.p3.10.m10.1.1.3.cmml" xref="S3.SS2.p3.10.m10.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.1c">1\times L</annotation></semantics></math>
pseudo image with <math id="S3.SS2.p3.11.m11.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS2.p3.11.m11.1a"><mrow id="S3.SS2.p3.11.m11.1.2.2" xref="S3.SS2.p3.11.m11.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.11.m11.1.2.2.1" xref="S3.SS2.p3.11.m11.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p3.11.m11.1.1" xref="S3.SS2.p3.11.m11.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS2.p3.11.m11.1.2.2.2" xref="S3.SS2.p3.11.m11.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m11.1b"><apply id="S3.SS2.p3.11.m11.1.2.1.cmml" xref="S3.SS2.p3.11.m11.1.2.2"><abs id="S3.SS2.p3.11.m11.1.2.1.1.cmml" xref="S3.SS2.p3.11.m11.1.2.2.1"></abs><ci id="S3.SS2.p3.11.m11.1.1.cmml" xref="S3.SS2.p3.11.m11.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m11.1c">|V|</annotation></semantics></math> channels, and it can be given into CNNs directly by
modifying the height of convolutional kernels into <math id="S3.SS2.p3.12.m12.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p3.12.m12.1a"><mn id="S3.SS2.p3.12.m12.1.1" xref="S3.SS2.p3.12.m12.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m12.1b"><cn type="integer" id="S3.SS2.p3.12.m12.1.1.cmml" xref="S3.SS2.p3.12.m12.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m12.1c">1</annotation></semantics></math> correspondingly.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.6" class="ltx_p">While one-hot embedding works well as inputs to CNNs in some
cases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, it is usually preferable to have a lower
dimensional embedding with two primary reasons. First, if <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.2.2" xref="S3.SS2.p4.1.m1.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p4.1.m1.1.2.2.1" xref="S3.SS2.p4.1.m1.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS2.p4.1.m1.1.2.2.2" xref="S3.SS2.p4.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.2.1.cmml" xref="S3.SS2.p4.1.m1.1.2.2"><abs id="S3.SS2.p4.1.m1.1.2.1.1.cmml" xref="S3.SS2.p4.1.m1.1.2.2.1"></abs><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">|V|</annotation></semantics></math> is large,
which is usually the case for word-based vocabulary, computation efficiency
is low due to the sparsity and high dimensionality of inputs. Second, one-hot
embedding is semantically meaningless. Thus, an extra embedding layer is
usually inserted before CNNs. This layer maps the <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.2.2" xref="S3.SS2.p4.2.m2.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p4.2.m2.1.2.2.1" xref="S3.SS2.p4.2.m2.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS2.p4.2.m2.1.2.2.2" xref="S3.SS2.p4.2.m2.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.2.1.cmml" xref="S3.SS2.p4.2.m2.1.2.2"><abs id="S3.SS2.p4.2.m2.1.2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.2.2.1"></abs><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">|V|</annotation></semantics></math>-component vectors
into <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">d</annotation></semantics></math>-component vectors, where <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">d</annotation></semantics></math> is much smaller than
<math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><mrow id="S3.SS2.p4.5.m5.1.2.2" xref="S3.SS2.p4.5.m5.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p4.5.m5.1.2.2.1" xref="S3.SS2.p4.5.m5.1.2.1.1.cmml">|</mo><mi id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS2.p4.5.m5.1.2.2.2" xref="S3.SS2.p4.5.m5.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.2.1.cmml" xref="S3.SS2.p4.5.m5.1.2.2"><abs id="S3.SS2.p4.5.m5.1.2.1.1.cmml" xref="S3.SS2.p4.5.m5.1.2.2.1"></abs><ci id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">|V|</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The embedding layer is
basically a multiplication of one-hot vectors with a <math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="|V|\times d" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><mrow id="S3.SS2.p4.6.m6.1.2" xref="S3.SS2.p4.6.m6.1.2.cmml"><mrow id="S3.SS2.p4.6.m6.1.2.2.2" xref="S3.SS2.p4.6.m6.1.2.2.1.cmml"><mo stretchy="false" id="S3.SS2.p4.6.m6.1.2.2.2.1" xref="S3.SS2.p4.6.m6.1.2.2.1.1.cmml">|</mo><mi id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">V</mi><mo rspace="0.055em" stretchy="false" id="S3.SS2.p4.6.m6.1.2.2.2.2" xref="S3.SS2.p4.6.m6.1.2.2.1.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S3.SS2.p4.6.m6.1.2.1" xref="S3.SS2.p4.6.m6.1.2.1.cmml">×</mo><mi id="S3.SS2.p4.6.m6.1.2.3" xref="S3.SS2.p4.6.m6.1.2.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><apply id="S3.SS2.p4.6.m6.1.2.cmml" xref="S3.SS2.p4.6.m6.1.2"><times id="S3.SS2.p4.6.m6.1.2.1.cmml" xref="S3.SS2.p4.6.m6.1.2.1"></times><apply id="S3.SS2.p4.6.m6.1.2.2.1.cmml" xref="S3.SS2.p4.6.m6.1.2.2.2"><abs id="S3.SS2.p4.6.m6.1.2.2.1.1.cmml" xref="S3.SS2.p4.6.m6.1.2.2.2.1"></abs><ci id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">𝑉</ci></apply><ci id="S3.SS2.p4.6.m6.1.2.3.cmml" xref="S3.SS2.p4.6.m6.1.2.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">|V|\times d</annotation></semantics></math> matrix to
perform a look-up operation. The embedding matrix can be trained as part of
the networks, which are task-specialized, or can be pre-trained using word
embedding like Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> or
GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Transforming Text Data. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides
a complete view of the transformations.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1705.06824/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="460" height="362" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration on employing CNNs to learn text
representations. Given a word-based vocabulary <math id="S3.F2.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.F2.1.m1.1b"><mi id="S3.F2.1.m1.1.1" xref="S3.F2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.F2.1.m1.1c"><ci id="S3.F2.1.m1.1.1.cmml" xref="S3.F2.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.1.m1.1d">V</annotation></semantics></math>, we first transform the
<math id="S3.F2.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.F2.2.m2.1b"><mi id="S3.F2.2.m2.1.1" xref="S3.F2.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.m2.1c"><ci id="S3.F2.2.m2.1.1.cmml" xref="S3.F2.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m2.1d">L</annotation></semantics></math>-word sentence into a <math id="S3.F2.3.m3.1" class="ltx_Math" alttext="1\times L" display="inline"><semantics id="S3.F2.3.m3.1b"><mrow id="S3.F2.3.m3.1.1" xref="S3.F2.3.m3.1.1.cmml"><mn id="S3.F2.3.m3.1.1.2" xref="S3.F2.3.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F2.3.m3.1.1.1" xref="S3.F2.3.m3.1.1.1.cmml">×</mo><mi id="S3.F2.3.m3.1.1.3" xref="S3.F2.3.m3.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.3.m3.1c"><apply id="S3.F2.3.m3.1.1.cmml" xref="S3.F2.3.m3.1.1"><times id="S3.F2.3.m3.1.1.1.cmml" xref="S3.F2.3.m3.1.1.1"></times><cn type="integer" id="S3.F2.3.m3.1.1.2.cmml" xref="S3.F2.3.m3.1.1.2">1</cn><ci id="S3.F2.3.m3.1.1.3.cmml" xref="S3.F2.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m3.1d">1\times L</annotation></semantics></math> image with <math id="S3.F2.4.m4.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.F2.4.m4.1b"><mrow id="S3.F2.4.m4.1.2.2" xref="S3.F2.4.m4.1.2.1.cmml"><mo stretchy="false" id="S3.F2.4.m4.1.2.2.1" xref="S3.F2.4.m4.1.2.1.1.cmml">|</mo><mi id="S3.F2.4.m4.1.1" xref="S3.F2.4.m4.1.1.cmml">V</mi><mo stretchy="false" id="S3.F2.4.m4.1.2.2.2" xref="S3.F2.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.4.m4.1c"><apply id="S3.F2.4.m4.1.2.1.cmml" xref="S3.F2.4.m4.1.2.2"><abs id="S3.F2.4.m4.1.2.1.1.cmml" xref="S3.F2.4.m4.1.2.2.1"></abs><ci id="S3.F2.4.m4.1.1.cmml" xref="S3.F2.4.m4.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m4.1d">|V|</annotation></semantics></math> channels by one-hot
vectorization. The blue units represent <math id="S3.F2.5.m5.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.F2.5.m5.1b"><mn id="S3.F2.5.m5.1.1" xref="S3.F2.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.F2.5.m5.1c"><cn type="integer" id="S3.F2.5.m5.1.1.cmml" xref="S3.F2.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m5.1d">1</annotation></semantics></math> and white units represent <math id="S3.F2.6.m6.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.F2.6.m6.1b"><mn id="S3.F2.6.m6.1.1" xref="S3.F2.6.m6.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.F2.6.m6.1c"><cn type="integer" id="S3.F2.6.m6.1.1.cmml" xref="S3.F2.6.m6.1.1">0</cn></annotation-xml></semantics></math> for
this layer. Through the embedding layer, the number of channels is reduced to
<math id="S3.F2.7.m7.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.F2.7.m7.1b"><mi id="S3.F2.7.m7.1.1" xref="S3.F2.7.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.F2.7.m7.1c"><ci id="S3.F2.7.m7.1.1.cmml" xref="S3.F2.7.m7.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.m7.1d">d</annotation></semantics></math> (Section <a href="#S3.SS2" title="3.2 Transforming Text Data. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Then CNNs with <math id="S3.F2.8.m8.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.F2.8.m8.1b"><mn id="S3.F2.8.m8.1.1" xref="S3.F2.8.m8.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.F2.8.m8.1c"><cn type="integer" id="S3.F2.8.m8.1.1.cmml" xref="S3.F2.8.m8.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.m8.1d">1</annotation></semantics></math>-D kernel can be directly
applied. After convolution, a max-pooling over the whole sentence is
performed to provide fix-sized inputs for the classifier
(Section <a href="#S3.SS4" title="3.4 Handling Variable-Length Inputs. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>). Wider and deeper convolution layers can be added
to this module easily.
The part in the dotted box illustrates the case where character-based
vocabulary is used to supplement word embedding vectors
(Section <a href="#S3.SS3" title="3.3 Word-Based versus Character-Based Representations. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). In addition to the word-based vocabulary, a
character-based vocabulary <math id="S3.F2.9.m9.1" class="ltx_Math" alttext="V\_c" display="inline"><semantics id="S3.F2.9.m9.1b"><mrow id="S3.F2.9.m9.1.1" xref="S3.F2.9.m9.1.1.cmml"><mi id="S3.F2.9.m9.1.1.2" xref="S3.F2.9.m9.1.1.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.F2.9.m9.1.1.1" xref="S3.F2.9.m9.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.F2.9.m9.1.1.3" xref="S3.F2.9.m9.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F2.9.m9.1.1.1b" xref="S3.F2.9.m9.1.1.1.cmml">​</mo><mi id="S3.F2.9.m9.1.1.4" xref="S3.F2.9.m9.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.9.m9.1c"><apply id="S3.F2.9.m9.1.1.cmml" xref="S3.F2.9.m9.1.1"><times id="S3.F2.9.m9.1.1.1.cmml" xref="S3.F2.9.m9.1.1.1"></times><ci id="S3.F2.9.m9.1.1.2.cmml" xref="S3.F2.9.m9.1.1.2">𝑉</ci><ci id="S3.F2.9.m9.1.1.3.cmml" xref="S3.F2.9.m9.1.1.3">_</ci><ci id="S3.F2.9.m9.1.1.4.cmml" xref="S3.F2.9.m9.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.m9.1d">V\_c</annotation></semantics></math> is provided. It transforms the
<math id="S3.F2.10.m10.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.F2.10.m10.1b"><mi id="S3.F2.10.m10.1.1" xref="S3.F2.10.m10.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.F2.10.m10.1c"><ci id="S3.F2.10.m10.1.1.cmml" xref="S3.F2.10.m10.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.m10.1d">C</annotation></semantics></math>-character word into a <math id="S3.F2.11.m11.1" class="ltx_Math" alttext="1\times C" display="inline"><semantics id="S3.F2.11.m11.1b"><mrow id="S3.F2.11.m11.1.1" xref="S3.F2.11.m11.1.1.cmml"><mn id="S3.F2.11.m11.1.1.2" xref="S3.F2.11.m11.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F2.11.m11.1.1.1" xref="S3.F2.11.m11.1.1.1.cmml">×</mo><mi id="S3.F2.11.m11.1.1.3" xref="S3.F2.11.m11.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.11.m11.1c"><apply id="S3.F2.11.m11.1.1.cmml" xref="S3.F2.11.m11.1.1"><times id="S3.F2.11.m11.1.1.1.cmml" xref="S3.F2.11.m11.1.1.1"></times><cn type="integer" id="S3.F2.11.m11.1.1.2.cmml" xref="S3.F2.11.m11.1.1.2">1</cn><ci id="S3.F2.11.m11.1.1.3.cmml" xref="S3.F2.11.m11.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.11.m11.1d">1\times C</annotation></semantics></math> image with <math id="S3.F2.12.m12.1" class="ltx_Math" alttext="|V\_c|" display="inline"><semantics id="S3.F2.12.m12.1b"><mrow id="S3.F2.12.m12.1.1.1" xref="S3.F2.12.m12.1.1.2.cmml"><mo stretchy="false" id="S3.F2.12.m12.1.1.1.2" xref="S3.F2.12.m12.1.1.2.1.cmml">|</mo><mrow id="S3.F2.12.m12.1.1.1.1" xref="S3.F2.12.m12.1.1.1.1.cmml"><mi id="S3.F2.12.m12.1.1.1.1.2" xref="S3.F2.12.m12.1.1.1.1.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.F2.12.m12.1.1.1.1.1" xref="S3.F2.12.m12.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.F2.12.m12.1.1.1.1.3" xref="S3.F2.12.m12.1.1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F2.12.m12.1.1.1.1.1b" xref="S3.F2.12.m12.1.1.1.1.1.cmml">​</mo><mi id="S3.F2.12.m12.1.1.1.1.4" xref="S3.F2.12.m12.1.1.1.1.4.cmml">c</mi></mrow><mo stretchy="false" id="S3.F2.12.m12.1.1.1.3" xref="S3.F2.12.m12.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.12.m12.1c"><apply id="S3.F2.12.m12.1.1.2.cmml" xref="S3.F2.12.m12.1.1.1"><abs id="S3.F2.12.m12.1.1.2.1.cmml" xref="S3.F2.12.m12.1.1.1.2"></abs><apply id="S3.F2.12.m12.1.1.1.1.cmml" xref="S3.F2.12.m12.1.1.1.1"><times id="S3.F2.12.m12.1.1.1.1.1.cmml" xref="S3.F2.12.m12.1.1.1.1.1"></times><ci id="S3.F2.12.m12.1.1.1.1.2.cmml" xref="S3.F2.12.m12.1.1.1.1.2">𝑉</ci><ci id="S3.F2.12.m12.1.1.1.1.3.cmml" xref="S3.F2.12.m12.1.1.1.1.3">_</ci><ci id="S3.F2.12.m12.1.1.1.1.4.cmml" xref="S3.F2.12.m12.1.1.1.1.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.m12.1d">|V\_c|</annotation></semantics></math> channels through
the same transformation. Then the embedding layer changes the number of
channels to <math id="S3.F2.13.m13.1" class="ltx_Math" alttext="d\_c" display="inline"><semantics id="S3.F2.13.m13.1b"><mrow id="S3.F2.13.m13.1.1" xref="S3.F2.13.m13.1.1.cmml"><mi id="S3.F2.13.m13.1.1.2" xref="S3.F2.13.m13.1.1.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.F2.13.m13.1.1.1" xref="S3.F2.13.m13.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.F2.13.m13.1.1.3" xref="S3.F2.13.m13.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F2.13.m13.1.1.1b" xref="S3.F2.13.m13.1.1.1.cmml">​</mo><mi id="S3.F2.13.m13.1.1.4" xref="S3.F2.13.m13.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.13.m13.1c"><apply id="S3.F2.13.m13.1.1.cmml" xref="S3.F2.13.m13.1.1"><times id="S3.F2.13.m13.1.1.1.cmml" xref="S3.F2.13.m13.1.1.1"></times><ci id="S3.F2.13.m13.1.1.2.cmml" xref="S3.F2.13.m13.1.1.2">𝑑</ci><ci id="S3.F2.13.m13.1.1.3.cmml" xref="S3.F2.13.m13.1.1.3">_</ci><ci id="S3.F2.13.m13.1.1.4.cmml" xref="S3.F2.13.m13.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.13.m13.1d">d\_c</annotation></semantics></math>. A CNN-based module followed by a global max-pooling
generates a word embedding, which is then concatenated to the word embedding
obtained from word-based vocabulary, generating the final pseudo image with
<math id="S3.F2.14.m14.1" class="ltx_Math" alttext="d+d\_c" display="inline"><semantics id="S3.F2.14.m14.1b"><mrow id="S3.F2.14.m14.1.1" xref="S3.F2.14.m14.1.1.cmml"><mi id="S3.F2.14.m14.1.1.2" xref="S3.F2.14.m14.1.1.2.cmml">d</mi><mo id="S3.F2.14.m14.1.1.1" xref="S3.F2.14.m14.1.1.1.cmml">+</mo><mrow id="S3.F2.14.m14.1.1.3" xref="S3.F2.14.m14.1.1.3.cmml"><mi id="S3.F2.14.m14.1.1.3.2" xref="S3.F2.14.m14.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.F2.14.m14.1.1.3.1" xref="S3.F2.14.m14.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S3.F2.14.m14.1.1.3.3" xref="S3.F2.14.m14.1.1.3.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F2.14.m14.1.1.3.1b" xref="S3.F2.14.m14.1.1.3.1.cmml">​</mo><mi id="S3.F2.14.m14.1.1.3.4" xref="S3.F2.14.m14.1.1.3.4.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.14.m14.1c"><apply id="S3.F2.14.m14.1.1.cmml" xref="S3.F2.14.m14.1.1"><plus id="S3.F2.14.m14.1.1.1.cmml" xref="S3.F2.14.m14.1.1.1"></plus><ci id="S3.F2.14.m14.1.1.2.cmml" xref="S3.F2.14.m14.1.1.2">𝑑</ci><apply id="S3.F2.14.m14.1.1.3.cmml" xref="S3.F2.14.m14.1.1.3"><times id="S3.F2.14.m14.1.1.3.1.cmml" xref="S3.F2.14.m14.1.1.3.1"></times><ci id="S3.F2.14.m14.1.1.3.2.cmml" xref="S3.F2.14.m14.1.1.3.2">𝑑</ci><ci id="S3.F2.14.m14.1.1.3.3.cmml" xref="S3.F2.14.m14.1.1.3.3">_</ci><ci id="S3.F2.14.m14.1.1.3.4.cmml" xref="S3.F2.14.m14.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.14.m14.1d">d+d\_c</annotation></semantics></math> channels. Note that here the CNN module is shared among different
words.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Word-Based versus Character-Based Representations.</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Note that once a vocabulary is built, the remaining process to transform
texts follows the same path for different vocabularies. It is clear that the
vocabulary <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">V</annotation></semantics></math> defines the pixels in the pseudo image. In the above example,
each word becomes a pixel. If the vocabulary is character-based, each
character, including space character and single punctuation, will be a pixel.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The main advantage of character-based vocabulary is that it produces much
longer inputs. This makes it possible for using deeper models. For long
texts, transforming text data using character-based vocabulary and applying
very deep CNNs leads to impressive
performances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Another advantage is
that characters may include knowledge about how to form words. However, for
short texts, the size of the transformed data is still small even with
character-based vocabulary. Our experiments show that effective models for
long texts with character-based vocabulary fail to obtain high performances
in VQA (Section <a href="#S3.SS5" title="3.5 Deeper Networks for Short Texts. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>). It is believed that the inputs are too short
for the models to learn that space is the delimiter for words,
which is naturally given in word-based vocabulary case.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">A combination of character-based and word-based vocabularies for short tests
has been explored in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and achieved comparable results. In
this method, characters corresponding to each word are grouped together. Each
group of characters is transformed by character-based vocabulary and then fed
into a smaller model to generate a word vector. The word vector is then
concatenated with the corresponding word embedding from word-based vocabulary
to form a larger word representation. More details are given in
Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Transforming Text Data. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This method is also explored in our experiments.
Nevertheless, character-based vocabulary does not seem to be helpful.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Handling Variable-Length Inputs.</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Another problem for text data is that each sentence is composed of different
numbers of words, which leads to variable-sized inputs and outputs of
convolution layers. However, the outputs of the whole CNN module are expected
to be fixed-sized, in order to serve as inputs to next module. Moreover, the
sizes of inputs to CNNs should also be consistent in consideration of
training.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.4" class="ltx_p">Inspired by the pooling layers in CNNs for images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>,
several pooling layers specialized for text data of variable lengths have
been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. We adopt the
method that applies one pooling for the whole sentence and selects the <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">k</annotation></semantics></math>
largest values instead of performing pooling locally. This is called <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">k</annotation></semantics></math>-max
pooling. By fixing <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">k</annotation></semantics></math> for the last pooling layer of CNNs, the requirement
for fixed-sized outputs is satisfied. If <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="k=1" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mrow id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">k</mi><mo id="S3.SS4.p2.4.m4.1.1.1" xref="S3.SS4.p2.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><eq id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1.1"></eq><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">𝑘</ci><cn type="integer" id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">k=1</annotation></semantics></math>, it results in a global
max-pooling. More details are given in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Transforming Text Data. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">While pooling layers can provide fixed-sized outputs regardless of the size
of inputs, fixed-sized inputs are also desired due to mini-batch training.
The solution is to perform padding and cropping. Cropping is usually used in
the case of long texts, especially with character-based vocabulary, which
simply cuts the part longer than a fixed length. For short texts like
questions in VQA, zero padding is typically used to pad each input to the
same length of the longest ones. This involves a problem that we only know
the longest length in the training set while there can be longer data. Thus
in practice, a combination of padding and cropping is used during testing.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Deeper Networks for Short Texts.</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">For long texts and images, deeper networks are important and beneficial.
Obstacles on going deeper are that very deep networks become hard to train
and suffer from the degradation problem. Residual networks
(ResNet) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> overcame these obstacles by adding skip
connections from inputs to outputs of one layer or several layers. These skip
connections are named residual connections. They enable CNNs with hundreds of
layers to be trained efficiently and avoid the accuracy saturation problem.
Modified ResNet with <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="49" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mn id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">49</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><cn type="integer" id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">49</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">49</annotation></semantics></math> layers for long texts has been explored in text
classification with character-based vocabulary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.2" class="ltx_p">We experiment with a ResNet with <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mn id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><cn type="integer" id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">8</annotation></semantics></math> layers on texts in VQA with
character-based vocabulary. The results indicate that the inputs are too
short, and deeper networks suffer from over-fitting instead of training and
degradation problems. In fact, comparing to long texts where most samples
have more than <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mn id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><cn type="integer" id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">1000</annotation></semantics></math> characters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and multi-layer CNNs
work well, the length of texts in VQA is not enough for obtaining promising
outcomes from multi-layer CNNs. We also explore adding one residual block to
simple one-layer models but it also hurts the performances. It turns out
that, unlike mappings learned by intermediate layers in very deep models, the
mappings learned by the text feature extractor in VQA is not similar to
identity function, making the application of skip connections inappropriate.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">These observations imply that CNNs on texts in VQA should not be deep. Our
experiments show that one-layer models achieved better performances.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Wider Networks through Inception Modules.</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Inception modules, proposed
by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, involve combining
convolutional kernels of different sizes in one convolution layer.
This technique enables wider convolution layers. The motivation for
using inception modules for texts is straight-forward; that is,
different-sized kernels extract features from phrases of different
lengths. Based on this interpretation, the choice of the number of
kernels and their corresponding sizes should be data-dependent,
because different-sized phrases may have diverse importance in
various text data. We explore the settings and several improvements
in our experiments.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Gated Convolutional Units.</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.5" class="ltx_p">LSTMs and GRUs improve RNNs by adding gates to control information
flow. In particular, the output gate controls information flow along
the sequential dimension. With this functionality, the output gate
can be used on any deep learning models.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> an output gate is also applied on CNNs.
Unlike LSTMs and GRUs that use fully-connected connections,
convolutional connections are used when generating output gates in
CNNs. Given an input to CNNs, which in our case is the transformed
data <math id="S3.SS7.p1.1.m1.1" class="ltx_Math" alttext="I\in\mathbb{R}^{1\times L\times d}" display="inline"><semantics id="S3.SS7.p1.1.m1.1a"><mrow id="S3.SS7.p1.1.m1.1.1" xref="S3.SS7.p1.1.m1.1.1.cmml"><mi id="S3.SS7.p1.1.m1.1.1.2" xref="S3.SS7.p1.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS7.p1.1.m1.1.1.1" xref="S3.SS7.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS7.p1.1.m1.1.1.3" xref="S3.SS7.p1.1.m1.1.1.3.cmml"><mi id="S3.SS7.p1.1.m1.1.1.3.2" xref="S3.SS7.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS7.p1.1.m1.1.1.3.3" xref="S3.SS7.p1.1.m1.1.1.3.3.cmml"><mn id="S3.SS7.p1.1.m1.1.1.3.3.2" xref="S3.SS7.p1.1.m1.1.1.3.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS7.p1.1.m1.1.1.3.3.1" xref="S3.SS7.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS7.p1.1.m1.1.1.3.3.3" xref="S3.SS7.p1.1.m1.1.1.3.3.3.cmml">L</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS7.p1.1.m1.1.1.3.3.1a" xref="S3.SS7.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS7.p1.1.m1.1.1.3.3.4" xref="S3.SS7.p1.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.1.m1.1b"><apply id="S3.SS7.p1.1.m1.1.1.cmml" xref="S3.SS7.p1.1.m1.1.1"><in id="S3.SS7.p1.1.m1.1.1.1.cmml" xref="S3.SS7.p1.1.m1.1.1.1"></in><ci id="S3.SS7.p1.1.m1.1.1.2.cmml" xref="S3.SS7.p1.1.m1.1.1.2">𝐼</ci><apply id="S3.SS7.p1.1.m1.1.1.3.cmml" xref="S3.SS7.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS7.p1.1.m1.1.1.3.1.cmml" xref="S3.SS7.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS7.p1.1.m1.1.1.3.2.cmml" xref="S3.SS7.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS7.p1.1.m1.1.1.3.3.cmml" xref="S3.SS7.p1.1.m1.1.1.3.3"><times id="S3.SS7.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS7.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S3.SS7.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS7.p1.1.m1.1.1.3.3.2">1</cn><ci id="S3.SS7.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS7.p1.1.m1.1.1.3.3.3">𝐿</ci><ci id="S3.SS7.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS7.p1.1.m1.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.1.m1.1c">I\in\mathbb{R}^{1\times L\times d}</annotation></semantics></math> from text data, two
independent <math id="S3.SS7.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS7.p1.2.m2.1a"><mn id="S3.SS7.p1.2.m2.1.1" xref="S3.SS7.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.2.m2.1b"><cn type="integer" id="S3.SS7.p1.2.m2.1.1.cmml" xref="S3.SS7.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.2.m2.1c">1</annotation></semantics></math>-D convolutional kernels <math id="S3.SS7.p1.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS7.p1.3.m3.1a"><mi id="S3.SS7.p1.3.m3.1.1" xref="S3.SS7.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.3.m3.1b"><ci id="S3.SS7.p1.3.m3.1.1.cmml" xref="S3.SS7.p1.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.3.m3.1c">K</annotation></semantics></math> and <math id="S3.SS7.p1.4.m4.1" class="ltx_Math" alttext="K_{g}" display="inline"><semantics id="S3.SS7.p1.4.m4.1a"><msub id="S3.SS7.p1.4.m4.1.1" xref="S3.SS7.p1.4.m4.1.1.cmml"><mi id="S3.SS7.p1.4.m4.1.1.2" xref="S3.SS7.p1.4.m4.1.1.2.cmml">K</mi><mi id="S3.SS7.p1.4.m4.1.1.3" xref="S3.SS7.p1.4.m4.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.4.m4.1b"><apply id="S3.SS7.p1.4.m4.1.1.cmml" xref="S3.SS7.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS7.p1.4.m4.1.1.1.cmml" xref="S3.SS7.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS7.p1.4.m4.1.1.2.cmml" xref="S3.SS7.p1.4.m4.1.1.2">𝐾</ci><ci id="S3.SS7.p1.4.m4.1.1.3.cmml" xref="S3.SS7.p1.4.m4.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.4.m4.1c">K_{g}</annotation></semantics></math> are used to
form the output <math id="S3.SS7.p1.5.m5.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S3.SS7.p1.5.m5.1a"><mi id="S3.SS7.p1.5.m5.1.1" xref="S3.SS7.p1.5.m5.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.5.m5.1b"><ci id="S3.SS7.p1.5.m5.1.1.cmml" xref="S3.SS7.p1.5.m5.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.5.m5.1c">O</annotation></semantics></math> of the convolution layer as follows:</p>
<table id="Sx2.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3.1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle g" display="inline"><semantics id="S3.E1.m1.1a"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle g</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S3.E1.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S3.E1.m2.1a"><mo id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b"><eq id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m3.1" class="ltx_Math" alttext="\displaystyle\sigma(K_{g}\ast I+b_{g})," display="inline"><semantics id="S3.E1.m3.1a"><mrow id="S3.E1.m3.1.1.1" xref="S3.E1.m3.1.1.1.1.cmml"><mrow id="S3.E1.m3.1.1.1.1" xref="S3.E1.m3.1.1.1.1.cmml"><mi id="S3.E1.m3.1.1.1.1.3" xref="S3.E1.m3.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m3.1.1.1.1.2" xref="S3.E1.m3.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m3.1.1.1.1.1.1" xref="S3.E1.m3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m3.1.1.1.1.1.1.2" xref="S3.E1.m3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m3.1.1.1.1.1.1.1" xref="S3.E1.m3.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m3.1.1.1.1.1.1.1.2" xref="S3.E1.m3.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E1.m3.1.1.1.1.1.1.1.2.2" xref="S3.E1.m3.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m3.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m3.1.1.1.1.1.1.1.2.2.2.cmml">K</mi><mi id="S3.E1.m3.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m3.1.1.1.1.1.1.1.2.2.3.cmml">g</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m3.1.1.1.1.1.1.1.2.1" xref="S3.E1.m3.1.1.1.1.1.1.1.2.1.cmml">∗</mo><mi id="S3.E1.m3.1.1.1.1.1.1.1.2.3" xref="S3.E1.m3.1.1.1.1.1.1.1.2.3.cmml">I</mi></mrow><mo id="S3.E1.m3.1.1.1.1.1.1.1.1" xref="S3.E1.m3.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E1.m3.1.1.1.1.1.1.1.3" xref="S3.E1.m3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m3.1.1.1.1.1.1.1.3.2" xref="S3.E1.m3.1.1.1.1.1.1.1.3.2.cmml">b</mi><mi id="S3.E1.m3.1.1.1.1.1.1.1.3.3" xref="S3.E1.m3.1.1.1.1.1.1.1.3.3.cmml">g</mi></msub></mrow><mo stretchy="false" id="S3.E1.m3.1.1.1.1.1.1.3" xref="S3.E1.m3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m3.1.1.1.2" xref="S3.E1.m3.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m3.1b"><apply id="S3.E1.m3.1.1.1.1.cmml" xref="S3.E1.m3.1.1.1"><times id="S3.E1.m3.1.1.1.1.2.cmml" xref="S3.E1.m3.1.1.1.1.2"></times><ci id="S3.E1.m3.1.1.1.1.3.cmml" xref="S3.E1.m3.1.1.1.1.3">𝜎</ci><apply id="S3.E1.m3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m3.1.1.1.1.1.1"><plus id="S3.E1.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.1"></plus><apply id="S3.E1.m3.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.2"><ci id="S3.E1.m3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.2.1">∗</ci><apply id="S3.E1.m3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m3.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m3.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.2.2.2">𝐾</ci><ci id="S3.E1.m3.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.2.2.3">𝑔</ci></apply><ci id="S3.E1.m3.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.2.3">𝐼</ci></apply><apply id="S3.E1.m3.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S3.E1.m3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m3.1.1.1.1.1.1.1.3.3">𝑔</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m3.1c">\displaystyle\sigma(K_{g}\ast I+b_{g}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3.2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle O" display="inline"><semantics id="S3.E2.m1.1a"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle O</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S3.E2.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S3.E2.m2.1a"><mo id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><eq id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m3.2" class="ltx_Math" alttext="\displaystyle g\odot\tanh(K\ast I+b)," display="inline"><semantics id="S3.E2.m3.2a"><mrow id="S3.E2.m3.2.2.1" xref="S3.E2.m3.2.2.1.1.cmml"><mrow id="S3.E2.m3.2.2.1.1" xref="S3.E2.m3.2.2.1.1.cmml"><mi id="S3.E2.m3.2.2.1.1.3" xref="S3.E2.m3.2.2.1.1.3.cmml">g</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m3.2.2.1.1.2" xref="S3.E2.m3.2.2.1.1.2.cmml">⊙</mo><mrow id="S3.E2.m3.2.2.1.1.1.1" xref="S3.E2.m3.2.2.1.1.1.2.cmml"><mi id="S3.E2.m3.1.1" xref="S3.E2.m3.1.1.cmml">tanh</mi><mo id="S3.E2.m3.2.2.1.1.1.1a" xref="S3.E2.m3.2.2.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m3.2.2.1.1.1.1.1" xref="S3.E2.m3.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m3.2.2.1.1.1.1.1.2" xref="S3.E2.m3.2.2.1.1.1.2.cmml">(</mo><mrow id="S3.E2.m3.2.2.1.1.1.1.1.1" xref="S3.E2.m3.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m3.2.2.1.1.1.1.1.1.2" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m3.2.2.1.1.1.1.1.1.2.2" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m3.2.2.1.1.1.1.1.1.2.1" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2.1.cmml">∗</mo><mi id="S3.E2.m3.2.2.1.1.1.1.1.1.2.3" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2.3.cmml">I</mi></mrow><mo id="S3.E2.m3.2.2.1.1.1.1.1.1.1" xref="S3.E2.m3.2.2.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E2.m3.2.2.1.1.1.1.1.1.3" xref="S3.E2.m3.2.2.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo stretchy="false" id="S3.E2.m3.2.2.1.1.1.1.1.3" xref="S3.E2.m3.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m3.2.2.1.2" xref="S3.E2.m3.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m3.2b"><apply id="S3.E2.m3.2.2.1.1.cmml" xref="S3.E2.m3.2.2.1"><csymbol cd="latexml" id="S3.E2.m3.2.2.1.1.2.cmml" xref="S3.E2.m3.2.2.1.1.2">direct-product</csymbol><ci id="S3.E2.m3.2.2.1.1.3.cmml" xref="S3.E2.m3.2.2.1.1.3">𝑔</ci><apply id="S3.E2.m3.2.2.1.1.1.2.cmml" xref="S3.E2.m3.2.2.1.1.1.1"><tanh id="S3.E2.m3.1.1.cmml" xref="S3.E2.m3.1.1"></tanh><apply id="S3.E2.m3.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m3.2.2.1.1.1.1.1.1"><plus id="S3.E2.m3.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m3.2.2.1.1.1.1.1.1.1"></plus><apply id="S3.E2.m3.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2"><ci id="S3.E2.m3.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2.1">∗</ci><ci id="S3.E2.m3.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2.2">𝐾</ci><ci id="S3.E2.m3.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m3.2.2.1.1.1.1.1.1.2.3">𝐼</ci></apply><ci id="S3.E2.m3.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m3.2.2.1.1.1.1.1.1.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m3.2c">\displaystyle g\odot\tanh(K\ast I+b),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS7.p1.11" class="ltx_p">where <math id="S3.SS7.p1.6.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS7.p1.6.m1.1a"><mi id="S3.SS7.p1.6.m1.1.1" xref="S3.SS7.p1.6.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.6.m1.1b"><ci id="S3.SS7.p1.6.m1.1.1.cmml" xref="S3.SS7.p1.6.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.6.m1.1c">g</annotation></semantics></math> is the output gate, <math id="S3.SS7.p1.7.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS7.p1.7.m2.1a"><mi id="S3.SS7.p1.7.m2.1.1" xref="S3.SS7.p1.7.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.7.m2.1b"><ci id="S3.SS7.p1.7.m2.1.1.cmml" xref="S3.SS7.p1.7.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.7.m2.1c">\sigma</annotation></semantics></math> is the <span id="S3.SS7.p1.11.1" class="ltx_text ltx_font_italic">sigmoid</span>
function, <math id="S3.SS7.p1.8.m3.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S3.SS7.p1.8.m3.1a"><mo id="S3.SS7.p1.8.m3.1.1" xref="S3.SS7.p1.8.m3.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.8.m3.1b"><ci id="S3.SS7.p1.8.m3.1.1.cmml" xref="S3.SS7.p1.8.m3.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.8.m3.1c">\ast</annotation></semantics></math> represents convolution, <math id="S3.SS7.p1.9.m4.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS7.p1.9.m4.1a"><mo id="S3.SS7.p1.9.m4.1.1" xref="S3.SS7.p1.9.m4.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.9.m4.1b"><csymbol cd="latexml" id="S3.SS7.p1.9.m4.1.1.cmml" xref="S3.SS7.p1.9.m4.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.9.m4.1c">\odot</annotation></semantics></math> denotes
element-wise multiplication, <math id="S3.SS7.p1.10.m5.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS7.p1.10.m5.1a"><mi id="S3.SS7.p1.10.m5.1.1" xref="S3.SS7.p1.10.m5.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.10.m5.1b"><ci id="S3.SS7.p1.10.m5.1.1.cmml" xref="S3.SS7.p1.10.m5.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.10.m5.1c">b</annotation></semantics></math> and <math id="S3.SS7.p1.11.m6.1" class="ltx_Math" alttext="b_{g}" display="inline"><semantics id="S3.SS7.p1.11.m6.1a"><msub id="S3.SS7.p1.11.m6.1.1" xref="S3.SS7.p1.11.m6.1.1.cmml"><mi id="S3.SS7.p1.11.m6.1.1.2" xref="S3.SS7.p1.11.m6.1.1.2.cmml">b</mi><mi id="S3.SS7.p1.11.m6.1.1.3" xref="S3.SS7.p1.11.m6.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.11.m6.1b"><apply id="S3.SS7.p1.11.m6.1.1.cmml" xref="S3.SS7.p1.11.m6.1.1"><csymbol cd="ambiguous" id="S3.SS7.p1.11.m6.1.1.1.cmml" xref="S3.SS7.p1.11.m6.1.1">subscript</csymbol><ci id="S3.SS7.p1.11.m6.1.1.2.cmml" xref="S3.SS7.p1.11.m6.1.1.2">𝑏</ci><ci id="S3.SS7.p1.11.m6.1.1.3.cmml" xref="S3.SS7.p1.11.m6.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.11.m6.1c">b_{g}</annotation></semantics></math> are bias terms. Gated
convolutional networks for language modeling was proposed
in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and the activation function for the
original outputs was removed. That is, Eq. (<a href="#S3.E2" title="In 3.7 Gated Convolutional Units. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) is replaced
with</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3.3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="O=g\odot(K\ast I+b)." display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">O</mi><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">g</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">⊙</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">∗</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">I</mi></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></eq><ci id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">𝑂</ci><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2">direct-product</csymbol><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">𝑔</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1">∗</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2">𝐾</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3">𝐼</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">O=g\odot(K\ast I+b).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS7.p1.12" class="ltx_p">In our experiments, we explore both methods and combined gates with
inception modules, where different-sized kernels also generate
different gates. We achieve our best results with the method in
Eq. (<a href="#S3.E3" title="In 3.7 Gated Convolutional Units. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span>fastText.</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p id="S3.SS8.p1.7" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> a shallow model named fastText was proposed, and it
achieved comparable results with deep learning models on several text
classification tasks with much less computation. In fastText, embedding
vectors of text data are directly averaged as sentence features. Formally, on
a word-based vocabulary, since the <math id="S3.SS8.p1.1.m1.1" class="ltx_Math" alttext="1\times L" display="inline"><semantics id="S3.SS8.p1.1.m1.1a"><mrow id="S3.SS8.p1.1.m1.1.1" xref="S3.SS8.p1.1.m1.1.1.cmml"><mn id="S3.SS8.p1.1.m1.1.1.2" xref="S3.SS8.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS8.p1.1.m1.1.1.1" xref="S3.SS8.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS8.p1.1.m1.1.1.3" xref="S3.SS8.p1.1.m1.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.1.m1.1b"><apply id="S3.SS8.p1.1.m1.1.1.cmml" xref="S3.SS8.p1.1.m1.1.1"><times id="S3.SS8.p1.1.m1.1.1.1.cmml" xref="S3.SS8.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS8.p1.1.m1.1.1.2.cmml" xref="S3.SS8.p1.1.m1.1.1.2">1</cn><ci id="S3.SS8.p1.1.m1.1.1.3.cmml" xref="S3.SS8.p1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.1.m1.1c">1\times L</annotation></semantics></math> pseudo image with <math id="S3.SS8.p1.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS8.p1.2.m2.1a"><mi id="S3.SS8.p1.2.m2.1.1" xref="S3.SS8.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.2.m2.1b"><ci id="S3.SS8.p1.2.m2.1.1.cmml" xref="S3.SS8.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.2.m2.1c">d</annotation></semantics></math> channels
is actually a concatenation of <math id="S3.SS8.p1.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS8.p1.3.m3.1a"><mi id="S3.SS8.p1.3.m3.1.1" xref="S3.SS8.p1.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.3.m3.1b"><ci id="S3.SS8.p1.3.m3.1.1.cmml" xref="S3.SS8.p1.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.3.m3.1c">L</annotation></semantics></math> <math id="S3.SS8.p1.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS8.p1.4.m4.1a"><mi id="S3.SS8.p1.4.m4.1.1" xref="S3.SS8.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.4.m4.1b"><ci id="S3.SS8.p1.4.m4.1.1.cmml" xref="S3.SS8.p1.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.4.m4.1c">d</annotation></semantics></math>-component word vectors, the average
over <math id="S3.SS8.p1.5.m5.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS8.p1.5.m5.1a"><mi id="S3.SS8.p1.5.m5.1.1" xref="S3.SS8.p1.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.5.m5.1b"><ci id="S3.SS8.p1.5.m5.1.1.cmml" xref="S3.SS8.p1.5.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.5.m5.1c">L</annotation></semantics></math> word vectors results in a <math id="S3.SS8.p1.6.m6.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS8.p1.6.m6.1a"><mi id="S3.SS8.p1.6.m6.1.1" xref="S3.SS8.p1.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.6.m6.1b"><ci id="S3.SS8.p1.6.m6.1.1.cmml" xref="S3.SS8.p1.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.6.m6.1c">d</annotation></semantics></math>-component sentence vector. This
sentence representation is given directly into the classifier. As compared to
deep learning models that use CNNs and RNNs, fastText obtains improvements in
terms of accuracy while achieving a <math id="S3.SS8.p1.7.m7.2" class="ltx_Math" alttext="15,000" display="inline"><semantics id="S3.SS8.p1.7.m7.2a"><mrow id="S3.SS8.p1.7.m7.2.3.2" xref="S3.SS8.p1.7.m7.2.3.1.cmml"><mn id="S3.SS8.p1.7.m7.1.1" xref="S3.SS8.p1.7.m7.1.1.cmml">15</mn><mo id="S3.SS8.p1.7.m7.2.3.2.1" xref="S3.SS8.p1.7.m7.2.3.1.cmml">,</mo><mn id="S3.SS8.p1.7.m7.2.2" xref="S3.SS8.p1.7.m7.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.7.m7.2b"><list id="S3.SS8.p1.7.m7.2.3.1.cmml" xref="S3.SS8.p1.7.m7.2.3.2"><cn type="integer" id="S3.SS8.p1.7.m7.1.1.cmml" xref="S3.SS8.p1.7.m7.1.1">15</cn><cn type="integer" id="S3.SS8.p1.7.m7.2.2.cmml" xref="S3.SS8.p1.7.m7.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.7.m7.2c">15,000</annotation></semantics></math>-fold speed-up due to the small
number of parameters.</p>
</div>
<div id="S3.SS8.p2" class="ltx_para">
<p id="S3.SS8.p2.1" class="ltx_p">The performance of fastText casts doubts on using deep learning models. However, it is
argued that simple text classification tasks may not take full advantage of
the higher representation power of deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. As
stated in Section <a href="#S3.SS1" title="3.1 Analysis of Texts in VQA. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the task of text understanding in VQA
is much more complicated and comprehensive. According to our experiments,
deep learning methods are superior to fastText in VQA, a result that is
consistent with our analysis.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Studies.</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>General Settings.</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.10" class="ltx_p">We report experimental results on COCO-VQA
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://visualqa.org/download.html</span></span></span></span>,
which consists of <math id="S4.SS1.p1.1.m1.2" class="ltx_Math" alttext="204,721" display="inline"><semantics id="S4.SS1.p1.1.m1.2a"><mrow id="S4.SS1.p1.1.m1.2.3.2" xref="S4.SS1.p1.1.m1.2.3.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">204</mn><mo id="S4.SS1.p1.1.m1.2.3.2.1" xref="S4.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">721</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.2b"><list id="S4.SS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.2"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">204</cn><cn type="integer" id="S4.SS1.p1.1.m1.2.2.cmml" xref="S4.SS1.p1.1.m1.2.2">721</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.2c">204,721</annotation></semantics></math> MSCOCO real images with <math id="S4.SS1.p1.2.m2.2" class="ltx_Math" alttext="614,163" display="inline"><semantics id="S4.SS1.p1.2.m2.2a"><mrow id="S4.SS1.p1.2.m2.2.3.2" xref="S4.SS1.p1.2.m2.2.3.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">614</mn><mo id="S4.SS1.p1.2.m2.2.3.2.1" xref="S4.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">163</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.2b"><list id="S4.SS1.p1.2.m2.2.3.1.cmml" xref="S4.SS1.p1.2.m2.2.3.2"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">614</cn><cn type="integer" id="S4.SS1.p1.2.m2.2.2.cmml" xref="S4.SS1.p1.2.m2.2.2">163</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.2c">614,163</annotation></semantics></math>
questions. The data are divided into <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><cn type="integer" id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">3</annotation></semantics></math> subsets: training (<math id="S4.SS1.p1.4.m4.2" class="ltx_Math" alttext="82,783" display="inline"><semantics id="S4.SS1.p1.4.m4.2a"><mrow id="S4.SS1.p1.4.m4.2.3.2" xref="S4.SS1.p1.4.m4.2.3.1.cmml"><mn id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">82</mn><mo id="S4.SS1.p1.4.m4.2.3.2.1" xref="S4.SS1.p1.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.4.m4.2.2" xref="S4.SS1.p1.4.m4.2.2.cmml">783</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.2b"><list id="S4.SS1.p1.4.m4.2.3.1.cmml" xref="S4.SS1.p1.4.m4.2.3.2"><cn type="integer" id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">82</cn><cn type="integer" id="S4.SS1.p1.4.m4.2.2.cmml" xref="S4.SS1.p1.4.m4.2.2">783</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.2c">82,783</annotation></semantics></math>
images with <math id="S4.SS1.p1.5.m5.2" class="ltx_Math" alttext="248,349" display="inline"><semantics id="S4.SS1.p1.5.m5.2a"><mrow id="S4.SS1.p1.5.m5.2.3.2" xref="S4.SS1.p1.5.m5.2.3.1.cmml"><mn id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">248</mn><mo id="S4.SS1.p1.5.m5.2.3.2.1" xref="S4.SS1.p1.5.m5.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.5.m5.2.2" xref="S4.SS1.p1.5.m5.2.2.cmml">349</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.2b"><list id="S4.SS1.p1.5.m5.2.3.1.cmml" xref="S4.SS1.p1.5.m5.2.3.2"><cn type="integer" id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">248</cn><cn type="integer" id="S4.SS1.p1.5.m5.2.2.cmml" xref="S4.SS1.p1.5.m5.2.2">349</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.2c">248,349</annotation></semantics></math> questions), validation (<math id="S4.SS1.p1.6.m6.2" class="ltx_Math" alttext="40,504" display="inline"><semantics id="S4.SS1.p1.6.m6.2a"><mrow id="S4.SS1.p1.6.m6.2.3.2" xref="S4.SS1.p1.6.m6.2.3.1.cmml"><mn id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">40</mn><mo id="S4.SS1.p1.6.m6.2.3.2.1" xref="S4.SS1.p1.6.m6.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.6.m6.2.2" xref="S4.SS1.p1.6.m6.2.2.cmml">504</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.2b"><list id="S4.SS1.p1.6.m6.2.3.1.cmml" xref="S4.SS1.p1.6.m6.2.3.2"><cn type="integer" id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">40</cn><cn type="integer" id="S4.SS1.p1.6.m6.2.2.cmml" xref="S4.SS1.p1.6.m6.2.2">504</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.2c">40,504</annotation></semantics></math> images with
<math id="S4.SS1.p1.7.m7.2" class="ltx_Math" alttext="121,512" display="inline"><semantics id="S4.SS1.p1.7.m7.2a"><mrow id="S4.SS1.p1.7.m7.2.3.2" xref="S4.SS1.p1.7.m7.2.3.1.cmml"><mn id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">121</mn><mo id="S4.SS1.p1.7.m7.2.3.2.1" xref="S4.SS1.p1.7.m7.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.7.m7.2.2" xref="S4.SS1.p1.7.m7.2.2.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.2b"><list id="S4.SS1.p1.7.m7.2.3.1.cmml" xref="S4.SS1.p1.7.m7.2.3.2"><cn type="integer" id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">121</cn><cn type="integer" id="S4.SS1.p1.7.m7.2.2.cmml" xref="S4.SS1.p1.7.m7.2.2">512</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.2c">121,512</annotation></semantics></math> questions) and testing (<math id="S4.SS1.p1.8.m8.2" class="ltx_Math" alttext="81,434" display="inline"><semantics id="S4.SS1.p1.8.m8.2a"><mrow id="S4.SS1.p1.8.m8.2.3.2" xref="S4.SS1.p1.8.m8.2.3.1.cmml"><mn id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">81</mn><mo id="S4.SS1.p1.8.m8.2.3.2.1" xref="S4.SS1.p1.8.m8.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.8.m8.2.2" xref="S4.SS1.p1.8.m8.2.2.cmml">434</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.2b"><list id="S4.SS1.p1.8.m8.2.3.1.cmml" xref="S4.SS1.p1.8.m8.2.3.2"><cn type="integer" id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">81</cn><cn type="integer" id="S4.SS1.p1.8.m8.2.2.cmml" xref="S4.SS1.p1.8.m8.2.2">434</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.2c">81,434</annotation></semantics></math> images with <math id="S4.SS1.p1.9.m9.2" class="ltx_Math" alttext="244,302" display="inline"><semantics id="S4.SS1.p1.9.m9.2a"><mrow id="S4.SS1.p1.9.m9.2.3.2" xref="S4.SS1.p1.9.m9.2.3.1.cmml"><mn id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">244</mn><mo id="S4.SS1.p1.9.m9.2.3.2.1" xref="S4.SS1.p1.9.m9.2.3.1.cmml">,</mo><mn id="S4.SS1.p1.9.m9.2.2" xref="S4.SS1.p1.9.m9.2.2.cmml">302</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.2b"><list id="S4.SS1.p1.9.m9.2.3.1.cmml" xref="S4.SS1.p1.9.m9.2.3.2"><cn type="integer" id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1">244</cn><cn type="integer" id="S4.SS1.p1.9.m9.2.2.cmml" xref="S4.SS1.p1.9.m9.2.2">302</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.2c">244,302</annotation></semantics></math>
questions). In COCO-VQA, answers from ten different individuals are
collected for each question as ground truths. For training, the top
<math id="S4.SS1.p1.10.m10.1" class="ltx_Math" alttext="K=3000" display="inline"><semantics id="S4.SS1.p1.10.m10.1a"><mrow id="S4.SS1.p1.10.m10.1.1" xref="S4.SS1.p1.10.m10.1.1.cmml"><mi id="S4.SS1.p1.10.m10.1.1.2" xref="S4.SS1.p1.10.m10.1.1.2.cmml">K</mi><mo id="S4.SS1.p1.10.m10.1.1.1" xref="S4.SS1.p1.10.m10.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.10.m10.1.1.3" xref="S4.SS1.p1.10.m10.1.1.3.cmml">3000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.10.m10.1b"><apply id="S4.SS1.p1.10.m10.1.1.cmml" xref="S4.SS1.p1.10.m10.1.1"><eq id="S4.SS1.p1.10.m10.1.1.1.cmml" xref="S4.SS1.p1.10.m10.1.1.1"></eq><ci id="S4.SS1.p1.10.m10.1.1.2.cmml" xref="S4.SS1.p1.10.m10.1.1.2">𝐾</ci><cn type="integer" id="S4.SS1.p1.10.m10.1.1.3.cmml" xref="S4.SS1.p1.10.m10.1.1.3">3000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.10.m10.1c">K=3000</annotation></semantics></math> frequent answers among all answers of the training set were
chosen to build the answer vocabulary. In each iteration, an
in-vocabulary answer is sampled as the label from ten ground truths
of each question. If all of the ten answers are out of the answer
vocabulary, the question is skipped. To evaluate the accuracy of a
generated answer, following evaluation metric was proposed
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>:</p>
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1.4" class="ltx_Math" alttext="\mbox{Accuracy}=\min\left(\frac{\#\,\mbox{of humans with the answer}}{3},1\right)," display="block"><semantics id="S4.Ex1.m1.4a"><mrow id="S4.Ex1.m1.4.4.1" xref="S4.Ex1.m1.4.4.1.1.cmml"><mrow id="S4.Ex1.m1.4.4.1.1" xref="S4.Ex1.m1.4.4.1.1.cmml"><mtext id="S4.Ex1.m1.4.4.1.1.2" xref="S4.Ex1.m1.4.4.1.1.2a.cmml">Accuracy</mtext><mo id="S4.Ex1.m1.4.4.1.1.1" xref="S4.Ex1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.Ex1.m1.4.4.1.1.3.2" xref="S4.Ex1.m1.4.4.1.1.3.1.cmml"><mi id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml">min</mi><mo id="S4.Ex1.m1.4.4.1.1.3.2a" xref="S4.Ex1.m1.4.4.1.1.3.1.cmml">⁡</mo><mrow id="S4.Ex1.m1.4.4.1.1.3.2.1" xref="S4.Ex1.m1.4.4.1.1.3.1.cmml"><mo id="S4.Ex1.m1.4.4.1.1.3.2.1.1" xref="S4.Ex1.m1.4.4.1.1.3.1.cmml">(</mo><mfrac id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><mrow id="S4.Ex1.m1.2.2.2" xref="S4.Ex1.m1.2.2.2.cmml"><mi mathvariant="normal" id="S4.Ex1.m1.2.2.2.2" xref="S4.Ex1.m1.2.2.2.2.cmml">#</mi><mo lspace="0.170em" rspace="0em" id="S4.Ex1.m1.2.2.2.1" xref="S4.Ex1.m1.2.2.2.1.cmml">​</mo><mtext id="S4.Ex1.m1.2.2.2.3" xref="S4.Ex1.m1.2.2.2.3a.cmml">of humans with the answer</mtext></mrow><mn id="S4.Ex1.m1.2.2.3" xref="S4.Ex1.m1.2.2.3.cmml">3</mn></mfrac><mo id="S4.Ex1.m1.4.4.1.1.3.2.1.2" xref="S4.Ex1.m1.4.4.1.1.3.1.cmml">,</mo><mn id="S4.Ex1.m1.3.3" xref="S4.Ex1.m1.3.3.cmml">1</mn><mo id="S4.Ex1.m1.4.4.1.1.3.2.1.3" xref="S4.Ex1.m1.4.4.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.Ex1.m1.4.4.1.2" xref="S4.Ex1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.4b"><apply id="S4.Ex1.m1.4.4.1.1.cmml" xref="S4.Ex1.m1.4.4.1"><eq id="S4.Ex1.m1.4.4.1.1.1.cmml" xref="S4.Ex1.m1.4.4.1.1.1"></eq><ci id="S4.Ex1.m1.4.4.1.1.2a.cmml" xref="S4.Ex1.m1.4.4.1.1.2"><mtext id="S4.Ex1.m1.4.4.1.1.2.cmml" xref="S4.Ex1.m1.4.4.1.1.2">Accuracy</mtext></ci><apply id="S4.Ex1.m1.4.4.1.1.3.1.cmml" xref="S4.Ex1.m1.4.4.1.1.3.2"><min id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1"></min><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><divide id="S4.Ex1.m1.2.2.1.cmml" xref="S4.Ex1.m1.2.2"></divide><apply id="S4.Ex1.m1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2"><times id="S4.Ex1.m1.2.2.2.1.cmml" xref="S4.Ex1.m1.2.2.2.1"></times><ci id="S4.Ex1.m1.2.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2.2">#</ci><ci id="S4.Ex1.m1.2.2.2.3a.cmml" xref="S4.Ex1.m1.2.2.2.3"><mtext id="S4.Ex1.m1.2.2.2.3.cmml" xref="S4.Ex1.m1.2.2.2.3">of humans with the answer</mtext></ci></apply><cn type="integer" id="S4.Ex1.m1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.3">3</cn></apply><cn type="integer" id="S4.Ex1.m1.3.3.cmml" xref="S4.Ex1.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.4c">\mbox{Accuracy}=\min\left(\frac{\#\,\mbox{of humans with the answer}}{3},1\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p1.11" class="ltx_p">where the generated answer is compared with each of the ten ground
truth answers, and the corresponding accuracy is computed. Since
evaluation on the testing set can only be processed on remote servers
during the VQA challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and the testing labels
are not published, we choose to train and validate our models on the
training set only instead of the training+validation set
like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and test on the
validation set.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Our baseline model is the challenge
winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which uses a <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">2</annotation></semantics></math>-layer LSTM as
the text feature extractor. This model is retrained on the training
set only. Meanwhile, unlike in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we do not
use additional data sources like the pre-trained word embedding
(Word2Vec, GloVe) and other dataset (Visual
Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>) to augment training. In order to
explore the power of models, we argue that additional data will
narrow the performance gap of different models. For comparison, we
only replace the LSTM text feature extractor with CNN models in
all experiments. All the results are reported in
Table <a href="#S4.T1" title="Table 1 ‣ 4.1 General Settings. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our code is publicly
available<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/divelab/vqa-text</span></span></span></span>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of different text feature extractors. Accuracies per
answer type are shown. Models are trained on the COCO-VQA training set and
tested on the validation set. The retrained baseline model is shown as
“LSTM” in Part <math id="S4.T1.2.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.T1.2.m1.1b"><mn id="S4.T1.2.m1.1.1" xref="S4.T1.2.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.1c"><cn type="integer" id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.1d">1</annotation></semantics></math>. The other parts are CNN-based
models. “Incep”, “Res”, “Bot”, “G(A)”, “G”, “w”, “c” is short
for “Inception”, “Residual”, “Bottleneck”, “Gate (tanh)”, “Gate”,
“work”, “char”, respectively.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<td id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold">Y/N</span></td>
<td id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold">No.</span></td>
<td id="S4.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.3.1.1.4.1" class="ltx_text ltx_font_bold">Other</span></td>
<td id="S4.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.3.1.1.5.1" class="ltx_text ltx_font_bold">All</span></td>
</tr>
<tr id="S4.T1.3.2.2" class="ltx_tr">
<th id="S4.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LSTM</th>
<td id="S4.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t">81.47</td>
<td id="S4.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">34.07</td>
<td id="S4.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">51.14</td>
<td id="S4.T1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">60.35</td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Non-Incep</th>
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">81.75</td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">35.55</td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">51.34</td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">60.73</td>
</tr>
<tr id="S4.T1.3.4.4" class="ltx_tr">
<th id="S4.T1.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Incep (w)</th>
<td id="S4.T1.3.4.4.2" class="ltx_td ltx_align_center">81.91</td>
<td id="S4.T1.3.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.4.3.1" class="ltx_text ltx_font_bold">35.99</span></td>
<td id="S4.T1.3.4.4.4" class="ltx_td ltx_align_center">51.67</td>
<td id="S4.T1.3.4.4.5" class="ltx_td ltx_align_center">61.03</td>
</tr>
<tr id="S4.T1.3.5.5" class="ltx_tr">
<th id="S4.T1.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Incep + Res</th>
<td id="S4.T1.3.5.5.2" class="ltx_td ltx_align_center">81.01</td>
<td id="S4.T1.3.5.5.3" class="ltx_td ltx_align_center">34.45</td>
<td id="S4.T1.3.5.5.4" class="ltx_td ltx_align_center">51.69</td>
<td id="S4.T1.3.5.5.5" class="ltx_td ltx_align_center">60.51</td>
</tr>
<tr id="S4.T1.3.6.6" class="ltx_tr">
<th id="S4.T1.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Incep + Bot</th>
<td id="S4.T1.3.6.6.2" class="ltx_td ltx_align_center">80.12</td>
<td id="S4.T1.3.6.6.3" class="ltx_td ltx_align_center">35.51</td>
<td id="S4.T1.3.6.6.4" class="ltx_td ltx_align_center">50.58</td>
<td id="S4.T1.3.6.6.5" class="ltx_td ltx_align_center">59.74</td>
</tr>
<tr id="S4.T1.3.7.7" class="ltx_tr">
<th id="S4.T1.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Incep + G(A)</th>
<td id="S4.T1.3.7.7.2" class="ltx_td ltx_align_center">82.09</td>
<td id="S4.T1.3.7.7.3" class="ltx_td ltx_align_center">35.47</td>
<td id="S4.T1.3.7.7.4" class="ltx_td ltx_align_center">51.84</td>
<td id="S4.T1.3.7.7.5" class="ltx_td ltx_align_center">61.10</td>
</tr>
<tr id="S4.T1.3.8.8" class="ltx_tr">
<th id="S4.T1.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Incep + G</th>
<td id="S4.T1.3.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.2.1" class="ltx_text ltx_font_bold">82.46</span></td>
<td id="S4.T1.3.8.8.3" class="ltx_td ltx_align_center">35.38</td>
<td id="S4.T1.3.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.4.1" class="ltx_text ltx_font_bold">52.02</span></td>
<td id="S4.T1.3.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.8.5.1" class="ltx_text ltx_font_bold">61.33</span></td>
</tr>
<tr id="S4.T1.3.9.9" class="ltx_tr">
<th id="S4.T1.3.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Incep (c)</th>
<td id="S4.T1.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">78.15</td>
<td id="S4.T1.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">33.79</td>
<td id="S4.T1.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">46.67</td>
<td id="S4.T1.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t">56.83</td>
</tr>
<tr id="S4.T1.3.10.10" class="ltx_tr">
<th id="S4.T1.3.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Deep Res</th>
<td id="S4.T1.3.10.10.2" class="ltx_td ltx_align_center">77.19</td>
<td id="S4.T1.3.10.10.3" class="ltx_td ltx_align_center">33.39</td>
<td id="S4.T1.3.10.10.4" class="ltx_td ltx_align_center">46.09</td>
<td id="S4.T1.3.10.10.5" class="ltx_td ltx_align_center">56.14</td>
</tr>
<tr id="S4.T1.3.11.11" class="ltx_tr">
<th id="S4.T1.3.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Incep (c+w)</th>
<td id="S4.T1.3.11.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.05</td>
<td id="S4.T1.3.11.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">35.39</td>
<td id="S4.T1.3.11.11.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">51.43</td>
<td id="S4.T1.3.11.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">60.88</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Word-Based Models.</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">Several CNN-based text feature extractors on word-based vocabulary
are implemented. The word-based vocabulary, which includes all words
that appear in the training set, has size <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="|V|=13321" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.2" xref="S4.SS2.p1.1.m1.1.2.cmml"><mrow id="S4.SS2.p1.1.m1.1.2.2.2" xref="S4.SS2.p1.1.m1.1.2.2.1.cmml"><mo stretchy="false" id="S4.SS2.p1.1.m1.1.2.2.2.1" xref="S4.SS2.p1.1.m1.1.2.2.1.1.cmml">|</mo><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">V</mi><mo stretchy="false" id="S4.SS2.p1.1.m1.1.2.2.2.2" xref="S4.SS2.p1.1.m1.1.2.2.1.1.cmml">|</mo></mrow><mo id="S4.SS2.p1.1.m1.1.2.1" xref="S4.SS2.p1.1.m1.1.2.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.2.3" xref="S4.SS2.p1.1.m1.1.2.3.cmml">13321</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.2"><eq id="S4.SS2.p1.1.m1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.2.1"></eq><apply id="S4.SS2.p1.1.m1.1.2.2.1.cmml" xref="S4.SS2.p1.1.m1.1.2.2.2"><abs id="S4.SS2.p1.1.m1.1.2.2.1.1.cmml" xref="S4.SS2.p1.1.m1.1.2.2.2.1"></abs><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑉</ci></apply><cn type="integer" id="S4.SS2.p1.1.m1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.2.3">13321</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">|V|=13321</annotation></semantics></math>. For word
embedding, we fix the dimension <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="d=300" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">d</mi><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">300</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑑</ci><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">d=300</annotation></semantics></math>. Dropout is applied on
text representations before they are given into next module. Part
<math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mn id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><cn type="integer" id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">2</annotation></semantics></math> in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 General Settings. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of these models.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">“<span id="S4.SS2.p2.2.1" class="ltx_text ltx_font_bold">Non-Inception</span>” model is a one-layer model with one
<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="1\times 3" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">1\times 3</annotation></semantics></math> convolutional kernel. With max-pooling over the whole
sentence, it produces a <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">2048</annotation></semantics></math>-component text vector
representation. This simple CNN-based model already outperforms the
baseline model, demonstrating that CNN-based model is better than
RNN-based one in VQA.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.3" class="ltx_p">“<span id="S4.SS2.p3.3.1" class="ltx_text ltx_font_bold">Inception (word)</span>” model explores wider CNNs by replacing
the single <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="1\times 3" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">1\times 3</annotation></semantics></math> kernel in “CNN Non-Inception” model with several
different-sized kernels in the same layer, as stated in
Section <a href="#S3.SS6" title="3.6 Wider Networks through Inception Modules. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>. Different kernel settings are explored and
their results are given in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Word-Based Models. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Settings are named
in the format “width of kernel (number of feature maps output by
this kernel)”. Note that the height of kernel is always <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn type="integer" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">1</annotation></semantics></math>. The
resulting text vector representation has <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><cn type="integer" id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">2048</annotation></semantics></math> components. All
these models outperform “CNN Non-Inception” model, showing that features
extracted from phrases of different lengths complement each other.
Table <a href="#S4.T1" title="Table 1 ‣ 4.1 General Settings. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> includes the best results. For all models
using inception modules, different kernel settings are explored. We
only report the best result for other models.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Overall accuracies for “CNN Inception (word)” models with different
kernel settings. Check Section <a href="#S4.SS2" title="4.2 Word-Based Models. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> for details.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Settings</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">2(512)+3(512)+4(512)+5(512)</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">61.03</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1(512)+3(512)+5(512)+7(512)</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">60.96</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3(1024)+5(512)+7(512)</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">60.97</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1(512)+3(1024)+5(512)</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center">60.95</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">3(1024)+5(1024)</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">60.80</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1705.06824/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="460" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of accuracy per question type between
the “Inception + Gate” model and “LSTM (baseline)”
model.</figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">“<span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">CNN Inception + Residual</span>” model tries going deeper. It
adds an identical layer with a residual connection from inputs to
outputs to “CNN Inception (word)” model (Section <a href="#S3.SS5" title="3.5 Deeper Networks for Short Texts. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>).
The best kernel setting is <math id="S4.SS2.p4.1.m1.4" class="ltx_Math" alttext="1(512)+3(512)+5(512)+7(512)" display="inline"><semantics id="S4.SS2.p4.1.m1.4a"><mrow id="S4.SS2.p4.1.m1.4.5" xref="S4.SS2.p4.1.m1.4.5.cmml"><mrow id="S4.SS2.p4.1.m1.4.5.2" xref="S4.SS2.p4.1.m1.4.5.2.cmml"><mn id="S4.SS2.p4.1.m1.4.5.2.2" xref="S4.SS2.p4.1.m1.4.5.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.4.5.2.1" xref="S4.SS2.p4.1.m1.4.5.2.1.cmml">​</mo><mrow id="S4.SS2.p4.1.m1.4.5.2.3.2" xref="S4.SS2.p4.1.m1.4.5.2.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.2.3.2.1" xref="S4.SS2.p4.1.m1.4.5.2.cmml">(</mo><mn id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">512</mn><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.2.3.2.2" xref="S4.SS2.p4.1.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p4.1.m1.4.5.1" xref="S4.SS2.p4.1.m1.4.5.1.cmml">+</mo><mrow id="S4.SS2.p4.1.m1.4.5.3" xref="S4.SS2.p4.1.m1.4.5.3.cmml"><mn id="S4.SS2.p4.1.m1.4.5.3.2" xref="S4.SS2.p4.1.m1.4.5.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.4.5.3.1" xref="S4.SS2.p4.1.m1.4.5.3.1.cmml">​</mo><mrow id="S4.SS2.p4.1.m1.4.5.3.3.2" xref="S4.SS2.p4.1.m1.4.5.3.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.3.3.2.1" xref="S4.SS2.p4.1.m1.4.5.3.cmml">(</mo><mn id="S4.SS2.p4.1.m1.2.2" xref="S4.SS2.p4.1.m1.2.2.cmml">512</mn><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.3.3.2.2" xref="S4.SS2.p4.1.m1.4.5.3.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p4.1.m1.4.5.1a" xref="S4.SS2.p4.1.m1.4.5.1.cmml">+</mo><mrow id="S4.SS2.p4.1.m1.4.5.4" xref="S4.SS2.p4.1.m1.4.5.4.cmml"><mn id="S4.SS2.p4.1.m1.4.5.4.2" xref="S4.SS2.p4.1.m1.4.5.4.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.4.5.4.1" xref="S4.SS2.p4.1.m1.4.5.4.1.cmml">​</mo><mrow id="S4.SS2.p4.1.m1.4.5.4.3.2" xref="S4.SS2.p4.1.m1.4.5.4.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.4.3.2.1" xref="S4.SS2.p4.1.m1.4.5.4.cmml">(</mo><mn id="S4.SS2.p4.1.m1.3.3" xref="S4.SS2.p4.1.m1.3.3.cmml">512</mn><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.4.3.2.2" xref="S4.SS2.p4.1.m1.4.5.4.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p4.1.m1.4.5.1b" xref="S4.SS2.p4.1.m1.4.5.1.cmml">+</mo><mrow id="S4.SS2.p4.1.m1.4.5.5" xref="S4.SS2.p4.1.m1.4.5.5.cmml"><mn id="S4.SS2.p4.1.m1.4.5.5.2" xref="S4.SS2.p4.1.m1.4.5.5.2.cmml">7</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.4.5.5.1" xref="S4.SS2.p4.1.m1.4.5.5.1.cmml">​</mo><mrow id="S4.SS2.p4.1.m1.4.5.5.3.2" xref="S4.SS2.p4.1.m1.4.5.5.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.5.3.2.1" xref="S4.SS2.p4.1.m1.4.5.5.cmml">(</mo><mn id="S4.SS2.p4.1.m1.4.4" xref="S4.SS2.p4.1.m1.4.4.cmml">512</mn><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.5.3.2.2" xref="S4.SS2.p4.1.m1.4.5.5.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.4b"><apply id="S4.SS2.p4.1.m1.4.5.cmml" xref="S4.SS2.p4.1.m1.4.5"><plus id="S4.SS2.p4.1.m1.4.5.1.cmml" xref="S4.SS2.p4.1.m1.4.5.1"></plus><apply id="S4.SS2.p4.1.m1.4.5.2.cmml" xref="S4.SS2.p4.1.m1.4.5.2"><times id="S4.SS2.p4.1.m1.4.5.2.1.cmml" xref="S4.SS2.p4.1.m1.4.5.2.1"></times><cn type="integer" id="S4.SS2.p4.1.m1.4.5.2.2.cmml" xref="S4.SS2.p4.1.m1.4.5.2.2">1</cn><cn type="integer" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">512</cn></apply><apply id="S4.SS2.p4.1.m1.4.5.3.cmml" xref="S4.SS2.p4.1.m1.4.5.3"><times id="S4.SS2.p4.1.m1.4.5.3.1.cmml" xref="S4.SS2.p4.1.m1.4.5.3.1"></times><cn type="integer" id="S4.SS2.p4.1.m1.4.5.3.2.cmml" xref="S4.SS2.p4.1.m1.4.5.3.2">3</cn><cn type="integer" id="S4.SS2.p4.1.m1.2.2.cmml" xref="S4.SS2.p4.1.m1.2.2">512</cn></apply><apply id="S4.SS2.p4.1.m1.4.5.4.cmml" xref="S4.SS2.p4.1.m1.4.5.4"><times id="S4.SS2.p4.1.m1.4.5.4.1.cmml" xref="S4.SS2.p4.1.m1.4.5.4.1"></times><cn type="integer" id="S4.SS2.p4.1.m1.4.5.4.2.cmml" xref="S4.SS2.p4.1.m1.4.5.4.2">5</cn><cn type="integer" id="S4.SS2.p4.1.m1.3.3.cmml" xref="S4.SS2.p4.1.m1.3.3">512</cn></apply><apply id="S4.SS2.p4.1.m1.4.5.5.cmml" xref="S4.SS2.p4.1.m1.4.5.5"><times id="S4.SS2.p4.1.m1.4.5.5.1.cmml" xref="S4.SS2.p4.1.m1.4.5.5.1"></times><cn type="integer" id="S4.SS2.p4.1.m1.4.5.5.2.cmml" xref="S4.SS2.p4.1.m1.4.5.5.2">7</cn><cn type="integer" id="S4.SS2.p4.1.m1.4.4.cmml" xref="S4.SS2.p4.1.m1.4.4">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.4c">1(512)+3(512)+5(512)+7(512)</annotation></semantics></math>. The extra
layer is supposed to further extract text features but hurt
performance in experiments. We conjecture that there is no need to
go deeper for the short inputs in VQA. Character-based vocabulary
will result in longer inputs and deeper models on it are discussed
in Section <a href="#S4.SS3" title="4.3 Character-Based Models. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">“<span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">CNN Inception + Bottleneck</span>” model is inspired by the
bottleneck architecture proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. We apply
bottleneck on the convolution layer of “CNN Inception (word)”
model with kernel setting <math id="S4.SS2.p5.1.m1.2" class="ltx_Math" alttext="3(1024)+5(1024)" display="inline"><semantics id="S4.SS2.p5.1.m1.2a"><mrow id="S4.SS2.p5.1.m1.2.3" xref="S4.SS2.p5.1.m1.2.3.cmml"><mrow id="S4.SS2.p5.1.m1.2.3.2" xref="S4.SS2.p5.1.m1.2.3.2.cmml"><mn id="S4.SS2.p5.1.m1.2.3.2.2" xref="S4.SS2.p5.1.m1.2.3.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p5.1.m1.2.3.2.1" xref="S4.SS2.p5.1.m1.2.3.2.1.cmml">​</mo><mrow id="S4.SS2.p5.1.m1.2.3.2.3.2" xref="S4.SS2.p5.1.m1.2.3.2.cmml"><mo stretchy="false" id="S4.SS2.p5.1.m1.2.3.2.3.2.1" xref="S4.SS2.p5.1.m1.2.3.2.cmml">(</mo><mn id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">1024</mn><mo stretchy="false" id="S4.SS2.p5.1.m1.2.3.2.3.2.2" xref="S4.SS2.p5.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p5.1.m1.2.3.1" xref="S4.SS2.p5.1.m1.2.3.1.cmml">+</mo><mrow id="S4.SS2.p5.1.m1.2.3.3" xref="S4.SS2.p5.1.m1.2.3.3.cmml"><mn id="S4.SS2.p5.1.m1.2.3.3.2" xref="S4.SS2.p5.1.m1.2.3.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p5.1.m1.2.3.3.1" xref="S4.SS2.p5.1.m1.2.3.3.1.cmml">​</mo><mrow id="S4.SS2.p5.1.m1.2.3.3.3.2" xref="S4.SS2.p5.1.m1.2.3.3.cmml"><mo stretchy="false" id="S4.SS2.p5.1.m1.2.3.3.3.2.1" xref="S4.SS2.p5.1.m1.2.3.3.cmml">(</mo><mn id="S4.SS2.p5.1.m1.2.2" xref="S4.SS2.p5.1.m1.2.2.cmml">1024</mn><mo stretchy="false" id="S4.SS2.p5.1.m1.2.3.3.3.2.2" xref="S4.SS2.p5.1.m1.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.2b"><apply id="S4.SS2.p5.1.m1.2.3.cmml" xref="S4.SS2.p5.1.m1.2.3"><plus id="S4.SS2.p5.1.m1.2.3.1.cmml" xref="S4.SS2.p5.1.m1.2.3.1"></plus><apply id="S4.SS2.p5.1.m1.2.3.2.cmml" xref="S4.SS2.p5.1.m1.2.3.2"><times id="S4.SS2.p5.1.m1.2.3.2.1.cmml" xref="S4.SS2.p5.1.m1.2.3.2.1"></times><cn type="integer" id="S4.SS2.p5.1.m1.2.3.2.2.cmml" xref="S4.SS2.p5.1.m1.2.3.2.2">3</cn><cn type="integer" id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">1024</cn></apply><apply id="S4.SS2.p5.1.m1.2.3.3.cmml" xref="S4.SS2.p5.1.m1.2.3.3"><times id="S4.SS2.p5.1.m1.2.3.3.1.cmml" xref="S4.SS2.p5.1.m1.2.3.3.1"></times><cn type="integer" id="S4.SS2.p5.1.m1.2.3.3.2.cmml" xref="S4.SS2.p5.1.m1.2.3.3.2">5</cn><cn type="integer" id="S4.SS2.p5.1.m1.2.2.cmml" xref="S4.SS2.p5.1.m1.2.2">1024</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.2c">3(1024)+5(1024)</annotation></semantics></math>. For models on
image tasks, this architecture improves the accuracies while
reducing the number of parameters. However, it causes a significant
decrease in accuracy to our one-layer model for VQA, which indicates
that the bottleneck design is only suitable to very deep models.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.3" class="ltx_p">“<span id="S4.SS2.p6.3.1" class="ltx_text ltx_font_bold">CNN Inception + Gate (tanh)</span>” model and
“<span id="S4.SS2.p6.3.2" class="ltx_text ltx_font_bold">CNN Inception + Gate</span>” model are CNN-based models with output
gates introduced in Section <a href="#S3.SS7" title="3.7 Gated Convolutional Units. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.7</span></a>, with Eqs. (<a href="#S3.E2" title="In 3.7 Gated Convolutional Units. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>)
and (<a href="#S3.E3" title="In 3.7 Gated Convolutional Units. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), respectively. Note that we combine the gate
architecture with the inception module: for each kernel <math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mi id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><ci id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">K</annotation></semantics></math> in the
same convolution layer, there is a corresponding <math id="S4.SS2.p6.2.m2.1" class="ltx_Math" alttext="K_{g}" display="inline"><semantics id="S4.SS2.p6.2.m2.1a"><msub id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml"><mi id="S4.SS2.p6.2.m2.1.1.2" xref="S4.SS2.p6.2.m2.1.1.2.cmml">K</mi><mi id="S4.SS2.p6.2.m2.1.1.3" xref="S4.SS2.p6.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.2.m2.1.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p6.2.m2.1.1.2.cmml" xref="S4.SS2.p6.2.m2.1.1.2">𝐾</ci><ci id="S4.SS2.p6.2.m2.1.1.3.cmml" xref="S4.SS2.p6.2.m2.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">K_{g}</annotation></semantics></math>. Both methods
improve “CNN Inception (word)” model by adding output gates. With
Eq. (<a href="#S3.E3" title="In 3.7 Gated Convolutional Units. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), we achieve our best text feature extractor with
<math id="S4.SS2.p6.3.m3.1" class="ltx_Math" alttext="61.33\%" display="inline"><semantics id="S4.SS2.p6.3.m3.1a"><mrow id="S4.SS2.p6.3.m3.1.1" xref="S4.SS2.p6.3.m3.1.1.cmml"><mn id="S4.SS2.p6.3.m3.1.1.2" xref="S4.SS2.p6.3.m3.1.1.2.cmml">61.33</mn><mo id="S4.SS2.p6.3.m3.1.1.1" xref="S4.SS2.p6.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.3.m3.1b"><apply id="S4.SS2.p6.3.m3.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.p6.3.m3.1.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p6.3.m3.1.1.2.cmml" xref="S4.SS2.p6.3.m3.1.1.2">61.33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.3.m3.1c">61.33\%</annotation></semantics></math> accuracy. See Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Word-Based Models. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for a comparison
in accuracy per question type between “Inception + Gate” model and
“LSTM (baseline)” model. We can see for most question types,
“Inception + Gate” model outperforms “LSTM (baseline)” model.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">We compare the numbers of parameters of CNN-based text feature extractor with
LSTM-based ones in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Word-Based Models. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. CNN models improve the accuracy
with much fewer training parameters. This reduces the risk of over-fitting
and increases the speed.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The number of parameters for each model. We only compute the
parameters of the text feature extractor.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Number of Parameters</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LSTM (baseline)</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">13,819,904</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CNN Non-Inception</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center">1,845,248</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CNN Inception (word)</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center">2,152,448</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">CNN Inception + Gate</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">4,304,896</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Character-Based Models.</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.6" class="ltx_p">Results for models that involve character-based vocabulary are
reported in parts <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="integer" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">3</annotation></semantics></math> and <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="integer" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">4</annotation></semantics></math> in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 General Settings. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The two
models in part <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mn id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><cn type="integer" id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">3</annotation></semantics></math> use character-based vocabulary only, while the
model in part <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><cn type="integer" id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">4</annotation></semantics></math> uses a combination of both vocabularies
(Section <a href="#S3.SS3" title="3.3 Word-Based versus Character-Based Representations. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). The character-based vocabulary collects
<math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="|V\_c|=45" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mrow id="S4.SS3.p1.5.m5.1.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS3.p1.5.m5.1.1.1.1.2" xref="S4.SS3.p1.5.m5.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS3.p1.5.m5.1.1.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.5.m5.1.1.1.1.1.2" xref="S4.SS3.p1.5.m5.1.1.1.1.1.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.5.m5.1.1.1.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS3.p1.5.m5.1.1.1.1.1.3" xref="S4.SS3.p1.5.m5.1.1.1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.5.m5.1.1.1.1.1.1a" xref="S4.SS3.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS3.p1.5.m5.1.1.1.1.1.4" xref="S4.SS3.p1.5.m5.1.1.1.1.1.4.cmml">c</mi></mrow><mo stretchy="false" id="S4.SS3.p1.5.m5.1.1.1.1.3" xref="S4.SS3.p1.5.m5.1.1.1.2.1.cmml">|</mo></mrow><mo id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">=</mo><mn id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml">45</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><eq id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2"></eq><apply id="S4.SS3.p1.5.m5.1.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.1.1"><abs id="S4.SS3.p1.5.m5.1.1.1.2.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1.1.2"></abs><apply id="S4.SS3.p1.5.m5.1.1.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1.1.1"><times id="S4.SS3.p1.5.m5.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1.1.1.1"></times><ci id="S4.SS3.p1.5.m5.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.1.1.1.2">𝑉</ci><ci id="S4.SS3.p1.5.m5.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.1.1.1.3">_</ci><ci id="S4.SS3.p1.5.m5.1.1.1.1.1.4.cmml" xref="S4.SS3.p1.5.m5.1.1.1.1.1.4">𝑐</ci></apply></apply><cn type="integer" id="S4.SS3.p1.5.m5.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3">45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">|V\_c|=45</annotation></semantics></math> characters: all lowercase characters in English,
punctuation as well as the space character. The kernel settings for
both inception-like models below are <math id="S4.SS3.p1.6.m6.4" class="ltx_Math" alttext="2(512)+3(512)+4(512)+5(512)" display="inline"><semantics id="S4.SS3.p1.6.m6.4a"><mrow id="S4.SS3.p1.6.m6.4.5" xref="S4.SS3.p1.6.m6.4.5.cmml"><mrow id="S4.SS3.p1.6.m6.4.5.2" xref="S4.SS3.p1.6.m6.4.5.2.cmml"><mn id="S4.SS3.p1.6.m6.4.5.2.2" xref="S4.SS3.p1.6.m6.4.5.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.6.m6.4.5.2.1" xref="S4.SS3.p1.6.m6.4.5.2.1.cmml">​</mo><mrow id="S4.SS3.p1.6.m6.4.5.2.3.2" xref="S4.SS3.p1.6.m6.4.5.2.cmml"><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.2.3.2.1" xref="S4.SS3.p1.6.m6.4.5.2.cmml">(</mo><mn id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml">512</mn><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.2.3.2.2" xref="S4.SS3.p1.6.m6.4.5.2.cmml">)</mo></mrow></mrow><mo id="S4.SS3.p1.6.m6.4.5.1" xref="S4.SS3.p1.6.m6.4.5.1.cmml">+</mo><mrow id="S4.SS3.p1.6.m6.4.5.3" xref="S4.SS3.p1.6.m6.4.5.3.cmml"><mn id="S4.SS3.p1.6.m6.4.5.3.2" xref="S4.SS3.p1.6.m6.4.5.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.6.m6.4.5.3.1" xref="S4.SS3.p1.6.m6.4.5.3.1.cmml">​</mo><mrow id="S4.SS3.p1.6.m6.4.5.3.3.2" xref="S4.SS3.p1.6.m6.4.5.3.cmml"><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.3.3.2.1" xref="S4.SS3.p1.6.m6.4.5.3.cmml">(</mo><mn id="S4.SS3.p1.6.m6.2.2" xref="S4.SS3.p1.6.m6.2.2.cmml">512</mn><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.3.3.2.2" xref="S4.SS3.p1.6.m6.4.5.3.cmml">)</mo></mrow></mrow><mo id="S4.SS3.p1.6.m6.4.5.1a" xref="S4.SS3.p1.6.m6.4.5.1.cmml">+</mo><mrow id="S4.SS3.p1.6.m6.4.5.4" xref="S4.SS3.p1.6.m6.4.5.4.cmml"><mn id="S4.SS3.p1.6.m6.4.5.4.2" xref="S4.SS3.p1.6.m6.4.5.4.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.6.m6.4.5.4.1" xref="S4.SS3.p1.6.m6.4.5.4.1.cmml">​</mo><mrow id="S4.SS3.p1.6.m6.4.5.4.3.2" xref="S4.SS3.p1.6.m6.4.5.4.cmml"><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.4.3.2.1" xref="S4.SS3.p1.6.m6.4.5.4.cmml">(</mo><mn id="S4.SS3.p1.6.m6.3.3" xref="S4.SS3.p1.6.m6.3.3.cmml">512</mn><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.4.3.2.2" xref="S4.SS3.p1.6.m6.4.5.4.cmml">)</mo></mrow></mrow><mo id="S4.SS3.p1.6.m6.4.5.1b" xref="S4.SS3.p1.6.m6.4.5.1.cmml">+</mo><mrow id="S4.SS3.p1.6.m6.4.5.5" xref="S4.SS3.p1.6.m6.4.5.5.cmml"><mn id="S4.SS3.p1.6.m6.4.5.5.2" xref="S4.SS3.p1.6.m6.4.5.5.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.6.m6.4.5.5.1" xref="S4.SS3.p1.6.m6.4.5.5.1.cmml">​</mo><mrow id="S4.SS3.p1.6.m6.4.5.5.3.2" xref="S4.SS3.p1.6.m6.4.5.5.cmml"><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.5.3.2.1" xref="S4.SS3.p1.6.m6.4.5.5.cmml">(</mo><mn id="S4.SS3.p1.6.m6.4.4" xref="S4.SS3.p1.6.m6.4.4.cmml">512</mn><mo stretchy="false" id="S4.SS3.p1.6.m6.4.5.5.3.2.2" xref="S4.SS3.p1.6.m6.4.5.5.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.4b"><apply id="S4.SS3.p1.6.m6.4.5.cmml" xref="S4.SS3.p1.6.m6.4.5"><plus id="S4.SS3.p1.6.m6.4.5.1.cmml" xref="S4.SS3.p1.6.m6.4.5.1"></plus><apply id="S4.SS3.p1.6.m6.4.5.2.cmml" xref="S4.SS3.p1.6.m6.4.5.2"><times id="S4.SS3.p1.6.m6.4.5.2.1.cmml" xref="S4.SS3.p1.6.m6.4.5.2.1"></times><cn type="integer" id="S4.SS3.p1.6.m6.4.5.2.2.cmml" xref="S4.SS3.p1.6.m6.4.5.2.2">2</cn><cn type="integer" id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1">512</cn></apply><apply id="S4.SS3.p1.6.m6.4.5.3.cmml" xref="S4.SS3.p1.6.m6.4.5.3"><times id="S4.SS3.p1.6.m6.4.5.3.1.cmml" xref="S4.SS3.p1.6.m6.4.5.3.1"></times><cn type="integer" id="S4.SS3.p1.6.m6.4.5.3.2.cmml" xref="S4.SS3.p1.6.m6.4.5.3.2">3</cn><cn type="integer" id="S4.SS3.p1.6.m6.2.2.cmml" xref="S4.SS3.p1.6.m6.2.2">512</cn></apply><apply id="S4.SS3.p1.6.m6.4.5.4.cmml" xref="S4.SS3.p1.6.m6.4.5.4"><times id="S4.SS3.p1.6.m6.4.5.4.1.cmml" xref="S4.SS3.p1.6.m6.4.5.4.1"></times><cn type="integer" id="S4.SS3.p1.6.m6.4.5.4.2.cmml" xref="S4.SS3.p1.6.m6.4.5.4.2">4</cn><cn type="integer" id="S4.SS3.p1.6.m6.3.3.cmml" xref="S4.SS3.p1.6.m6.3.3">512</cn></apply><apply id="S4.SS3.p1.6.m6.4.5.5.cmml" xref="S4.SS3.p1.6.m6.4.5.5"><times id="S4.SS3.p1.6.m6.4.5.5.1.cmml" xref="S4.SS3.p1.6.m6.4.5.5.1"></times><cn type="integer" id="S4.SS3.p1.6.m6.4.5.5.2.cmml" xref="S4.SS3.p1.6.m6.4.5.5.2">5</cn><cn type="integer" id="S4.SS3.p1.6.m6.4.4.cmml" xref="S4.SS3.p1.6.m6.4.4">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.4c">2(512)+3(512)+4(512)+5(512)</annotation></semantics></math>.
Dropout is also applied.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">“<span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">CNN Inception (char)</span>” model applies the same inception
module as “CNN Inception (word)” model but replaces the word-based
inputs with character-based inputs. The accuracy drops drastically.
As explained in Section <a href="#S3.SS3" title="3.3 Word-Based versus Character-Based Representations. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, it is due to the short
length of the inputs, which is not enough for the model to learn how
to separate characters into words.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.2" class="ltx_p">“<span id="S4.SS3.p3.2.1" class="ltx_text ltx_font_bold">CNN Deep Residual</span>” model attempts to take advantage of the longer
inputs provided by character-based vocabulary. We stack <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><cn type="integer" id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">5</annotation></semantics></math> convolution
layers with residual connections and <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mn id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><cn type="integer" id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">3</annotation></semantics></math> local pooling layers to build a deep
model. Contrast to the results of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>,
the model fails to work well. Again, comparison indicates the input length as
the cause of failure.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.3" class="ltx_p">“<span id="S4.SS3.p4.3.1" class="ltx_text ltx_font_bold">CNN Inception (char+word)</span>” model makes use of both
word-based and character-based vocabularies as shown in
Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Transforming Text Data. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In our model, the characters of each word
generate a <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="150" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mn id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><cn type="integer" id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">150</annotation></semantics></math>-component word embedding vector, which is
concatenated with the <math id="S4.SS3.p4.2.m2.1" class="ltx_Math" alttext="150" display="inline"><semantics id="S4.SS3.p4.2.m2.1a"><mn id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><cn type="integer" id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">150</annotation></semantics></math>-component word embedding from word-based
vocabulary to form a <math id="S4.SS3.p4.3.m3.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S4.SS3.p4.3.m3.1a"><mn id="S4.SS3.p4.3.m3.1.1" xref="S4.SS3.p4.3.m3.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.3.m3.1b"><cn type="integer" id="S4.SS3.p4.3.m3.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.3.m3.1c">300</annotation></semantics></math>-component vector representing the word.
As compared to “CNN Inception (word)” model, it leads to a slight
accuracy decrease. This demonstrates that using character-based
vocabulary is not able to provide useful information from
constituent characters of the word. Based on these experiments, we
conclude that character-based vocabulary is not helpful in short
input cases like texts in VQA.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Deep Learning Models versus fastText.</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">As introduced in Section <a href="#S3.SS8" title="3.8 fastText. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.8</span></a>, fastText is a shallow model that
achieves comparable results with deep learning models in
text classification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. This result contradicts the
common belief that deep learning models can learn better representations. It has been
conjectured that the simple text classification task may not be the right one
to evaluate text representation methods. Given the higher requirements for
text understanding in VQA, we compare these models in VQA. In addition to the
original fastText model (“<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">fastText (word)</span>”), which averages word
embedding vectors to obtain sentence representations, we also explore
fastText (“<span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_bold">fastText (char+word)</span>”) with character-based vocabulary.
Similar to the idea in Section <a href="#S3.SS3" title="3.3 Word-Based versus Character-Based Representations. ‣ 3 Text Representations in VQA. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, character embedding of each
word is averaged to generate part of the word embedding. The results are
given in Table <a href="#S4.T4" title="Table 4 ‣ 4.4 Deep Learning Models versus fastText. ‣ 4 Experimental Studies. ‣ Learning Convolutional Text Representations for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We can see the performance gap between deep
learning models and fastText. Clearly, it demonstrates the complexity of VQA
tasks and the power of deep learning.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of results between deep learning models and fastText.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LSTM (baseline)</th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">60.35</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CNN Inception + Gate</th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center">61.33</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">fastText (word)</th>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_center">59.30</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<th id="S4.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">fastText (char+word)</th>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">59.24</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions.</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We perform detailed analysis on texts in VQA and propose to employ CNNs as
text feature extractors in current VQA models. By incorporating recent
research achievements in CNNs for images, our best model improves text
representations and the overall accuracy. By comparing deep learning models
with the fastText, we show that the requirement for text understanding in VQA
is more comprehensive and complicated than simple tasks. Based on our
research, we believe that our proposed methods can be extensively used for
learning text representations in other tasks on text data of similar properties.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements.</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This work was supported in part by National Science Foundation grant
IIS-1633359.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span id="bib.bib1.1.1" class="ltx_text ltx_font_smallcaps">S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,
and D. Parikh</span>, <span id="bib.bib1.2.2" class="ltx_text ltx_font_italic">Vqa: Visual question answering</span>, in Proceedings of the
IEEE International Conference on Computer Vision, 2015, pp. 2425–2433.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
<span id="bib.bib2.1.1" class="ltx_text ltx_font_smallcaps">D. Bahdanau, K. Cho, and Y. Bengio</span>, <span id="bib.bib2.2.2" class="ltx_text ltx_font_italic">Neural machine translation by
jointly learning to align and translate</span>, arXiv preprint arXiv:1409.0473,
(2014).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
<span id="bib.bib3.1.1" class="ltx_text ltx_font_smallcaps">A. Conneau, H. Schwenk, L. Barrault, and Y. Lecun</span>, <span id="bib.bib3.2.2" class="ltx_text ltx_font_italic">Very deep
convolutional networks for natural language processing</span>, arXiv preprint
arXiv:1606.01781, (2016).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
<span id="bib.bib4.1.1" class="ltx_text ltx_font_smallcaps">Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier</span>, <span id="bib.bib4.2.2" class="ltx_text ltx_font_italic">Language modeling
with gated convolutional networks</span>, arXiv preprint arXiv:1612.08083, (2016).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
<span id="bib.bib5.1.1" class="ltx_text ltx_font_smallcaps">C. N. dos Santos and M. Gatti</span>, <span id="bib.bib5.2.2" class="ltx_text ltx_font_italic">Deep convolutional neural networks
for sentiment analysis of short texts.</span>, in COLING, 2014, pp. 69–78.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
<span id="bib.bib6.1.1" class="ltx_text ltx_font_smallcaps">A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach</span>,
<span id="bib.bib6.2.2" class="ltx_text ltx_font_italic">Multimodal compact bilinear pooling for visual question answering and
visual grounding</span>, arXiv preprint arXiv:1606.01847, (2016).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
<span id="bib.bib7.1.1" class="ltx_text ltx_font_smallcaps">K. He, X. Zhang, S. Ren, and J. Sun</span>, <span id="bib.bib7.2.2" class="ltx_text ltx_font_italic">Deep residual learning for
image recognition</span>, arXiv preprint arXiv:1512.03385, (2015).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
<span id="bib.bib8.1.1" class="ltx_text ltx_font_smallcaps">S. Hochreiter and J. Schmidhuber</span>, <span id="bib.bib8.2.2" class="ltx_text ltx_font_italic">Long short-term memory</span>, Neural
computation, 9 (1997), pp. 1735–1780.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
<span id="bib.bib9.1.1" class="ltx_text ltx_font_smallcaps">R. Johnson and T. Zhang</span>, <span id="bib.bib9.2.2" class="ltx_text ltx_font_italic">Effective use of word order for text
categorization with convolutional neural networks</span>, arXiv preprint
arXiv:1412.1058, (2014).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
<span id="bib.bib10.1.1" class="ltx_text ltx_font_smallcaps">A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov</span>, <span id="bib.bib10.2.2" class="ltx_text ltx_font_italic">Bag of tricks
for efficient text classification</span>, arXiv preprint arXiv:1607.01759, (2016).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
<span id="bib.bib11.1.1" class="ltx_text ltx_font_smallcaps">K. Kafle and C. Kanan</span>, <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Visual question answering: Datasets,
algorithms, and future challenges</span>, arXiv preprint arXiv:1610.01465, (2016).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
<span id="bib.bib12.1.1" class="ltx_text ltx_font_smallcaps">J.-H. Kim, K. W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang</span>, <span id="bib.bib12.2.2" class="ltx_text ltx_font_italic">Hadamard Product for Low-rank Bilinear Pooling</span>, in The 5th International
Conference on Learning Representations, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
<span id="bib.bib13.1.1" class="ltx_text ltx_font_smallcaps">Y. Kim</span>, <span id="bib.bib13.2.2" class="ltx_text ltx_font_italic">Convolutional neural networks for sentence classification</span>,
arXiv preprint arXiv:1408.5882, (2014).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
<span id="bib.bib14.1.1" class="ltx_text ltx_font_smallcaps">R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, et al.</span>, <span id="bib.bib14.2.2" class="ltx_text ltx_font_italic">Visual genome:
Connecting language and vision using crowdsourced dense image annotations</span>,
arXiv preprint arXiv:1602.07332, (2016).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
<span id="bib.bib15.1.1" class="ltx_text ltx_font_smallcaps">Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner</span>, <span id="bib.bib15.2.2" class="ltx_text ltx_font_italic">Gradient-based
learning applied to document recognition</span>, Proceedings of the IEEE, 86
(1998), pp. 2278–2324.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
<span id="bib.bib16.1.1" class="ltx_text ltx_font_smallcaps">T.-Y. Lin, A. RoyChowdhury, and S. Maji</span>, <span id="bib.bib16.2.2" class="ltx_text ltx_font_italic">Bilinear cnn models for
fine-grained visual recognition</span>, in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 1449–1457.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
<span id="bib.bib17.1.1" class="ltx_text ltx_font_smallcaps">J. Lu, J. Yang, D. Batra, and D. Parikh</span>, <span id="bib.bib17.2.2" class="ltx_text ltx_font_italic">Hierarchical question-image
co-attention for visual question answering</span>, arXiv preprint arXiv:1606.00061,
(2016).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
<span id="bib.bib18.1.1" class="ltx_text ltx_font_smallcaps">T. Mikolov, M. Karafiát, L. Burget, J. Cernockỳ, and
S. Khudanpur</span>, <span id="bib.bib18.2.2" class="ltx_text ltx_font_italic">Recurrent neural network based language model.</span>, in
Interspeech, vol. 2, 2010, p. 3.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
<span id="bib.bib19.1.1" class="ltx_text ltx_font_smallcaps">T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean</span>, <span id="bib.bib19.2.2" class="ltx_text ltx_font_italic">Distributed representations of words and phrases and their compositionality</span>,
in Advances in neural information processing systems, 2013, pp. 3111–3119.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
<span id="bib.bib20.1.1" class="ltx_text ltx_font_smallcaps">J. Pennington, R. Socher, and C. D. Manning</span>, <span id="bib.bib20.2.2" class="ltx_text ltx_font_italic">Glove: Global vectors
for word representation.</span>, in EMNLP, vol. 14, 2014, pp. 1532–43.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
<span id="bib.bib21.1.1" class="ltx_text ltx_font_smallcaps">D. E. Rumelhart, P. Smolensky, J. L. McClelland, and G. Hinton</span>, <span id="bib.bib21.2.2" class="ltx_text ltx_font_italic">Sequential thought processes in pdp models</span>, V, 2 (1986), pp. 3–57.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
<span id="bib.bib22.1.1" class="ltx_text ltx_font_smallcaps">I. Sutskever, O. Vinyals, and Q. V. Le</span>, <span id="bib.bib22.2.2" class="ltx_text ltx_font_italic">Sequence to sequence
learning with neural networks</span>, in Advances in neural information processing
systems, 2014, pp. 3104–3112.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
<span id="bib.bib23.1.1" class="ltx_text ltx_font_smallcaps">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich</span>, <span id="bib.bib23.2.2" class="ltx_text ltx_font_italic">Going deeper with convolutions</span>, in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1–9.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
<span id="bib.bib24.1.1" class="ltx_text ltx_font_smallcaps">A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves,
et al.</span>, <span id="bib.bib24.2.2" class="ltx_text ltx_font_italic">Conditional image generation with pixelcnn decoders</span>, in
Advances in Neural Information Processing Systems, 2016, pp. 4790–4798.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
<span id="bib.bib25.1.1" class="ltx_text ltx_font_smallcaps">Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun,
Y. Cao, Q. Gao, K. Macherey, et al.</span>, <span id="bib.bib25.2.2" class="ltx_text ltx_font_italic">Google’s neural machine
translation system: Bridging the gap between human and machine translation</span>,
arXiv preprint arXiv:1609.08144, (2016).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
<span id="bib.bib26.1.1" class="ltx_text ltx_font_smallcaps">C. Xiong, S. Merity, and R. Socher</span>, <span id="bib.bib26.2.2" class="ltx_text ltx_font_italic">Dynamic memory networks for
visual and textual question answering</span>, arXiv preprint arXiv:1603.01417,
(2016).

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
<span id="bib.bib27.1.1" class="ltx_text ltx_font_smallcaps">Z. Yang, X. He, J. Gao, L. Deng, and A. Smola</span>, <span id="bib.bib27.2.2" class="ltx_text ltx_font_italic">Stacked attention
networks for image question answering</span>, arXiv preprint arXiv:1511.02274,
(2015).

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
<span id="bib.bib28.1.1" class="ltx_text ltx_font_smallcaps">X. Zhang, J. Zhao, and Y. LeCun</span>, <span id="bib.bib28.2.2" class="ltx_text ltx_font_italic">Character-level convolutional
networks for text classification</span>, in Advances in Neural Information
Processing Systems, 2015, pp. 649–657.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1705.06823" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1705.06824" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1705.06824">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1705.06824" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1705.06825" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 12:54:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
