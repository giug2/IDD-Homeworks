<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1911.07588] An Annotated Corpus of Reference Resolution for Interpreting Common Grounding</title><meta property="og:description" content="Common grounding is the process of creating, repairing and updating mutual understandings, which is a fundamental aspect of natural language conversation. However, interpreting the process of common grounding is a chal…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An Annotated Corpus of Reference Resolution for Interpreting Common Grounding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="An Annotated Corpus of Reference Resolution for Interpreting Common Grounding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1911.07588">

<!--Generated on Sat Mar  2 14:01:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">An Annotated Corpus of Reference Resolution 
<br class="ltx_break">for Interpreting Common Grounding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Takuma Udagawa
<br class="ltx_break">The University of Tokyo, Tokyo, Japan
<br class="ltx_break">takuma_udagawa@nii.ac.jp
<span id="id1.1.id1" class="ltx_ERROR undefined">\And</span>Akiko Aizawa
<br class="ltx_break">National Institute of Informatics, Tokyo, Japan
<br class="ltx_break">aizawa@nii.ac.jp
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Common grounding is the process of creating, repairing and updating mutual understandings, which is a fundamental aspect of natural language conversation. However, interpreting the process of common grounding is a challenging task, especially under continuous and partially-observable context where complex ambiguity, uncertainty, partial understandings and misunderstandings are introduced. Interpretation becomes even more challenging when we deal with dialogue systems which still have limited capability of natural language understanding and generation. To address this problem, we consider reference resolution as the central subtask of common grounding and propose a new resource to study its intermediate process. Based on a simple and general annotation schema, we collected a total of 40,172 referring expressions in 5,191 dialogues curated from an existing corpus, along with multiple judgements of referent interpretations. We show that our annotation is highly reliable, captures the complexity of common grounding through a natural degree of reasonable disagreements, and allows for more detailed and quantitative analyses of common grounding strategies. Finally, we demonstrate the advantages of our annotation for interpreting, analyzing and improving common grounding in baseline dialogue systems.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/1911.07588/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="516" height="251" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A visualized example of the raw dialogue (left) and our annotated dialogue (right). In our annotation, referring expressions are detected and their intended referents are annotated based on the speaker’s view (only one judgement shown in this example). Background task is described in detail in Section <a href="#S3" title="3 Background Task ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and our annotation procedure in Section <a href="#S4" title="4 Annotation Procedure ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Common grounding is the process of creating, repairing and updating mutual understandings, which is a critical aspect of sophisticated human communication (<span id="S1.p1.1.1" class="ltx_text ltx_font_bold">?</span>) as well as a longstanding goal in dialogue modeling (<span id="S1.p1.1.2" class="ltx_text ltx_font_bold">?</span>). Recently, there have been several new proposals of dialogue tasks which require advanced skills of common grounding under <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">continuous</span> and <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">partially-observable</span> context (<span id="S1.p1.1.5" class="ltx_text ltx_font_bold">?</span>; <span id="S1.p1.1.6" class="ltx_text ltx_font_bold">?</span>). Their main contributions include proposal of clear evaluation metrics based on task success rate, collection of large-scale datasets (thousands of dialogues) and introduction of complex ambiguity, uncertainty, partial understandings and misunderstandings which are minimally observed under traditional settings based on either categorical or fully-observable context.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, interpretation of the process of common grounding remains largely an open problem. Although a formal theory such as <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">?</span> (<span id="S1.p2.1.2" class="ltx_text ltx_font_bold">?</span>) can account for some of the important details in common grounding, constructing such precise semantic representation is a difficult and costly process, especially under continuous and partially-observable context with high ambiguity and uncertainty. Interpretation becomes even more challenging when we deal with dialogue systems represented by end-to-end neural models (<span id="S1.p2.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="S1.p2.1.4" class="ltx_text ltx_font_bold">?</span>), which can converse fluently but still lack true competency of natural language understanding and generation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we approach this problem by <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">decomposing</span> the common grounding task based on its intermediate subtasks. Specifically, we consider <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">reference resolution</span> as the central subtask of common grounding (in the sense that mutual understanding can only be created through successful references to the entities in the task domain), define this subtask formally based on a simple and general annotation schema, and create a large-scale resource to study this subtask along with the original task of common grounding.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our annotated corpus consists of a total of 40,172 referring expressions in 5,191 dialogues curated from the existing corpus (<span id="S1.p4.1.1" class="ltx_text ltx_font_bold">?</span>), along with multiple (a minimum of 3) judgements for referent interpretations. A visualization of our annotation is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Through our corpus analysis, we show that our annotation has high agreement in general but also includes a natural degree of reasonable disagreements, which verified that our annotation can be conducted reliably while capturing the ambiguity and uncertainty under continuous and partially-observable context. In addition, we give a more quantitative analysis of <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">pragmatic expressions</span> as an illustrative example of analyses that can be conducted based on our annotation.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Finally, through our experiments we show that our annotation is critical for interpreting and analyzing common grounding in baseline dialogue systems, as well as improving their performance on difficult end tasks.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Overall, we propose a fundamental method and resource for interpreting the process of common grounding through its subtask of reference resolution. All materials related to this work will be publicly available at <span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/Alab-NII/onecommon</span>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">One of the most influential models of common grounding to date is the <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">contribution</span> model (<span id="S2.p1.1.2" class="ltx_text ltx_font_bold">?</span>), which distinguishes information in a dialogue into two phases: the <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">presentation phase</span> where a piece of information is introduced by a speaker, and the <span id="S2.p1.1.4" class="ltx_text ltx_font_italic">acceptance phase</span> where it gets accepted by a listener. However, applying such theory for analysis in realistic settings can be difficult or even problematic (<span id="S2.p1.1.5" class="ltx_text ltx_font_bold">?</span>), especially when contributions are implicit, indirect, unstructured, uncertain or partial. In contrast, we propose a more practical approach of decomposing common grounding based on well-defined subtasks: in our case we focus on reference resolution. Although our approach does not give a formal account of common grounding, we show that our annotation is <span id="S2.p1.1.6" class="ltx_text ltx_font_italic">general</span> with simple and clear definition, <span id="S2.p1.1.7" class="ltx_text ltx_font_italic">reliable</span> in terms of annotator agreement under complex settings, and <span id="S2.p1.1.8" class="ltx_text ltx_font_italic">useful</span> for interpreting and analyzing the intermediate process of common grounding.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Our work is also relevant to the recent literature of interpretable and explainable machine learning (<span id="S2.p2.1.1" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p2.1.2" class="ltx_text ltx_font_bold">?</span>). Especially the analysis of neural based models is gaining attention in NLP (<span id="S2.p2.1.3" class="ltx_text ltx_font_bold">?</span>), including end-to-end dialogue models (<span id="S2.p2.1.4" class="ltx_text ltx_font_bold">?</span>). The main novelty of our approach is that we decompose the original task (<span id="S2.p2.1.5" class="ltx_text ltx_font_italic">common grounding</span>) based on its central subtask (or could be subtasks), define the subtask (<span id="S2.p2.1.6" class="ltx_text ltx_font_italic">reference resolution</span>) formally with an annotation framework, and create a large-scale resource to study the subtask along with the original task. Our approach has several advantages compared to previous analysis methods. First, it is applicable to <span id="S2.p2.1.7" class="ltx_text ltx_font_italic">both humans and machines</span>, which is especially important in dialogue domains where they interact. Second, it can be used to study the <span id="S2.p2.1.8" class="ltx_text ltx_font_italic">relationships</span> between the original task and its subtasks, which is critical for a more <span id="S2.p2.1.9" class="ltx_text ltx_font_italic">skill-oriented</span> evaluation of artificial intelligence (<span id="S2.p2.1.10" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p2.1.11" class="ltx_text ltx_font_bold">?</span>). Third, it can be used for investigating <span id="S2.p2.1.12" class="ltx_text ltx_font_italic">the dataset</span> on which the models are trained: this is important in many aspects, such as understanding undesirable bias in the dataset (<span id="S2.p2.1.13" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p2.1.14" class="ltx_text ltx_font_bold">?</span>) or correct model predictions based on the <span id="S2.p2.1.15" class="ltx_text ltx_font_italic">wrong reasons</span> (<span id="S2.p2.1.16" class="ltx_text ltx_font_bold">?</span>). Finally, the collected resource can be used for both <span id="S2.p2.1.17" class="ltx_text ltx_font_italic">probing</span> whether the models solve the subtasks implicitly (<span id="S2.p2.1.18" class="ltx_text ltx_font_bold">?</span>) or <span id="S2.p2.1.19" class="ltx_text ltx_font_italic">developing</span> new models which can be explicitly supervised, evaluated and interpreted based on the subtasks.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Coreference and anaphora resolution have also been studied extensively in NLP (<span id="S2.p3.1.1" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p3.1.2" class="ltx_text ltx_font_bold">?</span>), including disagreements in their interpretations (<span id="S2.p3.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p3.1.4" class="ltx_text ltx_font_bold">?</span>). The main difference between our annotation schema and theirs is that we focus on exophoric references and directly annotate the referent entities of each referring expression in situated dialogues. We show that our annotation can be conducted reliably, even by using non-expert annotators for referent identifications. Our annotation does not capture explicit relations between anaphora, but they capture basic coreference relations as well as complex associative anaphora (such as <span id="S2.p3.1.5" class="ltx_text ltx_font_italic">part-of</span> relations), at least in an indirect way. Most importantly, they are compatible with such existing schema, and annotating explicit anaphoric relations could be a viable approach for future work.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Finally, visually grounded dialogues have been studied in a wide variety of settings. In comparison, the main strengths and novelty of our corpus can be summarized as follows:</p>
</div>
<div id="S2.p5" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Our corpus is based on the advanced setting of continuous and partially-observable context where complex common grounding strategies are required.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">B.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Our corpus has more simplicity and controllability compared to realistic visual dialogues, which makes controlled experiments and analyses easier.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">C.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Our corpus includes large-scale manual annotation of reference resolution and detailed analyses of agreements/disagreements based on multiple judgements.</p>
</div>
</li>
</ol>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Prior work in common grounding (<span id="S2.p6.1.1" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p6.1.2" class="ltx_text ltx_font_bold">?</span>) and visual reference resolution (<span id="S2.p6.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p6.1.4" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p6.1.5" class="ltx_text ltx_font_bold">?</span>) mostly focus on categorical or fully-observable settings and do not satisfy A. While visual dialogues (<span id="S2.p6.1.6" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p6.1.7" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p6.1.8" class="ltx_text ltx_font_bold">?</span>; <span id="S2.p6.1.9" class="ltx_text ltx_font_bold">?</span>) have the strengths of being more complex and realistic, they do not satisfy B and C. Although <span id="S2.p6.1.10" class="ltx_text ltx_font_bold">?</span> (<span id="S2.p6.1.11" class="ltx_text ltx_font_bold">?</span>) conducted a smaller-scale (and more loosely defined) annotation of reference resolution, they did not assess the reliability of the annotation (hence does not satisfy B and C). To the best of our knowledge, our work is the first to satisfy all of the above criteria.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background Task</h2>

<figure id="S3.F2" class="ltx_figure">
<table id="S3.F2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.F2.2.3.1" class="ltx_tr">
<th id="S3.F2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.F2.2.3.1.1.1" class="ltx_text" style="color:#FF0000;"><span id="S3.F2.2.3.1.1.1.1" class="ltx_text ltx_font_bold">Misunderstanding</span></span></th>
<th id="S3.F2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.F2.2.3.1.2.1" class="ltx_text" style="color:#0000FF;"><span id="S3.F2.2.3.1.2.1.1" class="ltx_text ltx_font_bold">Partial Understanding</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.F2.2.2" class="ltx_tr">
<td id="S3.F2.1.1.1" class="ltx_td ltx_align_center">
<span class="ltx_rule" style="width:0.0pt;height:129.2pt;background:black;display:inline-block;"></span>
 <svg id="S3.F2.1.1.1.pic1" class="ltx_picture" height="281" overflow="visible" version="1.1" width="446.35"><g transform="translate(0,281) matrix(1 0 0 -1 0 0) translate(140.5,0) translate(0,140.5)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -140.5 -140.5)" fill="#000000" stroke="#000000"><foreignObject width="281" height="281" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x2.png" id="S3.F2.1.1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="216" height="216" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 24.85 -140.5)" fill="#000000" stroke="#000000"><foreignObject width="281" height="281" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x3.png" id="S3.F2.1.1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="216" height="216" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -25.79 -93.24)" fill="#000000" stroke="#000000"><foreignObject width="51.58" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F2.1.1.1.pic1.3.3.3.1.1" class="ltx_text">A’s view</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 139.85 -93.24)" fill="#000000" stroke="#000000"><foreignObject width="51" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F2.1.1.1.pic1.4.4.4.1.1" class="ltx_text">B’s view</span></foreignObject></g></g></svg>
</td>
<td id="S3.F2.2.2.2" class="ltx_td ltx_align_center"><svg id="S3.F2.2.2.2.pic1" class="ltx_picture" height="283" overflow="visible" version="1.1" width="446.35"><g transform="translate(0,283) matrix(1 0 0 -1 0 0) translate(140.5,0) translate(0,141.5)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -140.5 -140.5)" fill="#000000" stroke="#000000"><foreignObject width="281" height="281" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x4.png" id="S3.F2.2.2.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="216" height="216" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 24.85 -141.5)" fill="#000000" stroke="#000000"><foreignObject width="281" height="283" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x5.png" id="S3.F2.2.2.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="216" height="218" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -25.79 -93.24)" fill="#000000" stroke="#000000"><foreignObject width="51.58" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F2.2.2.2.pic1.3.3.3.1.1" class="ltx_text">A’s view</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 139.85 -93.24)" fill="#000000" stroke="#000000"><foreignObject width="51" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F2.2.2.2.pic1.4.4.4.1.1" class="ltx_text">B’s view</span></foreignObject></g></g></svg>
</td>
</tr>
<tr id="S3.F2.2.4.1" class="ltx_tr">
<td id="S3.F2.2.4.1.1" class="ltx_td ltx_align_center">
<table id="S3.F2.2.4.1.1.1" class="ltx_tabular ltx_align_top">
<tr id="S3.F2.2.4.1.1.1.1" class="ltx_tr">
<td id="S3.F2.2.4.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">A: I see <span id="S3.F2.2.4.1.1.1.1.1.1" class="ltx_text" style="color:#00BF00;"> <span id="S3.F2.2.4.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">three smaller circles</span></span> almost in a line slanting down</td>
</tr>
<tr id="S3.F2.2.4.1.1.1.2" class="ltx_tr">
<td id="S3.F2.2.4.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">from right to left</td>
</tr>
<tr id="S3.F2.2.4.1.1.1.3" class="ltx_tr">
<td id="S3.F2.2.4.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">B: I think I see <span id="S3.F2.2.4.1.1.1.3.1.1" class="ltx_text" style="color:#FF0000;"> <span id="S3.F2.2.4.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">it</span></span>. Is <span id="S3.F2.2.4.1.1.1.3.1.2" class="ltx_text ltx_framed ltx_framed_underline">the left one</span> the largest? …</td>
</tr>
</table>
</td>
<td id="S3.F2.2.4.1.2" class="ltx_td ltx_align_center">
<table id="S3.F2.2.4.1.2.1" class="ltx_tabular ltx_align_top">
<tr id="S3.F2.2.4.1.2.1.1" class="ltx_tr">
<td id="S3.F2.2.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">A: I have <span id="S3.F2.2.4.1.2.1.1.1.1" class="ltx_text" style="color:#00BF00;"> <span id="S3.F2.2.4.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">5 larger dots</span></span> close together, <span id="S3.F2.2.4.1.2.1.1.1.2" class="ltx_text ltx_framed ltx_framed_underline">the bottom left one</span> is</td>
</tr>
<tr id="S3.F2.2.4.1.2.1.2" class="ltx_tr">
<td id="S3.F2.2.4.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">largest and darkest?</td>
</tr>
<tr id="S3.F2.2.4.1.2.1.3" class="ltx_tr">
<td id="S3.F2.2.4.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">B: i have <span id="S3.F2.2.4.1.2.1.3.1.1" class="ltx_text" style="color:#0000FF;"> <span id="S3.F2.2.4.1.2.1.3.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">three</span></span> that could be part of that …</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example of misunderstanding and partial understanding captured by our annotation.
</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our annotation is conducted on a recently proposed common grounding dataset, which is a minimal formalization of a collaborative referring task under <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">continuous</span> and <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">partially-observable</span> context (<span id="S3.p1.1.3" class="ltx_text ltx_font_bold">?</span>). In this task, two players are given slightly different but overlapping perspectives of a 2-dimensional grid. Both players have 7 entities in each view, but only 4, 5 or 6 of them are in common: this makes their setting <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">partially-observable</span> with different degrees of partial-observability. In addition, each entity only has <span id="S3.p1.1.5" class="ltx_text ltx_font_italic">continuous</span> attributes (color, size and location). The goal of the task is to find one of the common entities through natural language communication, and the task is successful if and only if they could find and select the same entity.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Some distinguishing characteristics of their dataset include its large size (a total of 6,760 dialogues, out of which 5,191 were successful on the task), rich linguistic variety with limited vocabulary (a total of only 2,035 unique tokens after preprocessing in our curated corpus), and most importantly the complexity of common grounding introduced by continuous and partially-observable context. As shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Background Task ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, there could be complex misunderstandings and partial understandings that need to be resolved through advanced skills of common grounding. We can also find various nuanced expressions (<span id="S3.p2.1.1" class="ltx_text ltx_font_italic">“almost in a line”</span>, <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">“I think I see …”</span>, <span id="S3.p2.1.3" class="ltx_text ltx_font_italic">“could be”</span>) and pragmatic expressions (<span id="S3.p2.1.4" class="ltx_text ltx_font_italic">“a line”</span>, <span id="S3.p2.1.5" class="ltx_text ltx_font_italic">“largest”</span>, <span id="S3.p2.1.6" class="ltx_text ltx_font_italic">“bottom left”</span>) which can be ambiguous or need to be interpreted based on their context.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Annotation Procedure</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The goal of our annotation is to provide a <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">general</span>, <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">reliable</span> and <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">useful</span> annotation of reference resolution to interpret the intermediate process of common grounding. In this work, we use the 5,191 successful dialogues from the existing corpus which are expected to be of higher quality (however, our annotation is applicable to unsuccessful dialogues as well). Our annotation procedure consists of two main steps: <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">markable detection</span> to semi-automatically annotate referring expressions currently under consideration and <span id="S4.p1.1.5" class="ltx_text ltx_font_italic">referent identification</span> to identify the referents of each referring expression.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">As an optional step, we also conducted <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">preprocessing</span> of the dialogues to correct obvious misspellings and grammatical errors. Due to the limited size of the vocabulary, we manually looked for rare unigrams and bigrams in the dialogue and carefully created rules to correct them. Our preprocessing step is reversible, so the collected annotation can also be applied to the original dialogues without preprocessing.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Markable Detection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this work, we define a <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">markable</span> as an independent referring expression of the entities currently under consideration (in our case, the dots in the circular view). Basically, we annotate a markable as a minimal noun phrase including all prenominal modifiers (such as determiners, quantifiers, and adjectives) but excluding all postnominal modifiers (such as prepositional phrases and relational clauses). This eliminates the complexity of the annotation because markables will not overlap or nest with each other. See the figures for many examples of the detected markables (underlined).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To reduce the annotation effort in the later process, we optionally annotate three attributes for each markable if they are obvious from the context: a <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">generic</span> attribute when the markable is not specific enough to identify the referents, <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">all-referents</span> when the markable is referring to all of the entities in the speaker’s view, and <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">no-referent</span> when the referents are empty. <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_italic">Generic</span> markables are ignored in our annotation, and the referents of <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_italic">all-referents</span> or <span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_italic">no-referent</span> are annotated automatically in the later process. To reduce the redundancy of annotation, we consider a predicative noun phrase as a markable only if there is no previous markable in the same utterance that refer to the same entities: for example, <span id="S4.SS1.p2.1.7" class="ltx_text ltx_font_italic">“a triangle”</span> in <span id="S4.SS1.p2.1.8" class="ltx_text ltx_font_italic">“<span id="S4.SS1.p2.1.8.1" class="ltx_text ltx_framed ltx_framed_underline">three dots</span> are forming a triangle”</span> is not considered as a markable since <span id="S4.SS1.p2.1.9" class="ltx_text ltx_font_italic">“three dots”</span> is already annotated, but it is considered a markable in <span id="S4.SS1.p2.1.10" class="ltx_text ltx_font_italic">“<span id="S4.SS1.p2.1.10.1" class="ltx_text ltx_framed ltx_framed_underline">one light dot</span> and <span id="S4.SS1.p2.1.10.2" class="ltx_text ltx_framed ltx_framed_underline">two dark dots</span> are forming <span id="S4.SS1.p2.1.10.3" class="ltx_text ltx_framed ltx_framed_underline">a triangle</span>”</span>. We also annotate obvious <span id="S4.SS1.p2.1.11" class="ltx_text ltx_font_italic">anaphoric</span> and <span id="S4.SS1.p2.1.12" class="ltx_text ltx_font_italic">cataphoric</span> relations in the same utterance: this way, the referents of anaphoric and cataphoric markables can be annotated automatically based on their antecedents or postcedents. Note that we do not annotate such relations across utterances, as they can actually refer to different entities (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Background Task ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for such example).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Detection of the markables, their attributes and relations are conducted using the brat annotation tool (<span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">?</span>). Annotators were trained extensively and had access to all available information (including original dialogues, players’ observations and selections) during annotation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Referent Identification</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/1911.07588/assets/x6.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="462" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visual interface for referent identification.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Next, we used crowdsourcing on Amazon Mechanical Turk to collect large-scale judgements of the referents of each markable. Our visual interface for referent identification is shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Referent Identification ‣ 4 Annotation Procedure ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Annotators were instructed to read the instructions carefully (including description of the background task), put a check on <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">ambiguous</span> box and select all possible candidates when the referents are ambiguous, and put a check on <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">unidentifiable</span> if the referents are completely unidentifiable based on the available information.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">To collect reliable annotations, we restricted the workers to those with at least 100 previously completed HITs and above 99% acceptance rate. We paid the workers well, with $0.25 for dialogues with less than 7 markables, $0.35 with less than 14 markables, and $0.45 otherwise. In addition, we automatically detected outliers based on several statistics (such as agreement with other workers) and manually reviewed them to encourage better work or reject clearly unacceptable works. The overall rejection rate was 1.18%.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">As a result of this careful crowdsourcing, we were able to collect a large-scale annotation of 103,894 judgements with at least 3 judgements for each of the 34,341 markables.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Annotated Corpus</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Basic Statistics</h3>

<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.3.1.1" class="ltx_tr">
<th id="S5.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Markables</th>
<th id="S5.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># All-Referents</th>
<th id="S5.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># No-Referent</th>
<th id="S5.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Anaphora</th>
<th id="S5.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Cataphora</th>
<th id="S5.T1.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">% Start Agreement</th>
<th id="S5.T1.3.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">% End Agreement</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.3.2.1" class="ltx_tr">
<td id="S5.T1.3.2.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">40,172</td>
<td id="S5.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">128</td>
<td id="S5.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1,149</td>
<td id="S5.T1.3.2.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">4,548</td>
<td id="S5.T1.3.2.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">6</td>
<td id="S5.T1.3.2.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">99.11 (96.32)</td>
<td id="S5.T1.3.2.1.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">99.06 (96.11)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Basic statistics of markable detection. Referents for <span id="S5.T1.8.1" class="ltx_text ltx_font_italic">all-referents</span>, <span id="S5.T1.9.2" class="ltx_text ltx_font_italic">no-referent</span>, <span id="S5.T1.10.3" class="ltx_text ltx_font_italic">anaphora</span> and <span id="S5.T1.11.4" class="ltx_text ltx_font_italic">cataphora</span> are annotated automatically. 130 dialogues with 3 independent annotations are used to compute agreement (Fleiss’s Multi-<math id="S5.T1.2.m1.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S5.T1.2.m1.1b"><mi id="S5.T1.2.m1.1.1" xref="S5.T1.2.m1.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S5.T1.2.m1.1c"><ci id="S5.T1.2.m1.1.1.cmml" xref="S5.T1.2.m1.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.m1.1d">\pi</annotation></semantics></math> in parenthesis).
</figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.3.1.1" class="ltx_tr">
<td id="S5.T2.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"># Markables</td>
<td id="S5.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"># Judgements</td>
<td id="S5.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">% Ambiguous</td>
<td id="S5.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">% Unidentifiable</td>
<td id="S5.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">% Agreement</td>
<td id="S5.T2.3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">% Exact Match</td>
</tr>
<tr id="S5.T2.3.2.2" class="ltx_tr">
<td id="S5.T2.3.2.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">34,341</td>
<td id="S5.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">103,894</td>
<td id="S5.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">4.65</td>
<td id="S5.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.77</td>
<td id="S5.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">96.26 (88.66)</td>
<td id="S5.T2.3.2.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">86.90</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Basic statistics of referent identification, along with the rate of <span id="S5.T2.6.1" class="ltx_text ltx_font_italic">ambiguous</span> and <span id="S5.T2.7.2" class="ltx_text ltx_font_italic">unidentifiable</span> checked in the judgements. Agreement is calculated at the entity level (Fleiss’s Multi-<math id="S5.T2.2.m1.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S5.T2.2.m1.1b"><mi id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><ci id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">\pi</annotation></semantics></math> in parenthesis) and exact match rate at the markable level.
</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">First, we report the basic statistics of the annotation for markable detection in Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Basic Statistics ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and referent identification in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Basic Statistics ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. All agreements are computed based on pairwise judgements. For markable detection, agreement is calculated for the markable text span (at the token level of whether each token is the start or end of the markables). Agreements for markable attributes and relations are also publicly available (but omitted in this paper since they were optional and annotated only in obvious cases). For referent identification, agreement is calculated based on binary judgements of whether each entity is included in the referents or not, and exact match is calculated only if the referents of the markable matched exactly. In addition, we compute Fleiss’s Multi-<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\pi</annotation></semantics></math> (<span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">?</span>) to remove the effect of chance level agreements.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Overall, we found high agreement for all annotations, which verified the reliability of our annotation framework.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Disagreement Analysis</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">However, it is natural that there is a certain degree of disagreements in referent interpretations. In fact, it is important to capture such disagreements as there can be genuine ambiguity and uncertainty under continuous and partially-observable context (see Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2 Disagreement Analysis ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for example). Therefore, in addition to <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">explicitly</span> annotating the ambiguity and unidentifiability as described in Subsection <a href="#S4.SS2" title="4.2 Referent Identification ‣ 4 Annotation Procedure ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we aim to capture them <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">implicitly</span> by collecting multiple judgements from different annotators, similar in approach to <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">?</span> (<span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_bold">?</span>).</p>
</div>
<figure id="S5.F4" class="ltx_figure"><svg id="S5.F4.pic1" class="ltx_picture" height="231.41" overflow="visible" version="1.1" width="326.35"><g transform="translate(0,231.41) matrix(1 0 0 -1 0 0) translate(80.5,0) translate(0,129.32)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -80.5 -80.5)" fill="#000000" stroke="#000000"><foreignObject width="161" height="161" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x7.png" id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="123" height="123" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 84.85 -80.5)" fill="#000000" stroke="#000000"><foreignObject width="161" height="161" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x8.png" id="S5.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="123" height="124" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -37.11 88.02)" fill="#000000" stroke="#000000"><foreignObject width="74.22" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.3.3.3.1.1" class="ltx_text">Annotator 1</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 128.24 88.02)" fill="#000000" stroke="#000000"><foreignObject width="74.22" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F4.pic1.4.4.4.1.1" class="ltx_text">Annotator 2</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -74.8 -101.19)" fill="#000000" stroke="#000000"><foreignObject width="314.96" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S5.F4.pic1.5.5.5.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:227.6pt;">
<span id="S5.F4.pic1.5.5.5.1.1.1" class="ltx_p"></span>
<span id="S5.F4.pic1.5.5.5.1.1.2" class="ltx_p ltx_align_left"><span id="S5.F4.pic1.5.5.5.1.1.2.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="color:#FF0000;">medium sized light gray dot</span> with <span id="S5.F4.pic1.5.5.5.1.1.2.2" class="ltx_text" style="color:#0000FF;"> <span id="S5.F4.pic1.5.5.5.1.1.2.2.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">a darker one</span></span> directly under <span id="S5.F4.pic1.5.5.5.1.1.2.3" class="ltx_text" style="color:#FF0000;"> <span id="S5.F4.pic1.5.5.5.1.1.2.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">it</span></span> and to the right?</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of seemingly reasonable disagreements captured by our annotation.
</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">To study the disagreements in detail, we compute the observed agreement statistics given <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">the number of referents</span> in each judgement. To be specific, for a certain number of referents (from 0 to 7), we consider all judgements with the number of referents, make all possible pairs with other judgements on the same markable, and compute the average of entity level agreement and exact match rate. The results are summarized in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Disagreement Analysis ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"># Referents</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">% Agreement</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">% Exact</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">% Judgements</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">78.04</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">17.78</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S5.T3.1.2.1.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.31</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center">97.45</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center">90.28</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center">71.81</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center">94.87</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center">82.17</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center">14.85</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<td id="S5.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r">3</td>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center">93.93</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center">83.03</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center">
<span id="S5.T3.1.5.4.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>7.51</td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<td id="S5.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_center">92.18</td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_align_center">76.66</td>
<td id="S5.T3.1.6.5.4" class="ltx_td ltx_align_center">
<span id="S5.T3.1.6.5.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>2.20</td>
</tr>
<tr id="S5.T3.1.7.6" class="ltx_tr">
<td id="S5.T3.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S5.T3.1.7.6.2" class="ltx_td ltx_align_center">90.31</td>
<td id="S5.T3.1.7.6.3" class="ltx_td ltx_align_center">71.03</td>
<td id="S5.T3.1.7.6.4" class="ltx_td ltx_align_center">
<span id="S5.T3.1.7.6.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.88</td>
</tr>
<tr id="S5.T3.1.8.7" class="ltx_tr">
<td id="S5.T3.1.8.7.1" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S5.T3.1.8.7.2" class="ltx_td ltx_align_center">90.75</td>
<td id="S5.T3.1.8.7.3" class="ltx_td ltx_align_center">78.14</td>
<td id="S5.T3.1.8.7.4" class="ltx_td ltx_align_center">
<span id="S5.T3.1.8.7.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.22</td>
</tr>
<tr id="S5.T3.1.9.8" class="ltx_tr">
<td id="S5.T3.1.9.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">7</td>
<td id="S5.T3.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb">81.47</td>
<td id="S5.T3.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb">62.50</td>
<td id="S5.T3.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S5.T3.1.9.8.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.21</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Agreement statistics given the number of referents in the judgement and the percentages of such judgements.
</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We can see that there is a significant amount of disagreements when the number of referents was judged to be either 0 or 7. This could be due to several reasons: obvious cases were already annotated as <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">no-referent</span> or <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_italic">all-referents</span> during markable detection (so only difficult cases were left), annotators simply made mistakes (e.g. forgot to annotate), or the referents were annotated as such when it was too difficult to identify them. Since the number of such judgements are relatively small, their effect can be mitigated after appropriate aggregation of multiple judgements. In addition, they could be a useful resource for studying whether the disagreements are caused by <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_italic">annotation error</span> or <span id="S5.SS2.p3.1.4" class="ltx_text ltx_font_italic">genuine difficulty</span> in the annotation, as suggested in <span id="S5.SS2.p3.1.5" class="ltx_text ltx_font_bold">?</span> (<span id="S5.SS2.p3.1.6" class="ltx_text ltx_font_bold">?</span>).</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">We also found that the exact match rate is highest when the number of referents is 1, and much lower as the number of referents increases. This is reasonable because referring expressions for multiple entities tend to be more pragmatic and ambiguous (e.g. <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_italic">“a cluster”</span>, <span id="S5.SS2.p4.1.2" class="ltx_text ltx_font_italic">“most of”</span>, <span id="S5.SS2.p4.1.3" class="ltx_text ltx_font_italic">“a line”</span>), and it would be more difficult to match the referents exactly. Note that entity level agreements are still at a high level, and the interpreted referents seem to mostly overlap with each other.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Next, as a preliminary analysis to study which expressions tend to have higher (or lower) disagreements, we compute the correlations between the occurrence of common tokens (represented by binary values) and the exact match rate of the pairwise judgements for each markable. Illustrative examples are shown in Table <a href="#S5.T4" title="Table 4 ‣ 5.2 Disagreement Analysis ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and the whole list will be publicly available.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.2.2" class="ltx_tr">
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<table id="S5.T4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">Low</td>
<td id="S5.T4.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"><math id="S5.T4.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.T4.1.1.1.1.1.1.m1.1a"><mi id="S5.T4.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.m1.1c">\rho</annotation></semantics></math></td>
<td id="S5.T4.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">Count</td>
</tr>
<tr id="S5.T4.1.1.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.2.1.1" class="ltx_text ltx_font_italic">it</span></td>
<td id="S5.T4.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">-0.149</td>
<td id="S5.T4.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">12.7K</td>
</tr>
<tr id="S5.T4.1.1.1.1.3" class="ltx_tr">
<td id="S5.T4.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.3.1.1" class="ltx_text ltx_font_italic">any</span></td>
<td id="S5.T4.1.1.1.1.3.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.103</td>
<td id="S5.T4.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.3.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.5K</td>
</tr>
<tr id="S5.T4.1.1.1.1.4" class="ltx_tr">
<td id="S5.T4.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.4.1.1" class="ltx_text ltx_font_italic">that</span></td>
<td id="S5.T4.1.1.1.1.4.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.100</td>
<td id="S5.T4.1.1.1.1.4.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">12.5K</td>
</tr>
<tr id="S5.T4.1.1.1.1.5" class="ltx_tr">
<td id="S5.T4.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.5.1.1" class="ltx_text ltx_font_italic">your</span></td>
<td id="S5.T4.1.1.1.1.5.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.083</td>
<td id="S5.T4.1.1.1.1.5.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.5.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.5K</td>
</tr>
<tr id="S5.T4.1.1.1.1.6" class="ltx_tr">
<td id="S5.T4.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.6.1.1" class="ltx_text ltx_font_italic">few</span></td>
<td id="S5.T4.1.1.1.1.6.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.081</td>
<td id="S5.T4.1.1.1.1.6.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.6.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.1K</td>
</tr>
<tr id="S5.T4.1.1.1.1.7" class="ltx_tr">
<td id="S5.T4.1.1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.7.1.1" class="ltx_text ltx_font_italic">what</span></td>
<td id="S5.T4.1.1.1.1.7.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.081</td>
<td id="S5.T4.1.1.1.1.7.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.7.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.4K</td>
</tr>
<tr id="S5.T4.1.1.1.1.8" class="ltx_tr">
<td id="S5.T4.1.1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.8.1.1" class="ltx_text ltx_font_italic">others</span></td>
<td id="S5.T4.1.1.1.1.8.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.064</td>
<td id="S5.T4.1.1.1.1.8.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.8.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.8K</td>
</tr>
<tr id="S5.T4.1.1.1.1.9" class="ltx_tr">
<td id="S5.T4.1.1.1.1.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.9.1.1" class="ltx_text ltx_font_italic">line</span></td>
<td id="S5.T4.1.1.1.1.9.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.062</td>
<td id="S5.T4.1.1.1.1.9.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.9.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.7K</td>
</tr>
<tr id="S5.T4.1.1.1.1.10" class="ltx_tr">
<td id="S5.T4.1.1.1.1.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.10.1.1" class="ltx_text ltx_font_italic">bunch</span></td>
<td id="S5.T4.1.1.1.1.10.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.060</td>
<td id="S5.T4.1.1.1.1.10.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.10.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.2K</td>
</tr>
<tr id="S5.T4.1.1.1.1.11" class="ltx_tr">
<td id="S5.T4.1.1.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.11.1.1" class="ltx_text ltx_font_italic">all</span></td>
<td id="S5.T4.1.1.1.1.11.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.048</td>
<td id="S5.T4.1.1.1.1.11.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.11.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.1K</td>
</tr>
<tr id="S5.T4.1.1.1.1.12" class="ltx_tr">
<td id="S5.T4.1.1.1.1.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.12.1.1" class="ltx_text ltx_font_italic">triangle</span></td>
<td id="S5.T4.1.1.1.1.12.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.046</td>
<td id="S5.T4.1.1.1.1.12.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.12.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>2.5K</td>
</tr>
<tr id="S5.T4.1.1.1.1.13" class="ltx_tr">
<td id="S5.T4.1.1.1.1.13.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.13.1.1" class="ltx_text ltx_font_italic">some</span></td>
<td id="S5.T4.1.1.1.1.13.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.042</td>
<td id="S5.T4.1.1.1.1.13.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.13.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>0.2K</td>
</tr>
<tr id="S5.T4.1.1.1.1.14" class="ltx_tr">
<td id="S5.T4.1.1.1.1.14.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.14.1.1" class="ltx_text ltx_font_italic">medium</span></td>
<td id="S5.T4.1.1.1.1.14.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.041</td>
<td id="S5.T4.1.1.1.1.14.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">12.5K</td>
</tr>
<tr id="S5.T4.1.1.1.1.15" class="ltx_tr">
<td id="S5.T4.1.1.1.1.15.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.15.1.1" class="ltx_text ltx_font_italic">another</span></td>
<td id="S5.T4.1.1.1.1.15.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-0.039</td>
<td id="S5.T4.1.1.1.1.15.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.15.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.4K</td>
</tr>
<tr id="S5.T4.1.1.1.1.16" class="ltx_tr">
<td id="S5.T4.1.1.1.1.16.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.1.1.1.1.16.1.1" class="ltx_text ltx_font_italic">and</span></td>
<td id="S5.T4.1.1.1.1.16.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">-0.029</td>
<td id="S5.T4.1.1.1.1.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.1.1.1.1.16.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.7K</td>
</tr>
</table>
</td>
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<table id="S5.T4.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.2.2.2.1.1" class="ltx_tr">
<td id="S5.T4.2.2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">High</td>
<td id="S5.T4.2.2.2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"><math id="S5.T4.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.T4.2.2.2.1.1.1.m1.1a"><mi id="S5.T4.2.2.2.1.1.1.m1.1.1" xref="S5.T4.2.2.2.1.1.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.1.1.m1.1b"><ci id="S5.T4.2.2.2.1.1.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.1.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.1.1.m1.1c">\rho</annotation></semantics></math></td>
<td id="S5.T4.2.2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">Count</td>
</tr>
<tr id="S5.T4.2.2.2.1.2" class="ltx_tr">
<td id="S5.T4.2.2.2.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.2.1.1" class="ltx_text ltx_font_italic">lower</span></td>
<td id="S5.T4.2.2.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">0.028</td>
<td id="S5.T4.2.2.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.2.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.3K</td>
</tr>
<tr id="S5.T4.2.2.2.1.3" class="ltx_tr">
<td id="S5.T4.2.2.2.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.3.1.1" class="ltx_text ltx_font_italic">two</span></td>
<td id="S5.T4.2.2.2.1.3.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.030</td>
<td id="S5.T4.2.2.2.1.3.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">14.7K</td>
</tr>
<tr id="S5.T4.2.2.2.1.4" class="ltx_tr">
<td id="S5.T4.2.2.2.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.4.1.1" class="ltx_text ltx_font_italic">three</span></td>
<td id="S5.T4.2.2.2.1.4.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.031</td>
<td id="S5.T4.2.2.2.1.4.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.4.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>4.2K</td>
</tr>
<tr id="S5.T4.2.2.2.1.5" class="ltx_tr">
<td id="S5.T4.2.2.2.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.5.1.1" class="ltx_text ltx_font_italic">darkest</span></td>
<td id="S5.T4.2.2.2.1.5.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.036</td>
<td id="S5.T4.2.2.2.1.5.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.5.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>2.1K</td>
</tr>
<tr id="S5.T4.2.2.2.1.6" class="ltx_tr">
<td id="S5.T4.2.2.2.1.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.6.1.1" class="ltx_text ltx_font_italic">larger</span></td>
<td id="S5.T4.2.2.2.1.6.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.039</td>
<td id="S5.T4.2.2.2.1.6.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.6.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>7.7K</td>
</tr>
<tr id="S5.T4.2.2.2.1.7" class="ltx_tr">
<td id="S5.T4.2.2.2.1.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.7.1.1" class="ltx_text ltx_font_italic">middle</span></td>
<td id="S5.T4.2.2.2.1.7.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.041</td>
<td id="S5.T4.2.2.2.1.7.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.7.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>2.1K</td>
</tr>
<tr id="S5.T4.2.2.2.1.8" class="ltx_tr">
<td id="S5.T4.2.2.2.1.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.8.1.1" class="ltx_text ltx_font_italic">smallest</span></td>
<td id="S5.T4.2.2.2.1.8.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.043</td>
<td id="S5.T4.2.2.2.1.8.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.8.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>2.0K</td>
</tr>
<tr id="S5.T4.2.2.2.1.9" class="ltx_tr">
<td id="S5.T4.2.2.2.1.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.9.1.1" class="ltx_text ltx_font_italic">very</span></td>
<td id="S5.T4.2.2.2.1.9.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.056</td>
<td id="S5.T4.2.2.2.1.9.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.9.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>6.1K</td>
</tr>
<tr id="S5.T4.2.2.2.1.10" class="ltx_tr">
<td id="S5.T4.2.2.2.1.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.10.1.1" class="ltx_text ltx_font_italic">top</span></td>
<td id="S5.T4.2.2.2.1.10.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.061</td>
<td id="S5.T4.2.2.2.1.10.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.10.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>5.2K</td>
</tr>
<tr id="S5.T4.2.2.2.1.11" class="ltx_tr">
<td id="S5.T4.2.2.2.1.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.11.1.1" class="ltx_text ltx_font_italic">light</span></td>
<td id="S5.T4.2.2.2.1.11.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.072</td>
<td id="S5.T4.2.2.2.1.11.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">18.7K</td>
</tr>
<tr id="S5.T4.2.2.2.1.12" class="ltx_tr">
<td id="S5.T4.2.2.2.1.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.12.1.1" class="ltx_text ltx_font_italic">tiny</span></td>
<td id="S5.T4.2.2.2.1.12.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.076</td>
<td id="S5.T4.2.2.2.1.12.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">
<span id="S5.T4.2.2.2.1.12.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>7.8K</td>
</tr>
<tr id="S5.T4.2.2.2.1.13" class="ltx_tr">
<td id="S5.T4.2.2.2.1.13.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.13.1.1" class="ltx_text ltx_font_italic">large</span></td>
<td id="S5.T4.2.2.2.1.13.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.084</td>
<td id="S5.T4.2.2.2.1.13.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">21.7K</td>
</tr>
<tr id="S5.T4.2.2.2.1.14" class="ltx_tr">
<td id="S5.T4.2.2.2.1.14.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.14.1.1" class="ltx_text ltx_font_italic">the</span></td>
<td id="S5.T4.2.2.2.1.14.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.125</td>
<td id="S5.T4.2.2.2.1.14.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">55.0K</td>
</tr>
<tr id="S5.T4.2.2.2.1.15" class="ltx_tr">
<td id="S5.T4.2.2.2.1.15.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.15.1.1" class="ltx_text ltx_font_italic">one</span></td>
<td id="S5.T4.2.2.2.1.15.2" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">0.136</td>
<td id="S5.T4.2.2.2.1.15.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">57.1K</td>
</tr>
<tr id="S5.T4.2.2.2.1.16" class="ltx_tr">
<td id="S5.T4.2.2.2.1.16.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S5.T4.2.2.2.1.16.1.1" class="ltx_text ltx_font_italic">black</span></td>
<td id="S5.T4.2.2.2.1.16.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">0.145</td>
<td id="S5.T4.2.2.2.1.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">26.9K</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
Tokens with low or high correlation with the exact match rate. Correlation scores are shown in <math id="S5.T4.4.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.T4.4.m1.1b"><mi id="S5.T4.4.m1.1.1" xref="S5.T4.4.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.4.m1.1c"><ci id="S5.T4.4.m1.1.1.cmml" xref="S5.T4.4.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.m1.1d">\rho</annotation></semantics></math>.
</figcaption>
</figure>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">In general, the correlations are very small and the amount of disagreements seem relatively constant across all token types. However, the general trend is still intuitive: ambiguous or complex expressions such as pronouns, interrogatives, quantifiers, and coordinating conjunctions tend to have negative correlations, while simple and less ambiguous expressions tend to have positive correlations.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.1" class="ltx_p">To summarize the analyses, our annotation has high overall agreement but also includes interesting, reasonable disagreements which capture the ambiguity and uncertainty under continuous and partially-observable context.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Pragmatic Expressions</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Finally, as an illustrative example of additional analyses that can be conducted based on our annotation, we give a more quantitative analysis of <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">pragmatic expressions</span> which have been pointed out to exist in previous work but without sufficient amount of evidence (<span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_bold">?</span>).</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">In this work, we focus on pragmatic expression of <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">color</span> and estimate the distribution of the actual color of the referents described by the common adjectives. We simply assume that the adjective in the minimal noun phrase describe the color of the referents, since the exceptions (such as negation in the prenominal modifier) seemed rare and ignorable. Distributions are calculated based on kernel density estimation. As we can see in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.3 Pragmatic Expressions ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, all adjectives (including the specific color <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic">black</span>) have smooth and wide distributions which overlap with each other. This is a strong evidence that the same color can be described in various ways and become more pragmatic under continuous context.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/1911.07588/assets/x9.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="242" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distribution of the actual color of the referents expressed by common adjectives (the range of color is 256 as in RGB scale, lower is darker).</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we evaluate and analyze baseline models based on three tasks. First is the <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">target selection</span> task proposed by <span id="S6.p1.1.2" class="ltx_text ltx_font_bold">?</span> (<span id="S6.p1.1.3" class="ltx_text ltx_font_bold">?</span>), which tries to predict the entity selected by each player at the end of the collaborative referring task: this requires correct recognition of the created common ground based on the dialogue and context (i.e. player’s view). Second is the <span id="S6.p1.1.4" class="ltx_text ltx_font_italic">reference resolution</span> task, where we focus on binary predictions of whether each entity is included in the referents or not. Last is the <span id="S6.p1.1.5" class="ltx_text ltx_font_italic">selfplay dialogue</span> task where the model plays the whole collaborative referring task (Section <a href="#S3" title="3 Background Task ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) against an identical copy of itself.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">For reference resolution, we use simple majority voting (at the entity level) and automatic annotation of the referents to create gold annotation. Markables are removed if the majority considered them as unidentifiable.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Model Architecture</h3>

<figure id="S6.F6" class="ltx_figure"><img src="/html/1911.07588/assets/x10.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Our baseline model architecture (best seen in color). <span id="S6.F6.5.1" class="ltx_text ltx_font_typewriter">TSEL</span> decoder is shown in green, <span id="S6.F6.6.2" class="ltx_text ltx_font_typewriter">REF</span> decoder and the input markable <span id="S6.F6.7.3" class="ltx_text ltx_font_italic">three black dots</span> are in red, and <span id="S6.F6.8.4" class="ltx_text ltx_font_typewriter">DIAL</span> decoder is in blue. All decoders share the entity-level attention module.</figcaption>
</figure>
<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The overall architecture of our baseline models is shown in Figure <a href="#S6.F6" title="Figure 6 ‣ 6.1 Model Architecture ‣ 6 Experiments ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<section id="S6.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Encoders</h4>

<div id="S6.SS1.SSSx1.p1" class="ltx_para">
<p id="S6.SS1.SSSx1.p1.1" class="ltx_p">Our baseline models have two encoders: one for encoding dialogue tokens and one for context information.</p>
</div>
<div id="S6.SS1.SSSx1.p2" class="ltx_para">
<p id="S6.SS1.SSSx1.p2.1" class="ltx_p">Dialogue tokens are encoded with a standard GRU (<span id="S6.SS1.SSSx1.p2.1.1" class="ltx_text ltx_font_bold">?</span>). To encode context information, we embed each entity using a shared <span id="S6.SS1.SSSx1.p2.1.2" class="ltx_text ltx_font_italic">entity encoder</span>. This consists of an <span id="S6.SS1.SSSx1.p2.1.3" class="ltx_text ltx_font_italic">attribute encoder</span> which embeds the attributes of each entity (size, color and location) with a matrix followed by a tanh layer, and a <span id="S6.SS1.SSSx1.p2.1.4" class="ltx_text ltx_font_italic">relational encoder</span> which embeds relative attributes of each entity pairs (e.g. distance) with another matrix followed by a tanh layer. The final embedding of each entity is the concatenation of its attribute embedding and the sum of relational embeddings with the other 6 entities.</p>
</div>
</section>
<section id="S6.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Decoders</h4>

<div id="S6.SS1.SSSx2.p1" class="ltx_para">
<p id="S6.SS1.SSSx2.p1.1" class="ltx_p">Our models can have up to three decoders: <span id="S6.SS1.SSSx2.p1.1.1" class="ltx_text ltx_font_typewriter">TSEL</span> for target selection, <span id="S6.SS1.SSSx2.p1.1.2" class="ltx_text ltx_font_typewriter">REF</span> for reference resolution, and <span id="S6.SS1.SSSx2.p1.1.3" class="ltx_text ltx_font_typewriter">DIAL</span> for predicting next tokens. Each decoder shares (some or all layers of) the <span id="S6.SS1.SSSx2.p1.1.4" class="ltx_text ltx_font_italic">attention module</span> based on MLP to compute a scalar score for each entity based on its embedding and certain positions of the GRU: <span id="S6.SS1.SSSx2.p1.1.5" class="ltx_text ltx_font_typewriter">TSEL</span> takes the final hidden state, <span id="S6.SS1.SSSx2.p1.1.6" class="ltx_text ltx_font_typewriter">REF</span> takes (the mean of) the start position of the markable, the end position of the markable, and the end position of the utterance including the markable, and <span id="S6.SS1.SSSx2.p1.1.7" class="ltx_text ltx_font_typewriter">DIAL</span> takes the current hidden state. Based on these attention scores, <span id="S6.SS1.SSSx2.p1.1.8" class="ltx_text ltx_font_typewriter">TSEL</span> simply computes the softmax and <span id="S6.SS1.SSSx2.p1.1.9" class="ltx_text ltx_font_typewriter">REF</span> computes logistic regressions for each entity. <span id="S6.SS1.SSSx2.p1.1.10" class="ltx_text ltx_font_typewriter">DIAL</span> reweights the entity embeddings based on these attention scores, concatenates it with the current hidden state and decodes with an MLP (<span id="S6.SS1.SSSx2.p1.1.11" class="ltx_text ltx_font_bold">?</span>).
<br class="ltx_break"></p>
</div>
<div id="S6.SS1.SSSx2.p2" class="ltx_para">
<p id="S6.SS1.SSSx2.p2.1" class="ltx_p">In this experiment, we built five models based on different combinations of the three decoders. All models are trained with the default hyperparameters with minimal tuning.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Results</h3>

<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.16" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T5.16.17.1" class="ltx_tr">
<th id="S6.T5.16.17.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S6.T5.16.17.1.1.1" class="ltx_text">Model</span></th>
<td id="S6.T5.16.17.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S6.T5.16.17.1.2.1" class="ltx_text">Target Selection</span></td>
<td id="S6.T5.16.17.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S6.T5.16.17.1.3.1" class="ltx_text">Reference Resolution (Exact Match)</span></td>
<td id="S6.T5.16.17.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">Selfplay Dialogue</td>
</tr>
<tr id="S6.T5.16.18.2" class="ltx_tr">
<td id="S6.T5.16.18.2.1" class="ltx_td ltx_align_center">#Shared=4</td>
<td id="S6.T5.16.18.2.2" class="ltx_td ltx_align_center">#Shared=5</td>
<td id="S6.T5.16.18.2.3" class="ltx_td ltx_align_center">#Shared=6</td>
</tr>
<tr id="S6.T5.1.1" class="ltx_tr">
<th id="S6.T5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S6.T5.1.1.2.1" class="ltx_text ltx_font_typewriter">TSEL</span></th>
<td id="S6.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_t">67.79<math id="S6.T5.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.1.1.1.m1.1a"><mo id="S6.T5.1.1.1.m1.1.1" xref="S6.T5.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.m1.1b"><csymbol cd="latexml" id="S6.T5.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.m1.1c">\pm</annotation></semantics></math>1.53</td>
<td id="S6.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S6.T5.3.3" class="ltx_tr">
<th id="S6.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T5.3.3.3.1" class="ltx_text ltx_font_typewriter">REF</span></th>
<td id="S6.T5.3.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T5.3.3.2" class="ltx_td ltx_align_center">
<span id="S6.T5.2.2.1.1" class="ltx_text ltx_font_bold">85.75<math id="S6.T5.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.2.2.1.1.m1.1a"><mo id="S6.T5.2.2.1.1.m1.1.1" xref="S6.T5.2.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.1.1.m1.1b"><csymbol cd="latexml" id="S6.T5.2.2.1.1.m1.1.1.cmml" xref="S6.T5.2.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.1.1.m1.1c">\pm</annotation></semantics></math>0.22</span> (<span id="S6.T5.3.3.2.2" class="ltx_text ltx_font_bold">33.91<math id="S6.T5.3.3.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.3.3.2.2.m1.1a"><mo id="S6.T5.3.3.2.2.m1.1.1" xref="S6.T5.3.3.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.3.3.2.2.m1.1b"><csymbol cd="latexml" id="S6.T5.3.3.2.2.m1.1.1.cmml" xref="S6.T5.3.3.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.3.2.2.m1.1c">\pm</annotation></semantics></math>0.86</span>)</td>
<td id="S6.T5.3.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S6.T5.3.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S6.T5.3.3.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T5.6.6" class="ltx_tr">
<th id="S6.T5.6.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T5.6.6.4.1" class="ltx_text ltx_font_typewriter">TSEL-REF</span></th>
<td id="S6.T5.4.4.1" class="ltx_td ltx_align_center"><span id="S6.T5.4.4.1.1" class="ltx_text ltx_font_bold">69.01<math id="S6.T5.4.4.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.4.4.1.1.m1.1a"><mo id="S6.T5.4.4.1.1.m1.1.1" xref="S6.T5.4.4.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.4.4.1.1.m1.1b"><csymbol cd="latexml" id="S6.T5.4.4.1.1.m1.1.1.cmml" xref="S6.T5.4.4.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.4.4.1.1.m1.1c">\pm</annotation></semantics></math>1.58</span></td>
<td id="S6.T5.6.6.3" class="ltx_td ltx_align_center">85.47<math id="S6.T5.5.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.5.5.2.m1.1a"><mo id="S6.T5.5.5.2.m1.1.1" xref="S6.T5.5.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.5.5.2.m1.1b"><csymbol cd="latexml" id="S6.T5.5.5.2.m1.1.1.cmml" xref="S6.T5.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.5.5.2.m1.1c">\pm</annotation></semantics></math>0.36 (32.88<math id="S6.T5.6.6.3.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.6.6.3.m2.1a"><mo id="S6.T5.6.6.3.m2.1.1" xref="S6.T5.6.6.3.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.6.6.3.m2.1b"><csymbol cd="latexml" id="S6.T5.6.6.3.m2.1.1.cmml" xref="S6.T5.6.6.3.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.6.6.3.m2.1c">\pm</annotation></semantics></math>1.28)</td>
<td id="S6.T5.6.6.5" class="ltx_td ltx_align_center">-</td>
<td id="S6.T5.6.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S6.T5.6.6.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T5.10.10" class="ltx_tr">
<th id="S6.T5.10.10.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S6.T5.10.10.5.1" class="ltx_text ltx_font_typewriter">TSEL-DIAL</span></th>
<td id="S6.T5.7.7.1" class="ltx_td ltx_align_center ltx_border_t">67.01<math id="S6.T5.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.7.7.1.m1.1a"><mo id="S6.T5.7.7.1.m1.1.1" xref="S6.T5.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.7.7.1.m1.1b"><csymbol cd="latexml" id="S6.T5.7.7.1.m1.1.1.cmml" xref="S6.T5.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.7.7.1.m1.1c">\pm</annotation></semantics></math>1.29</td>
<td id="S6.T5.10.10.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T5.8.8.2" class="ltx_td ltx_align_center ltx_border_t">42.07<math id="S6.T5.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.8.8.2.m1.1a"><mo id="S6.T5.8.8.2.m1.1.1" xref="S6.T5.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.8.8.2.m1.1b"><csymbol cd="latexml" id="S6.T5.8.8.2.m1.1.1.cmml" xref="S6.T5.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.8.8.2.m1.1c">\pm</annotation></semantics></math>1.27</td>
<td id="S6.T5.9.9.3" class="ltx_td ltx_align_center ltx_border_t">57.37<math id="S6.T5.9.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.9.9.3.m1.1a"><mo id="S6.T5.9.9.3.m1.1.1" xref="S6.T5.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.9.9.3.m1.1b"><csymbol cd="latexml" id="S6.T5.9.9.3.m1.1.1.cmml" xref="S6.T5.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.9.9.3.m1.1c">\pm</annotation></semantics></math>1.29</td>
<td id="S6.T5.10.10.4" class="ltx_td ltx_align_center ltx_border_t">77.00<math id="S6.T5.10.10.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.10.10.4.m1.1a"><mo id="S6.T5.10.10.4.m1.1.1" xref="S6.T5.10.10.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.10.10.4.m1.1b"><csymbol cd="latexml" id="S6.T5.10.10.4.m1.1.1.cmml" xref="S6.T5.10.10.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.10.10.4.m1.1c">\pm</annotation></semantics></math>1.13</td>
</tr>
<tr id="S6.T5.16.16" class="ltx_tr">
<th id="S6.T5.16.16.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T5.16.16.7.1" class="ltx_text ltx_font_typewriter">TSEL-REF-DIAL</span></th>
<td id="S6.T5.11.11.1" class="ltx_td ltx_align_center"><span id="S6.T5.11.11.1.1" class="ltx_text ltx_font_bold">69.09<math id="S6.T5.11.11.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.11.11.1.1.m1.1a"><mo id="S6.T5.11.11.1.1.m1.1.1" xref="S6.T5.11.11.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.11.11.1.1.m1.1b"><csymbol cd="latexml" id="S6.T5.11.11.1.1.m1.1.1.cmml" xref="S6.T5.11.11.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.11.11.1.1.m1.1c">\pm</annotation></semantics></math>1.12</span></td>
<td id="S6.T5.13.13.3" class="ltx_td ltx_align_center">
<span id="S6.T5.12.12.2.1" class="ltx_text ltx_font_bold">85.86<math id="S6.T5.12.12.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.12.12.2.1.m1.1a"><mo id="S6.T5.12.12.2.1.m1.1.1" xref="S6.T5.12.12.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.12.12.2.1.m1.1b"><csymbol cd="latexml" id="S6.T5.12.12.2.1.m1.1.1.cmml" xref="S6.T5.12.12.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.12.12.2.1.m1.1c">\pm</annotation></semantics></math>0.18</span> (<span id="S6.T5.13.13.3.2" class="ltx_text ltx_font_bold">33.66<math id="S6.T5.13.13.3.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.13.13.3.2.m1.1a"><mo id="S6.T5.13.13.3.2.m1.1.1" xref="S6.T5.13.13.3.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.13.13.3.2.m1.1b"><csymbol cd="latexml" id="S6.T5.13.13.3.2.m1.1.1.cmml" xref="S6.T5.13.13.3.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.13.13.3.2.m1.1c">\pm</annotation></semantics></math>0.93</span>)</td>
<td id="S6.T5.14.14.4" class="ltx_td ltx_align_center"><span id="S6.T5.14.14.4.1" class="ltx_text ltx_font_bold">45.78<math id="S6.T5.14.14.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.14.14.4.1.m1.1a"><mo id="S6.T5.14.14.4.1.m1.1.1" xref="S6.T5.14.14.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.14.14.4.1.m1.1b"><csymbol cd="latexml" id="S6.T5.14.14.4.1.m1.1.1.cmml" xref="S6.T5.14.14.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.14.14.4.1.m1.1c">\pm</annotation></semantics></math>2.15</span></td>
<td id="S6.T5.15.15.5" class="ltx_td ltx_align_center"><span id="S6.T5.15.15.5.1" class="ltx_text ltx_font_bold">61.95<math id="S6.T5.15.15.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.15.15.5.1.m1.1a"><mo id="S6.T5.15.15.5.1.m1.1.1" xref="S6.T5.15.15.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.15.15.5.1.m1.1b"><csymbol cd="latexml" id="S6.T5.15.15.5.1.m1.1.1.cmml" xref="S6.T5.15.15.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.15.15.5.1.m1.1c">\pm</annotation></semantics></math>1.72</span></td>
<td id="S6.T5.16.16.6" class="ltx_td ltx_align_center"><span id="S6.T5.16.16.6.1" class="ltx_text ltx_font_bold">80.01<math id="S6.T5.16.16.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T5.16.16.6.1.m1.1a"><mo id="S6.T5.16.16.6.1.m1.1.1" xref="S6.T5.16.16.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T5.16.16.6.1.m1.1b"><csymbol cd="latexml" id="S6.T5.16.16.6.1.m1.1.1.cmml" xref="S6.T5.16.16.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.16.16.6.1.m1.1c">\pm</annotation></semantics></math>1.61</span></td>
</tr>
<tr id="S6.T5.16.19.3" class="ltx_tr">
<th id="S6.T5.16.19.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Human</th>
<td id="S6.T5.16.19.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">90.79</td>
<td id="S6.T5.16.19.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">96.26 (86.90)</td>
<td id="S6.T5.16.19.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">65.83</td>
<td id="S6.T5.16.19.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">76.96</td>
<td id="S6.T5.16.19.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">87.00</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
Results of our baseline models. Human scores from <span id="S6.T5.19.1" class="ltx_text ltx_font_bold">?</span> (<span id="S6.T5.20.2" class="ltx_text ltx_font_bold">?</span>) and Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Basic Statistics ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> as a reference.
</figcaption>
</figure>
<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We run the experiments 10 times with different random seeds and dataset splits (8:1:1 for train, validation and test). For selfplay dialogues, we generated 1,000 scenarios with each number of shared entities (4, 5 or 6) and set the output temperature to 0.25 during next token prediction. We report the mean and standard deviation of the results in Table <a href="#S6.T5" title="Table 5 ‣ 6.2 Results ‣ 6 Experiments ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">In terms of <span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_italic">target selection</span> and <span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_italic">selfplay dialogue</span> tasks, we found consistent improvements by training the models jointly with reference resolution. This verified that we can indeed leverage the central subtask of reference resolution to improve performance on difficult end tasks. The results for <span id="S6.SS2.p2.1.3" class="ltx_text ltx_font_italic">reference resolution</span> are reasonably high in terms of entity level accuracy but much lower in terms of exact match rate. Considering the high agreements (Subsection <a href="#S5.SS1" title="5.1 Basic Statistics ‣ 5 Annotated Corpus ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>) and improved reliability of the gold annotation after aggregation, we expect there to be a huge room for further improvements.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">Overall, common grounding under continuous and partially-observable context is still a challenging task, and we expect our resource to be a fundamental testbed for solving this task through advanced skills of reference resolution.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Analysis</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">To demonstrate the advantages of our approach for interpreting and analyzing dialogue systems, we give a more detailed analysis of <span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">TSEL-REF-DIAL</span> model which performed well on all three tasks. In Table <a href="#S6.T6" title="Table 6 ‣ 6.3 Analysis ‣ 6 Experiments ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we show the results for reference resolution (entity level accuracy and exact match rate) grouped by the number of referents in the gold annotation. In terms of the exact match rate, we found that the model performs very well on 0 and 7 referents: this is because most of them can be recognized at the superficial level, such as <span id="S6.SS3.p1.1.2" class="ltx_text ltx_font_italic">“<span id="S6.SS3.p1.1.2.1" class="ltx_text ltx_framed ltx_framed_underline">none of them</span>”</span>, <span id="S6.SS3.p1.1.3" class="ltx_text ltx_font_italic">“<span id="S6.SS3.p1.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">all of mine</span>”</span> or <span id="S6.SS3.p1.1.4" class="ltx_text ltx_font_italic">“I don’t have <span id="S6.SS3.p1.1.4.1" class="ltx_text ltx_framed ltx_framed_underline">that</span>”</span>. However, the model struggles on all other cases: the results are especially worse for markables with more than 1 referent. This shows that the model still lacks the ability of precisely tracking multiple referents, which can be expressed in complex, pragmatic ways (such as groupings).</p>
</div>
<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.16" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.16.17.1" class="ltx_tr">
<th id="S6.T6.16.17.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"># Referents</th>
<th id="S6.T6.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">% Accuracy</th>
<th id="S6.T6.16.17.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">% Exact Match</th>
<th id="S6.T6.16.17.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Count</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.2.2" class="ltx_tr">
<th id="S6.T6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0</th>
<td id="S6.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_t">95.91<math id="S6.T6.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.1.1.1.m1.1a"><mo id="S6.T6.1.1.1.m1.1.1" xref="S6.T6.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.1.1.1.m1.1b"><csymbol cd="latexml" id="S6.T6.1.1.1.m1.1.1.cmml" xref="S6.T6.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.1.1.1.m1.1c">\pm</annotation></semantics></math>1.38</td>
<td id="S6.T6.2.2.2" class="ltx_td ltx_align_center ltx_border_t">83.53<math id="S6.T6.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.2.2.2.m1.1a"><mo id="S6.T6.2.2.2.m1.1.1" xref="S6.T6.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.2.2.2.m1.1b"><csymbol cd="latexml" id="S6.T6.2.2.2.m1.1.1.cmml" xref="S6.T6.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.2.2.2.m1.1c">\pm</annotation></semantics></math>4.65<span id="S6.T6.2.2.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>
</td>
<td id="S6.T6.2.2.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.2.2.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>148.5</td>
</tr>
<tr id="S6.T6.4.4" class="ltx_tr">
<th id="S6.T6.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1</th>
<td id="S6.T6.3.3.1" class="ltx_td ltx_align_center">89.34<math id="S6.T6.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.3.3.1.m1.1a"><mo id="S6.T6.3.3.1.m1.1.1" xref="S6.T6.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.3.3.1.m1.1b"><csymbol cd="latexml" id="S6.T6.3.3.1.m1.1.1.cmml" xref="S6.T6.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.3.3.1.m1.1c">\pm</annotation></semantics></math>0.17</td>
<td id="S6.T6.4.4.2" class="ltx_td ltx_align_center">36.86<math id="S6.T6.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.4.4.2.m1.1a"><mo id="S6.T6.4.4.2.m1.1.1" xref="S6.T6.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.4.4.2.m1.1b"><csymbol cd="latexml" id="S6.T6.4.4.2.m1.1.1.cmml" xref="S6.T6.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.4.4.2.m1.1c">\pm</annotation></semantics></math>1.32<span id="S6.T6.4.4.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>
</td>
<td id="S6.T6.4.4.4" class="ltx_td ltx_align_center">2782.5</td>
</tr>
<tr id="S6.T6.6.6" class="ltx_tr">
<th id="S6.T6.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2</th>
<td id="S6.T6.5.5.1" class="ltx_td ltx_align_center">78.14<math id="S6.T6.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.5.5.1.m1.1a"><mo id="S6.T6.5.5.1.m1.1.1" xref="S6.T6.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.5.5.1.m1.1b"><csymbol cd="latexml" id="S6.T6.5.5.1.m1.1.1.cmml" xref="S6.T6.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.5.5.1.m1.1c">\pm</annotation></semantics></math>1.07</td>
<td id="S6.T6.6.6.2" class="ltx_td ltx_align_center">20.59<math id="S6.T6.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.6.6.2.m1.1a"><mo id="S6.T6.6.6.2.m1.1.1" xref="S6.T6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.6.6.2.m1.1b"><csymbol cd="latexml" id="S6.T6.6.6.2.m1.1.1.cmml" xref="S6.T6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.6.6.2.m1.1c">\pm</annotation></semantics></math>1.90<span id="S6.T6.6.6.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>
</td>
<td id="S6.T6.6.6.4" class="ltx_td ltx_align_center">
<span id="S6.T6.6.6.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>587.9</td>
</tr>
<tr id="S6.T6.8.8" class="ltx_tr">
<th id="S6.T6.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3</th>
<td id="S6.T6.7.7.1" class="ltx_td ltx_align_center">70.64<math id="S6.T6.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.7.7.1.m1.1a"><mo id="S6.T6.7.7.1.m1.1.1" xref="S6.T6.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.7.7.1.m1.1b"><csymbol cd="latexml" id="S6.T6.7.7.1.m1.1.1.cmml" xref="S6.T6.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.7.7.1.m1.1c">\pm</annotation></semantics></math>1.02</td>
<td id="S6.T6.8.8.2" class="ltx_td ltx_align_center">13.63<math id="S6.T6.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.8.8.2.m1.1a"><mo id="S6.T6.8.8.2.m1.1.1" xref="S6.T6.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.8.8.2.m1.1b"><csymbol cd="latexml" id="S6.T6.8.8.2.m1.1.1.cmml" xref="S6.T6.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.8.8.2.m1.1c">\pm</annotation></semantics></math>2.06<span id="S6.T6.8.8.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>
</td>
<td id="S6.T6.8.8.4" class="ltx_td ltx_align_center">
<span id="S6.T6.8.8.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>283.3</td>
</tr>
<tr id="S6.T6.10.10" class="ltx_tr">
<th id="S6.T6.10.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4</th>
<td id="S6.T6.9.9.1" class="ltx_td ltx_align_center">69.12<math id="S6.T6.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.9.9.1.m1.1a"><mo id="S6.T6.9.9.1.m1.1.1" xref="S6.T6.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.9.9.1.m1.1b"><csymbol cd="latexml" id="S6.T6.9.9.1.m1.1.1.cmml" xref="S6.T6.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.9.9.1.m1.1c">\pm</annotation></semantics></math>2.69</td>
<td id="S6.T6.10.10.2" class="ltx_td ltx_align_center">10.16<math id="S6.T6.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.10.10.2.m1.1a"><mo id="S6.T6.10.10.2.m1.1.1" xref="S6.T6.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.10.10.2.m1.1b"><csymbol cd="latexml" id="S6.T6.10.10.2.m1.1.1.cmml" xref="S6.T6.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.10.10.2.m1.1c">\pm</annotation></semantics></math>3.47<span id="S6.T6.10.10.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>
</td>
<td id="S6.T6.10.10.4" class="ltx_td ltx_align_center">
<span id="S6.T6.10.10.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">00</span></span>81.0</td>
</tr>
<tr id="S6.T6.12.12" class="ltx_tr">
<th id="S6.T6.12.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">5</th>
<td id="S6.T6.11.11.1" class="ltx_td ltx_align_center">73.57<math id="S6.T6.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.11.11.1.m1.1a"><mo id="S6.T6.11.11.1.m1.1.1" xref="S6.T6.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.11.11.1.m1.1b"><csymbol cd="latexml" id="S6.T6.11.11.1.m1.1.1.cmml" xref="S6.T6.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.11.11.1.m1.1c">\pm</annotation></semantics></math>2.94</td>
<td id="S6.T6.12.12.2" class="ltx_td ltx_align_center">17.56<math id="S6.T6.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.12.12.2.m1.1a"><mo id="S6.T6.12.12.2.m1.1.1" xref="S6.T6.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.12.12.2.m1.1b"><csymbol cd="latexml" id="S6.T6.12.12.2.m1.1.1.cmml" xref="S6.T6.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.12.12.2.m1.1c">\pm</annotation></semantics></math>5.88<span id="S6.T6.12.12.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>
</td>
<td id="S6.T6.12.12.4" class="ltx_td ltx_align_center">
<span id="S6.T6.12.12.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">00</span></span>33.0</td>
</tr>
<tr id="S6.T6.14.14" class="ltx_tr">
<th id="S6.T6.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">6</th>
<td id="S6.T6.13.13.1" class="ltx_td ltx_align_center">78.69<math id="S6.T6.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.13.13.1.m1.1a"><mo id="S6.T6.13.13.1.m1.1.1" xref="S6.T6.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.13.13.1.m1.1b"><csymbol cd="latexml" id="S6.T6.13.13.1.m1.1.1.cmml" xref="S6.T6.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.13.13.1.m1.1c">\pm</annotation></semantics></math>4.45</td>
<td id="S6.T6.14.14.2" class="ltx_td ltx_align_center">13.18<math id="S6.T6.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.14.14.2.m1.1a"><mo id="S6.T6.14.14.2.m1.1.1" xref="S6.T6.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.14.14.2.m1.1b"><csymbol cd="latexml" id="S6.T6.14.14.2.m1.1.1.cmml" xref="S6.T6.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.14.14.2.m1.1c">\pm</annotation></semantics></math>7.31<span id="S6.T6.14.14.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>
</td>
<td id="S6.T6.14.14.4" class="ltx_td ltx_align_center">
<span id="S6.T6.14.14.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">00</span></span>43.0</td>
</tr>
<tr id="S6.T6.16.16" class="ltx_tr">
<th id="S6.T6.16.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">7</th>
<td id="S6.T6.15.15.1" class="ltx_td ltx_align_center ltx_border_bb">74.60<math id="S6.T6.15.15.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.15.15.1.m1.1a"><mo id="S6.T6.15.15.1.m1.1.1" xref="S6.T6.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.15.15.1.m1.1b"><csymbol cd="latexml" id="S6.T6.15.15.1.m1.1.1.cmml" xref="S6.T6.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.15.15.1.m1.1c">\pm</annotation></semantics></math>7.49</td>
<td id="S6.T6.16.16.2" class="ltx_td ltx_align_center ltx_border_bb">50.38<math id="S6.T6.16.16.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T6.16.16.2.m1.1a"><mo id="S6.T6.16.16.2.m1.1.1" xref="S6.T6.16.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T6.16.16.2.m1.1b"><csymbol cd="latexml" id="S6.T6.16.16.2.m1.1.1.cmml" xref="S6.T6.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.16.16.2.m1.1c">\pm</annotation></semantics></math>11.40</td>
<td id="S6.T6.16.16.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S6.T6.16.16.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">00</span></span>22.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>
Results of the reference resolution task grouped by the number of referents in the gold annotation (along with the average count of such markables in the test set).
</figcaption>
</figure>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">In addition, we found that the correlation between reference resolution score (average accuracy of reference resolution in each dialogue) and target selection score (binary result of target selection in each dialogue) was relatively weak, with an average of only <math id="S6.SS3.p2.1.m1.1" class="ltx_Math" alttext="0.23" display="inline"><semantics id="S6.SS3.p2.1.m1.1a"><mn id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml">0.23</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><cn type="float" id="S6.SS3.p2.1.m1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1">0.23</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">0.23</annotation></semantics></math> in 10 runs of the experiments. Indeed, we verified that the model is often correct for the target selection task based on the <span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_italic">wrong reason</span>, without tracking the referents correctly. Our annotation is also useful for <span id="S6.SS3.p2.1.2" class="ltx_text ltx_font_italic">error analysis</span> in recognizing the process of common grounding, by inspecting where the model made a mistake and lost track of the correct referents.</p>
</div>
<figure id="S6.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><svg id="S6.F7.pic1" class="ltx_picture ltx_centering ltx_figure_panel" height="178.35" overflow="visible" version="1.1" width="326.35"><g transform="translate(0,178.35) matrix(1 0 0 -1 0 0) translate(80.5,0) translate(0,97.85)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -80.5 -80.5)" fill="#000000" stroke="#000000"><foreignObject width="161" height="161" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x11.png" id="S6.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="123" height="123" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 84.85 -80.5)" fill="#000000" stroke="#000000"><foreignObject width="161" height="161" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1911.07588/assets/x12.png" id="S6.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="123" height="123" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -46.93 -93.24)" fill="#000000" stroke="#000000"><foreignObject width="93.48" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F7.pic1.3.3.3.1.1" class="ltx_text">Model A’s view</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 118.71 -93.24)" fill="#000000" stroke="#000000"><foreignObject width="92.9" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F7.pic1.4.4.4.1.1" class="ltx_text">Model B’s view</span></foreignObject></g></g></svg></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S6.F7.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.F7.1.1.1" class="ltx_tr">
<td id="S6.F7.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">A: I have <span id="S6.F7.1.1.1.1.1" class="ltx_text" style="color:#FF0000;"> <span id="S6.F7.1.1.1.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">a large black dot</span></span> with <span id="S6.F7.1.1.1.1.2" class="ltx_text" style="color:#0000FF;"> <span id="S6.F7.1.1.1.1.2.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">a smaller dark dot</span></span> to the</td>
</tr>
<tr id="S6.F7.1.2.2" class="ltx_tr">
<td id="S6.F7.1.2.2.1" class="ltx_td ltx_align_left">right of <span id="S6.F7.1.2.2.1.1" class="ltx_text" style="color:#FF0000;"> <span id="S6.F7.1.2.2.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">it</span></span>
</td>
</tr>
<tr id="S6.F7.1.3.3" class="ltx_tr">
<td id="S6.F7.1.3.3.1" class="ltx_td ltx_align_left ltx_border_bb">B: I see <span id="S6.F7.1.3.3.1.1" class="ltx_text" style="color:#00BF00;"> <span id="S6.F7.1.3.3.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">that</span></span> . Let’s pick <span id="S6.F7.1.3.3.1.2" class="ltx_text ltx_framed ltx_framed_underline">the large black dot</span>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example dialogue from the selfplay task by <span id="S6.F7.4.1" class="ltx_text ltx_font_typewriter">TSEL-REF-DIAL</span> model. Predicted referents are highlighted (no referents were predicted for <span id="S6.F7.5.2" class="ltx_text ltx_font_italic">the large black dot</span>).
</figcaption>
</figure>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">Finally, we show an example dialogue from the selfplay task along with the interpreted process of common grounding in Figure <a href="#S6.F7" title="Figure 7 ‣ 6.3 Analysis ‣ 6 Experiments ‣ An Annotated Corpus of Reference Resolution for Interpreting Common Grounding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Referring expressions are automatically detected by a BiLSTM-CRF tagger (<span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_bold">?</span>) trained on our corpus (with 98.9% accuracy at the token level). Based on the raw dialogue only, it is difficult to identify which dots the models are referring to. However, by visualizing the intended referents, we can see that model A is describing two dots in somewhat unnatural and inappropriate way (albeit using the anaphoric expression <span id="S6.SS3.p3.1.2" class="ltx_text ltx_font_italic">it</span> appropriately). In turn, model B acknowledges this in a perfectly coherent way but without predicting any referents for <span id="S6.SS3.p3.1.3" class="ltx_text ltx_font_italic">the large black dot</span>: we often observed such phenomena, where the utterance by a model cannot be interpreted correctly <span id="S6.SS3.p3.1.4" class="ltx_text ltx_font_italic">even by itself</span>. This way, our annotation allows for fine-grained analysis of both <span id="S6.SS3.p3.1.5" class="ltx_text ltx_font_italic">capabilities</span> and <span id="S6.SS3.p3.1.6" class="ltx_text ltx_font_italic">incapabilities</span> of existing dialogue systems. The generated dialogue is short in this example, but our approach would be even more critical for interpretation as the dialogues get longer and more complicated.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We propose a novel method of decomposing common grounding based on its subtasks to study the intermediate process of common grounding. We demonstrated the advantages of our approach through extensive analysis of the annotated corpus and the baseline models. Overall, we expect our work to be a fundamental step towards interpreting and improving common grounding through reference resolution.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by JSPS KAKENHI Grant Number 18H03297 and NEDO SIP-2 “Big-data and AI-enabled Cyberspace Technologies.” We also thank the anonymous reviewers for their valuable suggestions and comments.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Bahdanau, Cho, and
Bengio 2014]</span>
<span class="ltx_bibblock">
Bahdanau, D.; Cho, K.; and Bengio, Y.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock"><span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.0473</span>.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Belinkov and
Glass 2019]</span>
<span class="ltx_bibblock">
Belinkov, Y., and Glass, J.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Analysis methods in neural language processing: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>
7:49–72.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Bordes and Weston 2016]</span>
<span class="ltx_bibblock">
Bordes, A., and Weston, J.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Learning end-to-end goal-oriented dialog.

</span>
<span class="ltx_bibblock"><span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1605.07683.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chen et al<span id="bib.bibx4.1.1.1" class="ltx_text">.</span> 2019]</span>
<span class="ltx_bibblock">
Chen, H.; Suhr, A.; Misra, D.; Snavely, N.; and Artzi, Y.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Touchdown: Natural language navigation and spatial reasoning in
visual street environments.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx4.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, 12538–12547.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Cho et al<span id="bib.bibx5.1.1.1" class="ltx_text">.</span> 2014]</span>
<span class="ltx_bibblock">
Cho, K.; van Merrienboer, B.; Bahdanau, D.; and Bengio, Y.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">On the properties of neural machine translation: Encoder–decoder
approaches.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.3.1" class="ltx_text ltx_font_italic">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation</span>, 103–111.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Clark 1996]</span>
<span class="ltx_bibblock">
Clark, H. H.

</span>
<span class="ltx_bibblock">1996.

</span>
<span class="ltx_bibblock"><span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">Using language</span>.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Das et al<span id="bib.bibx7.1.1.1" class="ltx_text">.</span> 2017]</span>
<span class="ltx_bibblock">
Das, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura, J. M.; Parikh, D.;
and Batra, D.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Visual dialog.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx7.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, volume 2.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[De Vries et al<span id="bib.bibx8.1.1.1" class="ltx_text">.</span> 2017]</span>
<span class="ltx_bibblock">
De Vries, H.; Strub, F.; Chandar, S.; Pietquin, O.; Larochelle, H.; and
Courville, A.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Guesswhat?! visual object discovery through multi-modal dialogue.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx8.3.1" class="ltx_text ltx_font_italic">Proc. of CVPR</span>.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Doshi-Velez and Kim 2017]</span>
<span class="ltx_bibblock">
Doshi-Velez, F., and Kim, B.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Towards a rigorous science of interpretable machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1702.08608</span>.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Fleiss 1971]</span>
<span class="ltx_bibblock">
Fleiss, J. L.

</span>
<span class="ltx_bibblock">1971.

</span>
<span class="ltx_bibblock">Measuring nominal scale agreement among many raters.

</span>
<span class="ltx_bibblock"><span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">Psychological bulletin</span> 76(5):378.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Götze and
Boye 2016]</span>
<span class="ltx_bibblock">
Götze, J., and Boye, J.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">SpaceRef: A corpus of street-level geographic descriptions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC’16)</span>, 3822–3827.

</span>
<span class="ltx_bibblock">Portorož, Slovenia: European Language Resources Association
(ELRA).

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Gururangan et al<span id="bib.bibx12.1.1.1" class="ltx_text">.</span> 2018]</span>
<span class="ltx_bibblock">
Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.; Bowman, S.; and Smith,
N. A.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Annotation artifacts in natural language inference data.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx12.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</span>, 107–112.

</span>
<span class="ltx_bibblock">New Orleans, Louisiana: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Haber et al<span id="bib.bibx13.1.1.1" class="ltx_text">.</span> 2019]</span>
<span class="ltx_bibblock">
Haber, J.; Baumgärtner, T.; Takmaz, E.; Gelderloos, L.; Bruni, E.; and
Fernández, R.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">The PhotoBook dataset: Building common ground through
visually-grounded dialogue.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.3.1" class="ltx_text ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span>, 1895–1910.

</span>
<span class="ltx_bibblock">Florence, Italy: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hernndez-Orallo 2017]</span>
<span class="ltx_bibblock">
Hernndez-Orallo, J.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock"><span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">The Measure of All Minds: Evaluating Natural and Artificial
Intelligence</span>.

</span>
<span class="ltx_bibblock">New York, NY, USA: Cambridge University Press, 1st edition.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Huang, Xu, and
Yu 2015]</span>
<span class="ltx_bibblock">
Huang, Z.; Xu, W.; and Yu, K.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Bidirectional lstm-crf models for sequence tagging.

</span>
<span class="ltx_bibblock"><span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1508.01991.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ilinykh, Zarrieß, and
Schlangen 2019]</span>
<span class="ltx_bibblock">
Ilinykh, N.; Zarrieß, S.; and Schlangen, D.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Meetup! a corpus of joint activity dialogues in a visual environment.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.05084</span>.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Koschmann and
LeBaron 2003]</span>
<span class="ltx_bibblock">
Koschmann, T., and LeBaron, C. D.

</span>
<span class="ltx_bibblock">2003.

</span>
<span class="ltx_bibblock">Reconsidering common ground.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">ECSCW 2003</span>, 81–98.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Linzen, Dupoux, and
Goldberg 2016]</span>
<span class="ltx_bibblock">
Linzen, T.; Dupoux, E.; and Goldberg, Y.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Assessing the ability of lstms to learn syntax-sensitive
dependencies.

</span>
<span class="ltx_bibblock"><span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>
4:521–535.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Lipton 2016]</span>
<span class="ltx_bibblock">
Lipton, Z. C.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">The mythos of model interpretability.

</span>
<span class="ltx_bibblock"><span id="bib.bibx19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.03490</span>.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[McCoy, Pavlick, and
Linzen 2019]</span>
<span class="ltx_bibblock">
McCoy, T.; Pavlick, E.; and Linzen, T.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Right for the wrong reasons: Diagnosing syntactic heuristics in
natural language inference.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span>, 3428–3448.

</span>
<span class="ltx_bibblock">Florence, Italy: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Poesio and
Rieser 2010]</span>
<span class="ltx_bibblock">
Poesio, M., and Rieser, H.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock">Completions, coordination, and alignment in dialogue.

</span>
<span class="ltx_bibblock"><span id="bib.bibx21.1.1" class="ltx_text ltx_font_italic">Dialogue &amp; Discourse</span> 1(1).

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Poesio et al<span id="bib.bibx22.1.1.1" class="ltx_text">.</span> 2019]</span>
<span class="ltx_bibblock">
Poesio, M.; Chamberlain, J.; Paun, S.; Yu, J.; Uma, A.; and Kruschwitz, U.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">A crowdsourced corpus of multiple judgments and disagreement on
anaphoric interpretation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx22.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, 1778–1789.

</span>
<span class="ltx_bibblock">Minneapolis, Minnesota: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Poesio, Stuckardt, and
Versley 2016]</span>
<span class="ltx_bibblock">
Poesio, M.; Stuckardt, R.; and Versley, Y.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock"><span id="bib.bibx23.1.1" class="ltx_text ltx_font_italic">Anaphora resolution</span>.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Potts 2012]</span>
<span class="ltx_bibblock">
Potts, C.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Goal-driven answers in the cards dialogue corpus.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx24.1.1" class="ltx_text ltx_font_italic">Proceedings of the 30th west coast conference on formal
linguistics</span>, 1–20.

</span>
<span class="ltx_bibblock">Cascadilla Proceedings Project.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Pradhan et al<span id="bib.bibx25.1.1.1" class="ltx_text">.</span> 2011]</span>
<span class="ltx_bibblock">
Pradhan, S.; Ramshaw, L.; Marcus, M.; Palmer, M.; Weischedel, R.; and Xue, N.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Conll-2011 shared task: Modeling unrestricted coreference in
ontonotes.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.3.1" class="ltx_text ltx_font_italic">Proceedings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task</span>, 1–27.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Recasens, Martí, and
Orasan 2012]</span>
<span class="ltx_bibblock">
Recasens, M.; Martí, M. A.; and Orasan, C.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Annotating near-identity from coreference disagreements.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx26.1.1" class="ltx_text ltx_font_italic">Proceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012)</span>, 165–172.

</span>
<span class="ltx_bibblock">Istanbul, Turkey: European Languages Resources Association (ELRA).

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sankar et al<span id="bib.bibx27.1.1.1" class="ltx_text">.</span> 2019]</span>
<span class="ltx_bibblock">
Sankar, C.; Subramanian, S.; Pal, C.; Chandar, S.; and Bengio, Y.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Do neural dialog systems use the conversation history effectively? an
empirical study.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx27.3.1" class="ltx_text ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span>, 32–37.

</span>
<span class="ltx_bibblock">Florence, Italy: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Shore, Androulakaki, and
Skantze 2018]</span>
<span class="ltx_bibblock">
Shore, T.; Androulakaki, T.; and Skantze, G.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">KTH tangrams: A dataset for research on alignment and conceptual
pacts in task-oriented dialogue.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx28.1.1" class="ltx_text ltx_font_italic">Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018)</span>.

</span>
<span class="ltx_bibblock">Miyazaki, Japan: European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Stenetorp et al<span id="bib.bibx29.1.1.1" class="ltx_text">.</span> 2012]</span>
<span class="ltx_bibblock">
Stenetorp, P.; Pyysalo, S.; Topić, G.; Ohta, T.; Ananiadou, S.; and Tsujii,
J.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Brat: a web-based tool for nlp-assisted text annotation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx29.3.1" class="ltx_text ltx_font_italic">Proceedings of the Demonstrations at the 13th Conference of
the European Chapter of the Association for Computational Linguistics</span>,
102–107.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sugawara et al<span id="bib.bibx30.1.1.1" class="ltx_text">.</span> 2018]</span>
<span class="ltx_bibblock">
Sugawara, S.; Inui, K.; Sekine, S.; and Aizawa, A.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">What makes reading comprehension questions easier?

</span>
<span class="ltx_bibblock">In <span id="bib.bibx30.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</span>, 4208–4219.

</span>
<span class="ltx_bibblock">Brussels, Belgium: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sugawara, Yokono, and
Aizawa 2017]</span>
<span class="ltx_bibblock">
Sugawara, S.; Yokono, H.; and Aizawa, A.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Prerequisite skills for reading comprehension: Multi-perspective
analysis of mctest datasets and systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx31.1.1" class="ltx_text ltx_font_italic">AAAI</span>, 3089–3096.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tokunaga et al<span id="bib.bibx32.1.1.1" class="ltx_text">.</span> 2012]</span>
<span class="ltx_bibblock">
Tokunaga, T.; Iida, R.; Terai, A.; and Kuriyama, N.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">The REX corpora: A collection of multimodal corpora of referring
expressions in collaborative problem solving dialogues.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx32.3.1" class="ltx_text ltx_font_italic">Proceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC’12)</span>, 422–429.

</span>
<span class="ltx_bibblock">Istanbul, Turkey: European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Traum 1994]</span>
<span class="ltx_bibblock">
Traum, D. R.

</span>
<span class="ltx_bibblock">1994.

</span>
<span class="ltx_bibblock">A computational theory of grounding in natural language conversation.

</span>
<span class="ltx_bibblock">Technical report, ROCHESTER UNIV NY DEPT OF COMPUTER SCIENCE.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Udagawa and Aizawa 2019]</span>
<span class="ltx_bibblock">
Udagawa, T., and Aizawa, A.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">A natural language corpus of common grounding under continuous and
partially-observable context.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx34.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume 33, 7120–7127.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Vinyals and Le 2015]</span>
<span class="ltx_bibblock">
Vinyals, O., and Le, Q. V.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">A neural conversational model.

</span>
<span class="ltx_bibblock"><span id="bib.bibx35.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1506.05869.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zarrieß et al<span id="bib.bibx36.1.1.1" class="ltx_text">.</span> 2016]</span>
<span class="ltx_bibblock">
Zarrieß, S.; Hough, J.; Kennington, C.; Manuvinakurike, R.; DeVault, D.;
Fernández, R.; and Schlangen, D.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">PentoRef: A corpus of spoken references in task-oriented
dialogues.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx36.3.1" class="ltx_text ltx_font_italic">Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC’16)</span>, 125–131.

</span>
<span class="ltx_bibblock">Portorož, Slovenia: European Language Resources Association
(ELRA).

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1911.07587" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1911.07588" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1911.07588">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1911.07588" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1911.07589" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 14:01:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
