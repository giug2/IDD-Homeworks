<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.08466] Synthetic Data from Diffusion Models Improves ImageNet Classification</title><meta property="og:description" content="Deep generative models are becoming increasingly powerful, now generating diverse high fidelity photo-realistic samples given text prompts. Have they reached the point where models of natural images can be used for gen‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data from Diffusion Models Improves ImageNet Classification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data from Diffusion Models Improves ImageNet Classification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.08466">

<!--Generated on Thu Feb 29 14:09:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data from Diffusion Models
<br class="ltx_break">Improves ImageNet Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shekoofeh Azizi,
Simon Kornblith,
Chitwan Saharia<sup id="id6.1.id1" class="ltx_sup">*</sup>,
Mohammad Norouzi<sup id="id7.2.id2" class="ltx_sup"></sup>,
David J.¬†Fleet
<br class="ltx_break">
Google Research, Brain Team
</span><span class="ltx_author_notes">Work done at Google Research.{shekazizi,‚Äâskornblith,‚Äâdavidfleet}@google.com</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.4" class="ltx_p">Deep generative models are becoming increasingly powerful, now generating diverse high fidelity photo-realistic samples given text prompts. Have they reached the point where models of natural images can be used for generative data augmentation, helping to improve challenging discriminative tasks?
We show that large-scale text-to-image diffusion models can be fine-tuned to produce class-conditional models with SOTA FID (1.76 at <math id="id1.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">√ó</mo><mn id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><times id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></times><cn type="integer" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">256</cn><cn type="integer" id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">256\!\times\!256</annotation></semantics></math> resolution) and Inception Score
(239 at <math id="id2.2.m2.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">√ó</mo><mn id="id2.2.m2.1.1.3" xref="id2.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><times id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"></times><cn type="integer" id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">256</cn><cn type="integer" id="id2.2.m2.1.1.3.cmml" xref="id2.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">256\times 256</annotation></semantics></math>).
The model also yields a new SOTA in Classification Accuracy Scores (64.96 for <math id="id3.3.m3.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="id3.3.m3.1a"><mrow id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mn id="id3.3.m3.1.1.2" xref="id3.3.m3.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">√ó</mo><mn id="id3.3.m3.1.1.3" xref="id3.3.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><times id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1"></times><cn type="integer" id="id3.3.m3.1.1.2.cmml" xref="id3.3.m3.1.1.2">256</cn><cn type="integer" id="id3.3.m3.1.1.3.cmml" xref="id3.3.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">256\!\times\!256</annotation></semantics></math> generative samples, improving to 69.24 for <math id="id4.4.m4.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="id4.4.m4.1a"><mrow id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mn id="id4.4.m4.1.1.2" xref="id4.4.m4.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">√ó</mo><mn id="id4.4.m4.1.1.3" xref="id4.4.m4.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><times id="id4.4.m4.1.1.1.cmml" xref="id4.4.m4.1.1.1"></times><cn type="integer" id="id4.4.m4.1.1.2.cmml" xref="id4.4.m4.1.1.2">1024</cn><cn type="integer" id="id4.4.m4.1.1.3.cmml" xref="id4.4.m4.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">1024\!\times\!1024</annotation></semantics></math> samples).
Augmenting the ImageNet training set with samples from the resulting models yields significant improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep generative models are becoming increasingly mature to the point that they can generate high fidelity photo-realistic samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
Most recently, denoising diffusion probabilistic models (DDPMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> have emerged as a new category of generative techniques that are capable of generating images comparable to generative adversarial networks (GANs) in quality while introducing greater stability during training
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
This has been shown both for class-conditional generative models on classification datasets, and for open vocabulary text-to-image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">It is therefore natural to ask whether current models are powerful enough to generate natural image data that are effective for challenging discriminative tasks; i.e., <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">generative data augmentation</span>.
Specifically, are diffusion models capable of producing image samples of sufficient quality and diversity to improve performance on well-studied benchmark tasks like ImageNet classification?
Such tasks set a high bar, since existing architectures, augmentation strategies, and training recipes have been heavily tuned.
A closely related question is, to what extent large-scale text-to-image models can serve as good representation learners or foundation models for downstream tasks? We explore this issue in the context of generative data augmentation, showing that these models can be fine-tuned to produce state-of-the-art class-conditional generative models on ImageNet.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="227" height="139" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x2.png" id="S1.F1.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="221" height="144" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Top: Classification Accuracy Scores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> show that models trained on generated data are approaching those trained on real data.
Bottom: Augmenting real training data with generated images from our ImageNet model boosts classification accuracy for ResNet and Transformer models.</span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2304.08466/assets/x3.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="470" height="352" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.5.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.1" class="ltx_text" style="font-size:90%;">Example <math id="S1.F2.3.1.m1.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S1.F2.3.1.m1.1b"><mrow id="S1.F2.3.1.m1.1.1" xref="S1.F2.3.1.m1.1.1.cmml"><mn id="S1.F2.3.1.m1.1.1.2" xref="S1.F2.3.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S1.F2.3.1.m1.1.1.1" xref="S1.F2.3.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.F2.3.1.m1.1.1.3" xref="S1.F2.3.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.3.1.m1.1c"><apply id="S1.F2.3.1.m1.1.1.cmml" xref="S1.F2.3.1.m1.1.1"><times id="S1.F2.3.1.m1.1.1.1.cmml" xref="S1.F2.3.1.m1.1.1.1"></times><cn type="integer" id="S1.F2.3.1.m1.1.1.2.cmml" xref="S1.F2.3.1.m1.1.1.2">1024</cn><cn type="integer" id="S1.F2.3.1.m1.1.1.3.cmml" xref="S1.F2.3.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.3.1.m1.1d">1024\!\times\!1024</annotation></semantics></math> images from the fine-tuned Imagen (left) model vs.¬†vanilla Imagen (right). Fine-tuning and careful choice of guidance weights and other sampling parameters help to improve the alignment of images with class labels and sample diversity.
More samples are provide in the Appendix.
</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To this end, we demonstrate three key findings. First, we show that an Imagen model fine-tuned on ImageNet training data produces state-of-the-art class-conditional ImageNet models at multiple resolutions, according to their Fr√©chet Inception Distance (FID)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and Inception Score (IS)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>; e.g,, we obtain an FID of 1.76 and IS of 239 on <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mn id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><times id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1"></times><cn type="integer" id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">256</cn><cn type="integer" id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">256\!\times\!256</annotation></semantics></math> image samples.
These models outperform existing state-of-the-art models, with or without the use of guidance to improve model sampling.
We further establish that data from such fine-tuned class-conditional models also provide new state-of-the-art Classification Accuracy Scores (CAS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, computed by training ResNet-50 models on synthetic data and then evaluating them on the real ImageNet validation set (Fig. <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> - top).
Finally, we show that performance of models trained on generative data further improves by combining synthetic data with real data, with larger amounts of synthetic data, and with longer training times. These results hold across a host of convolutional and Transformer-based architectures (Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> - bottom).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Data.</span> The use of synthetic data has been widely explored for generating large amounts of labeled data for vision tasks that require extensive annotation.
Examples include tasks like semantic image segmentation
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, optical flow estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, human motion understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, and other dense prediction tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Previous work has explored 3D-rendered datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> and simulation environments with physically realistic engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Unlike methods that use model-based rendering, here we focus on the use of data-driven generative models of natural images, for which GANs have remained the predominant approach to date <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Very recent work has also explored the use of publicly available text-to-image diffusion models to generate synthetic data. We discuss this work further below.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Distillation and Transfer.</span> In our work, we use a diffusion model that has been pretrained on a large multimodal dataset and fine-tuned on ImageNet to provide synthetic data for a classification model. This setup has connections to previous work that has directly trained classification models on large-scale datasets and then fine-tuned them on ImageNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.
It is also related to knowledge distillation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> in that we transfer knowledge from the diffusion model to the classifier, although it differs from the traditional distillation setup in that we transfer this knowledge through generated data rather than labels. Our goal in this work is to show the viability of this kind of generative knowledge transfer with modern diffusion models.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Diffusion Model Applications.</span>
Diffusion models have been successfully applied to image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, speech generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and video generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, and have found applications in various image processing areas, including image colorization, super-resolution, inpainting, and semantic editing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. One notable application of diffusion models is large-scale text-to-image generation. Several text-to-image models including Stable Diffusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, DALL-E 2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, Imagen¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, eDiff <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and GLIDE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> have produced evocative high-resolution images.
However, the use of large-scale diffusion models to support downstream tasks is still in its infancy.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Very recently, large-scale text-to-image models have been used to augment training data.
He et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> show that synthetic data generated with GLIDE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> improves zero-shot and few-shot image classification performance. They further demonstrate that a synthetic dataset generated by fine-tuning GLIDE on CIFAR-100 images can provide a substantial boost to CIFAR-100 classification accuracy. Trabucco et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> explore strategies to augment individual images using a pretrained diffusion model, demonstrating improvements in few-shot settings. Most closely related to our work, two recent papers train ImageNet classifiers on images generated by diffusion models, although they explore only the pretrained Stable Diffusion model and do not fine-tune it¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. They find that images generated in this fashion do not improve accuracy on the clean ImageNet validation set. Here, we show that the Imagen text-to-image model can be fine-tuned for class-conditional ImageNet, yielding SOTA models.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.6" class="ltx_p"><span id="S3.p1.6.1" class="ltx_text ltx_font_bold">Diffusion.</span>
Diffusion models work by gradually destroying the data through the successive addition of Gaussian noise, and then learning to recover the data by reversing this noising process¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
In broad terms, in a forward process random noise is slowly added to the data as time <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">t</annotation></semantics></math> increases from 0 to <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">T</annotation></semantics></math>.
A learned reverse process inverts the forward process, gradually refining a sample of noise into an image.
To this end, samples at the current time step, <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="x_{t-1}" display="inline"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">x</mi><mrow id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">t</mi><mo id="S3.p1.3.m3.1.1.3.1" xref="S3.p1.3.m3.1.1.3.1.cmml">‚àí</mo><mn id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ùë•</ci><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><minus id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3.1"></minus><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">ùë°</ci><cn type="integer" id="S3.p1.3.m3.1.1.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">x_{t-1}</annotation></semantics></math> are drawn from a learned Gaussian distribution <math id="S3.p1.4.m4.5" class="ltx_Math" alttext="\mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t))" display="inline"><semantics id="S3.p1.4.m4.5a"><mrow id="S3.p1.4.m4.5.5" xref="S3.p1.4.m4.5.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.5.5.5" xref="S3.p1.4.m4.5.5.5.cmml">ùí©</mi><mo lspace="0em" rspace="0em" id="S3.p1.4.m4.5.5.4" xref="S3.p1.4.m4.5.5.4.cmml">‚Äã</mo><mrow id="S3.p1.4.m4.5.5.3.3" xref="S3.p1.4.m4.5.5.3.4.cmml"><mo stretchy="false" id="S3.p1.4.m4.5.5.3.3.4" xref="S3.p1.4.m4.5.5.3.4.cmml">(</mo><msub id="S3.p1.4.m4.3.3.1.1.1" xref="S3.p1.4.m4.3.3.1.1.1.cmml"><mi id="S3.p1.4.m4.3.3.1.1.1.2" xref="S3.p1.4.m4.3.3.1.1.1.2.cmml">x</mi><mrow id="S3.p1.4.m4.3.3.1.1.1.3" xref="S3.p1.4.m4.3.3.1.1.1.3.cmml"><mi id="S3.p1.4.m4.3.3.1.1.1.3.2" xref="S3.p1.4.m4.3.3.1.1.1.3.2.cmml">t</mi><mo id="S3.p1.4.m4.3.3.1.1.1.3.1" xref="S3.p1.4.m4.3.3.1.1.1.3.1.cmml">‚àí</mo><mn id="S3.p1.4.m4.3.3.1.1.1.3.3" xref="S3.p1.4.m4.3.3.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.p1.4.m4.5.5.3.3.5" xref="S3.p1.4.m4.5.5.3.4.cmml">;</mo><mrow id="S3.p1.4.m4.4.4.2.2.2" xref="S3.p1.4.m4.4.4.2.2.2.cmml"><msub id="S3.p1.4.m4.4.4.2.2.2.3" xref="S3.p1.4.m4.4.4.2.2.2.3.cmml"><mi id="S3.p1.4.m4.4.4.2.2.2.3.2" xref="S3.p1.4.m4.4.4.2.2.2.3.2.cmml">Œº</mi><mi id="S3.p1.4.m4.4.4.2.2.2.3.3" xref="S3.p1.4.m4.4.4.2.2.2.3.3.cmml">Œ∏</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.4.m4.4.4.2.2.2.2" xref="S3.p1.4.m4.4.4.2.2.2.2.cmml">‚Äã</mo><mrow id="S3.p1.4.m4.4.4.2.2.2.1.1" xref="S3.p1.4.m4.4.4.2.2.2.1.2.cmml"><mo stretchy="false" id="S3.p1.4.m4.4.4.2.2.2.1.1.2" xref="S3.p1.4.m4.4.4.2.2.2.1.2.cmml">(</mo><msub id="S3.p1.4.m4.4.4.2.2.2.1.1.1" xref="S3.p1.4.m4.4.4.2.2.2.1.1.1.cmml"><mi id="S3.p1.4.m4.4.4.2.2.2.1.1.1.2" xref="S3.p1.4.m4.4.4.2.2.2.1.1.1.2.cmml">x</mi><mi id="S3.p1.4.m4.4.4.2.2.2.1.1.1.3" xref="S3.p1.4.m4.4.4.2.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S3.p1.4.m4.4.4.2.2.2.1.1.3" xref="S3.p1.4.m4.4.4.2.2.2.1.2.cmml">,</mo><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">t</mi><mo stretchy="false" id="S3.p1.4.m4.4.4.2.2.2.1.1.4" xref="S3.p1.4.m4.4.4.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S3.p1.4.m4.5.5.3.3.6" xref="S3.p1.4.m4.5.5.3.4.cmml">,</mo><mrow id="S3.p1.4.m4.5.5.3.3.3" xref="S3.p1.4.m4.5.5.3.3.3.cmml"><msub id="S3.p1.4.m4.5.5.3.3.3.3" xref="S3.p1.4.m4.5.5.3.3.3.3.cmml"><mi mathvariant="normal" id="S3.p1.4.m4.5.5.3.3.3.3.2" xref="S3.p1.4.m4.5.5.3.3.3.3.2.cmml">Œ£</mi><mi id="S3.p1.4.m4.5.5.3.3.3.3.3" xref="S3.p1.4.m4.5.5.3.3.3.3.3.cmml">Œ∏</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.4.m4.5.5.3.3.3.2" xref="S3.p1.4.m4.5.5.3.3.3.2.cmml">‚Äã</mo><mrow id="S3.p1.4.m4.5.5.3.3.3.1.1" xref="S3.p1.4.m4.5.5.3.3.3.1.2.cmml"><mo stretchy="false" id="S3.p1.4.m4.5.5.3.3.3.1.1.2" xref="S3.p1.4.m4.5.5.3.3.3.1.2.cmml">(</mo><msub id="S3.p1.4.m4.5.5.3.3.3.1.1.1" xref="S3.p1.4.m4.5.5.3.3.3.1.1.1.cmml"><mi id="S3.p1.4.m4.5.5.3.3.3.1.1.1.2" xref="S3.p1.4.m4.5.5.3.3.3.1.1.1.2.cmml">x</mi><mi id="S3.p1.4.m4.5.5.3.3.3.1.1.1.3" xref="S3.p1.4.m4.5.5.3.3.3.1.1.1.3.cmml">t</mi></msub><mo id="S3.p1.4.m4.5.5.3.3.3.1.1.3" xref="S3.p1.4.m4.5.5.3.3.3.1.2.cmml">,</mo><mi id="S3.p1.4.m4.2.2" xref="S3.p1.4.m4.2.2.cmml">t</mi><mo stretchy="false" id="S3.p1.4.m4.5.5.3.3.3.1.1.4" xref="S3.p1.4.m4.5.5.3.3.3.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.p1.4.m4.5.5.3.3.7" xref="S3.p1.4.m4.5.5.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.5b"><apply id="S3.p1.4.m4.5.5.cmml" xref="S3.p1.4.m4.5.5"><times id="S3.p1.4.m4.5.5.4.cmml" xref="S3.p1.4.m4.5.5.4"></times><ci id="S3.p1.4.m4.5.5.5.cmml" xref="S3.p1.4.m4.5.5.5">ùí©</ci><list id="S3.p1.4.m4.5.5.3.4.cmml" xref="S3.p1.4.m4.5.5.3.3"><apply id="S3.p1.4.m4.3.3.1.1.1.cmml" xref="S3.p1.4.m4.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.3.3.1.1.1.1.cmml" xref="S3.p1.4.m4.3.3.1.1.1">subscript</csymbol><ci id="S3.p1.4.m4.3.3.1.1.1.2.cmml" xref="S3.p1.4.m4.3.3.1.1.1.2">ùë•</ci><apply id="S3.p1.4.m4.3.3.1.1.1.3.cmml" xref="S3.p1.4.m4.3.3.1.1.1.3"><minus id="S3.p1.4.m4.3.3.1.1.1.3.1.cmml" xref="S3.p1.4.m4.3.3.1.1.1.3.1"></minus><ci id="S3.p1.4.m4.3.3.1.1.1.3.2.cmml" xref="S3.p1.4.m4.3.3.1.1.1.3.2">ùë°</ci><cn type="integer" id="S3.p1.4.m4.3.3.1.1.1.3.3.cmml" xref="S3.p1.4.m4.3.3.1.1.1.3.3">1</cn></apply></apply><apply id="S3.p1.4.m4.4.4.2.2.2.cmml" xref="S3.p1.4.m4.4.4.2.2.2"><times id="S3.p1.4.m4.4.4.2.2.2.2.cmml" xref="S3.p1.4.m4.4.4.2.2.2.2"></times><apply id="S3.p1.4.m4.4.4.2.2.2.3.cmml" xref="S3.p1.4.m4.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.4.4.2.2.2.3.1.cmml" xref="S3.p1.4.m4.4.4.2.2.2.3">subscript</csymbol><ci id="S3.p1.4.m4.4.4.2.2.2.3.2.cmml" xref="S3.p1.4.m4.4.4.2.2.2.3.2">ùúá</ci><ci id="S3.p1.4.m4.4.4.2.2.2.3.3.cmml" xref="S3.p1.4.m4.4.4.2.2.2.3.3">ùúÉ</ci></apply><interval closure="open" id="S3.p1.4.m4.4.4.2.2.2.1.2.cmml" xref="S3.p1.4.m4.4.4.2.2.2.1.1"><apply id="S3.p1.4.m4.4.4.2.2.2.1.1.1.cmml" xref="S3.p1.4.m4.4.4.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.4.4.2.2.2.1.1.1.1.cmml" xref="S3.p1.4.m4.4.4.2.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.4.m4.4.4.2.2.2.1.1.1.2.cmml" xref="S3.p1.4.m4.4.4.2.2.2.1.1.1.2">ùë•</ci><ci id="S3.p1.4.m4.4.4.2.2.2.1.1.1.3.cmml" xref="S3.p1.4.m4.4.4.2.2.2.1.1.1.3">ùë°</ci></apply><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ùë°</ci></interval></apply><apply id="S3.p1.4.m4.5.5.3.3.3.cmml" xref="S3.p1.4.m4.5.5.3.3.3"><times id="S3.p1.4.m4.5.5.3.3.3.2.cmml" xref="S3.p1.4.m4.5.5.3.3.3.2"></times><apply id="S3.p1.4.m4.5.5.3.3.3.3.cmml" xref="S3.p1.4.m4.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.5.5.3.3.3.3.1.cmml" xref="S3.p1.4.m4.5.5.3.3.3.3">subscript</csymbol><ci id="S3.p1.4.m4.5.5.3.3.3.3.2.cmml" xref="S3.p1.4.m4.5.5.3.3.3.3.2">Œ£</ci><ci id="S3.p1.4.m4.5.5.3.3.3.3.3.cmml" xref="S3.p1.4.m4.5.5.3.3.3.3.3">ùúÉ</ci></apply><interval closure="open" id="S3.p1.4.m4.5.5.3.3.3.1.2.cmml" xref="S3.p1.4.m4.5.5.3.3.3.1.1"><apply id="S3.p1.4.m4.5.5.3.3.3.1.1.1.cmml" xref="S3.p1.4.m4.5.5.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.5.5.3.3.3.1.1.1.1.cmml" xref="S3.p1.4.m4.5.5.3.3.3.1.1.1">subscript</csymbol><ci id="S3.p1.4.m4.5.5.3.3.3.1.1.1.2.cmml" xref="S3.p1.4.m4.5.5.3.3.3.1.1.1.2">ùë•</ci><ci id="S3.p1.4.m4.5.5.3.3.3.1.1.1.3.cmml" xref="S3.p1.4.m4.5.5.3.3.3.1.1.1.3">ùë°</ci></apply><ci id="S3.p1.4.m4.2.2.cmml" xref="S3.p1.4.m4.2.2">ùë°</ci></interval></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.5c">\mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t))</annotation></semantics></math> where the mean of the distribution <math id="S3.p1.5.m5.2" class="ltx_Math" alttext="\mu_{\theta}(x_{t},t)" display="inline"><semantics id="S3.p1.5.m5.2a"><mrow id="S3.p1.5.m5.2.2" xref="S3.p1.5.m5.2.2.cmml"><msub id="S3.p1.5.m5.2.2.3" xref="S3.p1.5.m5.2.2.3.cmml"><mi id="S3.p1.5.m5.2.2.3.2" xref="S3.p1.5.m5.2.2.3.2.cmml">Œº</mi><mi id="S3.p1.5.m5.2.2.3.3" xref="S3.p1.5.m5.2.2.3.3.cmml">Œ∏</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.5.m5.2.2.2" xref="S3.p1.5.m5.2.2.2.cmml">‚Äã</mo><mrow id="S3.p1.5.m5.2.2.1.1" xref="S3.p1.5.m5.2.2.1.2.cmml"><mo stretchy="false" id="S3.p1.5.m5.2.2.1.1.2" xref="S3.p1.5.m5.2.2.1.2.cmml">(</mo><msub id="S3.p1.5.m5.2.2.1.1.1" xref="S3.p1.5.m5.2.2.1.1.1.cmml"><mi id="S3.p1.5.m5.2.2.1.1.1.2" xref="S3.p1.5.m5.2.2.1.1.1.2.cmml">x</mi><mi id="S3.p1.5.m5.2.2.1.1.1.3" xref="S3.p1.5.m5.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S3.p1.5.m5.2.2.1.1.3" xref="S3.p1.5.m5.2.2.1.2.cmml">,</mo><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">t</mi><mo stretchy="false" id="S3.p1.5.m5.2.2.1.1.4" xref="S3.p1.5.m5.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.2b"><apply id="S3.p1.5.m5.2.2.cmml" xref="S3.p1.5.m5.2.2"><times id="S3.p1.5.m5.2.2.2.cmml" xref="S3.p1.5.m5.2.2.2"></times><apply id="S3.p1.5.m5.2.2.3.cmml" xref="S3.p1.5.m5.2.2.3"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.2.3.1.cmml" xref="S3.p1.5.m5.2.2.3">subscript</csymbol><ci id="S3.p1.5.m5.2.2.3.2.cmml" xref="S3.p1.5.m5.2.2.3.2">ùúá</ci><ci id="S3.p1.5.m5.2.2.3.3.cmml" xref="S3.p1.5.m5.2.2.3.3">ùúÉ</ci></apply><interval closure="open" id="S3.p1.5.m5.2.2.1.2.cmml" xref="S3.p1.5.m5.2.2.1.1"><apply id="S3.p1.5.m5.2.2.1.1.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.p1.5.m5.2.2.1.1.1.2">ùë•</ci><ci id="S3.p1.5.m5.2.2.1.1.1.3.cmml" xref="S3.p1.5.m5.2.2.1.1.1.3">ùë°</ci></apply><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ùë°</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.2c">\mu_{\theta}(x_{t},t)</annotation></semantics></math>, is conditioned on the sample at the previous time step. The variance of the distribution <math id="S3.p1.6.m6.2" class="ltx_Math" alttext="\Sigma_{\theta}(x_{t},t)" display="inline"><semantics id="S3.p1.6.m6.2a"><mrow id="S3.p1.6.m6.2.2" xref="S3.p1.6.m6.2.2.cmml"><msub id="S3.p1.6.m6.2.2.3" xref="S3.p1.6.m6.2.2.3.cmml"><mi mathvariant="normal" id="S3.p1.6.m6.2.2.3.2" xref="S3.p1.6.m6.2.2.3.2.cmml">Œ£</mi><mi id="S3.p1.6.m6.2.2.3.3" xref="S3.p1.6.m6.2.2.3.3.cmml">Œ∏</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.6.m6.2.2.2" xref="S3.p1.6.m6.2.2.2.cmml">‚Äã</mo><mrow id="S3.p1.6.m6.2.2.1.1" xref="S3.p1.6.m6.2.2.1.2.cmml"><mo stretchy="false" id="S3.p1.6.m6.2.2.1.1.2" xref="S3.p1.6.m6.2.2.1.2.cmml">(</mo><msub id="S3.p1.6.m6.2.2.1.1.1" xref="S3.p1.6.m6.2.2.1.1.1.cmml"><mi id="S3.p1.6.m6.2.2.1.1.1.2" xref="S3.p1.6.m6.2.2.1.1.1.2.cmml">x</mi><mi id="S3.p1.6.m6.2.2.1.1.1.3" xref="S3.p1.6.m6.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S3.p1.6.m6.2.2.1.1.3" xref="S3.p1.6.m6.2.2.1.2.cmml">,</mo><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">t</mi><mo stretchy="false" id="S3.p1.6.m6.2.2.1.1.4" xref="S3.p1.6.m6.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.2b"><apply id="S3.p1.6.m6.2.2.cmml" xref="S3.p1.6.m6.2.2"><times id="S3.p1.6.m6.2.2.2.cmml" xref="S3.p1.6.m6.2.2.2"></times><apply id="S3.p1.6.m6.2.2.3.cmml" xref="S3.p1.6.m6.2.2.3"><csymbol cd="ambiguous" id="S3.p1.6.m6.2.2.3.1.cmml" xref="S3.p1.6.m6.2.2.3">subscript</csymbol><ci id="S3.p1.6.m6.2.2.3.2.cmml" xref="S3.p1.6.m6.2.2.3.2">Œ£</ci><ci id="S3.p1.6.m6.2.2.3.3.cmml" xref="S3.p1.6.m6.2.2.3.3">ùúÉ</ci></apply><interval closure="open" id="S3.p1.6.m6.2.2.1.2.cmml" xref="S3.p1.6.m6.2.2.1.1"><apply id="S3.p1.6.m6.2.2.1.1.1.cmml" xref="S3.p1.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.2.2.1.1.1.1.cmml" xref="S3.p1.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.6.m6.2.2.1.1.1.2.cmml" xref="S3.p1.6.m6.2.2.1.1.1.2">ùë•</ci><ci id="S3.p1.6.m6.2.2.1.1.1.3.cmml" xref="S3.p1.6.m6.2.2.1.1.1.3">ùë°</ci></apply><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ùë°</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.2c">\Sigma_{\theta}(x_{t},t)</annotation></semantics></math> follows a fixed schedule. In conditional diffusion models, the reverse process is associated with a conditioning signal, such as a class label in class-conditional models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Diffusion models have been the subject of many recent papers including important innovations in architectures and training (e.g.,¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>).
Important below, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> propose cascades of diffusion models at increasing image resolutions for high resolution images.
Other work has explored the importance of the generative sampling process, introducing new noise schedules, guidance mechanisms to trade-off diversity with image quality, distillation for efficiency, and different parameterizations of the denoising objective (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>).</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Classification Accuracy Score.</span> It is a standard practice to use FID¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and Inception Score¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> to evaluate the visual quality of generative models.
Due to their relatively low computation cost, these metrics are widely used as proxies for generative model training and tuning.
However, both methods tend to penalize non-GAN models harshly, and Inception Score produces overly optimistic scores in methods with sampling modifications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
More importantly, Ravuri and Vinyals¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> argued that these metrics do not show a consistent correlation with metrics that assess performance on downstream tasks like classification accuracy.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">An alternative way to evaluate the quality of samples from generative models is to examine the performance of a classifier that is trained on generated data and evaluated on real data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. To this end, Ravuri and Vinyals¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> propose classification accuracy score (CAS), which measures classification performance on the ImageNet validation set for ResNet-50 models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> trained on generated data. It is an intriguing proxy, as it requires generative models to produce high fidelity images across a broad range of categories, competing directly with models trained on real data.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">To date, CAS performance has not been particularly compelling.
Models trained exclusively on generated samples underperform those trained on real data. Moreover, performance drops when even relatively small amounts of synthetic data are added to real data during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
This performance drop may arise from issues with the quality of generated sample, the diversity (e.g., due to mode collapse in GAN models), or both.
Cascaded diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> have recently been shown to outperform BigGAN-deep <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
VQ-VAE-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> on CAS (and other metrics).
That said, there remains a sizeable gap in ImageNet test performance between models trained on real data and those trained on synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Here, we explore the use of diffusion models in greater depth, with much stronger results, demonstrating the advantage of large-scale models and fine-tuning.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2304.08466/assets/x4.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="174" height="122" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2304.08466/assets/x5.png" id="S3.F3.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="174" height="128" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2304.08466/assets/x6.png" id="S3.F3.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="174" height="123" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.10.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.5.1" class="ltx_text" style="font-size:90%;">Sampling refinement for <math id="S3.F3.5.1.m1.1" class="ltx_Math" alttext="64\!\times\!64" display="inline"><semantics id="S3.F3.5.1.m1.1b"><mrow id="S3.F3.5.1.m1.1.1" xref="S3.F3.5.1.m1.1.1.cmml"><mn id="S3.F3.5.1.m1.1.1.2" xref="S3.F3.5.1.m1.1.1.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S3.F3.5.1.m1.1.1.1" xref="S3.F3.5.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.F3.5.1.m1.1.1.3" xref="S3.F3.5.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.5.1.m1.1c"><apply id="S3.F3.5.1.m1.1.1.cmml" xref="S3.F3.5.1.m1.1.1"><times id="S3.F3.5.1.m1.1.1.1.cmml" xref="S3.F3.5.1.m1.1.1.1"></times><cn type="integer" id="S3.F3.5.1.m1.1.1.2.cmml" xref="S3.F3.5.1.m1.1.1.2">64</cn><cn type="integer" id="S3.F3.5.1.m1.1.1.3.cmml" xref="S3.F3.5.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.1.m1.1d">64\!\times\!64</annotation></semantics></math> base model. <span id="S3.F3.5.1.1" class="ltx_text ltx_font_bold">Left</span>: Validation set FID vs.¬†guidance weights for different values of log-variance. <span id="S3.F3.5.1.2" class="ltx_text ltx_font_bold">Center</span>: Pareto frontiers for training set FID and IS at different values of the guidance weight. <span id="S3.F3.5.1.3" class="ltx_text ltx_font_bold">Right</span>: Dependence of CAS on guidance weight.</span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Generative Model Training and Sampling</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In what follows we address two main questions: whether large-scale text-to-image models can be fine-tuned as class-conditional ImageNet models, and to what extent such models are useful for generative data augmentation.
For this purpose, we undertake a series of experiments to construct and evaluate such models, focused primarily on data sampling for use in training ImageNet classifiers.
ImageNet classification accuracy is a high bar as a domain for generative data augmentation as the task is widely studied, and existing architectures and training recipes are very well-honed.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.4" class="ltx_p">The ImageNet ILSVRC 2012 dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> (ImageNet-1K) comprises 1.28 million labeled training images and 50K validation images spanning 1000 categories. We adopt ImageNet-1K as our benchmark to assess the efficacy of the generated data, as this is one of the most widely and thoroughly studied benchmarks for which there is an extensive literature on architectures and training procedures, making it challenging to improve performance.
Since the images of ImageNet-1K dataset vary in dimensions and resolution with the average image resolution of 469<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p2.1.m1.1a"><mo id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><times id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\times</annotation></semantics></math>387¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, we examine synthetic data generation at different resolutions, including 64<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p2.2.m2.1a"><mo id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><times id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\times</annotation></semantics></math>64, 256<math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p2.3.m3.1a"><mo id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><times id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\times</annotation></semantics></math>256, and 1024<math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p2.4.m4.1a"><mo id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><times id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\times</annotation></semantics></math>1024.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">In contrast to previous work that trains diffusion models directly on ImageNet data (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>), here we leverage a large-scale text-to-image diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> as a foundation, in part to explore the potential benefits of pre-training on a larger, generic corpus.
A key challenge in doing so concerns the alignment of the text-to-image model with ImageNet classes.
If, for example, one naively uses short text descriptors like those produced for CLIP by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> as text prompts for each ImageNet class,
the data generated by the Imagen models is easily shown to produce very poor ImageNet classifier. One problem is that a given text label may be associated with multiple visual concepts in the wild, or visual concepts that differ systematically from ImageNet (see Figure <a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). This poor performance may also be a consequence of the high guidance weights used by Imagen, effectively sacrificing generative diversity for sample quality.
While there are several ways in which one might re-purpose a text-to-image model
as a class-conditional model, e.g., optimizing prompts in order to minimize
the distribution shift, here we fix the prompts
to be the one or two words class names from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, and
fine-tune the weights and sampling parameters of the diffusion-based generative model.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Imagen Fine-tuning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.6" class="ltx_p">We leverage the large-scale Imagen text-to-image model described in detail in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> as the backbone text-to-image generator that we fine-tune using the ImageNet training set.
It includes a pretrained text encoder that maps text to contextualized embeddings, and a cascade of conditional diffusion models that map these embeddings to images of increasing resolution.
Imagen uses a frozen T5-XXL encoder as a semantic text encoder to capture the complexity and compositionality of text inputs.
The cascade begins with a 2B parameter 64<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation></semantics></math>64 text-to-image base model.
Its outputs are then fed to a 600M parameter super-resolution model to upsample from 64<math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mo id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><times id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\times</annotation></semantics></math>64 to 256<math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mo id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><times id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\times</annotation></semantics></math>256, followed by a 400M parameter model to upsample from 256<math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mo id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><times id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\times</annotation></semantics></math>256 to 1024<math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mo id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><times id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\times</annotation></semantics></math>1024.
The base 64<math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mo id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><times id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">\times</annotation></semantics></math>64 model is conditioned on text embeddings via a pooled embedding vector added to the diffusion time-step embedding, like previous class-conditional diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. All three stages of the diffusion cascade include text cross-attention layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.10" class="ltx_p">Given the relative paucity of high resolution images in ImageNet, we fine-tune only the 64<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><times id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\times</annotation></semantics></math>64 base model and 64<math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mo id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><times id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\times</annotation></semantics></math>64<math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mo stretchy="false" id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\rightarrow</annotation></semantics></math>256<math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mo id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><times id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\times</annotation></semantics></math>256 super-resolution model
on the ImageNet-1K train split, keeping the final super-resolution module and text-encoder unchanged.
The 64<math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mo id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><times id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\times</annotation></semantics></math>64 base model is fine-tuned for 210K steps and the 64<math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mo id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><times id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\times</annotation></semantics></math>64<math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><mo stretchy="false" id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><ci id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">\rightarrow</annotation></semantics></math>256<math id="S4.SS1.p2.8.m8.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.8.m8.1a"><mo id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><times id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">\times</annotation></semantics></math>256 super-resolution model is fine-tuned for 490K steps, on 256 TPU-v4 chips with a batch size of 2048.
As suggested in the original Imagen training process, Adafactor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> is used to fine-tune the base 64<math id="S4.SS1.p2.9.m9.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.9.m9.1a"><mo id="S4.SS1.p2.9.m9.1.1" xref="S4.SS1.p2.9.m9.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.9.m9.1b"><times id="S4.SS1.p2.9.m9.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.9.m9.1c">\times</annotation></semantics></math>64 model because it had a smaller memory footprint compared to Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
For the 256<math id="S4.SS1.p2.10.m10.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.10.m10.1a"><mo id="S4.SS1.p2.10.m10.1.1" xref="S4.SS1.p2.10.m10.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.10.m10.1b"><times id="S4.SS1.p2.10.m10.1.1.cmml" xref="S4.SS1.p2.10.m10.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.10.m10.1c">\times</annotation></semantics></math>256 super-resolution model, we used Adam for better sample quality.
Throughout fine-tuning experiments, we select models based on FID score calculated over 10K samples from the default Imagen sampler and the ImageNet-1K validation set.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Sampling Parameters</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The quality, diversity, and speed of text-conditioned diffusion model sampling are strongly affected by multiple factors including the number of diffusion steps, noise condition augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>,
guidance weights for classifier-free guidance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>,
and the log-variance mixing coefficient used for prediction (Eq.¬†15 in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>), described in further detail in Appendix¬†<a href="#S1a" title="A.1 Hyper-parameters for Imagen fine-tuning and sample generation. ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.
We conduct a thorough analysis of the dependence of FID, IS and classification accuracy scores (CAS) in order to select good sampling parameters for the downstream classification task.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2304.08466/assets/x7.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="174" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.5.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.1" class="ltx_text" style="font-size:90%;">Training set FID vs.¬†CAS Pareto curves under varying noise conditions when the guidance weight is set to 1.0 for resolution <math id="S4.F4.3.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S4.F4.3.1.m1.1b"><mrow id="S4.F4.3.1.m1.1.1" xref="S4.F4.3.1.m1.1.1.cmml"><mn id="S4.F4.3.1.m1.1.1.2" xref="S4.F4.3.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S4.F4.3.1.m1.1.1.1" xref="S4.F4.3.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.F4.3.1.m1.1.1.3" xref="S4.F4.3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.3.1.m1.1c"><apply id="S4.F4.3.1.m1.1.1.cmml" xref="S4.F4.3.1.m1.1.1"><times id="S4.F4.3.1.m1.1.1.1.cmml" xref="S4.F4.3.1.m1.1.1.1"></times><cn type="integer" id="S4.F4.3.1.m1.1.1.2.cmml" xref="S4.F4.3.1.m1.1.1.2">256</cn><cn type="integer" id="S4.F4.3.1.m1.1.1.3.cmml" xref="S4.F4.3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.3.1.m1.1d">256\!\times\!256</annotation></semantics></math>. These curves depict the joint influence of the log-variance mixing coefficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and noise conditioning augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> on FID and CAS.</span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">The sampling parameters for the <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">64</cn><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">64\times 64</annotation></semantics></math> based model establish the overall quality and diversity of image samples.
We first sweep over guidance weight, log-variance, and number of sampling steps, to identify good hyperparameters based on FID-50K (vs.¬†the ImageNet validation set).
Using the DDPM sampler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for the base model, we sweep over guidance values of <math id="S4.SS2.p2.2.m2.6" class="ltx_Math" alttext="\left[1.0,1.25,1.5,1.75,2.0,5.0\right]" display="inline"><semantics id="S4.SS2.p2.2.m2.6a"><mrow id="S4.SS2.p2.2.m2.6.7.2" xref="S4.SS2.p2.2.m2.6.7.1.cmml"><mo id="S4.SS2.p2.2.m2.6.7.2.1" xref="S4.SS2.p2.2.m2.6.7.1.cmml">[</mo><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">1.0</mn><mo id="S4.SS2.p2.2.m2.6.7.2.2" xref="S4.SS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S4.SS2.p2.2.m2.2.2" xref="S4.SS2.p2.2.m2.2.2.cmml">1.25</mn><mo id="S4.SS2.p2.2.m2.6.7.2.3" xref="S4.SS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S4.SS2.p2.2.m2.3.3" xref="S4.SS2.p2.2.m2.3.3.cmml">1.5</mn><mo id="S4.SS2.p2.2.m2.6.7.2.4" xref="S4.SS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S4.SS2.p2.2.m2.4.4" xref="S4.SS2.p2.2.m2.4.4.cmml">1.75</mn><mo id="S4.SS2.p2.2.m2.6.7.2.5" xref="S4.SS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S4.SS2.p2.2.m2.5.5" xref="S4.SS2.p2.2.m2.5.5.cmml">2.0</mn><mo id="S4.SS2.p2.2.m2.6.7.2.6" xref="S4.SS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S4.SS2.p2.2.m2.6.6" xref="S4.SS2.p2.2.m2.6.6.cmml">5.0</mn><mo id="S4.SS2.p2.2.m2.6.7.2.7" xref="S4.SS2.p2.2.m2.6.7.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.6b"><list id="S4.SS2.p2.2.m2.6.7.1.cmml" xref="S4.SS2.p2.2.m2.6.7.2"><cn type="float" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">1.0</cn><cn type="float" id="S4.SS2.p2.2.m2.2.2.cmml" xref="S4.SS2.p2.2.m2.2.2">1.25</cn><cn type="float" id="S4.SS2.p2.2.m2.3.3.cmml" xref="S4.SS2.p2.2.m2.3.3">1.5</cn><cn type="float" id="S4.SS2.p2.2.m2.4.4.cmml" xref="S4.SS2.p2.2.m2.4.4">1.75</cn><cn type="float" id="S4.SS2.p2.2.m2.5.5.cmml" xref="S4.SS2.p2.2.m2.5.5">2.0</cn><cn type="float" id="S4.SS2.p2.2.m2.6.6.cmml" xref="S4.SS2.p2.2.m2.6.6">5.0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.6c">\left[1.0,1.25,1.5,1.75,2.0,5.0\right]</annotation></semantics></math> and log-variance of <math id="S4.SS2.p2.3.m3.5" class="ltx_Math" alttext="\left[0.0,0.2,0.3,0.4,1.0\right]" display="inline"><semantics id="S4.SS2.p2.3.m3.5a"><mrow id="S4.SS2.p2.3.m3.5.6.2" xref="S4.SS2.p2.3.m3.5.6.1.cmml"><mo id="S4.SS2.p2.3.m3.5.6.2.1" xref="S4.SS2.p2.3.m3.5.6.1.cmml">[</mo><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">0.0</mn><mo id="S4.SS2.p2.3.m3.5.6.2.2" xref="S4.SS2.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p2.3.m3.2.2" xref="S4.SS2.p2.3.m3.2.2.cmml">0.2</mn><mo id="S4.SS2.p2.3.m3.5.6.2.3" xref="S4.SS2.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p2.3.m3.3.3" xref="S4.SS2.p2.3.m3.3.3.cmml">0.3</mn><mo id="S4.SS2.p2.3.m3.5.6.2.4" xref="S4.SS2.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p2.3.m3.4.4" xref="S4.SS2.p2.3.m3.4.4.cmml">0.4</mn><mo id="S4.SS2.p2.3.m3.5.6.2.5" xref="S4.SS2.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p2.3.m3.5.5" xref="S4.SS2.p2.3.m3.5.5.cmml">1.0</mn><mo id="S4.SS2.p2.3.m3.5.6.2.6" xref="S4.SS2.p2.3.m3.5.6.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.5b"><list id="S4.SS2.p2.3.m3.5.6.1.cmml" xref="S4.SS2.p2.3.m3.5.6.2"><cn type="float" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">0.0</cn><cn type="float" id="S4.SS2.p2.3.m3.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2">0.2</cn><cn type="float" id="S4.SS2.p2.3.m3.3.3.cmml" xref="S4.SS2.p2.3.m3.3.3">0.3</cn><cn type="float" id="S4.SS2.p2.3.m3.4.4.cmml" xref="S4.SS2.p2.3.m3.4.4">0.4</cn><cn type="float" id="S4.SS2.p2.3.m3.5.5.cmml" xref="S4.SS2.p2.3.m3.5.5">1.0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.5c">\left[0.0,0.2,0.3,0.4,1.0\right]</annotation></semantics></math>, and denoise for 128, 500, or 1000 steps.
The results of this sweep, summarized in Figure¬†<a href="#S3.F3" title="Figure 3 ‚Ä£ 3 Background ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, suggest that optimal FID is obtained with a log-variance of 0 and 1000 denoising steps.
Given these parameter choices we then complete a more compute intensive sweep, sampling 1.2M images from the fine-tuned base model for different values of the guidance weights. We measure FID, IS and CAS for these samples on the validation set in order
to select the guidance weight for the model.
Figure¬†<a href="#S3.F3" title="Figure 3 ‚Ä£ 3 Background ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the Pareto frontiers for FID vs.¬†IS across different guidance weights, as well as
the dependence of CAS on guidance weight, suggesting that optimal FID and CAS are obtained at a guidance weight of 1.25.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.5" class="ltx_p">Given <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">64</cn><cn type="integer" id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">64\times 64</annotation></semantics></math> samples obtained with the optimal hyperparameters, we then analyze the impact of guidance weight, noise augmentation, and log-variance to select sampling parameters for the super-resolution models.
The noise augmentation value specifies the level of noise augmentation applied to the input to super-resolution stages in the Imagen cascade to regulate sample diversity (and improve robustness during model training).
Here, we sweep over guidance values of <math id="S4.SS2.p3.2.m2.5" class="ltx_Math" alttext="\left[1.0,2.0,5.0,10.0,30.0\right]" display="inline"><semantics id="S4.SS2.p3.2.m2.5a"><mrow id="S4.SS2.p3.2.m2.5.6.2" xref="S4.SS2.p3.2.m2.5.6.1.cmml"><mo id="S4.SS2.p3.2.m2.5.6.2.1" xref="S4.SS2.p3.2.m2.5.6.1.cmml">[</mo><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">1.0</mn><mo id="S4.SS2.p3.2.m2.5.6.2.2" xref="S4.SS2.p3.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.2.m2.2.2" xref="S4.SS2.p3.2.m2.2.2.cmml">2.0</mn><mo id="S4.SS2.p3.2.m2.5.6.2.3" xref="S4.SS2.p3.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.2.m2.3.3" xref="S4.SS2.p3.2.m2.3.3.cmml">5.0</mn><mo id="S4.SS2.p3.2.m2.5.6.2.4" xref="S4.SS2.p3.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.2.m2.4.4" xref="S4.SS2.p3.2.m2.4.4.cmml">10.0</mn><mo id="S4.SS2.p3.2.m2.5.6.2.5" xref="S4.SS2.p3.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.2.m2.5.5" xref="S4.SS2.p3.2.m2.5.5.cmml">30.0</mn><mo id="S4.SS2.p3.2.m2.5.6.2.6" xref="S4.SS2.p3.2.m2.5.6.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.5b"><list id="S4.SS2.p3.2.m2.5.6.1.cmml" xref="S4.SS2.p3.2.m2.5.6.2"><cn type="float" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">1.0</cn><cn type="float" id="S4.SS2.p3.2.m2.2.2.cmml" xref="S4.SS2.p3.2.m2.2.2">2.0</cn><cn type="float" id="S4.SS2.p3.2.m2.3.3.cmml" xref="S4.SS2.p3.2.m2.3.3">5.0</cn><cn type="float" id="S4.SS2.p3.2.m2.4.4.cmml" xref="S4.SS2.p3.2.m2.4.4">10.0</cn><cn type="float" id="S4.SS2.p3.2.m2.5.5.cmml" xref="S4.SS2.p3.2.m2.5.5">30.0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.5c">\left[1.0,2.0,5.0,10.0,30.0\right]</annotation></semantics></math>, noise conditioning augmentation values of <math id="S4.SS2.p3.3.m3.5" class="ltx_Math" alttext="\left[0.0,0.1,0.2,0.3,0.4\right]" display="inline"><semantics id="S4.SS2.p3.3.m3.5a"><mrow id="S4.SS2.p3.3.m3.5.6.2" xref="S4.SS2.p3.3.m3.5.6.1.cmml"><mo id="S4.SS2.p3.3.m3.5.6.2.1" xref="S4.SS2.p3.3.m3.5.6.1.cmml">[</mo><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">0.0</mn><mo id="S4.SS2.p3.3.m3.5.6.2.2" xref="S4.SS2.p3.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.3.m3.2.2" xref="S4.SS2.p3.3.m3.2.2.cmml">0.1</mn><mo id="S4.SS2.p3.3.m3.5.6.2.3" xref="S4.SS2.p3.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.3.m3.3.3" xref="S4.SS2.p3.3.m3.3.3.cmml">0.2</mn><mo id="S4.SS2.p3.3.m3.5.6.2.4" xref="S4.SS2.p3.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.3.m3.4.4" xref="S4.SS2.p3.3.m3.4.4.cmml">0.3</mn><mo id="S4.SS2.p3.3.m3.5.6.2.5" xref="S4.SS2.p3.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS2.p3.3.m3.5.5" xref="S4.SS2.p3.3.m3.5.5.cmml">0.4</mn><mo id="S4.SS2.p3.3.m3.5.6.2.6" xref="S4.SS2.p3.3.m3.5.6.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.5b"><list id="S4.SS2.p3.3.m3.5.6.1.cmml" xref="S4.SS2.p3.3.m3.5.6.2"><cn type="float" id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">0.0</cn><cn type="float" id="S4.SS2.p3.3.m3.2.2.cmml" xref="S4.SS2.p3.3.m3.2.2">0.1</cn><cn type="float" id="S4.SS2.p3.3.m3.3.3.cmml" xref="S4.SS2.p3.3.m3.3.3">0.2</cn><cn type="float" id="S4.SS2.p3.3.m3.4.4.cmml" xref="S4.SS2.p3.3.m3.4.4">0.3</cn><cn type="float" id="S4.SS2.p3.3.m3.5.5.cmml" xref="S4.SS2.p3.3.m3.5.5">0.4</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.5c">\left[0.0,0.1,0.2,0.3,0.4\right]</annotation></semantics></math>, and log-variance mixing coefficients of <math id="S4.SS2.p3.4.m4.2" class="ltx_Math" alttext="\left[0.1,0.3\right]" display="inline"><semantics id="S4.SS2.p3.4.m4.2a"><mrow id="S4.SS2.p3.4.m4.2.3.2" xref="S4.SS2.p3.4.m4.2.3.1.cmml"><mo id="S4.SS2.p3.4.m4.2.3.2.1" xref="S4.SS2.p3.4.m4.2.3.1.cmml">[</mo><mn id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">0.1</mn><mo id="S4.SS2.p3.4.m4.2.3.2.2" xref="S4.SS2.p3.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS2.p3.4.m4.2.2" xref="S4.SS2.p3.4.m4.2.2.cmml">0.3</mn><mo id="S4.SS2.p3.4.m4.2.3.2.3" xref="S4.SS2.p3.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.2b"><interval closure="closed" id="S4.SS2.p3.4.m4.2.3.1.cmml" xref="S4.SS2.p3.4.m4.2.3.2"><cn type="float" id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">0.1</cn><cn type="float" id="S4.SS2.p3.4.m4.2.2.cmml" xref="S4.SS2.p3.4.m4.2.2">0.3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.2c">\left[0.1,0.3\right]</annotation></semantics></math>, and denoise for 128, 500, or 1000 steps.
Figure¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Sampling Parameters ‚Ä£ 4 Generative Model Training and Sampling ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows Pareto curves of FID vs.¬†CAS for the <math id="S4.SS2.p3.5.m5.1" class="ltx_Math" alttext="64\!\times\!64\rightarrow 256\!\times\!256" display="inline"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mrow id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml"><mn id="S4.SS2.p3.5.m5.1.1.2.2" xref="S4.SS2.p3.5.m5.1.1.2.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S4.SS2.p3.5.m5.1.1.2.1" xref="S4.SS2.p3.5.m5.1.1.2.1.cmml">√ó</mo><mn id="S4.SS2.p3.5.m5.1.1.2.3" xref="S4.SS2.p3.5.m5.1.1.2.3.cmml">64</mn></mrow><mo stretchy="false" id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.cmml">‚Üí</mo><mrow id="S4.SS2.p3.5.m5.1.1.3" xref="S4.SS2.p3.5.m5.1.1.3.cmml"><mn id="S4.SS2.p3.5.m5.1.1.3.2" xref="S4.SS2.p3.5.m5.1.1.3.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S4.SS2.p3.5.m5.1.1.3.1" xref="S4.SS2.p3.5.m5.1.1.3.1.cmml">√ó</mo><mn id="S4.SS2.p3.5.m5.1.1.3.3" xref="S4.SS2.p3.5.m5.1.1.3.3.cmml">256</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><ci id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1">‚Üí</ci><apply id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2"><times id="S4.SS2.p3.5.m5.1.1.2.1.cmml" xref="S4.SS2.p3.5.m5.1.1.2.1"></times><cn type="integer" id="S4.SS2.p3.5.m5.1.1.2.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2.2">64</cn><cn type="integer" id="S4.SS2.p3.5.m5.1.1.2.3.cmml" xref="S4.SS2.p3.5.m5.1.1.2.3">64</cn></apply><apply id="S4.SS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.3"><times id="S4.SS2.p3.5.m5.1.1.3.1.cmml" xref="S4.SS2.p3.5.m5.1.1.3.1"></times><cn type="integer" id="S4.SS2.p3.5.m5.1.1.3.2.cmml" xref="S4.SS2.p3.5.m5.1.1.3.2">256</cn><cn type="integer" id="S4.SS2.p3.5.m5.1.1.3.3.cmml" xref="S4.SS2.p3.5.m5.1.1.3.3">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">64\!\times\!64\rightarrow 256\!\times\!256</annotation></semantics></math> super-resolution module across different noise conditioning augmentation values using a guidance weight of 1.0.
These curves demonstrate the combined impact of the log-variance mixing coefficient and condition noise augmentation in achieving an optimal balance between FID and CAS.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Overall, the results suggest that FID and CAS are highly correlated, with smaller guidance weights leading to better CAS but negatively affecting Inception Score.
We observe that using noise augmentation of 0 yields the lowest FID score for all values of guidance weights for super-resolution models.
Nevertheless, it is worth noting that while larger amounts of noise augmentation tend to increase FID, they also produce more diverse samples, as also observed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Results of these studies are available in the Appendix.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.3" class="ltx_p">Based on these sweeps, taking FID and CAS into account, we selected guidance of 1.25 when sampling from the base model,
and 1.0 for other resolutions.
We use DDPM sampler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> log-variance mixing coefficients of 0.0 and 0.1 for <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mrow id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml"><mn id="S4.SS2.p5.1.m1.1.1.2" xref="S4.SS2.p5.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p5.1.m1.1.1.1" xref="S4.SS2.p5.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p5.1.m1.1.1.3" xref="S4.SS2.p5.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><apply id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1"><times id="S4.SS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p5.1.m1.1.1.2.cmml" xref="S4.SS2.p5.1.m1.1.1.2">64</cn><cn type="integer" id="S4.SS2.p5.1.m1.1.1.3.cmml" xref="S4.SS2.p5.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">64\times 64</annotation></semantics></math> samples and <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mrow id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml"><mn id="S4.SS2.p5.2.m2.1.1.2" xref="S4.SS2.p5.2.m2.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p5.2.m2.1.1.1" xref="S4.SS2.p5.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p5.2.m2.1.1.3" xref="S4.SS2.p5.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><apply id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1"><times id="S4.SS2.p5.2.m2.1.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p5.2.m2.1.1.2.cmml" xref="S4.SS2.p5.2.m2.1.1.2">256</cn><cn type="integer" id="S4.SS2.p5.2.m2.1.1.3.cmml" xref="S4.SS2.p5.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">256\times 256</annotation></semantics></math> samples respectively, with 1000 denoising steps.
At resolution <math id="S4.SS2.p5.3.m3.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S4.SS2.p5.3.m3.1a"><mrow id="S4.SS2.p5.3.m3.1.1" xref="S4.SS2.p5.3.m3.1.1.cmml"><mn id="S4.SS2.p5.3.m3.1.1.2" xref="S4.SS2.p5.3.m3.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p5.3.m3.1.1.1" xref="S4.SS2.p5.3.m3.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p5.3.m3.1.1.3" xref="S4.SS2.p5.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m3.1b"><apply id="S4.SS2.p5.3.m3.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1"><times id="S4.SS2.p5.3.m3.1.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p5.3.m3.1.1.2.cmml" xref="S4.SS2.p5.3.m3.1.1.2">1024</cn><cn type="integer" id="S4.SS2.p5.3.m3.1.1.3.cmml" xref="S4.SS2.p5.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m3.1c">1024\times 1024</annotation></semantics></math> we use a DDIM sampler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> with 32 steps, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
We do not use noise conditioning augmentation for sampling.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Generation Protocol</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">We use the fine-tuned Imagen model with the optimized sampling hyper-parameters to generate synthetic data resembling the training split of ImageNet dataset.
This means that we aim to produce the same quantity of images for each class as found in the real ImageNet dataset while keeping the same class balance as the original dataset.
We then constructed large-scale training datasets with ranging from 1.2M to 12M images, i.e., between <math id="S4.SS3.p1.1.m1.1" class="ltx_math_unparsed" alttext="1\times" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1b"><mn id="S4.SS3.p1.1.m1.1.1">1</mn><mo lspace="0.222em" id="S4.SS3.p1.1.m1.1.2">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1\times</annotation></semantics></math> to <math id="S4.SS3.p1.2.m2.1" class="ltx_math_unparsed" alttext="10\times" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1b"><mn id="S4.SS3.p1.2.m2.1.1">10</mn><mo lspace="0.222em" id="S4.SS3.p1.2.m2.1.2">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">10\times</annotation></semantics></math> the size of the original ImageNet training set.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.2.1.1" class="ltx_tr">
<th id="S5.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S5.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID train</th>
<th id="S5.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID validation</th>
<th id="S5.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.2.2.1" class="ltx_tr">
<td id="S5.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">64x64 resolution</td>
<td id="S5.T1.2.2.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S5.T1.2.2.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S5.T1.2.2.1.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T1.2.3.2" class="ltx_tr">
<td id="S5.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_border_t">BigGAN-deep (Dhariwal &amp; Nichol, 2021)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S5.T1.2.3.2.2" class="ltx_td ltx_align_center">4.06</td>
<td id="S5.T1.2.3.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.3.2.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T1.2.4.3" class="ltx_tr">
<td id="S5.T1.2.4.3.1" class="ltx_td ltx_align_left">Improved DDPM (Nichol &amp; Dhariwal, 2021)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S5.T1.2.4.3.2" class="ltx_td ltx_align_center">2.92</td>
<td id="S5.T1.2.4.3.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.4.3.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T1.2.5.4" class="ltx_tr">
<td id="S5.T1.2.5.4.1" class="ltx_td ltx_align_left">ADM (Dhariwal &amp; Nichol, 2021)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S5.T1.2.5.4.2" class="ltx_td ltx_align_center">2.07</td>
<td id="S5.T1.2.5.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.5.4.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T1.2.6.5" class="ltx_tr">
<td id="S5.T1.2.6.5.1" class="ltx_td ltx_align_left">CDM (Ho et al, 2022)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S5.T1.2.6.5.2" class="ltx_td ltx_align_center">1.48</td>
<td id="S5.T1.2.6.5.3" class="ltx_td ltx_align_center">2.48</td>
<td id="S5.T1.2.6.5.4" class="ltx_td ltx_align_center">67.95 ¬± 1.97</td>
</tr>
<tr id="S5.T1.2.7.6" class="ltx_tr">
<td id="S5.T1.2.7.6.1" class="ltx_td ltx_align_left">RIN (Jabri et al., 2022)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S5.T1.2.7.6.2" class="ltx_td ltx_align_center">1.23</td>
<td id="S5.T1.2.7.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.7.6.4" class="ltx_td ltx_align_center">66.5</td>
</tr>
<tr id="S5.T1.2.8.7" class="ltx_tr">
<td id="S5.T1.2.8.7.1" class="ltx_td ltx_align_left">RIN + noise schedule (Chen, 2023)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T1.2.8.7.2" class="ltx_td ltx_align_center">2.04</td>
<td id="S5.T1.2.8.7.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.8.7.4" class="ltx_td ltx_align_center">55.8</td>
</tr>
<tr id="S5.T1.2.9.8" class="ltx_tr">
<td id="S5.T1.2.9.8.1" class="ltx_td ltx_align_left">
<span id="S5.T1.2.9.8.1.1" class="ltx_text ltx_font_bold">Ours</span> (Fine-tuned Imagen)</td>
<td id="S5.T1.2.9.8.2" class="ltx_td ltx_align_center">1.21</td>
<td id="S5.T1.2.9.8.3" class="ltx_td ltx_align_center">2.51</td>
<td id="S5.T1.2.9.8.4" class="ltx_td ltx_align_center">85.77 ¬± 0.06</td>
</tr>
<tr id="S5.T1.2.10.9" class="ltx_tr">
<td id="S5.T1.2.10.9.1" class="ltx_td ltx_align_left ltx_border_t">256x256 resolution</td>
<td id="S5.T1.2.10.9.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.2.10.9.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.2.10.9.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.2.11.10" class="ltx_tr">
<td id="S5.T1.2.11.10.1" class="ltx_td ltx_align_left ltx_border_t">BigGAN-deep (Brock et al., 2019)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S5.T1.2.11.10.2" class="ltx_td ltx_align_center">6.9</td>
<td id="S5.T1.2.11.10.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.11.10.4" class="ltx_td ltx_align_center">171.4 ¬± 2.00</td>
</tr>
<tr id="S5.T1.2.12.11" class="ltx_tr">
<td id="S5.T1.2.12.11.1" class="ltx_td ltx_align_left">VQ-VAE-2 (Razavi et al., 2019)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S5.T1.2.12.11.2" class="ltx_td ltx_align_center">31.11</td>
<td id="S5.T1.2.12.11.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.12.11.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T1.2.13.12" class="ltx_tr">
<td id="S5.T1.2.13.12.1" class="ltx_td ltx_align_left">SR3 (Saharia et al., 2021)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S5.T1.2.13.12.2" class="ltx_td ltx_align_center">11.30</td>
<td id="S5.T1.2.13.12.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.13.12.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T1.2.14.13" class="ltx_tr">
<td id="S5.T1.2.14.13.1" class="ltx_td ltx_align_left">LDM-4 (Rombach et al., 2022)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S5.T1.2.14.13.2" class="ltx_td ltx_align_center">10.56</td>
<td id="S5.T1.2.14.13.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.14.13.4" class="ltx_td ltx_align_center">103.49</td>
</tr>
<tr id="S5.T1.2.15.14" class="ltx_tr">
<td id="S5.T1.2.15.14.1" class="ltx_td ltx_align_left">DiT-XL/2 (Peebles &amp; Xie, 2022)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S5.T1.2.15.14.2" class="ltx_td ltx_align_center">9.62</td>
<td id="S5.T1.2.15.14.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.15.14.4" class="ltx_td ltx_align_center">121.5</td>
</tr>
<tr id="S5.T1.2.16.15" class="ltx_tr">
<td id="S5.T1.2.16.15.1" class="ltx_td ltx_align_left">ADM (Dhariwal &amp; Nichol, 2021)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S5.T1.2.16.15.2" class="ltx_td ltx_align_center">10.94</td>
<td id="S5.T1.2.16.15.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.16.15.4" class="ltx_td ltx_align_center">100.98</td>
</tr>
<tr id="S5.T1.2.17.16" class="ltx_tr">
<td id="S5.T1.2.17.16.1" class="ltx_td ltx_align_left">ADM+upsampling (Dhariwal &amp; Nichol, 2021)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S5.T1.2.17.16.2" class="ltx_td ltx_align_center">7.49</td>
<td id="S5.T1.2.17.16.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.17.16.4" class="ltx_td ltx_align_center">127.49</td>
</tr>
<tr id="S5.T1.2.18.17" class="ltx_tr">
<td id="S5.T1.2.18.17.1" class="ltx_td ltx_align_left">CDM (Ho et al, 2022)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S5.T1.2.18.17.2" class="ltx_td ltx_align_center">4.88</td>
<td id="S5.T1.2.18.17.3" class="ltx_td ltx_align_center">3.76</td>
<td id="S5.T1.2.18.17.4" class="ltx_td ltx_align_center">158.71 ¬± 2.26</td>
</tr>
<tr id="S5.T1.2.19.18" class="ltx_tr">
<td id="S5.T1.2.19.18.1" class="ltx_td ltx_align_left">RIN (Jabri et al., 2022)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S5.T1.2.19.18.2" class="ltx_td ltx_align_center">4.51</td>
<td id="S5.T1.2.19.18.3" class="ltx_td ltx_align_center">4.51</td>
<td id="S5.T1.2.19.18.4" class="ltx_td ltx_align_center">161.0</td>
</tr>
<tr id="S5.T1.2.20.19" class="ltx_tr">
<td id="S5.T1.2.20.19.1" class="ltx_td ltx_align_left">RIN + noise schedule (Chen, 2023)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T1.2.20.19.2" class="ltx_td ltx_align_center">3.52</td>
<td id="S5.T1.2.20.19.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T1.2.20.19.4" class="ltx_td ltx_align_center">186.2</td>
</tr>
<tr id="S5.T1.2.21.20" class="ltx_tr">
<td id="S5.T1.2.21.20.1" class="ltx_td ltx_align_left">Simple Diffusion (U-Net) (Hoogeboom et al., 2023)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S5.T1.2.21.20.2" class="ltx_td ltx_align_center">3.76</td>
<td id="S5.T1.2.21.20.3" class="ltx_td ltx_align_center">2.88</td>
<td id="S5.T1.2.21.20.4" class="ltx_td ltx_align_center">171.6 ¬± 3.07</td>
</tr>
<tr id="S5.T1.2.22.21" class="ltx_tr">
<td id="S5.T1.2.22.21.1" class="ltx_td ltx_align_left">Simple Diffusion (U-ViT L) (Hoogeboom et al., 2023)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S5.T1.2.22.21.2" class="ltx_td ltx_align_center">2.77</td>
<td id="S5.T1.2.22.21.3" class="ltx_td ltx_align_center">3.23</td>
<td id="S5.T1.2.22.21.4" class="ltx_td ltx_align_center">211.8 ¬± 2.93</td>
</tr>
<tr id="S5.T1.2.23.22" class="ltx_tr">
<td id="S5.T1.2.23.22.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S5.T1.2.23.22.1.1" class="ltx_text ltx_font_bold">Ours</span> (Fine-tuned Imagen)</td>
<td id="S5.T1.2.23.22.2" class="ltx_td ltx_align_center ltx_border_bb">1.76</td>
<td id="S5.T1.2.23.22.3" class="ltx_td ltx_align_center ltx_border_bb">2.81</td>
<td id="S5.T1.2.23.22.4" class="ltx_td ltx_align_center ltx_border_bb">239.18 ¬± 1.14</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.4.2" class="ltx_text" style="font-size:90%;">
Comparison of sample quality of synthetic ImageNet datasets measured by FID and Inception Score (IS) between our fine-tuned Imagen model and generative models in the literature. We achieve SOTA FID and IS on ImageNet generation among other existing models, including class-conditional and guidance-based sampling without any design changes.
</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Sample Quality: FID and IS</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Despite the shortcomings described in Sec.¬†<a href="#S3" title="3 Background ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, FID¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and Inception Score¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> remain standard metrics for evaluating generative models.
Table¬†<a href="#S5.T1" title="Table 1 ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports FID and IS for our approach and existing class-conditional and guidance-based approaches.
Our fine-tuned model outperforms all the existing methods, including state-of-the-art methods that use U-Nets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and larger U-ViT models trained solely on ImageNet data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. This suggests that large-scale pretraining followed by fine-tuning on domain-specific target data is an effective strategy to achieve better visual quality with diffusion models, as measured by FID and IS.
Figure <a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows imaage samples from the fine-tuned model (see Appendix for more).
Note that our state-of-the-art FID and IS on ImageNet are obtained without any design changes, i.e., by simply adapting an off-the-shelf, diffusion-based text-to-image model to new data through fine-tuning. This is a promising result indicating that in a resource-limited setting, one can improve the performance of diffusion models by fine-tuning model weights and adjusting sampling parameters.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2304.08466/assets/x8.png" id="S5.F5.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="175" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2304.08466/assets/x9.png" id="S5.F5.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="175" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2304.08466/assets/x10.png" id="S5.F5.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="175" height="120" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.13.4.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.9.3" class="ltx_text" style="font-size:90%;">Class-wise classification accuracy comparison accuracy of models trained on real data (blue) and generated data (red).
<span id="S5.F5.9.3.1" class="ltx_text ltx_font_bold">Left</span>: The <math id="S5.F5.7.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S5.F5.7.1.m1.1b"><mrow id="S5.F5.7.1.m1.1.1" xref="S5.F5.7.1.m1.1.1.cmml"><mn id="S5.F5.7.1.m1.1.1.2" xref="S5.F5.7.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S5.F5.7.1.m1.1.1.1" xref="S5.F5.7.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.F5.7.1.m1.1.1.3" xref="S5.F5.7.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F5.7.1.m1.1c"><apply id="S5.F5.7.1.m1.1.1.cmml" xref="S5.F5.7.1.m1.1.1"><times id="S5.F5.7.1.m1.1.1.1.cmml" xref="S5.F5.7.1.m1.1.1.1"></times><cn type="integer" id="S5.F5.7.1.m1.1.1.2.cmml" xref="S5.F5.7.1.m1.1.1.2">256</cn><cn type="integer" id="S5.F5.7.1.m1.1.1.3.cmml" xref="S5.F5.7.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.7.1.m1.1d">256\times 256</annotation></semantics></math> CDM model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. <span id="S5.F5.9.3.2" class="ltx_text ltx_font_bold">Middle and right:</span> Our fine-tuned Imagen model at <math id="S5.F5.8.2.m2.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S5.F5.8.2.m2.1b"><mrow id="S5.F5.8.2.m2.1.1" xref="S5.F5.8.2.m2.1.1.cmml"><mn id="S5.F5.8.2.m2.1.1.2" xref="S5.F5.8.2.m2.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S5.F5.8.2.m2.1.1.1" xref="S5.F5.8.2.m2.1.1.1.cmml">√ó</mo><mn id="S5.F5.8.2.m2.1.1.3" xref="S5.F5.8.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F5.8.2.m2.1c"><apply id="S5.F5.8.2.m2.1.1.cmml" xref="S5.F5.8.2.m2.1.1"><times id="S5.F5.8.2.m2.1.1.1.cmml" xref="S5.F5.8.2.m2.1.1.1"></times><cn type="integer" id="S5.F5.8.2.m2.1.1.2.cmml" xref="S5.F5.8.2.m2.1.1.2">256</cn><cn type="integer" id="S5.F5.8.2.m2.1.1.3.cmml" xref="S5.F5.8.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.8.2.m2.1d">256\times 256</annotation></semantics></math> and <math id="S5.F5.9.3.m3.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S5.F5.9.3.m3.1b"><mrow id="S5.F5.9.3.m3.1.1" xref="S5.F5.9.3.m3.1.1.cmml"><mn id="S5.F5.9.3.m3.1.1.2" xref="S5.F5.9.3.m3.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S5.F5.9.3.m3.1.1.1" xref="S5.F5.9.3.m3.1.1.1.cmml">√ó</mo><mn id="S5.F5.9.3.m3.1.1.3" xref="S5.F5.9.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F5.9.3.m3.1c"><apply id="S5.F5.9.3.m3.1.1.cmml" xref="S5.F5.9.3.m3.1.1"><times id="S5.F5.9.3.m3.1.1.1.cmml" xref="S5.F5.9.3.m3.1.1.1"></times><cn type="integer" id="S5.F5.9.3.m3.1.1.2.cmml" xref="S5.F5.9.3.m3.1.1.2">1024</cn><cn type="integer" id="S5.F5.9.3.m3.1.1.3.cmml" xref="S5.F5.9.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.9.3.m3.1d">1024\times 1024</annotation></semantics></math>.
</span></figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.2.3.1" class="ltx_tr">
<th id="S5.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Model</th>
<td id="S5.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">Top-1 Accuracy (%)</td>
<td id="S5.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">Top-5 Accuracy(%)</td>
</tr>
<tr id="S5.T2.2.4.2" class="ltx_tr">
<th id="S5.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Real</th>
<td id="S5.T2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_tt">73.09</td>
<td id="S5.T2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_tt">91.47</td>
</tr>
<tr id="S5.T2.2.5.3" class="ltx_tr">
<th id="S5.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BigGAN-deep (Brock et al., 2019)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S5.T2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">42.65</td>
<td id="S5.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">65.92</td>
</tr>
<tr id="S5.T2.2.6.4" class="ltx_tr">
<th id="S5.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQ-VAE-2 (Razavi et al, 2019)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S5.T2.2.6.4.2" class="ltx_td ltx_align_center">54.83</td>
<td id="S5.T2.2.6.4.3" class="ltx_td ltx_align_center">77.59</td>
</tr>
<tr id="S5.T2.2.7.5" class="ltx_tr">
<th id="S5.T2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CDM (Ho et al, 2022)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S5.T2.2.7.5.2" class="ltx_td ltx_align_center">63.02</td>
<td id="S5.T2.2.7.5.3" class="ltx_td ltx_align_center">84.06</td>
</tr>
<tr id="S5.T2.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T2.1.1.1.1" class="ltx_text ltx_font_bold">Ours</span> (256<math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><times id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\times</annotation></semantics></math>256 resolution)</th>
<td id="S5.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_t">64.96</td>
<td id="S5.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">85.66</td>
</tr>
<tr id="S5.T2.2.2" class="ltx_tr">
<th id="S5.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S5.T2.2.2.1.1" class="ltx_text ltx_font_bold">Ours</span> (1024<math id="S5.T2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.2.2.1.m1.1a"><mo id="S5.T2.2.2.1.m1.1.1" xref="S5.T2.2.2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.1.m1.1b"><times id="S5.T2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.1.m1.1c">\times</annotation></semantics></math>1024 resolution)</th>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb">69.24</td>
<td id="S5.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb">88.10</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.8.3.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.6.2" class="ltx_text" style="font-size:90%;">
Classification Accuracy Scores (CAS) for <math id="S5.T2.5.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S5.T2.5.1.m1.1b"><mrow id="S5.T2.5.1.m1.1.1" xref="S5.T2.5.1.m1.1.1.cmml"><mn id="S5.T2.5.1.m1.1.1.2" xref="S5.T2.5.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S5.T2.5.1.m1.1.1.1" xref="S5.T2.5.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.T2.5.1.m1.1.1.3" xref="S5.T2.5.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.5.1.m1.1c"><apply id="S5.T2.5.1.m1.1.1.cmml" xref="S5.T2.5.1.m1.1.1"><times id="S5.T2.5.1.m1.1.1.1.cmml" xref="S5.T2.5.1.m1.1.1.1"></times><cn type="integer" id="S5.T2.5.1.m1.1.1.2.cmml" xref="S5.T2.5.1.m1.1.1.2">256</cn><cn type="integer" id="S5.T2.5.1.m1.1.1.3.cmml" xref="S5.T2.5.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.1.m1.1d">256\!\times\!256</annotation></semantics></math> and <math id="S5.T2.6.2.m2.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S5.T2.6.2.m2.1b"><mrow id="S5.T2.6.2.m2.1.1" xref="S5.T2.6.2.m2.1.1.cmml"><mn id="S5.T2.6.2.m2.1.1.2" xref="S5.T2.6.2.m2.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S5.T2.6.2.m2.1.1.1" xref="S5.T2.6.2.m2.1.1.1.cmml">√ó</mo><mn id="S5.T2.6.2.m2.1.1.3" xref="S5.T2.6.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.6.2.m2.1c"><apply id="S5.T2.6.2.m2.1.1.cmml" xref="S5.T2.6.2.m2.1.1"><times id="S5.T2.6.2.m2.1.1.1.cmml" xref="S5.T2.6.2.m2.1.1.1"></times><cn type="integer" id="S5.T2.6.2.m2.1.1.2.cmml" xref="S5.T2.6.2.m2.1.1.2">1024</cn><cn type="integer" id="S5.T2.6.2.m2.1.1.3.cmml" xref="S5.T2.6.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.2.m2.1d">1024\!\times\!1024</annotation></semantics></math> generated samples. CAS for real data and other models are obtained from¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Our results indicate that the fine-tuned generative diffusion model outperforms the previous methods by a substantial margin.
</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Classification Accuracy Score</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.2" class="ltx_p">As noted above, classification accuracy score (CAS)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> is a better proxy than FID and IS for performance of downstream training on generated data.
CAS measures ImageNet classification accuracy on the real test data for a model trained solely on synthetic samples.
In keeping with the CAS protocol¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, we train a standard ResNet-50 architecture on a single crop from each training image. Models are trained for 90 epochs with a batch size of 1024 using SGD with momentum (see Appendix <a href="#S4a" title="A.4 Hyper-parameters and model selection for ImageNet classifiers. ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a> for details).
Regardless of the resolution of the generated data, for CAS training and evaluation, we resize images to <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mn id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><times id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">256\!\times\!256</annotation></semantics></math> (or, for real images, to 256 pixels on the shorter side) and then take a <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="224\!\times\!224" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mn id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">224</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><times id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">224</cn><cn type="integer" id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">224\!\times\!224</annotation></semantics></math> pixel center crop.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.6" class="ltx_p">Table¬†<a href="#S5.T2" title="Table 2 ‚Ä£ 5.1 Sample Quality: FID and IS ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports CAS for samples from our fine-tuned models at resolutions <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mn id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><times id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">256</cn><cn type="integer" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">256\!\times\!256</annotation></semantics></math> and <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mrow id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mn id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS2.p2.2.m2.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><times id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">1024</cn><cn type="integer" id="S5.SS2.p2.2.m2.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">1024\!\times\!1024</annotation></semantics></math>. CAS for real data and for other models are taken from¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
The results indicate that our fine-tuned class-conditional models outperform the previous methods in the literature at <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mrow id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml"><mn id="S5.SS2.p2.3.m3.1.1.2" xref="S5.SS2.p2.3.m3.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p2.3.m3.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p2.3.m3.1.1.3" xref="S5.SS2.p2.3.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"><times id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S5.SS2.p2.3.m3.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.2">256</cn><cn type="integer" id="S5.SS2.p2.3.m3.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">256\times 256</annotation></semantics></math> resolution by a good margin, for both Top-1 and Top-5 accuracy.
Interestingly, results are markedly better for <math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mrow id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml"><mn id="S5.SS2.p2.4.m4.1.1.2" xref="S5.SS2.p2.4.m4.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p2.4.m4.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p2.4.m4.1.1.3" xref="S5.SS2.p2.4.m4.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><apply id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1"><times id="S5.SS2.p2.4.m4.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1"></times><cn type="integer" id="S5.SS2.p2.4.m4.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.2">1024</cn><cn type="integer" id="S5.SS2.p2.4.m4.1.1.3.cmml" xref="S5.SS2.p2.4.m4.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">1024\times 1024</annotation></semantics></math> samples, even though these samples are down-sampled to <math id="S5.SS2.p2.5.m5.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><mrow id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml"><mn id="S5.SS2.p2.5.m5.1.1.2" xref="S5.SS2.p2.5.m5.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p2.5.m5.1.1.1" xref="S5.SS2.p2.5.m5.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p2.5.m5.1.1.3" xref="S5.SS2.p2.5.m5.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><apply id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1"><times id="S5.SS2.p2.5.m5.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1.1"></times><cn type="integer" id="S5.SS2.p2.5.m5.1.1.2.cmml" xref="S5.SS2.p2.5.m5.1.1.2">256</cn><cn type="integer" id="S5.SS2.p2.5.m5.1.1.3.cmml" xref="S5.SS2.p2.5.m5.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">256\times 256</annotation></semantics></math> during classifier training.
As reported in Table¬†<a href="#S5.T2" title="Table 2 ‚Ä£ 5.1 Sample Quality: FID and IS ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
we achieve the SOTA Top-1 classification accuracy score of 69.24% at resolution <math id="S5.SS2.p2.6.m6.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S5.SS2.p2.6.m6.1a"><mrow id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml"><mn id="S5.SS2.p2.6.m6.1.1.2" xref="S5.SS2.p2.6.m6.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS2.p2.6.m6.1.1.1" xref="S5.SS2.p2.6.m6.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p2.6.m6.1.1.3" xref="S5.SS2.p2.6.m6.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><apply id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1"><times id="S5.SS2.p2.6.m6.1.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1.1"></times><cn type="integer" id="S5.SS2.p2.6.m6.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1.2">1024</cn><cn type="integer" id="S5.SS2.p2.6.m6.1.1.3.cmml" xref="S5.SS2.p2.6.m6.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">1024\!\times\!1024</annotation></semantics></math>. This greatly narrows the gap with the ResNet-50 model trained on real data.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Figure <a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1 Sample Quality: FID and IS ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the accuracy of models trained on generative data (red) compared to a model trained on real data (blue) for each of the 1000 ImageNet classes
(cf.¬†Fig.¬†2 in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>).
From Figure <a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1 Sample Quality: FID and IS ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (left) one can see that the ResNet-50 trained on CDM samples is weaker than the model trained on real data, as most red points fall below the blue points.
For our fine-tuned Imagen models (Figure <a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1 Sample Quality: FID and IS ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> middle and right), however, there are more classes for which the models trained on generated data outperform the model trained on real data.
This is particularly clear at <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mn id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><times id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">1024</cn><cn type="integer" id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">1024\!\times\!1024</annotation></semantics></math>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Classification Accuracy with Different Models</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To further evaluate the discriminative power of the synthetic data, compared to the real ImageNet-1K data, we analyze the classification accuracy of models with different architectures, input resolutions, and model capacities.
We consider multiple ResNet-based and Vision Transformers (ViT)-based¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> classifiers including ResNet-50¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, ResNet-RS-50, ResNet-RS-152x2, ResNet-RS-350x2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, ViT-S/16¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and DeiT-B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>.
The models trained on real, synthetic, and the combination of real and synthetic data are all trained in the same way, consistent with the training recipes specified by authors of these models on ImageNet-1K, and our results on real data agree with the published results. The Appendix has more details on model training.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‚Ä£ 5.4 Merging Real and Synthetic Data at Scale ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reports the Top-1 validation accuracy of multiple ConvNet and Transformer models when trained with the 1.2M real ImageNet training images, with 1.2M generated images, and when the generative samples are used to augment the real data.
As one might expect, models trained solely on generated samples perform worse than models trained on real data.
Nevertheless, augmenting real data with synthetic images from the diffusion model yields a substantial boost in performance across all classifiers tested.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Merging Real and Synthetic Data at Scale</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We next consider how performance of a ResNet-50 classifier depends on the amount of generated data that is used to augment the real data. Here we follow conventional training recipe and train with random crops for 130 epochs, resulting in a higher ResNet-50 accuracy here than in the CAS results in Table <a href="#S5.T2" title="Table 2 ‚Ä£ 5.1 Sample Quality: FID and IS ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The Appendix provides training details.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Ravuri and Vinyals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> (Fig.¬†5) found that for almost all models tested, mixing generated samples with real data degrades Top-5 classifier accuracy.
For Big-GAN-deep <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> with low truncation values (sacrificing diversity for sample quality), accuracy increases marginally with small amounts of generated data, but then drops below models trained solely on real data when the amount of generated data approaches the size of the real train set.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.13" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Input Size</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Params (M)</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Real Only</td>
<td id="S5.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Generated Only</td>
<td id="S5.T3.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">Real + Generated</td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Performance <math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mi mathvariant="normal" id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">Œî</mi><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">Œî</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\Delta</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.13.14.1" class="ltx_tr">
<td id="S5.T3.13.14.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="7">ConvNets</td>
</tr>
<tr id="S5.T3.2.2" class="ltx_tr">
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">ResNet-50</td>
<td id="S5.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">224<math id="S5.T3.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.2.2.1.m1.1a"><mo id="S5.T3.2.2.1.m1.1.1" xref="S5.T3.2.2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.1.m1.1b"><times id="S5.T3.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">36</td>
<td id="S5.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">76.39</td>
<td id="S5.T3.2.2.5" class="ltx_td ltx_align_center ltx_border_tt">69.24</td>
<td id="S5.T3.2.2.6" class="ltx_td ltx_align_center ltx_border_tt">78.17</td>
<td id="S5.T3.2.2.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.2.2.7.1" class="ltx_text" style="color:#000000;">+1.78</span></td>
</tr>
<tr id="S5.T3.3.3" class="ltx_tr">
<td id="S5.T3.3.3.2" class="ltx_td ltx_align_left ltx_border_r">ResNet-101</td>
<td id="S5.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_r">224<math id="S5.T3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.3.3.1.m1.1a"><mo id="S5.T3.3.3.1.m1.1.1" xref="S5.T3.3.3.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.1.m1.1b"><times id="S5.T3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">45</td>
<td id="S5.T3.3.3.4" class="ltx_td ltx_align_center">78.15</td>
<td id="S5.T3.3.3.5" class="ltx_td ltx_align_center">71.31</td>
<td id="S5.T3.3.3.6" class="ltx_td ltx_align_center">79.74</td>
<td id="S5.T3.3.3.7" class="ltx_td ltx_align_center"><span id="S5.T3.3.3.7.1" class="ltx_text" style="color:#000000;">+1.59</span></td>
</tr>
<tr id="S5.T3.4.4" class="ltx_tr">
<td id="S5.T3.4.4.2" class="ltx_td ltx_align_left ltx_border_r">ResNet-152</td>
<td id="S5.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_r">224<math id="S5.T3.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.4.4.1.m1.1a"><mo id="S5.T3.4.4.1.m1.1.1" xref="S5.T3.4.4.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.1.m1.1b"><times id="S5.T3.4.4.1.m1.1.1.cmml" xref="S5.T3.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.4.4.3" class="ltx_td ltx_align_center ltx_border_r">64</td>
<td id="S5.T3.4.4.4" class="ltx_td ltx_align_center">78.59</td>
<td id="S5.T3.4.4.5" class="ltx_td ltx_align_center">72.38</td>
<td id="S5.T3.4.4.6" class="ltx_td ltx_align_center">80.15</td>
<td id="S5.T3.4.4.7" class="ltx_td ltx_align_center"><span id="S5.T3.4.4.7.1" class="ltx_text" style="color:#000000;">+1.56</span></td>
</tr>
<tr id="S5.T3.5.5" class="ltx_tr">
<td id="S5.T3.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ResNet-RS-50</td>
<td id="S5.T3.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">160<math id="S5.T3.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.5.5.1.m1.1a"><mo id="S5.T3.5.5.1.m1.1.1" xref="S5.T3.5.5.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.1.m1.1b"><times id="S5.T3.5.5.1.m1.1.1.cmml" xref="S5.T3.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.1.m1.1c">\times</annotation></semantics></math>160</td>
<td id="S5.T3.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36</td>
<td id="S5.T3.5.5.4" class="ltx_td ltx_align_center ltx_border_t">79.10</td>
<td id="S5.T3.5.5.5" class="ltx_td ltx_align_center ltx_border_t">70.72</td>
<td id="S5.T3.5.5.6" class="ltx_td ltx_align_center ltx_border_t">79.97</td>
<td id="S5.T3.5.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.5.5.7.1" class="ltx_text" style="color:#000000;">+0.87</span></td>
</tr>
<tr id="S5.T3.6.6" class="ltx_tr">
<td id="S5.T3.6.6.2" class="ltx_td ltx_align_left ltx_border_r">ResNet-RS-101</td>
<td id="S5.T3.6.6.1" class="ltx_td ltx_align_center ltx_border_r">160<math id="S5.T3.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.6.6.1.m1.1a"><mo id="S5.T3.6.6.1.m1.1.1" xref="S5.T3.6.6.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.1.m1.1b"><times id="S5.T3.6.6.1.m1.1.1.cmml" xref="S5.T3.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.1.m1.1c">\times</annotation></semantics></math>160</td>
<td id="S5.T3.6.6.3" class="ltx_td ltx_align_center ltx_border_r">64</td>
<td id="S5.T3.6.6.4" class="ltx_td ltx_align_center">80.11</td>
<td id="S5.T3.6.6.5" class="ltx_td ltx_align_center">72.73</td>
<td id="S5.T3.6.6.6" class="ltx_td ltx_align_center">80.89</td>
<td id="S5.T3.6.6.7" class="ltx_td ltx_align_center"><span id="S5.T3.6.6.7.1" class="ltx_text" style="color:#000000;">+0.78</span></td>
</tr>
<tr id="S5.T3.7.7" class="ltx_tr">
<td id="S5.T3.7.7.2" class="ltx_td ltx_align_left ltx_border_r">ResNet-RS-101</td>
<td id="S5.T3.7.7.1" class="ltx_td ltx_align_center ltx_border_r">190<math id="S5.T3.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.7.7.1.m1.1a"><mo id="S5.T3.7.7.1.m1.1.1" xref="S5.T3.7.7.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.1.m1.1b"><times id="S5.T3.7.7.1.m1.1.1.cmml" xref="S5.T3.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.1.m1.1c">\times</annotation></semantics></math>190</td>
<td id="S5.T3.7.7.3" class="ltx_td ltx_align_center ltx_border_r">64</td>
<td id="S5.T3.7.7.4" class="ltx_td ltx_align_center">81.29</td>
<td id="S5.T3.7.7.5" class="ltx_td ltx_align_center">73.63</td>
<td id="S5.T3.7.7.6" class="ltx_td ltx_align_center">81.80</td>
<td id="S5.T3.7.7.7" class="ltx_td ltx_align_center"><span id="S5.T3.7.7.7.1" class="ltx_text" style="color:#000000;">+0.51</span></td>
</tr>
<tr id="S5.T3.8.8" class="ltx_tr">
<td id="S5.T3.8.8.2" class="ltx_td ltx_align_left ltx_border_r">ResNet-RS-152</td>
<td id="S5.T3.8.8.1" class="ltx_td ltx_align_center ltx_border_r">224<math id="S5.T3.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.8.8.1.m1.1a"><mo id="S5.T3.8.8.1.m1.1.1" xref="S5.T3.8.8.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.1.m1.1b"><times id="S5.T3.8.8.1.m1.1.1.cmml" xref="S5.T3.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.8.8.3" class="ltx_td ltx_align_center ltx_border_r">87</td>
<td id="S5.T3.8.8.4" class="ltx_td ltx_align_center">82.81</td>
<td id="S5.T3.8.8.5" class="ltx_td ltx_align_center">74.46</td>
<td id="S5.T3.8.8.6" class="ltx_td ltx_align_center">83.10</td>
<td id="S5.T3.8.8.7" class="ltx_td ltx_align_center"><span id="S5.T3.8.8.7.1" class="ltx_text" style="color:#000000;">+0.29</span></td>
</tr>
<tr id="S5.T3.13.15.2" class="ltx_tr">
<td id="S5.T3.13.15.2.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="7">Transformers</td>
</tr>
<tr id="S5.T3.9.9" class="ltx_tr">
<td id="S5.T3.9.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">ViT-S/16</td>
<td id="S5.T3.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">224<math id="S5.T3.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.9.9.1.m1.1a"><mo id="S5.T3.9.9.1.m1.1.1" xref="S5.T3.9.9.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.1.m1.1b"><times id="S5.T3.9.9.1.m1.1.1.cmml" xref="S5.T3.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22</td>
<td id="S5.T3.9.9.4" class="ltx_td ltx_align_center ltx_border_tt">79.89</td>
<td id="S5.T3.9.9.5" class="ltx_td ltx_align_center ltx_border_tt">71.88</td>
<td id="S5.T3.9.9.6" class="ltx_td ltx_align_center ltx_border_tt">81.00</td>
<td id="S5.T3.9.9.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.9.9.7.1" class="ltx_text" style="color:#000000;">+1.11</span></td>
</tr>
<tr id="S5.T3.10.10" class="ltx_tr">
<td id="S5.T3.10.10.2" class="ltx_td ltx_align_left ltx_border_r">DeiT-S</td>
<td id="S5.T3.10.10.1" class="ltx_td ltx_align_center ltx_border_r">224<math id="S5.T3.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.10.10.1.m1.1a"><mo id="S5.T3.10.10.1.m1.1.1" xref="S5.T3.10.10.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.1.m1.1b"><times id="S5.T3.10.10.1.m1.1.1.cmml" xref="S5.T3.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.10.10.3" class="ltx_td ltx_align_center ltx_border_r">22</td>
<td id="S5.T3.10.10.4" class="ltx_td ltx_align_center">78.97</td>
<td id="S5.T3.10.10.5" class="ltx_td ltx_align_center">72.26</td>
<td id="S5.T3.10.10.6" class="ltx_td ltx_align_center">80.49</td>
<td id="S5.T3.10.10.7" class="ltx_td ltx_align_center"><span id="S5.T3.10.10.7.1" class="ltx_text" style="color:#000000;">+1.52</span></td>
</tr>
<tr id="S5.T3.11.11" class="ltx_tr">
<td id="S5.T3.11.11.2" class="ltx_td ltx_align_left ltx_border_r">DeiT-B</td>
<td id="S5.T3.11.11.1" class="ltx_td ltx_align_center ltx_border_r">224<math id="S5.T3.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.11.11.1.m1.1a"><mo id="S5.T3.11.11.1.m1.1.1" xref="S5.T3.11.11.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.11.11.1.m1.1b"><times id="S5.T3.11.11.1.m1.1.1.cmml" xref="S5.T3.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.11.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.11.11.3" class="ltx_td ltx_align_center ltx_border_r">87</td>
<td id="S5.T3.11.11.4" class="ltx_td ltx_align_center">81.79</td>
<td id="S5.T3.11.11.5" class="ltx_td ltx_align_center">74.55</td>
<td id="S5.T3.11.11.6" class="ltx_td ltx_align_center">82.84</td>
<td id="S5.T3.11.11.7" class="ltx_td ltx_align_center"><span id="S5.T3.11.11.7.1" class="ltx_text" style="color:#000000;">+1.04</span></td>
</tr>
<tr id="S5.T3.12.12" class="ltx_tr">
<td id="S5.T3.12.12.2" class="ltx_td ltx_align_left ltx_border_r">DeiT-B</td>
<td id="S5.T3.12.12.1" class="ltx_td ltx_align_center ltx_border_r">384<math id="S5.T3.12.12.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.12.12.1.m1.1a"><mo id="S5.T3.12.12.1.m1.1.1" xref="S5.T3.12.12.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.1.m1.1b"><times id="S5.T3.12.12.1.m1.1.1.cmml" xref="S5.T3.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.1.m1.1c">\times</annotation></semantics></math>384</td>
<td id="S5.T3.12.12.3" class="ltx_td ltx_align_center ltx_border_r">87</td>
<td id="S5.T3.12.12.4" class="ltx_td ltx_align_center">83.16</td>
<td id="S5.T3.12.12.5" class="ltx_td ltx_align_center">75.45</td>
<td id="S5.T3.12.12.6" class="ltx_td ltx_align_center">83.75</td>
<td id="S5.T3.12.12.7" class="ltx_td ltx_align_center"><span id="S5.T3.12.12.7.1" class="ltx_text" style="color:#000000;">+0.59</span></td>
</tr>
<tr id="S5.T3.13.13" class="ltx_tr">
<td id="S5.T3.13.13.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">DeiT-L</td>
<td id="S5.T3.13.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">224<math id="S5.T3.13.13.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.13.13.1.m1.1a"><mo id="S5.T3.13.13.1.m1.1.1" xref="S5.T3.13.13.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T3.13.13.1.m1.1b"><times id="S5.T3.13.13.1.m1.1.1.cmml" xref="S5.T3.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.13.13.1.m1.1c">\times</annotation></semantics></math>224</td>
<td id="S5.T3.13.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">307</td>
<td id="S5.T3.13.13.4" class="ltx_td ltx_align_center ltx_border_bb">82.22</td>
<td id="S5.T3.13.13.5" class="ltx_td ltx_align_center ltx_border_bb">74.60</td>
<td id="S5.T3.13.13.6" class="ltx_td ltx_align_center ltx_border_bb">83.05</td>
<td id="S5.T3.13.13.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.13.13.7.1" class="ltx_text" style="color:#000000;">+0.83</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.15.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.16.2" class="ltx_text" style="font-size:90%;">Comparison of classifier Top-1 Accuracy (%) performance when 1.2M generated images are used for generative data augmentation.
Models trained solely on generated samples perform worse than models trained on real data.
Nevertheless, augmenting the real data with data generated from the fine-tuned diffusion model provides a substantial boost in performance across many different classifiers.</span></figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.2.2" class="ltx_tr">
<th id="S5.T4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Train Set (M)</th>
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">256<math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><times id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\times</annotation></semantics></math>256</th>
<th id="S5.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">1024<math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><times id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">\times</annotation></semantics></math>1024</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.2.3.1" class="ltx_tr">
<th id="S5.T4.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">1.2</th>
<td id="S5.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">76.39 ¬± 0.21</td>
<td id="S5.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">76.39 ¬± 0.21</td>
</tr>
<tr id="S5.T4.2.4.2" class="ltx_tr">
<th id="S5.T4.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2.4</th>
<td id="S5.T4.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t">77.61 ¬± 0.08 <span id="S5.T4.2.4.2.2.1" class="ltx_text" style="font-size:70%;color:#28B23D;">(+1.22)</span>
</td>
<td id="S5.T4.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">78.12 ¬± 0.05 <span id="S5.T4.2.4.2.3.1" class="ltx_text" style="font-size:70%;color:#28B23D;">(+1.73)</span>
</td>
</tr>
<tr id="S5.T4.2.5.3" class="ltx_tr">
<th id="S5.T4.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3.6</th>
<td id="S5.T4.2.5.3.2" class="ltx_td ltx_align_center">77.16 ¬± 0.04 <span id="S5.T4.2.5.3.2.1" class="ltx_text" style="font-size:70%;color:#28B23D;">(+0.77)</span>
</td>
<td id="S5.T4.2.5.3.3" class="ltx_td ltx_align_center">77.48 ¬± 0.04 <span id="S5.T4.2.5.3.3.1" class="ltx_text" style="font-size:70%;color:#28B23D;">(+1.09)</span>
</td>
</tr>
<tr id="S5.T4.2.6.4" class="ltx_tr">
<th id="S5.T4.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4.8</th>
<td id="S5.T4.2.6.4.2" class="ltx_td ltx_align_center">76.52 ¬± 0.04 <span id="S5.T4.2.6.4.2.1" class="ltx_text" style="font-size:70%;color:#28B23D;">(+0.13)</span>
</td>
<td id="S5.T4.2.6.4.3" class="ltx_td ltx_align_center">76.75 ¬± 0.07 <span id="S5.T4.2.6.4.3.1" class="ltx_text" style="font-size:70%;color:#28B23D;">(+0.36)</span>
</td>
</tr>
<tr id="S5.T4.2.7.5" class="ltx_tr">
<th id="S5.T4.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">6.0</th>
<td id="S5.T4.2.7.5.2" class="ltx_td ltx_align_center">76.09 ¬± 0.08 <span id="S5.T4.2.7.5.2.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-0.30)</span>
</td>
<td id="S5.T4.2.7.5.3" class="ltx_td ltx_align_center">76.34 ¬± 0.13 <span id="S5.T4.2.7.5.3.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-0.05)</span>
</td>
</tr>
<tr id="S5.T4.2.8.6" class="ltx_tr">
<th id="S5.T4.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7.2</th>
<td id="S5.T4.2.8.6.2" class="ltx_td ltx_align_center">75.81 ¬± 0.08 <span id="S5.T4.2.8.6.2.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-0.58)</span>
</td>
<td id="S5.T4.2.8.6.3" class="ltx_td ltx_align_center">75.87 ¬± 0.09 <span id="S5.T4.2.8.6.3.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-0.52)</span>
</td>
</tr>
<tr id="S5.T4.2.9.7" class="ltx_tr">
<th id="S5.T4.2.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8.4</th>
<td id="S5.T4.2.9.7.2" class="ltx_td ltx_align_center">75.44 ¬± 0.06 <span id="S5.T4.2.9.7.2.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-0.95)</span>
</td>
<td id="S5.T4.2.9.7.3" class="ltx_td ltx_align_center">75.49 ¬± 0.07 <span id="S5.T4.2.9.7.3.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-0.90)</span>
</td>
</tr>
<tr id="S5.T4.2.10.8" class="ltx_tr">
<th id="S5.T4.2.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">9.6</th>
<td id="S5.T4.2.10.8.2" class="ltx_td ltx_align_center">75.28 ¬± 0.10 <span id="S5.T4.2.10.8.2.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-1.11)</span>
</td>
<td id="S5.T4.2.10.8.3" class="ltx_td ltx_align_center">74.72 ¬± 0.20 <span id="S5.T4.2.10.8.3.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-1.67)</span>
</td>
</tr>
<tr id="S5.T4.2.11.9" class="ltx_tr">
<th id="S5.T4.2.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10.8</th>
<td id="S5.T4.2.11.9.2" class="ltx_td ltx_align_center">75.11 ¬± 0.12 <span id="S5.T4.2.11.9.2.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-1.28)</span>
</td>
<td id="S5.T4.2.11.9.3" class="ltx_td ltx_align_center">74.14 ¬± 0.13 <span id="S5.T4.2.11.9.3.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-2.25)</span>
</td>
</tr>
<tr id="S5.T4.2.12.10" class="ltx_tr">
<th id="S5.T4.2.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">12.0</th>
<td id="S5.T4.2.12.10.2" class="ltx_td ltx_align_center ltx_border_bb">75.04 ¬± 0.05 <span id="S5.T4.2.12.10.2.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-1.35)</span>
</td>
<td id="S5.T4.2.12.10.3" class="ltx_td ltx_align_center ltx_border_bb">73.70 ¬± 0.09 <span id="S5.T4.2.12.10.3.1" class="ltx_text" style="font-size:70%;color:#ED1C1C;">(-2.69)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.10.4.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.8.3" class="ltx_text" style="font-size:90%;">Scaling the training dataset by adding synthetic images, at resolutions
<math id="S5.T4.6.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S5.T4.6.1.m1.1b"><mrow id="S5.T4.6.1.m1.1.1" xref="S5.T4.6.1.m1.1.1.cmml"><mn id="S5.T4.6.1.m1.1.1.2" xref="S5.T4.6.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S5.T4.6.1.m1.1.1.1" xref="S5.T4.6.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.T4.6.1.m1.1.1.3" xref="S5.T4.6.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.6.1.m1.1c"><apply id="S5.T4.6.1.m1.1.1.cmml" xref="S5.T4.6.1.m1.1.1"><times id="S5.T4.6.1.m1.1.1.1.cmml" xref="S5.T4.6.1.m1.1.1.1"></times><cn type="integer" id="S5.T4.6.1.m1.1.1.2.cmml" xref="S5.T4.6.1.m1.1.1.2">256</cn><cn type="integer" id="S5.T4.6.1.m1.1.1.3.cmml" xref="S5.T4.6.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.1.m1.1d">256\!\times\!256</annotation></semantics></math> and <math id="S5.T4.7.2.m2.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S5.T4.7.2.m2.1b"><mrow id="S5.T4.7.2.m2.1.1" xref="S5.T4.7.2.m2.1.1.cmml"><mn id="S5.T4.7.2.m2.1.1.2" xref="S5.T4.7.2.m2.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S5.T4.7.2.m2.1.1.1" xref="S5.T4.7.2.m2.1.1.1.cmml">√ó</mo><mn id="S5.T4.7.2.m2.1.1.3" xref="S5.T4.7.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.7.2.m2.1c"><apply id="S5.T4.7.2.m2.1.1.cmml" xref="S5.T4.7.2.m2.1.1"><times id="S5.T4.7.2.m2.1.1.1.cmml" xref="S5.T4.7.2.m2.1.1.1"></times><cn type="integer" id="S5.T4.7.2.m2.1.1.2.cmml" xref="S5.T4.7.2.m2.1.1.2">1024</cn><cn type="integer" id="S5.T4.7.2.m2.1.1.3.cmml" xref="S5.T4.7.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.2.m2.1d">1024\!\times\!1024</annotation></semantics></math>.
The baseline Top-1 accuracy of the classifier trained on real data is <math id="S5.T4.8.3.m3.1" class="ltx_Math" alttext="76.39\pm 0.21" display="inline"><semantics id="S5.T4.8.3.m3.1b"><mrow id="S5.T4.8.3.m3.1.1" xref="S5.T4.8.3.m3.1.1.cmml"><mn id="S5.T4.8.3.m3.1.1.2" xref="S5.T4.8.3.m3.1.1.2.cmml">76.39</mn><mo id="S5.T4.8.3.m3.1.1.1" xref="S5.T4.8.3.m3.1.1.1.cmml">¬±</mo><mn id="S5.T4.8.3.m3.1.1.3" xref="S5.T4.8.3.m3.1.1.3.cmml">0.21</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.8.3.m3.1c"><apply id="S5.T4.8.3.m3.1.1.cmml" xref="S5.T4.8.3.m3.1.1"><csymbol cd="latexml" id="S5.T4.8.3.m3.1.1.1.cmml" xref="S5.T4.8.3.m3.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T4.8.3.m3.1.1.2.cmml" xref="S5.T4.8.3.m3.1.1.2">76.39</cn><cn type="float" id="S5.T4.8.3.m3.1.1.3.cmml" xref="S5.T4.8.3.m3.1.1.3">0.21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.3.m3.1d">76.39\pm 0.21</annotation></semantics></math>. The number in parenthesis shows the change obtained over baseline with the addition of generated data.
</span></figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2304.08466/assets/x11.png" id="S5.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="179" height="116" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.5.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.1" class="ltx_text" style="font-size:90%;">Improved classification accuracy of ResNet-50 with increasing numbers of synthetic images added to real training data at resolution
<math id="S5.F6.3.1.m1.1" class="ltx_Math" alttext="64\!\times\!64" display="inline"><semantics id="S5.F6.3.1.m1.1b"><mrow id="S5.F6.3.1.m1.1.1" xref="S5.F6.3.1.m1.1.1.cmml"><mn id="S5.F6.3.1.m1.1.1.2" xref="S5.F6.3.1.m1.1.1.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S5.F6.3.1.m1.1.1.1" xref="S5.F6.3.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.F6.3.1.m1.1.1.3" xref="S5.F6.3.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F6.3.1.m1.1c"><apply id="S5.F6.3.1.m1.1.1.cmml" xref="S5.F6.3.1.m1.1.1"><times id="S5.F6.3.1.m1.1.1.1.cmml" xref="S5.F6.3.1.m1.1.1.1"></times><cn type="integer" id="S5.F6.3.1.m1.1.1.2.cmml" xref="S5.F6.3.1.m1.1.1.2">64</cn><cn type="integer" id="S5.F6.3.1.m1.1.1.3.cmml" xref="S5.F6.3.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.3.1.m1.1d">64\!\times\!64</annotation></semantics></math>.
</span></figcaption>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.4" class="ltx_p">Figure <a href="#S5.F6" title="Figure 6 ‚Ä£ 5.4 Merging Real and Synthetic Data at Scale ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows that, for <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mrow id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml"><mn id="S5.SS4.p3.1.m1.1.1.2" xref="S5.SS4.p3.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.p3.1.m1.1.1.1" xref="S5.SS4.p3.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.SS4.p3.1.m1.1.1.3" xref="S5.SS4.p3.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><apply id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1"><times id="S5.SS4.p3.1.m1.1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1.1"></times><cn type="integer" id="S5.SS4.p3.1.m1.1.1.2.cmml" xref="S5.SS4.p3.1.m1.1.1.2">64</cn><cn type="integer" id="S5.SS4.p3.1.m1.1.1.3.cmml" xref="S5.SS4.p3.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">64\times 64</annotation></semantics></math> images, performance continues to improve as the amount of generated data increases up to nine times the amount of real data, to a total dataset size of 12M images.
Performance with higher resolution images, however, does not continue to improve with similarly large amounts of generative data augmentation.
Table <a href="#S5.T4" title="Table 4 ‚Ä£ 5.4 Merging Real and Synthetic Data at Scale ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports performance as the amount of generated data increased over the same range, up to <math id="S5.SS4.p3.2.m2.1" class="ltx_math_unparsed" alttext="9\times" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mrow id="S5.SS4.p3.2.m2.1b"><mn id="S5.SS4.p3.2.m2.1.1">9</mn><mo lspace="0.222em" id="S5.SS4.p3.2.m2.1.2">√ó</mo></mrow><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">9\times</annotation></semantics></math> the amount of real data, at resolutions <math id="S5.SS4.p3.3.m3.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S5.SS4.p3.3.m3.1a"><mrow id="S5.SS4.p3.3.m3.1.1" xref="S5.SS4.p3.3.m3.1.1.cmml"><mn id="S5.SS4.p3.3.m3.1.1.2" xref="S5.SS4.p3.3.m3.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS4.p3.3.m3.1.1.1" xref="S5.SS4.p3.3.m3.1.1.1.cmml">√ó</mo><mn id="S5.SS4.p3.3.m3.1.1.3" xref="S5.SS4.p3.3.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.3.m3.1b"><apply id="S5.SS4.p3.3.m3.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1"><times id="S5.SS4.p3.3.m3.1.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1.1"></times><cn type="integer" id="S5.SS4.p3.3.m3.1.1.2.cmml" xref="S5.SS4.p3.3.m3.1.1.2">256</cn><cn type="integer" id="S5.SS4.p3.3.m3.1.1.3.cmml" xref="S5.SS4.p3.3.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.3.m3.1c">256\!\times\!256</annotation></semantics></math> and <math id="S5.SS4.p3.4.m4.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S5.SS4.p3.4.m4.1a"><mrow id="S5.SS4.p3.4.m4.1.1" xref="S5.SS4.p3.4.m4.1.1.cmml"><mn id="S5.SS4.p3.4.m4.1.1.2" xref="S5.SS4.p3.4.m4.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS4.p3.4.m4.1.1.1" xref="S5.SS4.p3.4.m4.1.1.1.cmml">√ó</mo><mn id="S5.SS4.p3.4.m4.1.1.3" xref="S5.SS4.p3.4.m4.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.4.m4.1b"><apply id="S5.SS4.p3.4.m4.1.1.cmml" xref="S5.SS4.p3.4.m4.1.1"><times id="S5.SS4.p3.4.m4.1.1.1.cmml" xref="S5.SS4.p3.4.m4.1.1.1"></times><cn type="integer" id="S5.SS4.p3.4.m4.1.1.2.cmml" xref="S5.SS4.p3.4.m4.1.1.2">1024</cn><cn type="integer" id="S5.SS4.p3.4.m4.1.1.3.cmml" xref="S5.SS4.p3.4.m4.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.4.m4.1c">1024\!\times\!1024</annotation></semantics></math>.
The performance boost remains significant with fine-tuned diffusion models for synthetic data up to a factor of 4 or
5 times the size of the real ImageNet training set, a significant improvement over results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.4" class="ltx_p">This paper asks to what extent generative data augmentation is effective with current diffusion models.
We do so in the context of ImageNet classification, a challenging domain as it is extensively explored with highly tuned architectures and training recipes.
Here we show that large-scale text-to-image diffusion models can be fine-tuned to produce class-conditional models with SOTA FID (1.76 at <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><times id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">256\!\times\!256</annotation></semantics></math> resolution) and Inception Score (239 at <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S6.p1.2.m2.1a"><mrow id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mn id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p1.2.m2.1.1.1" xref="S6.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S6.p1.2.m2.1.1.3" xref="S6.p1.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><times id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1.1"></times><cn type="integer" id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">256</cn><cn type="integer" id="S6.p1.2.m2.1.1.3.cmml" xref="S6.p1.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">256\!\times\!256</annotation></semantics></math>).
The resulting generative model also yields a new SOTA in Classification Accuracy Scores (64.96 for <math id="S6.p1.3.m3.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S6.p1.3.m3.1a"><mrow id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml"><mn id="S6.p1.3.m3.1.1.2" xref="S6.p1.3.m3.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p1.3.m3.1.1.1" xref="S6.p1.3.m3.1.1.1.cmml">√ó</mo><mn id="S6.p1.3.m3.1.1.3" xref="S6.p1.3.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><apply id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1"><times id="S6.p1.3.m3.1.1.1.cmml" xref="S6.p1.3.m3.1.1.1"></times><cn type="integer" id="S6.p1.3.m3.1.1.2.cmml" xref="S6.p1.3.m3.1.1.2">256</cn><cn type="integer" id="S6.p1.3.m3.1.1.3.cmml" xref="S6.p1.3.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">256\!\times\!256</annotation></semantics></math> models, improving to 69.24 for <math id="S6.p1.4.m4.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S6.p1.4.m4.1a"><mrow id="S6.p1.4.m4.1.1" xref="S6.p1.4.m4.1.1.cmml"><mn id="S6.p1.4.m4.1.1.2" xref="S6.p1.4.m4.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p1.4.m4.1.1.1" xref="S6.p1.4.m4.1.1.1.cmml">√ó</mo><mn id="S6.p1.4.m4.1.1.3" xref="S6.p1.4.m4.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.4.m4.1b"><apply id="S6.p1.4.m4.1.1.cmml" xref="S6.p1.4.m4.1.1"><times id="S6.p1.4.m4.1.1.1.cmml" xref="S6.p1.4.m4.1.1.1"></times><cn type="integer" id="S6.p1.4.m4.1.1.2.cmml" xref="S6.p1.4.m4.1.1.2">1024</cn><cn type="integer" id="S6.p1.4.m4.1.1.3.cmml" xref="S6.p1.4.m4.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.m4.1c">1024\!\times\!1024</annotation></semantics></math> generated samples).
And we have shown improvements to ImageNet classification accuracy
extend to large amounts of generated data, across a range of ResNet and Transformer-based models.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.6" class="ltx_p">While these results are encouraging, many questions remain. One concerns the boost in CAS at resolution <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S6.p2.1.m1.1a"><mrow id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml"><mn id="S6.p2.1.m1.1.1.2" xref="S6.p2.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p2.1.m1.1.1.1" xref="S6.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S6.p2.1.m1.1.1.3" xref="S6.p2.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><apply id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"><times id="S6.p2.1.m1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1"></times><cn type="integer" id="S6.p2.1.m1.1.1.2.cmml" xref="S6.p2.1.m1.1.1.2">1024</cn><cn type="integer" id="S6.p2.1.m1.1.1.3.cmml" xref="S6.p2.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">1024\!\times\!1024</annotation></semantics></math>, suggesting that the larger images capture more useful image structure than those at <math id="S6.p2.2.m2.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S6.p2.2.m2.1a"><mrow id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml"><mn id="S6.p2.2.m2.1.1.2" xref="S6.p2.2.m2.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p2.2.m2.1.1.1" xref="S6.p2.2.m2.1.1.1.cmml">√ó</mo><mn id="S6.p2.2.m2.1.1.3" xref="S6.p2.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><apply id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1"><times id="S6.p2.2.m2.1.1.1.cmml" xref="S6.p2.2.m2.1.1.1"></times><cn type="integer" id="S6.p2.2.m2.1.1.2.cmml" xref="S6.p2.2.m2.1.1.2">256</cn><cn type="integer" id="S6.p2.2.m2.1.1.3.cmml" xref="S6.p2.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">256\!\times\!256</annotation></semantics></math>, even though the <math id="S6.p2.3.m3.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S6.p2.3.m3.1a"><mrow id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml"><mn id="S6.p2.3.m3.1.1.2" xref="S6.p2.3.m3.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p2.3.m3.1.1.1" xref="S6.p2.3.m3.1.1.1.cmml">√ó</mo><mn id="S6.p2.3.m3.1.1.3" xref="S6.p2.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><apply id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1"><times id="S6.p2.3.m3.1.1.1.cmml" xref="S6.p2.3.m3.1.1.1"></times><cn type="integer" id="S6.p2.3.m3.1.1.2.cmml" xref="S6.p2.3.m3.1.1.2">1024</cn><cn type="integer" id="S6.p2.3.m3.1.1.3.cmml" xref="S6.p2.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">1024\!\times\!1024</annotation></semantics></math> images are downsampled to <math id="S6.p2.4.m4.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S6.p2.4.m4.1a"><mrow id="S6.p2.4.m4.1.1" xref="S6.p2.4.m4.1.1.cmml"><mn id="S6.p2.4.m4.1.1.2" xref="S6.p2.4.m4.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p2.4.m4.1.1.1" xref="S6.p2.4.m4.1.1.1.cmml">√ó</mo><mn id="S6.p2.4.m4.1.1.3" xref="S6.p2.4.m4.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.4.m4.1b"><apply id="S6.p2.4.m4.1.1.cmml" xref="S6.p2.4.m4.1.1"><times id="S6.p2.4.m4.1.1.1.cmml" xref="S6.p2.4.m4.1.1.1"></times><cn type="integer" id="S6.p2.4.m4.1.1.2.cmml" xref="S6.p2.4.m4.1.1.2">256</cn><cn type="integer" id="S6.p2.4.m4.1.1.3.cmml" xref="S6.p2.4.m4.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.4.m4.1c">256\!\times\!256</annotation></semantics></math> before being center-cropped to <math id="S6.p2.5.m5.1" class="ltx_Math" alttext="224\!\times\!224" display="inline"><semantics id="S6.p2.5.m5.1a"><mrow id="S6.p2.5.m5.1.1" xref="S6.p2.5.m5.1.1.cmml"><mn id="S6.p2.5.m5.1.1.2" xref="S6.p2.5.m5.1.1.2.cmml">224</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p2.5.m5.1.1.1" xref="S6.p2.5.m5.1.1.1.cmml">√ó</mo><mn id="S6.p2.5.m5.1.1.3" xref="S6.p2.5.m5.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.5.m5.1b"><apply id="S6.p2.5.m5.1.1.cmml" xref="S6.p2.5.m5.1.1"><times id="S6.p2.5.m5.1.1.1.cmml" xref="S6.p2.5.m5.1.1.1"></times><cn type="integer" id="S6.p2.5.m5.1.1.2.cmml" xref="S6.p2.5.m5.1.1.2">224</cn><cn type="integer" id="S6.p2.5.m5.1.1.3.cmml" xref="S6.p2.5.m5.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.5.m5.1c">224\!\times\!224</annotation></semantics></math> for input to ResNet-50.
Another concerns the sustained gains in classification accuracy with large amounts of synthetic data at <math id="S6.p2.6.m6.1" class="ltx_Math" alttext="64\!\times\!64" display="inline"><semantics id="S6.p2.6.m6.1a"><mrow id="S6.p2.6.m6.1.1" xref="S6.p2.6.m6.1.1.cmml"><mn id="S6.p2.6.m6.1.1.2" xref="S6.p2.6.m6.1.1.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S6.p2.6.m6.1.1.1" xref="S6.p2.6.m6.1.1.1.cmml">√ó</mo><mn id="S6.p2.6.m6.1.1.3" xref="S6.p2.6.m6.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.6.m6.1b"><apply id="S6.p2.6.m6.1.1.cmml" xref="S6.p2.6.m6.1.1"><times id="S6.p2.6.m6.1.1.1.cmml" xref="S6.p2.6.m6.1.1.1"></times><cn type="integer" id="S6.p2.6.m6.1.1.2.cmml" xref="S6.p2.6.m6.1.1.2">64</cn><cn type="integer" id="S6.p2.6.m6.1.1.3.cmml" xref="S6.p2.6.m6.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.6.m6.1c">64\!\times\!64</annotation></semantics></math> (Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.4 Merging Real and Synthetic Data at Scale ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>); there is less information at low resolutions for training, and hence a greater opportunity for augmentation with synthetic images.
At high resolutions (Tab.¬†<a href="#S5.T4" title="Table 4 ‚Ä£ 5.4 Merging Real and Synthetic Data at Scale ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) performance drops for synthetic datasets larger than 1M images, which may indicate bias in the generative model, and the need for more sophisticated training methods with synthetic data. These issues remain topics of on-going research.</p>
</div>
<section id="S6.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Acknowledgments</h3>

<div id="S6.SSx1.p1" class="ltx_para">
<p id="S6.SSx1.p1.1" class="ltx_p">We thank Jason Baldridge and Ting Chen for their valuable feedback. We also extend thanks to William Chan, Saurabh Saxena, and Lala Li for helpful discussions, feedback, and their support with the Imagen code.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten
Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras,
and Ming-Yu Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">ediff-i: Text-to-image diffusion models with an ensemble of expert
denoisers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">preprint arxiv.2211.01324,</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Hritik Bansal and Aditya Grover.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Leaving reality to imagination: Robust classification via generated
datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.02503</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem
Babenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Label-efficient semantic segmentation with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.03126</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Irwan Bello, William Fedus, Xianzhi Du, Ekin¬†Dogus Cubuk, Aravind Srinivas,
Tsung-Yi Lin, Jonathon Shlens, and Barret Zoph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Revisiting resnets: Improved training and scaling strategies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">,
34:22614‚Äì22627, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Better plain vit baselines for imagenet-1k.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.01580</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Andrew Brock, Jeff Donahue, and Karen Simonyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Large scale GAN training for high fidelity natural image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Cristian Bucilu«é, Rich Caruana, and Alexandru Niculescu-Mizil.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Model compression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 535‚Äì541, 2006.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Nanxin Chen, Yu Zhang, Heiga Zen, Ron¬†J Weiss, Mohammad Norouzi, and William
Chan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Wavegrad: Estimating gradients for waveform generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.00713</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Ting Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">On the importance of noise scheduling for diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.10972</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yuhua Chen, Wen Li, Xiaoran Chen, and Luc¬†Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Learning semantic segmentation from synthetic data: A geometrically
guided input-output adaptation approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 1841‚Äì1850, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Celso¬†M de Melo, Antonio Torralba, Leonidas Guibas, James DiCarlo, Rama
Chellappa, and Jessica Hodgins.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Next-generation deep learning based on simulators and synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Trends in cognitive sciences</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Prafulla Dhariwal and Alexander Nichol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Diffusion models beat GANs on image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">,
34:8780‚Äì8794, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
Vladimir Golkov, Patrick Van Der¬†Smagt, Daniel Cremers, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Flownet: Learning optical flow with convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 2758‚Äì2766, 2015.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Carla: An open urban driving simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on robot learning</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 1‚Äì16. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, J De¬†Freitas, J Kubilius, A
Bhandwaldar, N Haber, M Sano, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Threedworld: A platform for interactive multi-modal physical
simulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg,
Dan¬†Andrei Calian, and Timothy¬†A Mann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Improving robustness using generated data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">,
34:4218‚Äì4233, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel
Duckworth, David¬†J Fleet, Dan Gnanapragasam, Florian Golemo, Charles
Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji,
Hsueh-Ti¬†(Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz
Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S.¬†M.
Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora,
Ziyu Wang, Tianhao Wu, Kwang¬†Moo Yi, Fangcheng Zhong, and Andrea
Tagliasacchi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Kubric: A scalable dataset generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Xi Guo, Wei Wu, Dongliang Wang, Jing Su, Haisheng Su, Weihao Gan, Jian Huang,
and Qin Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Learning video representations of human motion from synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 20197‚Äì20207, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 770‚Äì778, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
Bai, and Xiaojuan Qi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Is synthetic data from generative models ready for image recognition?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.07574</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Distilling the knowledge in a neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1503.02531</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey
Gritsenko, Diederik¬†P Kingma, Ben Poole, Mohammad Norouzi, David¬†J Fleet,
et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Imagen video: High definition video generation with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.02303</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
33:6840‚Äì6851, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho, Chitwan Saharia, William Chan, David¬†J Fleet, Mohammad Norouzi,
and Tim Salimans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Cascaded diffusion models for high fidelity image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">J. Mach. Learn. Res.</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 23(47):1‚Äì33, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho and Tim Salimans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Classifier-free diffusion guidance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.12598</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">simple diffusion: End-to-end diffusion for high resolution images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.11093</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe,
Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Kinectfusion: real-time 3d reconstruction and interaction using a
moving depth camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 24th annual ACM symposium on User
interface software and technology</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 559‚Äì568, 2011.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Allan Jabri, David Fleet, and Ting Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Scalable adaptive computation for iterative generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.11972</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Elucidating the design space of diffusion-based generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Yo-whan Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">How Transferable are Video Representations Based on Synthetic
Data?</span><span id="bib.bib32.3.2" class="ltx_text" style="font-size:90%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">PhD thesis, Massachusetts Institute of Technology, 2022.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Diederik¬†P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arxiv:1412.6980</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
Sylvain Gelly, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Big transfer (bit): General visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2020: 16th European Conference,
Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part V 16</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 491‚Äì507.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Diffwave: A versatile diffusion model for audio synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.09761</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Daiqing Li, Huan Ling, Seung¬†Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio
Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 21330‚Äì21340, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Semantic segmentation with generative models: Semi-supervised
learning and strong out-of-domain generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 8300‚Äì8311, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Jianxin Ma, Shuai Bai, and Chang Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Pretrained diffusion models for unified human motion synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.02837</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der¬†Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Exploring the limits of weakly supervised pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 181‚Äì196, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
Bob McGrew, Ilya Sutskever, and Mark Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Glide: Towards photorealistic image generation and editing with
text-guided diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.10741</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Alexander¬†Quinn Nichol and Prafulla Dhariwal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Improved denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages
8162‚Äì8171. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
William Peebles, Jun-Yan Zhu, Richard Zhang, Antonio Torralba, Alexei¬†A Efros,
and Eli Shechtman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Gan-supervised dense visual alignment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 13470‚Äì13481, 2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong¬†Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages
8748‚Äì8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Hierarchical text-conditional image generation with clip latents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.06125</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Suman Ravuri and Oriol Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Classification accuracy score for conditional generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Ali Razavi, Aaron Van¬†den Oord, and Oriol Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Generating diverse high-fidelity images with vq-vae-2.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn
Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, pages 10684‚Äì10695, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Imagenet large scale visual recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">, 115:211‚Äì252, 2015.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim
Salimans, David Fleet, and Mohammad Norouzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Palette: Image-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM SIGGRAPH 2022 Conference Proceedings</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 1‚Äì10, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
Denton, Seyed Kamyar¬†Seyed Ghasemipour, Burcu¬†Karagol Ayan, S¬†Sara Mahdavi,
Rapha¬†Gontijo Lopes, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Photorealistic text-to-image diffusion models with deep language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.11487</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David¬†J Fleet, and
Mohammad Norouzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Image super-resolution via iterative refinement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
Xi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Improved techniques for training gans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 29, 2016.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Tim Salimans and Jonathan Ho.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Progressive distillation for fast sampling of diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.00512</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser¬†Nam Lim, and Rama
Chellappa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Learning from synthetic data: Addressing domain shift for semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages 3752‚Äì3761, 2018.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Shibani Santurkar, Ludwig Schmidt, and Aleksander Madry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">A classification-based study of covariate shift in gan distributions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages
4480‚Äì4489. PMLR, 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Mert¬†Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Fake it till you make it: Learning (s) from a synthetic imagenet
clone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.08420</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Noam Shazeer and Mitchell Stern.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Adafactor: Adaptive learning rates with sublinear memory cost.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint, arxiv:1804.04235</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan
Hu, Harry Yang, Oron Ashual, Oran Gafni, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Make-a-video: Text-to-video generation without text-video data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.14792</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Deep unsupervised learning using nonequilibrium thermodynamics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages
2256‚Äì2265. PMLR, 2015.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Jiaming Song, Chenlin Meng, and Stefano Ermon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Denoising diffusion implicit models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Confernece on Learning Representations</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Yang Song, Jascha Sohl-Dickstein, Diederik¬†P Kingma, Abhishek Kumar, Stefano
Ermon, and Ben Poole.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Score-based generative modeling through stochastic differential
equations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2011.13456</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Revisiting unreasonable effectiveness of data in deep learning era.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, pages 843‚Äì852, 2017.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael Krainin,
Huiwen Chang, Ramin Zabih, William¬†T Freeman, and Ce Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Autoflow: Learning a better training set for optical flow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, pages 10093‚Äì10102, 2021.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
Sablayrolles, and Herv√© J√©gou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Training data-efficient image transformers &amp; distillation through
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, pages
10347‚Äì10357. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Effective data augmentation with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.07944</span><span id="bib.bib65.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Nontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Repurposing gans for one-shot semantic part segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, pages 4475‚Äì4485, 2021.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael¬†J Black, Ivan
Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, pages 109‚Äì117, 2017.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo,
Han Zhang, Mohammad¬†Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru
Erhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Phenaki: Variable length video generation from open domain textual
description.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.02399</span><span id="bib.bib68.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy,
Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David¬†J Fleet, Radu Soricut,
et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Imagen editor and editbench: Advancing and evaluating text-guided
image inpainting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.06909</span><span id="bib.bib69.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, and Bolei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Generative hierarchical features from synthesizing images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, pages 4432‚Äì4442, 2021.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">LR-GAN: Layered recursive generative adversarial networks for
image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib71.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Scaling vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib72.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib72.5.3" class="ltx_text" style="font-size:90%;">, pages 12104‚Äì12113, 2022.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Structured3d: A large photo-realistic dataset for structured 3d
modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib73.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2020: 16th European Conference,
Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part IX 16</span><span id="bib.bib73.5.3" class="ltx_text" style="font-size:90%;">, pages 519‚Äì535.
Springer, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="id5.p1" class="ltx_para ltx_noindent ltx_align_center">
<p id="id5.p1.1" class="ltx_p"><span id="id5.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:173%;">Appendix
<br class="ltx_break"></span></p>
</div>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A.1 </span>Hyper-parameters for Imagen fine-tuning and sample generation.</h2>

<div id="S1a.p1" class="ltx_para">
<p id="S1a.p1.1" class="ltx_p">The quality, diversity, and speed of text-conditioned diffusion model sampling are strongly affected by multiple hyper-parameters.
These include the number of diffusion steps, where larger numbers of diffusion steps are often associated with higher quality images and lower FID.
Another hyper-parameter is the amount of noise-conditioning augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, which adds Gaussian noise to the output of one stage of the Imagen cascade at training time, prior to it being input to the subsequent super-resolution stage.
We considered noise levels between 0 and 0.5
(with images in the range [0,1]), where adding more noise during training degrades more fine-scale structure, thereby forcing the subsequent super-resolution stage to be more robust to variability in the images generated from the previous stage.</p>
</div>
<div id="S1a.p2" class="ltx_para">
<p id="S1a.p2.3" class="ltx_p">During sampling, we use classifier-free guidance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, but with smaller guidance weights than Imagen, favoring diversity over image fidelity to some degree.
With smaller guidance weights, one does not require dynamic thresholding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> during inference; instead we opt for a static threshold to clip large pixel values at each step of denoising.
Ho et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> identify upper and lower bounds on the predictive variance, <math id="S1a.p2.1.m1.2" class="ltx_Math" alttext="\Sigma_{\theta}(x_{t},t)" display="inline"><semantics id="S1a.p2.1.m1.2a"><mrow id="S1a.p2.1.m1.2.2" xref="S1a.p2.1.m1.2.2.cmml"><msub id="S1a.p2.1.m1.2.2.3" xref="S1a.p2.1.m1.2.2.3.cmml"><mi mathvariant="normal" id="S1a.p2.1.m1.2.2.3.2" xref="S1a.p2.1.m1.2.2.3.2.cmml">Œ£</mi><mi id="S1a.p2.1.m1.2.2.3.3" xref="S1a.p2.1.m1.2.2.3.3.cmml">Œ∏</mi></msub><mo lspace="0em" rspace="0em" id="S1a.p2.1.m1.2.2.2" xref="S1a.p2.1.m1.2.2.2.cmml">‚Äã</mo><mrow id="S1a.p2.1.m1.2.2.1.1" xref="S1a.p2.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S1a.p2.1.m1.2.2.1.1.2" xref="S1a.p2.1.m1.2.2.1.2.cmml">(</mo><msub id="S1a.p2.1.m1.2.2.1.1.1" xref="S1a.p2.1.m1.2.2.1.1.1.cmml"><mi id="S1a.p2.1.m1.2.2.1.1.1.2" xref="S1a.p2.1.m1.2.2.1.1.1.2.cmml">x</mi><mi id="S1a.p2.1.m1.2.2.1.1.1.3" xref="S1a.p2.1.m1.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S1a.p2.1.m1.2.2.1.1.3" xref="S1a.p2.1.m1.2.2.1.2.cmml">,</mo><mi id="S1a.p2.1.m1.1.1" xref="S1a.p2.1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S1a.p2.1.m1.2.2.1.1.4" xref="S1a.p2.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1a.p2.1.m1.2b"><apply id="S1a.p2.1.m1.2.2.cmml" xref="S1a.p2.1.m1.2.2"><times id="S1a.p2.1.m1.2.2.2.cmml" xref="S1a.p2.1.m1.2.2.2"></times><apply id="S1a.p2.1.m1.2.2.3.cmml" xref="S1a.p2.1.m1.2.2.3"><csymbol cd="ambiguous" id="S1a.p2.1.m1.2.2.3.1.cmml" xref="S1a.p2.1.m1.2.2.3">subscript</csymbol><ci id="S1a.p2.1.m1.2.2.3.2.cmml" xref="S1a.p2.1.m1.2.2.3.2">Œ£</ci><ci id="S1a.p2.1.m1.2.2.3.3.cmml" xref="S1a.p2.1.m1.2.2.3.3">ùúÉ</ci></apply><interval closure="open" id="S1a.p2.1.m1.2.2.1.2.cmml" xref="S1a.p2.1.m1.2.2.1.1"><apply id="S1a.p2.1.m1.2.2.1.1.1.cmml" xref="S1a.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S1a.p2.1.m1.2.2.1.1.1.1.cmml" xref="S1a.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S1a.p2.1.m1.2.2.1.1.1.2.cmml" xref="S1a.p2.1.m1.2.2.1.1.1.2">ùë•</ci><ci id="S1a.p2.1.m1.2.2.1.1.1.3.cmml" xref="S1a.p2.1.m1.2.2.1.1.1.3">ùë°</ci></apply><ci id="S1a.p2.1.m1.1.1.cmml" xref="S1a.p2.1.m1.1.1">ùë°</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S1a.p2.1.m1.2c">\Sigma_{\theta}(x_{t},t)</annotation></semantics></math>, used for sampling at each denoising step.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (Eq.¬†15) we use a linear (convex) combination of the log upper and lower bounds, the mixing parameter for which is referred to as the logvar parameter.
Figures <a href="#S3.F3" title="Figure 3 ‚Ä£ 3 Background ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and
<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Sampling Parameters ‚Ä£ 4 Generative Model Training and Sampling ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show the dependence of FID, IS and Classification Accuracy Scores on guidance weight and logvar mixing coefficient for the base model at resolution <math id="S1a.p2.2.m2.1" class="ltx_Math" alttext="64\!\times\!64" display="inline"><semantics id="S1a.p2.2.m2.1a"><mrow id="S1a.p2.2.m2.1.1" xref="S1a.p2.2.m2.1.1.cmml"><mn id="S1a.p2.2.m2.1.1.2" xref="S1a.p2.2.m2.1.1.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S1a.p2.2.m2.1.1.1" xref="S1a.p2.2.m2.1.1.1.cmml">√ó</mo><mn id="S1a.p2.2.m2.1.1.3" xref="S1a.p2.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S1a.p2.2.m2.1b"><apply id="S1a.p2.2.m2.1.1.cmml" xref="S1a.p2.2.m2.1.1"><times id="S1a.p2.2.m2.1.1.1.cmml" xref="S1a.p2.2.m2.1.1.1"></times><cn type="integer" id="S1a.p2.2.m2.1.1.2.cmml" xref="S1a.p2.2.m2.1.1.2">64</cn><cn type="integer" id="S1a.p2.2.m2.1.1.3.cmml" xref="S1a.p2.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1a.p2.2.m2.1c">64\!\times\!64</annotation></semantics></math> and the <math id="S1a.p2.3.m3.1" class="ltx_Math" alttext="64\!\rightarrow\!256" display="inline"><semantics id="S1a.p2.3.m3.1a"><mrow id="S1a.p2.3.m3.1.1" xref="S1a.p2.3.m3.1.1.cmml"><mn id="S1a.p2.3.m3.1.1.2" xref="S1a.p2.3.m3.1.1.2.cmml">64</mn><mo lspace="0.108em" rspace="0.108em" stretchy="false" id="S1a.p2.3.m3.1.1.1" xref="S1a.p2.3.m3.1.1.1.cmml">‚Üí</mo><mn id="S1a.p2.3.m3.1.1.3" xref="S1a.p2.3.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S1a.p2.3.m3.1b"><apply id="S1a.p2.3.m3.1.1.cmml" xref="S1a.p2.3.m3.1.1"><ci id="S1a.p2.3.m3.1.1.1.cmml" xref="S1a.p2.3.m3.1.1.1">‚Üí</ci><cn type="integer" id="S1a.p2.3.m3.1.1.2.cmml" xref="S1a.p2.3.m3.1.1.2">64</cn><cn type="integer" id="S1a.p2.3.m3.1.1.3.cmml" xref="S1a.p2.3.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1a.p2.3.m3.1c">64\!\rightarrow\!256</annotation></semantics></math> super-resolution model.
These were used to help choose model hyyper-parameters for large-scale sample generation.</p>
</div>
<div id="S1a.p3" class="ltx_para">
<p id="S1a.p3.1" class="ltx_p">Below are further results relate to hyperparameter selection and its impact on model metrics.</p>
</div>
<figure id="S1.F1a" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x12.png" id="S1.F1a.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="169" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x13.png" id="S1.F1a.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="169" height="125" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1a.8.2.1" class="ltx_text" style="font-size:90%;">Figure A.1</span>: </span><span id="S1.F1a.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Left<span id="S1.F1a.4.1.1" class="ltx_text ltx_font_medium">: CAS vs IS Pareto curves for train set resolution of <math id="S1.F1a.4.1.1.m1.1" class="ltx_Math" alttext="64\!\times\!64" display="inline"><semantics id="S1.F1a.4.1.1.m1.1b"><mrow id="S1.F1a.4.1.1.m1.1.1" xref="S1.F1a.4.1.1.m1.1.1.cmml"><mn id="S1.F1a.4.1.1.m1.1.1.2" xref="S1.F1a.4.1.1.m1.1.1.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S1.F1a.4.1.1.m1.1.1.1" xref="S1.F1a.4.1.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.F1a.4.1.1.m1.1.1.3" xref="S1.F1a.4.1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1a.4.1.1.m1.1c"><apply id="S1.F1a.4.1.1.m1.1.1.cmml" xref="S1.F1a.4.1.1.m1.1.1"><times id="S1.F1a.4.1.1.m1.1.1.1.cmml" xref="S1.F1a.4.1.1.m1.1.1.1"></times><cn type="integer" id="S1.F1a.4.1.1.m1.1.1.2.cmml" xref="S1.F1a.4.1.1.m1.1.1.2">64</cn><cn type="integer" id="S1.F1a.4.1.1.m1.1.1.3.cmml" xref="S1.F1a.4.1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1a.4.1.1.m1.1d">64\!\times\!64</annotation></semantics></math> showing the impact of guidance weights. </span>Right<span id="S1.F1a.4.1.2" class="ltx_text ltx_font_medium">: Train set FID vs IS Pareto curves for resolution of 64x64 showing the impact of guidance weights.</span></span></figcaption>
</figure>
<figure id="S1.F2a" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x14.png" id="S1.F2a.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="169" height="118" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x15.png" id="S1.F2a.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="168" height="117" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2a.8.2.1" class="ltx_text" style="font-size:90%;">Figure A.2</span>: </span><span id="S1.F2a.4.1" class="ltx_text" style="font-size:90%;">Sampling refinement for <math id="S1.F2a.4.1.m1.1" class="ltx_Math" alttext="64\!\times\!64" display="inline"><semantics id="S1.F2a.4.1.m1.1b"><mrow id="S1.F2a.4.1.m1.1.1" xref="S1.F2a.4.1.m1.1.1.cmml"><mn id="S1.F2a.4.1.m1.1.1.2" xref="S1.F2a.4.1.m1.1.1.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S1.F2a.4.1.m1.1.1.1" xref="S1.F2a.4.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.F2a.4.1.m1.1.1.3" xref="S1.F2a.4.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F2a.4.1.m1.1c"><apply id="S1.F2a.4.1.m1.1.1.cmml" xref="S1.F2a.4.1.m1.1.1"><times id="S1.F2a.4.1.m1.1.1.1.cmml" xref="S1.F2a.4.1.m1.1.1.1"></times><cn type="integer" id="S1.F2a.4.1.m1.1.1.2.cmml" xref="S1.F2a.4.1.m1.1.1.2">64</cn><cn type="integer" id="S1.F2a.4.1.m1.1.1.3.cmml" xref="S1.F2a.4.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2a.4.1.m1.1d">64\!\times\!64</annotation></semantics></math> base model. <span id="S1.F2a.4.1.1" class="ltx_text ltx_font_bold">Left</span>: Validation set FID vs.¬†guidance weights for different values of log-variance. <span id="S1.F2a.4.1.2" class="ltx_text ltx_font_bold">Right</span>: Validation set FID vs.¬†Inception score (IS) when increasing guidance from 1.0 to 5.0. </span></figcaption>
</figure>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2304.08466/assets/x16.png" id="S1.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="502" height="241" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.5.2.1" class="ltx_text" style="font-size:90%;">Figure A.3</span>: </span><span id="S1.F3.3.1" class="ltx_text" style="font-size:90%;">Top-1 and Top-5 classification accuracy score (CAS) vs train FID Pareto curves (sweeping over guidance weight) showing the impact of conditioning noise augmentation at <math id="S1.F3.3.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S1.F3.3.1.m1.1b"><mrow id="S1.F3.3.1.m1.1.1" xref="S1.F3.3.1.m1.1.1.cmml"><mn id="S1.F3.3.1.m1.1.1.2" xref="S1.F3.3.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S1.F3.3.1.m1.1.1.1" xref="S1.F3.3.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.F3.3.1.m1.1.1.3" xref="S1.F3.3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F3.3.1.m1.1c"><apply id="S1.F3.3.1.m1.1.1.cmml" xref="S1.F3.3.1.m1.1.1"><times id="S1.F3.3.1.m1.1.1.1.cmml" xref="S1.F3.3.1.m1.1.1.1"></times><cn type="integer" id="S1.F3.3.1.m1.1.1.2.cmml" xref="S1.F3.3.1.m1.1.1.2">256</cn><cn type="integer" id="S1.F3.3.1.m1.1.1.3.cmml" xref="S1.F3.3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F3.3.1.m1.1d">256\!\times\!256</annotation></semantics></math> when sampling with different number of steps. As indicated by number overlaid on each trend line, guidance weight is decreasing from 30 to 1.</span></figcaption>
</figure>
<figure id="S1.F4" class="ltx_figure"><img src="/html/2304.08466/assets/x17.png" id="S1.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="502" height="241" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F4.5.2.1" class="ltx_text" style="font-size:90%;">Figure A.4</span>: </span><span id="S1.F4.3.1" class="ltx_text" style="font-size:90%;">Top-1 and Top-5 classification accuracy score (CAS) vs train FID Pareto curves (sweeping over guidance weight) showing the impact of conditioning noise augmentation at <math id="S1.F4.3.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S1.F4.3.1.m1.1b"><mrow id="S1.F4.3.1.m1.1.1" xref="S1.F4.3.1.m1.1.1.cmml"><mn id="S1.F4.3.1.m1.1.1.2" xref="S1.F4.3.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S1.F4.3.1.m1.1.1.1" xref="S1.F4.3.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.F4.3.1.m1.1.1.3" xref="S1.F4.3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F4.3.1.m1.1c"><apply id="S1.F4.3.1.m1.1.1.cmml" xref="S1.F4.3.1.m1.1.1"><times id="S1.F4.3.1.m1.1.1.1.cmml" xref="S1.F4.3.1.m1.1.1.1"></times><cn type="integer" id="S1.F4.3.1.m1.1.1.2.cmml" xref="S1.F4.3.1.m1.1.1.2">256</cn><cn type="integer" id="S1.F4.3.1.m1.1.1.3.cmml" xref="S1.F4.3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F4.3.1.m1.1d">256\!\times\!256</annotation></semantics></math> when sampling with different number of steps at a fixed noise level. As indicated by number overlaid on each trend line guidance weight is decreasing from 30 to 1. At highest noise level (0.5) lowering number sampling step and decreasing guidance can lead to a better joint FID and CAS values. At lowest noise level (0.0) this effect is subtle and increasing sampling steps and lower guidance weight can help to improve CAS. </span></figcaption>
</figure>
<figure id="S1.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x18.png" id="S1.F5.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="169" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x19.png" id="S1.F5.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="169" height="120" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure A.5</span>: </span><span id="S1.F5.5.2" class="ltx_text" style="font-size:90%;">Fine-tuning of SR model helps to jointly improve classification accuracy as well as FID of the vanilla Imagen. </span></figcaption>
</figure>
<figure id="S1.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x20.png" id="S1.F6.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="168" height="118" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x21.png" id="S1.F6.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="168" height="120" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F6.8.2.1" class="ltx_text" style="font-size:90%;">Figure A.6</span>: </span><span id="S1.F6.4.1" class="ltx_text" style="font-size:90%;">Sampling refinement for <math id="S1.F6.4.1.m1.1" class="ltx_Math" alttext="1024\!\times\!2014" display="inline"><semantics id="S1.F6.4.1.m1.1b"><mrow id="S1.F6.4.1.m1.1.1" xref="S1.F6.4.1.m1.1.1.cmml"><mn id="S1.F6.4.1.m1.1.1.2" xref="S1.F6.4.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S1.F6.4.1.m1.1.1.1" xref="S1.F6.4.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.F6.4.1.m1.1.1.3" xref="S1.F6.4.1.m1.1.1.3.cmml">2014</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F6.4.1.m1.1c"><apply id="S1.F6.4.1.m1.1.1.cmml" xref="S1.F6.4.1.m1.1.1"><times id="S1.F6.4.1.m1.1.1.1.cmml" xref="S1.F6.4.1.m1.1.1.1"></times><cn type="integer" id="S1.F6.4.1.m1.1.1.2.cmml" xref="S1.F6.4.1.m1.1.1.2">1024</cn><cn type="integer" id="S1.F6.4.1.m1.1.1.3.cmml" xref="S1.F6.4.1.m1.1.1.3">2014</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F6.4.1.m1.1d">1024\!\times\!2014</annotation></semantics></math> super resolution model. <span id="S1.F6.4.1.1" class="ltx_text ltx_font_bold">Left</span>: CAS vs.¬†guidance weights under varying noise conditions. <span id="S1.F6.4.1.2" class="ltx_text ltx_font_bold">Right</span>: CAS vs.¬†Inception score (IS) when increasing guidance from 1.0 to 5.0 under varying noise conditions. </span></figcaption>
</figure>
<figure id="S1.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x22.png" id="S1.F7.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="185" height="127" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x23.png" id="S1.F7.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="185" height="127" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F7.6.2.1" class="ltx_text" style="font-size:90%;">Figure A.7</span>: </span><span id="S1.F7.4.1" class="ltx_text" style="font-size:90%;">Training set FID vs.¬†classification top-1 and top-5 accuracy Pareto curves under varying noise conditions when the guidance weight is set to 1.0 for resolution <math id="S1.F7.4.1.m1.1" class="ltx_Math" alttext="256\!\times\!256" display="inline"><semantics id="S1.F7.4.1.m1.1b"><mrow id="S1.F7.4.1.m1.1.1" xref="S1.F7.4.1.m1.1.1.cmml"><mn id="S1.F7.4.1.m1.1.1.2" xref="S1.F7.4.1.m1.1.1.2.cmml">256</mn><mo lspace="0.052em" rspace="0.052em" id="S1.F7.4.1.m1.1.1.1" xref="S1.F7.4.1.m1.1.1.1.cmml">√ó</mo><mn id="S1.F7.4.1.m1.1.1.3" xref="S1.F7.4.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F7.4.1.m1.1c"><apply id="S1.F7.4.1.m1.1.1.cmml" xref="S1.F7.4.1.m1.1.1"><times id="S1.F7.4.1.m1.1.1.1.cmml" xref="S1.F7.4.1.m1.1.1.1"></times><cn type="integer" id="S1.F7.4.1.m1.1.1.2.cmml" xref="S1.F7.4.1.m1.1.1.2">256</cn><cn type="integer" id="S1.F7.4.1.m1.1.1.3.cmml" xref="S1.F7.4.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F7.4.1.m1.1d">256\!\times\!256</annotation></semantics></math>. These curves depict the joint influence of the log-variance mixing coefficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and noise conditioning augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> on FID and CAS.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S2a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A.2 </span>Class Alignment of Imagen vs.¬†Fine-Tuned Imagen</h2>

<div id="S2a.p1" class="ltx_para">
<p id="S2a.p1.1" class="ltx_p">What follows are more samples to compare our fine-tuned model vs.¬†the Imagen model are provided in Figure¬†<a href="#S2.F8" title="Figure A.8 ‚Ä£ A.2 Class Alignment of Imagen vs. Fine-Tuned Imagen ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.8</span></a>,¬†<a href="#S2.F9" title="Figure A.9 ‚Ä£ A.2 Class Alignment of Imagen vs. Fine-Tuned Imagen ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.9</span></a>, and¬†<a href="#S2.F10" title="Figure A.10 ‚Ä£ A.2 Class Alignment of Imagen vs. Fine-Tuned Imagen ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.10</span></a>. In this comparison we sample our fine-tuned model using two strategies. First, we sample using the proposed vanilla Imagen hyper-parameters which use a guidance weight of 10 for the sampling of the base <math id="S2a.p1.1.m1.1" class="ltx_Math" alttext="64\!\times\!64" display="inline"><semantics id="S2a.p1.1.m1.1a"><mrow id="S2a.p1.1.m1.1.1" xref="S2a.p1.1.m1.1.1.cmml"><mn id="S2a.p1.1.m1.1.1.2" xref="S2a.p1.1.m1.1.1.2.cmml">64</mn><mo lspace="0.052em" rspace="0.052em" id="S2a.p1.1.m1.1.1.1" xref="S2a.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2a.p1.1.m1.1.1.3" xref="S2a.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2a.p1.1.m1.1b"><apply id="S2a.p1.1.m1.1.1.cmml" xref="S2a.p1.1.m1.1.1"><times id="S2a.p1.1.m1.1.1.1.cmml" xref="S2a.p1.1.m1.1.1.1"></times><cn type="integer" id="S2a.p1.1.m1.1.1.2.cmml" xref="S2a.p1.1.m1.1.1.2">64</cn><cn type="integer" id="S2a.p1.1.m1.1.1.3.cmml" xref="S2a.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2a.p1.1.m1.1c">64\!\times\!64</annotation></semantics></math> model and subsequent super-resolution (SR) models are sampled with guidance weights of 20 and 8, respectively. This is called the high guidance strategy in these figures. Second, we use the proposed sampling hyper-parameters as explained in the paper which includes sampling the based model with a guidance weight of 1.25 and the subsequent SR models with a guidance weight of 1.0. This is called the low guidance weight strategy in these figures.</p>
</div>
<figure id="S2.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x24.png" id="S2.F8.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="188" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x25.png" id="S2.F8.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="188" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x26.png" id="S2.F8.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="187" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F8.7.2.1" class="ltx_text" style="font-size:90%;">Figure A.8</span>: </span><span id="S2.F8.5.1" class="ltx_text" style="font-size:90%;">Example <math id="S2.F8.5.1.m1.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S2.F8.5.1.m1.1b"><mrow id="S2.F8.5.1.m1.1.1" xref="S2.F8.5.1.m1.1.1.cmml"><mn id="S2.F8.5.1.m1.1.1.2" xref="S2.F8.5.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S2.F8.5.1.m1.1.1.1" xref="S2.F8.5.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.F8.5.1.m1.1.1.3" xref="S2.F8.5.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F8.5.1.m1.1c"><apply id="S2.F8.5.1.m1.1.1.cmml" xref="S2.F8.5.1.m1.1.1"><times id="S2.F8.5.1.m1.1.1.1.cmml" xref="S2.F8.5.1.m1.1.1.1"></times><cn type="integer" id="S2.F8.5.1.m1.1.1.2.cmml" xref="S2.F8.5.1.m1.1.1.2">1024</cn><cn type="integer" id="S2.F8.5.1.m1.1.1.3.cmml" xref="S2.F8.5.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F8.5.1.m1.1d">1024\!\times\!1024</annotation></semantics></math> images from vanilla Imagen (first row) vs.¬†fine-tuned Imagen sampled with Imagen hyper-parameters (high guidance, second row) vs.¬†fine-tuned Imagen sampled with our proposed hyper-parameter (low guidance, third row). Fine-tuning and careful choice of sampling parameters help to improve the alignment of images with class labels, and also improve sample diversity. Sampling with higher guidance weight can improve photorealism, but lessens diversity.</span></figcaption>
</figure>
<figure id="S2.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x27.png" id="S2.F9.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="188" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x28.png" id="S2.F9.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="188" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x29.png" id="S2.F9.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="190" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F9.7.2.1" class="ltx_text" style="font-size:90%;">Figure A.9</span>: </span><span id="S2.F9.5.1" class="ltx_text" style="font-size:90%;">Example <math id="S2.F9.5.1.m1.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S2.F9.5.1.m1.1b"><mrow id="S2.F9.5.1.m1.1.1" xref="S2.F9.5.1.m1.1.1.cmml"><mn id="S2.F9.5.1.m1.1.1.2" xref="S2.F9.5.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S2.F9.5.1.m1.1.1.1" xref="S2.F9.5.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.F9.5.1.m1.1.1.3" xref="S2.F9.5.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F9.5.1.m1.1c"><apply id="S2.F9.5.1.m1.1.1.cmml" xref="S2.F9.5.1.m1.1.1"><times id="S2.F9.5.1.m1.1.1.1.cmml" xref="S2.F9.5.1.m1.1.1.1"></times><cn type="integer" id="S2.F9.5.1.m1.1.1.2.cmml" xref="S2.F9.5.1.m1.1.1.2">1024</cn><cn type="integer" id="S2.F9.5.1.m1.1.1.3.cmml" xref="S2.F9.5.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.5.1.m1.1d">1024\!\times\!1024</annotation></semantics></math> images from vanilla Imagen (first row) vs.¬†fine-tuned Imagen sampled with Imagen hyper-parameters (high guidance, second row) vs.¬†fine-tuned Imagen sampled with our proposed hyper-parameter (low guidance, third row). Fine-tuning and careful choice of sampling parameters help to improve the alignment of images with class labels, and also improve sample diversity. Sampling with higher guidance weight can improve photorealism, but lessens diversity.</span></figcaption>
</figure>
<figure id="S2.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x30.png" id="S2.F10.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="188" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2304.08466/assets/x31.png" id="S2.F10.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="428" height="188" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F10.6.2.1" class="ltx_text" style="font-size:90%;">Figure A.10</span>: </span><span id="S2.F10.4.1" class="ltx_text" style="font-size:90%;">Example <math id="S2.F10.4.1.m1.1" class="ltx_Math" alttext="1024\!\times\!1024" display="inline"><semantics id="S2.F10.4.1.m1.1b"><mrow id="S2.F10.4.1.m1.1.1" xref="S2.F10.4.1.m1.1.1.cmml"><mn id="S2.F10.4.1.m1.1.1.2" xref="S2.F10.4.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.052em" rspace="0.052em" id="S2.F10.4.1.m1.1.1.1" xref="S2.F10.4.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.F10.4.1.m1.1.1.3" xref="S2.F10.4.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F10.4.1.m1.1c"><apply id="S2.F10.4.1.m1.1.1.cmml" xref="S2.F10.4.1.m1.1.1"><times id="S2.F10.4.1.m1.1.1.1.cmml" xref="S2.F10.4.1.m1.1.1.1"></times><cn type="integer" id="S2.F10.4.1.m1.1.1.2.cmml" xref="S2.F10.4.1.m1.1.1.2">1024</cn><cn type="integer" id="S2.F10.4.1.m1.1.1.3.cmml" xref="S2.F10.4.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F10.4.1.m1.1d">1024\!\times\!1024</annotation></semantics></math> images from vanilla Imagen (first row) vs.¬†fine-tuned Imagen sampled with Imagen hyper-parameters (high guidance, second row) vs.¬†fine-tuned Imagen sampled with our proposed hyper-parameter (low guidance, third row). Fine-tuning and careful choice of sampling parameters help to improve the alignment of images with class labels, and also improve sample diversity. Sampling with higher guidance weight can improve photorealism, but lessens diversity.</span></figcaption>
</figure>
</section>
<section id="S3a" class="ltx_section ltx_pruned_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A.3 </span>High Resolution Random Samples from the ImageNet Model</h2>

<figure id="S3.F11" class="ltx_figure"><img src="/html/2304.08466/assets/x32.png" id="S3.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="518" height="258" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure A.11</span>: </span><span id="S3.F11.4.2" class="ltx_text" style="font-size:90%;"> Random samples at 1024√ó1024 resolution generated by our fine-tuned model. The classes are snail (113), panda (388), orange (950), badger (362), indigo bunting (14), steam locomotive (820), carved pumpkin (607), lion (291), loggerhead sea turtle (33),
golden retriever (207), tree frog (31), clownfish (393), dowitcher (142), lorikeet (90), school bus (779), macaw (88), marmot (336), green mamba (64).</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S4a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A.4 </span>Hyper-parameters and model selection for ImageNet classifiers.</h2>

<div id="S4a.p1" class="ltx_para">
<p id="S4a.p1.1" class="ltx_p">This section details all the hyper-parameters used in training our ResNet-based model for CAS calculation, as well as the other ResNet-based, ResNet-RS-based, and Transformer-based models, used to report classifier accuracy in Table <a href="#S5.T3" title="Table 3 ‚Ä£ 5.4 Merging Real and Synthetic Data at Scale ‚Ä£ 5 Results ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Table <a href="#S4.T1" title="Table A.1 ‚Ä£ A.4 Hyper-parameters and model selection for ImageNet classifiers. ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> and Table <a href="#S4.T2" title="Table A.2 ‚Ä£ A.4 Hyper-parameters and model selection for ImageNet classifiers. ‚Ä£ Synthetic Data from Diffusion Models Improves ImageNet Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a> summarize the hyper-parameters used to train the ConvNet architectures and vision transformer architectures, respectively.</p>
</div>
<div id="S4a.p2" class="ltx_para">
<p id="S4a.p2.1" class="ltx_p">For classification accuracy (CAS) calculation, as discussed before we follow the protocol suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Our CAS ResNet-50 classifier is trained using a single crop. To train the classifier, we employ an SGD momentum optimizer and run it for 90 epochs. The learning rate is scheduled to linearly increase from 0.0 to 0.4 for the first five epochs and then decrease by a factor of 10 at epochs 30, 60, and 80. For other ResNet-based classifiers we employ more advanced mechanisms such as using a cosine schedule instead of step-wise learning rate decay, larger batch size, random augmentation, dropout, and label smoothing to reach competitive performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. It is also important to emphasize that ResNet-RS achieved higher performance than ResNet models through a combination of enhanced scaling strategies, improved training methodologies, and the implementation of techniques like the Squeeze-Excitation module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. We follow the training strategy and hyper-parameter suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to train our ResNet-RS-based models.</p>
</div>
<div id="S4a.p3" class="ltx_para">
<p id="S4a.p3.1" class="ltx_p">For vision transformer architectures we mainly follow the recipe provided in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to train a competitive ViT-S/16 model and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> to train DeiT family models. In all cases we re-implemented and train all of our models from scratch using real only, real + generated data, and generated only data until convergence.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<td id="S4.T1.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S4.T1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">ResNet-50 (CAS)</span></td>
<td id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.2.1.1.3.1" class="ltx_text" style="font-size:90%;">ResNet-50</span></td>
<td id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.2.1.1.4.1" class="ltx_text" style="font-size:90%;">ResNet-101</span></td>
<td id="S4.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.2.1.1.5.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></td>
<td id="S4.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.2.1.1.6.1" class="ltx_text" style="font-size:90%;">ResNet-RS-50</span></td>
<td id="S4.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.2.1.1.7.1" class="ltx_text" style="font-size:90%;">ResNet-RS-101</span></td>
<td id="S4.T1.2.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S4.T1.2.1.1.8.1" class="ltx_text" style="font-size:90%;">ResNet-RS-152</span></td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Epochs</span></th>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.2.1" class="ltx_text" style="font-size:90%;">90</span></td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.3.1" class="ltx_text" style="font-size:90%;">130</span></td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.4.1" class="ltx_text" style="font-size:90%;">200</span></td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.5.1" class="ltx_text" style="font-size:90%;">200</span></td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.6.1" class="ltx_text" style="font-size:90%;">350</span></td>
<td id="S4.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.7.1" class="ltx_text" style="font-size:90%;">350</span></td>
<td id="S4.T1.2.2.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.8.1" class="ltx_text" style="font-size:90%;">350</span></td>
</tr>
<tr id="S4.T1.2.3.3" class="ltx_tr">
<th id="S4.T1.2.3.3.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Batch size</span></th>
<td id="S4.T1.2.3.3.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S4.T1.2.3.3.2.1" class="ltx_text" style="font-size:90%;">1024</span></td>
<td id="S4.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.3.3.3.1" class="ltx_text" style="font-size:90%;">4096</span></td>
<td id="S4.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.3.3.4.1" class="ltx_text" style="font-size:90%;">4096</span></td>
<td id="S4.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.3.3.5.1" class="ltx_text" style="font-size:90%;">4096</span></td>
<td id="S4.T1.2.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.3.3.6.1" class="ltx_text" style="font-size:90%;">4096</span></td>
<td id="S4.T1.2.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.3.3.7.1" class="ltx_text" style="font-size:90%;">4096</span></td>
<td id="S4.T1.2.3.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.2.3.3.8.1" class="ltx_text" style="font-size:90%;">4096</span></td>
</tr>
<tr id="S4.T1.2.4.4" class="ltx_tr">
<th id="S4.T1.2.4.4.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Optimizer</span></th>
<td id="S4.T1.2.4.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.2.4.4.2.1" class="ltx_text" style="font-size:90%;">Momentum</span></td>
<td id="S4.T1.2.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.4.3.1" class="ltx_text" style="font-size:90%;">Momentum</span></td>
<td id="S4.T1.2.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.4.4.1" class="ltx_text" style="font-size:90%;">Momentum</span></td>
<td id="S4.T1.2.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.4.5.1" class="ltx_text" style="font-size:90%;">Momentum</span></td>
<td id="S4.T1.2.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.4.6.1" class="ltx_text" style="font-size:90%;">Momentum</span></td>
<td id="S4.T1.2.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.4.7.1" class="ltx_text" style="font-size:90%;">Momentum</span></td>
<td id="S4.T1.2.4.4.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.4.4.8.1" class="ltx_text" style="font-size:90%;">Momentum</span></td>
</tr>
<tr id="S4.T1.2.5.5" class="ltx_tr">
<th id="S4.T1.2.5.5.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.2.5.5.1.1" class="ltx_text" style="font-size:90%;">Learning rate</span></th>
<td id="S4.T1.2.5.5.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.2.5.5.2.1" class="ltx_text" style="font-size:90%;">0.4</span></td>
<td id="S4.T1.2.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.5.3.1" class="ltx_text" style="font-size:90%;">1.6</span></td>
<td id="S4.T1.2.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.5.4.1" class="ltx_text" style="font-size:90%;">1.6</span></td>
<td id="S4.T1.2.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.5.5.1" class="ltx_text" style="font-size:90%;">1.6</span></td>
<td id="S4.T1.2.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.5.6.1" class="ltx_text" style="font-size:90%;">1.6</span></td>
<td id="S4.T1.2.5.5.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.5.7.1" class="ltx_text" style="font-size:90%;">1.6</span></td>
<td id="S4.T1.2.5.5.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.5.5.8.1" class="ltx_text" style="font-size:90%;">1.6</span></td>
</tr>
<tr id="S4.T1.2.6.6" class="ltx_tr">
<th id="S4.T1.2.6.6.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.2.6.6.1.1" class="ltx_text" style="font-size:90%;">Decay method</span></th>
<td id="S4.T1.2.6.6.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.2.6.6.2.1" class="ltx_text" style="font-size:90%;">Stepwise</span></td>
<td id="S4.T1.2.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.6.3.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T1.2.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.6.4.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T1.2.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.6.5.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T1.2.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.6.6.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T1.2.6.6.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.6.7.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T1.2.6.6.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.6.6.8.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
</tr>
<tr id="S4.T1.2.7.7" class="ltx_tr">
<th id="S4.T1.2.7.7.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.2.7.7.1.1" class="ltx_text" style="font-size:90%;">Weight decay</span></th>
<td id="S4.T1.2.7.7.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.2.7.7.2.1" class="ltx_text" style="font-size:90%;">1e-4</span></td>
<td id="S4.T1.2.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.7.7.3.1" class="ltx_text" style="font-size:90%;">1e-4</span></td>
<td id="S4.T1.2.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.7.7.4.1" class="ltx_text" style="font-size:90%;">1e-4</span></td>
<td id="S4.T1.2.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.7.7.5.1" class="ltx_text" style="font-size:90%;">1e-4</span></td>
<td id="S4.T1.2.7.7.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.7.7.6.1" class="ltx_text" style="font-size:90%;">4e-5</span></td>
<td id="S4.T1.2.7.7.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.7.7.7.1" class="ltx_text" style="font-size:90%;">4e-5</span></td>
<td id="S4.T1.2.7.7.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.7.7.8.1" class="ltx_text" style="font-size:90%;">4e-5</span></td>
</tr>
<tr id="S4.T1.2.8.8" class="ltx_tr">
<th id="S4.T1.2.8.8.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.2.8.8.1.1" class="ltx_text" style="font-size:90%;">Warmup epochs</span></th>
<td id="S4.T1.2.8.8.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.2.8.8.2.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T1.2.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.8.8.3.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T1.2.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.8.8.4.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T1.2.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.8.8.5.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T1.2.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.8.8.6.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T1.2.8.8.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.8.8.7.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T1.2.8.8.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.8.8.8.1" class="ltx_text" style="font-size:90%;">5</span></td>
</tr>
<tr id="S4.T1.2.9.9" class="ltx_tr">
<th id="S4.T1.2.9.9.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.2.9.9.1.1" class="ltx_text" style="font-size:90%;">Label smoothing</span></th>
<td id="S4.T1.2.9.9.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.2.9.9.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T1.2.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.9.9.3.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="S4.T1.2.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.9.9.4.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="S4.T1.2.9.9.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.9.9.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="S4.T1.2.9.9.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.9.9.6.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="S4.T1.2.9.9.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.9.9.7.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="S4.T1.2.9.9.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.9.9.8.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
</tr>
<tr id="S4.T1.2.10.10" class="ltx_tr">
<th id="S4.T1.2.10.10.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.2.10.10.1.1" class="ltx_text" style="font-size:90%;">Dropout rate</span></th>
<td id="S4.T1.2.10.10.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.2.10.10.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T1.2.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.10.10.3.1" class="ltx_text" style="font-size:90%;">0.25</span></td>
<td id="S4.T1.2.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.10.10.4.1" class="ltx_text" style="font-size:90%;">0.25</span></td>
<td id="S4.T1.2.10.10.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.10.10.5.1" class="ltx_text" style="font-size:90%;">0.25</span></td>
<td id="S4.T1.2.10.10.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.10.10.6.1" class="ltx_text" style="font-size:90%;">0.25</span></td>
<td id="S4.T1.2.10.10.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.10.10.7.1" class="ltx_text" style="font-size:90%;">0.25</span></td>
<td id="S4.T1.2.10.10.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.10.10.8.1" class="ltx_text" style="font-size:90%;">0.25</span></td>
</tr>
<tr id="S4.T1.2.11.11" class="ltx_tr">
<th id="S4.T1.2.11.11.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T1.2.11.11.1.1" class="ltx_text" style="font-size:90%;">Rand Augment</span></th>
<td id="S4.T1.2.11.11.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T1.2.11.11.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T1.2.11.11.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.2.11.11.3.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T1.2.11.11.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.2.11.11.4.1" class="ltx_text" style="font-size:90%;">15</span></td>
<td id="S4.T1.2.11.11.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.2.11.11.5.1" class="ltx_text" style="font-size:90%;">15</span></td>
<td id="S4.T1.2.11.11.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.2.11.11.6.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T1.2.11.11.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.2.11.11.7.1" class="ltx_text" style="font-size:90%;">15</span></td>
<td id="S4.T1.2.11.11.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T1.2.11.11.8.1" class="ltx_text" style="font-size:90%;">15</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table A.1: </span>Hyper-parameters used to train ConvNet architectures including ResNet-50 (CAS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, ResNet-50, ResNet-101, ResNet-152, ResNet-RS-50, ResNet-RS-101, and ResNet-RS-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T2.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<td id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.2.1" class="ltx_text" style="font-size:90%;">ViT-S/16</span></td>
<td id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.3.1" class="ltx_text" style="font-size:90%;">DeiT-S</span></td>
<td id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.4.1" class="ltx_text" style="font-size:90%;">DeiT-B</span></td>
<td id="S4.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.5.1" class="ltx_text" style="font-size:90%;">DeiT-L</span></td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Epochs</span></th>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">300</span></td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.3.1" class="ltx_text" style="font-size:90%;">300</span></td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.4.1" class="ltx_text" style="font-size:90%;">300</span></td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.5.1" class="ltx_text" style="font-size:90%;">300</span></td>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<th id="S4.T2.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Batch size</span></th>
<td id="S4.T2.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.3.3.2.1" class="ltx_text" style="font-size:90%;">1024</span></td>
<td id="S4.T2.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.3.3.3.1" class="ltx_text" style="font-size:90%;">4096</span></td>
<td id="S4.T2.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.3.3.4.1" class="ltx_text" style="font-size:90%;">4096</span></td>
<td id="S4.T2.2.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.3.3.5.1" class="ltx_text" style="font-size:90%;">4096</span></td>
</tr>
<tr id="S4.T2.2.4.4" class="ltx_tr">
<th id="S4.T2.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Optimizer</span></th>
<td id="S4.T2.2.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.2.1" class="ltx_text" style="font-size:90%;">AdamW</span></td>
<td id="S4.T2.2.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.3.1" class="ltx_text" style="font-size:90%;">AdamW</span></td>
<td id="S4.T2.2.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.4.1" class="ltx_text" style="font-size:90%;">AdamW</span></td>
<td id="S4.T2.2.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.5.1" class="ltx_text" style="font-size:90%;">AdamW</span></td>
</tr>
<tr id="S4.T2.2.5.5" class="ltx_tr">
<th id="S4.T2.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.5.5.1.1" class="ltx_text" style="font-size:90%;">Learning rate</span></th>
<td id="S4.T2.2.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.2.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="S4.T2.2.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.3.1" class="ltx_text" style="font-size:90%;">0.004</span></td>
<td id="S4.T2.2.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.4.1" class="ltx_text" style="font-size:90%;">0.004</span></td>
<td id="S4.T2.2.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.5.1" class="ltx_text" style="font-size:90%;">0.004</span></td>
</tr>
<tr id="S4.T2.2.6.6" class="ltx_tr">
<th id="S4.T2.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.6.6.1.1" class="ltx_text" style="font-size:90%;">Learning rate decay</span></th>
<td id="S4.T2.2.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.2.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T2.2.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.3.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T2.2.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.4.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
<td id="S4.T2.2.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.5.1" class="ltx_text" style="font-size:90%;">Cosine</span></td>
</tr>
<tr id="S4.T2.2.7.7" class="ltx_tr">
<th id="S4.T2.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.7.7.1.1" class="ltx_text" style="font-size:90%;">Weight decay</span></th>
<td id="S4.T2.2.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.2.1" class="ltx_text" style="font-size:90%;">0.0001</span></td>
<td id="S4.T2.2.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S4.T2.2.8.8" class="ltx_tr">
<th id="S4.T2.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.8.8.1.1" class="ltx_text" style="font-size:90%;">Warmup eepochs</span></th>
<td id="S4.T2.2.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.2.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T2.2.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.3.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T2.2.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.4.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="S4.T2.2.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.5.1" class="ltx_text" style="font-size:90%;">5</span></td>
</tr>
<tr id="S4.T2.2.9.9" class="ltx_tr">
<th id="S4.T2.2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.9.9.1.1" class="ltx_text" style="font-size:90%;">Label dmoothing</span></th>
<td id="S4.T2.2.9.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.3.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="S4.T2.2.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.4.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="S4.T2.2.9.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
</tr>
<tr id="S4.T2.2.10.10" class="ltx_tr">
<th id="S4.T2.2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.10.10.1.1" class="ltx_text" style="font-size:90%;">Rand Augment</span></th>
<td id="S4.T2.2.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.10.10.2.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S4.T2.2.10.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.10.10.3.1" class="ltx_text" style="font-size:90%;">9</span></td>
<td id="S4.T2.2.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.10.10.4.1" class="ltx_text" style="font-size:90%;">9</span></td>
<td id="S4.T2.2.10.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.10.10.5.1" class="ltx_text" style="font-size:90%;">9</span></td>
</tr>
<tr id="S4.T2.2.11.11" class="ltx_tr">
<th id="S4.T2.2.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.11.11.1.1" class="ltx_text" style="font-size:90%;">Mixup prob.</span></th>
<td id="S4.T2.2.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.11.11.2.1" class="ltx_text" style="font-size:90%;">0.2</span></td>
<td id="S4.T2.2.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.11.11.3.1" class="ltx_text" style="font-size:90%;">0.8</span></td>
<td id="S4.T2.2.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.11.11.4.1" class="ltx_text" style="font-size:90%;">0.8</span></td>
<td id="S4.T2.2.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.11.11.5.1" class="ltx_text" style="font-size:90%;">0.8</span></td>
</tr>
<tr id="S4.T2.2.12.12" class="ltx_tr">
<th id="S4.T2.2.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T2.2.12.12.1.1" class="ltx_text" style="font-size:90%;">Cutmix prob.</span></th>
<td id="S4.T2.2.12.12.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.12.12.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.2.12.12.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.12.12.3.1" class="ltx_text" style="font-size:90%;">1.0</span></td>
<td id="S4.T2.2.12.12.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.12.12.4.1" class="ltx_text" style="font-size:90%;">1.0</span></td>
<td id="S4.T2.2.12.12.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.12.12.5.1" class="ltx_text" style="font-size:90%;">1.0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table A.2: </span>Hyper-parameters used to train the vision transformer architectures, i.e., ViT-S/16¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, DeiT-S¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, DeiT-B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, and DeiT-L¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.08465" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.08466" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.08466">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.08466" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.08467" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 14:09:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
